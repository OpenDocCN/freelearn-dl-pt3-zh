["```\nIn: (0, 0, 0, 0, 1, 0, 1, 0, 1, 0) \nOut: 3\n```", "```\nimport numpy as np\n\n# The first dimension represents the mini-batch\nx = np.array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0]])\n\ny = np.array([3])\n```", "```\ndef step(s, x, U, W):\n   return x * U + s * W\n```", "```\ndef forward(x, U, W):\n    # Number of samples in the mini-batch\n    number_of_samples = len(x)\n\n    # Length of each sample\n    sequence_length = len(x[0])\n\n    # Initialize the state activation for each sample along the sequence\n    s = np.zeros((number_of_samples, sequence_length + 1))\n\n    # Update the states over the sequence\n    for t in range(0, sequence_length):\n        s[:, t + 1] = step(s[:, t], x[:, t], U, W)  # step function\n\n    return s\n```", "```\ndef backward(x, s, y, W):\n    sequence_length = len(x[0])\n\n    # The network output is just the last activation of sequence\n    s_t = s[:, -1]\n\n    # Compute the gradient of the output w.r.t. MSE loss function \n      at final state\n    gS = 2 * (s_t - y)\n\n    # Set the gradient accumulations to 0\n    gU, gW = 0, 0\n\n    # Accumulate gradients backwards\n    for k in range(sequence_length, 0, -1):\n        # Compute the parameter gradients and accumulate the\n          results\n        gU += np.sum(gS * x[:, k - 1])\n        gW += np.sum(gS * s[:, k - 1])\n\n        # Compute the gradient at the output of the previous layer\n        gS = gS * W\n\n    return gU, gW\n```", "```\ndef train(x, y, epochs, learning_rate=0.0005):\n    \"\"\"Train the network\"\"\"\n\n    # Set initial parameters\n    weights = (-2, 0) # (U, W)\n\n    # Accumulate the losses and their respective weights\n    losses = list()\n    gradients_u = list()\n    gradients_w = list()\n\n    # Perform iterative gradient descent\n    for i in range(epochs):\n        # Perform forward and backward pass to get the gradients\n        s = forward(x, weights[0], weights[1])\n\n        # Compute the loss\n        loss = (y[0] - s[-1, -1]) ** 2\n\n        # Store the loss and weights values for later display\n        losses.append(loss)\n\n        gradients = backward(x, s, y, weights[1])\n        gradients_u.append(gradients[0])\n        gradients_w.append(gradients[1])\n\n        # Update each parameter `p` by p = p - (gradient *\n          learning_rate).\n        # `gp` is the gradient of parameter `p`\n        weights = tuple((p - gp * learning_rate) for p, gp in\n        zip(weights, gradients))\n\n    print(weights)\n\n    return np.array(losses), np.array(gradients_u),\n    np.array(gradients_w)\n```", "```\ndef plot_training(losses, gradients_u, gradients_w):\n    import matplotlib.pyplot as plt\n\n    # remove nan and inf values\n    losses = losses[~np.isnan(losses)][:-1]\n    gradients_u = gradients_u[~np.isnan(gradients_u)][:-1]\n    gradients_w = gradients_w[~np.isnan(gradients_w)][:-1]\n\n    # plot the weights U and W\n    fig, ax1 = plt.subplots(figsize=(5, 3.4))\n\n    ax1.set_ylim(-3, 20)\n    ax1.set_xlabel('epochs')\n    ax1.plot(gradients_u, label='grad U', color='blue',\n    linestyle=':')\n    ax1.plot(gradients_w, label='grad W', color='red', linestyle='--\n    ')\n    ax1.legend(loc='upper left')\n\n    # instantiate a second axis that shares the same x-axis\n    # plot the loss on the second axis\n    ax2 = ax1.twinx()\n\n    # uncomment to plot exploding gradients\n    ax2.set_ylim(-3, 10)\n    ax2.plot(losses, label='Loss', color='green')\n    ax2.tick_params(axis='y', labelcolor='green')\n    ax2.legend(loc='upper right')\n\n    fig.tight_layout()\n\n    plt.show()\n```", "```\nlosses, gradients_u, gradients_w = train(x, y, epochs=150)\nplot_training(losses, gradients_u, gradients_w)\n```", "```\nx = np.array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]])\n\ny = np.array([12])\n\nlosses, gradients_u, gradients_w = train(x, y, epochs=150)\nplot_training(losses, gradients_u, gradients_w)\n```", "```\nSum of ones RNN from scratch\nchapter07-rnn/simple_rnn.py:5: RuntimeWarning: overflow encountered in multiply\n  return x * U + s * W\nchapter07-rnn/simple_rnn.py:40: RuntimeWarning: invalid value encountered in multiply\n  gU += np.sum(gS * x[:, k - 1])\nchapter07-rnn/simple_rnn.py:41: RuntimeWarning: invalid value encountered in multiply\n  gW += np.sum(gS * s[:, k - 1])\n(nan, nan)\n```", "```\nimport math\nimport typing\n\nimport torch\n```", "```\nclass LSTMCell(torch.nn.Module):\n\n    def __init__(self, input_size: int, hidden_size: int):\n        \"\"\"\n        :param input_size: input vector size\n        :param hidden_size: cell state vector size\n        \"\"\"\n\n        super(LSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # combine all gates in a single matrix multiplication\n        self.x_fc = torch.nn.Linear(input_size, 4 * hidden_size)\n        self.h_fc = torch.nn.Linear(hidden_size, 4 * hidden_size)\n\n        self.reset_parameters()\n```", "```\ndef reset_parameters(self):\n    \"\"\"Xavier initialization \"\"\"\n    size = math.sqrt(3.0 / self.hidden_size)\n    for weight in self.parameters():\n        weight.data.uniform_(-size, size)\n```", "```\ndef forward(self,\n            x_t: torch.Tensor,\n            hidden: typing.Tuple[torch.Tensor, torch.Tensor] =      (None, None)) \\\n        -> typing.Tuple[torch.Tensor, torch.Tensor]:\n    h_t_1, c_t_1 = hidden # t_1 is equivalent to t-1\n\n    # in case of more than 2-dimensional input\n    # flatten the tensor (similar to numpy.reshape)\n    x_t = x_t.view(-1, x_t.size(1))\n    h_t_1 = h_t_1.view(-1, h_t_1.size(1))\n    c_t_1 = c_t_1.view(-1, c_t_1.size(1))\n```", "```\ngates = self.x_fc(x_t) + self.h_fc(h_t_1)\n```", "```\ni_t, f_t, candidate_c_t, o_t = gates.chunk(4, 1)\n```", "```\ni_t, f_t, candidate_c_t, o_t = \\\n    i_t.sigmoid(), f_t.sigmoid(), candidate_c_t.tanh(), o_t.sigmoid()\n```", "```\nc_t = torch.mul(f_t, c_t_1) + torch.mul(i_t, candidate_c_t)\n```", "```\nh_t = torch.mul(o_t, torch.tanh(c_t))\nreturn h_t, c_t\n```", "```\nclass LSTMModel(torch.nn.Module):\n    def __init__(self, input_dim, hidden_size, output_dim):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n\n        # Our own LSTM implementation\n        self.lstm = LSTMCell(input_dim, hidden_size)\n\n        # Fully connected output layer\n        self.fc = torch.nn.Linear(hidden_size, output_dim)\n\n    def forward(self, x):\n        # Start with empty network output and cell state to initialize the sequence\n        c_t = torch.zeros((x.size(0), self.hidden_size)).to(x.device)\n        h_t = torch.zeros((x.size(0), self.hidden_size)).to(x.device)\n\n        # Iterate over all sequence elements across all sequences of the mini-batch\n        for seq in range(x.size(1)):\n            h_t, c_t = self.lstm(x[:, seq, :], (h_t, c_t))\n\n        # Final output layer\n        return self.fc(h_t)\n```", "```\ntrain = generate_dataset(SEQUENCE_LENGTH, TRAINING_SAMPLES)\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n\ntest = generate_dataset(SEQUENCE_LENGTH, TEST_SAMPLES)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)\n```", "```\nmodel = LSTMModel(input_size=1, hidden_size=HIDDEN_UNITS, output_size=1)\n```", "```\nloss_function = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\n```", "```\nfor epoch in range(EPOCHS):\n    print('Epoch {}/{}'.format(epoch + 1, EPOCHS))\n\n    train_model(model, loss_function, optimizer, train_loader)\n    test_model(model, loss_function, test_loader)\n```", "```\nimport math\nimport torch\n```", "```\nclass GRUCell(torch.nn.Module):\n\n    def __init__(self, input_size: int, hidden_size: int):\n        \"\"\"\n        :param input_size: input vector size\n        :param hidden_size: cell state vector size\n        \"\"\"\n\n        super(GRUCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # x to reset gate r\n        self.x_r_fc = torch.nn.Linear(input_size, hidden_size)\n\n        # x to update gate z\n        self.x_z_fc = torch.nn.Linear(input_size, hidden_size)\n\n        # x to candidate state h'(t)\n        self.x_h_fc = torch.nn.Linear(input_size, hidden_size)\n\n        # network output/state h(t-1) to reset gate r\n        self.h_r_fc = torch.nn.Linear(hidden_size, hidden_size)\n\n        # network output/state h(t-1) to update gate z\n        self.h_z_fc = torch.nn.Linear(hidden_size, hidden_size)\n\n        # network state h(t-1) passed through the reset gate r towards candidate state h(t)\n        self.hr_h_fc = torch.nn.Linear(hidden_size, hidden_size)\n```", "```\ndef forward(self,\n            x_t: torch.Tensor,\n            h_t_1: torch.Tensor = None) \\\n        -> torch.Tensor:\n\n    # compute update gate vector\n    z_t = torch.sigmoid(self.x_z_fc(x_t) + self.h_z_fc(h_t_1))\n\n    # compute reset gate vector\n    r_t = torch.sigmoid(self.x_r_fc(x_t) + self.h_r_fc(h_t_1))\n```", "```\ncandidate_h_t = torch.tanh(self.x_h_fc(x_t) + self.hr_h_fc(torch.mul(r_t, h_t_1)))\n```", "```\nh_t = torch.mul(z_t, h_t_1) + torch.mul(1 - z_t, candidate_h_t)\n```", "```\nimport torch\nimport torchtext\n```", "```\nTEXT = torchtext.data.Field(\n    tokenize='spacy',  # use SpaCy tokenizer\n    lower=True,  # convert all letters to lower case\n    include_lengths=True,  # include the length of the movie review\n)\n```", "```\nLABEL = torchtext.data.LabelField(dtype=torch.float)\n```", "```\ntrain, test = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n```", "```\nTEXT.build_vocab(train, vectors=torchtext.vocab.GloVe(name='6B', dim=100))\nLABEL.build_vocab(train)\n```", "```\ntrain_iter, test_iter = torchtext.data.BucketIterator.splits(\n    (train, test), sort_within_batch=True, batch_size=64, device=device)\n```", "```\nclass LSTMModel(torch.nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, pad_idx):\n        super().__init__()\n\n        # Embedding field\n        self.embedding=torch.nn.Embedding(num_embeddings=vocab_size,\n        embedding_dim=embedding_size,padding_idx=pad_idx)\n\n        # LSTM cell\n        self.rnn = torch.nn.LSTM(input_size=embedding_size,\n        hidden_size=hidden_size)\n\n        # Fully connected output\n        self.fc = torch.nn.Linear(hidden_size, output_size)\n\n    def forward(self, text_sequence, text_lengths):\n        # Extract embedding vectors\n        embeddings = self.embedding(text_sequence)\n\n        # Pad the sequences to equal length\n        packed_sequence =torch.nn.utils.rnn.pack_padded_sequence\n        (embeddings, text_lengths)\n\n        packed_output, (hidden, cell) = self.rnn(packed_sequence)\n\n        return self.fc(hidden)\n\nmodel = LSTMModel(vocab_size=len(TEXT.vocab),\n                  embedding_size=EMBEDDING_SIZE,\n                  hidden_size=HIDDEN_SIZE,\n                  output_size=1,\n                  pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n```", "```\nmodel.embedding.weight.data.copy_(TEXT.vocab.vectors)\n```", "```\nmodel.embedding.weight.data[TEXT.vocab.stoi[TEXT.unk_token]] = torch.zeros(EMBEDDING_SIZE)\nmodel.embedding.weight.data[TEXT.vocab.stoi[TEXT.pad_token]] = torch.zeros(EMBEDDING_SIZE)\n```", "```\noptimizer = torch.optim.Adam(model.parameters())\nloss_function = torch.nn.BCEWithLogitsLoss().to(device)\n\nmodel = model.to(device)\n\nfor epoch in range(5):\n    print(f\"Epoch {epoch + 1}/5\")\n    train_model(model, loss_function, optimizer, train_iter)\n    test_model(model, loss_function, test_iter)\n```"]