- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Temporal Difference Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Temporal difference** (**TD**) learning is one of the most popular and widely
    used model-free methods. The reason for this is that TD learning combines the
    advantages of both the **dynamic programming** (**DP**) method and the **Monte
    Carlo** (**MC**) method we covered in the previous chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: We will begin the chapter by understanding how exactly TD learning is beneficial
    compared to DP and MC methods. Later, we will learn how to perform the prediction
    task using TD learning. Going forward, we will learn how to perform TD control
    tasks with an on-policy TD control method called SARSA and an off-policy TD control
    method called Q learning.
  prefs: []
  type: TYPE_NORMAL
- en: We will also learn how to find the optimal policy in the Frozen Lake environment
    using SARSA and the Q learning method. At the end of the chapter, we will compare
    the DP, MC, and TD methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: TD learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TD prediction method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TD control method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On-policy TD control – SARSA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Off-policy TD control – Q learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing SARSA and Q learning to find the optimal policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between Q learning and SARSA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the DP, MC, and TD methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TD learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The TD learning algorithm was introduced by Richard S. Sutton in 1988\. In the
    introduction of the chapter, we learned that the reason the TD method became popular
    is that it combines the advantages of DP and the MC method. But what are those
    advantages?
  prefs: []
  type: TYPE_NORMAL
- en: First, let's recap quickly the advantages and disadvantages of DP and the MC
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic programming**—The advantage of the DP method is that it uses the
    Bellman equation to compute the value of a state. That is, we have learned that
    according to the Bellman equation, the value of a state can be obtained as the
    sum of the immediate reward and the discounted value of the next state. This is
    called bootstrapping. That is, to compute the value of a state, we don''t have
    to wait till the end of the episode, instead, using the Bellman equation, we can
    estimate the value of a state just based on the value of the next state, and this
    is called bootstrapping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember how we estimated the value function in DP methods (value and policy
    iteration)? We estimated the value function (the value of a state) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: As you may recollect, we learned that in order to find the value of a state,
    we didn't have to wait till the end of the episode. Instead, we bootstrap, that
    is, we estimate the value of the current state *V*(*s*) by estimating the value
    of the next state ![](img/B15558_05_002.png).
  prefs: []
  type: TYPE_NORMAL
- en: However, the disadvantage of DP is that we can apply the DP method only when
    we know the model dynamics of the environment. That is, DP is a model-based method
    and we should know the transition probability in order to use it. When we don't
    know the model dynamics of the environment, we cannot apply the DP method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Monte Carlo method**—The advantage of the MC method is that it is a model-free
    method, which means that it does not require the model dynamics of the environment
    to be known in order to estimate the value and Q functions.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the disadvantage of the MC method is that in order to estimate the
    state value or Q value we need to wait until the end of the episode, and if the
    episode is long then it will cost us a lot of time. Also, we cannot apply MC methods
    to continuous tasks (non-episodic tasks).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's get back to TD learning. The TD learning algorithm takes the benefits
    of the DP and the MC methods into account. So, just like in DP, we perform bootstrapping
    so that we don't have to wait until the end of an episode to compute the state
    value or Q value, and just like the MC method, it is a model-free method and so
    it does not require the model dynamics of the environment to compute the state
    value or Q value. Now that we have the basic idea behind the TD learning algorithm,
    let's get into the details and learn exactly how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to what we learned in *Chapter 4*, *Monte Carlo Methods*, we can use
    the TD learning algorithm for both the prediction and control tasks, and so we
    can categorize TD learning into:'
  prefs: []
  type: TYPE_NORMAL
- en: TD prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TD control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learned what the prediction and control methods mean in the previous chapter.
    Let's recap that a bit before going forward.
  prefs: []
  type: TYPE_NORMAL
- en: In the prediction method, a policy is given as an input and we try to predict
    the value function or Q function using the given policy. If we predict the value
    function using the given policy, then we can say how good it is for the agent
    to be in each state if it uses the given policy. That is, we can say what the
    expected return an agent can get in each state if it acts according to the given
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: In the control method, we are not given a policy as input, and the goal in the
    control method is to find the optimal policy. So, we initialize a random policy
    and then we try to find the optimal policy iteratively. That is, we try to find
    an optimal policy that gives us the maximum return.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's see how to use TD learning to perform prediction task, and then
    we will learn how to use TD learning for the control task.
  prefs: []
  type: TYPE_NORMAL
- en: TD prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the TD prediction method, the policy is given as input and we try to estimate
    the value function using the given policy. TD learning bootstraps like DP, so
    it does not have to wait till the end of the episode, and like the MC method,
    it does not require the model dynamics of the environment to compute the value
    function or the Q function. Now, let's see how the update rule of TD learning
    is designed, taking the preceding advantages into account.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the MC method, we estimate the value of a state by taking its return:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, a single return value cannot approximate the value of a state perfectly.
    So, we generate **N** episodes and compute the value of a state as the average
    return of a state across **N** episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_013.png)'
  prefs: []
  type: TYPE_IMG
- en: But with the MC method, we need to wait until the end of the episode to compute
    the value of a state and when the episode is long, it takes a lot of time. One
    more problem with the MC method is that we cannot apply it to non-episodic tasks
    (continuous tasks).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in TD learning, we make use of bootstrapping and estimate the value of
    a state as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_005.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation tells us that we can estimate the value of the state
    by only taking the immediate reward *r* and the discounted value of the next state
    ![](img/B15558_05_006.png). As you may observe from the preceding equation, similar
    to what we learned in DP methods (value and policy iteration), we perform bootstrapping
    but here we don't need to know the model dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, using TD learning, the value of a state is approximated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_005.png)'
  prefs: []
  type: TYPE_IMG
- en: However, a single value of ![](img/B15558_05_008.png) cannot approximate the
    value of a state perfectly. So, we can take a mean value and instead of taking
    an arithmetic mean, we can use the incremental mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the MC method, we learned how to use the incremental mean to estimate the
    value of the state and it given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, here in TD learning, we can use the incremental mean and estimate
    the value of the state, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This equation is called the TD learning update rule. As we can observe, the
    only difference between the TD learning and the MC method is that to compute the
    value of the state, in the MC method, we use the full return *R*, which is computed
    using the complete episode, whereas in the TD learning method, we use the bootstrap
    estimate ![](img/B15558_05_011.png) so that we don''t have to wait until the end
    of the episode to compute the value of the state. Thus, we can apply TD learning
    to non-episodic tasks as well. The following shows the difference between the
    MC method and TD learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: A comparison between MC and TD learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, our TD learning update rule is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We learned that ![](img/B15558_05_011.png) is an estimate of the value of state
    *V*(*s*). So, we can call ![](img/B15558_05_011.png) the TD target. Thus, subtracting
    *V*(*s*) from ![](img/B15558_05_011.png) implies that we are subtracting the predicted
    value from the target value, and this is usually called the TD error. Okay, what
    about that ![](img/B15558_05_016.png)? It is basically the learning rate, also
    called the step size. That is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our TD learning update rule basically implies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Value of a state = value of a state + learning rate (reward + discount factor(value
    of next state) - value of a state)*'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen the TD learning update rule and how TD learning is used
    to estimate the value of a state, in the next section, we will look into the TD
    prediction algorithm and get a clearer understanding of the TD learning method.
  prefs: []
  type: TYPE_NORMAL
- en: TD prediction algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned that, in the prediction task, given a policy, we estimate the value
    function using the given policy. So, we can say what the expected return an agent
    can obtain in each state if it acts according to the given policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that the TD learning update rule is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, using this equation, we can estimate the value function of the given policy.
  prefs: []
  type: TYPE_NORMAL
- en: Before looking into the algorithm directly, for better understanding, first,
    let's manually calculate and see how exactly the value of a state is estimated
    using the TD learning update rule.
  prefs: []
  type: TYPE_NORMAL
- en: The upcoming sections are explained with manual calculations, for a better understanding,
    follow along with a pen and paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore TD prediction with the Frozen Lake environment. We have learned
    that in the Frozen Lake environment, the goal of the agent is to reach the goal
    state **G** from the starting state **S** without visiting the hole states **H**.
    If the agent visits state **G**, we assign a reward of 1 and if it visits any
    other states, we assign a reward of 0\. *Figure 5.2* shows the Frozen Lake environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: The Frozen Lake environment'
  prefs: []
  type: TYPE_NORMAL
- en: We have four actions in our action space, which are *up*, *down*, *left*, and
    *right*, and we have 16 states from **S** to **G**. Instead of encoding the states
    and actions into numbers, for easier understanding, let's just keep them as they
    are. That is, let's just denote each action by the strings *up*, *down*, *left*,
    and *right*, and let's denote each state by their position in the grid. That is,
    the first state **S** is denoted by **(1,1)** and the second state **F** is denoted
    by **(1,2)** and so on to the last state **G**, which is denoted by **(4,4)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to perform TD prediction in the Frozen Lake environment.
    We know that in the TD prediction method, we will be given a policy and we predict the
    value function (state value) using a given policy. Let''s suppose we are given
    the following policy. It basically tells us what action to perform in each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 5.1: A policy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will see how to estimate the value function of the preceding policy
    using the TD learning method. Before going ahead, first, we initialize the values
    of all the states with random values, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Initialize the states with random values'
  prefs: []
  type: TYPE_NORMAL
- en: Say we are in state **(1,1)** and as per the given policy we take the *right*
    action and move to the next state **(1,2)**, and we receive a reward *r* of 0\.
    Let's keep the learning rate ![](img/B15558_05_016.png) as 0.1 and the discount
    factor ![](img/B15558_03_190.png) as 1 throughout this section. Now, how can we update
    the value of the state?
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the TD update equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the value of state *V*(*s*) with *V*(1,1) and the next state ![](img/B15558_05_020.png)
    with *V*(1,2) in the preceding equation, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the reward *r* = 0, the learning rate ![](img/B15558_05_022.png),
    and the discount factor ![](img/B15558_05_023.png), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can get the state values from the value table shown earlier. That is, from
    the preceding value table, we can observe that the value of state **(1,1)** is
    0.9 and the value of the next state **(1,2)** is 0.6\. Substituting these values
    in the preceding equation, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the value of state **(1,1)** becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we update the value of state **(1,1)** as **0.87** in the value table,
    as *Figure 5.4* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: The value of state (1,1) is updated'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are in state **(1,2)**. We select the *right* action according to the
    given policy in state **(1,2)** and move to the next state **(1,3)** and receive
    a reward *r* of 0\. We can compute the value of the state as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the value of state *V*(*s*) with *V*(1,2) and the next state ![](img/B15558_05_028.png)
    with *V*(1,3), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the reward *r* = 0, the learning rate ![](img/B15558_05_030.png),
    and the discount factor ![](img/B15558_05_031.png), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding value table, we can observe that the value of state **(1,2)**
    is 0.6 and the value of the next state **(1,3)** is 0.8, so we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the value of state **(1,2)** becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we update the value of state **(1,2)** to **0.62** in the value table,
    as *Figure 5.5* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: The value of state (1,2) is updated'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are in state **(1,3)**. We select the *left* action according to our
    policy and move to the next state **(1,2)** and receive a reward *r* of 0\. We
    can compute the value of the state as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the value of state *V*(*s*) with *V*(1,3) and the next state ![](img/B15558_05_020.png)
    with *V*(1,2), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the reward *r* = 0, the learning rate ![](img/B15558_05_038.png),
    and the discount factor ![](img/B15558_03_181.png), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that we use the updated values in every step, that is, the value of state
    **(1,2)** is updated with 0.62 in the previous step, as shown in the preceding
    value table. So, we substitute *V*(1,2) with 0.62 and *V*(1,3) with 0.8:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the value of state **(1,3)** becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we update the value of state **(1,3)** to **0.782** in the value table,
    as *Figure 5.6* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: The value of state (1,3) is updated'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in this way, we compute the value of every state using the given policy.
    However, computing the value of the state just for one episode will not be accurate.
    So, we repeat these steps for several episodes and compute the accurate estimates
    of the state value (the value function).
  prefs: []
  type: TYPE_NORMAL
- en: 'The TD prediction algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a value function *V*(*s*) with random values. A policy ![](img/B15558_03_082.png)
    is given.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize state *s*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform an action *a* in state *s* according to given policy ![](img/B15558_04_054.png),
    get the reward *r*, and move to the next state ![](img/B15558_05_045.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value of the state to ![](img/B15558_05_010.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update ![](img/B15558_05_047.png) (this step implies we are changing the next
    state ![](img/B15558_05_048.png) to the current state *s*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *s* is not the terminal state, repeat *steps 1* to *4*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned how the TD prediction method predicts the value function
    of the given policy, in the next section, let's learn how to implement the TD prediction
    method to predict the value of states in the Frozen Lake environment.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the value of states in the Frozen Lake environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have learned that in the prediction method, the policy is given as an input
    and we predict the value function using the given policy. So, let's initialize
    a random policy and predict the value function (state values) of the Frozen Lake
    environment using the random policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create the Frozen Lake environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the random policy, which returns the random action by sampling from
    the action space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the dictionary for storing the value of states, and we initialize
    the value of all the states to `0.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the discount factor ![](img/B15558_05_049.png) and the learning
    rate ![](img/B15558_05_050.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of episodes and the number of time steps in each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Compute the values of the states
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now, let's compute the value function (state values) using the given random
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For every step in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Select an action according to random policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action and store the next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the value of the state as ![](img/B15558_05_010.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the next state to the current state ![](img/B15558_05_052.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If the current state is the terminal state, then break:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: After all the iterations, we will have values of all the states according to
    the given random policy.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the values of the states
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s evaluate our value function (state values). First, let''s convert
    our value dictionary to a pandas data frame for more clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Before checking the values of the states, let''s recollect that in Gym, all
    the states in the Frozen Lake environment will be encoded into numbers. Since
    we have 16 states, all the states will be encoded into numbers from 0 to 15 as
    *Figure 5.7* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: States encoded as numbers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, Let''s check the value of the states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Value table'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, we now have the values of all the states. The value of state
    14 is high since we can reach goal state 15 from state 14 easily, and also, as
    we can see, the values of all the terminal states (hole states and the goal state)
    are zero.
  prefs: []
  type: TYPE_NORMAL
- en: Note that since we have initialized a random policy, you might get varying results
    every time you run the previous code.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how TD learning can be used for prediction tasks,
    in the next section, we will learn how to use TD learning for control tasks.
  prefs: []
  type: TYPE_NORMAL
- en: TD control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the control method, our goal is to find the optimal policy, so we will start
    off with an initial random policy and then we will try to find the optimal policy
    iteratively. In the previous chapter, we learned that the control method can be
    classified into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: On-policy control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Off-policy control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learned what on-policy and off-policy control means in the previous chapter.
    Let's recap that a bit before going ahead. In the **on-policy control**, the agent
    behaves using one policy and tries to improve the same policy. That is, in the
    on-policy method, we generate episodes using one policy and improve the same policy
    iteratively to find the optimal policy. In the **off-policy control** method,
    the agent behaves using one policy and tries to improve a different policy. That
    is, in the off-policy method, we generate episodes using one policy and we try
    to improve a different policy iteratively to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will learn how to perform control tasks using TD learning. First, we
    will learn how to perform on-policy TD control and then we will learn about off-policy
    TD control.
  prefs: []
  type: TYPE_NORMAL
- en: On-policy TD control – SARSA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look into the popular on-policy TD control algorithm
    called **SARSA**, which stands for **State-Action-Reward-State-Action**. We know
    that in TD control our goal is to find the optimal policy. First, how can we extract
    a policy? We can extract the policy from the Q function. That is, once we have
    the Q function then we can extract policy by selecting the action in each state
    that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, how can we compute the Q function in TD learning? First, let''s recall
    how we compute the value function. In TD learning, the value function is computed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can just rewrite this update rule in terms of the Q function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_054.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we compute the Q function using the preceding TD learning update rule,
    and then we extract a policy from them. We can also call the preceding update
    rule as the SARSA update rule.
  prefs: []
  type: TYPE_NORMAL
- en: But wait! In the prediction method, we were given a policy as input, so we acted
    in the environment using that policy and computed the value function. But here,
    we don't have a policy as input. So how can we act in the environment?
  prefs: []
  type: TYPE_NORMAL
- en: So, first we initialize the Q function with random values or with zeros. Then
    we extract a policy from this randomly initialized Q function and act in the environment.
    Our initial policy will definitely not be optimal as it is extracted from the
    randomly initialized Q function, but on every episode, we will update the Q function
    (Q values). So, on every episode, we can use the updated Q function to extract
    a new policy. Thus, we will obtain the optimal policy after a series of episodes.
  prefs: []
  type: TYPE_NORMAL
- en: One important point we need to note is that in the SARSA method, instead of
    making our policy act greedily, we use the epsilon-greedy policy. That is, in
    a greedy policy, we always select the action that has the maximum Q value. But,
    with the epsilon-greedy policy we select a random action with probability epsilon,
    and we select the best action (the action with the maximum Q value) with probability
    1-epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: Before looking into the algorithm directly, for a better understanding, first,
    let's manually calculate and see how exactly the Q function (Q value) is estimated
    using the SARSA update rule and how we can find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider the same Frozen Lake environment. Before going ahead, we initialize
    our Q table (Q function) with random values. *Figure 5.9* shows the Frozen Lake
    environment along with the Q table containing random values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: The Frozen Lake environment and Q table with random values'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we are in state **(4,2)**. Now we need to select an action in this
    state. How can we select an action? We learned that in the SARSA method, we select
    an action based on the epsilon-greedy policy. With probability epsilon, we select
    a random action and with probability 1-epsilon we select the best action (the
    action that has the maximum Q value). Suppose we use a probability 1-epsilon and
    select the best action. So, in state **(4,2)**, we move *right* as it has the
    highest Q value compared to the other actions, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Our agent is in state (4,2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so, we perform the *right* action in state **(4,2)** and move to the
    next state **(4,3)** as *Figure 5.11* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: We perform the action with the maximum Q value in state (4,2)'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we moved *right* in state **(4,2)** to the next state **(4,3)** and received
    a reward *r* of 0\. Let's keep the learning rate ![](img/B15558_05_055.png) at
    0.1, and the discount factor ![](img/B15558_05_056.png) at 1\. Now, how can we
    update the Q value?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let recall our SARSA update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_054.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the state-action pair *Q*(*s*,*a*) with *Q*((4,2), right) and
    the next state ![](img/B15558_03_001.png) with **(4,3)** in the preceding equation,
    we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_059.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the reward *r* = 0, the learning rate ![](img/B15558_05_030.png),
    and the discount factor ![](img/B15558_05_061.png), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_062.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous Q table, we can observe that the Q value of *Q*((4,2), right)
    is **0.8**. Thus, substituting *Q*((4,2), right) with **0.8**, we can rewrite
    the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_063.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, what about the term ![](img/B15558_05_064.png)? As you can see in the
    preceding equation, we have the term ![](img/B15558_05_064.png), which represents
    the Q value of the next state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: Because we have moved to the next state **(4,3)**, we need to select an action
    in this state in order to compute the Q value of the next state-action pair. So,
    we use our same epsilon-greedy policy to select the action. That is, we select
    a random action with a probability of epsilon, or we select the best action that
    has the maximum Q value with a probability of 1-epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we use probability epsilon and select the random action. In state **(4,3)**,
    we select the *right* action randomly, as *Figure 5.12* shows. As you can see,
    although the *right* action does not have the maximum Q value, we selected it
    randomly with probability epsilon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: We perform a random action in state (4,3)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, now our update rule becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_066.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding Q table, we can see that the Q value of **Q((4,3), right)**
    is **0.9**. Thus, substituting the value of **Q((4,3), right)** with **0.9**,
    we can rewrite the above equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_067.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our Q value becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_068.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, in this way, we update the Q function by updating the Q value of the state-action
    pair in each step of the episode. After completing an episode, we extract a new
    policy from the updated Q function and uses this new policy to act in the environment.
    (Remember that our policy is always an epsilon-greedy policy). We repeat this
    steps for several episodes to find the optimal policy. The SARSA algorithm given
    in the following will help us understand this better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SARSA algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a Q function *Q*(*s*, *a*) with random values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize state *s*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract a policy from *Q*(*s*, *a*) and select an action *a* to perform in state
    *s*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the action *a* and move to the next state ![](img/B15558_05_069.png)
    and observe the reward *r*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In state ![](img/B15558_05_069.png), select the action ![](img/B15558_05_071.png)
    using the epsilon-greedy policy
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Q value to ![](img/B15558_05_072.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update ![](img/B15558_05_052.png) and ![](img/B15558_05_074.png) (update the
    next state ![](img/B15558_05_075.png)-action ![](img/B15558_05_076.png) pair to
    the current state *s*-action *a* pair)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *s* is not a terminal state, repeat *steps 1* to *5*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned how the SARSA algorithm works, in the next section,
    let's implement the SARSA algorithm to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the optimal policy using SARSA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's implement SARSA to find the optimal policy in the Frozen Lake environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create the Frozen Lake environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the dictionary for storing the Q value of the state-action pair
    and initialize the Q value of all the state-action pairs to `0.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the epsilon-greedy policy. We generate a random number from
    the uniform distribution and if the random number is less than epsilon, we select
    the random action, else we select the best action that has the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the discount factor ![](img/B15558_03_005.png), the learning rate
    ![](img/B15558_05_055.png), and the epsilon value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of episodes and number of time steps in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Compute the policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action using the epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For each step in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action and store the next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action ![](img/B15558_05_079.png) in the next state ![](img/B15558_03_046.png)
    using the epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the Q value of the state-action pair as ![](img/B15558_05_072.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Update ![](img/B15558_05_082.png) and ![](img/B15558_05_083.png) (update the
    next state ![](img/B15558_03_073.png)-action ![](img/B15558_05_076.png) pair to
    the current state *s*-action *a* pair):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If the current state is the terminal state, then break:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that on every iteration we update the Q function. After all the iterations,
    we will have the optimal Q function. Once we have the optimal Q function then
    we can extract the optimal policy by selecting the action that has the maximum
    Q value in each state.
  prefs: []
  type: TYPE_NORMAL
- en: Off-policy TD control – Q learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will learn the off-policy TD control algorithm called Q
    learning. It is one of the very popular algorithms in reinforcement learning,
    and we will see that this algorithm keeps coming up in other chapters too. Q learning
    is an off-policy algorithm, meaning that we use two different policies, one policy
    for behaving in the environment (selecting an action in the environment) and the
    other for finding the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that in the SARSA method, we select action *a* in state *s* using
    the epsilon-greedy policy, move to the next state ![](img/B15558_03_018.png),
    and update the Q value using the update rule shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_054.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, in order to compute the Q value of next state-action
    pair, ![](img/B15558_05_088.png), we need to select an action. So, we select the
    action using the same epsilon-greedy policy and update the Q value of the next
    state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: But unlike SARSA, in Q learning, we use two different policies. One is the epsilon-greedy
    policy and the other is a greedy policy. To select an action in the environment
    we use an epsilon-greedy policy, but while updating the Q value of the next state-action
    pair we use a greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, we select action *a* in state *s* using the epsilon-greedy policy
    and move to the next state ![](img/B15558_05_048.png) and update the Q value using
    the update rule shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_054.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, in order to compute the Q value of the next state-action
    pair, ![](img/B15558_05_091.png), we need to select an action. Here, we select
    the action using the greedy policy and update the Q value of the next state-action
    pair. We know that the greedy policy always selects the action that has the maximum
    value. So, we can modify the equation to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_092.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe from the preceding equation, the **max** operator implies
    that in state ![](img/B15558_05_048.png), we select the action ![](img/B15558_05_094.png)
    that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to sum up, in the Q learning method we select an action in the environment
    using the epsilon-greedy policy, but while computing the Q value of the next state-action
    pair we use the greedy policy. Thus, update rule of Q learning is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_092.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand this better by manually calculating the Q value using our
    Q learning update rule. Let''s use the same Frozen Lake example. We initialize
    our Q table with random values. *Figure 5.13* shows the Frozen Lake environment,
    along with the Q table containing random values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: The Frozen Lake environment with a randomly initialized Q table'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we are in state **(3,2)**. Now, we need to select some action in this
    state. How can we select an action? We select an action using the epsilon-greedy
    policy. So, with probability epsilon, we select a random action and with probability
    1-epsilon we select the best action that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we use probability 1-epsilon and select the best action. So, in state **(3,2)**,
    we select the *down* action as it has the highest Q value compared to other actions
    in that state, as *Figure 5.14* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: We perform the action with the maximum Q value in state (3,2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so, we perform the *down* action in state **(3,2)** and move to the next
    state **(4,2)**, as *Figure 5.15* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: We move down to state (4,2)'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we move *down* in state **(3,2)** to the next state **(4,2)** and receive
    a reward *r* of 0\. Let's keep the learning rate ![](img/B15558_05_016.png) as
    0.1, and the discount factor ![](img/B15558_03_035.png) as 1\. Now, how can we
    update the Q value?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s recall our Q learning update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_098.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the state-action pair *Q*(*s*,*a*) with *Q*((3,2), down) and the
    next state ![](img/B15558_03_018.png) with **(4,2)** in the preceding equation,
    we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_100.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the reward, *r* = 0, the learning rate ![](img/B15558_05_101.png),
    and the discount factor ![](img/B15558_05_102.png), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_103.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous Q table, we can observe that the Q value of *Q*((3,2), down)
    is **0.8**. Thus, substituting *Q*((3,2), down) with **0.8**, we can rewrite the
    preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_104.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe, in the preceding equation we have the term ![](img/B15558_05_105.png),
    which represents the Q value of the next state-action pair as we moved to the
    new state **(4,2)**. In order to compute the Q value for the next state, first
    we need to select an action. Here, we select an action using the greedy policy,
    that is, the action that has maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'As *Figure 5.16* shows, the *right* action has the maximum Q value in state
    **(4,2)**. So, we select the *right* action and update the Q value of the next
    state-action pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: We perform the action with the maximum Q value in state (4,2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, now our update rule becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_106.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous Q table, we can observe that the Q value of *Q*((4,2), right)
    is **0.8**. Thus, substituting the value of *Q*((4,2), right) with **0.8**, we
    can rewrite the above equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_107.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our Q value becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_108.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we update the Q value for all state-action pairs. That is, we select
    an action in the environment using an epsilon-greedy policy, and while updating
    the Q value of the next state-action pair we use the greedy policy. Thus, we update
    the Q value for every state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in this way, we update the Q function by updating the Q value of the state-action
    pair in each step of the episode. We will extract a new policy from the updated
    Q function on every step of the episode and uses this new policy. (Remember that
    we select an action in the environment using epsilon-greedy policy but while updating
    Q value of the next state-action pair we use the greedy policy). After several
    episodes, we will have the optimal Q function. The Q learning algorithm given
    in the following will help us to understand this better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Q learning algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a Q function *Q*(*s*, *a*) with random values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize state *s*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract a policy from *Q*(*s*, *a*) and select an action *a* to perform in state
    *s*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the action *a*, move to the next state ![](img/B15558_05_109.png), and
    observe the reward *r*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Q value as ![](img/B15558_05_110.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update ![](img/B15558_05_111.png) (update the next state ![](img/B15558_03_018.png)
    to the current state s)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *s* is not a terminal state, repeat *steps 1* to *5*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned how the Q learning algorithm works, in the next section,
    let's implement Q learning to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the optimal policy using Q learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's implement Q learning to find the optimal policy in the Frozen Lake
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create the Frozen Lake environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the dictionary for storing the Q values of the state-action pairs,
    and initialize the Q values of all the state-action pairs to `0.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the epsilon-greedy policy. We generate a random number from
    the uniform distribution, and if the random number is less than epsilon we select
    the random action, else we select the best action that has the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the discount factor ![](img/B15558_05_056.png), the learning rate
    ![](img/B15558_05_055.png), and the epsilon value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of episodes and the number of time steps in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Compute the policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For each step in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action using the epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action and store the next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's compute the Q value of the state-action pair as ![](img/B15558_05_092.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, select the action ![](img/B15558_05_116.png) that has the maximum Q
    value in the next state ![](img/B15558_03_046.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compute the Q value of the state-action pair as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Update ![](img/B15558_05_118.png) (update the next state ![](img/B15558_03_073.png)
    to the current state *s*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'If the current state is the terminal state, then break:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: After all the iterations, we will have the optimal Q function. Then we can extract
    the optimal policy by selecting the action that has the maximum Q value in each
    state.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between Q learning and SARSA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the difference between Q learning and SARSA is very important.
    So, let's do a little recap on how Q learning and SARSA differ.
  prefs: []
  type: TYPE_NORMAL
- en: 'SARSA is an on-policy algorithm, meaning that we use a single epsilon-greedy
    policy for selecting an action in the environment and also to compute the Q value
    of the next state-action pair. The update rule of SARSA is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_054.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Q learning is an off-policy algorithm, meaning that we use an epsilon-greedy
    policy for selecting an action in the environment, but to compute the Q value
    of next state-action pair we use a greedy policy. The update rule of Q learning
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_05_092.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing the DP, MC, and TD methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned several interesting and important reinforcement learning
    algorithms, such as DP (value iteration and policy iteration), MC methods, and
    TD learning methods, to find the optimal policy. These are called the key algorithms
    in classic reinforcement learning, and understanding the differences between these
    three algorithms is very important. So, in this section, we will recap the differences
    between the DP, MC, and TD learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic programming** (**DP**), that is, the value and policy iteration methods,
    is a model-based method, meaning that we compute the optimal policy using the
    model dynamics of the environment. We cannot apply the DP method when we don''t
    have the model dynamics of the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about the **Monte Carlo** (**MC**) method. MC is a model-free
    method, meaning that we compute the optimal policy without using the model dynamics
    of the environment. But one problem we face with the MC method is that it is applicable
    only to episodic tasks and not to continuous tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about another interesting model-free method called **temporal difference**
    (**TD**) learning. TD learning takes advantage of both DP by bootstrapping and
    the MC method by being model free.
  prefs: []
  type: TYPE_NORMAL
- en: Many congratulations on learning about all the important reinforcement learning
    algorithms. In the next chapter, we will look into a case study called the multi-armed
    bandit problem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding what TD learning is and how it takes
    advantage of both DP and the MC method. We learned that, just like DP, TD learning
    bootstraps, and just like the MC method, TD learning is a model-free method.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned how to perform a prediction task using TD learning, and then
    we looked into the algorithm of the TD prediction method.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we learned how to use TD learning for a control task. First,
    we learned about the on-policy TD control method called SARSA, and then we learned
    about the off-policy TD control method called Q learning. We also learned how
    to find the optimal policy in the Frozen Lake environment using the SARSA and
    Q learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned the difference between SARSA and Q learning methods. We understood
    that SARSA is an on-policy algorithm, meaning that we use a single epsilon-greedy
    policy to select an action in the environment and also to compute the Q value
    of the next state-action pair, whereas Q learning is an off-policy algorithm,
    meaning that we use an epsilon-greedy policy to select an action in the environment
    but to compute the Q value of the next state-action pair we use a greedy policy.
    At the end of the chapter, we compared the DP, MC, and TD methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into an interesting problem called the multi-armed
    bandit problem.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s evaluate our newly acquired knowledge by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How does TD learning differ from the MC method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the advantage of using the TD learning method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is TD error?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the update rule of TD learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the TD prediction method work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is SARSA?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does Q learning differ from SARSA?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For further information, refer to the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning to Predict by the Methods of Temporal Differences** by *Richard
    S. Sutton*, available at [https://link.springer.com/content/pdf/10.1007/BF00115009.pdf](https://link.springer.com/content/pdf/10.1007/BF00115009.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
