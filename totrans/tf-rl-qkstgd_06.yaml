- en: Asynchronous Methods - A3C and A2C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We looked at the DDPG algorithm in the previous chapter. One of the main drawbacks
    of the DDPG algorithm (as well as the DQN algorithm that we saw earlier) is the
    use of a replay buffer to obtain independent and identically distributed samples
    of data for training. Using a replay buffer consumes a lot of memory, which is
    not desirable for robust RL applications. To overcome this problem, researchers
    at Google DeepMind came up with an on-policy algorithm called **Asynchronous Advantage
    Actor Critic** (**A3C**). A3C does not use a replay buffer; instead, it uses parallel
    worker processors, where different instances of the environment are created and
    the experience samples are collected. Once a finite and fixed number of samples
    are collected, they are used to compute the policy gradients, which are asynchronously
    sent to a central processor that updates the policy. This updated policy is then
    sent back to the worker processors. The use of parallel processors to experience
    different scenarios of the environment gives rise to independent and identically
    distributed samples that can be used to train the policy. This chapter will cover
    A3C, and will also briefly touch upon a variant of it called the **Advantage Actor
    Critic** (**A2C**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The A3C algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The A3C algorithm applied to CartPole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The A3C algorithm applied to LunarLander
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The A2C algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the A3C and A2C algorithms, as well as
    how to code them using Python and TensorFlow. We will also apply the A3C algorithm
    to solving two OpenAI Gym problems: CartPole and LunarLander.'
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To successfully complete this chapter, some knowledge of the following will
    be of great help:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow (version 1.4 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python (version 2 or 3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The A3C algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, we have parallel workers in A3C, and each worker will
    compute the policy gradients and pass them on to the central (or master) processor.
    The A3C paper also uses the `advantage` function to reduce variance in the policy
    gradients. The `loss` functions consist of three losses, which are weighted and
    added; they include the value loss, the policy loss, and an entropy regularization
    term. The value loss, *L[v]*, is an L2 loss of the state value and the target
    value, with the latter computed as a discounted sum of the rewards. The policy
    loss, *L[p]*, is the product of the logarithm of the policy distribution and the
    `advantage` function, *A*. The entropy regularization, *L[e]*, is the Shannon
    entropy, which is computed as the product of the policy distribution and its logarithm,
    with a minus sign included. The entropy regularization term is like a bonus for
    exploration; the higher the entropy, the better regularized the ensuing policy
    is. These three terms are weighted as 0.5, 1, and -0.005, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The value loss is computed as the weighted sum of three loss terms: the value
    loss, *L[v]*, the policy loss, *L[p]*, and the entropy regularization term, *L[e]*,
    which are evaluated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24dfebf5-0c27-4d51-bdd1-fa075fbf0e84.png)'
  prefs: []
  type: TYPE_IMG
- en: '*L* is the total loss, which has to be minimized. Note that we would like to
    maximize the `advantage` function, so we have a minus sign in *L[p]*, as we are
    minimizing *L*. Likewise, we would like to maximize the entropy term, and since
    we are minimizing *L*, we have a minus sign in the term *-0.005 L[e]* in *L.*'
  prefs: []
  type: TYPE_NORMAL
- en: CartPole and LunarLander
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will apply A3C to OpenAI Gym's CartPole and LunarLander.
  prefs: []
  type: TYPE_NORMAL
- en: CartPole
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CartPole consists of a vertical pole on a cart that needs to be balanced by
    moving the cart either to the left or to the right. The state dimension is four
    and the action dimension is two for CartPole.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following link for more details on CartPole: [https://gym.openai.com/envs/CartPole-v0/](https://gym.openai.com/envs/CartPole-v0/).
  prefs: []
  type: TYPE_NORMAL
- en: LunarLander
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LunarLander, as the name suggests, involves the landing of a lander on the
    lunar surface. For example, when Apollo 11''s Eagle lander touched down on the
    moon''s surface in 1969, the astronauts Neil Armstrong and Buzz Aldrin had to
    control the rocket thrusters during the final phase of the descent and safely
    land the spacecraft on the surface. After this, of course, Armstrong walked on
    the moon and remarked the now famous sentence: "*One small step for a man, one
    giant leap for mankind*". In LunarLander, there are two yellow flags on the lunar
    surface, and the goal is to land the spacecraft between these flags. Fuel in the
    lander is infinite, unlike the case in Apollo 11''s Eagle lander. The state dimension
    is eight and the action dimension is four for LunarLander, with the four actions
    being do nothing, fire the left thruster, fire the main thruster, or fire the
    right thruster.'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following link for a schematic of the environment: [https://gym.openai.com/envs/LunarLander-v2/](https://gym.openai.com/envs/LunarLander-v2/).
  prefs: []
  type: TYPE_NORMAL
- en: The A3C algorithm applied to CartPole
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will code A3C in TensorFlow and apply it so that we can train an agent
    to learn the CartPole problem. The following code files will be required to code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cartpole.py`: This will start the training or testing process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a3c.py`: This is where the A3C algorithm is coded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`utils.py`: This includes utility functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding cartpole.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now code `cartpole.py`. Follow these steps to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set the parameters for the problem. We only need to train for `200`
    episodes (yes, CartPole is an easy problem!). We set the discount factor gamma
    to `0.99`. The state and action dimensions are `4` and `2`, respectively, for
    CartPole. If you want to load a pre-trained model and resume training, set `load_model`
    to `True`; for fresh training from scratch, set this to `False`. We will also
    set the `model_path`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We reset the TensorFlow graph and also create a directory for storing our model.
    We will refer to the master processor as CPU 0\. Worker threads have non-zero
    CPU numbers. The master processor will undertake the following: first, it will
    create a count of global variables in the `global_episodes` object. The total
    number of worker threads will be stored in `num_workers`, and we can use Python''s
    multiprocessing library to obtain the number of available processors in our system
    by calling `cpu_count()`. We will use the Adam optimizer and store it in an object
    called `trainer`, along with an appropriate learning rate. We will later define
    an actor critic class called `AC`, so we must first create a master network object
    of the type `AC` class, called `master_network`. We will also pass the appropriate
    arguments to the class'' constructor. Then, for each worker thread, we will create
    a separate instance of the CartPole environment and an instance of a `Worker`
    class, which will soon be defined. Finally, for saving the model, we will also
    create a TensorFlow saver:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We then start the TensorFlow session. Inside it, we create a TensorFlow coordinator
    for the different workers. Then, we either load or restore a pre-trained model
    or run `tf.global_variables_initializer()` to assign initial values for all the
    weights and biases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start the `worker_threads`. Specifically, we call the `work()` function,
    which is part of the `Worker()` class (to be defined soon). `threading.Thread()`
    will assign one thread to each `worker`. By calling `start()`, we initiate the
    `worker` thread. In the end, we need to join the threads so that they wait until
    all the threads finish before they terminate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can find out more about TensorFlow coordinators at [https://www.tensorflow.org/api_docs/python/tf/train/Coordinator](https://www.tensorflow.org/api_docs/python/tf/train/Coordinator).
  prefs: []
  type: TYPE_NORMAL
- en: Coding a3c.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now code `a3c.py`. This involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initializers for weights and biases
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the `AC` class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the `Worker` class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we need to import the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to set the initializers for the weights and biases; specifically,
    we use the Xavier initializer for the weights and zero bias. For the last output
    layer of the network, the weights are uniform random numbers within a specified
    range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The AC class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now describe the `AC `class, which is also part of `a3c.py`. We define
    the constructor of the `AC `class with an input placeholder, two fully connected
    hidden layers with `256` and `128` neurons, respectively, and the `elu` activation
    function. This is followed by the policy network with the `softmax` activation,
    since our actions our discrete for CartPole. In addition, we also have a value
    network with no activation function. Note that we share the same hidden layers
    for both the policy and value, unlike in past examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For `worker` threads, we need to define the `loss` functions. Thus, when the
    TensorFlow scope is not `global`, we define an actions placeholder, as well as
    its one-hot representation; we also define placeholders for the `target` value
    and `advantage` functions. We then compute the product of the policy distribution
    and the one-hot actions, sum them, and store them in the `policy_times_a` object.
    Then, we combine these terms to construct the `loss` functions, as we mentioned
    previously. We compute the sum over the batch of the L2 loss for value; the Shannon
    entropy as the policy distribution multiplied with its logarithm, with a minus
    sign; the policy loss as the product of the logarithm of the policy distribution;
    and the `advantage` function, summed over the batch of samples. Finally, we use
    the appropriate weights to combine these losses to compute the total loss, which
    is stored in `self.loss`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you saw in the previous chapter, we use `tf.gradients()` to compute the
    policy gradients; specifically, we compute the gradients of the `loss` function
    with respect to the local network variables, with the latter obtained from `tf.get_collection()`.
    To mitigate the problem of exploding gradients, we clip the gradients to a magnitude
    of `40.0` using TensorFlow''s `tf.clip_by_global_norm()` function. We can then
    collect the network parameters of the global network using `tf.get_collection()`
    with a scope of `global` and apply the gradients in the Adam optimizer by using
    `apply_gradients()`. This will compute the policy gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The Worker() class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now describe the `Worker()` class, which each worker thread will use.
    First, we define the `__init__()` constructor for the class. Inside of it, we
    define the worker name, the number, the model path, the Adam optimizer, the count
    of global episodes, and the operator to increment it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We also create the local instance of the `AC` class, with the appropriate arguments
    passed in. We then create a TensorFlow operation to copy the model parameters
    from global to local. We also create a NumPy identity matrix with ones on the
    diagonal, as well as an environment object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the `train()` function, which is the most important part of
    the `Worker` class. The states, actions, rewards, next states, or observations
    and values are obtained from the experience list that''s received as an argument
    by the function. We computed the discounted sum over the rewards by using a utility
    function called `discount()`, which we will define soon. Similarly, the `advantage`
    function is also discounted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We then update the global network parameters by calling the TensorFlow operations
    that we defined earlier, along with the required input to the placeholders that
    are passed using TensorFlow''s `feed_dict` function. Note that since we have multiple
    worker threads performing this update on the master parameters, we need to avoid
    conflict. In other words, only one thread can update the master network parameters
    at a given time instant; two or more threads doing this update at the same time
    will not update the global parameters one after the other, and it can cause problems
    if one thread updates the global parameters while the other thread is in the process
    of updating the same. This means that the former''s update will be overwritten
    by the latter, which we do not desire. This is accomplished using Python''s threading
    library''s `Lock()` function. We create an instance of `Lock()` called `lock`.
    `lock.acquire()` will grant access to the current thread only, which will perform
    the update, after which it will release the lock using `lock.release()`. Finally,
    we return the losses from the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to define the workers' `work()` function. We first obtain the global
    episode count and set `total_steps` to zero. Then, inside a TensorFlow session,
    while the threads are still coordinated, we copy the global parameters to the
    local network using `self.update_local_ops`. We then start an episode. Since the
    episode hasn't been terminated, we obtain the policy distribution and store it
    in `a_dist`. We sample an action from this distribution using NumPy's `random.choice()` function.
    This action, `a`, is fed into the environment's `step()` function to obtain the
    new state, the reward, and the Terminal Boolean. We can shape the reward by dividing
    it by `100.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The experience is stored in the local buffer, called `episode_buffer`. We also
    add the reward to `episode_reward`, and we increment the `total_steps` count,
    as well as `episode_step_count`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we have `25` entries in the buffer, it''s time for an update. First, the
    value is computed and stored in `v1`, which is then passed to the `train()` function,
    which will output the three loss values: value, policy, and entropy. After this,
    the `episode_buffer` is reset. If the episode has terminated, we break from the
    loop. Finally, we print the episode count and reward on the screen. Note that
    we have used `25` entries as the time to do the update. Feel free to vary this
    and see how the training is affected by this hyperparameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After exiting the episode loop, we use the remaining samples in the buffer
    to train the network. `worker _0` contains the global or master network, which
    we can save by using `saver.save`. We can also call the `self.increment` operation
    to increment the global episode count by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That's it for `a3c.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Coding utils.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Last but not least, we will code the `utility` functions in `utils.py`. We
    will import the necessary packages, and we''ll also define the `update_target_graph()`
    function that we used earlier. It takes the scope of the source and destination
    parameters as arguments, and it copies the parameters from the source to the destination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The other utility function that we need is the `discount()` function. It runs
    the input list, `x` backwards, and sums them with a weight of `gamma`, which is
    the discount factor. The discounted value is then returned from the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Training on CartPole
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for `cartpole.py` can be run using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The code stores the episode rewards in the `performance.txt` file. A plot of
    the episode rewards during training is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d3de487-a0c6-42e0-8e2e-f899dc923ec4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Episode rewards for CartPole, which was trained using A3C'
  prefs: []
  type: TYPE_NORMAL
- en: Note that since we have shaped the reward, the episode reward that you can see
    in the preceding screenshot is different from the values that are typically reported
    by other researchers in papers and/or blogs.
  prefs: []
  type: TYPE_NORMAL
- en: The A3C algorithm applied to LunarLander
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will extend the same code to train an agent on the LunarLander problem,
    which is harder than CartPole. Most of the code is the same as before, so we will
    only describe the changes that need to be made to the preceding code. First, the
    reward shaping is different for the LunarLander problem. So, we will include a
    function called `reward_shaping()` in the `a3c.py` file. It will check if the
    lander has crashed on the lunar surface; if so, the episode will be terminated
    and there will be a `-1.0` penalty. If the lander is not moving, the episode will
    be terminated and a `-0.5` penalty will be paid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We will call this function after `env.step()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Coding lunar.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `cartpole.py` file from the previous exercise has been renamed to `lunar.py`.
    The changes that have been made are as follows. First, we set the maximum number
    of time steps per episode to `1000` for LunarLander, the discount factor to `gamma
    = 0.999`, and the state and action dimensions to `8` and `4`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The environment is set to `LunarLander-v2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: That's it for the code changes for training A3C on LunarLander.
  prefs: []
  type: TYPE_NORMAL
- en: Training on LunarLander
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can start the training by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will train the agent and store the episode rewards in the `performance.txt`
    file, which we can plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9abb1ac-a778-47d5-86bd-c02003be013a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Episode rewards for LunarLander using A3C'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the agent has learned to land the spacecraft on the lunar surface.
    Happy landings! Again, note that the episode reward is different from the values
    that have been reported in papers and blogs by other RL practitioners, since we
    have scaled the rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The A2C algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The difference between A2C and A3C is that A2C performs synchronous updates.
    Here, all the workers will wait until they have completed the collection of experiences
    and computed the gradients. Only after this are the global (or master) network's
    parameters updated. This is different from A3C, where the update is performed
    asynchronously, that is, where the worker threads do not wait for the others to
    finish. A2C is easier to code than A3C, but that is not undertaken here. If you
    are interested in this, you are encouraged to take the preceding A3C code and
    convert it to A2C, after which the performance of both algorithms can be compared.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the A3C algorithm, which is an on-policy algorithm
    that's applicable to both discrete and continuous action problems. You saw how
    three different loss terms are combined into one and optimized. Python's threading
    library is useful for running multiple threads, with a copy of the policy network
    in each thread. These different workers compute the policy gradients and pass
    them on to the master to update the neural network parameters. We applied A3C
    to train agents for the CartPole and the LunarLander problems, and the agents
    learned them very well. A3C is a very robust algorithm and does not require a
    replay buffer, although it does require a local buffer for collecting a small
    number of experiences, after which it is used to update the networks. Lastly,
    a synchronous version of the algorithm, called A2C, was also introduced.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter should have really improved your understanding of yet another deep
    RL algorithm. In the next chapter, we will study the last two RL algorithms in
    this book, TRPO and PPO.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is A3C an on-policy or off-policy algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the Shannon entropy term used?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the problems with using a large number of worker threads?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is softmax used in the policy neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need an `advantage` function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is left as an exercise: For the LunarLander problem, repeat the training
    without reward shaping and see if the agent learns faster/slower than what we
    saw in this chapter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Asynchronous Methods for Deep Reinforcement Learning*, by *Volodymyr Mnih*,
    *Adrià Puigdomènech Badia*, *Mehdi Mirza*, *Alex Graves*, *Timothy P. Lillicrap*,
    *Tim Harley*, *David Silver*, and *Koray Kavukcuoglu*, A3C paper from *DeepMind*
    arXiv:1602.01783: [https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Reinforcement Learning Hands-On*, by *Maxim Lapan*, *Packt Publishing*:
    [https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands](https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
