- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Policy Gradient Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how to use value-based reinforcement learning
    algorithms to compute the optimal policy. That is, we learned that with value-based
    methods, we compute the optimal Q function iteratively and from the optimal Q
    function, we extract the optimal policy. In this chapter, we will learn about policy-based
    methods, where we can compute the optimal policy without having to compute the
    optimal Q function.
  prefs: []
  type: TYPE_NORMAL
- en: We will start the chapter by looking at the disadvantages of computing a policy
    from the Q function, and then we will learn how policy-based methods learn the
    optimal policy directly without computing the Q function. Next, we will examine
    one of the most popular policy-based methods, called the policy gradient. We will
    first take a broad overview of the policy gradient algorithm, and then we will
    learn more about it in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we will also learn how to derive the policy gradient step by
    step and examine the algorithm of the policy gradient method in more detail. At
    the end of the chapter, we will learn about the variance reduction techniques
    in the policy gradient method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why policy-based methods?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradient intuition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving the policy gradient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm of policy gradient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradient with reward-to-go
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradient with baseline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm of policy gradient with baseline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why policy-based methods?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of reinforcement learning is to find the optimal policy, which
    is the policy that provides the maximum return. So far, we have learned several
    different algorithms for computing the optimal policy, and all these algorithms
    have been value-based methods. Wait, what are value-based methods? Let's recap
    what value-based methods are, and the problems associated with them, and then
    we will learn about policy-based methods. Recapping is always good, isn't it?
  prefs: []
  type: TYPE_NORMAL
- en: 'With value-based methods, we extract the optimal policy from the optimal Q
    function (Q values), meaning we compute the Q values of all state-action pairs
    to find the policy. We extract the policy by selecting an action in each state
    that has the maximum Q value. For instance, let''s say we have two states *s*[0]
    and *s*[1] and our action space has two actions; let the actions be 0 and 1\.
    First, we compute the Q value of all the state-action pairs, as shown in the following
    table. Now, we extract policy from the Q function (Q values) by selecting action
    0 in state *s*[0] and action 1 in state *s*[1] as they have the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 10.1: Q table'
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, we learned that it is difficult to compute the Q function when our environment
    has a large number of states and actions as it would be expensive to compute the
    Q values of all possible state-action pairs. So, we resorted to the **Deep Q Network**
    (**DQN**). In DQN, we used a neural network to approximate the Q function (Q value).
    Given a state, the network will return the Q values of all possible actions in
    that state. For instance, consider the grid world environment. Given a state,
    our DQN will return the Q values of all possible actions in that state. Then we
    select the action that has the highest Q value. As we can see in *Figure 10.1*,
    given state **E**, DQN returns the Q value of all possible actions (*up, down,
    left, right*). Then we select the *right* action in state **E** since it has the
    maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: DQN'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in value-based methods, we improve the Q function iteratively, and once
    we have the optimal Q function, then we extract optimal policy by selecting the
    action in each state that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: One of the disadvantages of the value-based method is that it is suitable only
    for discrete environments (environments with a discrete action space), and we
    cannot apply value-based methods in continuous environments (environments with
    a continuous action space).
  prefs: []
  type: TYPE_NORMAL
- en: We have learned that a discrete action space has a discrete set of actions;
    for example, the grid world environment has discrete actions (up, down, left,
    and right) and the continuous action space consists of actions that are continuous
    values, for example, controlling the speed of a car.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have only dealt with a discrete environment where we had a discrete
    action space, so we easily computed the Q value of all possible state-action pairs.
    But how can we compute the Q value of all possible state-action pairs when our
    action space is continuous? Say we are training an agent to drive a car and say
    we have one continuous action in our action space. Let the action be the speed
    of the car and the value of the speed of the car ranges from 0 to 150 kmph. In
    this case, how can we compute the Q value of all possible state-action pairs with
    the action being a continuous value?
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we can discretize the continuous actions into speed (0 to 10)
    as action 1, speed (10 to 20) as action 2, and so on. After discretization, we
    can compute the Q value of all possible state-action pairs. However, discretization
    is not always desirable. We might lose several important features and we might
    end up in an action space with a huge set of actions.
  prefs: []
  type: TYPE_NORMAL
- en: Most real-world problems have continuous action space, say, a self-driving car,
    or a robot learning to walk and more. Apart from having a continuous action space
    they also have a high dimension. Thus, the DQN and other value-based methods cannot
    deal with the continuous action space effectively.
  prefs: []
  type: TYPE_NORMAL
- en: So, we use the policy-based methods. With policy-based methods, we don't need
    to compute the Q function (Q values) to find the optimal policy; instead, we can
    compute them directly. That is, we don't need the Q function to extract the policy.
    Policy-based methods have several advantages over value-based methods, and they
    can handle both discrete and continuous action spaces.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that DQN takes care of the exploration-exploitation dilemma by using
    the epsilon-greedy policy. With the epsilon-greedy policy, we either select the
    best action with the probability 1-epsilon or a random action with the probability
    epsilon. Most policy-based methods use a stochastic policy. We know that with
    a stochastic policy, we select actions based on the probability distribution over
    the action space, which allows the agent to explore different actions instead
    of performing the same action every time. Thus, policy-based methods take care
    of the exploration-exploitation trade-off implicitly by using a stochastic policy.
    However, there are several policy-based methods that use a deterministic policy
    as well. We will learn more about them in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how do policy-based methods work, exactly? How do they find an optimal
    policy without computing the Q function? We will learn about this in the next
    section. Now that we have a basic understanding of what a policy gradient method
    is, and also the disadvantages of value-based methods, in the next section we
    will learn about a fundamental and interesting policy-based method called policy gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Policy gradient is one of the most popular algorithms in deep reinforcement
    learning. As we have learned, policy gradient is a policy-based method by which
    we can find the optimal policy without computing the Q function. It finds the
    optimal policy by directly parameterizing the policy using some parameter ![](img/B15558_09_044.png).
  prefs: []
  type: TYPE_NORMAL
- en: The policy gradient method uses a stochastic policy. We have learned that with
    a stochastic policy, we select an action based on the probability distribution
    over the action space. Say we have a stochastic policy ![](img/B15558_04_099.png),
    then it gives the probability of taking an action *a* given the state *s*. It
    can be denoted by ![](img/B15558_10_003.png). In the policy gradient method, we
    use a parameterized policy, so we can denote our policy as ![](img/B15558_10_004.png),
    where ![](img/B15558_09_002.png) indicates that our policy is parameterized.
  prefs: []
  type: TYPE_NORMAL
- en: Wait! What do we mean when we say a parameterized policy? What is it exactly?
    Remember with DQN, we learned that we parameterize our Q function to compute the
    Q value? We can do the same here, except instead of parameterizing the Q function,
    we will directly parameterize the policy to compute the optimal policy. That is,
    we can use any function approximator to learn the optimal policy, and ![](img/B15558_09_054.png)
    is the parameter of our function approximator. We generally use a neural network
    as our function approximator. Thus, we have a policy ![](img/B15558_03_084.png)
    parameterized by ![](img/B15558_10_008.png) where ![](img/B15558_10_009.png) is
    the parameter of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Say we have a neural network with a parameter ![](img/B15558_09_029.png). First,
    we feed the state of the environment as an input to the network and it will output
    the probability of all the actions that can be performed in the state. That is,
    it outputs a probability distribution over an action space. We have learned that
    with policy gradient, we use a stochastic policy. So, the stochastic policy selects
    an action based on the probability distribution given by the neural network. In
    this way, we can directly compute the policy without using the Q function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand how the policy gradient method works with an example. Let''s
    take our favorite grid world environment for better understanding. We know that
    in the grid world environment our action space has four possible actions: *up*,
    *down*, *left*, and *right*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given any state as an input, the neural network will output the probability
    distribution over the action space. That is, as shown in *Figure 10.2*, when we
    feed the state **E** as an input to the network, it will return the probability
    distribution over all actions in our action space. Now, our stochastic policy
    will select an action based on the probability distribution given by the neural
    network. So, it will select action *up* 10% of the time, *down* 10% of the time,
    *left* 10% of the time, and *right* 70% of the time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: A policy network'
  prefs: []
  type: TYPE_NORMAL
- en: We should not get confused with the DQN and the policy gradient method. With
    DQN, we feed the state as an input to the network, and it returns the Q values
    of all possible actions in that state, then we select an action that has a maximum
    Q value. But in the policy gradient method, we feed the state as input to the
    network, and it returns the probability distribution over an action space, and
    our stochastic policy uses the probability distribution returned by the neural
    network to select an action.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, in the policy gradient method, the network returns the probability distribution
    (action probabilities) over the action space, but how accurate are the probabilities?
    How does the network learn?
  prefs: []
  type: TYPE_NORMAL
- en: Unlike supervised learning, here we will not have any labeled data to train
    our network. So, our network does not know the correct action to perform in the
    given state; that is, the network does not know which action gives the maximum
    reward. So, the action probabilities given by our neural network will not be accurate
    in the initial iterations, and thus we might get a bad reward.
  prefs: []
  type: TYPE_NORMAL
- en: But that is fine. We simply select the action based on the probability distribution
    given by the network, store the reward, and move to the next state until the end
    of the episode. That is, we play an episode and store the states, actions, and
    rewards. Now, this becomes our training data. If we win the episode, that is,
    if we get a positive return or high return (the sum of the rewards of the episode),
    then we increase the probability of all the actions that we took in each state
    until the end of the episode. If we get a negative return or low return, then
    we decrease the probability of all the actions that we took in each state until
    the end of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this with an example. Say we have states *s*[1] to *s*[8]
    and our goal is to reach state *s*[8]. Say our action space consists of only two
    actions: *left and right.* So, when we feed any state to the network, then it
    will return the probability distribution over the two actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following trajectory (episode) ![](img/B15558_10_011.png), where
    we select an action in each state based on the probability distribution returned
    by the network using a stochastic policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Trajectory ![](img/B15558_10_012.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The return of this trajectory is ![](img/B15558_10_013.png). Since we got a
    positive return, we increase the probabilities of all the actions that we took
    in each state until the end of the episode. That is, we increase the probabilities
    of action *left* in *s*[1], action *right* in *s*[2], and so on until the end
    of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we generate another trajectory ![](img/B15558_10_014.png), where
    we select an action in each state based on the probability distribution returned
    by the network using a stochastic policy, as shown in *Figure 10.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Trajectory ![](img/B15558_10_015.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The return of this trajectory is ![](img/B15558_10_016.png). Since we got a
    negative return, we decrease the probabilities of all the actions that we took
    in each state until the end of the episode. That is, we will decrease the probabilities
    of action *right* in *s*[1], action *right* in *s*[3], and so on until the end
    of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, but how exactly do we increase and decrease these probabilities? We learned
    that if the return of the trajectory is positive, then we increase the probabilities
    of all actions in the episode, else we decrease it. How can we do this exactly?
    This is where backpropagation helps us. We know that we train the neural network
    by backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: So, during backpropagation, the network calculates gradients and updates the
    parameters of the network ![](img/B15558_09_098.png). Gradients updates are in
    such a way that actions yielding high return will get high probabilities and actions
    yielding low return will get low probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, in the policy gradient method, we use a neural network to find
    the optimal policy. We initialize the network parameter ![](img/B15558_09_054.png)
    with random values. We feed the state as an input to the network and it will return
    the action probabilities. In the initial iteration, since the network is not trained
    with any data, it will give random action probabilities. But we select actions
    based on the action probability distribution given by the network and store the
    state, action, and reward until the end of the episode. Now, this becomes our
    training data. If we win the episode, that is, if we get a high return, then we
    assign high probabilities to all the actions of the episode, else we assign low
    probabilities to all the actions of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using a neural network to find the optimal policy, we can call
    this neural network a policy network. Now that we have a basic understanding of
    the policy gradient method, in the next section, we will learn how exactly the
    neural network finds the optimal policy; that is, we will learn how exactly the
    gradient computation happens and how we train the network.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the policy gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last section, we learned that, in the policy gradient method, we update
    the gradients in such a way that actions yielding a high return will get a high
    probability, and actions yielding a low return will get a low probability. In
    this section, we will learn how exactly we do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the policy gradient method is to find the optimal parameter ![](img/B15558_09_118.png)
    of the neural network so that the network returns the correct probability distribution
    over the action space. Thus, the objective of our network is to assign high probabilities
    to actions that maximize the expected return of the trajectory. So, we can write
    our objective function *J* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_037.png) is the trajectory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_10_022.png) denotes that we are sampling the trajectory based
    on the policy ![](img/B15558_04_032.png) given by network parameterized by ![](img/B15558_09_056.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_10_025.png) is the return of the trajectory ![](img/B15558_10_026.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, maximizing our objective function maximizes the return of the trajectory.
    How can we maximize the preceding objective function? We generally deal with minimization
    problems, where we minimize the loss function (objective function) by calculating
    the gradients of our loss function and updating the parameter using gradient descent.
    But here, our goal is to maximize the objective function, so we calculate the
    gradients of our objective function and perform gradient ascent. That is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_027.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_10_028.png) implies the gradients of our objective function.
    Thus, we can find the optimal parameter ![](img/B15558_09_118.png) of our network
    using gradient ascent.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient ![](img/B15558_10_030.png) is derived as ![](img/B15558_10_031.png).
    We will learn how exactly we derived this gradient in the next section. In this
    section, let's focus only on getting a good fundamental understanding of the policy
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that we update our network parameter with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the value of gradient, our parameter update equation becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_034.png) represents the log probability of taking an action
    *a* given the state *s* at a time *t*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_10_035.png) represents the return of the trajectory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learned that we update the gradients in such a way that actions yielding
    a high return will get a high probability, and actions yielding a low return will
    get a low probability. Let's now see how exactly we are doing that.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we generate an episode (trajectory) using the policy ![](img/B15558_10_036.png),
    where ![](img/B15558_10_037.png) is the parameter of the network. After generating
    the episode, we compute the return of the episode. If the return of the episode
    is negative, say -1, that is, ![](img/B15558_10_038.png), then we decrease the
    probability of all the actions that we took in each state until the end of the
    episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that our parameter update equation is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, multiplying ![](img/B15558_10_040.png) by the negative
    return ![](img/B15558_10_038.png) implies that we are decreasing the log probability
    of action *a*[t] in state *s*[t]. Thus, we perform a negative update. That is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each step in the episode, *t* = 0, . . ., *T*-1, we update the parameter
    ![](img/B15558_09_087.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_043.png)'
  prefs: []
  type: TYPE_IMG
- en: It implies that we are decreasing the probability of all the actions that we
    took in each state until the end of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we generate an episode (trajectory) using the policy ![](img/B15558_10_044.png),
    where ![](img/B15558_09_054.png) is the parameter of the network. After generating
    the episode, we compute the return of the episode. If the return of the episode
    is positive, say +1, that is, ![](img/B15558_10_046.png), then we increase the
    probability of all the actions that we took in each state until the end of the
    episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that our parameter update equation is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, multiplying ![](img/B15558_10_048.png) by the positive
    return, ![](img/B15558_10_046.png), means that we are increasing the log probability
    of action *a*[t] in the state *s*[t]. Thus, we perform a positive update. That
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each step in the episode, *t* = 0, . . ., *T*-1, we update the parameter
    ![](img/B15558_09_098.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_051.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, if we get a positive return then we increase the probability of all the
    actions performed in that episode, else we will decrease the probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that, for each step in the episode, *t* = 0, . . ., *T*-1, we update
    the parameter ![](img/B15558_10_037.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can simply denote the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_054.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, if the episode (trajectory) gives a high return, we will increase the
    probabilities of all the actions of the episode, else we decrease the probabilities.
    We learned that ![](img/B15558_10_031.png). What about that expectation? We have
    not included that in our update equation yet. When we looked at the Monte Carlo
    method, we learned that we can approximate the expectation using the average.
    Thus, using the Monte Carlo approximation method, we change the expectation term to
    the sum over *N* trajectories. So, our update equation becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_056.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It shows that instead of updating the parameter based on a single trajectory,
    we collect a set of *N* trajectories following the policy ![](img/B15558_10_057.png)
    and update the parameter based on the average value, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, first, we collect *N* number of trajectories ![](img/B15558_10_058.png)
    following the policy ![](img/B15558_10_057.png) and compute the gradient as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_060.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And then we update our parameter as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_027.png)'
  prefs: []
  type: TYPE_IMG
- en: But we can't find the optimal parameter ![](img/B15558_09_098.png) by updating
    the parameter for just one iteration. So, we repeat the previous step for many
    iterations to find the optimal parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a fundamental understanding of how policy gradient method work,
    in the next section, we will learn how to derive the policy gradient ![](img/B15558_10_063.png).
    After that, we will learn about the policy gradient algorithm in detail step by
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Deriving the policy gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will get into more details and learn how to compute the
    gradient ![](img/B15558_10_064.png) and how it is equal to ![](img/B15558_10_065.png).
  prefs: []
  type: TYPE_NORMAL
- en: Let's deep dive into the interesting math and see how to calculate the derivative
    of our objective function *J* with respect to the model parameter ![](img/B15558_10_066.png)
    in simple steps. Don't get intimidated by the upcoming equations, it's actually
    a pretty simple derivation. Before going ahead, let's revise some math prerequisites
    in order to understand our derivation better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition of the expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *X* be a discrete random variable whose **probability mass function** (**pmf**)
    is given as *p*(*x*). Let *f* be a function of a discrete random variable *X*.
    Then the expectation of a function *f*(*X*) can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_067.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let *X* be a continuous random variable whose **probability density function**
    (**pdf**) is given as *p*(*x*). Let *f* be a function of a continuous random variable
    *X*. Then the expectation of a function *f*(*X*) can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_068.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A log derivative trick is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_069.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We learned that the objective of our network is to maximize the expected return
    of the trajectory. Thus, we can write our objective function *J* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_070.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_071.png) is the trajectory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_10_072.png) shows that we are sampling the trajectory based
    on the policy ![](img/B15558_03_139.png) given by network parameterized by ![](img/B15558_09_002.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_10_075.png) is the return of the trajectory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we can see, our objective function, equation (4), is in the expectation
    form. From the definition of the expectation given in equation (2), we can expand
    the expectation and rewrite equation (4) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_076.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we calculate the derivative of our objective function *J* with respect
    to ![](img/B15558_10_037.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_078.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Multiplying and dividing by ![](img/B15558_10_079.png), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_080.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Rearranging the preceding equation, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_081.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From equation (3), substituting ![](img/B15558_10_082.png) in the preceding
    equation, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_083.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the definition of expectation given in equation (2), we can rewrite the
    preceding equation in expectation form as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_084.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation gives us the gradient of the objective function. But
    we still haven't solved the equation yet. As we can see, in the preceding equation
    we have the term ![](img/B15558_10_085.png). Now we will see how we can compute
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability distribution of trajectory can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_086.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *p*(*s*[0]) is the initial state distribution. Taking the log on both
    sides, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_087.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that the log of a product is equal to the sum of the logs, that is,
    ![](img/B15558_10_088.png). Applying this log rule to the preceding equation,
    we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_089.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, we apply the same rule, log of product = sum of logs, and change the
    log ![](img/B15558_10_090.png) to ![](img/B15558_10_091.png) logs, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_094.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we compute the derivate with respect to ![](img/B15558_10_095.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_096.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that we are calculating derivative with respect to ![](img/B15558_09_087.png)
    and, as we can see in the preceding equation, the first and last term on the **right-hand
    side** (**RHS**) does not depend on the ![](img/B15558_09_106.png), and so they
    will become zero while calculating derivative. Thus our equation becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have found the value for ![](img/B15558_10_100.png), substituting
    this in equation (5) we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_101.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That''s it. But can we also get rid of that expectation? Yes! We can use a
    Monte Carlo approximation method and change the expectation to the sum over *N*
    trajectories. So, our final gradient becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_102.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation (6) shows that instead of updating a parameter based on a single trajectory,
    we collect *N* number of trajectories and update the parameter based on its average
    value over *N* trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, after computing the gradient, we can update our parameter as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_027.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, in this section, we have learned how to derive a policy gradient. In the
    next section, we will get into more details and learn about the policy gradient
    algorithm step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – policy gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The policy gradient algorithm we discussed so far is often called REINFORCE
    or **Monte Carlo policy gradient**. The algorithm of the REINFORCE method is given
    in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the network parameter ![](img/B15558_10_095.png) with random values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate *N* trajectories ![](img/B15558_10_105.png) following the policy ![](img/B15558_10_057.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return of the trajectory ![](img/B15558_10_107.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients![](img/B15558_10_108.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the network parameter as ![](img/B15558_10_109.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *5* for several iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see from this algorithm, the parameter ![](img/B15558_09_054.png)
    is getting updated in every iteration. Since we are using the parameterized policy
    ![](img/B15558_10_111.png), our policy is getting updated in every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The policy gradient algorithm we just learned is an on-policy method, as we
    are using only a single policy. That is, we are using a policy to generate trajectories
    and we are also improving the same policy by updating the network parameter ![](img/B15558_09_054.png)
    in every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that with the policy gradient method (the REINFORCE method), we use
    a policy network that returns the probability distribution over the action space
    and then we select an action based on the probability distribution returned by
    our network using a stochastic policy. But this applies only to a discrete action
    space, and we use categorical policy as our stochastic policy.
  prefs: []
  type: TYPE_NORMAL
- en: What if our action space is continuous? That is, when the action space is continuous,
    how can we select actions? Here, our policy network cannot return the probability
    distribution over the action space as the action space is continuous. So, in this
    case, our policy network will return the mean and variance of the action as output,
    and then we generate a Gaussian distribution using this mean and variance and
    select an action by sampling from this Gaussian distribution using the Gaussian
    policy. We will learn more about this in the upcoming chapters. Thus, we can apply
    the policy gradient method to both discrete and continuous action spaces. Next,
    we will look at two methods to reduce the variance of policy gradient updates.
  prefs: []
  type: TYPE_NORMAL
- en: Variance reduction methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned one of the simplest policy gradient methods,
    called the REINFORCE method. One major issue we face with the policy gradient
    method we learned in the previous section is that the gradient, ![](img/B15558_10_113.png),
    will have high variance in each update. The high variance is basically due to
    the major difference in the episodic returns. That is, we learned that policy
    gradient is the on-policy method, which means that we improve the same policy
    with which we are generating episodes in every iteration. Since the policy is
    getting improved on every iteration, our return varies greatly in each episode
    and it introduces a high variance in the gradient updates. When the gradients
    have high variance, then it will take a lot of time to attain convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, now we will learn the following two important methods to reduce the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradients with reward-to-go (causality)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradients with baseline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradient with reward-to-go
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We learned that the policy gradient is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_108.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we make a small change in the preceding equation. We know that the return
    of the trajectory is the sum of the rewards of that trajectory, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_115.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of using the return of trajectory ![](img/B15558_10_116.png), we use
    something called reward-to-go *R*[t]. Reward-to-go is basically the return of
    the trajectory starting from state *s*[t]. That is, instead of multiplying the
    log probabilities by the return of the full trajectory ![](img/B15558_10_117.png)
    in every step of the episode, we multiply them by the reward-to-go *R*[t]. The
    reward-to-go implies the return of the trajectory starting from state *s*[t].
    But why do we have to do this? Let's understand this in more detail with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that we generate *N* number of trajectories and compute the gradient
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_108.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For better understanding, let''s take only one trajectory by setting *N*=1,
    so we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_119.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Say, we generated the following trajectory with the policy ![](img/B15558_10_120.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Trajectory'
  prefs: []
  type: TYPE_NORMAL
- en: The return of the preceding trajectory is ![](img/B15558_10_121.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can compute gradient as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_122.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe from the preceding equation, in every step of the episode,
    we are multiplying the log probability of the action by the return of the full
    trajectory ![](img/B15558_10_107.png), which is 2 in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose we want to know how good the action *right* is in the state *s*[2].
    If we understand that the action *right* is a good action in the state *s*[2],
    then we can increase the probability of moving *right* in the state *s*[2], else
    we decrease it. Okay, how can we tell whether the action *right* is good in the
    state *s*[2]? As we learned in the previous section (when discussing the REINFORCE
    method), if the return of the trajectory ![](img/B15558_10_075.png) is high, then
    we increase the probability of the action *right* in the state *s*[2], else we decrease
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we don''t have to do that now. Instead, we can compute the return (the
    sum of the rewards of the trajectory) only starting from the state *s*[2] because
    there is no use in including all the rewards that we obtain from the trajectory
    before taking the action *right* in the state *s*[2]. As *Figure 10.6* shows,
    including all the rewards that we obtain before taking the action *right* in the
    state *s*[2] will not help us understand how good the action *right* is in the
    state *s*[2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Trajectory'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, instead of taking the complete return of the trajectory in all the steps
    of the episode, we use reward-to-go *R*[t], which is the return of the trajectory
    starting from the state *s*[t].
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, now we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_125.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *R*[0] indicates the return of the trajectory starting from the state
    *s*[0], *R*[1] indicates the return of the trajectory starting from the state
    *s*[1], and so on. If *R*[0] is high value, then we increase the probability of
    the action *up* in the state *s*[0], else we decrease it. If *R*[1] is high value,
    then we increase the probability of the action *down* in the state *s*[1], else
    we decrease it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, now, we can define the reward-to-go as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_126.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation states that the reward-to-go *R*[t] is the sum of rewards
    of the trajectory starting from the state *s*[t]. Thus, now we can rewrite our
    gradient with reward-to-go instead of the return of the trajectory as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_127.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can simply express the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_128.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the reward-to-go is ![](img/B15558_10_126.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the gradient, we update the parameter as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_027.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have understood what policy gradient with reward-to-go is, in the
    next section, we will look into the algorithm for more clarity.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – Reward-to-go policy gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The algorithm of policy gradient with reward-to-go is similar to the REINFORCE
    method, except now we compute the reward-to-go (return of the trajectory starting
    from a state *s*[t]) instead of using the full return of the trajectory, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the network parameter ![](img/B15558_09_056.png) with random values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate *N* number of trajectories ![](img/B15558_10_058.png) following the
    policy ![](img/B15558_10_057.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return (reward-to-go) *R*[t]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients:![](img/B15558_10_128.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the network parameter as ![](img/B15558_10_135.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *5* for several iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the preceding algorithm, we can observe that we are using reward-to-go
    instead of the return of the trajectory. To get a clear understanding of how the
    reward-to-go policy gradient works, let's implement it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cart pole balancing with policy gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's learn how to implement the policy gradient algorithm with reward-to-go
    for the cart pole balancing task.
  prefs: []
  type: TYPE_NORMAL
- en: For a clear understanding of how the policy gradient method works, we use TensorFlow
    in the non-eager mode by disabling TensorFlow 2 behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the cart pole environment using gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the state shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the number of actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Computing discounted and normalized reward
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using the rewards directly, we can use the discounted and normalized
    rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the discount factor, ![](img/B15558_03_190.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a function called `discount_and_normalize_rewards` for computing
    the discounted and normalized rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize an array for storing the discounted rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the discounted reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize and return the reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Building the policy network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let''s define the placeholder for the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the discounted reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Define layer 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Define layer 2\. Note that the number of units in layer 2 is set to the number
    of actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtain the probability distribution over the action space as an output of the
    network by applying the softmax function to the result of layer 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We learned that we compute gradient as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_128.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the gradient, we update the parameter of the network using
    gradient ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, it is a standard convention to perform minimization rather than maximization.
    So, we can convert the preceding maximization objective into the minimization
    objective by just adding a negative sign. We can implement this using `tf.nn.softmax_cross_entropy_with_logits_v2`.
    Thus, we can define the negative log policy as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the train operation for minimizing the loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Training the network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's train the network for several iterations. For simplicity, let's just
    generate one episode in every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all the TensorFlow variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize an empty list for storing the states, actions, and rewards obtained
    in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Set `done` to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `Return`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'While the episode is not over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Feed the state to the policy network and the network returns the probability
    distribution over the action space as output, which becomes our stochastic policy
    ![](img/B15558_10_139.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we select an action using this stochastic policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'One-hot encode the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the state, action, and reward in their respective lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the state to the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the discounted and normalized reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the feed dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the return for every 10 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have learned how to implement the policy gradient algorithm with
    reward-to-go, in the next section, we will learn another interesting variance
    reduction technique called policy gradient with baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient with baseline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have learned that we find the optimal policy by using a neural network and
    we update the parameter of our network using gradient ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the value of the gradient is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_141.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, to reduce variance, we introduce a new function called a baseline function.
    Subtracting the baseline *b* from the return (reward-to-go) *R*[t] reduces the
    variance, so we can rewrite the gradient as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_142.png)'
  prefs: []
  type: TYPE_IMG
- en: Wait. What is the baseline function? And how does subtracting it from *R*[t]
    reduce the variance? The purpose of the baseline is to reduce the variance in
    the return. Thus, if the baseline *b* is a value that can give us the expected
    return from the state the agent is in, then subtracting *b* in every step will
    reduce the variance in the return.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several choices for the baseline functions. We can choose any function
    as a baseline function but the baseline function should not depend on our network
    parameter. A simple baseline could be the average return of the sampled trajectories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_143.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, subtracting current return *R*[t] and the average return helps us to reduce
    variance. As we can see, our baseline function doesn't depend on the network parameter
    ![](img/B15558_09_106.png). So, we can use any function as a baseline function
    and it should not affect our network parameter ![](img/B15558_09_106.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular functions of the baseline is the value function. We
    learned that the value function or the value of a state is the expected return
    an agent would obtain starting from that state following the policy ![](img/B15558_10_146.png).
    Thus, subtracting the value of a state (the expected return) and the current return
    *R*[t] can reduce the variance. So, we can rewrite our gradient as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_147.png)'
  prefs: []
  type: TYPE_IMG
- en: Other than the value function, we can also use different baseline functions
    such as the Q function, the advantage function, and more. We will learn more about
    them in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: But now the question is how can we learn the baseline function? Say we are using
    the value function as the baseline function. How can we learn the optimal value
    function? Just like we are approximating the policy, we can also approximate the
    value function using another neural network parameterized by ![](img/B15558_10_148.png).
  prefs: []
  type: TYPE_NORMAL
- en: That is, we use another network for approximating the value function (the value
    of a state) and we can call this network a value network. Okay, how can we train
    this value network?
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the value of the state is a continuous value, we can train the network
    by minimizing the **mean squared error** (**MSE**). The MSE can be defined as
    the mean squared difference between the actual return *R*[t] and the predicted
    return ![](img/B15558_10_170.png). Thus, the objective function of the value network
    can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_149.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can minimize the error using the gradient descent and update the network
    parameter as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_150.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, in the policy gradient with the baseline method, we minimize the variance
    in the gradient updates by using the baseline function. A baseline function can
    be any function and it should not depend on the network parameter ![](img/B15558_09_118.png).
    We use the value function as a baseline function, then to approximate the value
    function we use a different neural network parameterized by ![](img/B15558_10_152.png),
    and we find the optimal value function by minimizing the MSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, in the policy gradient with the baseline function, we use two
    neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy network parameterized by ![](img/B15558_10_153.png)**: This finds
    the optimal policy by performing gradient ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_154.png)![](img/B15558_10_027.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Value network parameterized by ![](img/B15558_10_156.png)**: This is used
    to correct the variance in the gradient update by acting as a baseline, and it
    finds the optimal value of a state by performing gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_149.png)![](img/B15558_10_150.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the policy gradient with the baseline function is often referred to
    as the **REINFORCE** **with baseline** method.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how the policy gradient method with baseline works by
    using a policy and a value network, in the next section we will look into the
    algorithm to get more clarity.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – REINFORCE with baseline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The algorithm of the policy gradient method with the baseline function (REINFORCE
    with baseline) is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the policy network parameter ![](img/B15558_10_159.png) and value
    network parameter ![](img/B15558_10_148.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate *N* number of trajectories ![](img/B15558_10_058.png) following the
    policy ![](img/B15558_10_044.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return (reward-to-go) *R*[t]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the policy gradient:![](img/B15558_10_163.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy network parameter ![](img/B15558_09_123.png) using gradient
    ascent as ![](img/B15558_10_027.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the MSE of the value network:![](img/B15558_10_166.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute gradients ![](img/B15558_10_093.png) and update the value network parameter
    ![](img/B15558_10_148.png) using gradient descent as ![](img/B15558_10_150.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *7* for several iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by learning that with value-based methods, we extract
    the optimal policy from the optimal Q function (Q values). Then we learned that
    it is difficult to compute the Q function when our action space is continuous.
    We can discretize the action space; however, discretization is not always desirable,
    and it leads to the loss of several important features and an action space with
    a huge set of actions.
  prefs: []
  type: TYPE_NORMAL
- en: So, we resorted to the policy-based method. In the policy-based method, we compute
    the optimal policy without the Q function. We learned about one of the most popular
    policy-based methods called the policy gradient, in which we find the optimal
    policy directly by parameterizing the policy using some parameter ![](img/B15558_09_008.png).
  prefs: []
  type: TYPE_NORMAL
- en: We also learned that in the policy gradient method, we select actions based
    on the action probability distribution given by the network, and if we win the
    episode, that is, if we get a high return, then we assign high probabilities to
    all the actions in the episode, else we assign low probabilities to all the actions
    in the episode. Later, we learned how to derive the policy gradient step by step,
    and then we looked into the algorithm of policy gradient method in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we learned about the variance reduction methods such as reward-to-go
    and the policy gradient method with the baseline function. In the policy gradient
    method with the baseline function, we use two networks called the policy and value
    network. The role of the policy network is to find the optimal policy, and the
    role of the value network is to correct the gradient updates in the policy network
    by estimating the value function.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about another interesting set of algorithms
    called the actor-critic methods.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s evaluate our understanding of the policy gradient method by answering
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a value-based method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need a policy-based method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the policy gradient method work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we compute the gradient in the policy gradient method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a reward-to-go?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the policy gradient with the baseline function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the baseline function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information about the policy gradient, we can refer to the following
    paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy Gradient Methods for Reinforcement Learning with Function Approximation**
    by *Richard S. Sutton et al*., [https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
