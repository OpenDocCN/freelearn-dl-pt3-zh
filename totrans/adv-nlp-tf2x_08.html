<html><head></head><body>
  <div id="_idContainer190">
    <h1 class="chapterNumber">8</h1>
    <h1 id="_idParaDest-142" class="chapterTitle">Weakly Supervised Learning for Classification with Snorkel</h1>
    <p class="normal">Models such as BERT and GPT use massive amounts of unlabeled data along with an unsupervised training objective, such as a <strong class="keyword">masked language model</strong> (<strong class="keyword">MLM</strong>) for BERT or a next word prediction model for GPT, to learn the underlying structure of text. A small amount of task-specific data is used for fine-tuning the pre-trained model using transfer learning. Such models are quite large, with hundreds of millions of parameters, and require massive datasets for pre-training and lots of computation capacity for training and pre-training. Note that the critical problem being solved is the lack of adequate training data. If there were enough domain-specific training data, the gains from BERT-like pre-trained models would not be that big. In certain domains such as medicine, the vocabulary used in task-specific data is typical for the domain. Modest increases in training data can improve the quality of the model to a large extent. However, hand labeling data is a tedious, resource-intensive, and unscalable task for the amounts required for deep learning to be successful.</p>
    <p class="normal">We discuss an alternative approach in this chapter, based on the concept of weak supervision. Using the Snorkel library, we label tens of thousands of records in a couple of hours and exceed the accuracy of the model developed in <em class="chapterRef">Chapter 3</em>, <em class="italic">Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding</em> using, BERT. This chapter covers:</p>
    <ul>
      <li class="bullet">An overview of weakly supervised learning</li>
      <li class="bullet">An overview of the differences between generative and discriminative models</li>
      <li class="bullet">Building a baseline model with handcrafted features for labeling data</li>
      <li class="bullet">Snorkel library basics</li>
      <li class="bullet">Augmenting training data using Snorkel labeling functions at scale</li>
      <li class="bullet">Training models using noisy machine-labeled data</li>
    </ul>
    <p class="normal">It is essential to understand the concept of weakly supervised learning, so let's cover that first.</p>
    <h1 id="_idParaDest-143" class="title">Weak supervision</h1>
    <p class="normal">Deep learning models have <a id="_idIndexMarker607"/>delivered incredible results in the recent past. Deep learning architectures obviated the need for feature engineering, given enough training data. However, enormous amounts of data are needed for a deep learning model to learn the underlying structure of the data. On the one hand, deep learning reduced the manual effort required to handcraft features, but on the other, it significantly increased the need for labeled data for a specific task. In most domains, gathering a sizable set of high-quality, labeled data is an expensive and resource-intensive task.</p>
    <p class="normal">This problem can be solved in several different ways. In previous chapters, we have seen the use of transfer learning to train a model on a large dataset before fine-tuning the model for a specific task. <em class="italic">Figure 8.1</em> shows this and other approaches to acquiring labels:</p>
    <figure class="mediaobject"><img src="image/B16252_08_01.png" alt="A picture containing clock  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.1: Options for getting more labeled data</p>
    <p class="normal">Hand labeling the data is a common approach. Ideally, we have enough time and money to <a id="_idIndexMarker608"/>hire <strong class="keyword">subject matter experts</strong> (<strong class="keyword">SMEs</strong>) to hand label every piece of data, which is not practical. Consider labeling a tumor detection dataset and hiring oncologists for the labeling task. Labeling data is probably way lower in priority for an oncologist than treating tumor patients. In a previous company, we organized pizza parties where we would feed people lunch for labels. In an hour, a person could label about 100 records. Feeding 10 people monthly for a year resulted in 12,000 labeled records! This scheme was useful for ongoing maintenance of models, where we would sample the records that were out of distribution, or that the model had shallow confidence in. Thus, we adopted active learning, which determines the records upon labeling, which would have the highest impact on the performance of a classifier.</p>
    <p class="normal">Another option is to hire labelers that are not experts but are more abundant and cheaper. This is the approach taken by the Amazon Mechanical Turk service. There are a large number of companies that provide labeling services. Since the labelers are not experts, the same record is labeled by multiple people, and some mechanism, like majority vote, is used to decide on the final <a id="_idIndexMarker609"/>label of the record. The charge for labeling one record by one labeler may vary from a few cents to a few dollars depending on the complexity of the steps needed for associating a label. The output of such a process is a set of noisy labels that have high coverage, as long as your budget allows for it. We still need to figure out the quality of the labels acquired to see how these labels can be used in the eventual model.</p>
    <p class="normal">Weak supervision tries to address the problem differently. What if, using heuristics, an SME could hand label thousands of records in a fraction of the time? We will work on the IMDb movie review dataset and try to predict the sentiment of the review. We used the IMDb dataset in <em class="chapterRef">Chapter 4</em> , <em class="italic">Transfer Learning with BERT</em>, where we explored transfer learning. It is appropriate to use the same example to show an alternate technique to transfer learning.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Weak supervision techniques don't have to be used as substitutes for transfer learning. Weak supervision techniques help create larger domain-specific labeled datasets. In the absence of transfer learning, a larger labeled dataset improves model performance even with noisy labels coming from weak supervision. However, the gain in model performance will be even more significant if transfer learning and weak supervision are both used together.</p>
    </div>
    <p class="normal">An example of a simple heuristic function for labeling a review as having a positive sentiment can be shown with the following pseudocode:</p>
    <pre class="programlisting code"><code class="hljs-code">if movie.review has "amazing acting" in it:
then sentiment is positive
</code></pre>
    <p class="normal">While this may seem like a trivial example for our use case, you will be surprised how effective it can be. In a more complicated setting, an oncologist can provide some of these heuristics and define a few of these <a id="_idIndexMarker610"/>functions, which can be called labeling functions, to label some records. These functions may conflict or overlap with each other, similar to crowdsourced labels. Another approach for getting labels is through <em class="italic">distant supervision</em>. An external knowledge base, like Wikipedia, can be used to label data records heuristically. In a <strong class="keyword">Named-Entity Recognition</strong> (<strong class="keyword">NER</strong>) use case, a gazetteer is used to match entities to a list of known entities, as discussed in <em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>. In relation extraction between entities, for example, <em class="italic">employee of</em> or <em class="italic">spouse of</em>, the Wikipedia page of an entity can be mined to extract the relation, and the data record can be labeled. There are other methods of obtaining these labels, such as using thorough knowledge of the underlying distributions generating the data.</p>
    <p class="normal">For a given data set, there can be several sources for labels. Each crowdsourced labeler is a source. Each heuristic <a id="_idIndexMarker611"/>function, like the "amazing acting" one shown above, is also a source. The core problem in weak supervision is combining these multiple sources to yield labels of sufficient quality for the final classifier. The key points of the model are described in the next section.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The domain-specific model is being referred to as the classifier in this chapter as the example we are taking is the binary classification of movie review sentiment. However, the labels generated can be used for a variety of domain-specific models.</p>
    </div>
    <h2 id="_idParaDest-144" class="title">Inner workings of weak supervision with labeling functions</h2>
    <p class="normal">The <a id="_idIndexMarker612"/>idea that a few heuristic labeling functions with low coverage and less than perfect accuracy can help improve the accuracy of a discriminative model sounds fantastic. This section provides a high-level overview of how this works, before we see it in practice on the IMDb sentiment analysis dataset.</p>
    <p class="normal">We assume a binary classification problem for the sake of explanation though the scheme works for any number of labels. The set of labels for binary classification is {NEG, POS}. We have a set of unlabeled data points, <em class="italic">X</em>, with <em class="italic">m</em> samples. </p>
    <p class="normal">Note that we do not have access to the actual labels for these data points, but we represent the generated labels using <em class="italic">Y</em>. Let's assume we have <em class="italic">n</em> labeling functions <em class="italic">LF</em><sub class="" style="font-style: italic;">1</sub> to <em class="italic">LF</em><sub class="" style="font-style: italic;">n</sub>, each of which produces a label. However, we add another label for weak supervision – an abstain label. Each labeling function has the ability to choose whether it wants to apply a label or abstain from labeling. This is a vital aspect of the weak supervision approach. Hence, the set of labels produced by labeling functions is expanded to {NEG, ABSTAIN, POS}.</p>
    <p class="normal">In this setting, the objective is to train a generative model which models two things:</p>
    <ul>
      <li class="bullet">The probability of a given labeling function abstaining for a given data point</li>
      <li class="bullet">The probability of a given labeling function correctly assigning a label to a data point</li>
    </ul>
    <p class="normal">By applying all the labeling functions on all the data points, we generate an <em class="italic">m × n</em> matrix of data points and their labels. The label generated by the heuristic <em class="italic">LF</em><sub class="" style="font-style: italic;">j</sub> on the data point <em class="italic">X</em><sub class="" style="font-style: italic;">i</sub> can be represented by:</p>
    <figure class="mediaobject"><img src="image/B16252_08_001.png" alt="" style="max-height:20px;"/></figure>
    <p class="normal">The generative model is <a id="_idIndexMarker613"/>trying to learn from the agreements and disagreements between the labeling functions to learn the parameters.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">Generative versus Discriminative models</strong></p>
      <p class="Information-Box--PACKT-">If we have a set of data, <em class="italic">X</em>, and labels, <em class="italic">Y</em> corresponding to the data, then we can say that the discriminative model tries to capture the <em class="italic">conditional probability</em> <em class="italic">p(Y | X)</em>. A generative model captures the <em class="italic">joint probability p(X, Y)</em>. Generative models, as their name implies, can generate new data points. We saw examples of generative models in <em class="chapterRef">Chapter 5</em>, <em class="italic">Generating Text with RNNs and GPT-2</em>, where <a id="_idIndexMarker614"/>we generated news headlines. <strong class="keyword">GANs</strong> (<strong class="keyword">Generative Adversarial Networks</strong>) and AutoEncoders are well-known generative models. Discriminative models label data points in a given data set. It does so by drawing a plane in the space of features that separates the data points into different classes. Classifiers, like the IMDb sentiment review prediction model, are typically discriminative models.</p>
      <p class="Information-Box--PACKT-">As can be imagined, generative models have a much more challenging task of learning the whole underlying structure of the data.</p>
    </div>
    <p class="normal">The parameter weights, <em class="italic">w</em>, of the generative model <em class="italic">P</em><sub class="" style="font-style: italic;">w</sub> can be estimated via:</p>
    <figure class="mediaobject"><img src="image/B16252_08_002.png" alt="" style="max-height:50px;"/></figure>
    <p class="normal">Not that the log marginal likelihood of the observed labels factors out the predicted labels <em class="italic">Y</em>. Hence, this generative model works in an unsupervised fashion. Once the parameters of the generative model are computed, we can predict the labels for the data points as:</p>
    <figure class="mediaobject"><img src="image/B16252_08_003.png" alt="" style="max-height:25px;"/></figure>
    <p class="normal">Where <em class="italic">Y</em><sub class="" style="font-style: italic;">i</sub> represents labels based on labeling functions and <img src="image/B16252_08_004.png" alt="" style="max-height:20px;"/> represents the predicted label from the generative model. These predicted labels can be fed to a downstream discriminative model for classification.</p>
    <p class="normal">These concepts were implemented in the Snorkel library. The authors of the Snorkel library were the key contributors to introducing the <em class="italic">Data Programming</em> approach, in a paper of the same name <a id="_idIndexMarker615"/>presented at the Neural Information Process Systems conference in 2016. The Snorkel library was introduced formally in a paper titled <em class="italic">Snorkel: rapid training data creation with weak supervision</em> by Ratner et al. in 2019. Apple and Google have published papers using the Snorkel library, with papers on <em class="italic">Overton</em> and <em class="italic">Snorkel Drybell</em>, respectively. These papers can provide an in-depth discussion of the mathematical proof underlying the creation of training data with weak supervision.</p>
    <p class="normal">As complex as the underlying principles may be, using Snorkel for labeling data is not difficult in practice. Let us get started by preparing the data set.</p>
    <h1 id="_idParaDest-145" class="title">Using weakly supervised labels to improve IMDb sentiment analysis</h1>
    <p class="normal">Sentiment analysis of <a id="_idIndexMarker616"/>movie reviews on the IMDb <a id="_idIndexMarker617"/>website is a standard task for classification-type <strong class="keyword">Natural Language Processing</strong> (<strong class="keyword">NLP</strong>) models. We used this data in Chapter 4 to demonstrate transfer learning with GloVe and VERT embeddings. The IMDb data set has 25,000 training examples and 25,000 testing examples. The dataset also includes 50,000 unlabeled reviews. In previous attempts, we ignored these unsupervised data points. Adding more training data will improve the accuracy of the model. However, hand labeling would be a time-consuming and expensive exercise. We'll use Snorkel-powered labeling functions to see if the accuracy of the predictions can be improved on the testing set.</p>
    <h2 id="_idParaDest-146" class="title">Pre-processing the IMDb dataset</h2>
    <p class="normal">Previously, we used the <code class="Code-I -Text--PACKT-">tensorflow_datasets</code> package to download and manage the dataset. However, we <a id="_idIndexMarker618"/>need lower-level access to the data to enable writing the labeling functions. Hence, the first step is to download the dataset from the web.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">The code for this chapter is split across two files. The <code class="Code-I -Text--PACKT-">snorkel-labeling.ipynb</code> file contains the code for downloading data and generating labels using Snorkel. The second file, <code class="Code-I -Text--PACKT-">imdb-with-snorkel-labels.ipynb</code>, contains the code that trains models with and without the additional labeled data. If running the code, then it is best to run all the code in the <code class="Code-I -Text--PACKT-">snorkel-labeling.ipynb</code> file first so that all the labeled data files are generated.</p>
    </div>
    <p class="normal">The dataset is available in one compressed archive and can be downloaded and expanded like so, as shown in <code class="Code-I -Text--PACKT-">snorkel-labeling.ipynb</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">(tf24nlp) $ wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
(tf24nlp) $ tar xvzf aclImdb_v1.tar.gz
</code></pre>
    <p class="normal">This expands the archive in the <code class="Code-I -Text--PACKT-">aclImdb</code> directory. The training and unsupervised data is in the <code class="Code-I -Text--PACKT-">train/</code> subdirectory <a id="_idIndexMarker619"/>while the testing data is in the <code class="Code-I -Text--PACKT-">test/</code> subdirectory. There are additional files, but they can be ignored. <em class="italic">Figure 8.2</em> below shows the directory structure:</p>
    <figure class="mediaobject"><img src="image/B16252_08_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.2: Directory structure for the IMDb data</p>
    <p class="normal">Reviews are stored as individual text files inside the leaf directories. Each file is named using the format <code class="Code-I -Text--PACKT-">&lt;review_id&gt;_&lt;rating&gt;.txt</code>. Review identifiers are sequentially numbered from 0 to 24999 for training and testing examples. For the unsupervised data, the highest review number is 49999. </p>
    <p class="normal">The rating is a number between 0 and 9 and has meaning only in the test and training data. This number reflects the actual rating given to a certain review. The sentiment of all reviews in the <code class="Code-I -Text--PACKT-">pos/</code> subdirectory is positive. The sentiment of reviews in the <code class="Code-I -Text--PACKT-">neg/</code> subdirectory is negative. Ratings of 0 to 4 are considered negative, while ratings between 5 and 9 inclusive are considered positive. In this particular example, we do not use the actual rating and only consider the overall sentiment.</p>
    <p class="normal">We load the data into <code class="Code-I -Text--PACKT-">pandas</code> DataFrames for ease of processing. A convenience function is defined to load reviews from a subdirectory into a DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">load_reviews</span><span class="hljs-functio">(</span><span class="hljs-params">path, columns=[</span><span class="hljs-string">"filename"</span><span class="hljs-params">, </span><span class="hljs-string">'review'</span><span class="hljs-params">]</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(columns) == <span class="hljs-number">2</span>
    l = <span class="hljs-built_in">list</span>()
    <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> glob.glob(path):
        <span class="hljs-comment"># print(filename)</span>
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
            review = f.read()
            l.append((filename, review))
    <span class="hljs-keyword">return</span> pd.DataFrame(l, columns=columns)
</code></pre>
    <p class="normal">The method above loads the <a id="_idIndexMarker620"/>data into two columns – one for the name of the file and one for the text of the file. Using this method, the unsupervised dataset is loaded:</p>
    <pre class="programlisting code"><code class="hljs-code">unsup_df = load_reviews(<span class="hljs-string">"./aclImdb/train/unsup/*.txt"</span>)
unsup_df.describe()
</code></pre>
    <table id="table001-5" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <thead>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">filename
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">review
</code></pre>
          </td>
        </tr>
      </thead>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">count
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">50000
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">50000
</code></pre>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">unique
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">50000
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">49507
</code></pre>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">top
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">./aclImdb/train/unsup/24211_0.txt
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">Am not from America, I usually watch this show...
</code></pre>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">freq
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">1
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">5
</code></pre>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">A slightly different method is used for the training and testing datasets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">load_labelled_data</span><span class="hljs-functio">(</span><span class="hljs-params">path, neg=</span><span class="hljs-string">'/neg/'</span><span class="hljs-params">, </span>
<span class="hljs-params">                       pos=</span><span class="hljs-string">'/pos/'</span><span class="hljs-params">, shuffle=</span><span class="hljs-literal">True</span><span class="hljs-functio">):</span>
    neg_df = load_reviews(path + neg + <span class="hljs-string">"*.txt"</span>)
    pos_df = load_reviews(path + pos + <span class="hljs-string">"*.txt"</span>)
    neg_df[<span class="hljs-string">'sentiment'</span>] = <span class="hljs-number">0</span>
    pos_df[<span class="hljs-string">'sentiment'</span>] = <span class="hljs-number">1</span>
    df = pd.concat([neg_df, pos_df], axis=<span class="hljs-number">0</span>)
    <span class="hljs-keyword">if</span> shuffle:
        df = df.sample(frac=<span class="hljs-number">1</span>, random_state=<span class="hljs-number">42</span>)
    <span class="hljs-keyword">return</span> df
</code></pre>
    <p class="normal">This method returns three columns – the file name, the text of the review, and a sentiment label. The sentiment label is 0 if the sentiment is negative and 1 if the sentiment is positive, as determined by <a id="_idIndexMarker621"/>the directory the review is found in. </p>
    <p class="normal">The training dataset can now be loaded in like so:</p>
    <pre class="programlisting code"><code class="hljs-code">train_df = load_labelled_data(<span class="hljs-string">"./aclImdb/train/"</span>)
train_df.head()
</code></pre>
    <table id="table002-3" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <thead>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">filename
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">review
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">sentiment
</code></pre>
          </td>
        </tr>
      </thead>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">6868
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">./aclImdb/train//neg/6326_4.txt
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">If you're in the mood for some dopey light ent...
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">0
</code></pre>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">11516
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">./aclImdb/train//pos/11177_8.txt
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">*****Spoilers herein*****&lt;br /&gt;&lt;br /&gt;What real...
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">1
</code></pre>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">9668
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">./aclImdb/train//neg/2172_2.txt
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">Bottom of the barrel, unimaginative, and pract...
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">0
</code></pre>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">1140
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">./aclImdb/train//pos/2065_7.txt
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">Fearful Symmetry is a pleasant episode with a ...
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">1
</code></pre>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">1518
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">./aclImdb/train//pos/7147_10.txt
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">I found the storyline in this movie to be very...
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">1
</code></pre>
          </td>
        </tr>
      </tbody>
    </table>
    <div class="packt_tip">
      <p class="Tip--PACKT-">While we don't use the raw scores for the sentiment analysis, it is a good exercise for you to try predicting the score instead of the sentiment on your own. To help with processing the score from the raw files, the following code can be used, which extracts the scores from the file names:</p>
      <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">fn_to_score</span>(f):
    scr = f.split(<span class="hljs-string">"/"</span>)[<span class="hljs-number">-1</span>]  <span class="hljs-comment"># get file name</span>
    scr = scr.split(<span class="hljs-string">"."</span>)[<span class="hljs-number">0</span>] <span class="hljs-comment"># remove extension</span>
    scr = <span class="hljs-built_in">int</span> (scr.split(<span class="hljs-string">"_"</span>)[<span class="hljs-number">-1</span>]) <span class="hljs-comment">#the score</span>
    <span class="hljs-keyword">return</span> scr
train_df[<span class="hljs-string">'score'</span>] = train_df.filename.apply(fn_to_score)
</code></pre>
      <p class="Tip--PACKT-">This adds a new <em class="italic">score</em> column to the DataFrame, which can be used as a starting point.</p>
    </div>
    <p class="normal">The testing data can be loaded <a id="_idIndexMarker622"/>using the same convenience function by passing a different starting data directory.</p>
    <pre class="programlisting code"><code class="hljs-code">test_df = load_labelled_data(<span class="hljs-string">"./aclImdb/test/"</span>)
</code></pre>
    <p class="normal">Once the reviews are loaded in, the next step is to create a tokenizer.</p>
    <h2 id="_idParaDest-147" class="title">Learning a subword tokenizer</h2>
    <p class="normal">A subword tokenizer <a id="_idIndexMarker623"/>can be learned using the <code class="Code-I -Text--PACKT-">tensorflow_datasets</code> package. Note that we want to pass all the training and unsupervised reviews while learning this tokenizer. </p>
    <pre class="programlisting code"><code class="hljs-code">text = unsup_df.review.to_list() + train_df.review.to_list()
</code></pre>
    <p class="normal">This step creates a list of 75,000 items. If the text of the reviews is inspected, there are some HTML tags in the reviews as they were scraped from the IMDb website. We use the Beautiful Soup package to clean these tags.</p>
    <pre class="programlisting code"><code class="hljs-code">txt = [ BeautifulSoup(x).text <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> text ]
</code></pre>
    <p class="normal">Then, we learn the vocabulary with 8,266 entries.</p>
    <pre class="programlisting code"><code class="hljs-code">encoder = tfds.features.text.SubwordTextEncoder.\
                build_from_corpus(txt, target_vocab_size=<span class="hljs-number">2</span>**<span class="hljs-number">13</span>)
encoder.save_to_file(<span class="hljs-string">"imdb"</span>)
</code></pre>
    <p class="normal">This encoder is saved to disk. Learning the vocabulary can be a time-consuming task and needs to be done only once. Saving it to disk saves effort on subsequent runs of the code.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">A pre-trained subword encoder is supplied. It can be found in the GitHub folder corresponding to this chapter and is titled <code class="Code-I -Text--PACKT-">imdb.subwords</code> in case you want to skip these steps.</p>
    </div>
    <p class="normal">Before we jump into a model using data labeled with Snorkel, let us define a baseline model so that we can compare <a id="_idIndexMarker624"/>the performance of the models before and after the addition of weakly supervised labels.</p>
    <h2 id="_idParaDest-148" class="title">A BiLSTM baseline model</h2>
    <p class="normal">To understand the impact of <a id="_idIndexMarker625"/>additional labeled data on model performance, we need a point of comparison. So, we set up a BiLSTM model that we have seen previously as the baseline. There are a few steps of data processing, like tokenizing, vectorization, and padding/truncating the lengths of the data. Since this is code we have seen before in Chapter 3 and 4, it is replicated here for completeness with concise descriptions.</p>
    <p class="normal">Snorkel is effective when the training data size is 10x to 50x the original. IMDb provides 50,000 unlabeled examples. If all these were labeled, then the training data would be 3x the original, which is not enough to show the value of Snorkel. Consequently, we simulate an ~18x ratio by limiting the training data to only 2,000 records. The rest of the training records are treated as unlabeled data, and Snorkel is used to supply noisy labels. To prevent the leakage of labels, we split the training data and store two separate DataFrames. The code for this split can be found in the <code class="Code-I -Text--PACKT-">snorkel-labeling.ipynb</code> notebook. The code fragment used to generate the split is shown below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-comment"># Randomly split training into 2k / 23k sets</span>
train_2k, train_23k = train_test_split(train_df, test_size=<span class="hljs-number">23000</span>, 
                                      random_state=<span class="hljs-number">42</span>, 
                                      stratify=train_df.sentiment)
train_2k.to_pickle(<span class="hljs-string">"train_2k.df"</span>)
</code></pre>
    <p class="normal">A stratified split is used to ensure an equal number of positive and negative labels are sampled. A DataFrame with 2,000 records is saved. This DataFrame is used for training the baseline. Note that this may look like a contrived example but remember that the key feature of text data is that there is a lot of it; however, labels are scarce. Often the main barrier to labeling is the amount of effort required to label more data. Before we see how to label large amounts of data, let's complete training the baseline model for comparison.</p>
    <h3 id="_idParaDest-149" class="title">Tokenization and vectorizing data</h3>
    <p class="normal">We tokenize all <a id="_idIndexMarker626"/>reviews in the training set and <a id="_idIndexMarker627"/>truncate/pad to a maximum of 150 tokens. Reviews are passed through Beautiful Soup to remove any HTML markup. All the code for this section can be found in the section titled <em class="italic">Training Data Vectorization</em> in the <code class="Code-I -Text--PACKT-">imdb-with-snorkel-labels.ipynb</code> file. Only the specific pieces of code are shown here for brevity:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># we need a sample of 2000 reviews for training</span>
num_recs = <span class="hljs-number">2000</span>
train_small = pd.read_pickle(<span class="hljs-string">"train_2k.df"</span>)
<span class="hljs-comment"># we dont need the snorkel column</span>
train_small = train_small.drop(columns=[<span class="hljs-string">'snorkel'</span>])
<span class="hljs-comment"># remove markup</span>
cleaned_reviews = train_small.review.apply(<span class="hljs-keyword">lambda</span> x: BeautifulSoup(x).text)
<span class="hljs-comment"># convert pandas DF in to tf.Dataset</span>
train = tf.data.Dataset.from_tensor_slices(
                             (cleaned_reviews.values,
                             train_small.sentiment.values))
</code></pre>
    <p class="normal">Tokenization and vectorization are done through helper functions and applied over the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># transformation functions to be used with the dataset</span>
<span class="hljs-keyword">from</span> tensorflow.keras. pre-processing <span class="hljs-keyword">import</span> sequence
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">encode_pad_transform</span><span class="hljs-functio">(</span><span class="hljs-params">sample</span><span class="hljs-functio">):</span>
    encoded = imdb_encoder.encode(sample.numpy())
    pad = sequence.pad_sequences([encoded], padding=<span class="hljs-string">'post'</span>, maxlen=<span class="hljs-number">150</span>)
    <span class="hljs-keyword">return</span> np.array(pad[<span class="hljs-number">0</span>], dtype=np.int64)  
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">encode_tf_fn</span><span class="hljs-functio">(</span><span class="hljs-params">sample, label</span><span class="hljs-functio">):</span>
    encoded = tf.py_function(encode_pad_transform, 
                                       inp=[sample], 
                                       Tout=(tf.int64))
    encoded.set_shape([<span class="hljs-literal">None</span>])
    label.set_shape([])
    <span class="hljs-keyword">return</span> encoded, label
encoded_train = train.<span class="hljs-built_in">map</span>(encode_tf_fn,
                 num_parallel_calls=tf.data.experimental.AUTOTUNE)
</code></pre>
    <p class="normal">The test data <a id="_idIndexMarker628"/>is also processed <a id="_idIndexMarker629"/>similarly:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># remove markup</span>
cleaned_reviews = test_df.review.apply(
<span class="hljs-keyword">lambda</span> x: BeautifulSoup(x).text)
<span class="hljs-comment"># convert pandas DF in to tf.Dataset</span>
test = tf.data.Dataset.from_tensor_slices((cleaned_reviews.values, 
                                        test_df.sentiment.values))
encoded_test = test.<span class="hljs-built_in">map</span>(encode_tf_fn,
                 num_parallel_calls=tf.data.experimental.AUTOTUNE)
</code></pre>
    <p class="normal">Once the data is ready, the next step is setting up the model.</p>
    <h3 id="_idParaDest-150" class="title">Training using a BiLSTM model</h3>
    <p class="normal">The code for creating <a id="_idIndexMarker630"/>and training the baseline is in the <em class="italic">Baseline Model</em> section of the notebook. A modestly sized model is created as the focus is on showing the gains from unsupervised labeling as opposed to model complexity. Plus, a smaller model trains faster and allows more iteration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Length of the vocabulary </span>
vocab_size = imdb_encoder.vocab_size 
<span class="hljs-comment"># Number of RNN units</span>
rnn_units = <span class="hljs-number">64</span>
<span class="hljs-comment"># Embedding size</span>
embedding_dim = <span class="hljs-number">64</span>
<span class="hljs-comment">#batch size</span>
BATCH_SIZE=<span class="hljs-number">100</span>
</code></pre>
    <p class="normal">The model uses a small 64-dimensional embedding and RNN units. The function for creating the model is below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Embedding, LSTM, \
                                    Bidirectional, Dense,\
                                    Dropout
            
dropout=<span class="hljs-number">0.5</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">build_model_bilstm</span><span class="hljs-functio">(</span><span class="hljs-params">vocab_size, embedding_dim, rnn_units, batch_size, dropout=</span><span class="hljs-number">0.</span><span class="hljs-functio">):</span>
    model = tf.keras.Sequential([
        Embedding(vocab_size, embedding_dim, mask_zero=<span class="hljs-literal">True</span>,
                  batch_input_shape=[batch_size, <span class="hljs-literal">None</span>]),
        Bidirectional(LSTM(rnn_units, return_sequences=<span class="hljs-literal">True</span>)),
        Bidirectional(tf.keras.layers.LSTM(rnn_units)),
        Dense(rnn_units, activation=<span class="hljs-string">'relu'</span>),
        Dropout(dropout),
        Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)
      ])
    <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">A modest amount of <a id="_idIndexMarker631"/>dropout is added to have the model generalize better. This model has about 700K parameters. </p>
    <pre class="programlisting code"><code class="hljs-code">bilstm = build_model_bilstm(
  vocab_size = vocab_size,
  embedding_dim=embedding_dim,
  rnn_units=rnn_units,
  batch_size=BATCH_SIZE)
bilstm.summary()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (100, None, 64)           529024    
_________________________________________________________________
bidirectional_8 (Bidirection (100, None, 128)          66048     
_________________________________________________________________
bidirectional_9 (Bidirection (100, 128)                98816     
_________________________________________________________________
dense_6 (Dense)              (100, 64)                 8256      
_________________________________________________________________
dropout_6 (Dropout)          (100, 64)                 0         
_________________________________________________________________
dense_7 (Dense)              (100, 1)                  65        
=================================================================
Total params: 702,209
Trainable params: 702,209
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">The model is compiled with a binary cross-entropy loss function and the ADAM optimizer. Accuracy, precision, and <a id="_idIndexMarker632"/>recall metrics are tracked. This model is trained for 15 epochs and it can be seen that the model is saturated:</p>
    <pre class="programlisting code"><code class="hljs-code">bilstm.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'binary_crossentropy'</span>, 
             optimizer=<span class="hljs-string">'adam'</span>, 
             metrics=[<span class="hljs-string">'accuracy'</span>, <span class="hljs-string">'Precision'</span>, <span class="hljs-string">'Recall'</span>])
encoded_train_batched = encoded_train.shuffle(num_recs, seed=<span class="hljs-number">42</span>).\
                                    batch(BATCH_SIZE)
bilstm.fit(encoded_train_batched, epochs=<span class="hljs-number">15</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Train for 15 steps
Epoch 1/15
20/20 [==============================] - 16s 793ms/step - loss: 0.6943 - accuracy: 0.4795 - Precision: 0.4833 - Recall: 0.5940
…
Epoch 15/15
20/20 [==============================] - 4s 206ms/step - loss: 0.0044 - accuracy: 0.9995 - Precision: 0.9990 - Recall: 1.0000
</code></pre>
    <p class="normal">As we can see, the model is overfitting to the small training set even after dropout regularization.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-"><strong class="scree Text">Batch-and-Shuffle or Shuffle-and-Batch</strong></p>
      <p class="Tip--PACKT-">Note the second line of code in the fragment above, which shuffles and batches the data. The data is shuffled and then batched. Shuffling data between epochs is a form of regularization and enables the model to learn better. Shuffling before batching is a key point to remember in TensorFlow. If data is batched before shuffling, then only the order of the batches will be moved around when being fed to the model. However, the composition of each batch remains the same across epochs. By shuffling before batching, we ensure each batch looks different in each epoch. You are encouraged to train with and without shuffled data. While shuffling increases training time slightly, it gives better performance on the test set.</p>
    </div>
    <p class="normal">Let us see how this model does on the test data:</p>
    <pre class="programlisting code"><code class="hljs-code">bilstm.evaluate(encoded_test.batch(BATCH_SIZE))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">250/250 [==============================] - 33s 134ms/step - loss: 2.1440 - accuracy: 0.7591 - precision: 0.7455 - recall: 0.7866
</code></pre>
    <p class="normal">The <a id="_idIndexMarker633"/>model has 75.9% accuracy. The precision of the model is higher than the recall. Now that we have a baseline, we can see if weakly supervised labeling helps improve model performance. That is the focus of the next section.</p>
    <h1 id="_idParaDest-151" class="title">Weakly supervised labeling with Snorkel</h1>
    <p class="normal">The IMDb dataset has 50,000 <a id="_idIndexMarker634"/>unlabeled reviews. This is double <a id="_idIndexMarker635"/>the size of the training set, which has 25,000 labeled reviews. As explained in the previous section, we have reserved 23,000 records from the training data in addition to the unsupervised set for weakly supervised labeling. Labeling records in Snorkel is performed via labeling functions. Each labeling function can return one of the possible labels of abstain from labeling. Since this is a binary classification problem, corresponding constants are defined. A sample labeling function is also shown. All the code for this section can be found in the notebook titled <code class="Code-I -Text--PACKT-">snorkel-labeling.ipynb</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">POSITIVE = <span class="hljs-number">1</span>
NEGATIVE = <span class="hljs-number">0</span>
ABSTAIN = <span class="hljs-number">-1</span>
<span class="hljs-keyword">from</span> snorkel.labeling.lf <span class="hljs-keyword">import</span> labeling_function
<span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">time_waste</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = <span class="hljs-string">"time waste"</span>
    ex2 = <span class="hljs-string">"waste of time"</span>
    <span class="hljs-keyword">if</span> ex1 <span class="hljs-keyword">in</span> x.review.lower() <span class="hljs-keyword">or</span> ex2 <span class="hljs-keyword">in</span> x.review.lower():
        <span class="hljs-keyword">return</span> NEGATIVE
    <span class="hljs-keyword">return</span> ABSTAIN
</code></pre>
    <p class="normal">Labeling functions are annotated with a <code class="Code-I -Text--PACKT-">labeling_function()</code> provided Snorkel. Note that the Snorkel library needs to be installed. Detailed instructions can be found on GitHub in this chapter's subdirectory. In short, Snorkel can be installed by:</p>
    <pre class="programlisting con"><code class="hljs-con">(tf24nlp) $ pip install snorkel==0.9.5
</code></pre>
    <p class="normal">Any warnings you see can be safely ignored as the library uses different versions of components such as TensorBoard. To be doubly sure, you can create a separate <code class="Code-I -Text--PACKT-">conda</code>/virtual environment for Snorkel and its dependencies.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">This chapter would not have been possible without the support of the Snorkel.ai team. Frederic Sala and Alexander Ratner from Snorkel.ai were instrumental in providing guidance and the script for hyperparameter tuning to get the most out of Snorkel.</p>
    </div>
    <p class="normal">Coming back to the labeling function, the function above is expecting a row from a DataFrame. It is expecting that the row has a text "review" column. This function tries to see if the review states that <a id="_idIndexMarker636"/>the movie or show was a <a id="_idIndexMarker637"/>waste of time. If so, it returns a negative label; else, it abstains from labeling the row of data. Note that we are trying to label thousands of rows of data in a short time using these labeling functions. The best way to do this is to print some random samples of positive and negative reviews and use some words from the text as labeling functions. The central idea here is to create a number of functions that have good accuracy for a subset of the rows. Let's examine some negative reviews in the training set to see what labeling functions can be created:</p>
    <pre class="programlisting code"><code class="hljs-code">neg = train_df[train_df.sentiment==<span class="hljs-number">0</span>].sample(n=<span class="hljs-number">5</span>, random_state=<span class="hljs-number">42</span>)
<span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> neg.review.tolist():
    print(x)
</code></pre>
    <p class="normal">One of the reviews starts off as "A very cheesy and dull road movie," which gives an idea for a labeling function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">cheesy_dull</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = <span class="hljs-string">"cheesy"</span>
    ex2 = <span class="hljs-string">"dull"</span>
    <span class="hljs-keyword">if</span> ex1 <span class="hljs-keyword">in</span> x.review.lower() <span class="hljs-keyword">or</span> ex2 <span class="hljs-keyword">in</span> x.review.lower():
        <span class="hljs-keyword">return</span> NEGATIVE
    <span class="hljs-keyword">return</span> ABSTAIN
</code></pre>
    <p class="normal">There are a number of <a id="_idIndexMarker638"/>different words that occur <a id="_idIndexMarker639"/>in negative reviews. Here is a subset of negative labeling functions. The full list is in the notebook:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">garbage</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = <span class="hljs-string">"garbage"</span>
    <span class="hljs-keyword">if</span> ex1 <span class="hljs-keyword">in</span> x.review.lower():
        <span class="hljs-keyword">return</span> NEGATIVE
    <span class="hljs-keyword">return</span> ABSTAIN
<span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">terrible</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = <span class="hljs-string">"terrible"</span>
    <span class="hljs-keyword">if</span> ex1 <span class="hljs-keyword">in</span> x.review.lower():
        <span class="hljs-keyword">return</span> NEGATIVE
    <span class="hljs-keyword">return</span> ABSTAIN
<span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">unsatisfied</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = <span class="hljs-string">"unsatisf"</span>  <span class="hljs-comment"># unsatisfactory, unsatisfied</span>
    <span class="hljs-keyword">if</span> ex1 <span class="hljs-keyword">in</span> x.review.lower():
        <span class="hljs-keyword">return</span> NEGATIVE
    <span class="hljs-keyword">return</span> ABSTAIN
</code></pre>
    <p class="normal">All the negative labeling functions are added to a list:</p>
    <pre class="programlisting code"><code class="hljs-code">neg_lfs = [atrocious, terrible, piece_of, woefully_miscast, 
           bad_acting, cheesy_dull, disappoint, crap, garbage, 
           unsatisfied, ridiculous]
</code></pre>
    <p class="normal">Examining a sample of negative reviews can give us many ideas. Typically, a small amount of effort from a domain expert can yield multiple labeling functions that can be implemented easily. If you <a id="_idIndexMarker640"/>have ever watched a movie, you <a id="_idIndexMarker641"/>are an expert as far as this dataset is concerned. Examining a sample of positive reviews results in more labeling functions. Here is a sample of labeling functions that identify positive sentiment in reviews:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> re
<span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">classic</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = <span class="hljs-string">"a classic"</span>
    <span class="hljs-keyword">if</span> ex1 <span class="hljs-keyword">in</span> x.review.lower():
        <span class="hljs-keyword">return</span> POSITIVE
    <span class="hljs-keyword">return</span> ABSTAIN
<span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">great_direction</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = <span class="hljs-string">"(great|awesome|amazing|fantastic|excellent) direction"</span>
    <span class="hljs-keyword">if</span> re.search(ex1, x.review.lower()):
        <span class="hljs-keyword">return</span> POSITIVE
    <span class="hljs-keyword">return</span> ABSTAIN
<span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">great_story</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = <span class="hljs-string">"(great|awesome|amazing|fantastic|excellent|dramatic) (script|story)"</span>
    <span class="hljs-keyword">if</span> re.search(ex1, x.review.lower()):
        <span class="hljs-keyword">return</span> POSITIVE
    <span class="hljs-keyword">return</span> ABSTAIN
</code></pre>
    <p class="normal">All of the positive labeling functions can be seen in the notebook. Similar to the negative functions, a list of the positive labeling functions is defined:</p>
    <pre class="programlisting code"><code class="hljs-code">pos_lfs = [classic, must_watch, oscar, love, great_entertainment,
           very_entertaining, amazing, brilliant, fantastic, 
           awesome, great_acting, great_direction, great_story,
           favourite]
<span class="hljs-comment"># set of labeling functions</span>
lfs = neg_lfs + pos_lfs
</code></pre>
    <p class="normal">The development of labeling is an iterative process. Don't be intimidated by the number of labeling functions shown here. You can see that they are quite simple, for the most part. To help you understand the amount of effort, I spent a total of 3 hours on creating and testing labeling functions:</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Note that the notebook contains a large number of simple labeling functions, of which only a subset are shown here. Please refer to the actual code for all the labeling functions.</p>
    </div>
    <p class="normal">The process <a id="_idIndexMarker642"/>involved looking at some samples and creating the labeling functions, followed by evaluating the <a id="_idIndexMarker643"/>results on a subset of the data. Checking out examples of where the labeling functions disagreed with the labeled examples was very useful in making functions narrower or adding compensating functions. So, let's see how we can evaluate these functions so we can iterate on them.</p>
    <h2 id="_idParaDest-152" class="title">Iterating on labeling functions </h2>
    <p class="normal">Once a set of labeling <a id="_idIndexMarker644"/>functions are defined, they can be applied to a pandas DataFrame, and a model can be trained to compute the weights assigned to various labeling functions while computing the labels. Snorkel provides functions that help with these tasks. First, let us apply these labeling functions to compute a matrix. This matrix has as many columns as there are labeling functions for every row of data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># let's take a sample of 100 records from training set</span>
lf_train = train_df.sample(n=<span class="hljs-number">1000</span>, random_state=<span class="hljs-number">42</span>)
<span class="hljs-keyword">from</span> snorkel.labeling.model <span class="hljs-keyword">import</span> LabelModel
<span class="hljs-keyword">from</span> snorkel.labeling <span class="hljs-keyword">import</span> PandasLFApplier
<span class="hljs-comment"># Apply the LFs to the unlabeled training data</span>
applier = PandasLFApplier(lfs)
L_train = applier.apply(lf_train)
</code></pre>
    <p class="normal">In the code above, a sample of 1000 rows of data from the training data is extracted. Then, the list of all labeling functions created previously is passed to Snorkel and applied to this sample of training data. If we created 25 labeling functions, the shape of <code class="Code-I -Text--PACKT-">L_train</code> would be (1000, 25). Each column represents the output of a labeling function. A generative model can now be trained on this label matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Train the label model and compute the training labels</span>
label_model = LabelModel(cardinality=<span class="hljs-number">2</span>, verbose=<span class="hljs-literal">True</span>)
label_model.fit(L_train, n_epochs=<span class="hljs-number">500</span>, log_freq=<span class="hljs-number">50</span>, seed=<span class="hljs-number">123</span>)
lf_train[<span class="hljs-string">"snorkel"</span>] = label_model.predict(L=L_train, 
                                      <span class="code-highlight"><strong class="hljs-slc">tie_break_policy=</strong><strong class="hljs-string-slc">"abstain"</strong></span>)
</code></pre>
    <p class="normal">A <code class="Code-I -Text--PACKT-">LabelModel</code> instance is created with a parameter specifying how many labels are in the actual model. This model is then trained, and labels are predicted for the subset of data. These predicted labels are added as a new column to the DataFrame. Note the <code class="Code-I -Text--PACKT-">tie_break_policy</code> parameter <a id="_idIndexMarker645"/>being passed into the <code class="Code-I -Text--PACKT-">predict()</code> method. In case the model has conflicting outputs from labeling functions, and they have the same scores from the model, this parameter specifies how the conflict should be resolved. Here, we instruct the model to abstain from labeling the records in case of a conflict. Another possible setting is "random," where the model will randomly assign the output from one of the tied labeling functions. The main difference between these two options, in the context of the problem at hand, is precision. By asking the model to abstain from labeling, we get higher precision results, but fewer records will be labeled. Randomly choosing one of the functions that were tied results in higher coverage, but presumably at lower quality. This hypothesis can be tested by training the same model with the outputs of the two options separately. You are encouraged to try these options and see the results for yourself.</p>
    <p class="normal">Since the abstain policy was chosen, all of the 1000 rows may not have been labeled:</p>
    <pre class="programlisting code"><code class="hljs-code">pred_lfs = lf_train[lf_train.snorkel &gt; <span class="hljs-number">-1</span>]
pred_lfs.describe()
</code></pre>
    <table id="table003-2" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <thead>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">sentiment
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">score
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">snorkel
</code></pre>
          </td>
        </tr>
      </thead>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">count
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">598.000000
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">598.000000
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">598.000000
</code></pre>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Out of 1000 records, only 458 were labeled. Let's check how many of these were labeled incorrectly:</p>
    <pre class="programlisting code"><code class="hljs-code">pred_mistake = pred_lfs[pred_lfs.sentiment != pred_lfs.snorkel]
pred_mistake.describe()
</code></pre>
    <table id="table004-1" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <thead>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">sentiment
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">score
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">snorkel
</code></pre>
          </td>
        </tr>
      </thead>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">count
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">164.000000
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">164.000000
</code></pre>
          </td>
          <td class="No-Table-Style">
            <pre class="programlisting gen"><code class="hljs">164.000000
</code></pre>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Snorkel, armed with our labeling functions, labeled 598 records, out of which 434 labels were correct and 164 records were incorrectly labeled. The label model has an accuracy of ~72.6%. To get inspiration for <a id="_idIndexMarker646"/>more labeling functions, you should inspect a few of the rows where the label model produced the wrong results and update or add labeling functions. As mentioned above, a total of approximately 3 hours was spent on iterating and creating labeling functions to get a total of 25 functions. To get more out of Snorkel, we need to increase the amount of training data. The objective is to develop a method that gets us many labels quickly, without a lot of manual effort. One technique that can be used in this specific case is training a simple Naïve-Bayes model to get words that are highly correlated with positive or negative labels. This is the focus <a id="_idIndexMarker647"/>of the next section. <strong class="keyword">Naïve-Bayes</strong> (<strong class="keyword">NB</strong>) is a basic technique covered in many basic NLP books.</p>
    <h1 id="_idParaDest-153" class="title">Naïve-Bayes model for finding keywords</h1>
    <p class="normal">Building <a id="_idIndexMarker648"/>an NB model on this dataset takes under an hour and has the potential to significantly increase the quality and coverage of the labeling functions. The core model code for the NB model can be found in the <code class="Code-I -Text--PACKT-">spam-inspired-technique-naive-bayes.ipynb</code> notebook. Note that these explorations are aside from the main labeling code, and this section can be skipped if desired, as the learnings from this section are applied to construct better labeling functions outlined in the <code class="Code-I -Text--PACKT-">snorkel-labeling.ipynb</code> notebook.</p>
    <p class="normal">The main flow of the NB-based exploration is to load the reviews, remove stop words, take the top 2,000 words to construct a simple vectorization scheme, and train an NB model. Since data loading is the same as covered in previous sections, the details are skipped in this section.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">This section uses the NLTK and <code class="Code-I -Text--PACKT-">wordcloud</code> Python packages. NLTK should already be installed as we have used it in <em class="chapterRef">Chapter 1</em>, <em class="italic">Essentials of NLP</em>. <code class="Code-I -Text--PACKT-">wordcloud</code> can be installed with:</p>
      <p class="Information-Box--PACKT-"><code class="Code-I -Text--PACKT-">(tf24nlp) $ pip install wordcloud==1.8</code></p>
    </div>
    <p class="normal">Word clouds help get an aggregate understanding of the positive and negative review text. Note that counters are required for the top-2000 word vectorization scheme. A convenience function that cleans HTML text along with removing stop words and tokenizing the rest into a list is defined as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">en_stopw = <span class="hljs-built_in">set</span>(stopwords.words(<span class="hljs-string">"english"</span>))
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">get_words</span><span class="hljs-functio">(</span><span class="hljs-params">review, words, stopw=en_stopw</span><span class="hljs-functio">):</span>
    review = BeautifulSoup(review).text        <span class="hljs-comment"># remove HTML tags</span>
    review = re.sub(<span class="hljs-string">'[^A-Za-z]'</span>, <span class="hljs-string">' '</span>, review)  <span class="hljs-comment"># remove non letters</span>
    review = review.lower()
    tok_rev = wt(review)
    rev_word = [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> tok_rev <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopw]
    words += rev_word
</code></pre>
    <p class="normal">Then, the <a id="_idIndexMarker649"/>positive reviews are separated and a word cloud is generated for visualization purposes:</p>
    <pre class="programlisting code"><code class="hljs-code">pos_rev = train_df[train_df.sentiment == <span class="hljs-number">1</span>]
pos_words = []
pos_rev.review.apply(get_words, args=(pos_words,))
<span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
pos_words_sen = <span class="hljs-string">" "</span>.join(pos_words)
pos_wc = WordCloud(width = <span class="hljs-number">600</span>,height = <span class="hljs-number">512</span>).generate(pos_words_sen)
plt.figure(figsize = (<span class="hljs-number">12</span>, <span class="hljs-number">8</span>), facecolor = <span class="hljs-string">'k'</span>)
plt.imshow(pos_wc)
plt.axis(<span class="hljs-string">'off'</span>)
plt.tight_layout(pad = <span class="hljs-number">0</span>)
plt.show()
</code></pre>
    <p class="normal">The output of the preceding code is shown in <em class="italic">Figure 8.3</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_08_03.png" alt="A close up of a sign  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.3: Positive reviews word cloud</p>
    <p class="normal">It is not <a id="_idIndexMarker650"/>surprising that <em class="italic">movie</em> and <em class="italic">film</em> are the biggest words. However, there are a number of other suggestions for keywords that can be seen here. Similarly, a word cloud for the negative reviews can be generated, as shown in <em class="italic">Figure 8.4</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_08_04.png" alt="A close up of a sign  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.4: Negative reviews word cloud</p>
    <p class="normal">These <a id="_idIndexMarker651"/>visualizations are interesting; however, a clearer picture will emerge after training the model. Only the top 2,000 words are needed for training the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter
pos = Counter(pos_words)
neg = Counter(neg_words)
<span class="hljs-comment"># let's try to build a naive bayes model for sentiment classification</span>
tot_words = pos + neg
tot_words.most_common(<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[('movie', 44031),
 ('film', 40147),
 ('one', 26788),
 ('like', 20274),
 ('good', 15140),
 ('time', 12724),
 ('even', 12646),
 ('would', 12436),
 ('story', 11983),
 ('really', 11736)]
</code></pre>
    <p class="normal">Combined counters show the top 10 most frequently appearing words in all reviews. These are extracted into a list:</p>
    <pre class="programlisting code"><code class="hljs-code">top2k = [x <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> tot_words.most_common(<span class="hljs-number">2000</span>)]
</code></pre>
    <p class="normal">The vectorization of each review is fairly simple – each of the 2000 words becomes a column for a given review. If the word represented by the column is present in the review, the value of the <a id="_idIndexMarker652"/>column is marked as 1 for that review, or 0 otherwise. So, each review is represented by a sequence of 0s and 1s representing which of the top 2000 words the review contained. The code below shows this transformation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">featurize</span><span class="hljs-functio">(</span><span class="hljs-params">review, topk=top2k, stopw=en_stopw</span><span class="hljs-functio">):</span>
    review = BeautifulSoup(review).text        <span class="hljs-comment"># remove HTML tags</span>
    review = re.sub(<span class="hljs-string">'[^A-Za-z]'</span>, <span class="hljs-string">' '</span>, review)  <span class="hljs-comment"># remove nonletters</span>
    review = review.lower()
    tok_rev = wt(review)
    rev_word = [word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> tok_rev <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopw]
    features = {}
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> top2k:
        features[<span class="hljs-string">'contains({})'</span>.<span class="hljs-built_in">format</span>(word)] = (word <span class="hljs-keyword">in</span> rev_word)
    <span class="hljs-keyword">return</span> features
train = [(featurize(rev), senti) <span class="hljs-keyword">for</span> (rev, senti) <span class="hljs-keyword">in</span> 
                        <span class="hljs-built_in">zip</span>(train_df.review, train_df.sentiment)]
</code></pre>
    <p class="normal">Training the model is quite trivial. Note that the Bernoulli NB model is used here as each word is represented according to its presence or absence in the review. Alternatively, the frequency of the word in the review could also be used. If the frequency of the word is used while vectorizing the review above, then the multinomial form of NB should be used.</p>
    <p class="normal">NLTK also provides a way to inspect the most informative features:</p>
    <pre class="programlisting code"><code class="hljs-code">classifier = nltk.NaiveBayesClassifier.train(train)
<span class="hljs-comment"># 0: negative sentiment, 1: positive sentiment</span>
classifier.show_most_informative_features(<span class="hljs-number">20</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Most Informative Features
       contains(unfunny) = True      0 : 1      =     14.1 : 1.0
         contains(waste) = True      0 : 1      =     12.7 : 1.0
     contains(pointless) = True      0 : 1      =     10.4 : 1.0
     contains(redeeming) = True      0 : 1      =     10.1 : 1.0
     contains(laughable) = True      0 : 1      =      9.3 : 1.0
         contains(worst) = True      0 : 1      =      9.0 : 1.0
         contains(awful) = True      0 : 1      =      8.4 : 1.0
        contains(poorly) = True      0 : 1      =      8.2 : 1.0
   contains(wonderfully) = True      1 : 0      =      7.6 : 1.0
         contains(sucks) = True      0 : 1      =      7.0 : 1.0
          contains(lame) = True      0 : 1      =      6.9 : 1.0
      contains(pathetic) = True      0 : 1      =      6.4 : 1.0
    contains(delightful) = True      1 : 0      =      6.0 : 1.0
        contains(wasted) = True      0 : 1      =      6.0 : 1.0
          contains(crap) = True      0 : 1      =      5.9 : 1.0
   contains(beautifully) = True      1 : 0      =      5.8 : 1.0
      contains(dreadful) = True      0 : 1      =      5.7 : 1.0
          contains(mess) = True      0 : 1      =      5.6 : 1.0
      contains(horrible) = True      0 : 1      =      5.5 : 1.0
        contains(superb) = True      1 : 0      =      5.4 : 1.0
       contains(garbage) = True      0 : 1      =      5.3 : 1.0
         contains(badly) = True      0 : 1      =      5.3 : 1.0
        contains(wooden) = True      0 : 1      =      5.2 : 1.0
      contains(touching) = True      1 : 0      =      5.1 : 1.0
      contains(terrible) = True      0 : 1      =      5.1 : 1.0
</code></pre>
    <p class="normal">This whole <a id="_idIndexMarker653"/>exercise was done to find which words are most useful in predicting negative and positive reviews. The table above shows the words and the likelihood ratios. Taking the first row of the output for the word <em class="italic">unfunny</em> as an example, the model is saying that reviews containing <em class="italic">unfunny</em> are negative 14.1 times more often than they are positive. The labeling functions are updated using a number of these keywords.</p>
    <p class="normal">Upon analyzing the labels assigned by the labeling functions in <code class="Code-I -Text--PACKT-">snorkel-labeling.ipynb</code>, it can be seen that more negative reviews are being labeled as compared to positive reviews. Consequently, the labeling functions use a larger list of words for positive labels as compared to negative labels. Note that imbalanced datasets have issues with <a id="_idIndexMarker654"/>overall training accuracy and specifically with recall. The following code fragment shows augmented labeling functions using the keywords discovered through NB above:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Some positive high prob words - arbitrary cutoff of 4.5x</span>
<span class="hljs-string">'''</span>
<span class="hljs-string">   contains(wonderfully) = True       1 : 0      =      7.6 : 1.0</span>
<span class="hljs-string">    contains(delightful) = True       1 : 0      =      6.0 : 1.0</span>
<span class="hljs-string">   contains(beautifully) = True       1 : 0      =      5.8 : 1.0</span>
<span class="hljs-string">        contains(superb) = True       1 : 0      =      5.4 : 1.0</span>
<span class="hljs-string">      contains(touching) = True       1 : 0      =      5.1 : 1.0</span>
<span class="hljs-string">   contains(brilliantly) = True       1 : 0      =      4.7 : 1.0</span>
<span class="hljs-string">    contains(friendship) = True       1 : 0      =      4.6 : 1.0</span>
<span class="hljs-string">        contains(finest) = True       1 : 0      =      4.5 : 1.0</span>
<span class="hljs-string">      contains(terrific) = True       1 : 0      =      4.5 : 1.0</span>
<span class="hljs-string">           contains(gem) = True       1 : 0      =      4.5 : 1.0</span>
<span class="hljs-string">   contains(magnificent) = True       1 : 0      =      4.5 : 1.0</span>
<span class="hljs-string">'''</span>
wonderfully_kw = make_keyword_lf(keywords=[<span class="hljs-string">"wonderfully"</span>], 
label=POSITIVE)
delightful_kw = make_keyword_lf(keywords=[<span class="hljs-string">"delightful"</span>], 
label=POSITIVE)
superb_kw = make_keyword_lf(keywords=[<span class="hljs-string">"superb"</span>], label=POSITIVE)
pos_words = [<span class="hljs-string">"beautifully"</span>, <span class="hljs-string">"touching"</span>, <span class="hljs-string">"brilliantly"</span>, 
<span class="hljs-string">"friendship"</span>, <span class="hljs-string">"finest"</span>, <span class="hljs-string">"terrific"</span>, <span class="hljs-string">"magnificent"</span>]
pos_nb_kw = make_keyword_lf(keywords=pos_words, label=POSITIVE)
<span class="hljs-meta">@labeling_function()</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">superlatives</span><span class="hljs-functio">(</span><span class="hljs-params">x</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x.review, <span class="hljs-built_in">str</span>):
        <span class="hljs-keyword">return</span> ABSTAIN
    ex1 = [<span class="hljs-string">"best"</span>, <span class="hljs-string">"super"</span>, <span class="hljs-string">"great"</span>,<span class="hljs-string">"awesome"</span>,<span class="hljs-string">"amaz"</span>, <span class="hljs-string">"fantastic"</span>, 
           <span class="hljs-string">"excellent"</span>, <span class="hljs-string">"favorite"</span>]
    pos_words = [<span class="hljs-string">"beautifully"</span>, <span class="hljs-string">"touching"</span>, <span class="hljs-string">"brilliantly"</span>, 
<span class="hljs-string">                 "friendship"</span>, <span class="hljs-string">"finest"</span>, <span class="hljs-string">"terrific"</span>, <span class="hljs-string">"magnificent"</span>, 
<span class="hljs-string">                 "wonderfully"</span>, <span class="hljs-string">"delightful"</span>]
    ex1 += pos_words
    rv = x.review.lower()
    counts = [rv.count(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> ex1]
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span>(counts) &gt;= <span class="hljs-number">3</span>:
        <span class="hljs-keyword">return</span> POSITIVE
    <span class="hljs-keyword">return</span> ABSTAIN
</code></pre>
    <p class="normal">Since keyword-based labeling functions are quite common, Snorkel provides an easy way to define such functions. The following code fragment uses two programmatic ways of converting a list of words into a set of labeling functions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Utilities for defining keywords based functions</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">keyword_lookup</span><span class="hljs-functio">(</span><span class="hljs-params">x, keywords, label</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">any</span>(word <span class="hljs-keyword">in</span> x.review.lower() <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> keywords):
        <span class="hljs-keyword">return</span> label
    <span class="hljs-keyword">return</span> ABSTAIN
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">make_keyword_lf</span><span class="hljs-functio">(</span><span class="hljs-params">keywords, label</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">return</span> LabelingFunction(
        name=<span class="hljs-string">f"keyword_</span><span class="hljs-subst">{keywords[</span><span class="hljs-number">0</span><span class="hljs-subst">]}</span><span class="hljs-string">"</span>,
        f=keyword_lookup,
        resources=<span class="hljs-built_in">dict</span>(keywords=keywords, label=label),
    )
</code></pre>
    <p class="normal">The first <a id="_idIndexMarker655"/>function does the simple matching and returns the specific label, or it abstains. Check out the <code class="Code-I -Text--PACKT-">snorkel-labeling.ipynb</code> file for the full list of labeling functions that were iteratively developed. All in all, I spent approximately 12-14 hours on labeling functions and investigations.</p>
    <p class="normal">Before we try to train the model using this data, let us evaluate the accuracy of this model on the entire training data set.</p>
    <h2 id="_idParaDest-154" class="title">Evaluating weakly supervised labels on the training set</h2>
    <p class="normal">We apply the <a id="_idIndexMarker656"/>labeling functions and train a model on the entire training dataset just to evaluate the quality of this model:</p>
    <pre class="programlisting code"><code class="hljs-code">L_train_full = applier.apply(train_df)
label_model = LabelModel(cardinality=<span class="hljs-number">2</span>, verbose=<span class="hljs-literal">True</span>)
label_model.fit(L_train_full, n_epochs=<span class="hljs-number">500</span>, log_freq=<span class="hljs-number">50</span>, seed=<span class="hljs-number">123</span>)
metrics = label_model.score(L=L_train_full, Y=train_df.sentiment, 
                            tie_break_policy=<span class="hljs-string">"abstain"</span>,
                            metrics=[<span class="hljs-string">"accuracy"</span>, <span class="hljs-string">"coverage"</span>, 
                                     <span class="hljs-string">"precision"</span>, 
                                     <span class="hljs-string">"recall"</span>, <span class="hljs-string">"f1"</span>])
print(<span class="hljs-string">"All Metrics: "</span>, metrics)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Label Model Accuracy:     78.5%
All Metrics:  {<span class="code-highlight"><strong class="hljs-con-slc">'accuracy': 0.7854110013835218, 'coverage': 0.83844</strong></span>, 'precision': 0.8564883605745418, 'recall': 0.6744344773790951, 'f1': 0.7546367008509709}
</code></pre>
    <p class="normal">Our <a id="_idIndexMarker657"/>set of labeling functions covers 83.4% of the 25,000 training records, with 85.6% correct labels. Snorkel provides the ability to analyze the performance of each labeling function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> snorkel.labeling <span class="hljs-keyword">import</span> LFAnalysis
LFAnalysis(L=L_train_full, lfs=lfs).lf_summary()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">                      j Polarity  Coverage  Overlaps  Conflicts
atrocious             0      [0]   0.00816   0.00768    0.00328
terrible              1      [0]   0.05356   0.05356    0.02696
piece_of              2      [0]   0.00084   0.00080    0.00048
woefully_miscast      3      [0]   0.00848   0.00764    0.00504
<span class="code-highlight"><strong class="hljs-con-slc">bad_acting            4      [0]   0.08748   0.08348    0.04304</strong></span>
cheesy_dull           5      [0]   0.05136   0.04932    0.02760
bad                  11      [0]   0.03624   0.03624    0.01744
keyword_waste        12      [0]   0.07336   0.06848    0.03232
keyword_pointless    13      [0]   0.01956   0.01836    0.00972
keyword_redeeming    14      [0]   0.01264   0.01192    0.00556
keyword_laughable    15      [0]   0.41036   0.37368    0.20884
negatives            16      [0]   0.35300   0.34720    0.17396
classic              17      [1]   0.01684   0.01476    0.00856
must_watch           18      [1]   0.00176   0.00140    0.00060
oscar                19      [1]   0.00064   0.00060    0.00016
love                 20      [1]   0.08660   0.07536    0.04568
great_entertainment  21      [1]   0.00488   0.00488    0.00292
very_entertaining    22      [1]   0.00544   0.00460    0.00244
<span class="code-highlight"><strong class="hljs-con-slc">amazing              23      [1]   0.05028   0.04516    0.02340</strong></span>
great                31      [1]   0.27728   0.23568    0.13800
keyword_wonderfully  32      [1]   0.01248   0.01248    0.00564
keyword_delightful   33      [1]   0.01188   0.01100    0.00500
keyword_superb       34      [1]   0.02948   0.02636    0.01220
keyword_beautifully  35      [1]   0.08284   0.07428    0.03528
superlatives         36      [1]   0.14656   0.14464    0.07064
keyword_remarkable   37      [1]   0.32052   0.26004    0.14748
</code></pre>
    <p class="normal">Note that a snipped version of the output has been presented here. The full output is available in the notebook. For each labeling function, the table presents what labels are produced and the coverage of the function – that is, the fraction of records it provides a label for, the fraction where it overlaps with another function producing the same label, and <a id="_idIndexMarker658"/>the fraction where it conflicts with another function producing a different label. A positive and a negative label function are highlighted. The <code class="Code-I -Text--PACKT-">bad_acting()</code> function covers 8.7% of the records but overlaps with other functions about 8.3% of the time. However, it conflicts with a function producing a positive label about 4.3% of the time. The <code class="Code-I -Text--PACKT-">amazing()</code> function covers about 5% of the dataset. It conflicts about 2.3% of the time. This data can be used to fine-tune specific functions further and examine how we've separated the data. <em class="italic">Figure 8.5</em> shows the balance between positive, negative, and abstain labels:</p>
    <figure class="mediaobject"><img src="image/B16252_08_05.png" alt="A screen shot of a social media post  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.5: Distribution of labels generated by Snorkel</p>
    <p class="normal">Snorkel has several options for hyperparameter tuning to improve the quality of labeling even further. We <a id="_idIndexMarker659"/>execute a grid search over the parameters to find the best training parameters, while we exclude the labeling functions that are adding noise in the final output.</p>
    <p class="normal">Hyperparameter tuning is done via choosing different learning rates, L2 regularizations, numbers of epochs to run training on, and optimizers to use. Finally, a threshold is used to determine which labeling functions should be kept for the actual labeling task:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Grid Search</span>
<span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> product
lrs = [<span class="hljs-number">1e-1</span>, <span class="hljs-number">1e-2</span>, <span class="hljs-number">1e-3</span>]
l2s = [<span class="hljs-number">0</span>, <span class="hljs-number">1e-1</span>, <span class="hljs-number">1e-2</span>]
n_epochs = [<span class="hljs-number">100</span>, <span class="hljs-number">200</span>, <span class="hljs-number">500</span>]
optimizer = [<span class="hljs-string">"sgd"</span>, <span class="hljs-string">"adam"</span>]
thresh = [<span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>]
lma_best = <span class="hljs-number">0</span>
params_best = []
<span class="hljs-keyword">for</span> params <span class="hljs-keyword">in</span> product(lrs, l2s, n_epochs, optimizer, thresh):
    <span class="hljs-comment"># do the initial pass to access the accuracies</span>
    label_model.fit(L_train_full, n_epochs=params[<span class="hljs-number">2</span>], log_freq=<span class="hljs-number">50</span>, 
                    seed=<span class="hljs-number">123</span>, optimizer=params[<span class="hljs-number">3</span>], lr=params[<span class="hljs-number">0</span>], 
                    l2=params[<span class="hljs-number">1</span>])
    
    <span class="hljs-comment"># accuracies</span>
    weights = label_model.get_weights()
    
    <span class="hljs-comment"># LFs above our threshold </span>
    vals = weights &gt; params[<span class="hljs-number">4</span>]
    
    <span class="hljs-comment"># the LM requires at least 3 LFs to train</span>
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span>(vals) &gt;= <span class="hljs-number">3</span>:
        L_filtered = L_train_full[:, vals]
        label_model.fit(L_filtered, n_epochs=params[<span class="hljs-number">2</span>], 
                        log_freq=<span class="hljs-number">50</span>, seed=<span class="hljs-number">123</span>, 
                        optimizer=params[<span class="hljs-number">3</span>], lr=params[<span class="hljs-number">0</span>], 
                        l2=params[<span class="hljs-number">1</span>])
        label_model_acc = label_model.score(L=L_filtered, 
                          Y=train_df.sentiment, 
                          tie_break_policy=<span class="hljs-string">"abstain"</span>)[<span class="hljs-string">"accuracy"</span>]
        <span class="hljs-keyword">if</span> label_model_acc &gt; lma_best:
            lma_best = label_model_acc
            params_best = params
        
print(<span class="hljs-string">"best = "</span>, lma_best, <span class="hljs-string">" params "</span>, params_best)
</code></pre>
    <p class="normal">Snorkel <a id="_idIndexMarker660"/>may print a warning that metrics are being calculated over non-abstain labels only. This is by design, as we are interested in high-confidence labels. If there is a conflict between labeling functions, then our model abstains from giving it a label. The best parameters printed out are:</p>
    <pre class="programlisting code"><code class="hljs-code">best =  <span class="hljs-number">0.8399649430324277</span>  params  (<span class="hljs-number">0.001</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">200</span>, <span class="hljs-string">'adam'</span>, <span class="hljs-number">0.9</span>)
</code></pre>
    <p class="normal">Through this tuning, the accuracy of the model improved from 78.5% to 84%! </p>
    <p class="normal">Using these parameters, we label the 23k records from the training set and 50k records from the unsupervised set. For the first part, we label all the 25k training records and then split them into two sets. This particular part of splitting was referenced in the baseline model section above:</p>
    <pre class="programlisting code"><code class="hljs-code">train_df[<span class="hljs-string">"snorkel"</span>] = label_model.predict(L=L_filtered, 
                               tie_break_policy=<span class="hljs-string">"abstain"</span>)
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-comment"># Randomly split training into 2k / 23k sets</span>
train_2k, train_23k = train_test_split(train_df, test_size=<span class="hljs-number">23000</span>, 
                                       random_state=<span class="hljs-number">42</span>, 
                                       stratify=train_df.sentiment)
train_23k.snorkel.hist()
train_23k.sentiment.hist()
</code></pre>
    <p class="normal">The <a id="_idIndexMarker661"/>last two lines of code inspect the state of the labels and contrasts with actual labels and generate the graph shown in <em class="italic">Figure 8.6</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_08_06.png" alt="A picture containing screen, building, drawing, food  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.6: Comparison of labels in the training set versus labels generated using Snorkel</p>
    <p class="normal">When the Snorkel model abstains from labeling, it assigns -1 for the label. We see that the model is able to label a lot more negative reviews than positive labels. We filter out the rows where Snorkel abstained from labeling and saved the records:</p>
    <pre class="programlisting code"><code class="hljs-code">lbl_train = train_23k[train_23k.snorkel &gt; <span class="hljs-number">-1</span>]
lbl_train = lbl_train.drop(columns=[<span class="hljs-string">"sentiment"</span>])
p_sup = lbl_train.rename(columns={<span class="hljs-string">"snorkel"</span>: <span class="hljs-string">"sentiment"</span>})
p_sup.to_pickle(<span class="hljs-string">"snorkel_train_labeled.df"</span>)
</code></pre>
    <p class="normal">However, the <a id="_idIndexMarker662"/>key question that we face is that if we augmented the training data with these noisy labels, which are 84% accurate, would it make our model perform better or worse? Note that the baseline model had an accuracy of ~74%.</p>
    <p class="normal">To answer this question, we label the unsupervised set and then train the same model architecture as the baseline.</p>
    <h2 id="_idParaDest-155" class="title">Generating unsupervised labels for unlabeled data</h2>
    <p class="normal">As we saw in the <a id="_idIndexMarker663"/>previous section, where we labeled the training data set, it is quite simple to run the model on the unlabeled reviews of the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Now apply this to all the unsupervised reviews</span>
<span class="hljs-comment"># Apply the LFs to the unlabeled training data</span>
applier = PandasLFApplier(lfs)
<span class="hljs-comment"># now let's apply on the unsupervised dataset</span>
L_train_unsup = applier.apply(unsup_df)
label_model = LabelModel(cardinality=<span class="hljs-number">2</span>, verbose=<span class="hljs-literal">True</span>)
label_model.fit(L_train_unsup[:, vals], n_epochs=params_best[<span class="hljs-number">2</span>], 
                optimizer=params_best[<span class="hljs-number">3</span>], 
                lr=params_best[<span class="hljs-number">0</span>], l2=params_best[<span class="hljs-number">1</span>], 
                log_freq=<span class="hljs-number">100</span>, seed=<span class="hljs-number">42</span>)
unsup_df[<span class="hljs-string">"snorkel"</span>] = label_model.predict(L=L_train_unsup[:, vals], 
                                   tie_break_policy=<span class="hljs-string">"abstain"</span>)
<span class="hljs-comment"># rename snorkel to sentiment &amp; concat to the training dataset</span>
pred_unsup_lfs = unsup_df[unsup_df.snorkel &gt; <span class="hljs-number">-1</span>]
p2 = pred_unsup_lfs.rename(columns={<span class="hljs-string">"snorkel"</span>: <span class="hljs-string">"sentiment"</span>})
print(p2.info())
p2.to_pickle(<span class="hljs-string">"snorkel-unsup-nbs.df"</span>)
</code></pre>
    <p class="normal">Now the label model is trained, and predictions are added to an additional column of the unsupervised dataset. The model labels 29,583 records out of 50,000. This is almost equal to the size of the training dataset. Assuming that the error rate on the unsupervised set is similar to that observed on the training set, we just added ~24,850 records with correct labels and ~4,733 records with incorrect labels into the training set. However, the balance of this dataset is very tilted, as positive label coverage is still poor. There are <a id="_idIndexMarker664"/>approximately 9,000 positive labels for over 20,000 negative labels. The <em class="italic">Increase Positive Label Coverage</em> section of the notebook tries to further improve the coverage of the positive labels by adding more keyword functions. </p>
    <p class="normal">This results in a slightly more balanced set, as shown in the following chart:</p>
    <figure class="mediaobject"><img src="image/B16252_08_07.png" alt="A screen shot of a social media post  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.7: Further improvements in labeling functions applied to the unsupervised dataset improves the positive labels</p>
    <p class="normal">This dataset is saved to disk for use during training:</p>
    <pre class="programlisting code"><code class="hljs-code">p3 = pred_unsup_lfs2.rename(columns={<span class="hljs-string">"snorkel2"</span>: <span class="hljs-string">"sentiment"</span>})
print(p3.info())
p3.to_pickle(<span class="hljs-string">"snorkel-unsup-nbs-v2.df"</span>)
</code></pre>
    <div class="note">
      <p class="Information-Box--PACKT-">Labeled datasets are saved to disk and reloaded in the training code for better modularity and ease of readability. In a production pipeline, intermediate outputs may not be persisted and fed directly into the training steps. Another small consideration here is the separation of virtual/conda environments for running Snorkel. Having a separate script for weakly supervised labeling allows the use of a different Python environment as well.</p>
    </div>
    <p class="normal">We switch our focus back to the <code class="Code-I -Text--PACKT-">imdb-with-snorkel-labels.ipynb</code> notebook, which has the models for training. The code for this part begins from the section <em class="italic">With Snorkel Labeled Data</em>. <a id="_idIndexMarker665"/>The newly labeled records need to be loaded from disk, cleansed, vectorized, and padded before training can be run. We extract the labeled records and remove HTML markup, as shown below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># labelled version of training data split</span>
p1 = pd.read_pickle(<span class="hljs-string">"snorkel_train_labeled.df"</span>)
p2 = pd.read_pickle(<span class="hljs-string">"snorkel-unsup-nbs-v2.df"</span>)
p2 = p2.drop(columns=[<span class="hljs-string">'snorkel'</span>]) <span class="hljs-comment"># so that everything aligns</span>
<span class="hljs-comment"># now concatenate the three DFs</span>
p2 = pd.concat([train_small, p1, p2]) <span class="hljs-comment"># training plus snorkel labelled data</span>
print(<span class="hljs-string">"showing hist of additional data"</span>)
<span class="hljs-comment"># now balance the labels</span>
pos = p2[p2.sentiment == <span class="hljs-number">1</span>]
neg = p2[p2.sentiment == <span class="hljs-number">0</span>]
recs = <span class="hljs-built_in">min</span>(pos.shape[<span class="hljs-number">0</span>], neg.shape[<span class="hljs-number">0</span>])
pos = pos.sample(n=recs, random_state=<span class="hljs-number">42</span>)
neg = neg.sample(n=recs, random_state=<span class="hljs-number">42</span>)
p3 = pd.concat((pos,neg))
p3.sentiment.hist()
</code></pre>
    <p class="normal">The original training dataset was balanced across positive and negative labels. However, there is an imbalance in the data labeled using Snorkel. We balance the dataset and ignore the excess rows with negative labels. Note that the 2,000 training records used in the baseline model also need to be added, resulting in a total of 33,914 training records. As mentioned before, it really shines when the amount of data is 10x to 50x the original dataset. Here, we achieve a ratio closer to 17x, or 18x if the 2,000 training records are also included.</p>
    <figure class="mediaobject"><img src="image/B16252_08_08.png" alt="A picture containing screen, orange, drawing  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 8.8: Distribution of records after using Snorkel and weak supervision</p>
    <p class="normal">As shown in <em class="italic">Figure 8.8</em> above, the <a id="_idIndexMarker666"/>records in blue are dropped to balance the dataset. Next, the data needs to be cleansed and vectorized using the subword vocabulary:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># remove markup</span>
cleaned_unsup_reviews = p3.review.apply(
                             <span class="hljs-keyword">lambda</span> x: BeautifulSoup(x).text)
snorkel_reviews = pd.concat((cleaned_reviews, cleaned_unsup_reviews))
snorkel_labels = pd.concat((train_small.sentiment, p3.sentiment))
</code></pre>
    <p class="normal">Finally, we convert the pandas DataFrames into TensorFlow data sets and vectorize and pad them:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># convert pandas DF in to tf.Dataset</span>
snorkel_train = tf.data.Dataset.from_tensor_slices((
                                   snorkel_reviews.values,
                                   snorkel_labels.values))
encoded_snorkel_train = snorkel_train.<span class="hljs-built_in">map</span>(encode_tf_fn,
                 num_parallel_calls=tf.data.experimental.AUTOTUNE)
</code></pre>
    <p class="normal">We are ready to try <a id="_idIndexMarker667"/>training our BiLSTM model to see if the performance improves on this task.</p>
    <h2 id="_idParaDest-156" class="title">Training BiLSTM on weakly supervised data from Snorkel</h2>
    <p class="normal">To ensure we are <a id="_idIndexMarker668"/>comparing apples to apples, we use the same BiLSTM as the baseline model. We instantiate a <a id="_idIndexMarker669"/>model with 64-dimensional embeddings, 64 RNN units, and a batch size of 100. The model uses the binary cross-entropy loss and the Adam optimizer. Accuracy, precision, and recall are tracked as the model is trained. An important step is to shuffle the datasets every epoch to help the model keep errors to a minimum. </p>
    <p class="normal">This is an important concept. Deep models work on the assumption that the loss is a convex surface, and the gradient is descending to the bottom of this surface. The surface has many local minima or saddle points in reality. If the model gets stuck in local minima during a mini-batch, it will be hard for the model to come out of it as across epochs, it receives the same data points again and again. Shuffling the data changes the data set and the order in which the model receives it. This enables the model to learn better by getting out of these local minima faster. The code for this section is in the <code class="Code-I -Text--PACKT-">imdb-with-snorkel-labels.ipynb</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code">shuffle_size = snorkel_reviews.shape[<span class="hljs-number">0</span>] // BATCH_SIZE * BATCH_SIZE
encoded_snorkel_batched = encoded_snorkel_train.shuffle( 
                                  buffer_size=shuffle_size,
                                  seed=<span class="hljs-number">42</span>).batch(BATCH_SIZE,
                                  drop_remainder=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Note that we cache all the records that will be part of the batch so that we can get perfect buffering. This comes at the cost of slightly slower training and higher memory use. Also, since our batch size is 100 and the dataset has 35,914 records, we drop the remainder of the records. We train the model for 20 epochs, a little more than the baseline model. The baseline model was overfitting at 15 epochs. So, it was not useful to train it longer. This model has a lot more data to train on. Consequently, it needs more epochs to learn:</p>
    <pre class="programlisting code"><code class="hljs-code">bilstm2.fit(encoded_snorkel_batched, epochs=<span class="hljs-number">20</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Train for 359 steps
Epoch 1/20
359/359 [==============================] - 92s 257ms/step - loss: 0.4399 - accuracy: 0.7860 - Precision: 0.7900 - Recall: 0.7793
…
Epoch 20/20
359/359 [==============================] - 82s 227ms/step - loss: 0.0339 - accuracy: 0.9886 - Precision: 0.9879 - Recall: 0.9893
</code></pre>
    <p class="normal">The model achieves an accuracy of 98.9%. The precision and recall numbers are quite close to each other. Evaluating the baseline model on the test data gave an accuracy score of 76.23%, which clearly proved that it was overfitting to the training data. Upon evaluating the model trained with weakly supervised labeling, the following results are obtained:</p>
    <pre class="programlisting code"><code class="hljs-code">bilstm2.evaluate(encoded_test.batch(BATCH_SIZE))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">250/250 [==============================] - 35s 139ms/step - loss: 1.9134 - accuracy: 0.7658 - precision: 0.7812 - recall: 0.7386
</code></pre>
    <p class="normal">This model trained on weakly supervised noisy labels achieves 76.6% accuracy, which is 0.7%% higher than baseline mode. Also note that the precision went from 74.5% to 78.1% but recall decreased. In this <a id="_idIndexMarker670"/>toy setting, we kept a lot of the variables constant, such as model type, dropout ratio, etc. In a realistic <a id="_idIndexMarker671"/>setting, we can drive the accuracy even higher by optimizing the model architecture and hyperparameter tuning. There are other options to try. Recall that we instruct Snorkel to abstain from labeling if it is unsure. </p>
    <p class="normal">By changing that to a majority vote or some other policy, the amount of training data could be increased even further. You could also try and train on unbalanced datasets and see the impact. The focus here was on showing the value of weak supervision for massively increasing the amount of training data rather than building the best model. However, you should be able to take these lessons and apply them to your projects.</p>
    <p class="normal">It is important to take a moment and think about the causes of this result. There are a few important deep learning lessons hidden in this story. First, more labeled data is always good, given a model of sufficient complexity. There is a correlation between the amount of data and model capacity. Models with higher capacities can handle more complex relationships in the data. They also need much larger datasets to learn the complexities. However, if the model is kept a constant and with sufficient capacity, the quantity of labeled data makes a huge difference, as evidenced here. There are some limits to how much of an improvement we can achieve by increasing labeled data scale. In a paper titled <em class="italic">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</em> by Chen Sun et al., published at ICCV 2017, the authors examine the role of data in the computer vision domain. They report that the performance of models increases logarithmically with an increase in training <a id="_idIndexMarker672"/>data. The second result they report is that learning representations through pretraining helps downstream <a id="_idIndexMarker673"/>tasks quite a bit. Techniques in this chapter can be applied to generate more data for the fine-tuning step, which will significantly boost the performance of the fine-tuned model.</p>
    <p class="normal">The second lesson is one about the basics of machine learning – shuffling the training data set has a disproportionate impact on the performance of the model. In the book, we have not always done this in order to manage training times. For training production models, it is important to focus on basics such as shuffling data sets before each epoch.</p>
    <p class="normal">Let's review everything we learned in this chapter.</p>
    <h1 id="_idParaDest-157" class="title">Summary</h1>
    <p class="normal">It is apparent that deep models perform very well when they have a lot of data. BERT and GPT models have shown the value of pre-training on massive amounts of data. It is still very hard to get good-quality labeled data for use in pretraining or fine-tuning. We used the concepts of weak supervision combined with generative models to cheaply label data. With relatively small amounts of effort, we were able to multiply the amount of training data by 18x. Even though the additional training data was noisy, the BiLSTM model was able to learn effectively and beat the baseline model by 0.6%.</p>
    <p class="normal">Representation learning or pre-training leads to transfer learning and fine-tuning models performing well on their downstream tasks. However, in many domains like medicine, the amount of labeled data may be small or quite expensive to acquire. Using the techniques learned in this chapter, the amount of training data can be expanded rapidly with little effort. Building a state-of-the-art- beating model helped recall some basic lessons in deep learning, such as how larger data boosts performance quite a bit, and that larger models are not always better.</p>
    <p class="normal">Now, we turn our focus to conversational AI. Building a conversational AI system is a very challenging task with many layers. The material covered so far in the book can help in building various parts of chatbots. The next chapter goes over the key parts of conversational AI or chatbot systems and outlines effective ways to build them.</p>
  </div>
</body></html>