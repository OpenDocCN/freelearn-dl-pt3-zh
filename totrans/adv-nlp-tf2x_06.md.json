["```\ndef load_data():\n    print(\"Loading the dataset\")\n    (ds_train, ds_val, ds_test), ds_info = tfds.load(\n        'gigaword',\n        split=['train', 'validation', 'test'],\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n    )\n    return ds_train, ds_val, ds_test \n```", "```\nif __name__ == \"__main__\":\n    setupGPU()  # OPTIONAL – only if using GPU\n    ds_train, _, _ = load_data() \n```", "```\nDownloading and preparing dataset gigaword/1.2.0 (download: 551.61 MiB, generated: Unknown size, total: 551.61 MiB) to /xxx/tensorflow_datasets/gigaword/1.2.0...\n/xxx/anaconda3/envs/tf21g/lib/python3.7/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning, \n  InsecureRequestWarning,\nShuffling and writing examples to /xxx/tensorflow_datasets/gigaword/1.2.0.incomplete1FP5M4/gigaword-train.tfrecord\n100%\n<snip/>\n100%\n1950/1951 [00:00<00:00, 45393.40 examples/s]\nDataset gigaword downloaded and prepared to /xxx/tensorflow_datasets/gigaword/1.2.0\\. Subsequent calls will reuse this data. \n```", "```\ndef get_tokenizer(data, file=\"gigaword32k.enc\"):\n    if os.path.exists(file+.subwords):\n        # data has already been tokenized - just load and return\n        tokenizer = \\\ntfds.features.text.SubwordTextEncoder.load_from_file(file)\n    else:\n        # This takes a while\n        tokenizer = \\\ntfds.features.text.SubwordTextEncoder.build_from_corpus(\n        ((art.numpy() + b\" \" + smm.numpy()) for art, smm in data),\n        target_vocab_size=2**15\n        )  # End tokenizer construction\n        tokenizer.save_to_file(file)  # save for future iterations\n\n   print(\"Tokenizer ready. Total vocabulary size: \", tokenizer.vocab_size)\n   return tokenizer \n```", "```\nif __name__ == \"__main__\":\n    setupGPU()  # OPTIONAL - only if using GPU\n    ds_train, _, _ = load_data()\n    tokenizer = get_tokenizer(ds_train)\n    # Test tokenizer\n    txt = \"Coronavirus spread surprised everyone\"\n    print(txt, \" => \", tokenizer.encode(txt.lower()))\n    for ts in tokenizer.encode(txt.lower()):\n        print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\n    # add start and end of sentence tokens\n    start = tokenizer.vocab_size + 1 \n    end = tokenizer.vocab_size\n    vocab_size = end + 2 \n```", "```\ndef encode(article, summary, start=start, end=end, \n           tokenizer=tokenizer, art_max_len=128, \n           smry_max_len=50):\n    # vectorize article\n    tokens = tokenizer.encode(article.numpy())\n    if len(tokens) > art_max_len:\n        tokens = tokens[:art_max_len]\n    art_enc = sequence.pad_sequences([tokens], padding='post',\n                                 maxlen=art_max_len).squeeze()\n    # vectorize summary\n    tokens = [start] + tokenizer.encode(summary.numpy())\n    if len(tokens) > smry_max_len:\n        tokens = tokens[:smry_max_len]\n    else:\n        tokens = tokens + [end]\n\n    smry_enc = sequence.pad_sequences([tokens], padding='post',\n                                 maxlen=smry_max_len).squeeze()\n    return art_enc, smry_enc \n```", "```\ndef tf_encode(article, summary):\n    art_enc, smry_enc = tf.py_function(encode, [article, summary],\n                                     [tf.int64, tf.int64])\n    art_enc.set_shape([None])\n    smry_enc.set_shape([None])\n    return art_enc, smry_enc \n```", "```\nBUFFER_SIZE = 1500000  # dataset is 3.8M samples, using less\nBATCH_SIZE = 64  # try bigger batch for faster training\ntrain = ds_train.take(BUFFER_SIZE)  # 1.5M samples\nprint(\"Dataset sample taken\")\ntrain_dataset = train.map(s2s.tf_encode)\n# train_dataset = train_dataset.shuffle(BUFFER_SIZE) – optional \ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\nprint(\"Dataset batching done\") \n```", "```\nembedding_dim = 128\nunits = 256  # from pointer generator paper \n```", "```\nclass Embedding(object):\n    embedding = None  # singleton\n    @classmethod\n    def get_embedding(self, vocab_size, embedding_dim):\n        if self.embedding is None:\n            self.embedding = tf.keras.layers.Embedding(vocab_size,\n                                                      embedding_dim,\n                                                      mask_zero=True)\n        return self.embedding \n```", "```\n# Encoder\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.enc_units = enc_units\n        # Shared embedding layer\n        self.embedding = Embedding.get_embedding(vocab_size, \n                                                 embedding_dim) \n```", "```\n self.bigru = Bidirectional(GRU(self.enc_units,\n                          return_sequences=True,\n                          return_state=True,\n                          recurrent_initializer='glorot_uniform'),\n                          merge_mode='concat'\n                        )\n        self.relu = Dense(self.enc_units, activation='relu') \n```", "```\n def call(self, x, hidden):\n        x = self.embedding(x)  # We are using a mask\n        output, forward_state, backward_state = self.bigru(x, initial_state = hidden)\n        # now, concat the hidden states through the dense ReLU layer\n        hidden_states = tf.concat([forward_state, backward_state], \n                                  axis=1)\n        output_state = self.relu(hidden_states)\n\n        return output, output_state \n```", "```\ndef initialize_hidden_state(self):\n        return [tf.zeros((self.batch_size, self.enc_units)) \n                 for i in range(2)] \n```", "```\nclass BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1) \n```", "```\ndef call(self, decoder_hidden, enc_output):\n    # decoder hidden state shape == (64, 256) \n    # [batch size, decoder units]\n    # encoder output shape == (64, 128, 256) \n    # which is [batch size, max sequence length, encoder units]\n    query = decoder_hidden # to map our code to generic \n    # form of attention\n    values = enc_output\n\n    # query_with_time_axis shape == (batch_size, 1, hidden size)\n    # we are doing this to broadcast addition along the time axis\n    query_with_time_axis = tf.expand_dims(query, 1)\n    # score shape == (batch_size, max_length, 1)\n    score = self.V(tf.nn.tanh(\n        self.W1(query_with_time_axis) + self.W2(values)))\n    # attention_weights shape == (batch_size, max_length, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n    return context_vector, attention_weights \n```", "```\nclass Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        # Unique embedding layer\n        self.embedding = tf.keras.layers.Embedding(vocab_size, \n                                                   embedding_dim,\n                                                   mask_zero=True)\n        # Shared embedding layer\n        # self.embedding = Embedding.get_embedding(vocab_size, \n        # embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.dec_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer=\\\n                                       'glorot_uniform')\n        self.fc1 = tf.keras.layers.Dense(vocab_size, \n                               activation='softmax')\n        # used for attention\n        self.attention = BahdanauAttention(self.dec_units) \n```", "```\ndef call(self, x, hidden, enc_output):\n    # enc_output shape == (batch_size, max_length, hidden_size)\n    context_vector, attention_weights = self.attention(hidden,\n                                                       enc_output)\n    # x shape after passing through embedding\n    # == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n    output = tf.reshape(output, (-1, output.shape[2]))\n\n    x = self.fc1(output)\n\n    return x, state, attention_weights \n```", "```\nModel: \"encoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nembedding (Embedding)        multiple                  4211072\n_________________________________________________________________\nbidirectional (Bidirectional multiple                  592896\n_________________________________________________________________\ndense (Dense)                multiple                  131328\n=================================================================\nTotal params: 4,935,296\nTrainable params: 4,935,296\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nembedding_1 (Embedding)      multiple                  4211072\n_________________________________________________________________\ngru_1 (GRU)                  multiple                  689664\n_________________________________________________________________\ndense_1 (Dense)              multiple                  8455043\n_________________________________________________________________\nbahdanau_attention (Bahdanau multiple                  197377\n=================================================================\nTotal params: 13,553,156\nTrainable params: 13,553,156\nNon-trainable params: 0 \n```", "```\n@tf.function\ndef train_step(inp, targ, enc_hidden, max_gradient_norm=5):\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        # print(\"inside gradient tape\")\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n\n        dec_hidden = enc_hidden\n        dec_input = tf.expand_dims([start] * BATCH_SIZE, 1)\n\n        # Teacher forcing - feeding the target as the next input\n        for t in range(1, targ.shape[1]):\n            # passing enc_output to the decoder\n            predictions, dec_hidden, _ = decoder(dec_input,   \n                                           dec_hidden, enc_output)\n\n            loss += s2s.loss_function(targ[:, t], predictions)\n            # using teacher forcing\n            dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = (loss / int(targ.shape[1]))\n\n    variables = encoder.trainable_variables + \\\ndecoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    # Gradient clipping\n    clipped_gradients, _ = tf.clip_by_global_norm(\n                                    gradients, max_gradient_norm)\n    optimizer.apply_gradients(zip(clipped_gradients, variables))\n    return batch_loss \n```", "```\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n                    from_logits=False, reduction='none')\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_mean(loss_) \n```", "```\nsteps_per_epoch = BUFFER_SIZE // BATCH_SIZE\nembedding_dim = 128\nunits = 256  # from pointer generator paper\nEPOCHS = 16\n\nencoder = s2s.Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\ndecoder = s2s.Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n# Learning rate scheduler\nlr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n                   0.001,\n                   decay_steps=steps_per_epoch*(EPOCHS/2),\n                   decay_rate=2,\n                   staircase=False)\noptimizer = tf.keras.optimizers.Adam(lr_schedule) \n```", "```\nif args.checkpoint is None:\n    dt = datetime.datetime.today().strftime(\"%Y-%b-%d-%H-%M-%S\")\n    checkpoint_dir = './training_checkpoints-' + dt\nelse:\n    checkpoint_dir = args.checkpoint\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)\nif args.checkpoint is not None:\n    # restore last model\n    print(\"Checkpoint being restored: \",\n tf.train.latest_checkpoint(checkpoint_dir))\n    chkpt_status = checkpoint.restore(\ntf.train.latest_checkpoint(checkpoint_dir))\n    # to check loading worked\n chkpt_status.assert_existing_objects_matched()  \nelse:\n    print(\"Starting new training run from scratch\")\nprint(\"New checkpoints will be stored in: \", checkpoint_dir) \n```", "```\nprint(\"Starting Training. Total number of steps / epoch: \", steps_per_epoch)\n    for epoch in range(EPOCHS):\n        start_tm = time.time()\n        enc_hidden = encoder.initialize_hidden_state()\n        total_loss = 0\n        for (batch, (art, smry)) in enumerate(train_dataset.take(steps_per_epoch)):\n            batch_loss = train_step(art, smry, enc_hidden)\n            total_loss += batch_loss\n            if batch % 100 == 0:\n                ts = datetime.datetime.now().\\\nstrftime(\"%d-%b-%Y (%H:%M:%S)\")\n                print('[{}] Epoch {} Batch {} Loss {:.6f}'.\\\n                        format(ts,epoch + 1, batch,\n                        batch_loss.numpy())) # end print\n        # saving (checkpoint) the model every 2 epochs\n        if (epoch + 1) % 2 == 0:\n            checkpoint.save(file_prefix = checkpoint_prefix)\n        print('Epoch {} Loss {:.6f}'.\\\n                format(epoch + 1, total_loss / steps_per_epoch))\n\n        print('Time taken for 1 epoch {} sec\\n'.\\\n                  format(time.time() - start_tm)) \n```", "```\n$ python s2s-training.py \n```", "```\nLoading the dataset\nTokenizer ready. Total vocabulary size:  32897\nCoronavirus spread surprised everyone  =>  [16166, 2342, 1980, 7546, 21092]\n16166 ----> corona\n2342 ----> virus\n1980 ----> spread\n7546 ----> surprised\n21092 ----> everyone\nDataset sample taken\nDataset batching done\nStarting new training run from scratch\nNew checkpoints will be stored in:  ./training_checkpoints-2021-Jan-04-04-33-42\nStarting Training. Total number of steps / epoch:  31\n[04-Jan-2021 (04:34:45)] Epoch 1 Batch 0 Loss 2.063991\n...\nEpoch 1 Loss 1.921176\nTime taken for 1 epoch 83.241370677948 sec\n[04-Jan-2021 (04:35:06)] Epoch 2 Batch 0 Loss 1.487815\nEpoch 2 Loss 1.496654\nTime taken for 1 epoch 21.058568954467773 sec \n```", "```\nBUFFER_SIZE = 2000  # 3500000 takes 7hr/epoch \n```", "```\n$ python s2s-trainingo.py --checkpoint training_checkpoints-2021-Jan-04-04-33-42 \n```", "```\nBATCH_SIZE = 1  # for inference\nembedding_dim = 128\nunits = 256  # from pointer generator paper\nvocab_size = end + 2\n# Create encoder and decoder objects\nencoder = s2s.Encoder(vocab_size, embedding_dim, units, \n                        BATCH_SIZE)\ndecoder = s2s.Decoder(vocab_size, embedding_dim, units, \n                        BATCH_SIZE)\noptimizer = tf.keras.optimizers.Adam() \n```", "```\n# Hydrate the model from saved checkpoint\ncheckpoint_dir = 'training_checkpoints-2021-Jan-25-09-26-31'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder) \n```", "```\n# The last training checkpoint\ntf.train.latest_checkpoint(checkpoint_dir) \n```", "```\n'training_checkpoints-2021-Jan-25-09-26-31/ckpt-11' \n```", "```\nchkpt_status = checkpoint.restore(\n                        tf.train.latest_checkpoint(checkpoint_dir))\nchkpt_status.assert_existing_objects_matched() \n```", "```\n<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f603ae03c90> \n```", "```\nfor layer in decoder.layers:\n    print(layer.name) \n```", "```\nembedding_1\ngru_1\nfc1 \n```", "```\ntf.train.list_variables(\n       tf.train.latest_checkpoint('./<chkpt_dir>/')\n) \n```", "```\nself.fc1 = tf.keras.layers.Dense(\n                vocab_size, activation='softmax', \n                name='fc1') \n```", "```\n# function for plotting the attention weights\ndef plot_attention(attention, article, summary):\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1, 1, 1)\n    # https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html \n    # for scales\n    ax.matshow(attention, cmap='cividis')\n    fontdict = {'fontsize': 14}\n    ax.set_xticklabels([''] + article, fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + summary, fontdict=fontdict)\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    plt.show() \n```", "```\nart_max_len = 128\nsmry_max_len = 50\ndef greedy_search(article):\n    # To store attention plots of the output\n    attention_plot = np.zeros((smry_max_len, art_max_len))\n    tokens = tokenizer.encode(article) \n    if len(tokens) > art_max_len:\n        tokens = tokens[:art_max_len]\n    inputs = sequence.pad_sequences([tokens], padding='post',\n                                 maxlen=art_max_len).squeeze()\n    inputs = tf.expand_dims(tf.convert_to_tensor(inputs), 0)\n\n    # output summary tokens will be stored in this\n    summary = \"\n    hidden = [tf.zeros((1, units)) for i in range(2)] #BiRNN\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([start], 0)\n    for t in range(smry_max_len):\n        predictions, dec_hidden, attention_weights = \\\ndecoder(dec_input, dec_hidden, enc_out)\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        if predicted_id == end:\n            return summary, article, attention_plot\n        # storing the attention weights to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        summary += tokenizer.decode([predicted_id])\n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n    return summary, article, attention_plot \n```", "```\n# Summarize\ndef summarize(article, algo='greedy'):\n    if algo == 'greedy':\n        summary, article, attention_plot = greedy_search(article)\n    else:\n        print(\"Algorithm {} not implemented\".format(algo))\n        return\n\n    print('Input: %s' % (article))\n    print('** Predicted Summary: {}'.format(summary))\n    attention_plot = \\\nattention_plot[:len(summary.split(' ')), :len(article.split(' '))]\n    plot_attention(attention_plot, article.split(' '), \n                     summary.split(' ')) \n```", "```\n# Test Summarization\ntxt = \"president georgi parvanov summoned france 's ambassador on wednesday in a show of displeasure over comments from french president jacques chirac chiding east european nations for their support of washington on the issue of iraq .\"\nsummarize(txt.lower()) \n```", "```\nInput: president georgi parvanov summoned france's ambassador on wednesday in a show of displeasure over comments from french president jacques chirac chiding east european nations for their support of washington on the issue of iraq .\n** Predicted Summary: **bulgarian president summons french ambassador over remarks on iraq** \n```", "```\n# initial beam with (tokens, last hidden state, attn, score)\nstart_pt = [([start], dec_hidden, attention_plot, 0.0)]  # initial beam \nfor t in range(smry_max_len):\n    options = list() # empty list to store candidates\n    for row in start_pt:\n        # handle beams emitting end signal\n        allend = True\n        dec_input = row[0][-1]\n        if dec_input != end_tk:\n              # last token\n            dec_input = tf.expand_dims([dec_input], 0)  \n            dec_hidden = row[1]  # second item is hidden states\n            attn_plt = np.zeros((smry_max_len, art_max_len)) +\\\n                       row[2] # new attn vector\n\n            predictions, dec_hidden, attention_weights = \\\ndecoder(dec_input, dec_hidden, enc_out)\n            # storing the attention weights to plot later on\n            attention_weights = tf.reshape(attention_weights, (-1, ))\n            attn_plt[t] = attention_weights.numpy() \n\n            # take top-K in this beam\n            values, indices = tf.math.top_k(predictions[0],\n                                               k=beam_width)\n            for tokid, scre in zip(indices, values):\n                score = row[3] - np.log(scre)\n                options.append((row[0]+[tokid], dec_hidden, \n                                   attn_plt, score))\n            allend=False\n        else:\n            options.append(row)  # add ended beams back in\n\n    if allend:\n        break # end for loop as all sequences have ended\n    start_pt = sorted(options, key=lambda tup:tup[3])[:beam_width] \n```", "```\n if verbose:  # to control output\n        # print all the final summaries\n        for idx, row in enumerate(start_pt):\n            tokens = [x for x in row[0] if x < end_tk]\n            print(\"Summary {} with {:5f}: {}\".format(idx, row[3], \n                                        tokenizer.decode(tokens))) \n```", "```\n # return final sequence\n    summary = tokenizer.decode([x for x in start_pt[0][0] if x < end_tk])\n    attention_plot = start_pt[0][2] # third item in tuple\n    return summary, article, attention_plot \n```", "```\n# Summarize\ndef summarize(article, algo='greedy', beam_width=3, verbose=True):\n    if algo == 'greedy':\n        summary, article, attention_plot = greedy_search(article)\n    elif algo=='beam':\n        summary, article, attention_plot = beam_search(article, \n                                                beam_width=beam_width,\n                                                verbose=verbose)\n    else:\n        print(\"Algorithm {} not implemented\".format(algo))\n        return\n\n    print('Input: %s' % (article))\n    print('** Predicted Summary: {}'.format(summary))\n    attention_plot = attention_plot[:len(summary.split(' ')), \n                                    :len(article.split(' '))]\n    plot_attention(attention_plot, article.split(' '), \n                   summary.split(' ')) \n```", "```\ndef length_wu(step, score, alpha=0.):\n    # NMT length re-ranking score from\n    # \"Google's Neural Machine Translation System\" paper by Wu et al\n    modifier = (((5 + step) ** alpha) /\n                ((5 + 1) ** alpha))\n    return (score / modifier) \n```", "```\n# Beam search implementation with normalization\ndef beam_search_norm(article, beam_width=3, \n                         art_max_len=128, \n                         smry_max_len=50,\n                         end_tk=end,\n                         alpha=0.,\n                         verbose=True) \n```", "```\n for tokid, scre in zip(indices, values):\n            score = row[3] - np.log(scre) \n            score = length_wu(t, score, alpha) \n```", "```\nfrom rouge_score import rouge_scorer as rs\nscorer = rs.RougeScorer(['rougeL'], use_stemmer=True) \n```", "```\n# total eval size: 189651\narticles = 1000\nf1 = 0.\nprec = 0.\nrec = 0.\nbeam_width = 1\nfor art, smm in ds_val.take(articles):\n    summ = summarize_quietly(str(art.numpy()), algo='beam-norm', \n                             beam_width=1, verbose=False)\n    score = scorer.score(str(smm.numpy()), summ)\n    f1 += score['rougeL'].fmeasure / articles\n    prec += score['rougeL'].precision / articles\n    rec += score['rougeL'].recall / articles\n    # see if a sample needs to be printed\n    if random.choices((True, False), [1, 99])[0] is True: \n    # 1% samples printed out\n        print(\"Article: \", art.numpy())\n        print(\"Ground Truth: \", smm.numpy())\n        print(\"Greedy Summary: \", summarize_quietly(str(art.numpy()),\n              algo='beam-norm', \n              beam_width=1, verbose=False))\n        print(\"Beam Search Summary :\", summ, \"\\n\")\nprint(\"Precision: {:.6f}, Recall: {:.6f}, F1-Score: {:.6f}\".format(prec, rec, f1)) \n```", "```\nPrecision: 0.344725, Recall: 0.249029, F1-Score: 0.266480 \n```", "```\nPrecision: 0.382001, Recall: 0.226766, F1-Score: 0.260703 \n```", "```\nPrecision: 0.400730, Recall: 0.219472, F1-Score: 0.258531 \n```", "```\nPrecision: 0.356155, Recall: 0.253459, F1-Score: 0.271813 \n```", "```\nPrecision: 0.356993, Recall: 0.252384, F1-Score: 0.273171 \n```"]