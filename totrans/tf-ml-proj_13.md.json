["```\n def load_data():\n \"\"\"\n Loading Data\n \"\"\"\n input_file = os.path.join(TEXT_SAVE_DIR)\n with open(input_file, \"r\") as f:\n data = f.read()\n\nreturn data\n```", "```\n def define_tokens():\n \"\"\"\n Generate a dict to turn punctuation into a token. Note that Sym before each text denotes Symbol\n :return: Tokenize dictionary where the key is the punctuation and the value is the token\n \"\"\"\n dict = {'.':'_Sym_Period_',\n ',':'_Sym_Comma_',\n '\"':'_Sym_Quote_',\n ';':'_Sym_Semicolon_',\n '!':'_Sym_Exclamation_',\n '?':'_Sym_Question_',\n '(':'_Sym_Left_Parentheses_',\n ')':'_Sym_Right_Parentheses_',\n '--':'_Sym_Dash_',\n '\\n':'_Sym_Return_',\n }\n return dict\n```", "```\n def create_map(input_text):\n \"\"\"\n Map words in vocab to int and vice versa for easy lookup\n :param input_text: TV Script data split into words\n :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n \"\"\"\n vocab = set(input_text)\n vocab_to_int = {c: i for i, c in enumerate(vocab)}\n int_to_vocab = dict(enumerate(vocab))\n return vocab_to_int, int_to_vocab\n```", "```\ndef preprocess_and_save_data():\n \"\"\"\n Preprocessing the TV Scripts Dataset\n \"\"\"\n generate_text_data_from_csv()\n text = load_data()\n text= text[14:] # Ignoring the STARTraw_text part of the dataset\n token_dict = define_tokens()\n for key, token in token_dict.items():\n text = text.replace(key, ' {} '.format(token))\n\ntext = text.lower()\n text = text.split()\n\nvocab_to_int, int_to_vocab = create_map(text)\n int_text = [vocab_to_int[word] for word in text]\n pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('processed_text.p', 'wb'))\n```", "```\n with tf.variable_scope('Input'):\n self.X = tf.placeholder(tf.int32, [None, None], name='input')\n self.Y = tf.placeholder(tf.int32, [None, None], name='target')\n self.input_shape = tf.shape(self.X)\n```", "```\n lstm = tf.contrib.rnn.BasicLSTMCell(RNN_SIZE)\n cell = tf.contrib.rnn.MultiRNNCell([lstm] * 2) # Defining two LSTM layers for this case\n self.initial_state = cell.zero_state(self.input_shape[0], tf.float32)\n self.initial_state = tf.identity(self.initial_state, name=\"initial_state\")\n```", "```\nembedding = tf.Variable(tf.random_uniform((self.vocab_size, RNN_SIZE), -1, 1))\nembed = tf.nn.embedding_lookup(embedding, self.X)\n```", "```\noutputs, self.final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=None, dtype=tf.float32)\nself.final_state = tf.identity(self.final_state, name='final_state')\n```", "```\nself.final_state = tf.identity(self.final_state, name='final_state')\nself.predictions = tf.contrib.layers.fully_connected(outputs, self.vocab_size, activation_fn=None)\n# Probabilities for generating words\nprobs = tf.nn.softmax(self.predictions, name='probs')\n```", "```\n def define_loss(self):\n # Defining the sequence loss\n with tf.variable_scope('Sequence_Loss'):\n self.loss = seq2seq.sequence_loss(self.predictions, self.Y,\n tf.ones([self.input_shape[0], self.input_shape[1]]))\n```", "```\n def define_optimizer(self):\n with tf.variable_scope(\"Optimizer\"):\n optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n # Gradient Clipping\n gradients = optimizer.compute_gradients(self.loss)\n capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n self.train_op = optimizer.apply_gradients(capped_gradients)\n```", "```\n def generate_batch_data(int_text):\n \"\"\"\n Generate batch data of x (inputs) and y (targets)\n :param int_text: Text with the words replaced by their ids\n :return: Batches as a Numpy array\n \"\"\"\n num_batches = len(int_text) // (BATCH_SIZE * SEQ_LENGTH)\n\nx = np.array(int_text[:num_batches * (BATCH_SIZE * SEQ_LENGTH)])\ny = np.array(int_text[1:num_batches * (BATCH_SIZE * SEQ_LENGTH) + 1])\n\nx_batches = np.split(x.reshape(BATCH_SIZE, -1), num_batches, 1) y_batches = np.split(y.reshape(BATCH_SIZE, -1), num_batches, 1)\n batches = np.array(list(zip(x_batches, y_batches)))\n return batches\n```", "```\ndef train(model,int_text):\n# Creating the checkpoint directory\n if not os.path.exists(CHECKPOINT_PATH_DIR):\n os.makedirs(CHECKPOINT_PATH_DIR)\n\nbatches = generate_batch_data(int_text)\nwith tf.Session() as sess:\n if RESTORE_TRAINING:\n saver = tf.train.Saver()\n ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)\n saver.restore(sess, ckpt.model_checkpoint_path)\n print('Model Loaded')\n start_epoch = int(str(ckpt.model_checkpoint_path).split('-')[-1])\n else:\n start_epoch = 0\n tf.global_variables_initializer().run()\n print('All variables initialized')\n\nfor epoch in range(start_epoch, NUM_EPOCHS):\n saver = tf.train.Saver()\n state = sess.run(model.initial_state, {model.X: batches[0][0]})\n\nfor batch, (x, y) in enumerate(batches):\n feed = {\n model.X: x,\n model.Y: y,\n model.initial_state: state}\n train_loss, state, _ = sess.run([model.loss, model.final_state, model.train_op], feed)\n\nif (epoch * len(batches) + batch) % 200 == 0:\n print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f}'.format(\n epoch,\n batch,\n len(batches),\n train_loss))\n # Save Checkpoint for restoring if required\n saver.save(sess, CHECKPOINT_PATH_DIR + '/model.tfmodel', global_step=epoch + 1)\n\n# Save Model\n saver.save(sess, SAVE_DIR)\n print('Model Trained and Saved')\n save_params((SEQ_LENGTH, SAVE_DIR))\n\n```", "```\n def extract_tensors(tf_graph):\n \"\"\"\n Get input, initial state, final state, and probabilities tensor from the graph\n :param loaded_graph: TensorFlow graph loaded from file\n :return: Tuple (tensor_input,tensor_initial_state,tensor_final_state, tensor_probs)\n \"\"\"\n tensor_input = tf_graph.get_tensor_by_name(\"Input/input:0\")\n tensor_initial_state = tf_graph.get_tensor_by_name(\"Network/initial_state:0\")\n tensor_final_state = tf_graph.get_tensor_by_name(\"Network/final_state:0\")\n tensor_probs = tf_graph.get_tensor_by_name(\"Network/probs:0\")\n return tensor_input, tensor_initial_state, tensor_final_state, tensor_probs\n```", "```\n# Sentences generation setup\nsentences = [first_word]\nprevious_state = sess.run(initial_state, {input_text: np.array([[1]])})\n```", "```\n def select_next_word(probs, int_to_vocab):\n \"\"\"\n Select the next work for the generated text\n :param probs: list of probabilities of all the words in vocab which can be selected as next word\n :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n :return: predicted next word\n \"\"\"\n index = np.argmax(probs)\n word = int_to_vocab[index]\n return word\n```", "```\n for i in range(script_length):\n\n # Dynamic Input\n dynamic_input = [[vocab_to_int[word] for word in sentences[-seq_length:]]]\n dynamic_seq_length = len(dynamic_input[0])\n\n# Get Prediction\n probabilities, previous_state = sess.run([probs, final_state], {input_text: dynamic_input, initial_state: previous_state})\n probabilities= np.squeeze(probabilities)\n\npred_word = select_next_word(probabilities[dynamic_seq_length - 1], int_to_vocab)\n sentences.append(pred_word)\n```", "```\n# Scraping out tokens from the words\nbook_script = ' '.join(sentences)\nfor key, token in token_dict.items():\n    book_script = book_script.replace(' ' + token.lower(), key)\nbook_script = book_script.replace('\\n ', '\\n')\nbook_script = book_script.replace('( ', '(')\n```", "```\n postgresql comparatively).\none transaction is important, you can be used\n\ncreate index is seen a transaction will be provided this index.\nan index scan is a lot of a index\nthe index is time.\nto be an index.\nyou can see is to make expensive,\nthe following variable is an index\n\nthe table will index have to a transaction isolation level\nthe transaction isolation level will use a transaction will use the table of the following index creation.\nthe index is marked.\nthe following number is one of the following one lock is not a good source of a transaction will use the following strategies\nin this is not, it will be a table\nin postgresql.\nthe postgresql cost errors is not possible to use a transaction.\npostgresql 10\\. 0\\. 0\\. you can see that the data is not free into more than a transaction ids, the same time. the first scan is an example\nthe same number.\none index is not that the same time is needed in the following strategies\n\nin the same will copy block numbers.\nthe same data is a table if you can not be a certain way, you can see, you will be able to create statistics.\npostgresql will\n```"]