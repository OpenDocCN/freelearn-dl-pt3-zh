- en: Evaluating Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in the previous chapters, several AI solutions are available
    to achieve certain cybersecurity goals, so it is important to learn how to evaluate
    the effectiveness of various alternative solutions, using appropriate analysis
    metrics. At the same time, it is important to prevent phenomena such as overfitting,
    which can compromise the reliability of forecasts when switching from training
    data to test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering best practices in dealing with raw data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate a detector's performance using the ROC curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to appropriately split sample data into training and test sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to manage algorithms' overfitting and bias–variance trade-offs with cross
    validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's begin our discussion of we need feature engineering by examining
    the very nature of raw data.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices of feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we looked at different **artificial intelligence**
    (**AI**) algorithms, analyzing their application to the different scenarios and
    their use cases in a cybersecurity context. Now, the time has come to learn how
    to evaluate these algorithms, starting from the assumption that algorithms are
    the foundation of data-driven learning models.
  prefs: []
  type: TYPE_NORMAL
- en: We will therefore have to deal with the very nature of the data, which is the
    basis of the algorithm learning process, which aims to make generalizations in
    the form of predictions based on the samples received as input in the training
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of algorithm will therefore fall on the one that is best for generalizing
    beyond the training data, thereby obtaining the best predictions when facing new
    data. In fact, it is relatively simple to identify an algorithm that fits the
    training data; the problem becomes more complicated when the algorithm must correctly
    make predictions on data that has never been seen before. In fact, we will see
    that the tendency to optimize the accuracy of the algorithm's predictions on training
    data gives rise to the phenomenon known as **overfitting**, where predictions
    become worse when dealing with new test data.
  prefs: []
  type: TYPE_NORMAL
- en: It therefore becomes important to understand how to correctly perform the algorithm
    training, from the selection of the training dataset up to the correct tuning
    of the learning parameters characterizing the chosen algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods available for performing algorithm training, such
    as using the same training dataset (for example, by dividing the training dataset
    into two separate subsets, one for training and one for testing) and choosing
    a suitable percentage of the original training dataset to be assigned to the two
    distinct subsets.
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy is based on cross validation, which, as we will see, consists
    of randomly dividing the training dataset into a certain number of subsets on
    which to train the algorithm and calculate the average of the results obtained
    in order to verify the accuracy of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Better algorithms or more data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While it is true that, in order to make correct predictions (which, in turn,
    are nothing more than generalizations starting from sample data), the data alone
    is not enough; you need to combine the data with algorithms (which, in turn, are
    nothing more than data representations). In practice, however, we are often faced
    with a dilemma when improving our predictions: should we design a better algorithm,
    or do we just need to collect more data? The answer to this question has not always
    been the same over time, since, when research in the field of AI began, the emphasis
    was on the quality of the algorithms, since the availability of data was dictated
    by the cost of storage.'
  prefs: []
  type: TYPE_NORMAL
- en: With the reduction in costs associated with storage, in recent years, we have
    witnessed an unprecedented explosion in the availability of data, which has given
    rise to new analytical techniques based on big data, and the emphasis has consequently
    shifted to the availability of data. However, as the amount of data available
    increases, the amount of time required to analyze it increases accordingly, so,
    in choosing between the quality of the algorithms and the amount of training data,
    we must face a trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: In general, practical experience shows us that even a dumb algorithm powered
    by large amounts of data is able to produce better predictions than a clever algorithm
    fed with less data.
  prefs: []
  type: TYPE_NORMAL
- en: However, the very nature of the data is often the element that makes the difference.
  prefs: []
  type: TYPE_NORMAL
- en: The very nature of raw data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The emphasis given to the relevance of data often resonates in the motto that
    states let the data speak for itself. In reality, data is almost never able to
    speak for itself, and when it does, it usually deceives us. The raw data is nothing
    more than fragments of information that behave like pieces of a puzzle where we
    do not (yet) know the bigger picture.
  prefs: []
  type: TYPE_NORMAL
- en: To make sense of the raw data, we therefore need models that help us to distinguish
    the necessary pieces (the signal) from the useless pieces (the noise), in addition
    to identifying the missing pieces to complete our puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: The models, in the case of AI, take the form of mathematical relations between
    features, through which we are able to show the different aspects and the different
    functions that the data represents, based on the purpose we intend to achieve
    with our analysis. In order for the raw data to be inserted in to our mathematical
    models, it must first be treated appropriately, thereby becoming the feature of
    our models. A feature, in fact, is nothing but the numerical representation of
    the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, raw data often does not occur in numerical form. However, the representation
    of data in numerical form is a necessary prerequisite, in order to get processed
    by algorithms. Therefore, we must convert the raw data into numerical form before
    feeding it to our algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering to the rescue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Therefore, in implementing our predictive models, we must not simply limit ourselves
    to specifying the choice of the algorithm(s), but we must also define the features
    required to power them. As such, the correct definition of features is an essential
    step, both for the achievement of our goals and for the efficiency of the implementation
    of our predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: As we have said, the features constitute the numerical representations of the
    raw data. There are obviously different ways to convert raw data into numerical
    form, and these are distinguished according to the varying nature of the raw data,
    in addition to the types of algorithms of choice. Different algorithms, in fact,
    require different features in order to work.
  prefs: []
  type: TYPE_NORMAL
- en: The number of features is also equally important for the predictive performance
    of our model. Choosing the quality and quantity of features therefore constitutes
    a preliminary process, known as feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with raw data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A first screening is conducted on the basis of the nature of the numerical values
    that are to be associated with our models. We should ask ourselves whether the
    number values we require are only positive or negative, or just Boolean values,
    whether we can limit ourselves to certain orders of magnitude, whether we can
    determine in advance the maximum value and the minimum value that the features
    can assume, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We can also artificially create complex features, starting from simple features,
    in order to increase the explanatory, as well as the predictive, capacity of our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the most common transformations applicable to transforming
    raw data into model features:'
  prefs: []
  type: TYPE_NORMAL
- en: Data binarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logarithmic data transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now examine each of these transformations in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Data binarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most basic forms of transformation, based on raw data counting, is
    binarization, which consists of assigning the value `1` to all the counts greater
    than `0`, and assigning the value `0` in the remaining cases. To understand the
    usefulness of binarization, we only need to consider the development of a predictive
    model whose goal is to predict user preferences based on video visualizations.
    We could therefore decide to assess the preferences of the individual users simply
    by counting their respective visualizations of videos; however, the problem is
    that the order of magnitude of the visualizations varies according to the habits
    of the individual users.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the absolute value of the visualizations—that is, the raw count—does
    not constitute a reliable measure of the greater or lesser preference accorded
    to each video. In fact, some users have the habit of repeatedly visualizing the
    same video, without paying particular attention to it, while other users prefer
    to focus their attention, thereby reducing the number of visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the different orders of magnitude associated with video visualizations
    by each user, varying from tens to hundreds, or even thousands of views, based
    on a user's habits, makes some statistical measurements, such as the arithmetic
    average, less representative of individual preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of using the raw count of visualizations, we can binarize the counts,
    associating the value `1` with all the videos that obtained a number of visualizations
    greater than `0` and the value `0` otherwise. Obtaining the results in this way
    is more efficient and robust measure of individual preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Data binning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing the different orders of magnitude of the counts is a problem that occurs
    in different situations, and there are many algorithms that behave badly when
    faced with data that exhibits a wide ranges of values, such as clustering algorithms
    that measure similarity on the basis of Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way to binarization, it is possible to reduce the dimensional scale
    by grouping the raw data counts into containers called **bins**, with fixed amplitude
    (fixed-with binning), sorted in ascending order, thereby scaling their absolute
    values linearly or exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: Logarithmic data transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly, it is possible to reduce the magnitude of raw data counts by replacing
    their absolute values with logarithms.
  prefs: []
  type: TYPE_NORMAL
- en: A peculiar feature of the logarithmic transformation is precisely that of reducing
    the relevance of greater values, and, at the same time, of amplifying smaller
    values, thereby achieving greater uniformity of value distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to logarithms, it is possible to use other power functions, which
    allow the stabilization of the variance of a data distribution (such as the **Box–Cox
    transformation**).
  prefs: []
  type: TYPE_NORMAL
- en: Data normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Also known as feature normalization or feature scaling, data normalization improves
    the performance of algorithms that can be influenced by the scale of input values.
  prefs: []
  type: TYPE_NORMAL
- en: The following are the most common examples of feature normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Min–max scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the min–max scaling transformation, we let the data fall within a limited
    range of values: `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformation of the data involves replacing the original values ![](img/b50a1285-4c07-480b-9570-0cfe78242d22.png)
    with the values calculated with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f345394-1d4d-40bd-a45e-4812c056ff5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/b040dc4a-5be0-4b03-b363-0cd1a90eb0f8.png) represents the minimum
    value of the entire distribution, and ![](img/61bfae4a-ae32-4882-8498-a06ebf8fe84f.png)
    the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: Variance scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another very common data normalization method involves subtracting the mean
    of the distribution from each single ![](img/627215c2-d54f-4636-840d-349182d572f7.png)
    value, and then dividing the result obtained by the variance of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Following normalization (also known as **standardization**), the distribution
    of the recalculated data shows a mean equal to `0` and a variance equal to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for variance scaling is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d352f705-2937-4bb7-b122-f20007038cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: How to manage categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Raw data can be represented by categorical variables that take non-numeric values.
  prefs: []
  type: TYPE_NORMAL
- en: A typical example of a categorical variable is nationality. In order to mathematically
    manage categorical variables, we need to use some form of category transformation
    in numerical values, also called encoding.
  prefs: []
  type: TYPE_NORMAL
- en: The following are the most common methods of categorical encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An intuitive approach to encoding could be to assign a single progressive value
    to the individual categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f0f4e40-c79a-4776-b3da-ce4424d745db.png)'
  prefs: []
  type: TYPE_IMG
- en: The advantage and disadvantage of this encoding method is that the transformed
    values may be numerically ordered, even when this numerical ordering has no real
    meaning.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the one-hot encoding method, a set of bits is assigned to each variable,
    with each bit representing a distinct category.
  prefs: []
  type: TYPE_NORMAL
- en: 'The set of bits enables us to distinguish the variables that cannot belong
    to more than one category, resulting in only one bit data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05b97bc3-bed1-4916-a6bc-322d838ca09a.png)'
  prefs: []
  type: TYPE_IMG
- en: Dummy encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The one-hot encoding method actually wastes a bit (that, in fact, is not strictly
    necessary), which can be eliminated using the dummy encoding method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/715eddd8-8a7f-4a82-949a-97112dabc963.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature engineering examples with sklearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's look at some examples of feature engineering implementation using
    the NumPy library and the preprocessing package of the `scikit-learn` library.
  prefs: []
  type: TYPE_NORMAL
- en: Min–max scaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code, we see an example of feature engineering using the `MinMaxScaler`
    class of `scikit-learn`, aimed at scaling features to lie between a given range
    of values (minimum and maximum), such as `0` and `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Standard scaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following example shows the `StandardScaler` class of `scikit-learn` in
    action, used to compute the mean and standard deviation on a training set by leveraging
    the `transform()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Power transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we see the `PowerTransformer` class of `scikit-learn` in
    action, applying the zero-mean, unit-variance normalization to the transformed
    output using a Box–Cox transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Ordinal encoding with sklearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we see how to encode categorical features into integers
    using the `OrdinalEncoder` class of `scikit-learn` and its `transform()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: One-hot encoding with sklearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following example shows how to transform categorical features into binary
    representation, making use of the `OneHotEncoder` class of `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After having described feature engineering best practices, we can move on to
    evaluating the performance of our models.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a detector's performance with ROC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have previously encountered the ROC curve and AUC measure ([Chapter 5](a6eab48a-f031-44c9-ae4a-0cfd5db2e05e.xhtml), *Network
    Anomaly Detection with AI**,* and [Chapter 7](98ce7db1-f53d-47ca-b6ca-ec0e5f882566.xhtml),
    *Fraud Prevention with Cloud AI Solutions*) to evaluate and compare the performance
    of different classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s explore the topic in a more systematic way, introducing the confusion
    matrix associated with all the possible results returned by a fraud-detection
    classifier, comparing the predicted values with the real values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da832600-40a3-4ae9-9307-cd648dae232d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then calculate the following values (listed with their interpretation)
    based on the previous confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity** = **Recall** = **Hit rate** = ***TP/(TP + FP)***: This value
    measures the rate of correctly labeled fraudsters and represents the **true positive
    rate** (**TPR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive Rate** ***(FPR) = FP/(FP + TN)***: FPR is also calculated
    as 1 – Specificity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification accuracy** = ***(TP + TN)/(TP + FP + FN + TN)***: This value
    represents the percentage of correctly classified observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification error** = ***(FP + FN)/(TP + FP + FN + TN)***: This value
    represents the misclassification rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity** = ***TN/(FP + TN)***: This value measures the rate of correctly
    labeled nonfraudsters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision** = ***TP/(TP + FP)***: This value measures how many of the predicted
    fraudsters are actually fraudsters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***F*** - ***measure*** = ***2*** x **(*Precision*** x ***Recall*)/(*Precision***
    + ***Recall*)**: This value represents the weighted harmonic mean of the precision
    and recall. The *F*-measure varies from 0 (worst score) to 1 (best value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are now able to analyze in detail the ROC curve and its associated AUC measure.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve and AUC measure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Among the techniques most commonly used to compare the performance of different
    classifiers, we have the **receiving operating characteristic** (**ROC**) curve,
    which describes the relationship between the TPR, or sensitivity, and the FPR,
    or 1 – Specificity, associated with each classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8318045e-fdbd-4639-8182-2f3d23ffe968.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(Image Credits: http://scikit-learn.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: How can the performances of the different classifiers be compared?
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by taking into consideration the characteristics that the best possible
    classifier should have. Its curve would correspond to the pair of values, ​​***x***
    = ***0*** and ***y*** = ***1***, in the ROC space. In other words, the best possible
    classifier is the one that correctly identifies all cases of fraud without generating
    any false positives, meaning that ***FPR ***= ***0*** and ***TPR*** = ***1***
    are ideal.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the performance of a random classifier, which makes predictions at
    random, would fall on the diagonal described by the pair of coordinates [***x***
    = ***0***, ***y*** = ***0***] and [***x*** = ***1***, ***y*** = ***1***].
  prefs: []
  type: TYPE_NORMAL
- en: The comparison between the performances of different classifiers would therefore
    involve verifying how much their curves deviate from the *L*-curve (corresponding
    to the best classifier).
  prefs: []
  type: TYPE_NORMAL
- en: To achieve a more precise measure of performance, we can calculate the **area
    under the ROC curve **(**AUC**) metric associated with the individual classifiers.
    The values ​​that the AUC metric can assume fall in the range between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: The best classifier's AUC metric is equal to 1, corresponding to the maximum
    value of AUC. The AUC metric can also be interpreted as a measure of the probability.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, a random classifier, whose curve corresponds to the diagonal in the
    ROC space, would have an AUC value of 0.5\. Consequently, the performance of any
    other classifier should fall between a minimum AUC value of 0.5 and a maximum
    value of 1\. Values ​​of ***AUC*** < ***0.5*** indicate that the chosen classifier
    behaves worse than the random classifier.
  prefs: []
  type: TYPE_NORMAL
- en: To correctly evaluate the quality of the estimated probabilities of the individual
    classifiers, we can use the **Brier score** (**BS**), which measures the average
    of the differences between the estimated probabilities and the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the BS formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e252f1d0-4e6a-421d-8e08-7fd7071da4d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *P**[i]* is the estimated probability for observation *i*, and ![](img/5d83a679-ee27-42df-b126-418635c4971a.png)
    is a binary estimator (that assumes values of 0 or 1) for the actual value, *i*. Also,
    the value of *BS* falls in the interval between 0 and 1, but, unlike the AUC,
    smaller values of *BS* (that is, *BS* values closer to 0) correspond to more accurate
    probability estimates.
  prefs: []
  type: TYPE_NORMAL
- en: The following are some examples of calculations of the ROC curves and of the
    metrics associated with them, using the `scikit-learn` library.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of ROC metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code, we can see an example of a calculation of ROC metrics,
    using scikit-learn''s methods, such as `precision_recall_curve()`, `average_precision_score()`,
    `recall_score()`, and `f1_score()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ROC curve example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code shows how to calculate an ROC curve using the `roc_curve()`
    method of `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: AUC score example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example code, we can see how to calculate the AUC curve using
    the `roc_auc_score()` method of `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Brier score example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we evaluate the quality of estimated probabilities
    using the `brier_score_loss()` method of `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's continue the evaluation of model performances by introducing the
    effects deriving from the splitting of the sample dataset into training and testing
    subsets.
  prefs: []
  type: TYPE_NORMAL
- en: How to split data into training and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most commonly used methods to evaluate the learning effectiveness
    of our models is to test the predictions made by the algorithms on data it has
    never seen before. However, it is not always possible to feed fresh data into
    our models. One alternative involves subdividing the data at our disposal into
    training and testing subsets, varying the percentages of data to be assigned to
    each subset. The percentages usually chosen vary between 70% and 80% for the training
    subset, with the remaining 20–30% assigned to the testing subset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The subdivision of the original sample dataset into two subsets for training
    and testing can be easily performed using the `scikit-learn` library, as we have
    done several times in our examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By invoking the `train_test_split()` method of the `sklearn.model_selection`
    package and setting the `test_size = 0.2` parameter, we are splitting the original
    sample dataset into training and testing subsets, reserving a percentage equal
    to 20% of the original dataset for the testing dataset and assigning the remaining
    80% to the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This technique, however simple, may nonetheless have important effects on the
    learning effectiveness of our algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm generalization error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen, the purpose of the algorithms is to learn to make correct
    predictions by generalizing from the training samples. All the algorithms, as
    a result of this learning process, manifest a generalization error that can be
    expressed as the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d7a09e9-ab03-4acf-a353-122d729b597b.png)'
  prefs: []
  type: TYPE_IMG
- en: By *Bias*, we mean the systematic error made by the algorithm in carrying out
    its predictions, and by *Variance*, we mean the sensitivity of the algorithm to
    the variations affecting the analyzed data. Finally, *Noise* is an irreducible
    component that characterizes the data being analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram shows different estimators that are characterized by their
    ability to adapt to data. Starting from the simplest estimator and moving up to
    the most complex estimator, we can see how the components of *Bias* and *Variance*
    vary. A lower complexity of the estimator usually corresponds to a higher *Bias*
    (systematic error) and a reduced variance—that is, sensitivity to data change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, as the complexity of the model increases, the *Bias* is reduced
    but the *Variance* is increased, so that more complex models tend to over-adapt
    (overfit) their predictions to training data, thereby producing inferior predictions
    when switching from training data to testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e2393d9-95d0-4e22-936e-27a7d913053d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(Image Credits: http://scikit-learn.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: A method to reduce the *Variance* associated with the complexity of the algorithm
    involves increasing the amount of data constituting the training dataset; however,
    it is not always easy to distinguish which component (the *Bias* or the *Variance*)
    assumes greater importance in the determination of the generalization error. Therefore,
    we must use appropriate tools to distinguish the role played by the various components
    in the generalization error determination.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm learning curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A useful tool for identifying a component between the *Bias* and the *Variance* is
    important in determining the generalization error of an algorithm. This is the
    learning curve, through which the predictive performance of the algorithm is compared
    with the amount of training data. This way, it is possible to evaluate how the
    training score and the testing score of an algorithm vary as the training dataset
    changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c82113c4-7be1-4048-af6a-e548e938369c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(Image Credits: Wikipedia https://commons.wikimedia.org/wiki/File:Variance-bias.svg)*'
  prefs: []
  type: TYPE_NORMAL
- en: If the training score and the testing score tend to converge when the training
    dataset grows (as shown in the preceding diagram), in order to improve our predictions,
    we will have to increase the complexity of our algorithm, thereby reducing the
    *Bias* component.
  prefs: []
  type: TYPE_NORMAL
- en: If instead the training score is constantly higher than the testing score, an
    increase in the training dataset would improve the predictive performance of our
    algorithm, thereby reducing the *Variance* component. Finally, in the case where
    the training score and the testing score do not converge, our model is characterized
    by high variance, and we will therefore have to act both on the complexity of
    the algorithm and on the size of the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see how to use the `learning_curve()` method
    of the `sklearn.model_selection` package to obtain the values necessary to design
    a learning curve combined with a **support vector classifier** (**SVC**), based
    on different training dataset sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, we can say that our choice of the size of the training dataset
    affects on the learning effectiveness of our algorithms. In principle, by reducing
    the percentage assigned to the training dataset, we are increasing the *Bias*
    error component. If instead we increase the size of the training dataset while
    maintaining the original sample dataset's size constant, we risk over-adapting
    the algorithm to the training data, which results in inferior predictions when
    we feed our algorithm new data, not to mention the fact that some highly informative
    samples could be excluded from the training dataset, due to the simple effect
    of the case, in relation to the specific splitting strategy we choose.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, if the training dataset is characterized by high dimensionality,
    the similarities between the testing and training data might only be apparent,
    thus making the learning process more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the simple strategy of splitting the sample dataset according to
    fixed percentages is not always the best solution, especially when it comes to
    evaluating and fine-tuning the performance of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative solution is to use cross validation.
  prefs: []
  type: TYPE_NORMAL
- en: Using cross validation for algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The type of cross validation most commonly used is known as k-folds cross validation,
    and it involves randomly dividing the sample dataset into a number of folds, *k*
    corresponding to equal portions (if possible) of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning process is performed in an iterative way, based on the different
    compositions of folds, used both as a training dataset and as a testing dataset.
    In this way, each fold is used in turn as a training dataset or as a testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75f8f430-1f26-4878-a2c8-caea432fc8f3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(Image Credits: http://scikit-learn.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the different folds (randomly generated) alternate in the role
    of training and testing datasets, and the iterative process ends when all the
    k-folds have been used both as training and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Since, at each iteration, the generalization error generated is different (as
    the algorithm is trained and tested with different datasets), we can calculate
    the average of these errors as a representative measure of the cross validation
    strategy.
  prefs: []
  type: TYPE_NORMAL
- en: K-folds cross validation pros and cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'K-folds cross validation has several advantages, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It enables all the available data to be used, both for training and testing
    goals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific composition of the individual folds is irrelevant since each fold
    is used at most once for training, and once for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can increase the number of folds by increasing the *k* value, thereby increasing
    the size of the training dataset to reduce the *Bias* component of the generalization
    error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of disadvantages, we must stress that k-folds cross validation considers
    the order that characterizes our original sample dataset to be irrelevant. In
    case the order of the data constitutes relevant information (as in the case of
    time series datasets), we will have to use a different strategy that takes into
    account the original sequence—perhaps by splitting the data in accordance with
    the oldest data—to be used as a training dataset, thereby reserving the most recent
    data for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: K-folds cross validation example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following example, we will use the k-folds cross validation implemented
    by the `scikit-learn` package, `sklearn.model_selection`. For simplicity, we will
    assign the value `2` to the variable `k`, thereby obtaining a 2-folds cross validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample dataset consists of just four samples. Therefore, each fold will
    contain two arrays to be used in turn, one for training and the other for testing.
    Finally, note how it is possible to associate the different folds with training
    and testing data, using the syntax provided by `numpy` indexing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at the different techniques commonly adopted
    to evaluate the predictive performances of different algorithms. We looked at
    how to transform raw data into features, following feature engineering best practices,
    thereby allowing algorithms to use data that does not have a numeric form, such
    as categorical variables. We then focused on the techniques needed to correctly evaluate
    the various components (such as bias and variance) that constitute the generalization
    error associated with the algorithms, and finally, we learned how to perform the
    cross validation of the algorithms to improve the training process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to assess your AI arsenal.
  prefs: []
  type: TYPE_NORMAL
