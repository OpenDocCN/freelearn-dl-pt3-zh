["```\nimport neural_structured_learning as nsl\nimport tensorflow as tf\n```", "```\n# Cora dataset path\nTRAIN_DATA_PATH = 'data/train_merged_examples.tfr'\nTEST_DATA_PATH = 'data/test_examples.tfr'\n# Constants used to identify neighbor features in the input.\nNBR_FEATURE_PREFIX = 'NL_nbr_'\nNBR_WEIGHT_SUFFIX = '_weight'\n# Dataset parameters\nNUM_CLASSES = 7\nMAX_SEQ_LENGTH = 1433\n# Number of neighbors to consider in the composite loss function\nNUM_NEIGHBORS = 1\n# Training parameters\nBATCH_SIZE = 128\n\n```", "```\ndef make_dataset(file_path: str, training=False) -> tf.data.TFRecordDataset:\n    dataset = tf.data.TFRecordDataset([file_path])\n    if training:\n        dataset = dataset.shuffle(10000)\n    dataset = dataset.map(parse_example).batch(BATCH_SIZE)\n\n    return dataset\n```", "```\ndef parse_example(example_proto: tf.train.Example) -> tuple:\n```", "```\n    feature_spec = {\n        'words':\n            tf.io.FixedLenFeature(shape=[MAX_SEQ_LENGTH],\n                                  dtype=tf.int64,\n                                  default_value=tf.constant(\n                                      value=0,\n                                      dtype=tf.int64,\n                                      shape=[MAX_SEQ_LENGTH])),\n        'label':\n            tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n    }\n```", "```\n    for i in range(NUM_NEIGHBORS):\n        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n        nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)\n        feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n            shape=[MAX_SEQ_LENGTH],\n            dtype=tf.int64,\n            default_value=tf.constant(\n                value=0, dtype=tf.int64, shape=[MAX_SEQ_LENGTH]))\n\n        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n            shape=[1], dtype=tf.float32, default_value=tf.constant([0.0]))\n\n    features = tf.io.parse_single_example(example_proto, feature_spec)\n\n    labels = features.pop('label')\n    return features, labels\n```", "```\n    features = tf.io.parse_single_example(example_proto, feature_spec)\n```", "```\ntrain_dataset = make_dataset(TRAIN_DATA_PATH, training=True)\ntest_dataset = make_dataset(TEST_DATA_PATH)\n```", "```\ndef build_model(dropout_rate):\n    \"\"\"Creates a sequential multi-layer perceptron model.\"\"\"\n    return tf.keras.Sequential([\n        # one-hot encoded input.\n        tf.keras.layers.InputLayer(\n            input_shape=(MAX_SEQ_LENGTH,), name='words'),\n\n        # 2 fully connected layers + dropout\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dropout(dropout_rate),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dropout(dropout_rate),\n\n        # Softmax output\n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n```", "```\nmodel = build_model(dropout_rate=0.5)\n```", "```\ngraph_reg_config = nsl.configs.make_graph_reg_config(\n    max_neighbors=NUM_NEIGHBORS,\n    multiplier=0.1,\n    distance_type=nsl.configs.DistanceType.L2,\n    sum_over_axis=-1)\ngraph_reg_model = nsl.keras.GraphRegularization(model,\n                                                graph_reg_config)\n```", "```\ngraph_reg_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy'])\n\n# run eagerly to prevent epoch warnings\ngraph_reg_model.run_eagerly = True\n\ngraph_reg_model.fit(train_dataset, epochs=100, verbose=1)\n```", "```\neval_results = dict(\n    zip(graph_reg_model.metrics_names,\n        graph_reg_model.evaluate(test_dataset)))\nprint('Evaluation accuracy: {}'.format(eval_results['accuracy']))\nprint('Evaluation loss: {}'.format(eval_results['loss']))\n```", "```\nEvaluation accuracy: 0.8137432336807251\nEvaluation loss: 1.1235489577054978\n```"]