<html><head></head><body>
		<div id="_idContainer004">
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>Preface</h1>
			<p>The release of TensorFlow 2.x in 2019 was one of the biggest and most anticipated events in the deep learning and artificial intelligence arena, because it brought with it long-overdue improvements to this popular and relevant framework, mainly focused on simplicity and ease of use.</p>
			<p>The adoption of Keras as the official TensorFlow high-level API, the ability to switch back and forth between eager and graph-based execution (thanks to <strong class="source-inline">tf.function</strong>), and the ability to create complex data pipelines with <strong class="source-inline">tf.data</strong> are just a few of the great additions that TensorFlow 2.x brings to the table.</p>
			<p>In this book, you will discover a vast amount of recipes that will teach you how to take advantage of these advancements in the context of deep learning applied to computer vision. We will cover a wide gamut of applications, ranging from image classification to more challenging ones, such as object detection, image segmentation, and <strong class="bold">Automated Machine Learning</strong> (<strong class="bold">AutoML</strong>).</p>
			<p>By the end of this book, you’ll be prepared and confident enough to tackle any computer vision problem that comes your way with the invaluable help of TensorFlow 2.x!</p>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>Who this book is for</h1>
			<p>This book is for computer vision developers, computer vision engineers, and deep learning practitioners looking for go-to solutions to various problems faced in computer vision. You will discover how to employ modern machine learning techniques and deep learning architectures to perform a plethora of computer vision tasks. Basic knowledge of Python programming and computer vision is required.</p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>What this book covers</h1>
			<p><a href="B14768_01_Final_JM_ePub.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with TensorFlow 2.x for Computer Vision</em>, serves as an overview of basic deep learning concepts, as well as being a first look at some important TensorFlow 2.x features, such as the Keras and <strong class="source-inline">tf.data.Dataset</strong> APIs. It also teaches you about common and necessary tasks such as saving and loading a model and visualizing a network architecture. It ends with the implementation of a simple image classifier.</p>
			<p><a href="B14768_02_Final_JM_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 2</em></a>, <em class="italic">Performing Image Classification</em>, goes in-depth about the most common application of deep neural networks to computer vision: image classification. It explores the common varieties of classification, such as binary and multiclass classification, and then transitions to examples of multilabel classification and out-of-the-box solutions using transfer learning and TensorFlow Hub.</p>
			<p><a href="B14768_03_Final_JM_ePub.xhtml#_idTextAnchor099"><em class="italic">Chapter 3</em></a>, <em class="italic">Harnessing the Power of Pre-Trained Networks with Transfer Learning</em>, focuses on transfer learning, a powerful technique to reuse networks pre-trained on massive datasets to increase development productivity and the performance of deep learning-powered computer vision applications. This chapter starts by seeing you use pre-trained networks as feature extractors. Then, you will learn how to combine deep learning with traditional machine learning algorithms through a procedure called incremental learning. Finally, the chapter closes with two examples of fine-tuning: the first using the Keras API and the second relying on TensorFlow Hub.</p>
			<p><a href="B14768_04_Final_JM_ePub.xhtml#_idTextAnchor140"><em class="italic">Chapter 4</em></a>, <em class="italic">Enhancing and Styling Images with DeepDream, Neural Style Transfer, and Image Super-Resolution</em>, focuses on fun and less conventional applications of deep neural networks in computer vision, namely DeepDream, neural style transfer, and image super-resolution. </p>
			<p><a href="B14768_05_Final_JM_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 5</em></a>, <em class="italic">Reducing Noise with Autoencoders</em>, goes over autoencoders, a composite architecture used in domains such as image restoration, inverse image search indexes, and image denoising. It starts by introducing the dense and convolutional variants of autoencoders and then explains several applications, such as inverse image search engines and outlier detection.</p>
			<p><a href="B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214"><em class="italic">Chapter 6</em></a>, <em class="italic">Generative Models and Adversarial Attacks</em>, introduces you to many examples and applications of <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>). The chapter ends with an example of how to perform an adversarial attack on convolutional neural networks.</p>
			<p><a href="B14768_07_Final_JM_ePub.xhtml#_idTextAnchor248"><em class="italic">Chapter 7</em></a>, <em class="italic">Captioning Images with CNNs and RNNs</em>, focuses on how to combine both convolutional and recurrent neural networks to generate textual descriptions of images.</p>
			<p><a href="B14768_08_Final_JM_ePub.xhtml#_idTextAnchor270"><em class="italic">Chapter 8</em></a>, <em class="italic">Fine-Grained Understanding of Images through Segmentation</em>, focuses on image segmentation, a fine-grained version of image classification, at the pixel level. It covers seminal segmentation architectures, such as U-Net and Mask-RCNN.</p>
			<p><a href="B14768_09_Final_JM_ePub.xhtml#_idTextAnchor298"><em class="italic">Chapter 9</em></a>, <em class="italic">Localizing Elements in Images with Object Detection</em>, covers the complex and yet common task of object detection. It goes over both traditional approaches based on image pyramids and sliding windows and more modern solutions, such as YOLO. It includes a thorough explanation of how to leverage the TensorFlow Object Detection API to train state-of-the-art models on custom datasets.</p>
			<p><a href="B14768_10_Final_JM_ePub.xhtml#_idTextAnchor321"><em class="italic">Chapter 10</em></a>, <em class="italic">Applying the Power of Deep Learning to Videos</em>, expands the application of deep neural networks to videos. Here, you will find examples of how to detect emotions, recognize actions, and generate frames in a video. </p>
			<p><a href="B14768_11_Final_JM_ePub.xhtml#_idTextAnchor345"><em class="italic">Chapter 11</em></a>, <em class="italic">Streamlining Network Implementation with AutoML</em>, explores the exciting subfield of AutoML using Autokeras, an experimental library built on top of TensorFlow 2.x, which uses <strong class="bold">Neural Architecture Search</strong> (<strong class="bold">NAS</strong>) to arrive at the best model possible for a given problem. The chapter starts by exploring the basic features of Autokeras and closes by using AutoML to create an age and gender prediction tool.</p>
			<p><a href="B14768_12_Final_JM_ePub.xhtml#_idTextAnchor370"><em class="italic">Chapter 12</em></a>, <em class="italic">Boosting Performance</em>, explains in detail many different techniques that can be used to boost the performance of a network, from simple but powerful methods, such as using ensembles, to more advanced ones, such as using GradientTape to tailor the training process to the specific needs of a project.</p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>To get the most out of this book</h1>
			<p>You will need a version of TensorFlow 2 installed. All the recipes in this book have been implemented and tested using TensorFlow 2.3 on macOS X and Ubuntu 20.04, but they should work with future stable versions as well. Please note that Windows is not supported.</p>
			<p>Although not strictly necessary, access to a GPU-enabled machine, either on-premises or in the cloud, is highly encouraged, as it reduces the runtime of the examples dramatically. </p>
			<div>
				<div id="_idContainer003" class="IMG---Figure">
					<img src="image/B14768_Preface_Table_1.jpg" alt=""/>
				</div>
			</div>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code via the GitHub repository (link available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code</strong>.</p>
			<p>Because this is a hands-on book, focused on practical examples to solve varied situations, I encourage you to expand your knowledge on any topics that you find interesting in any particular recipe. In the <em class="italic">See also</em> section of each recipe, you will find links, references, and suggestions for recommended reads or extension points that will cement your understanding of the techniques explained in that example.</p>
			<h2 id="_idParaDest-11"><a id="_idTextAnchor010"/>Download the example code files</h2>
			<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook</a>. In case there’s an update to the code, it will be updated on the existing GitHub repository.</p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Code in Action</h1>
			<p>Code in Action videos for this book can be viewed at <a href="https://bit.ly/2NmdZ5G">https://bit.ly/2NmdZ5G</a>.</p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Download the color images</h1>
			<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://static.packt-cdn.com/downloads/9781838829131_ColorImages.pdf">https://static.packt-cdn.com/downloads/9781838829131_ColorImages.pdf</a>.</p>
			<h1 id="_idParaDest-14"><a id="_idTextAnchor013"/>Conventions used</h1>
			<p>There are a number of text conventions used throughout this book.</p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: “Using <strong class="source-inline">image_generator</strong>, we’ll pick and display a random batch of 10 images directly from the directory they are stored in.”</p>
			<p>A block of code is set as follows:</p>
			<p class="source-code">iterator = (image_generator</p>
			<p class="source-code">           .flow_from_directory(directory=data_directory, </p>
			<p class="source-code">                                 batch_size=10))</p>
			<p class="source-code">for batch, _ in iterator:</p>
			<p class="source-code">plt.figure(figsize=(5, 5))</p>
			<p class="source-code">for index, image in enumerate(batch, start=1):</p>
			<p class="source-code">ax = plt.subplot(5, 5, index)</p>
			<p class="source-code">plt.imshow(image)</p>
			<p class="source-code">plt.axis(‘off’)</p>
			<p class="source-code">plt.show()</p>
			<p class="source-code">break</p>
			<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
			<p class="source-code">[default]</p>
			<p class="source-code">exten =&gt; s,1,Dial(Zap/1|30)</p>
			<p class="source-code">exten =&gt; s,2,Voicemail(u100)</p>
			<p class="source-code"><strong class="bold">exten =&gt; s,102,Voicemail(b100)</strong></p>
			<p class="source-code">exten =&gt; i,1,Voicemail(s0)</p>
			<p>Any command-line input or output is written as follows:</p>
			<p class="source-code">$ pip install tensorflow-hub Pillow</p>
			<p class="source-code">$ pip install tensorflow-datasets tqdm</p>
			<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: “Select <strong class="bold">System info</strong> from the <strong class="bold">Administration</strong> panel.”</p>
			<p class="callout-heading">Tips or important notes</p>
			<p class="callout">We’ll use the modified version of the Stanford Cars dataset we just worked on in future recipes in this chapter.</p>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Sections</h1>
			<p>In this book, you will find several headings that appear frequently (<em class="italic">Getting ready</em>, <em class="italic">How to do it...</em>, <em class="italic">How it works...</em>, <em class="italic">There’s more...</em>, and <em class="italic">See also</em>).</p>
			<p>To give clear instructions on how to complete a recipe, use these sections as follows:</p>
			<p>Getting ready</p>
			<p>This section tells you what to expect in the recipe and describes how to set up any software or any preliminary settings required for the recipe.</p>
			<h2 id="_idParaDest-16"><a id="_idTextAnchor015"/>How to do it…</h2>
			<p>This section contains the steps required to follow the recipe.</p>
			<h2 id="_idParaDest-17"><a id="_idTextAnchor016"/>How it works…</h2>
			<p>This section usually consists of a detailed explanation of what happened in the previous section.</p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor017"/>There’s more…</h2>
			<p>This section consists of additional information about the recipe in order to make you more knowledgeable about the recipe.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>See also</h2>
			<p>This section provides helpful links to other useful information for the recipe.</p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>Get in touch</h1>
			<p>Feedback from our readers is always welcome.</p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, mention the book title in the subject of your message and email us at <strong class="source-inline">customercare@packtpub.com</strong>.</p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <strong class="source-inline">copyright@packt.com</strong> with a link to the material.</p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a>.</p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/>Reviews</h1>
			<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
			<p>For more information about Packt, please visit <a href="http://packt.com">packt.com</a>.</p>
		</div>
	</body></html>