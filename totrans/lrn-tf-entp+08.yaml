- en: 'Chapter 5:'
  prefs: []
  type: TYPE_NORMAL
- en: Training at Scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we build and train more complex models or use large amounts of data in
    an ingestion pipeline, we naturally want to make better use of all the compute
    time and memory resources at our disposal in a more efficient way. This is the
    major purpose of this chapter, as we are going to integrate what we learned in
    previous chapters with techniques for distributed training running in a cluster
    of compute nodes.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow has developed a high-level API for distributed training. Furthermore,
    this API integrates with the Keras API very well. As it turns out, the Keras API
    is now a first-class citizen in the TensorFlow ecosystem. Compared to the estimator
    API, Keras receives the most support when it comes to a distributed training strategy.
    Therefore, this chapter will predominantly focus on using the Keras API with a
    distributed training strategy. We will leverage Google Cloud resources to demonstrate
    how to make minimal changes to the Keras API code we are already familiar with
    and integrate it with the distributed training strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to leverage Google Cloud's AI Platform and
    use the `TFRecordDataset` into the model training workflow, and designate a distributed
    training strategy for the TPU and GPU accelerators. All the code for this chapter
    can be found at [https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_05](https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_05).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Cloud TPU through AI Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Cloud GPU through AI Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Cloud TPU through AI Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we begin, let''s briefly discuss the possible costs you might incur
    in the **Google Cloud Platform** (**GCP**). All the scripts and examples here
    are catered to didactic purposes. Therefore, training epochs are usually set to
    minimally reasonable values. With that in mind, it is still worth noting that
    as we start to leverage cloud resources, we need to keep in mind the compute cluster''s
    cost. You will find more information on AI Platform training charges here: [https://cloud.google.com/ai-platform/training/pricing#examples_calculate_training_cost_using_price_per_hour](https://cloud.google.com/ai-platform/training/pricing#examples_calculate_training_cost_using_price_per_hour).'
  prefs: []
  type: TYPE_NORMAL
- en: The examples in this book typically use the predefined scale tiers. In the predefined
    scale tiers listing at the preceding link, you will see the price per hour for
    different tiers. For example, `BASIC_TPU` is much more expensive than `BASIC_GPU`.
    We will use both in this chapter, as we will learn how to submit a training job
    to either the TPU or GPU. In my experience, each example in this book should complete
    its run between 20 to 60 minutes, with the parameters set either here in the book
    or in the GitHub repo. Your experience may vary, depending on your region and
    the availability of the compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: This cost does not include the cost of cloud storage, where you will read and
    write data or model artifacts. Remember to delete cloud storage when you are not
    using it. FYI, the cloud storage cost for the content and work related to this
    book is a very small fraction of the overall cost.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, when the GPU is in high demand, you may want to use the TPU, which
    is the fastest cluster that GCP offers. It may reduce the training time significantly,
    and likewise your expenses.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you haven''t done so already, go ahead and clone the repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we have seen in previous chapters, Google's AI Platform offers a convenient
    development environment known as JupyterLab. It integrates with other Google Cloud
    services, such as BigQuery or cloud storage buckets, through SDKs. In this section,
    we are going to leverage Google Cloud's TPU for a distributed training workload.
  prefs: []
  type: TYPE_NORMAL
- en: The TPU is a custom-built ASIC per Google's specification and design. It is
    an accelerator that is specifically optimized to handle deep learning calculations
    and algorithms. For this reason, a TPU is ideally suited for training complex
    neural networks and machine learning models with a virtually unlimited amount
    of training data. It completes a training routine in minutes where it would have
    taken hours in a single node machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, there are four types of TPU offerings: **V2**, **V2 Pod**, **V3**,
    and **V3 Pod**. For more details, refer to the official link, which includes Google
    Cloud''s description of the benefits of the Cloud TPU: [https://cloud.google.com/tpu/?_ga=2.138028336.-1825888872.1592693180](https://cloud.google.com/tpu/?_ga=2.138028336.-1825888872.1592693180)).
    For AI Platform instances that run TensorFlow Enterprise 2.1 or above, V3 is the
    preferred choice. With either V2 or V3, a **TPU pod** consists of multiple TPUs.
    A pod is basically a cluster of TPUs. For more details about the TPU and TPU pods,
    the following link describes the different versions of TPU pods and their runtimes
    for different machine learning training jobs: [https://cloud.google.com/tpu/docs/system-architecture#configurations](https://cloud.google.com/tpu/docs/system-architecture#configurations).
    Each pod, whether it''s V2 or V3, can perform up to 100 petaFLOPS. This performance
    is reported at this link: [https://techcrunch.com/2019/05/07/googles-newest-cloud-tpu-pods-feature-over-1000-tpus/](https://techcrunch.com/2019/05/07/googles-newest-cloud-tpu-pods-feature-over-1000-tpus/).'
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of a pod over a single TPU is the training speed and memory at your
    disposal for the training workflow. Compared to a V2 pod (512 cores = eight cores
    per TPU multiplied by 64 TPUs), each V2 TPU consists of eight cores, and each
    core is the basic unit for training data parallelism. At the core level, the TensorFlow
    distributed training strategy is executed. For demonstration and didactic purposes,
    all the examples in this section distribute a training strategy among eight cores
    within a TPU. The `tf.distribute.TPUStrategy` API is the means to distribute training
    in the TPU. This strategy implements synchronous distributed training coupled
    with the TPU's all-reduce operations across multiple TPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: We will use a Cloud TPU and submit a training job. In this example, we are going
    to see how to submit such a training job using the `tfrecord` format with the
    images' original dimensions. The `tfrecord` images are stored in a Google Cloud
    storage bucket (it is assumed that your `tfrecord` is ready; generating `tfrecord`
    formatted data from raw images is not covered in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: The training workflow will generate checkpoints and save model artifacts when
    the training is complete. These items are likewise saved in the storage bucket.
    Therefore, we will have to grant the TPU read and write access to our working
    storage bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin using the TPU, there are a few administrative items in Google
    Cloud to take care of. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Cloud SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install the Cloud SDK in the client node, download and install Google Cloud
    SDK. There is a good instruction page about how to install the Cloud SDK for different
    types of systems, be it Mac, Linux, or Windows. It is strongly recommended to
    follow the instructions at this link to install Google Cloud SDK: [https://cloud.google.com/sdk/docs#install_the_latest_cloud_sdk_version](https://cloud.google.com/sdk/docs#install_the_latest_cloud_sdk_version).
    Once the installation is done, you can verify it with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will return the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – gcloud SDK verification'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – gcloud SDK verification
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.1* shows the general format of the `gcloud` command. Use *Ctrl* +
    *C* to exit this mode and recover your Command Prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Granting the Cloud TPU access to your project
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, the setup instructions are from Google Cloud''s own documentation
    site at this URL: [https://cloud.google.com/ai-platform/training/docs/using-tpus#tpu-runtime-versions](https://cloud.google.com/ai-platform/training/docs/using-tpus#tpu-runtime-versions):'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we are going to retrieve a cloud TPU service account name per
    our project ID. We can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding command will return the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2 – TPU service account retrieval'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image0031.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.2 – TPU service account retrieval
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Make a note of the `serviceAccountProject` and `tpuServiceAccount` details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we know our TPU service account, we will have to initialize it as per
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding command generates a Cloud TPU service account for you. Make sure
    you put your `<serviceAccountProject>` details in the URL.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a TPU service account as a member of the project
  prefs: []
  type: TYPE_NORMAL
- en: 'The project we use must also know about the TPU service account. In *step 3*
    of the previous section, we passed our project''s Bearer Token to our TPU service
    account so the TPU can access our project. Basically, it is similar to adding
    another member to this project, and in this case, the new member is the TPU service
    account:'
  prefs: []
  type: TYPE_NORMAL
- en: We can use Google Cloud Console to achieve this, as shown in *Figure 5.**3*:![Figure
    5.3 – The IAM & Admin entry
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0051.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.3 – The IAM & Admin entry
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the **IAM** screen, click the **ADD** button to add our TPU to this project,
    as in *Figure 5.4*:![Figure 5.4 – Adding a member to a project
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0071.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.4 – Adding a member to a project
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, fill in the TPU service account details in the **New members** box. Under
    **Select a role**, find **Service Agent Roles**, and then find **Cloud ML Service
    Agent**. This is shown in *Figure 5.5*:![Figure 5.5 – Assigning the Cloud ML Service
    Agent role to the TPU service account
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0091.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.5 – Assigning the Cloud ML Service Agent role to the TPU service account
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are not done with the TPU service account yet. We also have to let it access
    our training data and our storage to write the training results to, such as checkpoints
    and model assets. This means adding a couple of new roles for our TPU service
    account.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's click **Add another role** and proceed to look for **Project**, as in
    *Figure 5.6*:![Figure 5.6 – Assigning the Project Viewer role to the TPU service
    account
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0111.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.6 – Assigning the Project Viewer role to the TPU service account
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Likewise, we also add the **Cloud Storage Admin** role, as in *Figure 5.7*:![Figure
    5.7 – Assigning the Storage Admin role to the TPU service account
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0131.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.7 – Assigning the Storage Admin role to the TPU service account
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you have all three roles set up, click **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Whitelisting access for reading training data and writing artifacts (alternative)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous method grants rather broad permissions to the TPU service. It
    allows the TPU to have an admin credential to all of your storage buckets. If
    you''d prefer to limit the TPU service to only certain buckets, then you can put
    the TPU service account in each bucket''s **access control list** (**ACL**). You
    can do this for your training data bucket and if you want the training job to
    write to another bucket, then do the same for that as well:'
  prefs: []
  type: TYPE_NORMAL
- en: We can start by editing the bucket permissions, as shown in *Figure 5.8*. Select
    the **PERMISSIONS** tab:![Figure 5.8 – Editing the storage bucket permissions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0151.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.8 – Editing the storage bucket permissions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, click on **ADD**, as shown in *Figure 5.**9*:![Figure 5.9 – Adding the
    TPU service account to the bucket ACL
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0171.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.9 – Adding the TPU service account to the bucket ACL
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, add two new roles to the TPU service account by filling in the service
    account name, as in *Figure 5.10*. In this example, we will use the same bucket
    for hosting training data and writing training artifacts. Therefore, we need to
    add two roles from **Cloud Storage Legacy**: **Storage Legacy Bucket Reader**
    and **Storage Legacy Bucket Writer**:![Figure 5.10 – Whitelisting the TPU service
    account for two roles in the storage bucket'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0192.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 5.10 – Whitelisting the TPU service account for two roles in the storage
    bucket
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once these roles are added, click **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have completed the minimum administrative work in order to use the Cloud
    TPU for our model training workflow. In the next section, we are going to see
    how to refactor our code and set up the distributed training strategy for the
    TPU.
  prefs: []
  type: TYPE_NORMAL
- en: Execution command and format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI Platform also provides model training as a service. It enables users to
    submit a training job from their local environment''s command line. The job will
    run on Google Cloud''s compute cluster (with options for the CPU, TPU, or GPU
    in different pricing tiers). If you are new to the concept of training-as-a-service,
    refer to this link for details: [https://cloud.google.com/ai-platform/training/docs/training-jobs](https://cloud.google.com/ai-platform/training/docs/training-jobs).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides cloud training jobs, AI Platform can also do cloud inferencing jobs.
    The command we are going to run is for submitting a cloud training job. You can
    keep this link as a reference as you follow along with this chapter''s exercises:
    [https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training](https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training).
    Since the approach is to keep the training script in a client node (that is, a
    local computer with Google Cloud SDK installed), we need to let the Google Cloud
    runtime know, when executing `gcloud ai-platform jobs submit training`, where
    all the training scripts are. Also, because there are libraries that will be imported
    in our script, we need to specify the information, such as their versions and
    the library names and versions, in a file named `setup.py`. In order to accomplish
    this, it is necessary to create a small `setup.py` file in your working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the command terminal (for Mac OS X or Linux) of your working directory,
    type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we are ready to set up the command for distributed training in the
    Cloud TPU, let''s first take a look at the command and execution format. Recall
    that we stated earlier that this task will be executed using the Cloud SDK running
    in a client. In general, the client node will issue the `gcloud` command with
    input flags, such as this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are many more flags (user input parameters) available than shown here.
    For detailed descriptions of all the possible input parameters, refer to the TensorFlow
    Google Cloud AI Platform reference ([https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training](https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training))
    and the Cloud SDK documentation ([https://cloud.google.com/sdk/gcloud/reference/ai-platform](https://cloud.google.com/sdk/gcloud/reference/ai-platform)).
  prefs: []
  type: TYPE_NORMAL
- en: Cloud command arguments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The example command (discussed ahead in this section), illustrates a directory
    structure as shown in *Figure 5.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Directory structure and file organization in a local client
    for an example training run'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0211.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 – Directory structure and file organization in a local client for
    an example training run
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the folder names in the preceding figure are personal choices: `vs_code`,
    `python`, `ScriptProject`. You can name these folders to your preference. The
    training script named `traincloudtpu_resnet_cache` is also a personal choice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at this example command. This example command can be divided
    into two parts based on `-- \`. The first part of the command includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This command is executed in the `vs_code` directory shown in *Figure 5.11*.
    In this directory, you will find `setup.py`. This is the file that tells the `gcloud`
    runtime about the dependencies or packages required for the training script. `cloudtpu`
    is just a name we provided for this training run. We also need to specify a staging
    bucket (a cloud storage bucket) for serialization of model artifacts during and
    after training.
  prefs: []
  type: TYPE_NORMAL
- en: '`package-path` is the folder for organizing projects. In this case, within
    this package, we are interested in executing a training script, `traincloudtpu_resnet_cache.py`.
    In order for the `gcloud` runtime to find it, we need to specify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We then specify the TensorFlow Enterprise version to be 2.1 and the Python interpreter
    version to be 3.7, and a scale tier of `BASIC_TPU` should suffice for this example.
    We also set the region to be `us-central1`. The `BASIC_TPU` scale tier provides
    us with a master VM and a TPU VM with eight TPU V2 cores.
  prefs: []
  type: TYPE_NORMAL
- en: As stated earlier, `-- \` separates the `gcloud` system flags from any other
    user-defined flags that are specified and serve as input parameters to the training
    script. This separation is necessary and by design. Do not mix system flags with
    user-defined flags. See the SDK reference ([https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training](https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training))
    for details about positional arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the second half of this command, which consists
    of user-defined flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We specify `distribution_strategy=tpu` as a user-defined flag because we may
    use this value in conditional logic to select the proper distribution strategy.
    We also specify `model_dir`, which is a cloud storage path that we grant write
    permissions to the TPU service in order to serialize checkpoints and model assets.
    Then, for the remaining flags, we specify the number of epochs for training in
    `train_epochs`, and the path to the training data indicated by `data_dir`, which
    is also a cloud storage path that we grant read permissions to the TPU service.
    The TPU's distributed training strategy ([https://www.tensorflow.org/guide/distributed_training#tpustrategy](https://www.tensorflow.org/guide/distributed_training#tpustrategy))
    implements all necessary operations across multiple cores.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing the training script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The general structure of the training script for this example follows a minimalist
    style. In practice, it is common to organize Python code into multiple files and
    modules. Therefore, we will have everything we need in one Python script, `train.py`.
    Its pseudo-code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Once the `main()` routine is run, it will invoke `run()`, where a training strategy
    is defined, then a data pipeline is built, followed by building and training a
    model, then finally, it saves the results to cloud storage.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into the actual code for `train.py`. Let's start with the
    data streaming pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Data streaming pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Currently, the only way to stream a dataset when using Google Cloud AI Platform
    is through `tf.io` and `tf.dataTFRecordDataset`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our dataset (TFRecord) is already in a storage bucket. It is organized as shown
    in *Figure 5.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Cloud storage for the flower image classification dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image023.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12 – Cloud storage for the flower image classification dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In our training script''s `run` function, we need to specify the path to the
    cloud storage for the training data. We can leverage `tf.io.gfile` to encode the
    filename pattern for multiple parts. Then, we use `tf.data.TFRecordDataset` to
    instantiate a dataset object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As the preceding code indicates, after we encode the dataset name patterns,
    we then use `tf.data.Dataset.list_files` to encode a list of all the filenames
    that follow the pattern. Then, `tf.data.TFRecordDataset` instantiates a dataset
    reader object. Once these lines are executed during runtime, it effectively establishes
    the connection between the TPU and cloud storage. The dataset object streams the
    data into the model during the training workflow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why don't we use a `tf.keras` generator pattern, such as `ImageDataGenerator`
    or `flow_from_directory`? Well, it turns out this is not yet supported by the
    `gcloud ai-platform jobs submit training` command. This pattern is very convenient
    when the data is mounted or directly available in the filesystem, and it also
    easily takes care of the one-hot encoding of labels for classification problems,
    image normalization, and standardization processes via optional arguments through
    user input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We have to handle image standardization (resampling to a different height and
    width) by writing our own function. Here is a function that performs these operations
    on `TFRecordDataset`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This `decode_and_resize` function parses the dataset to a JPEG image with the
    corresponding color value range, then parse the labels, one-hot encodes the image,
    and resizes the image using the nearest neighbor method in order to standardize
    it to 224 by 224 pixels for our model of choice (ResNet). This function also provides
    different ways to return the label, whether it is as plain text or an integer.
    If you wish, you can return labels in different notations and styles by simply
    adding the notations of your interest to the `return` tuple:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, unpack the return tuple according to their position (the order as indicated
    in the preceding `return` statement) in the caller function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the `decode_and_resize` function ready, this is how we apply
    it to every element in our `dataset` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we rescale or normalize the pixel values to be in a range of `[0, 1]`
    in each image so that all the images are within same pixel range for training.
    Let''s create a `normalize` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to prepare the training data by applying a batch operation. We use
    the following function to achieve this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding function accepts a dataset, then shuffles it and batches it based
    on a global variable, `BATCH_SIZE`, and prefetches it for the training pipeline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We use the `map` method again to apply the `normalize` operation to our training
    and validation datasets:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the data pipeline part of the `run` function. We are not done with the
    `run` function yet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we are going to set up the model and conduct the training. We will leverage
    the popular transfer learning technique, where a prebuilt model is applied and
    trained with our own training dataset. The prebuilt model of interest here is
    the ResNet-50 image classification model. Remember how we already specified our
    TPU-based distributed strategy for training? We can simply wrap the model definition
    and optimizer choice here with the strategy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code describes the model architecture, designates the optimization
    strategy for training, and compiles the model. We use the ResNet-50 feature vector
    as our base model for the classification of the five flower types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we set up the checkpoint and callbacks with the help of the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Callbacks will save the model weights and biases for each epoch of training
    separately as checkpoints.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we need to set up sample sizes at each epoch for training and cross validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, at last, this is the code for the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This concludes the `run` function. This is a rather long function. Make sure
    you observe all the proper indentation demarcation. This is just a minimalist
    example for Google Cloud AI Platform. It includes all the necessary code and patterns
    for a scalable data pipeline, distributed training workflow, and TPU utilization.
    In your practice, you can organize and refactor your code to best suit your needs
    for clarity and maintainability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Submitting the training script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now is the time to submit our training script. We submit it from the `vs_code`
    directory according to the local directory organization mentioned in *Figure 5.11*.
    The TensorFlow runtime version in Cloud AI Platform is not necessarily most up
    to date with respect to the TensorFlow stable release in Cloud AI Notebook. As
    we know, the current stable release in Cloud Notebook is TFE 2.3\. However, in
    Cloud AI Platform, the most recent release is 2.2\. Therefore we use `--runtime-version=2.2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check this link to ascertain the latest runtime: [https://cloud.google.com/ai-platform/prediction/docs/runtime-version-list](https://cloud.google.com/ai-platform/prediction/docs/runtime-version-list).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the command and terminal output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Your job is still active. You can view the status of your job with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, you can continue streaming the logs with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command submits a piece of training code to the Cloud TPU. From
    the current directory (`vs_code`), there is a subfolder (`python`), which contains
    a `ScriptProject` module. In `ScriptProject`, there is a part of the script named
    `trainer.py`. You can see the content of `trainer.py` in the GitHub repo at [https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py](https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have to specify the following parameters, which are used in `trainer.py`
    ([https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py](https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as we submit the preceding command, it will be in the queue for execution
    in your Cloud AI Platform instance. To find out where we can monitor the training
    process, we can run `gcloud ai-platform` `jobs describe traincloudtpu_tfk_resnet50`
    to retrieve a URL to the running log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: You can view job in Cloud Console at [https://console.cloud.google.com/mlengine/jobs/traincloudtpu_tfk_resnet50?project=project1-190517](https://console.cloud.google.com/mlengine/jobs/traincloudtpu_tfk_resnet50?project=project1-190517).
  prefs: []
  type: TYPE_NORMAL
- en: You can view the logs at [https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Ftraincloudtpu_tfk_resnet50&project=project1-190517](https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Ftraincloudtpu_tfk_resnet50&project=project1-190517).
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the preceding code the preceding highlighted output, we can go to the
    logs URL in a browser and observe the training progress. The following are some
    example excerpts (see *Figure 5.13* and *Figure 5.14*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Google Cloud AI Platform TPU training log example excerpt 1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 – Google Cloud AI Platform TPU training log example excerpt 1
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a lengthy log that will run until the training job is complete. Toward
    the end of the training run, the log will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Google Cloud AI Platform TPU training log example excerpt 2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image027.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 – Google Cloud AI Platform TPU training log example excerpt 2
  prefs: []
  type: TYPE_NORMAL
- en: 'In the storage bucket, we see the folder created by the training workflow (*Figure
    5.15*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Folder created by the TPU training workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image029.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.15 – Folder created by the TPU training workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside this bucket, we see the checkpoints and model assets (*Figure 5.16*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Model checkpoints and assets after the training workflow is
    completed by the TPU'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 – Model checkpoints and assets after the training workflow is completed
    by the TPU
  prefs: []
  type: TYPE_NORMAL
- en: It is exactly the same as if the training is done on a local standalone machine.
    The complete `trainer.py` is available at [https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py](https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to take a look at how to reuse what we have learned here.
    As it turns out, if we want to use a model available in TensorFlow Hub, we can
    reuse the training patterns, file organization, and workflow. However, a slight
    change is required. This is because currently, the TPU has no direct access to
    TensorFlow Hub's module URL.
  prefs: []
  type: TYPE_NORMAL
- en: Working with models in TensorFlow Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TensorFlow Hub hosts a huge collection of pre-trained models. However, in order
    to use them, the user or client code must be able to connect to the hub and download
    the model via the RESTful API to the client''s TensorFlow runtime. Currently,
    this cannot be done with the TPU. Therefore, we have to download the model we
    are interested in from TensorFlow Hub to our local computer, then upload it to
    cloud storage, where it can be accessed by the TPU. Typically, the following is
    how you would implement a pre-trained model from TensorFlow Hub using the `tf.keras`
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding lines of code, the URL to a pre-trained model is
    passed into `KerasLayer`. However, currently, the TPU running in Cloud AI Platform
    has no direct access to TensorFlow Hub''s URL. To download the model, follow the
    simple instructions from TensorFlow Hub''s site, as shown in *Figure 5.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Downloading a pre-trained model from TensorFlow Hub'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image033.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.17 – Downloading a pre-trained model from TensorFlow Hub
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is compressed. Once it is extracted, you will see the content, as
    shown in *Figure 5.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Downloaded pre-trained model from TensorFlow Hub'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image035.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.18 – Downloaded pre-trained model from TensorFlow Hub
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is downloaded and extracted, let''s upload it to a storage bucket
    accessible by the TPU service account, as in *Figure 5.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Uploading the pre-trained model to cloud storage'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image037.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.19 – Uploading the pre-trained model to cloud storage
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we created a `model-cache-dir` folder in our storage bucket, then
    selected **UPLOAD FOLDER**, and the model folder is now available for the TPU
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, inside the `run` function, we need to leverage an environmental variable
    to tell the TPU where to find this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'This line can be inserted before the model definition in the `run` function.
    In the model definition, we will specify the model architecture by using `hub.KerasLayer`,
    as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we have the `TFHUB_CACHE_DIR` environmental variable already defined
    with our cloud storage name and path, when the TPU executes the `hub.KerasLayer`
    part of the model architecture code, the TPU runtime will look for the model from
    `TFHUB_CACHE_DIR` first instead of attempting to go through a RESTful API call
    to retrieve the model. After these small modifications are made to the training
    script, we can rename it as `trainer_hub.py`. The training work can be launched
    with a similar invocation style:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Your job is still active. You may view the status of your job with the command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: or continue streaming the logs with the command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: The complete `trainer_hub.py` code is in [https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/tfhub_on_tpu/tfhub_resnet_fv_on_tpu/trainer_hub.py](https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/tfhub_on_tpu/tfhub_resnet_fv_on_tpu/trainer_hub.py).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to take a look at how to use the `gcloud ai-platform` command
    to leverage the GPU for a similar training job.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Google Cloud GPU through AI Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having worked through the previous section for utilizing Cloud TPU with AI Platform,
    we are ready to do the same with the GPU. As it turns out, the formats of training
    script and invocation commands are very similar. With the exception of a few more
    parameters and slight differences in the distributed strategy definition, everything
    else remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several distributed strategies ([https://www.tensorflow.org/guide/distributed_training#types_of_strategies](https://www.tensorflow.org/guide/distributed_training#types_of_strategies))
    currently available. For a TensorFlow Enterprise distribution in Google AI Platform,
    `MirroredStrategy` and `TPUStrategy` are the only two that are fully supported.
    All the others are experimental. Therefore, in this section''s example, we will
    use `MirroredStrategy`. This strategy creates copies of all the variables in the
    model on each GPU. As these variables are updated at each gradient decent step,
    the value updates are copied to each GPU synchronously. By default, this strategy
    uses an `NVIDIA NCCL` all-reduce implementation. Now, we are going to start with
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start with small modifications to the TPU training script used in the
    previous section. Let''s implement a condition for selecting a distributed strategy
    based on the choice of TPU or GPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The full implementation of the training script for using `MirroredStrategy`
    with the GPU can be found at [https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/tfhub_on_gpu/tfhub_resnet_fv_on_gpu/trainer_hub_gpu.py](https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/tfhub_on_gpu/tfhub_resnet_fv_on_gpu/trainer_hub_gpu.py).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For `MirroredStrategy`, we will set `scale-tier` to `BASIC_GPU`. This will
    give us a single worker instance with one NVIDIA Tesla K80 GPU. The command to
    invoke training with `trainer_hub_gpu.py` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Your job is still active. You may view the status of your job with the command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: or continue streaming the logs with the command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we changed `scale-tier` to `BASIC_GPU`. We set our script-specific
    `distribution_strategy` flag to `gpu`. This is how we specify what compute instance
    we want and the distribution strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From all the examples that we have covered in this chapter, we learned how to
    leverage a distributed training strategy with the TPU and GPU through AI Platform,
    which runs on TensorFlow Enterprise 2.2 distributions. AI Platform is a service
    that wraps around TPU or GPU accelerator hardware and manages the configuration
    and setup for your training job.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, in Google AI Platform, the data ingestion pipeline relies on `TFRecordDataset`
    to stream training data in batches into the model training workflow. We also learned
    how to leverage a prebuilt model downloaded from TensorFlow Hub through the use
    of the `TFHUB_CACHE_DIR` environment variable. This is also the means to import
    your own saved model from an offline estate into Google AI Platform. Overall,
    in this platform, we used a TensorFlow Enterprise 2.2 distribution to achieve
    scalable data streaming and distributed training on Google Cloud's TPU or GPU
    and serialized all the model checkpoints and assets back to cloud storage.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will use Cloud AI Platform to submit a hyperparameter
    tuning job. Hyperparameter tuning is typically very time-consuming and resource-intensive.
    We will see how to take advantage of the compute power of the Cloud TPU for this
    process.
  prefs: []
  type: TYPE_NORMAL
