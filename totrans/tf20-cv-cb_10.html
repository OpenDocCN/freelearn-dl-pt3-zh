<html><head></head><body>
		<div id="_idContainer116">
			<h1 id="_idParaDest-272"><em class="italic"><a id="_idTextAnchor321"/>Chapter 10</em>: Applying the Power of Deep Learning to Videos</h1>
			<p>Computer vision is focused on the understanding of visual data. Of course, that includes videos, which, at their core, are a sequence of images, which means we can leverage most of our knowledge regarding deep learning for image processing to videos and reap great results.</p>
			<p>In this chapter, we'll start training a convolutional neuronal network to detect emotions in human faces, and then we'll learn how to apply it in a real-time context using our webcam.</p>
			<p>Then, in the remaining recipes, we'll use very advanced implementations of architectures, hosted in <strong class="bold">TensorFlow Hub</strong> (<strong class="bold">TFHub</strong>), specially tailored to tackle interesting video-related problems such as action recognition, frames generation, and text-to-video retrieval.</p>
			<p>Here are the recipes that we will be covering shortly:</p>
			<ul>
				<li>Detecting emotions in real time</li>
				<li>Recognizing actions with TensorFlow Hub</li>
				<li>Generating the middle frames of a video with TensorFlow Hub</li>
				<li>Performing text-to-video retrieval with TensorFlow Hub </li>
			</ul>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor322"/>Technical requirements</h1>
			<p>As usual, having access to a GPU is a great plus, particularly for the first recipe, where we'll implement a network from scratch. Because the rest of the chapter leverages models in TFHub, your CPU should be enough, although a GPU will give you a pretty nice speed boost! In the <em class="italic">Getting ready</em> section, you'll find the preparatory steps for each recipe. You can find the code for this chapter here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch10">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch10</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3qkTJ2l">https://bit.ly/3qkTJ2l</a>.</p>
			<h1 id="_idParaDest-274"><a id="_idTextAnchor323"/><a id="_idTextAnchor324"/>Detecting emotions in real time</h1>
			<p>At its most basic <a id="_idIndexMarker1014"/>form, a video is just a series of images. By leveraging this seemingly simple or trivial fact, we can adapt what we know about image classification to create very interesting video processing pipelines powered by deep learning. </p>
			<p>In this recipe, we'll build an algorithm to detect emotions in real time (webcam streaming) or from video files. Pretty interesting, right? </p>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor325"/>Getting ready</h2>
			<p>First, we must install several external libraries, such as <strong class="source-inline">OpenCV</strong> and <strong class="source-inline">imutils</strong>. Execute the following command to install them:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python imutils</p>
			<p>To train an emotion classifier network, we'll use the dataset from the Kaggle competition <strong class="bold">Challenges in Representation Learning: Facial Expression Recognition Challenge</strong>, which is available here: <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data</a>. You must sign in or sign up in order to download the dataset. Place the file in a location of your preference (we'll assume it's located in the <strong class="source-inline">~/.keras/datasets</strong> folder), extract it as <strong class="source-inline">emotion_recognition</strong>, and then unzip the <strong class="source-inline">fer2013.tar.gz</strong> file.</p>
			<p>Here are some sample images:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B14768_10_001.jpg" alt="Figure 10.1 – Sample images. Emotions from left to right: sad, angry, scared, surprised, happy, and neutral"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Sample images. Emotions from left to right: sad, angry, scared, surprised, happy, and neutral</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor326"/>How to do it…</h2>
			<p>By the end of this<a id="_idIndexMarker1015"/> recipe, you'll have your own emotion detector!</p>
			<ol>
				<li>Import all the dependencies:<p class="source-code">import csv</p><p class="source-code">import glob</p><p class="source-code">import pathlib</p><p class="source-code">import cv2</p><p class="source-code">import imutils</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow.keras.callbacks import ModelCheckpoint</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p><p class="source-code">from tensorflow.keras.utils import to_categorical</p></li>
				<li>Define a list of all possible emotions in our dataset, along with a color associated with each one:<p class="source-code">EMOTIONS = ['angry', 'scared', 'happy', 'sad', </p><p class="source-code">          'surprised','neutral']</p><p class="source-code">COLORS = {'angry': (0, 0, 255),</p><p class="source-code">    'scared': (0, 128, 255),</p><p class="source-code">    'happy': (0, 255, 255),</p><p class="source-code">    'sad': (255, 0, 0),</p><p class="source-code">    'surprised': (178, 255, 102),</p><p class="source-code">    'neutral': (160, 160, 160)</p><p class="source-code">}</p></li>
				<li>Define a <a id="_idIndexMarker1016"/>method to build the emotion classifier architecture. It receives the input shape and the number of classes in the dataset: <p class="source-code">def build_network(input_shape, classes):</p><p class="source-code">    input = Input(shape=input_shape)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               padding='same',</p><p class="source-code">               kernel_initializer='he_normal')(input)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=32,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               kernel_initializer='he_normal',</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x) </p></li>
				<li>Each block in the network is comprised of two ELU activated, batch-normalized convolutions, followed by a max pooling layer, and ending with a dropout layer. The block<a id="_idIndexMarker1017"/> defined previously had 32 filters per convolution, while the following one has 64 filters per convolution:<p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               kernel_initializer='he_normal',</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=64,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               kernel_initializer='he_normal',</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p></li>
				<li>The third block has 128 filters per convolution:<p class="source-code">    x = Conv2D(filters=128,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               kernel_initializer='he_normal',</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Conv2D(filters=128,</p><p class="source-code">               kernel_size=(3, 3),</p><p class="source-code">               kernel_initializer='he_normal',</p><p class="source-code">               padding='same')(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = MaxPooling2D(pool_size=(2, 2))(x)</p><p class="source-code">    x = Dropout(rate=0.25)(x)</p></li>
				<li>Next, we have <a id="_idIndexMarker1018"/>two dense, ELU activated, batch-normalized layers, also followed by a dropout, each with 64 units:<p class="source-code">    x = Flatten()(x)</p><p class="source-code">    x = Dense(units=64,</p><p class="source-code">              kernel_initializer='he_normal')(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.5)(x)</p><p class="source-code">    x = Dense(units=64,</p><p class="source-code">              kernel_initializer='he_normal')(x)</p><p class="source-code">    x = ELU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.5)(x)</p></li>
				<li>Finally, we encounter the output layer, with as many neurons as classes in the dataset. Of course, it's softmax-activated:<p class="source-code">    x = Dense(units=classes,</p><p class="source-code">              kernel_initializer='he_normal')(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return Model(input, output)</p></li>
				<li><strong class="source-inline">load_dataset()</strong> loads both the images and labels for the training, validation, and<a id="_idIndexMarker1019"/> test datasets:<p class="source-code">def load_dataset(dataset_path, classes):</p><p class="source-code">    train_images = []</p><p class="source-code">    train_labels = []</p><p class="source-code">    val_images = []</p><p class="source-code">    val_labels = []</p><p class="source-code">    test_images = []</p><p class="source-code">    test_labels = []</p></li>
				<li>The data in this dataset is in a CSV file, separated into <strong class="source-inline">emotion</strong>, <strong class="source-inline">pixels</strong>, and <strong class="source-inline">Usage</strong> columns. Let's parse the <strong class="source-inline">emotion</strong> column first. Although the dataset contains faces for seven classes, we'll combine <em class="italic">disgust</em> and <em class="italic">angry</em> (encoded as <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, respectively) because both share most of the facial features, and merging them leads to better results:<p class="source-code">    with open(dataset_path, 'r') as f:</p><p class="source-code">        reader = csv.DictReader(f)</p><p class="source-code">        for line in reader:</p><p class="source-code">            label = int(line['emotion'])</p><p class="source-code">            if label &lt;= 1:</p><p class="source-code">              label = 0  # This merges classes 1 and 0.</p><p class="source-code">            if label &gt; 0:</p><p class="source-code">              label -= 1  # All classes start from 0.</p></li>
				<li>Next, we parse<a id="_idIndexMarker1020"/> the <strong class="source-inline">pixels</strong> column, which is 2,034 whitespace-separated integers, corresponding to the grayscale pixels for the image (48x48=2034):<p class="source-code">            image = np.array(line['pixels'].split</p><p class="source-code">                                    (' '),</p><p class="source-code">                             dtype='uint8')</p><p class="source-code">            image = image.reshape((48, 48))</p><p class="source-code">            image = img_to_array(image)</p></li>
				<li>Now, to figure out to which subset this image and label belong, we must look at the <strong class="source-inline">Usage</strong> column:<p class="source-code">            if line['Usage'] == 'Training':</p><p class="source-code">                train_images.append(image)</p><p class="source-code">                train_labels.append(label)</p><p class="source-code">            elif line['Usage'] == 'PrivateTest':</p><p class="source-code">                val_images.append(image)</p><p class="source-code">                val_labels.append(label)</p><p class="source-code">            else:</p><p class="source-code">                test_images.append(image)</p><p class="source-code">                test_labels.append(label)</p></li>
				<li>Convert all the images to NumPy arrays:<p class="source-code">    train_images = np.array(train_images)</p><p class="source-code">    val_images = np.array(val_images)</p><p class="source-code">    test_images = np.array(test_images)</p></li>
				<li>Then, one-hot <a id="_idIndexMarker1021"/>encode all the labels:<p class="source-code">    train_labels = </p><p class="source-code">    to_categorical(np.array(train_labels),</p><p class="source-code">                                  classes)</p><p class="source-code">    val_labels = to_categorical(np.array(val_labels), </p><p class="source-code">                                 classes)</p><p class="source-code">    test_labels = to_categorical(np.array(test_labels),</p><p class="source-code">                                 classes)</p></li>
				<li>Return all the images and labels:<p class="source-code">    return (train_images, train_labels), \</p><p class="source-code">           (val_images, val_labels), \</p><p class="source-code">           (test_images, test_labels)</p></li>
				<li>Define a function to compute the area of a rectangle. We'll use this later to get the largest face detection:<p class="source-code">def rectangle_area(r):</p><p class="source-code">    return (r[2] - r[0]) * (r[3] - r[1])</p></li>
				<li>We'll now create a bar plot to display the probability distribution of the emotions detected in each frame. The following function is used to plot each bar, corresponding <a id="_idIndexMarker1022"/>to a particular emotion, in said plot:<p class="source-code">def plot_emotion(emotions_plot, emotion, probability, </p><p class="source-code">                 index):</p><p class="source-code">    w = int(probability * emotions_plot.shape[1])</p><p class="source-code">    cv2.rectangle(emotions_plot,</p><p class="source-code">                  (5, (index * 35) + 5),</p><p class="source-code">                  (w, (index * 35) + 35),</p><p class="source-code">                  color=COLORS[emotion],</p><p class="source-code">                  thickness=-1)</p><p class="source-code">    white = (255, 255, 255)</p><p class="source-code">    text = f'{emotion}: {probability * 100:.2f}%'</p><p class="source-code">    cv2.putText(emotions_plot,</p><p class="source-code">                text,</p><p class="source-code">                (10, (index * 35) + 23),</p><p class="source-code">                fontFace=cv2.FONT_HERSHEY_COMPLEX,</p><p class="source-code">                fontScale=0.45,</p><p class="source-code">                color=white,</p><p class="source-code">                thickness=2)</p><p class="source-code">    return emotions_plot</p></li>
				<li>We'll also draw a bounding box around the detected face, captioned with the recognized emotion:<p class="source-code">def plot_face(image, emotion, detection):</p><p class="source-code">    frame_x, frame_y, frame_width, frame_height = detection</p><p class="source-code">    cv2.rectangle(image,</p><p class="source-code">                  (frame_x, frame_y),</p><p class="source-code">                  (frame_x + frame_width,</p><p class="source-code">                   frame_y + frame_height),</p><p class="source-code">                  color=COLORS[emotion],</p><p class="source-code">                  thickness=2)</p><p class="source-code">    cv2.putText(image,</p><p class="source-code">                emotion,</p><p class="source-code">                (frame_x, frame_y - 10),</p><p class="source-code">                fontFace=cv2.FONT_HERSHEY_COMPLEX,</p><p class="source-code">                fontScale=0.45,</p><p class="source-code">                color=COLORS[emotion],</p><p class="source-code">                thickness=2)</p><p class="source-code">    return image</p></li>
				<li>Define<a id="_idIndexMarker1023"/> the <strong class="source-inline">predict_emotion()</strong> function, which takes the emotion classifier and an input image and returns the predictions output by the model:<p class="source-code">def predict_emotion(model, roi):</p><p class="source-code">    roi = cv2.resize(roi, (48, 48))</p><p class="source-code">    roi = roi.astype('float') / 255.0</p><p class="source-code">    roi = img_to_array(roi)</p><p class="source-code">    roi = np.expand_dims(roi, axis=0)</p><p class="source-code">    predictions = model.predict(roi)[0]</p><p class="source-code">    return predictions</p></li>
				<li>Load a saved model if there is one:<p class="source-code">checkpoints = sorted(list(glob.glob('./*.h5')), reverse=True)</p><p class="source-code">if len(checkpoints) &gt; 0:</p><p class="source-code">    model = load_model(checkpoints[0])</p></li>
				<li>Otherwise, train the model from scratch. First, build the path to the CSV with the data and then compute the number of classes in the dataset:<p class="source-code">else:</p><p class="source-code">    base_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">                 'datasets' /</p><p class="source-code">                 'emotion_recognition' / 'fer2013')</p><p class="source-code">    input_path = str(base_path / 'fer2013.csv')</p><p class="source-code">    classes = len(EMOTIONS)</p></li>
				<li>Then, load<a id="_idIndexMarker1024"/> each subset of data:<p class="source-code">    (train_images, train_labels), \</p><p class="source-code">    (val_images, val_labels), \</p><p class="source-code">    (test_images, test_labels) = load_dataset(input_path,</p><p class="source-code">                                              classes)</p></li>
				<li>Build the network and compile it. Also, define a <strong class="source-inline">ModelCheckpoint</strong> callback to save the best performing model, based on the validation loss:<p class="source-code">    model = build_network((48, 48, 1), classes)</p><p class="source-code">    model.compile(loss='categorical_crossentropy',</p><p class="source-code">                  optimizer=Adam(lr=0.003),</p><p class="source-code">                  metrics=['accuracy'])</p><p class="source-code">    checkpoint_pattern = ('model-ep{epoch:03d}-</p><p class="source-code">                          loss{loss:.3f}'</p><p class="source-code">                          '-val_loss{val_loss:.3f}.h5')</p><p class="source-code">    checkpoint = ModelCheckpoint(checkpoint_pattern,</p><p class="source-code">                                 monitor='val_loss',</p><p class="source-code">                                 verbose=1,</p><p class="source-code">                                 save_best_only=True,</p><p class="source-code">                                 mode='min')</p></li>
				<li>Define the augmenters and generator for the training and validation sets. Notice that we're only<a id="_idIndexMarker1025"/> augmenting the training set, while we just rescale the images in the validation set:<p class="source-code">    BATCH_SIZE = 128</p><p class="source-code">    train_augmenter = ImageDataGenerator(rotation_</p><p class="source-code">                            range=10,zoom_range=0.1,</p><p class="source-code">                              horizontal_flip=True,</p><p class="source-code">                                    rescale=1. / 255.,</p><p class="source-code">                                fill_mode='nearest')</p><p class="source-code">    train_gen = train_augmenter.flow(train_images,</p><p class="source-code">                                     train_labels,</p><p class="source-code">                                 batch_size=BATCH_SIZE)</p><p class="source-code">    train_steps = len(train_images) // BATCH_SIZE</p><p class="source-code">    val_augmenter = ImageDataGenerator(rescale=1. / 255.)</p><p class="source-code">    val_gen = val_augmenter.flow(val_images,val_labels,</p><p class="source-code">                         batch_size=BATCH_SIZE)</p></li>
				<li>Fit the model for 300 epochs and then evaluate it on the test set (we only rescale the<a id="_idIndexMarker1026"/> images in this subset):<p class="source-code">    EPOCHS = 300</p><p class="source-code">    model.fit(train_gen,</p><p class="source-code">              steps_per_epoch=train_steps,</p><p class="source-code">              validation_data=val_gen,</p><p class="source-code">              epochs=EPOCHS,</p><p class="source-code">              verbose=1,</p><p class="source-code">              callbacks=[checkpoint])</p><p class="source-code">   test_augmenter = ImageDataGenerator(rescale=1. / 255.)</p><p class="source-code">    test_gen = test_augmenter.flow(test_images,</p><p class="source-code">                                   test_labels,</p><p class="source-code">                                   batch_size=BATCH_SIZE)</p><p class="source-code">    test_steps = len(test_images) // BATCH_SIZE</p><p class="source-code">    _, accuracy = model.evaluate(test_gen, </p><p class="source-code">                                 steps=test_steps)</p><p class="source-code">    print(f'Accuracy: {accuracy * 100}%')</p></li>
				<li>Instantiate a <strong class="source-inline">cv2.VideoCapture()</strong> object to fetch the frames in a test video. If you want to use your webcam, replace <strong class="source-inline">video_path</strong> with <strong class="source-inline">0</strong>:<p class="source-code">video_path = 'emotions.mp4'</p><p class="source-code">camera = cv2.VideoCapture(video_path)  # Pass 0 to use webcam</p></li>
				<li>Create a <strong class="bold">Haar Cascades</strong> face <a id="_idIndexMarker1027"/>detector (this is a topic outside the scope of this book. If you <a id="_idIndexMarker1028"/>want to learn more about Haar Cascades, refer to the <em class="italic">See also</em> section in this recipe):<p class="source-code">cascade_file = 'resources/haarcascade_frontalface_default.xml'</p><p class="source-code">det = cv2.CascadeClassifier(cascade_file)</p></li>
				<li>Iterate over each frame in the video (or webcam stream), exiting only if there are no more frames to read, or if the user presses the Q key:<p class="source-code">while True:</p><p class="source-code">    frame_exists, frame = camera.read()</p><p class="source-code">    if not frame_exists:</p><p class="source-code">        break</p></li>
				<li>Resize the frame to have a width of 380 pixels (the height will be computed automatically to preserve the aspect ratio). Also, create a canvas of where to draw the emotions bar plot, and a copy of the input frame in terms of where to plot the detected faces:<p class="source-code">    frame = imutils.resize(frame, width=380)</p><p class="source-code">    emotions_plot = np.zeros_like(frame, </p><p class="source-code">                                  dtype='uint8')</p><p class="source-code">    copy = frame.copy()</p></li>
				<li>Because Haar Cascades work on grayscale images, we must convert the input frame to black and white. Then, we run the face detector on it:<p class="source-code">    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</p><p class="source-code">    detections = \</p><p class="source-code">        det.detectMultiScale(gray,scaleFactor=1.1,</p><p class="source-code">                             minNeighbors=5,</p><p class="source-code">                             minSize=(35, 35),</p><p class="source-code">                             </p><p class="source-code">                        flags=cv2.CASCADE_SCALE_IMAGE)</p></li>
				<li>Verify whether<a id="_idIndexMarker1029"/> there are any detections and fetch the one with the largest area:<p class="source-code">    if len(detections) &gt; 0:</p><p class="source-code">        detections = sorted(detections,</p><p class="source-code">                            key=rectangle_area)</p><p class="source-code">        best_detection = detections[-1]</p></li>
				<li>Extract the region of interest (<strong class="source-inline">roi</strong>) corresponding to the detected face and extract the emotions from it:<p class="source-code">        (frame_x, frame_y,</p><p class="source-code">         frame_width, frame_height) = best_detection</p><p class="source-code">        roi = gray[frame_y:frame_y + frame_height,</p><p class="source-code">                   frame_x:frame_x + frame_width]</p><p class="source-code">        predictions = predict_emotion(model, roi)</p><p class="source-code">        label = EMOTIONS[predictions.argmax()]</p></li>
				<li>Create the emotion distribution plot:<p class="source-code">        for i, (emotion, probability) in \</p><p class="source-code">                enumerate(zip(EMOTIONS, predictions)):</p><p class="source-code">            emotions_plot = plot_emotion(emotions_plot,</p><p class="source-code">                                         emotion,</p><p class="source-code">                                         probability,</p><p class="source-code">                                         i)</p></li>
				<li>Plot the detected<a id="_idIndexMarker1030"/> face along with the emotion it displays:<p class="source-code">        clone = plot_face(copy, label, best_detection)</p></li>
				<li>Show the result:<p class="source-code">    cv2.imshow('Face &amp; emotions',</p><p class="source-code">               np.hstack([copy, emotions_plot]))</p></li>
				<li>Check whether the user pressed Q, and if they did, break out of the loop:<p class="source-code">    if cv2.waitKey(1) &amp; 0xFF == ord('q'):</p><p class="source-code">        break</p></li>
				<li>Finally, release the resources:<p class="source-code">camera.release()</p><p class="source-code">cv2.destroyAllWindows()</p><p>After 300 epochs, I obtained a test accuracy of 65.74%. Here you can see some snapshots of the emotions detected in the test video: </p></li>
			</ol>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B14768_10_002.jpg" alt="Figure 10.2 – Emotions detected in two different snapshots&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Emotions detected in two different snapshots</p>
			<p>We can see<a id="_idIndexMarker1031"/> that the network correctly identifies sadness in the top frame, and happiness in the bottom one. Let's take a look at another example:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B14768_10_003.jpg" alt="Figure 10.3 – Emotions detected in three different snapshots&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Emotions detected in three different snapshots</p>
			<p>In the first frame, the girl clearly has a neutral expression, which was correctly picked up by the network. In the second frame, her face shows anger, which the classifier also detects. The third frame is more interesting, because her expression displays surprise, but it could also be interpreted as fear. Our detector seems to be split between these two emotions as well.</p>
			<p>Let's head over to the next section, shall we?</p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor327"/>How it works…</h2>
			<p>In this recipe, we implemented a fairly capable emotion detector for video streams, either from a built-in webcam, or a stored video file. We started by parsing the <strong class="source-inline">FER 2013</strong> data, which, unlike most other image datasets, is in CSV format. Then, we trained an emotion classifier on its images, achieving a respectable 65.74% accuracy on the test set. </p>
			<p>We must take into consideration the fact that facial expressions are tricky to interpret, even for <a id="_idIndexMarker1032"/>humans. At a given time, we might display mixed emotions. Also, there are many expressions that share traits, such as <em class="italic">anger</em> and <em class="italic">disgust</em>, and <em class="italic">fear</em> and <em class="italic">surprise</em>, among others.</p>
			<p>The last step in this first recipe consisted of passing each frame in the input video stream to a Haar Cascade face detector, and then getting the emotions, using the trained classifier, from the regions of interest corresponding to the detected faces.</p>
			<p>Although this approach works well for this particular problem, we must take into account that we overlooked a crucial assumption: each frame is independent. Simply put, we treated each frame in the video as an isolated image, but in reality, that's not the case when we're dealing with videos, because there's a time dimension that, when accounted for, yields more stable and better results.</p>
			<h2 id="_idParaDest-278"><a id="_idTextAnchor328"/>See also</h2>
			<p>Here's a great<a id="_idIndexMarker1033"/> resource for understanding the Haar Cascade classifier: <a href="https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html">https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html</a>.</p>
			<h1 id="_idParaDest-279"><a id="_idTextAnchor329"/>Recognizing actions with TensorFlow Hub</h1>
			<p>A very interesting<a id="_idIndexMarker1034"/> application of deep <a id="_idIndexMarker1035"/>learning to video processing involves action recognition. This is a challenging problem, because it not only presents the typical difficulties associated with classifying the contents of an image, but also includes a temporal component. An action in a video can vary depending on the order in which the frames are presented to us.</p>
			<p>The good news is that there is an architecture that is perfectly suited to this kind of problem, known <a id="_idIndexMarker1036"/>as <strong class="bold">Inflated 3D Convnet</strong> (<strong class="bold">I3D</strong>), and in this recipe we'll use a trained version hosted in TFHub to recognize actions in a varied selection of videos!</p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor330"/>Getting ready</h2>
			<p>We need to install several supplementary libraries, such as <strong class="source-inline">OpenCV</strong>, <strong class="source-inline">TFHub</strong>, and <strong class="source-inline">imageio</strong>. Execute the following command:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python tensorflow-hub imageio</p>
			<p>That's it! Let's begin implementing. </p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor331"/>How to do it…</h2>
			<p>Perform the following steps to complete the recipe:</p>
			<ol>
				<li value="1">Import all the required dependencies:<p class="source-code">import os</p><p class="source-code">import random</p><p class="source-code">import re</p><p class="source-code">import ssl</p><p class="source-code">import tempfile</p><p class="source-code">from urllib import request</p><p class="source-code">import cv2</p><p class="source-code">import imageio</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as tfhub</p><p class="source-code">from tensorflow_docs.vis import embed</p></li>
				<li>Define the path to the <strong class="source-inline">UCF101 – Action Recognition</strong> dataset, from where we'll fetch the test videos that we will pass to the model later on:<p class="source-code">UCF_ROOT = 'https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/'</p></li>
				<li>Define <a id="_idIndexMarker1037"/>the path to the labels file <a id="_idIndexMarker1038"/>of the <strong class="source-inline">Kinetics</strong> dataset, the one used to train the 3D convolutional network we'll use shortly:<p class="source-code">KINETICS_URL = ('https://raw.githubusercontent.com/deepmind/'</p><p class="source-code">                'kinetics-i3d/master/data/label_map.txt')</p></li>
				<li>Create a temporary directory to cache the downloaded resources:<p class="source-code">CACHE_DIR = tempfile.mkdtemp()</p></li>
				<li>Create an unverified SSL context. We need this to be able to download data from UCF's site (at the time of writing this book, it appears that their certificate has expired):<p class="source-code">UNVERIFIED_CONTEXT = ssl._create_unverified_context()</p></li>
				<li>Define the <strong class="source-inline">fetch_ucf_videos()</strong> function, which downloads the list of the possible videos we'll choose from to test our action recognizer:<p class="source-code">def fetch_ucf_videos():</p><p class="source-code">    index = \</p><p class="source-code">        (request</p><p class="source-code">         .urlopen(UCF_ROOT, </p><p class="source-code">                  context=UNVERIFIED_CONTEXT)</p><p class="source-code">         .read()</p><p class="source-code">         .decode('utf-8'))</p><p class="source-code">    videos = re.findall('(v_[\w]+\.avi)', index)</p><p class="source-code">    return sorted(set(videos))</p></li>
				<li>Define <a id="_idIndexMarker1039"/>the <strong class="source-inline">fetch_kinetics_labels()</strong> function, used to download and parse<a id="_idIndexMarker1040"/> the labels of the <strong class="source-inline">Kinetics</strong> dataset:<p class="source-code">def fetch_kinetics_labels():</p><p class="source-code">    with request.urlopen(KINETICS_URL) as f:</p><p class="source-code">        labels = [line.decode('utf-8').strip()</p><p class="source-code">                  for line in f.readlines()]</p><p class="source-code">    return labels</p></li>
				<li>Define the <strong class="source-inline">fetch_random_video()</strong> function, which selects a random video from our list of <strong class="source-inline">UCF101</strong> videos and downloads it to the temporary directory created in <em class="italic">Step 4</em>:<p class="source-code">def fetch_random_video(videos_list):</p><p class="source-code">    video_name = random.choice(videos_list)</p><p class="source-code">    cache_path = os.path.join(CACHE_DIR, video_name)</p><p class="source-code">    if not os.path.exists(cache_path):</p><p class="source-code">        url = request.urljoin(UCF_ROOT, video_name)</p><p class="source-code">        response = (request</p><p class="source-code">                    .urlopen(url,</p><p class="source-code">                             </p><p class="source-code">                     context=UNVERIFIED_CONTEXT)</p><p class="source-code">                    .read())</p><p class="source-code">        with open(cache_path, 'wb') as f:</p><p class="source-code">            f.write(response)</p><p class="source-code">    return cache_path</p></li>
				<li>Define the <strong class="source-inline">crop_center()</strong> function, which takes an image and crops a squared <a id="_idIndexMarker1041"/>selection corresponding <a id="_idIndexMarker1042"/>to the center of the received frame:<p class="source-code">def crop_center(frame):</p><p class="source-code">    height, width = frame.shape[:2]</p><p class="source-code">    smallest_dimension = min(width, height)</p><p class="source-code">    x_start = (width // 2) - (smallest_dimension // 2)</p><p class="source-code">    x_end = x_start + smallest_dimension</p><p class="source-code">    y_start = (height // 2) - (smallest_dimension // 2)</p><p class="source-code">    y_end = y_start + smallest_dimension</p><p class="source-code">    roi = frame[y_start:y_end, x_start:x_end]</p><p class="source-code">    return roi</p></li>
				<li>Define the <strong class="source-inline">read_video()</strong> function, which reads up to <strong class="source-inline">max_frames</strong> from a video stored in our cache and returns a list of all the read frames. It also crops the center <a id="_idIndexMarker1043"/>of each frame, resizes <a id="_idIndexMarker1044"/>it to 224x224x3 (the input shape expected by the network), and normalizes it:<p class="source-code">def read_video(path, max_frames=32, resize=(224, 224)):</p><p class="source-code">    capture = cv2.VideoCapture(path)</p><p class="source-code">    frames = []</p><p class="source-code">    while len(frames) &lt;= max_frames:</p><p class="source-code">        frame_read, frame = capture.read()</p><p class="source-code">        if not frame_read:</p><p class="source-code">            break</p><p class="source-code">        frame = crop_center(frame)</p><p class="source-code">        frame = cv2.resize(frame, resize)</p><p class="source-code">        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)</p><p class="source-code">        frames.append(frame)</p><p class="source-code">    capture.release()</p><p class="source-code">    frames = np.array(frames)</p><p class="source-code">    return frames / 255.</p></li>
				<li>Define the <strong class="source-inline">predict()</strong> function, used to get the top five most likely actions recognized by the model in the input video:<p class="source-code">def predict(model, labels, sample_video):</p><p class="source-code">    model_input = tf.constant(sample_video,</p><p class="source-code">                              dtype=tf.float32)</p><p class="source-code">    model_input = model_input[tf.newaxis, ...]</p><p class="source-code">    logits = model(model_input)['default'][0]</p><p class="source-code">    probabilities = tf.nn.softmax(logits)</p><p class="source-code">    print('Top 5 actions:')</p><p class="source-code">    for i in np.argsort(probabilities)[::-1][:5]:</p><p class="source-code">        print(f'{labels[i]}:  {probabilities[i] * 100:5.2f}%')</p></li>
				<li>Define<a id="_idIndexMarker1045"/> the <strong class="source-inline">save_as_gif()</strong> function, which takes a list of frames corresponding to a video, and<a id="_idIndexMarker1046"/> uses them to create a GIF representation:<p class="source-code">def save_as_gif(images, video_name):</p><p class="source-code">    converted_images = np.clip(images * 255, 0, 255)</p><p class="source-code">    converted_images = converted_images.astype(np.uint8)</p><p class="source-code">    imageio.mimsave(f'./{video_name}.gif',</p><p class="source-code">                    converted_images,</p><p class="source-code">                    fps=25)</p></li>
				<li>Fetch the videos and labels:<p class="source-code">VIDEO_LIST = fetch_ucf_videos()</p><p class="source-code">LABELS = fetch_kinetics_labels()</p></li>
				<li>Fetch a random video and read its frames:<p class="source-code">video_path = fetch_random_video(VIDEO_LIST)</p><p class="source-code">sample_video = read_video(video_path)</p></li>
				<li>Load <a id="_idIndexMarker1047"/>the<a id="_idIndexMarker1048"/> I3D from TFHub:<p class="source-code">model_path = 'https://tfhub.dev/deepmind/i3d-kinetics-400/1'</p><p class="source-code">model = tfhub.load(model_path)</p><p class="source-code">model = model.signatures['default']</p></li>
				<li>Finally, pass the video through the network to obtain the predictions, and then save the video as a GIF:<p class="source-code">predict(model, LABELS, sample_video)</p><p class="source-code">video_name = video_path.rsplit('/', maxsplit=1)[1][:-4]</p><p class="source-code">save_as_gif(sample_video, video_name)</p><p>Here's the first frame of the random video I obtained: </p></li>
			</ol>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B14768_10_004.jpg" alt="Figure 10.4 – Frame of the random UCF101 video&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Frame of the random UCF101 video</p>
			<p>And here are the top five predictions produced by the model:</p>
			<p class="source-code">Top 5 actions:</p>
			<p class="source-code">mopping floor:  75.29%</p>
			<p class="source-code">cleaning floor:  21.11%</p>
			<p class="source-code">sanding floor:   0.85%</p>
			<p class="source-code">spraying:   0.69%</p>
			<p class="source-code">sweeping floor:   0.64%</p>
			<p>It appears that <a id="_idIndexMarker1049"/>the network <a id="_idIndexMarker1050"/>understands that the action portrayed in the video has to do with the floor, because four out of five predictions have to do with it. However, <strong class="source-inline">mopping floor</strong> is the correct one.</p>
			<p>Let's now move to the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor332"/>How it works…</h2>
			<p>In this recipe, we leveraged the power of a 3D convolutional network to recognize actions in videos. A 3D convolution, as the name suggests, is a natural extension of a bi-dimensional convolution, which moves in two directions. Naturally, 3D convolutions consider width and height, but also depth, making them the perfect fit for special kinds of images, such as Magnetic Resonance Imaging (MRI) or, in this case, videos, which are just a series of images stacked together.</p>
			<p>We started by fetching a series of videos from the <strong class="source-inline">UCF101</strong> dataset and a set of action labels from the <strong class="source-inline">Kinetics</strong> dataset. It's important to remember that the I3D we downloaded from TFHub was trained on Kinetics. Therefore, the videos we passed to it are unseen.</p>
			<p>Next, we implemented a series of helper functions to obtain, preprocess, and shape each input video<a id="_idIndexMarker1051"/> in the way the I3D <a id="_idIndexMarker1052"/>expects. Then, we loaded the aforementioned network from TFHub and used it to display the top five actions it recognized in the video.</p>
			<p>One interesting extension you can make to this solution is to read custom videos from your filesystem, or better yet, pass a stream of images from your webcam to the network in order to see how well it performs!</p>
			<h2 id="_idParaDest-283"><a id="_idTextAnchor333"/>See also</h2>
			<p>I3D is a groundbreaking architecture for video processing, so I highly recommend you read the original paper here: <a href="https://arxiv.org/abs/1705.07750">https://arxiv.org/abs/1705.07750</a>. Here's a pretty interesting article that explains the difference between 1D, 2D, and 3D convolutions: <a href="https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610">https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610</a>. You can learn more about the <strong class="source-inline">UCF101</strong> dataset here: https://www.crcv.ucf.edu/data/UCF101.php. If you're interested in the <strong class="source-inline">Kinetics</strong> dataset, access this link: https://deepmind.com/research/open-source/kinetics. Lastly, you can find more details about the I3D implementation we used here: <a href="https://tfhub.dev/deepmind/i3d-kinetics-400/1">https://tfhub.dev/deepmind/i3d-kinetics-400/1</a>.</p>
			<h1 id="_idParaDest-284"><a id="_idTextAnchor334"/>Generating the middle frames of a video with TensorFlow Hub</h1>
			<p>Another <a id="_idIndexMarker1053"/>interesting <a id="_idIndexMarker1054"/>application of deep learning to videos involves frame generation. A fun and practical example of this technique is slow motion, where a network decides, based on the context, how to create intervening frames, thus expanding the length of a video and creating the illusion it was recorded with a high-speed camera (if you want to read more about it, refer to the <em class="italic">See also…</em> section). </p>
			<p>In this recipe, we'll use a 3D convolutional network to produce the middle frames of a video, given only its first and last frames. </p>
			<p>For this purpose, we'll rely on TFHub. </p>
			<p>Let's start this recipe.</p>
			<h2 id="_idParaDest-285"><a id="_idTextAnchor335"/>Getting ready</h2>
			<p>We must install TFHub and <strong class="source-inline">TensorFlow Datasets</strong>:</p>
			<p class="source-code">$&gt; pip install tensorflow-hub tensorflow-datasets</p>
			<p>The model we'll use was trained on the <strong class="source-inline">BAIR Robot Pushing Videos</strong> dataset, which is<a id="_idIndexMarker1055"/> available<a id="_idIndexMarker1056"/> in <strong class="source-inline">TensorFlow Datasets</strong>. However, if we access it through the library, we'll download way more data than we need for the purposes of this recipe. Instead, we'll use a smaller subset of the test set. Execute the following command to download it and place it inside the <strong class="source-inline">~/.keras/datasets/bair_robot_pushing</strong> folder:</p>
			<p class="source-code">$&gt; wget -nv https://storage.googleapis.com/download.tensorflow.org/data/bair_test_traj_0_to_255.tfrecords -O ~/.keras/datasets/bair_robot_pushing/traj_0_to_255.tfrecords</p>
			<p>Now we're all set! Let's begin implementing.</p>
			<h2 id="_idParaDest-286"><a id="_idTextAnchor336"/>How to do it…</h2>
			<p>Perform the <a id="_idIndexMarker1057"/>following steps to learn how to generate middle frames using <strong class="bold">Direct 3D Convolutions</strong>, through a model hosted in TFHub:</p>
			<ol>
				<li value="1">Import the dependencies:<p class="source-code">import pathlib</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as tfhub</p><p class="source-code">from tensorflow_datasets.core import SplitGenerator</p><p class="source-code">from tensorflow_datasets.video.bair_robot_pushing import \</p><p class="source-code">    BairRobotPushingSmall</p></li>
				<li>Define the <strong class="source-inline">plot_first_and_last_for_sample()</strong> function, which creates <a id="_idIndexMarker1058"/>a plot of <a id="_idIndexMarker1059"/>the first and last frames of a sample of four videos:<p class="source-code">def plot_first_and_last_for_sample(frames, batch_size):</p><p class="source-code">    for i in range(4):</p><p class="source-code">        plt.subplot(batch_size, 2, 1 + 2 * i)</p><p class="source-code">        plt.imshow(frames[i, 0] / 255.)</p><p class="source-code">        plt.title(f'Video {i}: first frame')</p><p class="source-code">        plt.axis('off')</p><p class="source-code">        plt.subplot(batch_size, 2, 2 + 2 * i)</p><p class="source-code">        plt.imshow(frames[i, 1] / 255.)</p><p class="source-code">        plt.title(f'Video {i}: last frame')</p><p class="source-code">        plt.axis('off')</p></li>
				<li>Define the <strong class="source-inline">plot_generated_frames_for_sample()</strong> function, which graphs the middle frames generated for a sample of four videos:<p class="source-code">def plot_generated_frames_for_sample(gen_videos):</p><p class="source-code">    for video_id in range(4):</p><p class="source-code">        fig = plt.figure(figsize=(10 * 2, 2))</p><p class="source-code">        for frame_id in range(1, 16):</p><p class="source-code">            ax = fig.add_axes(</p><p class="source-code">                [frame_id / 16., 0, (frame_id + 1) / </p><p class="source-code">                       16., 1],</p><p class="source-code">                xmargin=0, ymargin=0)</p><p class="source-code">            ax.imshow(gen_videos[video_id, frame_id])</p><p class="source-code">            ax.axis('off')</p></li>
				<li>We need <a id="_idIndexMarker1060"/>to <a id="_idIndexMarker1061"/>patch the <strong class="source-inline">BarRobotPushingSmall()</strong> (see <em class="italic">Step 6</em>) dataset builder to only expect the test split to be available, instead of both the training and test ones. Therefore, we must create a custom <strong class="source-inline">SplitGenerator()</strong>:<p class="source-code">def split_gen_func(data_path):</p><p class="source-code">    return [SplitGenerator(name='test',</p><p class="source-code">                           gen_kwargs={'filedir': </p><p class="source-code">                                       data_path})]</p></li>
				<li>Define the path to the data:<p class="source-code">DATA_PATH = str(pathlib.Path.home() / '.keras' / </p><p class="source-code">                   'datasets' /</p><p class="source-code">                'bair_robot_pushing')</p></li>
				<li>Create a <strong class="source-inline">BarRobotPushingSmall()</strong> builder, pass it the custom split generator created in <em class="italic">Step 4</em>, and then prepare the dataset:<p class="source-code">builder = BairRobotPushingSmall()</p><p class="source-code">builder._split_generators = lambda _:split_gen_func(DATA_PATH)</p><p class="source-code">builder.download_and_prepare()</p></li>
				<li>Get the first batch of videos:<p class="source-code">BATCH_SIZE = 16</p><p class="source-code">dataset = builder.as_dataset(split='test')</p><p class="source-code">test_videos = dataset.batch(BATCH_SIZE)</p><p class="source-code">for video in test_videos:</p><p class="source-code">    first_batch = video</p><p class="source-code">    break</p></li>
				<li>Keep <a id="_idIndexMarker1062"/>only the <a id="_idIndexMarker1063"/>first and last frame of each video in the batch:<p class="source-code">input_frames = first_batch['image_aux1'][:, ::15]</p><p class="source-code">input_frames = tf.cast(input_frames, tf.float32)</p></li>
				<li>Load the generator model from TFHub:<p class="source-code">model_path = 'https://tfhub.dev/google/tweening_conv3d_bair/1'</p><p class="source-code">model = tfhub.load(model_path)</p><p class="source-code">model = model.signatures['default']</p></li>
				<li>Pass the batch of videos through the model to generate the middle frames:<p class="source-code">middle_frames = model(input_frames)['default']</p><p class="source-code">middle_frames = middle_frames / 255.0</p></li>
				<li>Concatenate the first and last frames of each video in the batch with the corresponding middle frames produced by the network in <em class="italic">Step 10</em>:<p class="source-code">generated_videos = np.concatenate(</p><p class="source-code">    [input_frames[:, :1] / 255.0,  # All first frames</p><p class="source-code">     middle_frames,  # All inbetween frames</p><p class="source-code">     input_frames[:, 1:] / 255.0],  # All last frames</p><p class="source-code">    axis=1)</p></li>
				<li>Finally, plot the first and last frames, and also the middle frames:<p class="source-code">plt.figure(figsize=(4, 2 * BATCH_SIZE))</p><p class="source-code">plot_first_and_last_for_sample(input_frames, </p><p class="source-code">                                BATCH_SIZE)</p><p class="source-code">plot_generated_frames_for_sample(generated_videos)</p><p class="source-code">plt.show()</p><p>In <em class="italic">Figure 10.5</em>, we <a id="_idIndexMarker1064"/>can <a id="_idIndexMarker1065"/>observe the first and last frame of each video in our sample of four:</p></li>
			</ol>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B14768_10_005.jpg" alt="Figure 10.5 – First and last frame of each video in the sample&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – First and last frame of each video in the sample</p>
			<p>In <em class="italic">Figure 10.6</em>, we observe the 14 middle frames generated by the model for each video. Close inspection reveals they are coherent with the first and last real <a id="_idIndexMarker1066"/>frames<a id="_idIndexMarker1067"/> passed to the network:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B14768_10_006.jpg" alt="Figure 10.6 – Middle frames produced by the model for each sample video&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Middle frames produced by the model for each sample video</p>
			<p>Let's go to the <em class="italic">How it works…</em> section to review what we did.</p>
			<h2 id="_idParaDest-287"><a id="_idTextAnchor337"/>How it works…</h2>
			<p>In this recipe, we learned about another useful and interesting application of deep learning to videos, particularly 3D convolutional networks, in the context of generative models.</p>
			<p>We took a state-of-the-art architecture trained on the <strong class="source-inline">BAIR Robot Pushing Videos</strong> dataset, hosted in TFHub, and used it to produce an entirely new video sequence, taking only as seeds the first and last frames of a video.</p>
			<p>Because downloading the entire 30 GBs of the <strong class="source-inline">BAIR</strong> dataset would have been an overkill, given we only needed a way smaller subset to test our solution, we couldn't rely directly on the TensorFlow dataset's <strong class="source-inline">load()</strong> method. Instead, we downloaded a subset of the test videos and made the necessary adjustments to the <strong class="source-inline">BairRobotPushingSmall()</strong> builder to load and prepare the sample videos.</p>
			<p>It must be <a id="_idIndexMarker1068"/>mentioned<a id="_idIndexMarker1069"/> that this model was trained on a very specific dataset, but it certainly showcases the powerful generation capabilities of this architecture. I encourage you to check out the <em class="italic">See also</em> section for a list of useful resources that could be of help if you want to implement a video generation network on your own data.</p>
			<h2 id="_idParaDest-288"><a id="_idTextAnchor338"/>See also</h2>
			<p>You can learn more about the <strong class="source-inline">BAIR Robot Pushing Videos</strong> dataset here: <a href="https://arxiv.org/abs/1710.05268">https://arxiv.org/abs/1710.05268</a>. I encourage you to read the paper entitled <strong class="bold">Video Inbetweening Using Direct 3D Convolutions</strong>, where the network we used in this recipe was proposed: https://arxiv.org/abs/1905.10240. You can find the TFHub model we relied on at the following link: https://tfhub.dev/google/tweening_conv3d_bair/1. Lastly, here's an interesting read about an AI that transforms regular footage into slow motion: <a href="https://petapixel.com/2020/09/08/this-ai-can-transform-regular-footage-into-slow-motion-with-no-artifacts/">https://petapixel.com/2020/09/08/this-ai-can-transform-regular-footage-into-slow-motion-with-no-artifacts/</a>.</p>
			<h1 id="_idParaDest-289">Perfo<a id="_idTextAnchor339"/><a id="_idTextAnchor340"/>rming text-to-video retrieval with TensorFlow Hub</h1>
			<p>The applications of<a id="_idIndexMarker1070"/> deep <a id="_idIndexMarker1071"/>learning to videos are not limited to classification, categorization, or even generation. One of the biggest resources of neural networks is their internal representation of data features. The better a network is at a given task, the better their internal mathematical model is. We can take advantage of the inner workings of state-of-the-art models to build interesting applications on top of them.</p>
			<p>In this recipe, we'll create a small search engine based on the embeddings produced by an <strong class="bold">S3D</strong> model, trained and ready to be used, which lives in TFHub.</p>
			<p>Are you ready? Let's begin!</p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor341"/>Getting ready</h2>
			<p>First, we must install <strong class="source-inline">OpenCV</strong> and TFHub, as follows:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python tensorflow-hub</p>
			<p>That's all <a id="_idIndexMarker1072"/>we need, so let's<a id="_idIndexMarker1073"/> start this recipe!</p>
			<h2 id="_idParaDest-291"><a id="_idTextAnchor342"/>How to do it…</h2>
			<p>Perform the following steps to learn how to perform text-to-video retrieval using TFHub:</p>
			<ol>
				<li value="1">The first step is to import all the dependencies that we'll use:<p class="source-code">import math</p><p class="source-code">import os</p><p class="source-code">import uuid</p><p class="source-code">import cv2</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as tfhub</p><p class="source-code">from tensorflow.keras.utils import get_file</p></li>
				<li>Define a function to produce the text and video embeddings using an instance of S3D:<p class="source-code">def produce_embeddings(model, input_frames, input_words):</p><p class="source-code">    frames = tf.cast(input_frames, dtype=tf.float32)</p><p class="source-code">    frames = tf.constant(frames)</p><p class="source-code">    video_model = model.signatures['video']</p><p class="source-code">    video_embedding = video_model(frames)</p><p class="source-code">    video_embedding = video_embedding['video_embedding']</p><p class="source-code">    words = tf.constant(input_words)</p><p class="source-code">    text_model = model.signatures['text']</p><p class="source-code">    text_embedding = text_model(words)</p><p class="source-code">    text_embedding = text_embedding['text_embedding']</p><p class="source-code">    return video_embedding, text_embedding</p></li>
				<li>Define<a id="_idIndexMarker1074"/> the <strong class="source-inline">crop_center()</strong> function, which<a id="_idIndexMarker1075"/> takes an image and crops a squared selection corresponding to the center of the received frame:<p class="source-code">def crop_center(frame):</p><p class="source-code">    height, width = frame.shape[:2]</p><p class="source-code">    smallest_dimension = min(width, height)</p><p class="source-code">    x_start = (width // 2) - (smallest_dimension // 2)</p><p class="source-code">    x_end = x_start + smallest_dimension</p><p class="source-code">    y_start = (height // 2) - (smallest_dimension // </p><p class="source-code">                                    2)</p><p class="source-code">    y_end = y_start + smallest_dimension</p><p class="source-code">    roi = frame[y_start:y_end, x_start:x_end]</p><p class="source-code">    return roi</p></li>
				<li>Define the <strong class="source-inline">fetch_and_read_video()</strong> function, which, as its name indicates, downloads<a id="_idIndexMarker1076"/> a <a id="_idIndexMarker1077"/>video and then reads it. For this last part, we use OpenCV. Let's start by getting the video from a given URL:<p class="source-code">def fetch_and_read_video(video_url,</p><p class="source-code">                         max_frames=32,</p><p class="source-code">                         resize=(224, 224)):</p><p class="source-code">    extension = video_url.rsplit(os.path.sep,</p><p class="source-code">                                 maxsplit=1)[-1]</p><p class="source-code">    path = get_file(f'{str(uuid.uuid4())}.{extension}',</p><p class="source-code">                    video_url,</p><p class="source-code">                    cache_dir='.',</p><p class="source-code">                    cache_subdir='.')</p><p>We extract the video format from the URL. Then, we save the video in the current folder, with a random UUID as its name.</p></li>
				<li>Next, we'll load <strong class="source-inline">max_frames</strong> of this fetched video: <p class="source-code">    capture = cv2.VideoCapture(path)</p><p class="source-code">    frames = []</p><p class="source-code">    while len(frames) &lt;= max_frames:</p><p class="source-code">        frame_read, frame = capture.read()</p><p class="source-code">        if not frame_read:</p><p class="source-code">            break</p><p class="source-code">        frame = crop_center(frame)</p><p class="source-code">        frame = cv2.resize(frame, resize)</p><p class="source-code">        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)</p><p class="source-code">        frames.append(frame)</p><p class="source-code">    capture.release()</p><p class="source-code">    frames = np.array(frames)</p></li>
				<li>If the video <a id="_idIndexMarker1078"/>doesn't <a id="_idIndexMarker1079"/>have enough frames, we'll repeat the process until we reach the desired capacity:<p class="source-code">    if len(frames) &lt; max_frames:</p><p class="source-code">        repetitions = math.ceil(float(max_frames) /        </p><p class="source-code">                                len(frames))</p><p class="source-code">        repetitions = int(repetitions)</p><p class="source-code">        frames = frames.repeat(repetitions, axis=0)</p></li>
				<li>Return the normalized frames:<p class="source-code">    frames = frames[:max_frames]</p><p class="source-code">    return frames / 255.0</p></li>
				<li>Define the URLs of the videos:<p class="source-code">URLS = [</p><p class="source-code">    ('https://media.giphy.com/media/'</p><p class="source-code">     'WWYSFIZo4fsLC/source.gif'),</p><p class="source-code">    ('https://media.giphy.com/media/'</p><p class="source-code">     'fwhIy2QQtu5vObfjrs/source.gif'),</p><p class="source-code">    ('https://media.giphy.com/media/'</p><p class="source-code">     'W307DdkjIsRHVWvoFE/source.gif'),</p><p class="source-code">    ('https://media.giphy.com/media/'</p><p class="source-code">     'FOcbaDiNEaqqY/source.gif'),</p><p class="source-code">    ('https://media.giphy.com/media/'</p><p class="source-code">     'VJwck53yG6y8s2H3Og/source.gif')]</p></li>
				<li>Fetch<a id="_idIndexMarker1080"/> and read<a id="_idIndexMarker1081"/> each video: <p class="source-code">VIDEOS = [fetch_and_read_video(url) for url in URLS]</p></li>
				<li>Define the queries (captions) associated with each video. Notice that they must be in the correct order:<p class="source-code">QUERIES = ['beach', 'playing drums', 'airplane taking </p><p class="source-code">              off',</p><p class="source-code">           'biking', 'dog catching frisbee']</p></li>
				<li>Load S3D from TFHub:<p class="source-code">model = tfhub.load</p><p class="source-code">('https://tfhub.dev/deepmind/mil-nce/s3d/1')</p></li>
				<li>Obtain the text and video embeddings:<p class="source-code">video_emb, text_emb = produce_embeddings(model,</p><p class="source-code">                              np.stack(VIDEOS, axis=0),</p><p class="source-code">                                         np.array(QUERIES))</p></li>
				<li>Compute the similarity scores between the text and video embeddings:<p class="source-code">scores = np.dot(text_emb, tf.transpose(video_emb))</p></li>
				<li>Take the first frame of each video, rescale it back to [0, 255], and then convert it to BGR <a id="_idIndexMarker1082"/>space so <a id="_idIndexMarker1083"/>that we can display it with OpenCV. We do this to display the results of our experiment:<p class="source-code">first_frames = [v[0] for v in VIDEOS]</p><p class="source-code">first_frames = [cv2.cvtColor((f * 255.0).astype('uint8'),</p><p class="source-code">                             cv2.COLOR_RGB2BGR) for f </p><p class="source-code">                                in  first_frames]</p></li>
				<li>Iterate over each (query, video, score) triplet and display the most similar videos for each query:<p class="source-code">for query, video, query_scores in zip(QUERIES,VIDEOS,scores):</p><p class="source-code">    sorted_results = sorted(list(zip(QUERIES,</p><p class="source-code">                                     first_frames,</p><p class="source-code">                                     query_scores)),</p><p class="source-code">                            key=lambda p: p[-1],</p><p class="source-code">                            reverse=True)</p><p class="source-code">    annotated_frames = []</p><p class="source-code">    for i, (q, f, s) in enumerate(sorted_results, </p><p class="source-code">                                 start=1):</p><p class="source-code">        frame = f.copy()</p><p class="source-code">        cv2.putText(frame,</p><p class="source-code">                    f'#{i} - Score: {s:.2f}',</p><p class="source-code">                    (8, 15),</p><p class="source-code">                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,</p><p class="source-code">                    fontScale=0.6,</p><p class="source-code">                    color=(0, 0, 255),</p><p class="source-code">                    thickness=2)</p><p class="source-code">        annotated_frames.append(frame)</p><p class="source-code">    cv2.imshow(f'Results for query “{query}”',</p><p class="source-code">               np.hstack(annotated_frames))</p><p class="source-code">    cv2.waitKey(0)</p><p>First, we'll<a id="_idIndexMarker1084"/> see <a id="_idIndexMarker1085"/>the result of the <em class="italic">beach</em> query:</p></li>
			</ol>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B14768_10_007.jpg" alt="Figure 10.7 – Ranked results for the BEACH query&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Ranked results for the BEACH query</p>
			<p>As expected, the first result, which is the highest score, is an image of a beach. Let's now try with <em class="italic">playing drums</em>:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B14768_10_008.jpg" alt="Figure 10.8 – Ranked results for the PLAYING DRUMS query&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Ranked results for the PLAYING DRUMS query</p>
			<p>Awesome! It <a id="_idIndexMarker1086"/>seems<a id="_idIndexMarker1087"/> that the similarity between the query text and the images is stronger in this instance. Up next, a more difficult one:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B14768_10_009.jpg" alt="Figure 10.9 – Ranked results for the AIRPLANE TAKING OFF query&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Ranked results for the AIRPLANE TAKING OFF query</p>
			<p>Although <em class="italic">airplane taking off</em> is a somewhat more complex query, our solution had no problem producing the correct results. Let's now try with <em class="italic">biking</em>:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B14768_10_010.jpg" alt="Figure 10.10 – Ranked results for the BIKING query&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Ranked results for the BIKING query</p>
			<p>Another match! How about <em class="italic">dog catching frisbee</em>?</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B14768_10_011.jpg" alt="Figure 10.11 – Ranked results for the DOG CATCHING FRISBEE query&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – Ranked results for the DOG CATCHING FRISBEE query</p>
			<p>No problem <a id="_idIndexMarker1088"/>at <a id="_idIndexMarker1089"/>all! The satisfying results we've seen are due to the great job S3D does at mapping images with the words that best describe them. If you have read the paper where S3D was introduced, you won't be surprised by this fact, given the humongous amount of data it was trained on.</p>
			<p>Let's now proceed to the next section.</p>
			<h2 id="_idParaDest-292"><a id="_idTextAnchor343"/>How it works…</h2>
			<p>In this recipe, we exploited the ability of the S3D model to generate embeddings, both for text and video, to create a small database we used as the basis of a toy search engine. This<a id="_idIndexMarker1090"/> way, we<a id="_idIndexMarker1091"/> demonstrated the usefulness of having a network capable of producing richly informative vectorial two-way mappings between images and text.</p>
			<h2 id="_idParaDest-293"><a id="_idTextAnchor344"/>See also</h2>
			<p>I highly recommend that you read the paper where the model we used in this recipe was published as it's very interesting! Here's the link: https://arxiv.org/pdf/1912.06430.pdf. Speaking of the model, you'll find it here: https://tfhub.dev/deepmind/mil-nce/s3d/1.</p>
		</div>
	</body></html>