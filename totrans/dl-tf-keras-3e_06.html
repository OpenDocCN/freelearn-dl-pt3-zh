<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer246">
<h1 class="chapterNumber">6</h1>
<h1 class="chapterTitle" id="_idParaDest-144">Transformers</h1>
<p class="normal">The transformer-based architectures have become almost universal in <strong class="keyWord">Natural Language Processing </strong>(<strong class="keyWord">NLP</strong>) (and beyond) when it comes to solving a wide variety of tasks, such as:</p>
<ul>
<li class="bulletList">Neural machine translation</li>
<li class="bulletList">Text summarization</li>
<li class="bulletList">Text generation</li>
<li class="bulletList">Named entity recognition</li>
<li class="bulletList">Question answering</li>
<li class="bulletList">Text classification</li>
<li class="bulletList">Text similarity</li>
<li class="bulletList">Offensive message/profanity detection</li>
<li class="bulletList">Query understanding </li>
<li class="bulletList">Language modeling</li>
<li class="bulletList">Next-sentence prediction</li>
<li class="bulletList">Reading comprehension</li>
<li class="bulletList">Sentiment analysis</li>
<li class="bulletList">Paraphrasing</li>
</ul>
<p class="normal">and a lot more.</p>
<p class="normal">In less than four years, when the <em class="italic">Attention Is All You Need</em> paper was published by Google Research in 2017, transformers managed to take the NLP community by storm, breaking any record achieved over the previous thirty years.</p>
<p class="normal">Transformer-based models use the so-called attention mechanisms that identify complex relationships between words in each input sequence, such as a sentence. Attention helped resolve the challenge of encoding “pairwise correlations”—something that its “predecessors,” such as LSTM RNNs and even CNNS, couldn’t achieve when modeling sequential data, such as text.</p>
<p class="normal">Models—such as BERT, T5, and GPT (covered in more detail later in this chapter)—now constitute the state-of-the-art fundamental building blocks for new applications in almost every field, from computer vision to speech recognition, translation, or protein and coding sequences. Attention has also been applied in reinforcement learning for games: in DeepMind’s AlphaStar (<a href="https://rdcu.be/bVI7G"><span class="url">https://rdcu.be/bVI7G</span></a> and <a href="https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning"><span class="url">https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning</span></a>), observations of player and opponent StarCraft game units were processed with self-attention, for example. For this reason, Stanford has recently introduced the term “foundation models” to define a set of <strong class="keyWord">Large Language Models</strong> (<strong class="keyWord">LLMs</strong>) based on giant pretrained transformers.</p>
<p class="normal">This progress has been made thanks to a few simple ideas, which we are going to review in the next few sections.</p>
<p class="normal">You will learn:</p>
<ul>
<li class="bulletList">What transformers are</li>
<li class="bulletList">How they evolved over time</li>
<li class="bulletList">Some optimization techniques</li>
<li class="bulletList">Dos and don’ts </li>
<li class="bulletList">What the future will look like</li>
</ul>
<p class="normal">Let’s start turning our attention to transformers. You will be surprised to discover that attention indeed is all you need!</p>
<h1 class="heading-1" id="_idParaDest-145">Architecture</h1>
<p class="normal">Even though a typical transformer<a id="_idIndexMarker605"/> architecture is usually different from that of recurrent networks, it is based on several key ideas that originated in RNNs. At the time of writing this book, the transformer represents the next evolutionary step of deep learning architectures related to texts and any data that can be represented as sequences, and as such, it should be an essential part of your toolbox.</p>
<p class="normal">The original transformer architecture is a variant of the encoder-decoder architecture, where the recurrent layers are replaced with (self-)attention layers. The transformer was initially proposed by Google in the seminal paper titled <em class="italic">Attention Is All You Need</em> by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, 2017, <a href="https://arxiv.org/abs/1706.03762"><span class="url">https://arxiv.org/abs/1706.03762</span></a>, to which a reference implementation was provided, which we will refer to throughout this discussion.</p>
<p class="normal">The architecture is an instance of the encoder-decoder models that have been popular since 2014-2015 (such as <em class="italic">Sequence to Sequence Learning with Neural Networks</em> by Sutskever et al. (2014), <a href="https://arxiv.org/abs/1409.3215"><span class="url">https://arxiv.org/abs/1409.3215</span></a>). Prior to that, attention had been used together<a id="_idIndexMarker606"/> with <strong class="keyWord">Long-Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>) and other <strong class="keyWord">RNN</strong> (<strong class="keyWord">Recurrent Neural Network</strong>) models<a id="_idIndexMarker607"/> discussed in a previous chapter. Attention was introduced in 2014 in <em class="italic">Neural Machine Translation by Jointly Learning to Align and Translate</em> by Bahdanau et al., <a href="https://arxiv.org/abs/1409.0473"><span class="url">https://arxiv.org/abs/1409.0473</span></a>, and applied to neural machine translation in 2015 in <em class="italic">Effective Approaches to Attention-based Neural Machine Translation</em> by Luong et al., <a href="https://arxiv.org/abs/1508.04025"><span class="url">https://arxiv.org/abs/1508.04025</span></a>, and there have been other combinations of attention with other types of models. </p>
<p class="normal">In 2017, the first transformer <a id="_idIndexMarker608"/>demonstrated that you<a id="_idIndexMarker609"/> could remove LSTMs from <strong class="keyWord">Neural Machine Translation</strong> (<strong class="keyWord">NMT</strong>) models and use the so-called (self-)attention blocks (hence the paper title <em class="italic">Attention Is All You Need</em>).</p>
<h2 class="heading-2" id="_idParaDest-146">Key intuitions</h2>
<p class="normal">Let’s start by defining some concepts that will be useful later on in this chapter. The innovation introduced with the transformer in 2017 is based on four main key ideas:</p>
<ul>
<li class="bulletList">Positional encoding</li>
<li class="bulletList">Attention</li>
<li class="bulletList">Self-attention</li>
<li class="bulletList">Multi-head (self-)attention</li>
</ul>
<p class="normal">In the next sections, we will discuss them in greater detail.</p>
<h3 class="heading-3" id="_idParaDest-147">Positional encoding</h3>
<p class="normal">RNNs keep the word order <a id="_idIndexMarker610"/>by processing words sequentially. The advantage of this approach is simplicity, but one of the disadvantages is that this makes parallelization hard (training on multiple hardware accelerators). If we want to effectively leverage highly parallel architectures, such as GPUs and TPUs, we’d need an alternative way to represent ordering.</p>
<p class="normal">The transformer uses a simple alternative order representation called positional encoding, which associates each word with a number representing its position in the text. For instance:</p>
<pre class="programlisting code"><code class="hljs-code">[("Transformers", 1), ("took", 2), ("NLP", 3), ("by", 4), ("storm", 5)]
</code></pre>
<p class="normal">The key intuition is that enriching transformers with a position allows the model to learn the importance of the position of each token (a word in the text/sentence). Note that positional encoding existed before transformers (as discussed in the chapter on RNNs), but this intuition is particularly important in the context of creating transformer-based models. After (absolute) positional encoding was introduced in the original transformer paper, there have been other variants, such as relative positional encoding (<em class="italic">Self-Attention with Relative Position Representations</em> by Shaw et al., 2018, <a href="https://arxiv.org/abs/1803.02155"><span class="url">https://arxiv.org/abs/1803.02155</span></a>, and rotary positional encoding (<em class="italic">RoFormer: Enhanced Transformer with Rotary Position Embedding</em> by Su et al., 2021, <a href="https://arxiv.org/abs/2104.09864"><span class="url">https://arxiv.org/abs/2104.09864</span></a>).</p>
<p class="normal">Now that we have defined positional encoding, let’s turn our attention to the <em class="italic">attention</em> mechanism.</p>
<h3 class="heading-3" id="_idParaDest-148">Attention</h3>
<p class="normal">Another crucial ingredient of<a id="_idIndexMarker611"/> the transformer recipe is attention. This mechanism was first introduced in the context of machine translation in 2014 by Bahdanou et al. in <em class="italic">Neural Machine Translation by Jointly Learning to Align and Translate</em> by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio, <a href="https://arxiv.org/pdf/1409.0473.pdf"><span class="url">https://arxiv.org/pdf/1409.0473.pdf</span></a>. Some research papers also attribute the idea behind attention to Alex Graves’ <em class="italic">Generating Sequences with Recurrent Neural Networks</em>, which dates back to 2013, <a href="https://arxiv.org/pdf/1308.0850.pdf"><span class="url">https://arxiv.org/pdf/1308.0850.pdf</span></a>. </p>
<p class="normal">This ingredient—this key idea—has since become a part of the title of the first transformer paper, <em class="italic">Attention is All You Need</em>. To get a high-level overview, let’s consider this example from the paper that<a id="_idIndexMarker612"/> introduced attention:</p>
<p class="normal"><em class="italic">The agreement on the European Economic Area was signed in August 1992.</em></p>
<p class="normal">In French, this can be translated as: </p>
<p class="normal"><em class="italic">L’accord sur la zone économique européenne a été signé en août 1992.</em></p>
<p class="normal">The initial attempts to perform automatic machine translation back in the early 80s were based on the sequential translation of each word. This approach was very limiting because the text structure can change from a source language to a target language in many ways. For instance, some words in the French translation can have a different order: in English, adjectives usually precede nouns, like in “European Economic Area,” whereas in French, adjectives can go after nouns—”la zone économique européenne.” Moreover, unlike in English, the French language has gendered words. So, for example, the adjectives “économique” and “européenne” must be in their feminine form as they belong to the feminine noun “la zone.”</p>
<p class="normal">The key intuition behind the attention approach is to build a text model that “looks at” every single word in the source sentence when translating words into the output language. In the original 2017 transformer paper, the authors point out that the cost of doing this is quadratic, but the gain achieved in terms of more accurate translation is considerable. More recent <a id="_idIndexMarker613"/>works reduced this initial quadratic complexity, such as the <strong class="keyWord">Fast Attention Via positive Orthogonal Random</strong> (<strong class="keyWord">FAVOR</strong>+) features from the <em class="italic">Rethinking Attention with Performers</em> paper by Choromanski et al. (2020) from Google, DeepMind, the University of Cambridge, and the Alan Turing Institute.</p>
<p class="normal">Let’s go over a nice example from that original attention paper by Bahdanou et al. (2014):</p>
<figure class="mediaobject"><img alt="A picture containing graphical user interface  Description automatically generated" height="542" src="../Images/B18331_06_01.png" width="570"/></figure>
<p class="packt_figref">Figure 6.1: An example of attention for the English sentence “The agreement on the European Economic Area was signed in August 1992.” The plot visualizes “annotation weights”—the weights associated with the annotations. Source: “Neural Machine Translation by Jointly Learning to Align and Translate” by Bahdanau et al. (2014) (https://arxiv.org/abs/1409.0473)</p>
<p class="normal">Using the attention<a id="_idIndexMarker614"/> mechanism, the neural network can learn a heatmap of each source English word in relation to each target French word. Note that relationships are not only on the diagonal but might spread across the whole matrix. For instance, when the model outputs the French word “européenne,” it will pay a lot of attention to the input words “European” and “Economic.” (In <em class="italic">Figure 6.1</em>, this corresponds to the diagonal and the adjacent cell.) The 2014 attention paper by Bahdanou et al. demonstrated that the model (which used an RNN encoder-decoder framework with attention) can <em class="italic">learn to align and attend</em> to the input elements without supervision, and, as <em class="italic">Figure 6.1</em> shows, translate the input English sentences into French. And, of course, the larger the training set is, the greater the number of correlations that the attention-based model can learn.</p>
<p class="normal">In short, the attention mechanism can access all previous words and weigh them according to a <em class="italic">learned</em> measure of relevancy. This way, attention can provide relevant information about tokens<a id="_idIndexMarker615"/> located far away in the target sentence.</p>
<p class="normal">Now, we can focus on another key ingredient of the transformer—”self-attention.”</p>
<h3 class="heading-3" id="_idParaDest-149">Self-attention</h3>
<p class="normal">The third key idea popularized by the original transformer paper is the use of attention within the same sentence in the source<a id="_idIndexMarker616"/> language—self-attention. With this mechanism, neural networks can be trained to learn the relationships among all words (or other elements) in each input sequence (such as a sentence) irrespective of their positions before focusing on (machine) translation. Self-attention can be attributed to the idea from the 2016 paper called <em class="italic">Long Short-Term Memory-Networks for Machine Reading</em> by Cheng et al., <a href="https://arxiv.org/pdf/1601.06733.pdf"><span class="url">https://arxiv.org/pdf/1601.06733.pdf</span></a>.</p>
<p class="normal">Let’s go through an example with the following two sentences:</p>
<p class="center">“Server, can I have the check?”</p>
<p class="center">“Looks like I just crashed the server.”</p>
<p class="normal">Clearly, the word “server” has a very different meaning in either sentence and self-attention can understand each word considering the context of the surrounding words. Just to reiterate, the attention mechanism can access all previous words and weigh them according to a learned measure of relevancy. Self-attention provides relevant information about tokens located far away in the source sentence.</p>
<h3 class="heading-3" id="_idParaDest-150">Multi-head (self-)attention</h3>
<p class="normal">The original transformer<a id="_idIndexMarker617"/> performs a (self-)attention function multiple times. A single set of the so-called weight matrices (which are covered in detail in the <em class="italic">How to compute Attention</em> section) is named an attention head. When you have several sets of these matrices, you have multiple attention heads. The multi-head (self-)attention layer usually has several parallel (self-)attention layers. Note that the introduction of multiple heads allows us to have many definitions of which word is “relevant” to each other. Plus, all these definitions of relevance can be computed in parallel by modern hardware accelerators, thus speeding up the computation. </p>
<p class="normal">Now that we have gone through the high-level definitions of the key ingredients of the transformers, let’s deep dive into how to compute the attention mechanism.</p>
<h2 class="heading-2" id="_idParaDest-151">How to compute attention</h2>
<p class="normal">In the original<a id="_idIndexMarker618"/> transformer, the self-attention function is computed by using the so-called scaled dot-product units. The authors of the 2017 paper even called their attention method <em class="italic">Scaled Dot-Product Attention</em>. You might remember from high school studies that the dot-product between two vectors provides a good sense of how “close” the vectors are.</p>
<p class="normal">Each input token sequence (for example, of a sentence) embedding that passes into the transformer (encoder and/or decoder) produces <em class="italic">attention weights</em> (covered in detail below) that are simultaneously calculated between every sequence element (such as a word). The output results in embeddings produced for every token containing the token itself together with every other relevant token weighted by its relative attention weight.</p>
<p class="normal">The attention layer transforms the input vectors into query, key, and value matrices, which are then split into attention heads (hence, multi-head attention):</p>
<ul>
<li class="bulletList">The query word can be interpreted as the word <em class="italic">for which</em> we are calculating the attention function.</li>
<li class="bulletList">The key and value words are the words <em class="italic">to which</em> we are paying attention.</li>
</ul>
<p class="normal">The dot-product (explained further below) tells us the similarity between words. If the vectors for two words are more aligned, the <em class="italic">attention score</em> will be higher. The transformer will learn the weights in such a way that if two words in a sentence are relevant to each other, then their word vectors will be aligned.</p>
<p class="normal">Each attention layer learns three weight matrices:</p>
<ul>
<li class="bulletList">The query weights <em class="italic">W</em><sub class="italic">Q</sub></li>
<li class="bulletList">The key weights <em class="italic">W</em><sub class="italic">K</sub></li>
<li class="bulletList">The value weights <em class="italic">W</em><sub class="italic">V</sub></li>
</ul>
<p class="normal">For each word <em class="italic">i</em>, an input word embedding <em class="italic">x</em><sub class="italic">i</sub> is computed producing:</p>
<ul>
<li class="bulletList">A query vector <em class="italic">q</em><sub class="italic">i</sub> = <em class="italic">x</em><sub class="italic">i</sub><em class="italic">W</em><sub class="italic">Q</sub></li>
<li class="bulletList">A key vector <em class="italic">k</em><sub class="italic">i</sub> = <em class="italic">x</em><sub class="italic">i</sub><em class="italic">W</em><sub class="italic">K</sub></li>
<li class="bulletList">A value vector <em class="italic">v</em><sub class="italic">i</sub> = <em class="italic">x</em><sub class="italic">i</sub><em class="italic">W</em><sub class="italic">V</sub></li>
</ul>
<p class="normal">Given the query and the corresponding key vectors, the following dot-product formula produces the <em class="italic">attention weight</em> in the original transformer paper:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_06_001.png" style="height: 1.35em !important;" width="192"/></p>
<p class="normal">where:</p>
<ul>
<li class="bulletList"><em class="italic">a</em><sub class="italic">i,j</sub> is the attention from word <em class="italic">i</em> to a word <em class="italic">j</em>.</li>
<li class="bulletList">. is the dot-product of the query with keys, which will give a sense of how “close” the vectors are.</li>
</ul>
<p class="normal">Note that the attention unit for word <em class="italic">i</em> is the weighted sum of the value vectors of all words, weighted by <em class="italic">a</em><sub class="italic">i,j</sub>, the attention from word <em class="italic">i</em> to a word <em class="italic">j</em>.</p>
<p class="normal">Now, to stabilize gradients during the training, the attention weights are divided by the square root of<a id="_idIndexMarker619"/> the dimension of the key vectors <img alt="" height="58" src="../Images/B18331_06_002.png" style="height: 1.45em !important; vertical-align: -0.33em !important;" width="75"/>.</p>
<p class="normal">Then, the results are passed through a softmax function to normalize the weight. Note that the attention function from a word <em class="italic">i</em> to a word <em class="italic">j</em> is not the same as the attention from word <em class="italic">j</em> to a word <em class="italic">i</em>.</p>
<p class="normal">Note that since modern deep learning accelerators work well with matrices, we can compute attention for all words using large matrices.</p>
<p class="normal">Define <em class="italic">q</em><sub class="italic">i</sub>, <em class="italic">k</em><sub class="italic">i</sub>, <em class="italic">v</em><sub class="italic">i</sub> (where <em class="italic">i</em> is the <em class="italic">i</em>th row) as matrices <em class="italic">Q</em>, <em class="italic">K</em>, <em class="italic">V</em>, respectively. Then, we can summarize the attention function as an attention matrix:</p>
<p class="center"><img alt="" height="88" src="../Images/B18331_06_003.png" style="height: 2.20em !important; vertical-align: -0.81em !important;" width="629"/> </p>
<p class="normal">In this section, we discussed how to compute the attention function introduced in the original transformer paper. Next, let’s discuss the encoder-decoder architecture.</p>
<h2 class="heading-2" id="_idParaDest-152">Encoder-decoder architecture</h2>
<p class="normal">Similar to the seq2seq models, (<em class="italic">Sequence to Sequence Learning with Neural Networks</em> by Ilya Sutskever, Oriol Vinyals, Quoc V. Le (2014)) described in <em class="chapterRef">Chapter 5</em>, <em class="italic">Recurrent Neural Networks</em>, the original<a id="_idIndexMarker620"/> transformer model also used an encoder-decoder architecture:</p>
<ul>
<li class="bulletList">The encoder takes the input (source) sequence of embeddings and transforms it into a new fixed-length vector of input embeddings.</li>
<li class="bulletList">The decoder takes the output embeddings vector from the encoder and transforms it into a sequence of output embeddings.</li>
<li class="bulletList">Both the encoder and the decoder consist of several stacked layers. Each encoder and decoder layer is using the attention mechanism described earlier.</li>
</ul>
<p class="normal">We’ll learn about the<a id="_idIndexMarker621"/> transformer architecture in much more detail later in this section.</p>
<p class="normal">Since the introduction of the transformer architecture, other newer networks have used only the encoder or the decoder components (or both), which are discussed in the <em class="italic">Categories of transformers</em> section of this chapter.</p>
<p class="normal">Next, let’s briefly go over the other components of the original transformer—the residual and normalization layers.</p>
<h2 class="heading-2" id="_idParaDest-153">Residual and normalization layers</h2>
<p class="normal">Typically, transformer-based <a id="_idIndexMarker622"/>networks reuse other existing state-of-the-art machine learning methodologies, such as attention mechanisms. You <a id="_idIndexMarker623"/>shall therefore not be surprised if both encoder and decoder layers combine neural networks with residual connections (<em class="italic">Deep Residual Learning for Image Recognition</em> by He et al., 2016, <a href="https://arxiv.org/abs/1512.03385"><span class="url">https://arxiv.org/abs/1512.03385</span></a>) and normalization steps (<em class="italic">Layer Normalization</em> by Ba et al., 2016, <a href="https://arxiv.org/abs/1607.06450"><span class="url">https:/arxiv.org/abs/1607.06450</span></a>).</p>
<p class="normal">OK, we now have all the key ingredients to deep dive into transformers.</p>
<h2 class="heading-2" id="_idParaDest-154">An overview of the transformer architecture</h2>
<p class="normal">Now that we have covered <a id="_idIndexMarker624"/>some of the key concepts behind the original transformer, let’s deep dive into the architecture introduced in the seminal 2017 paper. Note that transformer-based models are usually built by leveraging various attention mechanisms without using RNNs. This is also a consequence of the fact that attention mechanisms themselves can match and outperform RNN (encoder-decoder) models with attention. That’s why the seminal paper was titled <em class="italic">Attention is all You Need</em>.</p>
<p class="normal"><em class="italic">Figure 6.2</em> shows a seq2seq network with RNNs and attention, and compares it to the original transformer network.</p>
<p class="normal">The transformer is similar to seq2seq with an attention model in the following ways:</p>
<ul>
<li class="bulletList">Both approaches work with source (inputs) and target (output) <em class="italic">sequences.</em></li>
<li class="bulletList">Both use an encoder-decoder architecture, as mentioned before.</li>
<li class="bulletList">The output of the last block of the encoder is used as a context—or thought vector—for computing the attention function in the decoder.</li>
<li class="bulletList">The target (output) sequence embeddings are fed into dense (fully connected) blocks, which convert the output embeddings to the final sequence of an integer form:</li>
</ul>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="510" src="../Images/B18331_06_02.png" width="788"/></figure>
<p class="packt_figref">Figure 6.2: Flow of data in (a) seq2seq + Attention, and (b) Transformer architecture. Image Source: Zhang, et al.</p>
<p class="normal">And the two architectures <a id="_idIndexMarker625"/>differ in the following ways:</p>
<ul>
<li class="bulletList">The seq2seq network uses the recurrent and attention layers in the encoder, and the recurrent layer in the decoder.
    <p class="normal">The transformer replaced those layers with so-called transformer blocks (a stack of N identical layers), as <em class="italic">Figure 6.2</em> demonstrates:</p>
<ul>
<li class="bulletList">In the encoder, the transformer block consists of a sequence of sub-layers: a multi-head (self-)attention layer and a position-wise feedforward layer. Each of those two layers has a residual connection, followed by a normalization layer.</li>
<li class="bulletList">In the decoder, the transformer block contains a variant of a multi-head (self-)attention layer with <em class="italic">masking—</em>a masked multi-head self-attention—and a feedforward layer like in the encoder (with identical residual connections and normalization layers). Masking helps prevent positions from attending into the future. Additionally, the decoder contains a second multi-head (self-)attention layer which computes attention over the outputs of the encoder’s transformer blockmasking is covered in more detail later in this section.)</li>
</ul>
</li>
</ul>
<ul>
<li class="bulletList">In the seq2seq with<a id="_idIndexMarker626"/> attention network, the encoder state is passed to the first recurrent time step as with the seq2seq with attention network.
    <p class="normal">In the transformer, the encoder state is passed to every transformer block in the decoder. This allows the transformer network to work in parallel across time steps since there is no longer a temporal dependency as with the seq2seq networks.</p>
<p class="normal">The last decoder is followed by a final linear transformation (a dense layer) with a softmax function to produce the output (next-token) probabilities.</p>
</li>
</ul>
<ul>
<li class="bulletList">Because of the parallelism referred to in the previous point, an encoding layer is added to provide positional information to distinguish the position of each element in the transformer network sequence (positional encoding layer). This way, the first encoder takes the positional information and embeddings of the input sequence as inputs, rather than only encodings, thus allowing taking positional information into account.</li>
</ul>
<p class="normal">Let’s walk through the process of data flowing through the transformer network. Later in this chapter, we will use TensorFlow with the Keras API to create and train a transformer model from scratch:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">As part of data preprocessing, the inputs and the outputs are tokenized and converted to embeddings.</li>
<li class="numberedList">Next, positional encoding is applied to the input and output embeddings to have information about the relative position of tokens in the sequences. In the encoder section:<ul>
<li class="bulletList">As per <em class="italic">Figure 6.2</em>, the encoder side consists of an embedding and a positional encoding layer, followed by six identical transformer blocks (there were six “layers” in the original transformer). As we learned earlier, each transformer block in the encoder consists of a multi-head (self-)attention layer and a position-wise feedforward layer.</li>
</ul>
<p class="normal">We have already briefly seen that self-attention is the process of attending to parts of the same sequence. When we process a sentence, we might want to know what other words are most aligned with the current one.</p>
<ul>
<li class="bulletList">The multi-head attention layer consists of multiple (eight in the reference implementation contained in the seminal paper) parallel self-attention layers. Self-attention is carried out by constructing three vectors <em class="italic">Q</em> (query), <em class="italic">K</em> (key), and <em class="italic">V</em> (value), out of the input embedding. These vectors are created by multiplying the input embedding with three trainable weight matrices <em class="italic">W</em><sub class="italic">Q</sub>, <em class="italic">W</em><sub class="italic">K</sub>, and <em class="italic">W</em><sub class="italic">V</sub>. The output vector <em class="italic">Z</em> is created by combining <em class="italic">K</em>, <em class="italic">Q</em>, and <em class="italic">V</em> at each self-attention layer using the following formula. Here, <em class="italic">d</em><sub class="italic">K</sub> refers to the dimension of the <em class="italic">K</em>, <em class="italic">Q</em>, and <em class="italic">V</em> vectors (64 in the reference implementation contained in the seminal paper):</li>
</ul>
<p class="center"><img alt="" height="117" src="../Images/B18331_06_004.png" style="height: 2.92em !important; vertical-align: 0.03em !important;" width="367"/></p>
<ul>
<li class="bulletList">The multi-head<a id="_idIndexMarker627"/> attention layer will create multiple values for <em class="italic">Z</em> (based on multiple trainable weight matrices <em class="italic">W</em><sub class="italic">Q</sub>, <em class="italic">W</em><sub class="italic">K</sub>, and <em class="italic">W</em><sub class="italic">V</sub> at each self-attention layer), and then concatenate them for inputs into the position-wise feedforward layer.</li>
<li class="bulletList">The inputs to the position-wise feedforward layer consist of embeddings for the different elements in the sequence (or words in the sentence), attended via self-attention in the multi-head attention layer. Each token is represented internally by a fixed-length embedding vector (512 in the reference implementation introduced in the seminal paper). Each vector is run through the feedforward layer in parallel. The outputs of the FFN are the inputs to (or fed into) the multi-head attention layer in the following transformer block. In the last transformer block of the encoder, the outputs are the context vector that is passed to the decoder.</li>
<li class="bulletList">Both the multi-head attention and position-wise FFN layers send out not only the signal from the previous layer but also a residual signal from their inputs to their outputs. The outputs and residual inputs are passed through a layer-normalization step, and this is shown in <em class="italic">Figure 6.2</em> as the “Add &amp; Norm” layer.</li>
<li class="bulletList">Since the entire sequence is consumed in parallel on the encoder, the information about the positions of individual elements gets lost. To compensate for this, the input embeddings are augmented with a positional embedding, which is implemented as a sinusoidal function without learned parameters. The positional embedding is added to the input embedding.</li>
</ul>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">Next, let’s walk through <a id="_idIndexMarker628"/>how the data flows through the decoder:<ul>
<li class="bulletList">The output of the encoder results in a pair of attention vectors <em class="italic">K</em> and <em class="italic">V</em>, which are sent in parallel to all the transformer blocks in the decoder. The transformer block in the decoder is similar to that in the encoder, except that it has an additional multi-head attention layer to handle the attention vectors from the encoder. This additional multi-head attention layer works similarly to the one in the encoder and the one below it, except that it combines the <em class="italic">Q</em> vector from the layer below it and the <em class="italic">K</em> and <em class="italic">Q</em> vectors from the encoder state.</li>
<li class="bulletList">Similar to the seq2seq network, the output sequence generates one token at a time, using the input from the previous time step. As for the input to the encoder, the input to the decoder is also augmented with a positional embedding. Unlike the encoder, the self-attention process in the decoder is only allowed to attend to tokens at previous time points. This is done by masking out tokens at future time points.</li>
<li class="bulletList">The output of the last transformer block in the decoder is a sequence of low-dimensional embeddings (512 for reference implementation in the seminal paper as noted earlier). This is passed to the dense layer, which converts it into a sequence of probability distributions across the target vocabulary, from which we generate the most probable word either greedily or by a more sophisticated technique such as beam search.</li>
</ul>
</li>
</ol>
<p class="normal"><em class="italic">Figure 6.3</em> shows the<a id="_idIndexMarker629"/> transformer architecture covering everything that’s just been described:</p>
<figure class="mediaobject"><img alt="Attention_diagram_transformer" height="503" src="../Images/B18331_06_03.png" width="874"/></figure>
<p class="packt_figref">Figure 6.3: The transformer architecture based on original images from “Attention Is All You Need” by Vaswani et al. (2017)</p>
<h2 class="heading-2" id="_idParaDest-155">Training</h2>
<p class="normal">Transformers are typically<a id="_idIndexMarker630"/> trained via semi-supervised learning in two steps:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, an unsupervised pretraining, typically on a very large corpus.</li>
<li class="numberedList">Then, a supervised fine-tuning on a smaller labeled dataset.</li>
</ol>
<p class="normal">Both pretraining and fine-tuning might require significant resources in terms of GPU/TPU, memory, and time. This is especially true, considering that large language models (in short, LLMs) have an increasing number of parameters as we will see in the next section.</p>
<p class="normal">Sometimes, the second phase has a very limited set of labeled data. This is the so-called few-shot learning, which considers making predictions based on a limited number of samples.</p>
<h1 class="heading-1" id="_idParaDest-156">Transformers’ architectures</h1>
<p class="normal">In this section, we have <a id="_idIndexMarker631"/>provided a high-level overview of both the most important architectures used by transformers and of the different ways used to compute attention.</p>
<h2 class="heading-2" id="_idParaDest-157">Categories of transformers</h2>
<p class="normal">In this section, we<a id="_idIndexMarker632"/> are going to classify transformers into different categories. The next paragraph will introduce the most common transformers.</p>
<h3 class="heading-3" id="_idParaDest-158">Decoder or autoregressive</h3>
<p class="normal">A typical <a id="_idIndexMarker633"/>example is a <strong class="keyWord">GPT</strong> (<strong class="keyWord">Generative Pre-Trained</strong>) model, which you can learn more about in the GPT-2 and GPT-3 sections later<a id="_idIndexMarker634"/> in this chapter, or refer to <a href="https://openai.com/blog/language-unsupervised"><span class="url">https://openai.com/blog/language-unsupervised</span></a>). Autoregressive models use only the decoder of the original transformer model, with the attention heads that can only see what is before in the text and not after with a masking mechanism used on the full sentence. Autoregressive models use pretraining to guess the next token after observing all the previous ones. Typically, autoregressive<a id="_idIndexMarker635"/> models are used for <strong class="keyWord">Natural Language Generation</strong> (<strong class="keyWord">NLG</strong>) text generation tasks. Other examples of autoregressive models include the original GPT, GPT-2, Transformer-XL, Reformer, and XLNet, which are covered later in this chapter.</p>
<h3 class="heading-3" id="_idParaDest-159">Encoder or autoencoding</h3>
<p class="normal">A typical example is <strong class="keyWord">BERT</strong> (<strong class="keyWord">Bidirectional Encoder Representations from Transformers</strong>), which is covered later in<a id="_idIndexMarker636"/> this chapter. Autoencoders correspond to the encoder in the original transformer model having access to the full input tokens with no masks. Autoencoding models use pretraining by masking/altering the input tokens and then trying to reconstruct the original sentence. Frequently, the models build a bidirectional representation of the full sentences. Note that the only difference between autoencoders and autoregressive is the pretraining phase, so the same architecture can be used in both ways. Autoencoders can be used for NLG, as well as for classification and many other NLP tasks. Other examples of autoencoding models, apart from BERT, include ALBERT, RoBERTa, and ELECTRA, which you can learn about later in this chapter.</p>
<h3 class="heading-3" id="_idParaDest-160">Seq2seq</h3>
<p class="normal">A typical example is <strong class="keyWord">T5</strong> (<strong class="keyWord">Text-to-Text Transfer Transformer</strong>) and the original transformer. Sequence-to-sequence models use both the encoder and the decoder of the original transformer<a id="_idIndexMarker637"/> architecture. Seq2seq can be fine-tuned to many tasks such as translation, summarization, ranking, and question answering. Another example of a seq2seq model, apart from the original transformer and T5, is <strong class="keyWord">Multitask Unified Model</strong> (<strong class="keyWord">MUM</strong>).</p>
<h3 class="heading-3" id="_idParaDest-161">Multimodal</h3>
<p class="normal">A typical example<a id="_idIndexMarker638"/> is MUM. Multimodal models mix text inputs with other kinds of content (for example, images, videos, and audio).</p>
<h3 class="heading-3" id="_idParaDest-162">Retrieval</h3>
<p class="normal">A typical example is RETRO. Some<a id="_idIndexMarker639"/> models use document retrieval during (pre)training and inference. This is frequently a good strategy to reduce the size of the model and rapidly access memorized information saving on the number of used parameters.</p>
<h2 class="heading-2" id="_idParaDest-163">Attention</h2>
<p class="normal">Now that we<a id="_idIndexMarker640"/> have understood how to classify transformers, let’s focus on attention!</p>
<p class="normal">There is a wide variety of attention mechanisms, such as self-attention, local/hard attention, and global/soft attention, to name a few. Below, we’ll focus on some of the examples.</p>
<h3 class="heading-3" id="_idParaDest-164">Full versus sparse</h3>
<p class="normal">As discussed, the (scaled) dot-product<a id="_idIndexMarker641"/> attention from the original 2017 transformer paper is typically computed on a full squared matrix <em class="italic">O</em>(<em class="italic">L</em><sup class="superscript">2</sup>) where <em class="italic">L</em> is the length of the maximal considered sequence (in some configurations <em class="italic">L</em> = 512). The BigBird type of transformer, proposed by Google Research in 2020 and discussed in more detail later in this chapter, introduced the idea of using sparse attention by leveraging sparse matrices (based on the 2019 work by OpenAI’s <em class="italic">Generating long sequences with sparse transformers</em> by Child et al., <a href="https://arxiv.org/abs/1904.10509"><span class="url">https://arxiv.org/abs/1904.10509</span></a>).</p>
<h3 class="heading-3" id="_idParaDest-165">LSH attention</h3>
<p class="normal">The Reformer<a id="_idIndexMarker642"/> introduced the idea of reducing the attention mechanism complexity with hashing—the model’s authors called it locality-sensitive hashing attention. The approach is based on the notion of using only the largest elements when <em class="italic">softmax</em>(<em class="italic">QK</em><sup class="italic">T</sup>) is computed. In other words, for each query <img alt="" height="46" src="../Images/B18331_06_005.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="96"/> only the keys <img alt="" height="42" src="../Images/B18331_06_006.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="100"/> that are close to <em class="italic">q</em> are computed. For computing closeness, several hash functions are computed according to local sensitive hashing techniques.</p>
<h3 class="heading-3" id="_idParaDest-166">Local attention</h3>
<p class="normal">Some transformers<a id="_idIndexMarker643"/> adopted the idea of having only a local window of context (e.g. a few tokens on the right and a few tokens on the left). The idea is that using fewer parameters allows us to consider longer sequences but with a limited degree of attention. For this reason, local attention is less popular.</p>
<h1 class="heading-1" id="_idParaDest-167">Pretraining</h1>
<p class="normal">As you have learned earlier, the original transformer had an encoder-decoder architecture. However, the research <a id="_idIndexMarker644"/>community understood that there are situations where it is beneficial to have only the encoder, or only the decoder, or both.</p>
<h2 class="heading-2" id="_idParaDest-168">Encoder pretraining</h2>
<p class="normal">As discussed, these models<a id="_idIndexMarker645"/> are also called auto-encoding and they use only the encoder<a id="_idIndexMarker646"/> during the pretraining. Pretraining is carried out by masking words in the input sequence and training the model to reconstruct the sequence. Typically, the encoder can access all the input words. Encoder-only models are generally used for classification.</p>
<h2 class="heading-2" id="_idParaDest-169">Decoder pretraining</h2>
<p class="normal">Decoder models are<a id="_idIndexMarker647"/> referred to as autoregressive. During pretraining, the decoder is optimized to predict the next word. In particular, the decoder can only <a id="_idIndexMarker648"/>access all the words positioned before a given word in the sequence. Decoder-only models are generally used for text generation.</p>
<h2 class="heading-2" id="_idParaDest-170">Encoder-decoder pretraining</h2>
<p class="normal">In this case, the<a id="_idIndexMarker649"/> model can use both the encoder and the decoder. Attention in the encoder can use all the words in the sequence, while attention in<a id="_idIndexMarker650"/> the decoder can only use the words preceding a given word in the sequence. Encoder-decoder has a large range of applications including text generation, translation, summarization, and generative question answer.</p>
<h2 class="heading-2" id="_idParaDest-171">A taxonomy for pretraining tasks</h2>
<p class="normal">It can be useful to <a id="_idIndexMarker651"/>organize pretraining into a taxonomy suggested by <em class="italic">Pre-trained Models for Natural Language Processing: A Survey</em>, Xipeng Qiu, 2020, <a href="https://arxiv.org/abs/2003.08271"><span class="url">https://arxiv.org/abs/2003.08271</span></a>:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Language Modeling</strong> (<strong class="keyWord">LM</strong>): For<a id="_idIndexMarker652"/> unidirectional LM, the task is to predict the next token. For bidirectional LM, the task is to predict the previous and next tokens.</li>
<li class="bulletList"><strong class="keyWord">Masked Language Modeling</strong> (<strong class="keyWord">MLM</strong>): The key idea is to mask out some tokens from the input <a id="_idIndexMarker653"/>sentences. Then, the model is trained to predict the masked tokens given the non-masked tokens.</li>
<li class="bulletList"><strong class="keyWord">Permuted Language Modeling</strong> (<strong class="keyWord">PLM</strong>): This is similar to LM, but a random permutation of input <a id="_idIndexMarker654"/>sequences is performed. Then, a subset of tokens is chosen as the target, and the model is trained to predict these targets.</li>
<li class="bulletList"><strong class="keyWord">Denoising Autoencoder</strong> (<strong class="keyWord">DAE</strong>): Deliberately provide partially corrupted input. For instance, randomly sample<a id="_idIndexMarker655"/> input tokens and replace them with special [MASK] elements. Alternatively, randomly delete input tokens. Alternatively, shuffle sentences in random order. The task is to recover the original undistorted input.</li>
<li class="bulletList"><strong class="keyWord">Contrastive Learning</strong> (<strong class="keyWord">CTL</strong>): The task is to<a id="_idIndexMarker656"/> learn a score function for text pairs by assuming that some observed pairs of text are more semantically similar than randomly sampled text. This class of techniques includes a number of specific techniques such as: <ul>
<li class="bulletList"><strong class="keyWord">Deep InfoMax</strong> (<strong class="keyWord">DIM</strong>): Maximize mutual information between an input image representation and <a id="_idIndexMarker657"/>various local regions of the same image.</li>
<li class="bulletList"><strong class="keyWord">Replaced Token Detection</strong> (<strong class="keyWord">RTD</strong>): Predict<a id="_idIndexMarker658"/> whether an input token is replaced given its surroundings.</li>
<li class="bulletList"><strong class="keyWord">Next Sentence Prediction</strong> (<strong class="keyWord">NSP</strong>): The <a id="_idIndexMarker659"/>model is trained to distinguish whether two input sentences are contiguous in the training corpus.</li>
<li class="bulletList"><strong class="keyWord">Sentence Order Prediction</strong> (<strong class="keyWord">SOP</strong>): The same ideas as NSP with some additional signals: two consecutive <a id="_idIndexMarker660"/>segments are positive examples, and two swapped segments are negative examples.</li>
</ul>
</li>
</ul>
<p class="normal">In this section, we have briefly<a id="_idIndexMarker661"/> reviewed different pretraining techniques. The next section will review a selection of the most used transformers.</p>
<h1 class="heading-1" id="_idParaDest-172">An overview of popular and well-known models</h1>
<p class="normal">After the seminal paper <em class="italic">Attention is All You Need</em>, a very large number of alternative transformer-based models have been proposed. Let’s review some of the most popular and well-known ones.</p>
<h2 class="heading-2" id="_idParaDest-173">BERT</h2>
<p class="normal">BERT, or Bidirectional Encoder <a id="_idIndexMarker662"/>Representations from Transformers, is a language representation model developed by the Google AI research team in 2018. Let’s go over the main intuition behind that model:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">BERT considers the context of each word from both the left and the right side using the so-called “bidirectional self-attention.”</li>
<li class="numberedList">Training happens by randomly masking the input word tokens, and avoiding cycles so that words cannot see themselves indirectly. In NLP jargon, this is called “fill in the blank.” In other words, the pretraining task involves masking a small subset <a id="_idIndexMarker663"/>of unlabeled inputs and then training the network to recover these original inputs. (This is an example of MLM.) </li>
<li class="numberedList">The model uses classification for pretraining to predict whether a sentence sequence S is before a sentence T. This way, BERT can understand relationships among sentences (“Next Sentence Prediction”), such as “does sentence T come after sentence S?” The idea of pretraining became a new standard for LLM.</li>
<li class="numberedList">BERT—namely BERT Large—became one of the first large language models with 24 transformer blocks, 1024-hidden layers, 16 self-attention heads, and 340M parameters. The model is trained on a large 3.3 billion words corpus.</li>
</ol>
<p class="normal">BERT produced state-of-the-art results for 11 NLP tasks, including:</p>
<ul>
<li class="bulletList">GLUE score of 80.4%, a 7.6% absolute improvement from the previous best result.</li>
<li class="bulletList">93.2% accuracy on SQuAD 1.1 and outperforming human performance by 2%.</li>
</ul>
<p class="normal">We will see GLUE and SQuAD metrics later in this chapter. If you want to know more, you can explore the following material:</p>
<ul>
<li class="bulletList">The original research paper: <em class="italic">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em> by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018, <a href="https://arxiv.org/abs/1810.04805"><span class="url">https://arxiv.org/abs/1810.04805</span></a>.</li>
<li class="bulletList">The Google AI blog post: <em class="italic">Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing</em>, 2018, which discusses the advancement of the (then) state-of-the-art model for 11 NLP tasks (<a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.xhtml"><span class="url">https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.xhtml</span></a>.)</li>
<li class="bulletList">An open source TensorFlow implementation and the pretrained BERT models are available at <a href="http://goo.gl/language/bert"><span class="url">http://goo.gl/language/bert</span></a> and from TensorFlow Model Garden at <a href="https://github.com/tensorflow/models/tree/master/official/nlp/modeling/models"><span class="url">https://github.com/tensorflow/models/tree/master/official/nlp/modeling/models</span></a>. </li>
<li class="bulletList">A Colab notebook for BERT is available here: <a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb"><span class="url">https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb</span></a>.</li>
<li class="bulletList">BERT FineTuning<a id="_idIndexMarker664"/> with Cloud TPU: A tutorial that shows how to train the BERT model on Cloud TPU for sentence and sentence-pair classification tasks: <a href="https://cloud.google.com/tpu/docs/tutorials/bert"><span class="url">https://cloud.google.com/tpu/docs/tutorials/bert</span></a>.</li>
<li class="bulletList">A Google blog post about applying BERT to Google Search to improve language understanding. According to Google, BERT “<em class="italic">will help Search better understand one in 10 searches in the U.S. in English.</em>” Moreover, the post mentions that “<em class="italic">A powerful characteristic of these systems is that they can take learnings from one language and apply them to others. So we can take models that learn from improvements in English (a language where the vast majority of web content exists) and apply them to other languages.</em>” (from <em class="italic">Understanding search better than ever before</em>): <a href="https://blog.google/products/search/search-language-understanding-bert/"><span class="url">https://blog.google/products/search/search-language-understanding-bert/</span></a>.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-174">GPT-2</h2>
<p class="normal">GPT-2 is a model introduced by OpenAI in <em class="italic">Language Models Are Unsupervised Multitask Learners </em>by Alec Radford, Jeffrey Wu, Rewon<a id="_idIndexMarker665"/> Child, David Luan, Dario Amodei, and Ilya Sutskever, <a href="https://openai.com/blog/better-language-models/"><span class="url">https://openai.com/blog/better-language-models/</span></a>, <a href="https://openai.com/blog/gpt-2-6-month-follow-up/"><span class="url">https://openai.com/blog/gpt-2-6-month-follow-up/</span></a>, <a href="https://www.openai.com/blog/gpt-2-1-5b-release/"><span class="url">https://www.openai.com/blog/gpt-2-1-5b-release/</span></a>, and <a href="https://github.com/openai/gpt-2"><span class="url">https://github.com/openai/gpt-2</span></a>.)</p>
<p class="normal">Let’s review the key intuitions:</p>
<ul>
<li class="bulletList">The largest of four model <a id="_idIndexMarker666"/>sizes was a 1.5 billion-parameter transformer with 48 layers trained on a new dataset called Webtext containing text from 45 million webpages.</li>
<li class="bulletList">GPT-2 used the original 2017 transformer-based architecture and a modified version of the original GPT model (also developed by OpenAI) by Radford et al., 2018, <em class="italic">Improving Language Understanding by Generative Pre-Training</em>, <a href="https://openai.com/blog/language-unsupervised/"><span class="url">https://openai.com/blog/language-unsupervised/</span></a>, and <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"><span class="url">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</span></a>). </li>
<li class="bulletList">The research demonstrated that an LLM trained on a large and diverse dataset can perform well on a wide variety of NLP tasks, such as question answering, machine translation, reading comprehension, and summarization. Previously, the tasks had been typically approached with supervised learning on task-specific datasets. GPT-2 was trained in an unsupervised manner and performed well at zero-shot task transfer.</li>
<li class="bulletList">Initially, OpenAI released<a id="_idIndexMarker667"/> only a smaller version of GPT-2 with 117 M parameters, “due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale.” Then, the model was released: <a href="https://openai.com/blog/gpt-2-1-5b-release/"><span class="url">https://openai.com/blog/gpt-2-1-5b-release/</span></a>.</li>
<li class="bulletList">Interestingly enough, OpenAI developed an ML-based detection method to test whether an actor is generating synthetic texts for propaganda. The detection rates were ~95% for detecting 1.5B GPT-2-generated text: <a href="https://github.com/openai/gpt-2-output-dataset"><span class="url">https://github.com/openai/gpt-2-output-dataset</span></a>.</li>
</ul>
<p class="normal">Similar to the original GPT from 2018, GPT-2 does not require the encoder part of the original transformer model – it uses a multi-layer decoder for language modeling. The decoder can only get information from the prior words in the sentence. It takes word vectors as input and produces estimates for the probability of the next word as output, but it is <em class="italic">autoregressive</em>, meaning that each token in the sentence relies on the context of the previous words. On the other hand, BERT is not autoregressive, as it uses the entire surrounding context all at once.</p>
<p class="normal">GPT-2 was the first LLM showing commonsense reasoning, capable of performing a number of NLP tasks including translation, question answering, and reading comprehension. The model achieved state-of-the-art results on 7 out of 8 tested language modeling datasets.</p>
<h2 class="heading-2" id="_idParaDest-175">GPT-3</h2>
<p class="normal">GPT-3 is an autoregressive language model <a id="_idIndexMarker668"/>developed by OpenAI and introduced in 2019 in <em class="italic">Language Models are Few-Shot Learners</em> by Tom B. Brown, et al., <a href="https://arxiv.org/abs/2005.14165"><span class="url">https://arxiv.org/abs/2005.14165</span></a>. Let’s look at the key intuitions:</p>
<ul>
<li class="bulletList">GPT-3 uses an architecture and model similar to GPT-2 with a major difference consisting of the adoption of a sparse attention mechanism.</li>
<li class="bulletList">For each task, the model evaluation has three different approaches:<ul>
<li class="bulletList"><strong class="keyWord">Few-shot learning</strong>: The <a id="_idIndexMarker669"/>model receives a few demonstrations of the task (typically, less than one hundred) at inference time. However, no weight updates are allowed.</li>
<li class="bulletList"><strong class="keyWord">One-shot learning</strong>: The model<a id="_idIndexMarker670"/> receives only one demonstration and a natural language description of the task.</li>
<li class="bulletList"><strong class="keyWord">Zero-shot learning</strong>: The <a id="_idIndexMarker671"/>model receives no demonstration, but it has access only to a natural language description of the task.</li>
</ul>
</li>
<li class="bulletList">For all tasks, GPT-3 is applied without any gradient updates, complete with tasks and few-shot demonstrations specified purely via text interaction with the model.</li>
</ul>
<p class="normal">The number of parameters the researchers trained GPT-3 with ranges from 125 million (GPT-3 Small) to 175 billion (GPT-3 175B). With no fine-tuning, the model achieves significant results on many NLP tasks including translation and question answering, sometimes surpassing state-of-the-art models. In particular, GPT-3 showed impressive results in NLG, creating news articles that were hard to distinguish from real ones. The model demonstrated it was able to solve tasks requiring on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic.</p>
<p class="normal">GPT-3’s underlying model is not publicly available and we can’t pretrain the model, but some datasets statistics are available at <a href="https://github.com/openai/gpt-3"><span class="url">https://github.com/openai/gpt-3</span></a> and we can run data on and fine-tune GPT-3 engines.</p>
<h2 class="heading-2" id="_idParaDest-176">Reformer</h2>
<p class="normal">The Reformer model was introduced in the 2020 paper <em class="italic">Reformer: The Efficient Transformer</em> by UC Berkeley and Google AI researchers Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya, <a href="https://arxiv.org/abs/2001.04451"><span class="url">https://arxiv.org/abs/2001.04451</span></a>.</p>
<p class="normal">Let’s look at the key intuitions:</p>
<ul>
<li class="bulletList">The authors <a id="_idIndexMarker672"/>demonstrated you can train the Reformer model, which performs on par with transformer models in a more memory-efficient and faster way on long sequences.</li>
<li class="bulletList">One limitation of transformers is the capacity of dealing with long sequences, due to the quadratic time needed for computing attention.</li>
<li class="bulletList">Reformer addresses the computations and memory challenges during the training of transformers by using three techniques.</li>
<li class="bulletList">First, Reformer replaced the (scaled) dot-product attention with an approximation using locality-sensitive hashing attention (described briefly earlier in this chapter). The paper’s authors changed the former’s <em class="italic">O</em>(<em class="italic">L</em><sup class="superscript">2</sup>) factor in attention layers with <em class="italic">O</em>(<em class="italic">LlogL</em>), where <em class="italic">L</em> is the length of the sequence (see <em class="italic">Figure 6.4</em> where LSH is applied to chunks in sequence). Refer to local sensitive <a id="_idIndexMarker673"/>hashing introduced in computer science to learn more: <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing"><span class="url">https://en.wikipedia.org/wiki/Locality-sensitive_hashing</span></a>.</li>
<li class="bulletList">Second, the model combined the attention and feedforward layers with reversible residual layers instead of normal residual layers (based on the idea from <em class="italic">The reversible residual network: Backpropagation without storing activations</em> by Gomez et al., 2017, <a href="https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.xhtml"><span class="url">https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.xhtml</span></a>). Reversible residual layers allow for storage activations once instead of <em class="italic">N</em> times, thus reducing the cost in terms of memory and time complexity.</li>
<li class="bulletList">Third, Reformer used a chunking technique for certain computations, including one for the feedforward layer and for a backward pass.</li>
<li class="bulletList">You can read the Google AI blog post to learn more about how the Reformer reached efficiency at <a href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml"><span class="url">https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml</span></a>:</li>
</ul>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="615" src="../Images/B18331_06_04.png" width="767"/></figure>
<p class="packt_figref">Figure 6.4: Local Sensitive Hashing to improve the transformers’ efficiency – source: https://ai.googleblog.com/2020/01/reformer-efficient-transformer.xhtml</p>
<h2 class="heading-2" id="_idParaDest-177">BigBird</h2>
<p class="normal">BigBird is another type of<a id="_idIndexMarker674"/> transformer introduced in 2020 by Google Research that uses a sparse attention mechanism for tackling the quadratic complexity needed to compute full attention for long sequences. For a deeper overview, see the paper <em class="italic">Big Bird: Transformers for Longer Sequences</em> by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed, <a href="https://arxiv.org/pdf/2007.14062.pdf"><span class="url">https://arxiv.org/pdf/2007.14062.pdf</span></a>.</p>
<p class="normal">Let’s look at the<a id="_idIndexMarker675"/> key intuitions:</p>
<ul>
<li class="bulletList">The authors demonstrated that BigBird was capable of handling longer context—much longer sequences of up to 8x with BERT on similar hardware. Its performance was “drastically” better on certain NLP tasks, such as question answering and document summarization.</li>
<li class="bulletList">BigBird runs on a sparse attention mechanism for overcoming the quadratic dependency of BERT. Researchers proved that the complexity reduced from <em class="italic">O</em>(<em class="italic">L</em><sup class="superscript">2</sup>) to <em class="italic">O</em>(<em class="italic">L</em>).</li>
<li class="bulletList">This way, BigBird can<a id="_idIndexMarker676"/> process sequences of length up to 8x more than what was possible with BERT. In other words, BERT’s limit was 512 tokens and BigBird increased to 4,096 tokens.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-178">Transformer-XL</h2>
<p class="normal">Transformer-XL is a self-attention-based model <a id="_idIndexMarker677"/>introduced in 2019 by Carnegie Mellon University and Google Brain researchers in the paper <em class="italic">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</em> by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov, <a href="https://aclanthology.org/P19-1285.pdf"><span class="url">https://aclanthology.org/P19-1285.pdf</span></a>.</p>
<p class="normal">Let’s look at the key intuitions:</p>
<ul>
<li class="bulletList">Unlike the original<a id="_idIndexMarker678"/> transformer and RNNs, Transformer-XL demonstrated it can model longer-term dependency beyond a fixed-length context while generating relatively coherent text.</li>
<li class="bulletList">Transformer-XL introduced a new segment-level recurrence mechanism and a new type of relative positional encodings (as opposed to absolute ones), allowing the model to learn dependencies that are 80% longer than RNNs and 450% longer than vanilla transformers. Traditionally, transformers split the entire corpus into shorter segments due to computational limits and only train the model within each segment.</li>
<li class="bulletList">During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the following new segment, as shown in <em class="italic">Figure 6.5</em>. Although the gradient still remains within a segment, this additional input allows the network to exploit information in history, leading to an ability of modeling longer-term dependency and avoiding context fragmentation.</li>
<li class="bulletList">During evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the vanilla model case. This way, Transformer-XL proved to be up to 1,800+ times faster than the vanilla model during evaluation:</li>
</ul>
<figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" height="179" src="../Images/B18331_06_05.png" width="814"/></figure>
<p class="packt_figref">Figure 6.5: Transformer-XL and the input with recurrent caching of previous segments</p>
<h2 class="heading-2" id="_idParaDest-179">XLNet</h2>
<p class="normal">XLNet is an unsupervised language representation learning method developed by Carnegie Mellon University and Google<a id="_idIndexMarker679"/> Brain researchers in 2019. It is based on generalized permutation language modeling objectives. XLNet employs Transformer-XL as a backbone model. The reference paper here is <em class="italic">XLNet: Generalized Autoregressive Pre-training for Language Understanding</em> by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le, <a href="https://arxiv.org/abs/1906.08237"><span class="url">https://arxiv.org/abs/1906.08237</span></a>. </p>
<p class="normal">Let’s see the key intuitions:</p>
<ul>
<li class="bulletList">Like BERT, XLNet uses a<a id="_idIndexMarker680"/> bidirectional context, looking at the words before and after a given token to predict what it should be.</li>
<li class="bulletList">XLNet maximizes the expected log-likelihood of a sequence with respect to all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In other words, XLNet captures bidirectional context.</li>
<li class="bulletList">XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks.</li>
<li class="bulletList">Code and pretrained models are available here: <a href="https://github.com/zihangdai/xlnet"><span class="url">https://github.com/zihangdai/xlnet</span></a>.</li>
</ul>
<p class="normal">XLNet is considered better than BERT in almost all NLP tasks, outperforming BERT on 20 tasks, often by a large margin. When it was introduced, the model achieved state-of-the-art performance on 18 NLP tasks, including sentiment analysis, natural language inference, question answering, and document ranking.</p>
<h2 class="heading-2" id="_idParaDest-180">RoBERTa</h2>
<p class="normal">RoBERTa (a Robustly Optimized BERT) is a<a id="_idIndexMarker681"/> model introduced in 2019 by researchers at the University of Washington and Facebook AI (Meta) in <em class="italic">RoBERTa: A Robustly Optimized BERT Pretraining Approach</em> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov, <a href="https://arxiv.org/abs/1907.11692"><span class="url">https://arxiv.org/abs/1907.11692</span></a>.</p>
<p class="normal">Let’s look at the key intuitions:</p>
<ul>
<li class="bulletList">When replicating BERT, the<a id="_idIndexMarker682"/> researchers discovered that BERT was “significantly undertrained.”</li>
<li class="bulletList">RoBERTa’s authors proposed a BERT variant that modifies key hyperparameters (longer training, larger batches, more data), removing the next-sentence pretraining objective, and training on longer sequences. The authors also proposed dynamically changing the masking pattern applied to the training data.</li>
<li class="bulletList">The researchers collected a new dataset called CC-News of similar size to other privately used datasets.</li>
<li class="bulletList">The code is available here: <a href="https://github.com/pytorch/fairseq"><span class="url">https://github.com/pytorch/fairseq</span></a>.</li>
</ul>
<p class="normal">RoBERTa outperformed BERT on GLUE and SQuAD tasks and matched XLNet on some of them.</p>
<h2 class="heading-2" id="_idParaDest-181">ALBERT</h2>
<p class="normal"><strong class="keyWord">ALBERT</strong> (<strong class="keyWord">A Lite BERT</strong>) is a model introduced<a id="_idIndexMarker683"/> in 2019 by researchers at Google Research and Toyota Technological Institute at Chicago in the paper titled <em class="italic">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</em> by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut, <a href="https://arxiv.org/abs/1909.11942v1"><span class="url">https://arxiv.org/abs/1909.11942v1</span></a>.</p>
<p class="normal">Let’s see the key <a id="_idIndexMarker684"/>intuitions:</p>
<ul>
<li class="bulletList">Large models typically aim at increasing the model size when pretraining natural language representations in order to get improved performance. However, increasing the model size might become difficult due to GPU/TPU memory limitations, longer training times, and unexpected model degradation.</li>
<li class="bulletList">ALBERT attempts to address the memory limitation, the communication overhead, and model degradation problems with an architecture that incorporates two parameter-reduction techniques: factorized embedding parameterization and cross-layer parameter sharing. With factorized embedding parameterization, the size of the hidden layers is separated from the size of vocabulary embeddings by decomposing the large vocabulary-embedding matrix into two small matrices. With cross-layer parameter sharing, the model prevents the number of parameters from growing along with the network depth. Both of these techniques improved parameter efficiency without “seriously” affecting the performance.</li>
<li class="bulletList">ALBERT has 18x fewer parameters and 1.7x faster training compared to the original BERT-Large <a id="_idIndexMarker685"/>model and achieves only slightly worse performance.</li>
<li class="bulletList">The code is available here: <a href="https://github.com/brightmart/albert_zh"><span class="url">https://github.com/brightmart/albert_zh</span></a>.</li>
</ul>
<p class="normal">ALBERT claimed it established new state-of-the-art results on all of the current state-of-the-art language benchmarks like GLUE, SQuAD, and RACE.</p>
<h2 class="heading-2" id="_idParaDest-182">StructBERT</h2>
<p class="normal">StructBERT is a model<a id="_idIndexMarker686"/> introduced in 2019’s paper called <em class="italic">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</em> by Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si, <a href="https://arxiv.org/abs/1908.04577"><span class="url">https://arxiv.org/abs/1908.04577</span></a>.</p>
<p class="normal">Let’s see the key intuitions:</p>
<ul>
<li class="bulletList">The Alibaba team<a id="_idIndexMarker687"/> suggested extending BERT by leveraging word-level and sentence-level ordering during the pretraining procedure. BERT masking during pretraining is extended by mixing up a number of tokens and then the model has to predict the right order.</li>
<li class="bulletList">In addition, the model randomly shuffles the sentence order and predicts the next and the previous sentence with a specific prediction task.</li>
<li class="bulletList">This additional wording and sentence shuffling together with the task of predicting the original order allow StructBERT to learn linguistic structures during the pretraining procedure.</li>
</ul>
<p class="normal">StructBERT from Alibaba claimed<a id="_idIndexMarker688"/> to have achieved state-of-the-art results on different NLP tasks, such as sentiment classification, natural language inference, semantic textual similarity, and question answering, outperforming BERT.</p>
<h2 class="heading-2" id="_idParaDest-183">T5 and MUM</h2>
<p class="normal">In 2019, Google researchers introduced a framework dubbed Text-to-Text Transfer Transformer (in short, T5) in <em class="italic">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</em> by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu, <a href="https://arxiv.org/abs/1910.10683"><span class="url">https://arxiv.org/abs/1910.10683</span></a>. This paper is a fundamental one for transformers.</p>
<p class="normal">Here are some of the key ideas:</p>
<ul>
<li class="bulletList">T5 addresses many<a id="_idIndexMarker689"/> NLP tasks as a “text-to-text” problem. T5 is a single model (with different numbers of parameters) that can be trained on a wide number of tasks. The framework is so powerful that it can be applied to summarization, sentiment analysis, question answering, and machine translation.</li>
<li class="bulletList">Transfer learning, where a model is first pretrained on a data-rich task before being fine-tuned on a downstream task, is extensively analyzed by comparing pretraining objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks.</li>
<li class="bulletList">Similar to the original transformer, T5: 1) uses an encoder-decoder structure; 2) maps the input sequences to learned embeddings and positional embeddings, which are passed to the encoder; 3) uses self-attention blocks with self-attention and feedforward layers (each with normalization and skip connections) in both the encoder and the decoder.</li>
<li class="bulletList">Training happens on a “Colossal Clean Crawled Corpus’’ (C4) dataset and the number of parameters per each T5 model varies from 60 million (T5 Small) up to 11 billion.</li>
<li class="bulletList">The computation costs were similar to BERT, but with twice as many parameters.</li>
<li class="bulletList">The code is available here: <a href="https://github.com/google-research/text-to-text-transfer-transformer"><span class="url">https://github.com/google-research/text-to-text-transfer-transformer</span></a>.</li>
<li class="bulletList">Google also offers T5 with a free TPU in a Colab tutorial at <a href="https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb"><span class="url">https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb</span></a>. We will discuss this in <a id="_idIndexMarker690"/>more detail later this chapter.</li>
</ul>
<p class="normal">When presented, the T5 model with 11 billion parameters achieved state-of-the-art performances on 17 out of 24 tasks considered and became de-facto one of the best LMs available:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="317" src="../Images/B18331_06_06.png" width="877"/></figure>
<p class="packt_figref">Figure 6.6: T5 uses the same model, loss function, hyperparameters, etc. across our diverse set of tasks —including translation, question answering, and classification</p>
<p class="normal">mT5, developed by Xue et al. at Google Research in 2020, extended T5 by using a single transformer to model multiple languages. It was pretrained on a Common Crawl-based dataset covering 101 languages. You can read more about it in <em class="italic">mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer</em>, <a href="https://arxiv.org/pdf/2010.11934.pdf"><span class="url">https://arxiv.org/pdf/2010.11934.pdf</span></a>.</p>
<p class="normal"><strong class="keyWord">MUM</strong> (short for <strong class="keyWord">Multitask Unified Model</strong>) is a model using the T5 text-to-text framework and according to Google is 1,000 times more <a id="_idIndexMarker691"/>powerful than BERT. Not only does MUM understand language, but it also generates it. It is also multimodal, covering modalities like text and images (expanding to more modalities in the future). The model was trained across 75 different languages and many different tasks at once. MUM is <a id="_idIndexMarker692"/>currently used to support Google Search ranking: <a href="https://blog.google/products/search/introducing-mum/"><span class="url">https://blog.google/products/search/introducing-mum/</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-184">ELECTRA</h2>
<p class="normal">ELECTRA is a model introduced in <a id="_idIndexMarker693"/>2020 by Stanford University and Google Brain researchers in <em class="italic">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</em> by Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning, <a href="https://arxiv.org/abs/2003.10555"><span class="url">https://arxiv.org/abs/2003.10555</span></a>.</p>
<p class="normal">Let’s look at the key intuitions: </p>
<ul>
<li class="bulletList">BERT pretraining consists of <a id="_idIndexMarker694"/>masking a small subset of unlabeled inputs and then training the network to recover them. Typically only a small fraction of words are used (~15%).</li>
<li class="bulletList">The ELECTRA authors proposed a new pretraining task named “replaced token detection.” The idea is to replace some tokens with alternatives generated by a small language model. Then, the pretrained discriminator is used to predict whether each token is an original or a replacement. This way, the model can learn from all the tokens instead of a subset:</li>
</ul>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="243" src="../Images/B18331_06_07.png" width="818"/></figure>
<p class="packt_figref">Figure 6.7: ELECTRA replacement strategy. The discriminator’s task is to detect whether the word is an original one or a replacement – source: https://arxiv.org/pdf/2003.10555.pdf </p>
<p class="normal">ELECTRA outperformed previous state-of-the-art models, requiring at the same time less pretraining efforts. The code is available at <a href="https://github.com/google-research/electra"><span class="url">https://github.com/google-research/electra</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-185">DeBERTa</h2>
<p class="normal">DeBERTa is a model introduced <a id="_idIndexMarker695"/>by Microsoft’s researchers in 2020 in <em class="italic">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</em> by Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen, <a href="https://arxiv.org/abs/2006.03654"><span class="url">https://arxiv.org/abs/2006.03654</span></a>.</p>
<p class="normal">Let’s look at the most important<a id="_idIndexMarker696"/> ideas:</p>
<ul>
<li class="bulletList">BERT’s self-attention is focused on content-to-content and content-to-position, where the content and position embedding are added before self-attention. DeBERTa keeps two separate vectors representing content and position so that self-attention is calculated between content-to-content, content-to-position, position-to-content, and position-to-position.</li>
<li class="bulletList">DeBERTa keeps absolute position information along with the related position information.</li>
</ul>
<p class="normal">Due to additional structural information used by the model, DeBERTa claimed to have achieved state-of-the-art results with half the training data when compared with other models such as RoBERTa. The code is available at <a href="https://github.com/microsoft/DeBERTa"><span class="url">https://github.com/microsoft/DeBERTa</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-186">The Evolved Transformer and MEENA</h2>
<p class="normal">The Evolved Transformer was<a id="_idIndexMarker697"/> introduced in 2019 by Google Brain researchers in the paper <em class="italic">The Evolved Transformer</em> by David R. So, Chen Liang, and Quoc V. Le, <a href="https://arxiv.org/abs/1901.11117"><span class="url">https://arxiv.org/abs/1901.11117</span></a>.</p>
<p class="normal">Let’s go over the main<a id="_idIndexMarker698"/> ideas:</p>
<ul>
<li class="bulletList">Transformers are a class of architectures that are manually drafted. The Evolved Transformers researchers <a id="_idIndexMarker699"/>applied <strong class="keyWord">Neural Architecture Search</strong> (<strong class="keyWord">NAS</strong>), a set of automatic optimization techniques that learn how to combine basic architectural building blocks to find models better than the ones manually designed by humans.</li>
<li class="bulletList">NAS was applied to both the transformer encoder and decoder blocks resulting in a new architecture shown in <em class="italic">Figures 6.8</em> and <em class="italic">6.9</em>.</li>
</ul>
<p class="normal">Evolved Transformers demonstrated consistent improvement compared to the original transformer architecture. The model is at the <a id="_idIndexMarker700"/>core of MEENA, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from social media conversations on public domains. MEENA uses Evolved Transformers with 2.6 billion parameters with a single Evolved Transformer encoder block and 13 Evolved Transformer decoder blocks. The objective function used for training focuses on minimizing perplexity, the uncertainty of predicting the next token. MEENA <a id="_idIndexMarker701"/>can conduct conversations that are more sensitive and specific<a id="_idIndexMarker702"/> than existing state-of-the-art chatbots. Refer to the Google blog post <em class="italic">Towards a Conversational Agent that Can Chat About…Anything</em>, <a href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.xhtml"><span class="url">https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.xhtml</span></a>:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="677" src="../Images/B18331_06_08.png" width="885"/></figure>
<p class="packt_figref">Figure 6.8: The Evolved Transformer encoder block, source: https://arxiv.org/pdf/1901.11117.pdf </p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="657" src="../Images/B18331_06_09.png" width="754"/></figure>
<p class="packt_figref">Figure 6.9: The Evolved Transformer decoder block, source: https://arxiv.org/pdf/1901.11117.pdf </p>
<h2 class="heading-2" id="_idParaDest-187">LaMDA </h2>
<p class="normal">LaMDA is a model introduced in 2022 by <a id="_idIndexMarker703"/>Google’s researchers in <em class="italic">LaMDA: Language Models for Dialog Applications</em> by Romal Thoppilan, et al., <a href="https://arxiv.org/abs/2201.08239"><span class="url">https://arxiv.org/abs/2201.08239</span></a>. It is a family of transformer-based neural language models specialized for dialog. Let’s <a id="_idIndexMarker704"/>see the key intuitions:</p>
<ul>
<li class="bulletList">In the pretraining stage, LaMDA uses a dataset of 1.56 trillion words — nearly 40x more than what was previously used for LLMs — from public dialog data and other public web documents. After tokenizing the dataset into 2.81 trillion SentencePiece tokens, the pretraining predicts every next token in a sentence, given the previous tokens.</li>
<li class="bulletList">In the fine-tuning stage, LaMDA performs a mix of generative tasks to generate natural-language responses to given contexts, and classification tasks on whether a response is safe and of high quality. The combination of generation and classification provides the final answer (see <em class="italic">Figure 6.10</em>).</li>
<li class="bulletList">LaMDA defines a robust set <a id="_idIndexMarker705"/>of metrics for quality, safety, and groundedness:<ul>
<li class="bulletList">Quality: This measure is decomposed into three dimensions, <strong class="keyWord">Sensibleness, Specificity, and Interestingness</strong> (<strong class="keyWord">SSI</strong>). Sensibleness considers whether the <a id="_idIndexMarker706"/>model produces responses that make sense in the dialog context. Specificity judges whether the response is specific to the preceding dialog context, and not a generic response that could apply to most contexts. Interestingness measures whether the model produces responses that are also insightful, unexpected, or witty.</li>
<li class="bulletList">Safety: Takes into account how to avoid any unintended result that creates risks of harm for the user, and to avoid reinforcing unfair bias.</li>
<li class="bulletList">Groundedness: Takes into account plausible information that is, however, contradicting information that can be supported by authoritative external sources.</li>
</ul>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="95" src="../Images/B18331_06_10.png" width="745"/></figure>
<p class="packt_figref">Figure 6.10: LaMDA generates and then scores a response candidate. Source: https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml </p>
</li>
</ul>
<p class="normal">LaMDA demonstrated results that were impressively close to the human brain ones. According to Google (<a href="https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml"><span class="url">https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml</span></a>), LaMDA significantly outperformed the pretrained model in every dimension and across all model sizes. Quality metrics (Sensibleness, Specificity, and Interestingness) generally improved with the number of model parameters, with or without fine-tuning. Safety did not seem to benefit from model scaling alone, but it did improve with fine-tuning. Groundedness improved as model size increased, perhaps because larger models have a greater capacity to memorize uncommon knowledge, but fine-tuning allows the model to <a id="_idIndexMarker707"/>access external knowledge sources and to effectively shift some of the load of remembering knowledge to an external knowledge source. With fine-tuning, the quality gap to human levels can be shrunk, though the model performance remains below human levels in safety and groundedness:</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated with medium confidence" height="759" src="../Images/B18331_06_11.png" width="796"/></figure>
<p class="packt_figref">Figure 6.11: LaMDA performance – source: https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml </p>
<h2 class="heading-2" id="_idParaDest-188">Switch Transformer</h2>
<p class="normal">The Switch Transformer is a model<a id="_idIndexMarker708"/> introduced in 2021 by Google’s researchers in <em class="italic">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</em> by William Fedus, Barret Zoph, and Noam Shazeer, introduced in <a href="https://arxiv.org/abs/2101.03961"><span class="url">https://arxiv.org/abs/2101.03961</span></a>.</p>
<p class="normal">Let’s look at the key intuitions: </p>
<ul>
<li class="bulletList">The Switch Transformer <a id="_idIndexMarker709"/>was trained from 7 billion to 1.6 trillion parameters. As discussed, a typical transformer is a deep stack of multi-headed self-attention layers, and at the end of each layer, there’s an FFN aggregating the outputs coming from its multiple heads. The Switch Transformer replaces this single FFN with multiple FFNs and calls them “experts.” On each forward pass, at each layer, for each token at the input, the model activates exactly one expert:
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="409" src="../Images/B18331_06_12.png" width="820"/></figure>
<p class="packt_figref">Fig 6.12: The Switch Transformer with multiple routing FFN – The dense FFN layer present in the transformer is replaced with a sparse Switch FFN layer (light blue). Source: https://arxiv.org/pdf/2101.03961.pdf </p>
</li>
</ul>
<ul>
<li class="bulletList">Switch-Base (7 billion parameters) and Switch-Large (26 billion parameters) outperformed T5-Base (0.2 billion parameters) and T5-Large (0.7 billion parameters) on tasks such as language modeling, classification, coreference resolution, question answering, and summarization.</li>
</ul>
<p class="normal">An example implementation of Switch Transformer is available at <a href="https://keras.io/examples/nlp/text_classification_with_switch_transformer/"><span class="url">https://keras.io/examples/nlp/text_classification_with_switch_transformer/</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-189">RETRO</h2>
<p class="normal"><strong class="keyWord">RETRO</strong> (<strong class="keyWord">Retrieval-Enhanced Transformer</strong>) is a<a id="_idIndexMarker710"/> retrieval-enhanced autoregressive language model introduced by DeepMind in 2022 in <em class="italic">Improving language models by retrieving from trillions of tokens</em> by Sebastian Borgeaud et al., <a href="https://arxiv.org/pdf/2112.04426/"><span class="url">https://arxiv.org/pdf/2112.04426/</span></a>. Let’s look at the key<a id="_idIndexMarker711"/> intuitions:</p>
<ul>
<li class="bulletList">Scaling the number of parameters in an LLM has proven to be a way to improve the quality of the results. However, this approach is not sustainable because it is computationally expensive. </li>
<li class="bulletList">RETRO couples a retrieval<strong class="keyWord"> Database</strong> (<strong class="keyWord">DB</strong>) with a <a id="_idIndexMarker712"/>transformer in a hybrid architecture. The idea is first to search with the Nearest Neighbors algorithm on pre-computed BERT embeddings stored in a retrieval DB. Then, these embeddings are used as input to a transformer’s encoder.</li>
<li class="bulletList">The combination of retrieval and transformers allows RETRO (scaled from 150 million to 7 billion non-embedding parameters) to save on the number of parameters used by the LLM.</li>
</ul>
<p class="normal">For instance, consider the sample query “The 2021 Women’s US Open was won” and <em class="italic">Figure 6.13</em>, where the cached BERT embeddings are passed to a transformer encoder to get the final result:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="538" src="../Images/B18331_06_13.png" width="812"/></figure>
<p class="packt_figref">Figure 6.13: A high-level overview of Retrieval Enhanced Transformers (RETRO). Source: https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens</p>
<h2 class="heading-2" id="_idParaDest-190">Pathways and PaLM</h2>
<p class="normal">Google Research announced Pathways (<a href="https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/"><span class="url">https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/</span></a>), a single model that could generalize across domains and tasks while being highly efficient. Then, Google<a id="_idIndexMarker713"/> introduced <strong class="keyWord">Pathways Language Model</strong> (<strong class="keyWord">PaLM</strong>), a 540-billion parameter, dense decoder-only transformer model, which enabled us to efficiently train a single model across multiple TPU v4 Pods. Google evaluated PaLM on hundreds of language understanding and generation tasks and found that it achieves state-of-the-art performance across most tasks, by significant margins in many cases (see <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.xhtml?m=1"><span class="url">https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.xhtml?m=1</span></a>).</p>
<h1 class="heading-1" id="_idParaDest-191">Implementation</h1>
<p class="normal">In this section, we will go<a id="_idIndexMarker714"/> through a few tasks using transformers.</p>
<h2 class="heading-2" id="_idParaDest-192">Transformer reference implementation: An example of translation</h2>
<p class="normal">In this section, we will briefly review a <a id="_idIndexMarker715"/>transformer reference implementation available at <a href="https://www.tensorflow.org/text/tutorials/transformer"><span class="url">https://www.tensorflow.org/text/tutorials/transformer</span></a> and specifically, we will use the opportunity to run the code in a Google Colab.</p>
<p class="normal">Not everyone realizes the number of GPUs it takes to train a transformer. Luckily, you can play with resources available for free at <a href="https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb"><span class="url">https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb</span></a>.</p>
<p class="normal">Note that implementing transformers from scratch is probably not the best choice unless you need to realize some very specific customization or you are interested in core research. If you are not interested in learning the internals, then you can skip to the next section. Our tutorial is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. The specific task we are going to perform is translating from Portuguese to English. Let’s have a look at the code, step by step:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, let’s install datasets and import the right libraries. Note that the Colab available online is apparently missing the line <code class="inlineCode">import tensorflow_text</code>, which is, however, added here:
        <pre class="programlisting code"><code class="hljs-code">!pip install tensorflow_datasets
!pip install -U <span class="hljs-string">'tensorflow-text==2.8.*'</span>
<span class="hljs-keyword">import</span> logging
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> tensorflow_text
<span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
logging.getLogger(<span class="hljs-string">'tensorflow'</span>).setLevel(logging.ERROR)  <span class="hljs-comment"># suppress warnings</span>
</code></pre>
</li>
<li class="numberedList">Then, load the Portuguese to English dataset:
        <pre class="programlisting code"><code class="hljs-code">examples, metadata = tfds.load(<span class="hljs-string">'ted_hrlr_translate/pt_to_en'</span>, with_info=<span class="hljs-literal">True</span>,
                               as_supervised=<span class="hljs-literal">True</span>)
train_examples, val_examples = examples[<span class="hljs-string">'train'</span>], examples[<span class="hljs-string">'validation'</span>]
</code></pre>
</li>
<li class="numberedList">Now, let’s convert text to sequences of token IDs, which are used as indices into an embedding:
        <pre class="programlisting code"><code class="hljs-code">model_name = <span class="hljs-string">'ted_hrlr_translate_pt_en_converter'</span>
tf.keras.utils.get_file(
    <span class="hljs-string">f'</span><span class="hljs-subst">{model_name}</span><span class="hljs-string">.zip'</span>,
    <span class="hljs-string">f'https://storage.googleapis.com/download.tensorflow.org/models/</span><span class="hljs-subst">{model_name}</span><span class="hljs-string">.zip'</span>,
    cache_dir=<span class="hljs-string">'.'</span>, cache_subdir=<span class="hljs-string">''</span>, extract=<span class="hljs-literal">True</span>
)
tokenizers = tf.saved_model.load(model_name)
</code></pre>
</li>
<li class="numberedList">Let’s see the <a id="_idIndexMarker716"/>tokenized IDs and tokenized words:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> pt_examples, en_examples <span class="hljs-keyword">in</span> train_examples.batch(<span class="hljs-number">3</span>).take(<span class="hljs-number">1</span>):
  print(<span class="hljs-string">'&gt; Examples in Portuguese:'</span>)
<span class="hljs-keyword">for</span> en <span class="hljs-keyword">in</span> en_examples.numpy():
  <span class="hljs-built_in">print</span>(en.decode(<span class="hljs-string">'utf-8'</span>))
</code></pre>
<pre class="programlisting con"><code class="hljs-con">and when you improve searchability , you actually take away the one advantage of print , which is serendipity .
but what if it were active ?
but they did n't test for curiosity .
</code></pre>
<pre class="programlisting code"><code class="hljs-code">encoded = tokenizers.en.tokenize(en_examples)
<span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> encoded.to_list():
  <span class="hljs-built_in">print</span>(row)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]
[2, 87, 90, 107, 76, 129, 1852, 30, 3]
[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]
</code></pre>
<pre class="programlisting code"><code class="hljs-code">round_trip = tokenizers.en.detokenize(encoded)
<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> round_trip.numpy():
  <span class="hljs-built_in">print</span>(line.decode(<span class="hljs-string">'utf-8'</span>))
</code></pre>
<pre class="programlisting con"><code class="hljs-con">and when you improve searchability , you actually take away the one advantage of print , which is serendipity .
but what if it were active ?
but they did n ' t test for curiosity .
</code></pre>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="5">Now let’s create <a id="_idIndexMarker717"/>an input pipeline. First, we define a function to drop the examples longer than <code class="inlineCode">MAX_TOKENS</code>. Second, we define a function that tokenizes the batches of raw text. Third, we create the batches:
        <pre class="programlisting code"><code class="hljs-code">MAX_TOKENS=<span class="hljs-number">128</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">filter_max_tokens</span>(<span class="hljs-params">pt, en</span>):
  num_tokens = tf.maximum(tf.shape(pt)[<span class="hljs-number">1</span>],tf.shape(en)[<span class="hljs-number">1</span>])
  <span class="hljs-keyword">return</span> num_tokens &lt; MAX_TOKENS
<span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_pairs</span>(<span class="hljs-params">pt, en</span>):
    pt = tokenizers.pt.tokenize(pt)
    <span class="hljs-comment"># Convert from ragged to dense, padding with zeros.</span>
    pt = pt.to_tensor()
    en = tokenizers.en.tokenize(en)
    <span class="hljs-comment"># Convert from ragged to dense, padding with zeros.</span>
    en = en.to_tensor()
    <span class="hljs-keyword">return</span> pt, en
BUFFER_SIZE = <span class="hljs-number">20000</span>
BATCH_SIZE = <span class="hljs-number">64</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">make_batches</span>(<span class="hljs-params">ds</span>):
  <span class="hljs-keyword">return</span> (
      ds
      .cache()
      .shuffle(BUFFER_SIZE)
      .batch(BATCH_SIZE)
      .<span class="hljs-built_in">map</span>(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)
      .<span class="hljs-built_in">filter</span>(filter_max_tokens)
      .prefetch(tf.data.AUTOTUNE))
train_batches = make_batches(train_examples)
val_batches = make_batches(val_examples)
</code></pre>
</li>
<li class="numberedList">Now we add<a id="_idIndexMarker718"/> positional encoding, forcing tokens to be closer to each other based on the similarity of their meaning and their position in the sentence, in the d-dimensional embedding space:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_angles</span>(<span class="hljs-params">pos, i, d_model</span>):
  angle_rates = <span class="hljs-number">1</span> / np.power(<span class="hljs-number">10000</span>, (<span class="hljs-number">2</span> * (i//<span class="hljs-number">2</span>)) / np.float32(d_model))
  <span class="hljs-keyword">return</span> pos * angle_rates
<span class="hljs-keyword">def</span> <span class="hljs-title">positional_encoding</span>(<span class="hljs-params">position, d_model</span>):
  angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                          np.arange(d_model)[np.newaxis, :],
                          d_model)
  <span class="hljs-comment"># apply sin to even indices in the array; 2i</span>
  angle_rads[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = np.sin(angle_rads[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>])
  <span class="hljs-comment"># apply cos to odd indices in the array; 2i+1</span>
  angle_rads[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = np.cos(angle_rads[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>])
  pos_encoding = angle_rads[np.newaxis, ...]
  <span class="hljs-keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)
</code></pre>
</li>
<li class="numberedList">Let’s now focus on the masking process. The look-ahead mask is used to mask the future tokens in a sequence, with the mask indicating which entries should not be used. For instance, to predict the third token, only the first and second tokens will be used, and to predict the fourth token, only the first, second, and third tokens will be used, and so on:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">create_padding_mask</span>(<span class="hljs-params">seq</span>):
  seq = tf.cast(tf.math.equal(seq, <span class="hljs-number">0</span>), tf.float32)
  <span class="hljs-comment"># add extra dimensions to add the padding</span>
  <span class="hljs-comment"># to the attention logits.</span>
  <span class="hljs-keyword">return</span> seq[:, tf.newaxis, tf.newaxis, :]  <span class="hljs-comment"># (batch_size, 1, 1, seq_len)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">create_look_ahead_mask</span>(<span class="hljs-params">size</span>):
  mask = <span class="hljs-number">1</span> - tf.linalg.band_part(tf.ones((size, size)), -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
  <span class="hljs-keyword">return</span> mask  <span class="hljs-comment"># (seq_len, seq_len)</span>
</code></pre>
</li>
<li class="numberedList">We are getting closer <a id="_idIndexMarker719"/>and closer to the essence of transformers. Let’s define the attention function as a scaled dot-product:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">scaled_dot_product_attention</span>(<span class="hljs-params">q, k, v, mask</span>):
  <span class="hljs-string">"""</span><span class="hljs-string">Calculate the attention weights.</span>
<span class="hljs-string">  q, k, v must have matching leading dimensions.</span>
<span class="hljs-string">  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.</span>
<span class="hljs-string">  The mask has different shapes depending on its type(padding or look ahead)</span>
<span class="hljs-string">  but it must be broadcastable for addition.</span>
<span class="hljs-string">  Args:</span>
<span class="hljs-string">    q: query shape == (..., seq_len_q, depth)</span>
<span class="hljs-string">    k: key shape == (..., seq_len_k, depth)</span>
<span class="hljs-string">    v: value shape == (..., seq_len_v, depth_v)</span>
<span class="hljs-string">    mask: Float tensor with shape broadcastable</span>
<span class="hljs-string">          to (..., seq_len_q, seq_len_k). Defaults to None.</span>
<span class="hljs-string">  Returns:</span>
<span class="hljs-string">    output, attention_weights</span>
<span class="hljs-string">  """</span>
  matmul_qk = tf.matmul(q, k, transpose_b=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># (..., seq_len_q, seq_len_k)</span>
  <span class="hljs-comment"># scale matmul_qk</span>
  dk = tf.cast(tf.shape(k)[-<span class="hljs-number">1</span>], tf.float32)
  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
  <span class="hljs-comment"># add the mask to the scaled tensor.</span>
  <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
    scaled_attention_logits += (mask * -<span class="hljs-number">1e9</span>)
  <span class="hljs-comment"># softmax is normalized on the last axis (seq_len_k) so that the scores</span>
  <span class="hljs-comment"># add up to 1.</span>
  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (..., seq_len_q, seq_len_k)</span>
  output = tf.matmul(attention_weights, v)  <span class="hljs-comment"># (..., seq_len_q, depth_v)</span>
  <span class="hljs-keyword">return</span> output, attention_weights
</code></pre>
</li>
<li class="numberedList">Now that the<a id="_idIndexMarker720"/> attention is defined, we need to implement the multi-head mechanism. There are three parts: linear layers, scaled dot-product attention, and the final linear layer (see <em class="italic">Figure 6.14</em>):
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="499" src="../Images/B18331_06_14.png" width="358"/></figure>
<p class="packt_figref">Figure 6.14: Multi-head attention</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiHeadAttention</span>(tf.keras.layers.Layer):
  <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,*, d_model, num_heads</span>):
    <span class="hljs-built_in">super</span>(MultiHeadAttention, self).__init__()
    self.num_heads = num_heads
    self.d_model = d_model
    <span class="hljs-keyword">assert</span> d_model % self.num_heads == <span class="hljs-number">0</span>
    self.depth = d_model // self.num_heads
    self.wq = tf.keras.layers.Dense(d_model)
    self.wk = tf.keras.layers.Dense(d_model)
    self.wv = tf.keras.layers.Dense(d_model)
    self.dense = tf.keras.layers.Dense(d_model)
  <span class="hljs-keyword">def</span> <span class="hljs-title">split_heads</span>(<span class="hljs-params">self, x, batch_size</span>):
    <span class="hljs-string">"""Split the last dimension into (num_heads, depth).</span>
<span class="hljs-string">    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)</span>
<span class="hljs-string">    """</span>
    x = tf.reshape(x, (batch_size, -<span class="hljs-number">1</span>, self.num_heads, self.depth))
    <span class="hljs-keyword">return</span> tf.transpose(x, perm=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])
  <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, v, k, q, mask</span>):
    batch_size = tf.shape(q)[<span class="hljs-number">0</span>]
    q = self.wq(q)  <span class="hljs-comment"># (batch_size, seq_len, d_model)</span>
    k = self.wk(k)  <span class="hljs-comment"># (batch_size, seq_len, d_model)</span>
    v = self.wv(v)  <span class="hljs-comment"># (batch_size, seq_len, d_model)</span>
    q = self.split_heads(q, batch_size)  <span class="hljs-comment"># (batch_size, num_heads, seq_len_q, depth)</span>
    k = self.split_heads(k, batch_size)  <span class="hljs-comment"># (batch_size, num_heads, seq_len_k, depth)</span>
    v = self.split_heads(v, batch_size)  <span class="hljs-comment"># (batch_size, num_heads, seq_len_v, depth)</span>
    <span class="hljs-comment"># scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)</span>
    <span class="hljs-comment"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span>
    scaled_attention, attention_weights = scaled_dot_product_attention(
        q, k, v, mask)
    scaled_attention = tf.transpose(scaled_attention, perm=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])  <span class="hljs-comment"># (batch_size, seq_len_q, num_heads, depth)</span>
    concat_attention = tf.reshape(scaled_attention,
                                  (batch_size, -<span class="hljs-number">1</span>, self.d_model))  <span class="hljs-comment"># (batch_size, seq_len_q, d_model)</span>
    output = self.dense(concat_attention)  <span class="hljs-comment"># (batch_size, seq_len_q, d_model)</span>
    <span class="hljs-keyword">return</span> output, attention_weights
</code></pre>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="10">Now, we can define a<a id="_idIndexMarker721"/> point-wise feedforward network that consists of two fully connected layers with a ReLU activation in between:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">point_wise_feed_forward_network</span>(<span class="hljs-params">d_model, dff</span>):
 <span class="hljs-keyword">return</span> tf.keras.Sequential([
     tf.keras.layers.Dense(dff, activation='relu'),  <span class="hljs-comment"># (batch_size, seq_len, dff)</span>
     tf.keras.layers.Dense(d_model)  <span class="hljs-comment"># (batch_size, seq_len, d_model)</span>
 ])
</code></pre>
</li>
<li class="numberedList">We can now concentrate on defining the encoder and decoder parts as described in <em class="italic">Figure 6.15</em>. Remember that the traditional transformer takes the input sentence through <em class="italic">N</em> encoder layers, while the decoder uses the encoder output and its own input (self-attention) to predict the next word. Each encoder layer has sublayers made by multi-head attention (with a padding mask) and then point-wise feedforward networks. Each sublayer uses a residual connection to contain the problem of vanishing gradient, and a normalization layer:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">EncoderLayer</span>(tf.keras.layers.Layer):
  <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,*, d_model, num_heads, dff, rate=</span><span class="hljs-number">0.1</span>):
    <span class="hljs-built_in">super</span>(EncoderLayer, self).__init__()
    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)
    self.ffn = point_wise_feed_forward_network(d_model, dff)
    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=<span class="hljs-number">1e-6</span>)
    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=<span class="hljs-number">1e-6</span>)
    self.dropout1 = tf.keras.layers.Dropout(rate)
    self.dropout2 = tf.keras.layers.Dropout(rate)
  <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, x, training, mask</span>):
    attn_output, _ = self.mha(x, x, x, mask)  <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
    attn_output = self.dropout1(attn_output, training=training)
    out1 = self.layernorm1(x + attn_output)  <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
    ffn_output = self.ffn(out1)  <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
    ffn_output = self.dropout2(ffn_output, training=training)
    out2 = self.layernorm2(out1 + ffn_output)  <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
    <span class="hljs-keyword">return</span> out2
</code></pre>
</li>
<li class="numberedList">Each decoder layer is made of sublayers. First, a masked multi-head attention (with <a id="_idIndexMarker722"/>a look-ahead mask and padding mask). Then, a multi-head attention (with a padding mask), V (value), and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublayer and, finally, the point-wise feedforward networks:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">DecoderLayer</span>(tf.keras.layers.Layer):
  <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,*, d_model, num_heads, dff, rate=</span><span class="hljs-number">0.1</span>):
    <span class="hljs-built_in">super</span>(DecoderLayer, self).__init__()
    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)
    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)
    self.ffn = point_wise_feed_forward_network(d_model, dff)
    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=<span class="hljs-number">1e-6</span>)
    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=<span class="hljs-number">1e-6</span>)
    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=<span class="hljs-number">1e-6</span>)
    self.dropout1 = tf.keras.layers.Dropout(rate)
    self.dropout2 = tf.keras.layers.Dropout(rate)
    self.dropout3 = tf.keras.layers.Dropout(rate)
  <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, x, enc_output, training,</span>
<span class="hljs-params">           look_ahead_mask, padding_mask</span>):
    <span class="hljs-comment"># enc_output.shape == (batch_size, input_seq_len, d_model)</span>
    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  <span class="hljs-comment"># (batch_size, target_seq_len, d_model)</span>
    attn1 = self.dropout1(attn1, training=training)
    out1 = self.layernorm1(attn1 + x)
    attn2, attn_weights_block2 = self.mha2(
        enc_output, enc_output, out1, padding_mask)  <span class="hljs-comment"># (batch_size, target_seq_len, d_model)</span>
    attn2 = self.dropout2(attn2, training=training)
    out2 = self.layernorm2(attn2 + out1)  <span class="hljs-comment"># (batch_size, target_seq_len, d_model)</span>
    ffn_output = self.ffn(out2)  <span class="hljs-comment"># (batch_size, target_seq_len, d_model)</span>
    ffn_output = self.dropout3(ffn_output, training=training)
    out3 = self.layernorm3(ffn_output + out2)  <span class="hljs-comment"># (batch_size, target_seq_len, d_model)</span>
    <span class="hljs-keyword">return</span> out3, attn_weights_block1, attn_weights_block2
</code></pre>
</li>
<li class="numberedList">Now that we have defined the encoder layer, we can use it to define the proper encoder. This <a id="_idIndexMarker723"/>consists of three stages: input embedding, positional encoding, and <em class="italic">N</em> encoder layers:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span>(tf.keras.layers.Layer):
  <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,*, num_layers, d_model, num_heads, dff, input_vocab_size,</span>
<span class="hljs-params">               rate=</span><span class="hljs-number">0.1</span>):
    <span class="hljs-built_in">super</span>(Encoder, self).__init__()
    self.d_model = d_model
    self.num_layers = num_layers
    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
    self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)
    self.enc_layers = [
        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)]
    self.dropout = tf.keras.layers.Dropout(rate)
  <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, x, training, mask</span>):
    seq_len = tf.shape(x)[<span class="hljs-number">1</span>]
    <span class="hljs-comment"># adding embedding and position encoding.</span>
    x = self.embedding(x)  <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x += self.pos_encoding[:, :seq_len, :]
    x = self.dropout(x, training=training)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):
      x = self.enc_layers[i](x, training, mask)
    <span class="hljs-keyword">return</span> x  <span class="hljs-comment"># (batch_size, input_seq_len, d_model)</span>
</code></pre>
</li>
<li class="numberedList"> We can now focus our attention on the decoder itself. It consists of output embedding, positional <a id="_idIndexMarker724"/>encoding, and <em class="italic">N</em> decoder layers:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(tf.keras.layers.Layer):
  <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,*, num_layers, d_model, num_heads, dff, target_vocab_size,</span>
<span class="hljs-params">               rate=</span><span class="hljs-number">0.1</span>):
    <span class="hljs-built_in">super</span>(Decoder, self).__init__()
    self.d_model = d_model
    self.num_layers = num_layers
    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
    self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)
    self.dec_layers = [
        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers)]
    self.dropout = tf.keras.layers.Dropout(rate)
  <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, x, enc_output, training,</span>
<span class="hljs-params">           look_ahead_mask, padding_mask</span>):
    seq_len = tf.shape(x)[<span class="hljs-number">1</span>]
    attention_weights = {}
    x = self.embedding(x)  <span class="hljs-comment"># (batch_size, target_seq_len, d_model)</span>
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x += self.pos_encoding[:, :seq_len, :]
    x = self.dropout(x, training=training)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):
      x, block1, block2 = self.dec_layers[i](x, enc_output, training,
                                             look_ahead_mask, padding_mask)
      attention_weights[<span class="hljs-string">f'decoder_layer</span><span class="hljs-subst">{i+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string">_block1'</span>] = block1
      attention_weights[<span class="hljs-string">f'decoder_layer</span><span class="hljs-subst">{i+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string">_block2'</span>] = block2
    <span class="hljs-comment"># x.shape == (batch_size, target_seq_len, d_model)</span>
    <span class="hljs-keyword">return</span> x, attention_weights
</code></pre>
</li>
<li class="numberedList">Now that we have<a id="_idIndexMarker725"/> defined the encoder and decoder, we can now turn our attention to the transformer itself, which is composed of an encoder, a decoder, and a final linear layer (see <em class="italic">Figure 6.15</em>):
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Transformer</span>(tf.keras.Model):
  <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,*, num_layers, d_model, num_heads, dff, input_vocab_size,</span>
<span class="hljs-params">               target_vocab_size, rate=</span><span class="hljs-number">0.1</span>):
    <span class="hljs-built_in">super</span>().__init__()
    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           input_vocab_size=input_vocab_size, rate=rate)
    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           target_vocab_size=target_vocab_size, rate=rate)
    self.final_layer = tf.keras.layers.Dense(target_vocab_size)
  <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs, training</span>):
    <span class="hljs-comment"># Keras models prefer if you pass all your inputs in the first argument</span>
    inp, tar = inputs
    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)
    enc_output = self.encoder(inp, training, enc_padding_mask)  <span class="hljs-comment"># (batch_size, inp_seq_len, d_model)</span>
    <span class="hljs-comment"># dec_output.shape == (batch_size, tar_seq_len, d_model)</span>
    dec_output, attention_weights = self.decoder(
        tar, enc_output, training, look_ahead_mask, dec_padding_mask)
    final_output = self.final_layer(dec_output)  <span class="hljs-comment"># (batch_size, tar_seq_len, target_vocab_size)</span>
    <span class="hljs-keyword">return</span> final_output, attention_weights
  <span class="hljs-keyword">def</span> <span class="hljs-title">create_masks</span>(<span class="hljs-params">self, inp, tar</span>):
    <span class="hljs-comment"># Encoder padding mask</span>
    enc_padding_mask = create_padding_mask(inp)
    <span class="hljs-comment"># Used in the 2nd attention block in the decoder.</span>
    <span class="hljs-comment"># This padding mask is used to mask the encoder outputs.</span>
    dec_padding_mask = create_padding_mask(inp)
    <span class="hljs-comment"># Used in the 1st attention block in the decoder.</span>
    <span class="hljs-comment"># It is used to pad and mask future tokens in the input received by</span>
    <span class="hljs-comment"># the decoder.</span>
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[<span class="hljs-number">1</span>])
    dec_target_padding_mask = create_padding_mask(tar)
    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)
    <span class="hljs-keyword">return</span> enc_padding_mask, look_ahead_mask, dec_padding_mask
</code></pre>
</li>
</ol>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="701" src="../Images/B18331_06_15.png" width="617"/></figure>
<p class="packt_figref">Figure 6.15: The traditional transformer</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="16">We are almost <a id="_idIndexMarker726"/>done. We just need to define hyperparameters and the optimizer, using exactly the same settings as the seminal paper, and the loss function:
        <pre class="programlisting code"><code class="hljs-code">num_layers = <span class="hljs-number">4</span>
d_model = <span class="hljs-number">128</span>
dff = <span class="hljs-number">512</span>
num_heads = <span class="hljs-number">8</span>
dropout_rate = <span class="hljs-number">0.1</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">CustomSchedule</span>(tf.keras.optimizers.schedules.LearningRateSchedule):
  <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, d_model, warmup_steps=</span><span class="hljs-number">4000</span>):
    <span class="hljs-built_in">super</span>(CustomSchedule, self).__init__()
    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)
    self.warmup_steps = warmup_steps
  <span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span>(<span class="hljs-params">self, step</span>):
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -<span class="hljs-number">1.5</span>)
    <span class="hljs-keyword">return</span> tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
learning_rate = CustomSchedule(d_model)
optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=<span class="hljs-number">0.9</span>, beta_2=<span class="hljs-number">0.98</span>,
                                     epsilon=<span class="hljs-number">1e-9</span>)
<span class="hljs-keyword">def</span> <span class="hljs-title">loss_function</span>(<span class="hljs-params">real, pred</span>):
  mask = tf.math.logical_not(tf.math.equal(real, <span class="hljs-number">0</span>))
  loss_ = loss_object(real, pred)
  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask
  <span class="hljs-keyword">return</span> tf.reduce_sum(loss_)/tf.reduce_sum(mask)
<span class="hljs-keyword">def</span> <span class="hljs-title">accuracy_function</span>(<span class="hljs-params">real, pred</span>):
  accuracies = tf.equal(real, tf.argmax(pred, axis=<span class="hljs-number">2</span>))
  mask = tf.math.logical_not(tf.math.equal(real, <span class="hljs-number">0</span>))
  accuracies = tf.math.logical_and(mask, accuracies)
  accuracies = tf.cast(accuracies, dtype=tf.float32)
  mask = tf.cast(mask, dtype=tf.float32)
  <span class="hljs-keyword">return</span> tf.reduce_sum(accuracies)/tf.reduce_sum(mask)
train_loss = tf.keras.metrics.Mean(name=<span class="hljs-string">'train_loss'</span>)
train_accuracy = tf.keras.metrics.Mean(name=<span class="hljs-string">'train_accuracy'</span>)
</code></pre>
</li>
<li class="numberedList">Time to define the transformer. Let’s see the code:
        <pre class="programlisting code"><code class="hljs-code">transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),
    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),
    rate=dropout_rate)
</code></pre>
</li>
<li class="numberedList">Let’s also define the<a id="_idIndexMarker727"/> checkpoints with the following code:
        <pre class="programlisting code"><code class="hljs-code">checkpoint_path = <span class="hljs-string">'./checkpoints/train'</span>
ckpt = tf.train.Checkpoint(transformer=transformer,
                           optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=<span class="hljs-number">5</span>)
<span class="hljs-comment"># if a checkpoint exists, restore the latest checkpoint.</span>
<span class="hljs-keyword">if</span> ckpt_manager.latest_checkpoint:
  ckpt.restore(ckpt_manager.latest_checkpoint)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">'Latest checkpoint restored!!'</span>)
</code></pre>
</li>
<li class="numberedList">Remember that the transformer is autoregressive. The current output is used to predict what will happen next. We use a look-ahead mask, to prevent the model from peeking at the expected output. We are now ready to define <code class="inlineCode">train_step</code>:
        <pre class="programlisting code"><code class="hljs-code">train_step_signature = [
    tf.TensorSpec(shape=(<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>), dtype=tf.int64),
    tf.TensorSpec(shape=(<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>), dtype=tf.int64),
]
<span class="hljs-meta">@tf.function(</span><span class="hljs-params">input_signature=train_step_signature</span><span class="hljs-meta">)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">train_step</span>(<span class="hljs-params">inp, tar</span>):
  tar_inp = tar[:, :-<span class="hljs-number">1</span>]
  tar_real = tar[:, <span class="hljs-number">1</span>:]
  <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
    predictions, _ = transformer([inp, tar_inp],
                                 training = <span class="hljs-literal">True</span>)
    loss = loss_function(tar_real, predictions)
  gradients = tape.gradient(loss, transformer.trainable_variables)
  optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, transformer.trainable_variables))
  train_loss(loss)
  train_accuracy(accuracy_function(tar_real, predictions))
EPOCHS = <span class="hljs-number">20</span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):
  start = time.time()
  train_loss.reset_states()
  train_accuracy.reset_states()
  <span class="hljs-comment"># inp -&gt; portuguese, tar -&gt; english</span>
  <span class="hljs-keyword">for</span> (batch, (inp, tar)) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_batches):
    train_step(inp, tar)
    <span class="hljs-keyword">if</span> batch % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:
      <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Epoch </span><span class="hljs-subst">{epoch + </span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> Batch </span><span class="hljs-subst">{batch}</span><span class="hljs-string"> Loss </span><span class="hljs-subst">{train_loss.result():</span><span class="hljs-number">.4</span><span class="hljs-subst">f}</span><span class="hljs-string"> Accuracy </span><span class="hljs-subst">{train_accuracy.result():</span><span class="hljs-number">.4</span><span class="hljs-subst">f}</span><span class="hljs-string">'</span>)
  <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:
    ckpt_save_path = ckpt_manager.save()
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Saving checkpoint for epoch </span><span class="hljs-subst">{epoch+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> at </span><span class="hljs-subst">{ckpt_save_path}</span><span class="hljs-string">'</span>)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Epoch </span><span class="hljs-subst">{epoch + </span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> Loss </span><span class="hljs-subst">{train_loss.result():</span><span class="hljs-number">.4</span><span class="hljs-subst">f}</span><span class="hljs-string"> Accuracy </span><span class="hljs-subst">{train_accuracy.result():</span><span class="hljs-number">.4</span><span class="hljs-subst">f}</span><span class="hljs-string">'</span>)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Time taken for 1 epoch: </span><span class="hljs-subst">{time.time() - start:</span><span class="hljs-number">.2</span><span class="hljs-subst">f}</span><span class="hljs-string"> secs\n'</span>)
</code></pre>
<p class="normal">After running the<a id="_idIndexMarker728"/> training step in Colab, we get the following situation:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 20 Loss 1.5030 Accuracy 0.6720
Time taken for 1 epoch: 169.01 secs
</code></pre>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="20">We are now ready for<a id="_idIndexMarker729"/> translation. The following steps are used to translate:<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Encode the input sentence using the Portuguese tokenizer (<code class="inlineCode">tokenizers.pt</code>). </li>
<li class="numberedList">The decoder input is initialized to the [START] token.</li>
<li class="numberedList">Calculate the padding masks and the look-ahead masks.</li>
<li class="numberedList">The decoder then outputs the predictions by looking at the encoder output and its own output (self-attention).</li>
<li class="numberedList">Concatenate the predicted token to the decoder input and pass it to the decoder:</li>
</ol>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Translator</span>(tf.Module):
  <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, tokenizers, transformer</span>):
    self.tokenizers = tokenizers
    self.transformer = transformer
  <span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span>(<span class="hljs-params">self, sentence, max_length=MAX_TOKENS</span>):
    <span class="hljs-comment"># input sentence is portuguese, hence adding the start and end token</span>
    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(sentence, tf.Tensor)
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(sentence.shape) == <span class="hljs-number">0</span>:
      sentence = sentence[tf.newaxis]
    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()
    encoder_input = sentence
    <span class="hljs-comment"># As the output language is english, initialize the output with the</span>
    <span class="hljs-comment"># english start token.</span>
    start_end = self.tokenizers.en.tokenize([<span class="hljs-string">''</span>])[<span class="hljs-number">0</span>]
    start = start_end[<span class="hljs-number">0</span>][tf.newaxis]
    end = start_end[<span class="hljs-number">1</span>][tf.newaxis]
    <span class="hljs-comment"># 'tf.TensorArray' is required here (instead of a python list) so that the</span>
    <span class="hljs-comment"># dynamic-loop can be traced by 'tf.function'.</span>
    output_array = tf.TensorArray(dtype=tf.int64, size=<span class="hljs-number">0</span>, dynamic_size=<span class="hljs-literal">True</span>)
    output_array = output_array.write(<span class="hljs-number">0</span>, start)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tf.<span class="hljs-built_in">range</span>(max_length):
      output = tf.transpose(output_array.stack())
      predictions, _ = self.transformer([encoder_input, output], training=<span class="hljs-literal">False</span>)
      <span class="hljs-comment"># select the last token from the seq_len dimension</span>
      predictions = predictions[:, -<span class="hljs-number">1</span>:, :]  <span class="hljs-comment"># (batch_size, 1, vocab_size)</span>
      predicted_id = tf.argmax(predictions, axis=-<span class="hljs-number">1</span>)
      <span class="hljs-comment"># concatentate the predicted_id to the output which is given to the decoder</span>
      <span class="hljs-comment"># as its input.</span>
      output_array = output_array.write(i+<span class="hljs-number">1</span>, predicted_id[<span class="hljs-number">0</span>])
      <span class="hljs-keyword">if</span> predicted_id == end:
        <span class="hljs-keyword">break</span>
    output = tf.transpose(output_array.stack())
    <span class="hljs-comment"># output.shape (1, tokens)</span>
    text = tokenizers.en.detokenize(output)[<span class="hljs-number">0</span>]  <span class="hljs-comment"># shape: ()</span>
    tokens = tokenizers.en.lookup(output)[<span class="hljs-number">0</span>]
    <span class="hljs-comment"># 'tf.function' prevents us from using the attention_weights that were</span>
    <span class="hljs-comment"># calculated on the last iteration of the loop. So recalculate them outside</span>
    <span class="hljs-comment"># the loop.</span>
    _, attention_weights = self.transformer([encoder_input, output[:,:-<span class="hljs-number">1</span>]], training=<span class="hljs-literal">False</span>)
    <span class="hljs-keyword">return</span> text, tokens, attention_weights
</code></pre>
</li>
<li class="numberedList">Let’s call the translator <a id="_idIndexMarker730"/>on a sample sentence with this code snippet:
        <pre class="programlisting code"><code class="hljs-code">translator = Translator(tokenizers, transformer)
<span class="hljs-keyword">def</span> <span class="hljs-title">print_translation</span>(<span class="hljs-params">sentence, tokens, ground_truth</span>):
  <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{</span><span class="hljs-string">"Input:"</span><span class="hljs-subst">:15s}</span><span class="hljs-string">: </span><span class="hljs-subst">{sentence}</span><span class="hljs-string">'</span>)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{</span><span class="hljs-string">"Prediction"</span><span class="hljs-subst">:15s}</span><span class="hljs-string">: </span><span class="hljs-subst">{tokens.numpy().decode(</span><span class="hljs-string">"utf-8"</span><span class="hljs-subst">)}</span><span class="hljs-string">'</span>)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">f'</span><span class="hljs-subst">{</span><span class="hljs-string">"Ground truth"</span><span class="hljs-subst">:15s}</span><span class="hljs-string">: </span><span class="hljs-subst">{ground_truth}</span><span class="hljs-string">'</span>)
sentence = <span class="hljs-string">'os meus vizinhos ouviram sobre esta ideia.'</span>
ground_truth = <span class="hljs-string">'and my neighboring homes heard about this idea .'</span>
translated_text, translated_tokens, attention_weights = translator(
    tf.constant(sentence))
print_translation(sentence, translated_text, ground_truth)
</code></pre>
<p class="normal">Getting as<a id="_idIndexMarker731"/> the result:</p>
<pre class="programlisting con"><code class="hljs-con">Input:         : os meus vizinhos ouviram sobre esta ideia.
Prediction     : my neighbors have heard about this idea .
Ground truth   : and my neighboring homes heard about this idea .
</code></pre>
</li>
</ol>
<p class="normal">In this detailed analysis, we have discussed how a traditional transformer is implemented taking positional encoding, multi-head attention, and masking into account. The analyzed code is at <a href="https://www.tensorflow.org/text/tutorials/transformer"><span class="url">https://www.tensorflow.org/text/tutorials/transformer</span></a>.</p>
<p class="normal">Next, we will discuss how to use transformers making use of higher-level libraries.</p>
<h2 class="heading-2" id="_idParaDest-193">Hugging Face</h2>
<p class="normal">As discussed, implementing transformers from scratch is probably not the best choice unless you need to realize some very specific customization, or you are interested in core research. This is useful if you want to understand the internal details of a transformer architecture, or perhaps modify the transformer architecture to produce a new variant. Nowadays, there are very good libraries providing high-quality solutions. One of them is Hugging Face, which <a id="_idIndexMarker732"/>provides some efficient tools. Hugging Face is built around the idea of commercializing its open source transformers library. Let’s see why the library became so popular:</p>
<ul>
<li class="bulletList">Hugging Face <a id="_idIndexMarker733"/>provides a common API to handle many transformer architectures.</li>
<li class="bulletList">It not only provides the base model, but models with different types of “head” to handle specific tasks (for example, for the BERT architecture it provides <code class="inlineCode">TFBertModel</code>, and the <code class="inlineCode">TFBertForSequenceClassification</code> for tasks like sentiment analysis, <code class="inlineCode">TFBertForTokenClassification</code> for tasks like named entity recognition, and <code class="inlineCode">TFBertForQuestionAnswering</code> for Q and A, among others).</li>
<li class="bulletList">You can also create your own network for a specific task quite easily by using the pretrained weights provided here, for example, by using <code class="inlineCode">TFBertForPreTraining</code>.</li>
<li class="bulletList">In addition to the <code class="inlineCode">pipeline()</code> method in the next subsection, we can also define a model in the <a id="_idIndexMarker734"/>regular way and use <code class="inlineCode">fit()</code> to train it and <code class="inlineCode">predict()</code> to make inferences against it, just like a normal TF model (PyTorch also has the Trainer interface). We will see an example later on in this chapter.</li>
</ul>
<p class="normal">Now, let’s check some <a id="_idIndexMarker735"/>examples of how to use Hugging Face.</p>
<h3 class="heading-3" id="_idParaDest-194">Generating text </h3>
<p class="normal">In this section, we are going to use GPT-2 for natural language generation, a software process for producing<a id="_idIndexMarker736"/> natural language outputs. Let’s start from the beginning by installing the Hugging Face library:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">The first step is to create a dedicated virtual environment, where we can install the transformer library. In my case, I use the library for TensorFlow 2.0:
        <pre class="programlisting code"><code class="hljs-code">python -m venv .env
source .env/<span class="hljs-built_in">bin</span>/activate
pip install transformers[tf-cpu]
</code></pre>
</li>
<li class="numberedList">Then let’s verify that everything is working correctly by downloading a pretrained model used for sentiment analysis:
        <pre class="programlisting code"><code class="hljs-code">python -c <span class="hljs-string">"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"</span>
</code></pre>
<p class="normal">Since the expected sentiment should be very positive, we shall see something like the following:</p>
<pre class="programlisting con"><code class="hljs-con">[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
</code></pre>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">Now let’s focus on generating text with GPT-2:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
generator = pipeline(task=<span class="hljs-string">"text-generation"</span>)
</code></pre>
<p class="normal">You should see something like the following:</p>
<pre class="programlisting con"><code class="hljs-con">No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)
Downloading: 100%|██████████████████████████████| 665/665 [00:00&lt;00:00, 167kB/s]
Downloading: 100%|███████████████████████████| 475M/475M [03:24&lt;00:00, 2.44MB/s
</code></pre>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="4">Let’s pass some text to the generator and see what the result is. The first sentence is<a id="_idIndexMarker737"/> extracted from Tolkien’s work, the second from Einstein’s theories, and the third one comes from “Harry Potter”:
        <pre class="programlisting code"><code class="hljs-code">generator(<span class="hljs-string">"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone"</span>)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Setting 'pad_token_id' to 50256 (first 'eos_token_id') to generate sequence
[{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone and Eight for the Dwarves in their halls of rock! Three new Rings of the Elven-kings under the sky, Seven for'}]
</code></pre>
<pre class="programlisting code"><code class="hljs-code">generator (<span class="hljs-string">"The original theory of relativity is based upon the premise that all coordinate systems in relative uniform translatory motion to each other are equally valid and equivalent "</span>)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Setting 'pad_token_id' to 50256 (first 'eos_token_id') to generate sequence
[{'generated_text': 'The original theory of relativity is based upon the premise that all coordinate systems in relative uniform translatory motion to each other are equally valid and equivalent \xa0to one another. In other words, they can all converge, and therefore all the laws are valid'}]
</code></pre>
<pre class="programlisting code"><code class="hljs-code">generator (<span class="hljs-string">"It takes a great deal of bravery to stand up to our enemies"</span>)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Setting 'pad_token_id' to 50256 (first 'eos_token_id') to generate sequence
[{'generated_text': 'It takes a great deal of bravery to stand up to our enemies that day. She still has a lot to learn from it, or it could take decades to do.\n\nWhile some braver men struggle, many are not as lucky'}]
</code></pre>
</li>
</ol>
<p class="normal">Pretty<a id="_idIndexMarker738"/> easy, isn’t it?</p>
<h3 class="heading-3" id="_idParaDest-195">Autoselecting a model and autotokenization</h3>
<p class="normal">Hugging Face does a great job of <a id="_idIndexMarker739"/>helping the developer to automate <a id="_idIndexMarker740"/>as many steps as possible. Let’s see some examples:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">You can easily import a pretrained model among the several dozen available. A complete list of available models is here: <a href="https://huggingface.co/docs/transformers/model_doc/auto"><span class="url">https://huggingface.co/docs/transformers/model_doc/auto</span></a>:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification
model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"distilbert-base-uncased"</span>)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Downloading: 100%|█████████████████████████████| 483/483 [00:00&lt;00:00, 68.9kB/s]
Downloading: 100%|███████████████████████████| 347M/347M [01:05&lt;00:00, 5.59MB/s]
…
</code></pre>
<p class="normal">You should probably train this model on a downstream task to use it for predictions and inference.</p>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="2">You can use <code class="inlineCode">AutoTokenizer</code> to transform words into tokens used by the models:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-uncased"</span>)
sequence = <span class="hljs-string">"The original theory of relativity is based upon the premise that all coordinate systems"</span>
<span class="hljs-built_in">print</span>(tokenizer(sequence))
</code></pre>
<pre class="programlisting con"><code class="hljs-con">{'input_ids': [101, 1996, 2434, 3399, 1997, 20805, 2003, 2241, 2588, 1996, 18458, 2008, 2035, 13530, 3001, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</code></pre>
</li>
</ol>
<h3 class="heading-3" id="_idParaDest-196">Named entity recognition</h3>
<p class="normal"><strong class="keyWord">Named Entity Recognition</strong> (<strong class="keyWord">NER</strong>) is a classical NLP <a id="_idIndexMarker741"/>task. According to Wikipedia, named entity recognition – also known as (named) entity identification, entity chunking, and entity extraction – is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, and percentages, among others.</p>
<p class="normal">Let’s see how easily this task can be <a id="_idIndexMarker742"/>performed with Hugging Face:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First of all, let’s create a NER pipeline:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
ner_pipe = pipeline(<span class="hljs-string">"ner"</span>)
sequence = <span class="hljs-string">"""Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much."""</span>
<span class="hljs-keyword">for</span> entity <span class="hljs-keyword">in</span> ner_pipe(sequence):
    <span class="hljs-built_in">print</span>(entity) 
</code></pre>
</li>
<li class="numberedList">You will be able to see something like the following, where the entities are recognized:
        <pre class="programlisting con"><code class="hljs-con">{'entity': 'I-PER', 'score': 0.99908304, 'index': 6, 'word': 'Du', 'start': 13, 'end': 15}
{'entity': 'I-PER', 'score': 0.9869529, 'index': 7, 'word': '##rs', 'start': 15, 'end': 17}
{'entity': 'I-PER', 'score': 0.9784202, 'index': 8, 'word': '##ley', 'start': 17, 'end': 20}
{'entity': 'I-ORG', 'score': 0.6860208, 'index': 14, 'word': 'P', 'start': 38, 'end': 39}
{'entity': 'I-ORG', 'score': 0.7713562, 'index': 15, 'word': '##rive', 'start': 39, 'end': 43}
{'entity': 'I-ORG', 'score': 0.76567733, 'index': 16, 'word': '##t', 'start': 43, 'end': 44}
{'entity': 'I-ORG', 'score': 0.8087192, 'index': 17, 'word': 'Drive', 'start': 45, 'end': 50}
</code></pre>
</li>
</ol>
<p class="normal">Named entity recognition can<a id="_idIndexMarker743"/> understand nine different classes:</p>
<ul>
<li class="bulletList"><code class="inlineCode">O</code>: Outside of a named entity.</li>
<li class="bulletList"><code class="inlineCode">B-MIS</code>: Beginning of a miscellaneous entity right after another miscellaneous entity.</li>
<li class="bulletList"><code class="inlineCode">I-MIS</code>: Miscellaneous entity.</li>
<li class="bulletList"><code class="inlineCode">B-PER</code>: Beginning of a person’s name right after another person’s name.</li>
<li class="bulletList"><code class="inlineCode">I-PER</code>: A person’s name.</li>
<li class="bulletList"><code class="inlineCode">B-ORG</code>: Beginning of an organization right after another organization.</li>
<li class="bulletList"><code class="inlineCode">I-ORG</code>: Organization.</li>
<li class="bulletList"><code class="inlineCode">B-LOC</code>: Beginning of a location right after another location.</li>
<li class="bulletList"><code class="inlineCode">I-LOC</code>: Location.</li>
</ul>
<p class="normal">These entities are defined in the CoNLL-2003 dataset typically used for this task and automatically selected by Hugging Face.</p>
<h3 class="heading-3" id="_idParaDest-197">Summarization</h3>
<p class="normal">Let’s now turn our attention to <a id="_idIndexMarker744"/>summarization, meaning the task of expressing the most important facts or ideas about something or someone in a short and clear form. Hugging Face makes it incredibly easy to use the T5 model as default for this task. Let’s see the code:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First of all, let’s create a summarization pipeline using the default T5 small model:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
summarizer = pipeline(<span class="hljs-string">"summarization"</span>)
ARTICLE = <span class="hljs-string">"""</span>
<span class="hljs-string"> Mr.</span>
<span class="hljs-string"> and Mrs.</span>
<span class="hljs-string"> Dursley, of number four, Privet Drive, were proud to say</span>
<span class="hljs-string"> that they were perfectly normal, thank you very much.</span>
<span class="hljs-string"> They were the last</span>
<span class="hljs-string"> people you'd expect to be involved in anything strange or mysterious,</span>
<span class="hljs-string">because they just didn't hold with such nonsense.</span>
<span class="hljs-string"> Mr.</span>
<span class="hljs-string"> Dursley was the director of a firm called Grunnings, which made</span>
<span class="hljs-string"> drills.</span>
<span class="hljs-string"> He was a big, beefy man with hardly any neck, although he did</span>
<span class="hljs-string"> have a very large mustache.</span>
<span class="hljs-string"> Mrs.</span>
<span class="hljs-string"> Dursley was thin and blonde and had</span>
<span class="hljs-string"> nearly twice the usual amount of neck, which came in very useful as she</span>
<span class="hljs-string"> spent so much of her time craning over garden fences, spying on the</span>
<span class="hljs-string"> neighbors.</span>
<span class="hljs-string"> The Dursleys had a small son called Dudley and in their</span>
<span class="hljs-string"> opinion there was no finer boy anywhere"""</span>
<span class="hljs-built_in">print</span>(summarizer(ARTICLE, max_length=<span class="hljs-number">130</span>, min_length=<span class="hljs-number">30</span>, do_sample=<span class="hljs-literal">False</span>))
</code></pre>
</li>
<li class="numberedList">As a result, we <a id="_idIndexMarker745"/>will see something similar to the following:
        <pre class="programlisting con"><code class="hljs-con">No model was supplied, defaulted to t5-small (https://huggingface.co/t5-small)
Downloading: 100%|██████████████████████████| 1.17k/1.17k [00:00&lt;00:00, 300kB/s]
Downloading: 100%|███████████████████████████| 231M/231M [01:29&lt;00:00, 2.71MB/s]
[{'summary_text': "Mr. and Mrs. Dursley, of number four, were the last people you'd expect to be involved in anything strange or mysterious . the Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere ."}]
</code></pre>
</li>
<li class="numberedList">Suppose that you <a id="_idIndexMarker746"/>want to change to a different model. That’s extremely simple as you only need to change one parameter:
        <pre class="programlisting code"><code class="hljs-code">summarizer = pipeline(<span class="hljs-string">"summarization"</span>, model=<span class="hljs-string">'t5-base'</span>)
</code></pre>
</li>
<li class="numberedList">As a result, we can see something like the following:
        <pre class="programlisting con"><code class="hljs-con">Downloading: 100%|████████████████████████████████████████████████████████████| 773k/773k [00:00&lt;00:00, 1.28MB/s]
Downloading: 100%|██████████████████████████████████████████████████████████| 1.32M/1.32M [00:00&lt;00:00, 1.93MB/s]
[{'summary_text': "bob greene says he and his wife were perfectly normal . he says they were the last people you'd expect to be involved in anything strange or mysterious . greene: they were a big, beefy man with hardly any neck, but had a very large mustache ."}]
</code></pre>
</li>
</ol>
<h3 class="heading-3" id="_idParaDest-198">Fine-tuning</h3>
<p class="normal">One common usage pattern for transformers is to use a pretrained LLM and then fine-tune the model for specific <a id="_idIndexMarker747"/>downstream tasks. Of course, the fine-tuning steps will take place on your own dataset, while pretraining is performed on very large datasets. The advantages of this two-step strategy are in terms of both saving computation costs and in reducing the carbon footprint. Plus, fine-tuning allows you to use state-of-the-art models without having to train one from scratch. Let’s see how to fine-tune a model with TF. This example is available at <a href="https://huggingface.co/docs/transformers/training"><span class="url">https://huggingface.co/docs/transformers/training</span></a>, where the pretrained model used is <code class="inlineCode">bert-base-cased</code>, which is fine-tuned on the “Yelp Reviews” dataset (available at <a href="https://huggingface.co/datasets/yelp_review_full"><span class="url">https://huggingface.co/datasets/yelp_review_full</span></a>). Let’s see the code from <a href="https://huggingface.co/docs/transformers/training"><span class="url">https://huggingface.co/docs/transformers/training</span></a></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, let’s load <a id="_idIndexMarker748"/>and tokenize the Yelp dataset:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
dataset = load_dataset(<span class="hljs-string">"yelp_review_full"</span>)
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)
<span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">"text"</span>], padding=<span class="hljs-string">"max_length"</span>, truncation=<span class="hljs-literal">True</span>)
tokenized_datasets = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)
small_train_dataset = tokenized_datasets[<span class="hljs-string">"train"</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
small_eval_dataset = tokenized_datasets[<span class="hljs-string">"test"</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
</code></pre>
</li>
<li class="numberedList">Then let’s convert them to TF format datasets:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator
data_collator = DefaultDataCollator(return_tensors=<span class="hljs-string">"tf"</span>)
<span class="hljs-comment"># convert the tokenized datasets to TensorFlow datasets</span>
tf_train_dataset = small_train_dataset.to_tf_dataset(
    columns=[<span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>],
    label_cols=[<span class="hljs-string">"labels"</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)
tf_validation_dataset = small_eval_dataset.to_tf_dataset(
    columns=[<span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>],
    label_cols=[<span class="hljs-string">"labels"</span>],
    shuffle=<span class="hljs-literal">False</span>,
    collate_fn=data_collator,
    batch_size=<span class="hljs-number">8</span>,
)
</code></pre>
</li>
<li class="numberedList">Now, we can use <code class="inlineCode">TFAutoModelForSequenceClassification</code>, specifically selecting <code class="inlineCode">bert-base-cased</code>:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification
model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>, num_labels=<span class="hljs-number">5</span>)
</code></pre>
</li>
<li class="numberedList">Finally, the fine-tuning is simply using the standard way to train a model used in Keras/TF 2.0 by compiling the model and then using <code class="inlineCode">fit</code> on it:
        <pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(
    optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">5e-5</span>),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),
    metrics=tf.metrics.SparseCategoricalAccuracy(),
)
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=<span class="hljs-number">3</span>)
</code></pre>
</li>
</ol>
<p class="normal">If you want, you can<a id="_idIndexMarker749"/> test the code on a public Colab notebook (available at <a href="https://huggingface.co/docs/transformers/training"><span class="url">https://huggingface.co/docs/transformers/training</span></a>). If you run the code yourself, you should be able to see something similar to <em class="italic">Figure 6.16</em>:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" height="156" src="../Images/B18331_06_16.png" width="880"/></figure>
<p class="packt_figref">Figure 6.16: Fine-tuning BERT on a Colab notebook</p>
<p class="normal">Next, we are going to<a id="_idIndexMarker750"/> introduce TFHub.</p>
<h2 class="heading-2" id="_idParaDest-199">TFHub</h2>
<p class="normal">In the previous section, we discussed <a id="_idIndexMarker751"/>how to use the Hugging Face Transformer library. Now, we will have <a id="_idIndexMarker752"/>a look at another library known as TFHub available at <a href="https://tfhub.dev/"><span class="url">https://tfhub.dev/</span></a>. TensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. The key idea is to reuse trained models like BERT and Faster R-CNN with just a few lines of code.</p>
<p class="normal">Using TFHub is as easy as writing a few lines of code. Let’s see a simple example where we load a pretrained <a id="_idIndexMarker753"/>model for computing embeddings. In this case, we use <code class="inlineCode">nnlm-en-dim128</code>, a token-based text embedding trained on the English Google News 200B corpus:</p>
<pre class="programlisting code"><code class="hljs-code">!pip install --upgrade tensorflow_hub
<span class="hljs-keyword">import</span> tensorflow_hub <span class="hljs-keyword">as</span> hub
model = hub.KerasLayer(<span class="hljs-string">"https://tfhub.dev/google/nnlm-en-dim128/2"</span>)
embeddings = model([<span class="hljs-string">"The rain in Spain."</span>, <span class="hljs-string">"falls"</span>,
                    <span class="hljs-string">"mainly"</span>, <span class="hljs-string">"In the plain!"</span>])
<span class="hljs-built_in">print</span>(embeddings.shape)  <span class="hljs-comment">#(4,128)</span>
</code></pre>
<p class="normal">Now let’s see how to use BERT. This code is adapted from <a href="https://www.tensorflow.org/hub/tutorials/bert_experts"><span class="url">https://www.tensorflow.org/hub/tutorials/bert_experts</span></a>, and it is also available on Hugging Face (<a href="https://huggingface.co/docs/transformers/training"><span class="url">https://huggingface.co/docs/transformers/training</span></a>):</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Let’s set up the environment and import useful modules:
        <pre class="programlisting code"><code class="hljs-code">!pip install seaborn
!pip install sklearn
!pip install tensorflow_hub
!pip install tensorflow_text
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> pairwise
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow_hub <span class="hljs-keyword">as</span> hub
<span class="hljs-keyword">import</span> tensorflow_text <span class="hljs-keyword">as</span> text  <span class="hljs-comment"># Imports TF ops for preprocessing.</span>
</code></pre>
</li>
<li class="numberedList">Let’s define<a id="_idIndexMarker754"/> a few sentences used for comparing their similarities:
        <pre class="programlisting code"><code class="hljs-code">sentences = [
    <span class="hljs-string">"</span><span class="hljs-string">Do not pity the dead, Harry. Pity the living, and, above all those who live without love."</span>,
    <span class="hljs-string">"It is impossible to manufacture or imitate love"</span>,
    <span class="hljs-string">"Differences of habit and language are nothing at all if our aims are identical and our hearts are open."</span>,
    <span class="hljs-string">"What do I care how he looks? I am good-looking enough for both of us, I theenk! All these scars show is zat my husband is brave!"</span>,
    <span class="hljs-string">"Love as powerful as your mother's for you leaves it's own mark. To have been loved so deeply, even though the person who loved us is gone, will give us some protection forever."</span>,
    <span class="hljs-string">"Family…Whatever yeh say, blood's important. . . ."</span>,
    <span class="hljs-string">"I cared more for your happiness than your knowing the truth, more for your peace of mind than my plan, more for your life than the lives that might be lost if the plan failed."</span>
]
</code></pre>
</li>
<li class="numberedList">Then, let’s use a pretrained BERT model available on TFHub to compute embeddings on the input sentences just defined. BERT’s output is the set of embeddings itself:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#@title Configure the model { run: "auto" }</span>
BERT_MODEL = <span class="hljs-string">"https://tfhub.dev/google/experts/bert/wiki_books/2"</span> <span class="hljs-comment"># @param {type: "string"} ["https://tfhub.dev/google/experts/bert/wiki_books/2", "https://tfhub.dev/google/experts/bert/wiki_books/mnli/2", "https://tfhub.dev/google/experts/bert/wiki_books/qnli/2", "https://tfhub.dev/google/experts/bert/wiki_books/qqp/2", "https://tfhub.dev/google/experts/bert/wiki_books/squad2/2", "https://tfhub.dev/google/experts/bert/wiki_books/sst2/2",  "https://tfhub.dev/google/experts/bert/pubmed/2", "https://tfhub.dev/google/experts/bert/pubmed/squad2/2"]</span>
<span class="hljs-comment"># Preprocessing must match the model, but all the above use the same.</span>
PREPROCESS_MODEL = <span class="hljs-string">"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"</span>
preprocess = hub.load(PREPROCESS_MODEL)
bert = hub.load(BERT_MODEL)
inputs = preprocess(sentences)
outputs = bert(inputs)
</code></pre>
</li>
<li class="numberedList">Now let’s define <a id="_idIndexMarker755"/>some auxiliary functions to show the similarity among embeddings based on <code class="inlineCode">pairwise.cosine_similarity</code>:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_similarity</span>(<span class="hljs-params">features, labels</span>):
  <span class="hljs-string">"""Plot a similarity matrix of the embeddings."""</span>
  cos_sim = pairwise.cosine_similarity(features)
  sns.<span class="hljs-built_in">set</span>(font_scale=<span class="hljs-number">1.2</span>)
  cbar_kws=<span class="hljs-built_in">dict</span>(use_gridspec=<span class="hljs-literal">False</span>, location=<span class="hljs-string">"left"</span>)
  g = sns.heatmap(
      cos_sim, xticklabels=labels, yticklabels=labels,
      vmin=<span class="hljs-number">0</span>, vmax=<span class="hljs-number">1</span>, cmap=<span class="hljs-string">"Blues"</span>, cbar_kws=cbar_kws)
  g.tick_params(labelright=<span class="hljs-literal">True</span>, labelleft=<span class="hljs-literal">False</span>)
  g.set_yticklabels(labels, rotation=<span class="hljs-number">0</span>)
  g.set_title(<span class="hljs-string">"Semantic Textual Similarity"</span>)
plot_similarity(outputs[<span class="hljs-string">"pooled_output"</span>], sentences)
</code></pre>
</li>
</ol>
<p class="normal">The interested reader can access the Colab notebook online on the Hugging Face website (available at <a href="https://huggingface.co/docs/transformers/training"><span class="url">https://huggingface.co/docs/transformers/training</span></a>) and visualize a heatmap showing similarities among sentences. Overall, using LLMs with TFHub is pretty easy, isn’t it?</p>
<h1 class="heading-1" id="_idParaDest-200">Evaluation</h1>
<p class="normal">Evaluating transformers involves<a id="_idIndexMarker756"/> considering multiple classes of metrics and understanding the cost tradeoffs among these classes. Let’s see the main ones.</p>
<h2 class="heading-2" id="_idParaDest-201">Quality</h2>
<p class="normal">The quality of<a id="_idIndexMarker757"/> transformers can be measured against a number of generally available datasets. Let’s see the most commonly used ones.</p>
<h3 class="heading-3" id="_idParaDest-202">GLUE</h3>
<p class="normal">The <strong class="keyWord">General Language Understanding Evaluation</strong> (<strong class="keyWord">GLUE</strong>) benchmark is a collection of resources for training, evaluating, and analyzing <a id="_idIndexMarker758"/>natural language<a id="_idIndexMarker759"/> understanding systems. GLUE is available at <a href="https://gluebenchmark.com/"><span class="url">https://gluebenchmark.com/</span></a>.</p>
<p class="normal">GLUE consists of:</p>
<ul>
<li class="bulletList">A benchmark of<a id="_idIndexMarker760"/> nine sentence or sentence-pair language understanding tasks built on established existing datasets and selected to cover a diverse range of dataset sizes, text genres, and degrees of difficulty</li>
<li class="bulletList">A diagnostic dataset designed to evaluate and analyze model performance with respect to a wide range of linguistic phenomena found in natural language</li>
<li class="bulletList">A public leaderboard for tracking performance on the benchmark and a dashboard for visualizing the performance of models on the diagnostic set</li>
</ul>
<p class="normal"><em class="italic">Figure 6.17</em> shows the GLUE dashboard from March 2022:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application, email  Description automatically generated" height="577" src="../Images/B18331_06_17.png" width="877"/></figure>
<p class="packt_figref">Figure 6.17: GLUE dashboard</p>
<h3 class="heading-3" id="_idParaDest-203">SuperGLUE</h3>
<p class="normal">In recent years, new models and <a id="_idIndexMarker761"/>methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently come close to the level of non-expert humans, suggesting limited headroom for further research. </p>
<p class="normal">SuperGLUE is a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, improved resources, and a new public leaderboard. <em class="italic">Figure 6.18</em> is the SuperGLUE leaderboard from March 2022:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" height="575" src="../Images/B18331_06_18.png" width="865"/></figure>
<p class="packt_figref">Figure 6.18: SuperGLUE leaderboard</p>
<h3 class="heading-3" id="_idParaDest-204">SQuAD</h3>
<p class="normal">SQuAD is a dataset used to<a id="_idIndexMarker762"/> evaluate questions and answers, <a href="https://rajpurkar.github.io/SQuAD-explorer/"><span class="url">https://rajpurkar.github.io/SQuAD-explorer/</span></a>. Specifically, the <strong class="keyWord">Stanford Question Answering Dataset</strong> (<strong class="keyWord">SQuAD</strong>) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, otherwise the question might be unanswerable.</p>
<p class="normal">SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible but also determine when no answer is supported by the paragraph and abstain from answering.</p>
<h3 class="heading-3" id="_idParaDest-205">RACE</h3>
<p class="normal">The <strong class="keyWord">ReAding Comprehension dataset from Examinations</strong> (<strong class="keyWord">RACE</strong>) dataset is a machine reading comprehension dataset <a id="_idIndexMarker763"/>consisting of 27,933 passages and 97,867 questions from English exams, targeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has 69,574. Each question is associated with four candidate answers, one of which is correct. The data generation process of RACE differs from most machine reading comprehension datasets. Instead of generating questions and answers by heuristics or crowdsourcing, questions in RACE are specifically designed for testing human reading skills and are created by domain experts. RACE is available at <a href="https://www.cs.cmu.edu/~glai1/data/race/"><span class="url">https://www.cs.cmu.edu/~glai1/data/race/</span></a>. <em class="italic">Figure 6.19</em> shows the RACE leaderboard:</p>
<figure class="mediaobject"><img alt="Table  Description automatically generated" height="334" src="../Images/B18331_06_19.png" width="882"/></figure>
<p class="packt_figref">Figure 6.19: RACE leaderboard</p>
<h3 class="heading-3" id="_idParaDest-206">NLP-progress</h3>
<p class="normal">NLP-progress is a repository, made to track progress in NLP, including the datasets and the current state-of-the-art <a id="_idIndexMarker764"/>models for the most common NLP tasks. The site aims to track the progress in NLP and gives an overview of the state-of-the-art models across the most common NLP tasks and their corresponding datasets. NLP-progress aims to cover both traditional and core NLP tasks such as dependency parsing and part-of-speech tagging as well as more recent ones such as reading comprehension and <a id="_idIndexMarker765"/>natural language inference. If you need a good starting point to find quality metrics for your task, then <a href="http://nlpprogress.com/"><span class="url">http://nlpprogress.com/</span></a> is the place to start with.</p>
<h2 class="heading-2" id="_idParaDest-207">Size</h2>
<p class="normal">The previous <a id="_idIndexMarker766"/>section provided an overview of quality metrics. This section focuses on the number of parameters used in various transformer architectures. As shown in <em class="italic">Figure 6.20</em>, there has been a race to increase transformers’ size during the last few years. Back in 2018, BERT’s size was about 340 million parameters, then in 2021, T5 reached 11 billion, and Megatron passed 500 billion. The very recent Switch Transformer has more than one trillion parameters and there is an expectation that soon we will see the first model with 100 trillion parameters. Indeed, there is evidence that the larger the model is the merrier, which can memorize information and generalize. However, training such large models requires massive computational resources:</p>
<figure class="mediaobject"><img alt="Table  Description automatically generated" height="420" src="../Images/B18331_06_20.png" width="697"/></figure>
<p class="packt_figref">Figure 6.20: Transformers’ size in billions of parameters</p>
<p class="normal">Trillion parameter transformers are on their way!</p>
<p class="normal">In fact, the paper <a href="https://arxiv.org/pdf/1906.02243.pdf"><span class="url">https://arxiv.org/pdf/1906.02243.pdf</span></a> warns about the sustainability impact of training large models (see <em class="italic">Figure 6.21</em>) in terms of both cloud computing cost and CO2 emissions:</p>
<figure class="mediaobject"><img alt="Table  Description automatically generated" height="228" src="../Images/B18331_06_21.png" width="825"/></figure>
<p class="packt_figref">Figure 6.21: Estimated cost of training a model in terms of CO2 emissions (lbs) and cloud computing cost (USD) - source: https://arxiv.org/pdf/1906.02243.pdf</p>
<p class="normal">So, size is not the only factor that enables the quality of transformers to improve, as larger sizes can <a id="_idIndexMarker767"/>in reality give only marginal gains and require significant computational resources for training.</p>
<h3 class="heading-3" id="_idParaDest-208">Larger doesn’t always mean better</h3>
<p class="normal">At the beginning of 2022, a new trend is emerging consisting of a hybrid approach where large models are used together with a more traditional retrieval mechanism. We discussed this approach earlier in the chapter when we discussed RETRO. The RETRO language model implements a learning scheme based on the use of external memory. DeepMind claimed that RETRO (or “Retrieval Enhanced Transformer”) performs like a neural network 25 times its size. GPT-3 has 175 billion parameters and RETRO uses just seven billion of them. Of course, this requires less time, energy, and computing power to train.</p>
<h2 class="heading-2" id="_idParaDest-209">Cost of serving</h2>
<p class="normal">The cost of serving a model depends <a id="_idIndexMarker768"/>on many factors and it’s difficult to estimate it without making reasonable assumptions. Of course, serving is a function of the number of parameters in the model. In addition, the number of queries submitted to the model for inference is another factor. Then, it’s important to consider whether or not a cloud provider manages the model or is served in your on-prem infrastructure. In this <a id="_idIndexMarker769"/>context, it might be useful to remember that MLOps (see <a href="https://en.wikipedia.org/wiki/MLOps"><span class="url">https://en.wikipedia.org/wiki/MLOps</span></a>) is the process of developing a machine learning model and deploying it as a production system. Of course, MLOps’ best practices might be adopted to optimize the costs of serving.</p>
<p class="normal">In this section, we have <a id="_idIndexMarker770"/>seen some key factors used to evaluate transformers, namely quality, size, and cost of serving. The list is clearly not inclusive, and a proper evaluation will take into account what the optimal tradeoff is among these factors. In the next section, we will discuss optimization.</p>
<h1 class="heading-1" id="_idParaDest-210">Optimization</h1>
<p class="normal">Optimizing a<a id="_idIndexMarker771"/> transformer involves building lightweight, responsive, and <a id="_idIndexMarker772"/>energy-efficient models. Let’s see the most common ideas adopted to optimize a model.</p>
<h2 class="heading-2" id="_idParaDest-211">Quantization</h2>
<p class="normal">The key idea behind quantization<a id="_idIndexMarker773"/> is to approximate the weights of a network with a smaller precision. The idea is very simple, but it works quite well in practice. If you are interested in knowing more, we recommend the paper <em class="italic">A Survey of Quantization Methods for Efficient Neural Network Inference</em>, by Amir Gholami et al., <a href="https://arxiv.org/pdf/2103.13630.pdf"><span class="url">https://arxiv.org/pdf/2103.13630.pdf</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-212">Weight pruning</h2>
<p class="normal">The key idea behind weight pruning<a id="_idIndexMarker774"/> is to remove some connections in the network. Magnitude-based weight pruning tends to zero out of model weights during training to increase model sparsity. This simple technique has benefits both in terms of model size and in cost of serving, as magnitude-based weight pruning gradually zeroes out of model weights during the training process to achieve model sparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency improvements. </p>
<p class="normal">One more time, weight pruning is about tradeoffs as it might generate some quality losses although normally, they are rather small. If you <a id="_idIndexMarker775"/>are interested to know more, please have a look at the TensorFlow guide about pruning: <a href="https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide"><span class="url">https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-213">Distillation</h2>
<p class="normal">The key idea behind knowledge distillation is to have a small model trained to reproduce the behavior of a larger model. This <a id="_idIndexMarker776"/>compression technique is sometimes referred to as teacher-student learning. The seminal paper you should check is <em class="italic">Distilling the Knowledge in a Neural Network</em> by Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, <a href="https://arxiv.org/abs/1503.02531"><span class="url">https://arxiv.org/abs/1503.02531</span></a>. </p>
<p class="normal">During the last few years, we have seen a number of distilled transformers. For instance, DistilBERT is a small, fast, cheap, and light transformer model based on the BERT architecture. Knowledge distillation is performed during the pretraining phase to reduce the size of a BERT model by 40%. Hugging Face has some ready-to-use Python scripts for distilling seq2seq T5 models available online at <a href="https://github.com/huggingface/transformers/tree/master/examples/research_projects/seq2seq-distillation"><span class="url">https://github.com/huggingface/transformers/tree/master/examples/research_projects/seq2seq-distillation</span></a>. Using the script is quite intuitive:</p>
<pre class="programlisting con"><code class="hljs-con">python distillation.py --teacher t5-small --data_dir cnn_dm \
--student_decoder_layers 3 --student_encoder_layers 6 --tokenizer_name t5-small \
--learning_rate=3e-4 --freeze_encoder --no_teacher --freeze_embeds \
--do_train --train_batch_size 32 \
--do_predict \
--model_name_or_path t5-small --eval_beams 2 --eval_max_gen_length 142 \
--val_check_interval 0.25 --n_val 1000 \
--output_dir distilt5 --gpus 1 --logger_name wandb
</code></pre>
<p class="normal">In this section, we have discussed a few techniques used to optimize transformers, namely quantization, weight pruning, and distillation. In the next section, we will discuss common pitfalls for transformers.</p>
<h1 class="heading-1" id="_idParaDest-214">Common pitfalls: dos and don’ts</h1>
<p class="normal">In this section, we will give five dos and a few don’ts that are typically recommended when dealing with transformers.</p>
<h2 class="heading-2" id="_idParaDest-215">Dos</h2>
<p class="normal">Let’s start with recommended <a id="_idIndexMarker777"/>best practices:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Do use pretrained large models.</strong> Today, it is almost always convenient to start from an already available pretrained model such as T5, instead of training your transformer from scratch. If you use a pretrained model, you for sure stand on the giant’s shoulders; think about it!</li>
<li class="bulletList"><strong class="keyWord">Do start with few-shot learning.</strong> When you start working with transformers, it’s always a good idea to start with a pretrained model and then perform a lightweight few-shot learning step. Generally, this would improve the quality of results without high computational costs.</li>
<li class="bulletList"><strong class="keyWord">Do use fine-tuning on your domain data and on your customer data.</strong> After playing with pretraining models and few-shot learning, you might consider doing a proper fine-tuning on your own proprietary data or on publicly available data for your domain of interest</li>
<li class="bulletList"><strong class="keyWord">Get yourself familiar with transformers’ libraries.</strong> Hugging Face or TFHub provide already<a id="_idIndexMarker778"/> available state-of-the-art implementations of almost all the known transformers. It might be useful to start from there unless you either have some very peculiar needs or are doing some innovative research work.</li>
<li class="bulletList"><strong class="keyWord">Get yourself familiar with the most commonly used evaluation metrics.</strong> When you use transformers, it is ideal to take into account the tradeoff faced in terms of quality, size, cost of serving, and many other factors.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-216">Don’ts</h2>
<p class="normal">Now let’s have a look at<a id="_idIndexMarker779"/> some of the pitfalls that you should avoid!</p>
<ul>
<li class="bulletList"><strong class="keyWord">Do not use very large models as a starting point.</strong> Large models come with a cost in terms of training and serving. You will need significant resources to fine-tune, and you might pay high costs for serving each query. It might be better to start with smaller models and understand whether they are useful for your quality needs.</li>
<li class="bulletList"><strong class="keyWord">Do not use unoptimized models.</strong> Nowadays, quantization, pruning, and distillation are standard techniques that need to be used by any transformer system that is put into production.</li>
</ul>
<p class="normal">In this section, we have seen some of the best practices for transformers. In the next section, we will talk about future solutions for these architectures.</p>
<h1 class="heading-1" id="_idParaDest-217">The future of transformers</h1>
<p class="normal">Transformers<a id="_idIndexMarker780"/> found their initial applications in NLP tasks, while CNNs are typically used for image processing systems. Recently, transformers have started to be successfully used for vision processing tasks. Vision transformers compute relationships among pixels in various small sections of an image (for example, 16 x 16 pixels). This approach has been proposed in the seminar paper <em class="italic">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em> by Alexey Dosovitskiy et al., <a href="https://arxiv.org/abs/2010.11929"><span class="url">https://arxiv.org/abs/2010.11929</span></a>, to make the attention computation feasible.</p>
<p class="normal"><strong class="keyWord">Vision transformers</strong> (<strong class="keyWord">ViTs</strong>) are today <a id="_idIndexMarker781"/>used for complex applications such as autonomous driving. Tesla’s engineers showed that their Tesla Autopilot uses a transformer on the multi-camera system in cars. Of course, ViTs are also used for more traditional computer vision tasks, including but not limited to image classification, object detection, video deepfake detection, image segmentation, anomaly detection, image synthesis, and cluster analysis. The results are frequently better than CNNs.</p>
<p class="normal">Another direction to consider is few-shot learning. Few-shot learning refers to the practice of feeding a machine learning model with a very small amount of training data to guide its predictions, like a few examples at inference time, as opposed to standard fine-tuning techniques that require a relatively large amount of training data for the pretrained model to adapt to the desired task with accuracy.</p>
<p class="normal">So, a model trained for a specific task can be reused for completely new tasks with very marginal costs. For instance, suppose we train a text model to generate text. Then, we want to perform new tasks such as translation or summarization. What we do is give a few examples of translations (say with pairs of text manually translated), or a few examples of summarization (again a few pairs). That’s it, no need for retraining or fine-tuning training.</p>
<p class="normal">Since FSL is proven to work well in a number of increasing domains, don’t be surprised that the training phase will be less and less relevant for future AI. More information can be found in this paper, <em class="italic">Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code</em> by Patrick Bareiß, Beatriz Souza, Marcelo d’Amorim, and Michael Pradel. The authors propose to use FSL to generate programming code with CodeGen, an open source mode for program synthesis (see <a href="https://github.com/salesforce/CodeGen"><span class="url">https://github.com/salesforce/CodeGen</span></a>).</p>
<h1 class="heading-1" id="_idParaDest-218">Summary</h1>
<p class="normal">In this chapter, we discussed transformers, a deep learning architecture that has revolutionized the traditional natural language processing field. We started reviewing the key intuitions behind the architecture, and various categories of transformers together with a deep dive into the most popular models. Then, we focused on implementations both based on vanilla architecture and on popular libraries such as Hugging Face and TFHub. After that, we briefly discussed evaluation, optimization, and some of the best practices commonly adopted when using transformers. The last section was devoted to reviewing how transformers can be used to perform computer vision tasks, a totally different domain from NLP. That requires a careful definition of the attention mechanism. In the end, attention is all you need! And at the core of attention is nothing more than the cosine similarity between vectors.</p>
<p class="normal">The next chapter is devoted to unsupervised learning.</p>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>