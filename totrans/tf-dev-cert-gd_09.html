<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer139">
<h1 class="chapter-number" id="_idParaDest-162"><a id="_idTextAnchor210"/>9</h1>
<h1 id="_idParaDest-163"><a id="_idTextAnchor211"/>Transfer Learning</h1>
<p>One <a id="_idIndexMarker470"/>of the most significant developments of the last decade in the <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) space was the concept <a id="_idIndexMarker471"/>of <strong class="bold">transfer learning</strong>, and rightfully so. Transfer learning is the process of applying knowledge gained from solving a source task to a target task, which is a different but related task. This approach has proven not only effective in saving computational resources required to train a deep neural network but also in cases where the target dataset is limited in size. Transfer learning reuses learned features from a pre-trained model, enabling us to build better-performing models and attain convergence much faster. Because of its numerous benefits, transfer learning has become an area of extensive research, with several studies exploring the application of transfer learning across different domains, such as image classification, object detection, natural language processing, and <span class="No-Break">speech recognition.</span></p>
<p>In this chapter, we will introduce the concept of transfer learning, examining how it works, and some best practices around the application of transfer learning in various use cases. We will apply the concept of transfer learning in a real-world application with the aid of well-known pre-trained models. We will see in action how to apply these pre-trained models as a feature extractor and also learn how to fine-tune them to achieve optimal results. By the end of this chapter, you will have a solid understanding of what transfer learning is and how to apply it effectively to build real-world <span class="No-Break">image classifiers.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>An introduction to <span class="No-Break">transfer learning</span></li>
<li>Types of <span class="No-Break">transfer learning</span></li>
<li>Building a real-world image classifier with <span class="No-Break">transfer learning</span></li>
</ul>
<h1 id="_idParaDest-164"><a id="_idTextAnchor212"/>Technical requirements</h1>
<p>We will use Google Colab to run the coding exercise, which requires <strong class="source-inline">python &gt;= 3.8.0</strong>, along with the following packages, which can be installed using the <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install</strong></span><span class="No-Break"> command:</span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline">tensorflow&gt;=2.7.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">os</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">matplotlib &gt;=3.4.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">pathlib</strong></span></li>
</ul>
<p>The code bundle for this book is available at the following GitHub link: <a href="https://github.com/PacktPublishing/TensorFlow-Developer-Certificate">https://github.com/PacktPublishing/TensorFlow-Developer-Certificate</a>. Also, solutions to all exercises can be found in the GitHub <span class="No-Break">repo itself.</span></p>
<h1 id="_idParaDest-165">Introduction to transfer learning<a id="_idTextAnchor213"/><a id="_idTextAnchor214"/></h1>
<p>As humans, it is<a id="_idIndexMarker472"/> easy for us to transfer knowledge gained from one task or activity to another. For instance, if you have a good grasp of Python (the programming language, not the snake) and you decide to learn Rust, because of your background knowledge in Python, you will find it easier to learn Rust compared to someone who has never written a basic program in any programming language. This is because certain concepts, such as object-oriented programming, have similarities across different programming languages. Transfer learning follows the <span class="No-Break">same principle.</span></p>
<p>Transfer learning is a technique in which we leverage a model pre-trained on <em class="italic">task A</em> to solve a different but related <em class="italic">task B</em>. For example, we use a neural network trained on one task and transfer the knowledge gained to multiple related tasks. In image classification, we often use deep learning models that have been trained on very large datasets, such as ImageNet, which is made up of more than 1,000,000 images across 1,000 categories. The knowledge gained by these pre-trained models can be applied to many different tasks, such as classifying different breeds of dogs in a photograph. Just like how we can learn Rust quicker because of our knowledge of Python, the same applies here – pre-trained models can leverage information gained from a source task and apply it to the target task, reducing both the training time and the need for a large amount of annotated data, which may not be available or difficult to collect for the <span class="No-Break">target task.</span></p>
<p>Transfer learning is <a id="_idIndexMarker473"/>not limited to image classification tasks; it can also be applied to other deep learning tasks, such as natural language processing, speech recognition, and object detection. In <a href="B18118_11.xhtml#_idTextAnchor267"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">NLP with TensorFlow,</em> we will apply transfer learning to text classification. There, we will see how pretrained models (which we will access from TensorFlow Hub), trained on large text corpora, can be fine-tuned for <span class="No-Break">text classification.</span></p>
<p>In classic ML, as illustrated in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1(a)</em>, we train the model from scratch for each task, as we have done so far in this book. This approach is resource- <span class="No-Break">and data-intensive.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer132">
<img alt="Figure 9.1 – Traditional ML versus transfer learning" height="616" src="image/B18118_09_01.jpg" width="1207"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Traditional ML versus transfer learning</p>
<p>However, researchers discovered it was possible to learn visual features so that a model learns low-level features from a massive dataset, such as ImageNet, and applies this to a new, related task, as illustrated in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1(b)</em> – for example, in the classification of our weather dataset, which we used in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">,  Handling Overfitting</em>. By applying transfer learning, we can take advantage of the knowledge gained by a model during its training on a large dataset and adapt it to solve different but related tasks effectively. This approach proved useful, as it not only saves training time and resources but has also learned to improve performance, even in scenarios where a limited amount of data is available for the <span class="No-Break">target task.</span></p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor215"/>Types of transfer learning</h1>
<p>There are two <a id="_idIndexMarker474"/>main ways we can apply transfer learning in CNNs. First, we can use the pre-trained model as a feature extractor. Here, we freeze the weights of the convolutional layers to preserve the knowledge gained from the source task and add a new classifier, which is trained for classification of the second task. This works because the convolutional layers are reusable, since they only learned the low-level features such as edges, corners, and textures, which are generic and applicable in different images, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>, while the fully connected layers are added to learn high-level details, which<a id="_idIndexMarker475"/> are used to classify different objects in <span class="No-Break">a photograph.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<img alt="Figure 9.2 – Transfer learning as a feature extractor" height="282" src="image/B18118_09_02.jpg" width="611"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Transfer learning as a feature extractor</p>
<p>The second method of applying transfer learning is to unfreeze some layers of the pre-trained model and add a classifier model to identify the high-level features, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.3</em>. Here, we train both the unfrozen layers and the new classifier together. The pre-trained model is applied as the starting point of the new task, and the weight of the unfrozen layers is fine-tuned along with the <a id="_idIndexMarker476"/>classification layer to adapt the model to the <span class="No-Break">new task.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<img alt="Figure 9.3 – Transfer learning as a fine-tuned model" height="290" src="image/B18118_09_03.jpg" width="482"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Transfer learning as a fine-tuned mo<a id="_idTextAnchor216"/>del</p>
<p>Pre-trained models are deep networks that have been trained on large datasets. By leveraging the knowledge and weights these models have already acquired, we can use them as a feature extractor or fine-tune them for our use case, with a smaller dataset and less training time. Transfer learning provides ML practitioners with access to state-of-the-art models, which can be quickly and easily accessed in TensorFlow using an API. This means we don’t always have to train our model from scratch, saving time and computational resources, as fine-tuning a model is faster than training it from <span class="No-Break">the beginning.</span></p>
<p>We can apply pre-trained models to relevant use cases, potentially leading to higher accuracy and faster convergence. However, if the source and target domains are unrelated, transfer learning may not only fail but also harm the performance of the target task, due to irrelevant learned features, a situation known as negative transfer. Let's apply transfer learning to a real-world image classification task. We will explore some of the top-performing pretrained models, such as VGG, Inception, MobileNetV2, and EfficientNet. These models have been pretrained for image classification tasks. Let’s see how they will fare on the <span class="No-Break">given task.</span></p>
<h1 id="_idParaDest-167"><a id="_idTextAnchor217"/>Building a real-world image classifier with Transfer learning</h1>
<p>In this case study, your <a id="_idIndexMarker477"/>company secured a medical project, and you are assigned the responsibility to build a pneumonia classifier for GETWELLAI. You have been provided with over 5,000 X-ray JPEG images, made up of two categories (pneumonia and normal). The dataset was annotated by expert physicians and low-quality images have been removed. Let's see how we can tackle this problem using the two types of transfer learning techniques we have discussed <span class="No-Break">so far.</span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor218"/>Loading the data</h2>
<p>Perform the<a id="_idIndexMarker478"/> following steps to load <span class="No-Break">the data:</span></p>
<ol>
<li>As usual, we start by loading the necessary libraries that we will need for <span class="No-Break">our project:</span><pre class="source-code">
#Import necessary libraries</pre><pre class="source-code">
import os</pre><pre class="source-code">
import pathlib</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><pre class="source-code">
import matplotlib.image as mpimg</pre><pre class="source-code">
import random</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
from tensorflow import keras</pre><pre class="source-code">
from tensorflow.keras.preprocessing.image import ImageDataGenerator</pre><pre class="source-code">
from tensorflow.keras.callbacks import EarlyStopping</pre><pre class="source-code">
from tensorflow.keras import regularizer</pre></li>
<li>Next, let's load the X-ray dataset. To do this, we will use the <strong class="source-inline">wget</strong> command to download the file from the <span class="No-Break">specified URL:</span><pre class="source-code">
!wget https://storage.googleapis.com/x_ray_dataset/dataset.zip</pre></li>
<li>The <a id="_idIndexMarker479"/>downloaded file is saved in the current working directory of our Colab instance as a ZIP file, which contains a dataset of the <span class="No-Break">X-ray images.</span></li>
<li>Next, we will extract the contents of the <strong class="source-inline">zip</strong> folder by running the <span class="No-Break">following code:</span><pre class="source-code">
!unzip dataset.zip</pre></li>
</ol>
<p>When we run the code, we extract a <strong class="source-inline">dataset</strong> folder that holds <strong class="source-inline">test</strong>, <strong class="source-inline">val</strong>, and <strong class="source-inline">train</strong> sub-directories, with each sub-directory holding data for both normal and pneumonia X-ray images, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<img alt="Figure 9.4 – A snapshot of the current working directory that holds the extracted ZIP file" height="843" src="image/B18118_09_04.jpg" width="412"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – A snapshot of the current working directory that holds the extracted ZIP file</p>
<ol>
<li value="5">We will <a id="_idIndexMarker480"/>use the following code block to extract the sub-directories and the number of images in them. We also saw this code block in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, </em><span class="No-Break"><em class="italic">Handling Overfitting</em></span><span class="No-Break">:</span><pre class="source-code">
root_dir = "/content/dataset"</pre><pre class="source-code">
for dirpath, dirnames, filenames in os.walk(root_dir):</pre><pre class="source-code">
    print(f"Directory: {dirpath}")</pre><pre class="source-code">
    print(f"Number of images: {len(filenames)}")</pre><pre class="source-code">
    print()</pre></li>
</ol>
<p>It gives us a snapshot of the data in each folder and a sense of the <span class="No-Break">data distribution.</span></p>
<ol>
<li value="6">Next, we will use the <strong class="source-inline">view_random_images</strong> function to display some random images and their shapes from the <span class="No-Break"><strong class="source-inline">train</strong></span><span class="No-Break"> directory:</span><pre class="source-code">
view_random_images(</pre><pre class="source-code">
    target_dir="/content/dataset/train",num_images=4)</pre></li>
</ol>
<p>When we <a id="_idIndexMarker481"/>run the code, we will get a result similar to <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<img alt="Figure 9.5 – Random images displayed from the training samples of the X-ray dataset" height="358" src="image/B18118_09_05.jpg" width="1065"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Random images displayed from the training samples of the X-ray dataset</p>
<ol>
<li value="7">We will create an instance of the <strong class="source-inline">ImageDataGenerator</strong> class for our training and validation data. We will add the <strong class="source-inline">rescale</strong> parameter to rescale our images and ensure that all the pixel values are within the range of 0 to 1. We do this to improve stability and enhance convergence during the training process. The resulting <strong class="source-inline">train_datagen</strong> and <strong class="source-inline">valid_datagen</strong> objects are used to generate batches of training and validation <span class="No-Break">data, respectively:</span><pre class="source-code">
train_datagen = ImageDataGenerator(rescale=1./255)</pre><pre class="source-code">
valid_datagen = ImageDataGenerator(rescale=1./255)</pre></li>
<li>Next, we set up the <strong class="source-inline">train</strong>, <strong class="source-inline">validation</strong>, and <span class="No-Break"><strong class="source-inline">test</strong></span><span class="No-Break"> directories.</span><pre class="source-code">
# Set up the train and test directories</pre><pre class="source-code">
train_dir = "/content/dataset/train/"</pre><pre class="source-code">
val_dir = "/content/dataset/val"</pre><pre class="source-code">
test_dir = "/content/dataset/test"</pre></li>
<li>We used<a id="_idIndexMarker482"/> the <strong class="source-inline">flow_from_directory()</strong> method to load images from the training directory. The <strong class="source-inline">target_size</strong> argument is used to resize all the images to 224 x 224 pixels. One key difference between this code and the one we used in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, Handling Overfitting</em> is that the class mode argument is set to <strong class="source-inline">binary</strong> because we are dealing with a binary classification problem (i.e., normal <span class="No-Break">and pneumonia):</span><pre class="source-code">
train_data=train_datagen.flow_from_directory(</pre><pre class="source-code">
    train_dir,target_size=(224,224),</pre><pre class="source-code">
# convert all images to be 224 x 224</pre><pre class="source-code">
    class_mode="binary")</pre><pre class="source-code">
valid_data=valid_datagen.flow_from_directory(val_dir,</pre><pre class="source-code">
    target_size=(224,224),</pre><pre class="source-code">
    class_mode="binary",</pre><pre class="source-code">
    shuffle=False)</pre><pre class="source-code">
test_data=valid_datagen.flow_from_directory(test_dir,</pre><pre class="source-code">
    target_size=(224,224),</pre><pre class="source-code">
    class_mode="binary",</pre><pre class="source-code">
    shuffle=False)</pre></li>
</ol>
<p>The <strong class="source-inline">valid_data</strong> and <strong class="source-inline">test_data</strong> generators are quite similar to the <strong class="source-inline">train_data</strong> generator, as they both have their target size set to 224 x 224 as well; the key difference is they have set <strong class="source-inline">shuffle</strong> to <strong class="source-inline">false</strong>, which means the images will not be shuffled. If we set this to <strong class="source-inline">true</strong>, the images <span class="No-Break">get shuffled.</span></p>
<h2 id="_idParaDest-169">Mo<a id="_idTextAnchor219"/>deling</h2>
<p>We will start by<a id="_idIndexMarker483"/> using the same model we applied in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, Handling Overfitting</em>. To avoid redundancy, let’s focus on the fully connected layer, where we have one neuron in the output layer, as this is a binary classification task. We will compare the result with using <span class="No-Break">transfer learning:</span></p>
<pre class="source-code">
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1050, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])
# Compile the model
model_1.compile(loss="binary_crossentropy",
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"])
#Fit the model
# Add an early stopping callback
callbacks = [tf.keras.callbacks.EarlyStopping(
    monitor="val_accuracy", patience=3,
    restore_best_weights=True)]
history_1 = model_1.fit(train_data,epochs=20,
    validation_data=valid_data,
    callbacks=[callbacks]</pre>
<p>In this case, we have one neuron in the output layer, and we changed the activation function to a sigmoid function, since we are building a binary classifier. In the compile step, we also change the loss function to binary cross entropy; everything else remains the same. Then, we fit <span class="No-Break">the model.</span></p>
<p>The <a id="_idIndexMarker484"/>training ends on the 7th epoch, as the validation loss fails to <span class="No-Break">drop further:</span></p>
<pre class="source-code">
Epoch 4/20
163/163 [==============================] – 53s 324ms/step – loss: 0.0632 – accuracy: 0.9774 – val_loss: 0.0803 – val_accuracy: 1.0000
Epoch 5/20
163/163 [==============================] – 53s 324ms/step – loss: 0.0556 – accuracy: 0.9797 – val_loss: 0.0501 – val_accuracy: 1.0000
Epoch 6/20
163/163 [==============================] – 53s 323ms/step – loss: 0.0412 – accuracy: 0.9854 – val_loss: 0.1392 – val_accuracy: 0.8750
Epoch 7/20
163/163 [==============================] – 54s 334ms/step – loss: 0.0314 – accuracy: 0.9875 – val_loss: 0.2450 – val_accuracy: 0.8750</pre>
<p>On the fifth epoch, the model reached a validation accuracy of 100 percent, which looks promising. Let’s evaluate <span class="No-Break">the model:</span></p>
<pre class="source-code">
model_1.evaluate(test_data)</pre>
<p>When we <a id="_idIndexMarker485"/>evaluate the model on test data, we recorded only an accuracy of 0.7580. This points to signs of overfitting. Of course, we can try a combination of the ideas we learned in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, Handling Overfitting</em> to improve the model’s performance and you are encouraged to do so. However, let's learn how to use pre-trained models and see whether we can transfer the knowledge gained by these models to our use case and, if possible, get better results. Let’s do <span class="No-Break">that next.</span></p>
<h2 id="_idParaDest-170">Modeling with transfe<a id="_idTextAnchor220"/>r learning</h2>
<p>In this section, we will utilize <a id="_idIndexMarker486"/>three widely used pre-trained CNNs for image classification – VGG16, InceptionV3, and MobileNet. We will demonstrate the application of transfer learning as a feature extractor using these models, followed by adding a<a id="_idIndexMarker487"/> fully connected layer for label classification. We will also learn how to fine-tune a pre-trained model by unfreezing some of its layers. Before we can use these models, we need to import them. We can do this using a single line <span class="No-Break">of code:</span></p>
<pre class="source-code">
from tensorflow.keras.applications import InceptionV3,
    MobileNet, VGG16, ResNet50</pre>
<p>Now that we have our models and we are set to go, let’s begin <span class="No-Break">with VGG16.</span></p>
<h3>VGG16</h3>
<p>VGG16 is a <a id="_idIndexMarker488"/>CNN architecture developed by the Visual Geometry Group at the University of Oxford. It was trained on the ImageNet dataset. The VGG16 architecture secured second place in the image classification category in the ImageNet Challenge 2014 submission. VGG16 is made up of 13 (a 3 x 3 filter) convolutional layers, 5 (2x2) max-pooling layers, and 3 fully connected layers, as illustrated in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.6</em>. This gives us 16 layers with learnable parameters; recall that max-pooling layers are for dimensionality reduction and they have no weight. This one takes an input tensor of the 224 x 224 <span class="No-Break">RGB image.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="Figure 9.6 – The VGG16 model’s architecture (Source: https://medium.com/analytics-vidhya/car-brand-classification-using-vgg16-transfer-learning-f219a0f09765)" height="1085" src="image/B18118_09_06.jpg" width="1562"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – The VGG16 model’s architecture (Source: https://medium.com/analytics-vidhya/car-brand-classification-using-vgg16-transfer-learning-f219a0f09765)</p>
<p>Let's begin by loading <a id="_idIndexMarker489"/>VGG16 from Keras. We want to load the model and use pre-trained weights from the ImageNet dataset. To do this, we set the <strong class="source-inline">weights</strong> parameter to <strong class="source-inline">imagenet</strong>; we also set the <strong class="source-inline">include_top</strong> parameter to <strong class="source-inline">false</strong>. This is done because we want to use the model as a feature extractor. This way, we can add our own custom-made, fully connected layers for classification. We set the input size to (224,224,3) as this is the input image size that <span class="No-Break">VGG16 expects:</span></p>
<pre class="source-code">
# Instantiate the VGG16 model
vgg16 = VGG16(weights='imagenet', include_top=False,
    input_shape=(224, 224, 3))</pre>
<p>The next step enables us to freeze the weights of the model because we want to use VGG 16 as a feature extractor. When we freeze all the layers, this makes them untrainable, which means their weights will not be updated <span class="No-Break">during training:</span></p>
<pre class="source-code">
# Freeze all layers in the VGG16 model
for layer in vgg16.layers:
    layer.trainable = False</pre>
<p>The next code<a id="_idIndexMarker490"/> block creates a new sequential model that uses VGG as its top layer, after which we add a fully connected layer made up of a dense layer with 1,024 neurons, a dropout layer, and an output layer with one neuron, and then we set the activation to sigmoid for <span class="No-Break">binary classification:</span></p>
<pre class="source-code">
# Create a new model on top of VGG16
model_4 = tf.keras.models.Sequential()
model_4.add(vgg16)
model_4.add(tf.keras.layers.Flatten())
model_4.add(tf.keras.layers.Dense(1024, activation='relu'))
model_4.add(tf.keras.layers.Dropout(0.5))
model_4.add(tf.keras.layers.Dense(1, activation='sigmoid'))</pre>
<p>We compile and fit our model to <span class="No-Break">the data:</span></p>
<pre class="source-code">
# Compile the model
model_4.compile(optimizer='adam',
    loss='binary_crossentropy', metrics=['accuracy'])
# Fit the model
callbacks = [tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy', patience=3,
    restore_best_weights=True)]
history_4 = model_4.fit(train_data,
    epochs=20,
    validation_data=valid_data,
    callbacks=[callbacks]
    )</pre>
<p>In four<a id="_idIndexMarker491"/> epochs, our model stops training. It reaches a training accuracy of 0.9810 but on the validation set, we get an accuracy <span class="No-Break">of 0.875:</span></p>
<pre class="source-code">
Epoch 1/20
163/163 [==============================] - 63s 360ms/step - loss: 0.2737 - accuracy: 0.9375 - val_loss: 0.2021 - val_accuracy: 0.8750
Epoch 2/20
163/163 [==============================] - 57s 347ms/step - loss: 0.0818 - accuracy: 0.9699 - val_loss: 0.4443 - val_accuracy: 0.8750
Epoch 3/20
163/163 [==============================] - 56s 346ms/step - loss: 0.0595 - accuracy: 0.9774 - val_loss: 0.1896 - val_accuracy: 0.8750
Epoch 4/20
163/163 [==============================] - 58s 354ms/step - loss: 0.0556 - accuracy: 0.9810 - val_loss: 0.4209 - val_accuracy: 0.8750</pre>
<p>When we evaluate the model, we reach an accuracy of 84.29. Now, let's use another pre-trained model as a <span class="No-Break">feature extractor.</span></p>
<h3>MobileNet</h3>
<p>MobileNet is a <a id="_idIndexMarker492"/>lightweight CNN model developed by engineers at Google. The model is light and efficient, making it a choice model to develop mobile and embedded vision apps. Like VGG16, MobileNet was also trained on the ImageNet dataset, and it was able to achieve state-of-the-art results. MobileNet has a streamlined architecture that makes use of depth-wise separable convolutions. The underlining idea is to reduce the number of parameters required during training while <span class="No-Break">maintaining accuracy.</span></p>
<p>To apply<a id="_idIndexMarker493"/> MobileNet as a feature extractor, the steps are similar to what we just did with VGG16; hence, let’s look at the code block. We will load the model, freeze the layers, and add a fully connected layer <span class="No-Break">as before:</span></p>
<pre class="source-code">
# Instantiate the MobileNet model
mobilenet = MobileNet(weights='imagenet',
    include_top=False, input_shape=(224, 224, 3))
# Freeze all layers in the MobileNet model
for layer in mobilenet.layers:
    layer.trainable = False
# Create a new model on top of MobileNet
model_10 = tf.keras.models.Sequential()
model_10.add(mobilenet)
model_10.add(tf.keras.layers.Flatten())
model_10.add(tf.keras.layers.Dense(1024,activation='relu'))
model_10.add(tf.keras.layers.Dropout(0.5))
model_10.add(tf.keras.layers.Dense(1,activation='sigmoid'))</pre>
<p>Next, we compile and fit <span class="No-Break">the model:</span></p>
<pre class="source-code">
# Compile the model
model_10.compile(optimizer='adam',
    loss='binary_crossentropy', metrics=['accuracy'])
# Fit the model
callbacks = [tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy', patience=3,
    restore_best_weights=True)]
history_10 = model_10.fit(train_data,
    epochs=20,
    validation_data=valid_data,
    callbacks=[callbacks])</pre>
<p>In just four<a id="_idIndexMarker494"/> epochs, the model reaches a validation accuracy <span class="No-Break">of 87.50%:</span></p>
<pre class="source-code">
Epoch 1/20
163/163 [==============================] - 55s 321ms/step - loss: 3.1179 - accuracy: 0.9402 - val_loss: 1.8479 - val_accuracy: 0.8750
Epoch 2/20
163/163 [==============================] - 51s 313ms/step - loss: 0.3896 - accuracy: 0.9737 - val_loss: 1.1031 - val_accuracy: 0.8750
Epoch 3/20
163/163 [==============================] - 52s 320ms/step - loss: 0.0795 - accuracy: 0.9896 - val_loss: 0.8590 - val_accuracy: 0.8750
Epoch 4/20
163/163 [==============================] - 52s 318ms/step - loss: 0.0764 - accuracy: 0.9877 - val_loss: 1.1536 - val_accuracy: 0.8750</pre>
<p>Next, let's try fine-tuning a pre-trained <span class="No-Break">model hands-on.</span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor221"/>Transfer learning as a fine-tuned model</h2>
<p>InceptionV3<a id="_idIndexMarker495"/> is another <a id="_idIndexMarker496"/>CNN architecture developed by Google. It combines 1x1 and 3x3 filters to capture different aspects of an image. Let’s unfreeze some layers of this pre-trained model so that we can train both the layers we unfroze and the fully <span class="No-Break">connected layer.</span></p>
<p>First, we <a id="_idIndexMarker497"/>will load the InceptionV3 model. We set <strong class="source-inline">include_top=False</strong> to remove the classification layer of InceptionV3 and use weights from ImageNet. We unfreeze the last 50 layers by setting <strong class="source-inline">trainable</strong> to <strong class="source-inline">true</strong> for these layers. This enables us to train these layers on the <span class="No-Break">X-ray dataset:</span></p>
<pre class="source-code">
# Load the InceptionV3 model
inception = InceptionV3(weights='imagenet',
    include_top=False, input_shape=(224, 224, 3))
# Unfreeze the last 50 layers of the InceptionV3 model
for layer in inception.layers[-50:]:
    layer.trainable = True</pre>
<p class="callout-heading">Note:</p>
<p class="callout">Unfreezing and fine-tuning too many layers on a small dataset is a bad strategy, as this can lead <span class="No-Break">to overfitting.</span></p>
<p>We will create, fit, and compile the model, as we have done so far, and the new model reaches a validation accuracy of 100 percent on the <span class="No-Break">fifth epoch:</span></p>
<pre class="source-code">
Epoch 5/10
163/163 [==============================] - 120s 736ms/step - loss: 0.1168 - accuracy: 0.9584 - val_loss: 0.1150 - val_accuracy: 1.0000
Epoch 6/10
163/163 [==============================] - 117s 716ms/step - loss: 0.1098 - accuracy: 0.9624 - val_loss: 0.2713 - val_accuracy: 0.8125
Epoch 7/10
163/163 [==============================] - 123s 754ms/step - loss: 0.1011 - accuracy: 0.9613 - val_loss: 0.2765 - val_accuracy: 0.7500
Epoch 8/10
163/163 [==============================] - 120s 733ms/step - loss: 0.0913 - accuracy: 0.9668 - val_loss: 0.2711 - val_accuracy: 0.8125</pre>
<p>Next, let's <a id="_idIndexMarker498"/>evaluate the models using our <strong class="source-inline">evaluate_models</strong> <span class="No-Break">helper function:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="Figure 9.7 – An evaluation result from our experiments" height="422" src="image/B18118_09_07.jpg" width="279"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – An evaluation result from our experiments</p>
<p>From the <a id="_idIndexMarker499"/>results in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.7</em>, MobileNet, VGG16, and InceptionV3 came out on top. We can see that these models performed much better than our baseline model (<strong class="bold">model 1</strong>). We also report the results of a few other models from our notebook. We can spot signs of overfitting; hence, you can combine some of the ideas we discussed in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, Handling Overfitting</em> to improve <span class="No-Break">your result.</span></p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor222"/>Summary</h1>
<p>Transfer learning has gained traction in the deep learning community, due to its improved performance, speed, and accuracy in building deep learning models. We discussed the rationale behind transfer learning and explored transfer learning as a feature extractor and a fine-tuned model. We built a couple of solutions using the top-performing pre-trained models and saw how they outperformed our baseline model when applied to the <span class="No-Break">X-ray dataset.</span></p>
<p>By now, you should have gained a solid understanding of transfer learning and its applications. Equipped with this knowledge, you should be able to apply transfer learning as either a feature extractor or a fine-tuned model when building real-world deep learning solutions for a wide range <span class="No-Break">of tasks.</span></p>
<p>With this, we have come to the end of this chapter and this section of the book. In the next chapter, we will discuss <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), where we will build exciting NLP applications <span class="No-Break">using TensorFlow.</span></p>
<h1 id="_idParaDest-173"><a id="_idTextAnchor223"/>Questions</h1>
<p>Let’s test what we learned in <span class="No-Break">this chapter:</span></p>
<ol>
<li>Using the test notebook, load the cat and <span class="No-Break">dog dataset.</span></li>
<li>Preprocess the image data using the image <span class="No-Break">data generator.</span></li>
<li>Use a VGG16 model as a feature extractor and build a new <span class="No-Break">CNN model.</span></li>
<li>Unfreeze 40 layers of an InceptionV3 model and build a new <span class="No-Break">CNN model.</span></li>
<li>Evaluate both the VGG16 and <span class="No-Break">InceptionV3 models.</span></li>
</ol>
<h1 id="_idParaDest-174"><a id="_idTextAnchor224"/>Further reading</h1>
<p>To learn more, you can check out the <span class="No-Break">following resources:</span></p>
<ul>
<li>Kapoor, A., Gulli, A. and Pal, S. (2020) <em class="italic">Deep Learning with TensorFlow and Keras Third Edition: Build and deploy supervised, unsupervised, deep, and reinforcement learning models</em>. Packt <span class="No-Break">Publishing Ltd.</span></li>
<li><em class="italic">Adapting Deep Convolutional Neural Networks for Transfer Learning: A Comparative Study</em> by C. M. B. Al-Rfou, G. Alain, and Y. Bengio, published in arXiv <span class="No-Break">preprint arXiv:1511.</span></li>
<li><em class="italic">Very Deep Convolutional Networks for Large-Scale Image Recognition</em> by K. Simonyan and A. Zisserman, published in arXiv preprint arXiv:1409.1556 <span class="No-Break">in 2014.</span></li>
<li><em class="italic">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</em> by M. Tan and Q. Le, published in <em class="italic">International Conference on Machine Learning</em> <span class="No-Break">in 2019.</span></li>
<li><em class="italic">MobileNetV2: Inverted Residuals and Linear Bottlenecks</em> by M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen, published in arXiv preprint arXiv:1801.04381 <span class="No-Break">in 2018.</span></li>
<li><em class="italic">DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</em> by Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., &amp; Darrell, <span class="No-Break">T. (2014).</span></li>
<li><em class="italic">Harnessing the power of transfer learning for medical image classification</em> by Ryan Burke, <em class="italic">Towards Data </em><span class="No-Break"><em class="italic">Science</em></span><span class="No-Break">. </span><a href="https://towardsdatascience.com/harnessing-the-power-of-transfer-learning-for-medical-image-classification-fd772054fdc7"><span class="No-Break">https://towardsdatascience.com/harnessing-the-power-of-transfer-learning-for-medical-image-classification-fd772054fdc7</span></a></li>
</ul>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer140">
<h1 id="_idParaDest-175" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor225"/>Part 3 – Natural Language Processing with TensorFlow</h1>
<p>In this part, you will learn to build <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) applications with TensorFlow. You will understand how to perform text processing and build models for text classification. In this part, you will also learn to generate text <span class="No-Break">using LSTMs.</span></p>
<p>This section comprises the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B18118_10.xhtml#_idTextAnchor226"><em class="italic">Chapter 10</em></a>, <em class="italic">Introduction to Natural Language Processing</em></li>
<li><a href="B18118_11.xhtml#_idTextAnchor267"><em class="italic">Chapter 11</em></a>, <em class="italic">NLP with TensorFlow</em></li>
</ul>
</div>
<div>
<div id="_idContainer141">
</div>
</div>
<div>
<div id="_idContainer142">
</div>
</div>
<div>
<div id="_idContainer143">
</div>
</div>
<div>
<div id="_idContainer144">
</div>
</div>
<div>
<div id="_idContainer145">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer146">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer147">
</div>
</div>
<div>
<div id="_idContainer148">
</div>
</div>
<div>
<div id="_idContainer149">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer150">
</div>
</div>
</div></body></html>