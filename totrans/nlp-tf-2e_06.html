<html><head></head><body>
  <div id="_idContainer293" class="Basic-Text-Frame">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-132" class="chapterTitle">Recurrent Neural Networks</h1>
    <p class="normal"><strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>) are a special family of neural networks that are designed to cope<a id="_idIndexMarker520"/> with sequential data (that is, time-series data), such as stock market prices or a sequence of texts (for example, variable-length sentences). RNNs maintain a state variable that captures the various patterns present in sequential data; therefore, they are able to model sequential data. In comparison, conventional feed-forward neural networks do not have this ability unless the data is represented with a feature representation that captures the important patterns present in the sequence. However, coming up with such feature representations is extremely difficult. Another alternative for feed-forward models to model sequential data is to have a separate set of parameters for each position in time/sequence so that the set of parameters assigned to a certain position learns about the patterns that occur at that position. This will greatly increase the memory requirement for your model.</p>
    <p class="normal">However, as opposed to having a separate set of parameters for each position like feed-forward networks, RNNs share the same set of parameters over time. Sharing parameters over time is an important part of RNNs and in fact is one of the main enablers for learning temporal patterns. Then the state variable is updated over time for each input we observe in the sequence. These parameters shared over time, combined with the state vector, are able to predict the next value of a sequence, given the previously observed values of the sequence. Furthermore, since we process a single element of a sequence at a time (for example, one word in a document at a time), RNNs can process data of arbitrary lengths without padding data with special tokens.</p>
    <p class="normal">In this chapter, we will dive into the details of RNNs. First, we will discuss how an RNN can be formed by starting with a simple feed-forward model. </p>
    <p class="normal">After this we will discuss the basic functionality of an RNN. We will also delve into the underlying equations, such as output calculation and parameter update rules of RNNs, and discuss several variants of applications of RNNs: one-to-one, one-to-many, and many-to-many RNNs. We will walk through an example of using RNNs to identify named entities (e.g. person names, organization, etc.), which has valuable downstream use cases like building knowledge bases. We will discuss a more complex RNN model that can read text both forward and backward, and uses convolutional layers to increase the model accuracy. This chapter will cover this through the following main topics:</p>
    <ul>
      <li class="bulletList">Understanding RNNs</li>
      <li class="bulletList">Backpropagation Through Time</li>
      <li class="bulletList">Applications of RNNs</li>
      <li class="bulletList">Named Entity Recognition (NER) with RNNs</li>
      <li class="bulletList">NER with character and token embeddings</li>
    </ul>
    <h1 id="_idParaDest-133" class="heading-1">Understanding RNNs</h1>
    <p class="normal">In this section, we will discuss<a id="_idIndexMarker521"/> what an RNN is by starting with a gentle introduction, and then move on to more in-depth technical details. We mentioned earlier that RNNs maintain a state variable that evolves over time as the RNN sees more data, thus giving it the power to model sequential data. In particular, this state variable is updated over time by a set of recurrent connections. The existence of recurrent connections is the main structural difference between an RNN and a feed-forward network. The recurrent connections can be understood as links between a series of memories that the RNN learned in the past, connecting to the current state variable of the RNN. In other words, the recurrent connections update the current state variable with respect to the past memory the RNN has, enabling the RNN to make a prediction based on the current input as well as the previous inputs. </p>
    <div class="note">
      <p class="normal">The term RNN is sometimes used to refer to the family of recurrent models, which has many different models. In other words, it is sometimes used as a generalization of a specific RNN variant. Here, we are using the term RNN to refer to one of the earliest<a id="_idIndexMarker522"/> implementations of an RNN model known as the Elman network.</p>
    </div>
    <p class="normal">In the upcoming section, we will discuss the following topics. First, we will discuss how we can start by representing a feed-forward network as a computational graph. </p>
    <p class="normal">Then we will see through an example why a feed-forward network might fail at a sequential task. Then we will adapt that feed-forward graph to model sequential data, which will give us the basic computational graph<a id="_idIndexMarker523"/> of an RNN. We will also discuss the technical details (for example, update rules) of an RNN. Finally, we will discuss the details of how we can train RNN models.</p>
    <h2 id="_idParaDest-134" class="heading-2">The problem with feed-forward neural networks</h2>
    <p class="normal">To understand the limits<a id="_idIndexMarker524"/> of feed-forward neural networks and how RNNs address them, let’s imagine a sequence of data: </p>
    <p class="center"><img src="../Images/B14070_06_001.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Next, let’s assume that, in the real world, <em class="italic">x</em> and <em class="italic">y</em> are linked in the following relationship:</p>
    <p class="center"><img src="../Images/B14070_06_002.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_06_003.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Here, <em class="italic">g</em><sub class="subscript">1</sub> and <em class="italic">g</em><sub class="subscript">2</sub> are transformations (e.g. multiplying with a weight matrix followed by a non-linear transformation). This means that the current output <em class="italic">y</em><sub class="subscript">t</sub> depends on the current state <em class="italic">h</em><sub class="subscript">t,</sub> where <em class="italic">h</em><sub class="subscript">t</sub> is calculated with the current input <em class="italic">x</em><sub class="subscript">t</sub> and previous state <em class="italic">h</em><sub class="subscript">t-1</sub>. The state encodes information about previous inputs observed historically by the model.</p>
    <p class="normal">Now, let’s imagine a simple feed-forward neural network, which we will represent with the following: </p>
    <p class="center"><img src="../Images/B14070_06_004.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Here, <em class="italic">y</em><sub class="subscript">t</sub> is the predicted output for some input <em class="italic">x</em><sub class="subscript">t</sub>.</p>
    <p class="normal">If we use a feed-forward neural network to solve this task, the network will have to produce <img src="../Images/B14070_06_005.png" alt="" style="height: 1.25em !important; vertical-align: -0.28em !important;"/> one at a time, by taking <img src="../Images/B14070_06_006.png" alt="" style="height: 1.15em !important; vertical-align: -0.24em !important;"/> as inputs, one at a time. Now, let’s consider the problem we face in this solution for a time-series problem.</p>
    <p class="normal">The predicted output <em class="italic">y</em><sub class="subscript">t</sub> at time <em class="italic">t</em> of a feed-forward neural network depends only on the current input <em class="italic">x</em><sub class="subscript">t</sub>. In other words, it does not have any knowledge about the inputs that led to <em class="italic">x</em><sub class="subscript">t</sub> (that is, <img src="../Images/B14070_06_007.png" alt="" style="height: 1.15em !important; vertical-align: -0.36em !important;"/>). For this reason, a feed-forward neural network will fail at a task where the current output not only depends on the current input but also on the previous inputs. Let’s understand this through an example.</p>
    <p class="normal">Say we need to train a neural network<a id="_idIndexMarker525"/> to fill in missing words. We have the following phrase, and we would like to predict the next word:</p>
    <p class="normal"><em class="italic">James has a cat and it likes to drink ____.</em></p>
    <p class="normal">If we are to process one word at a time and use a feed-forward neural network, we will only have the input <em class="italic">drink</em> and this is not enough at all to understand the phrase or even to understand the context (the word <em class="italic">drink</em> can appear in many different contexts). One can argue that we can achieve good results by processing the full sentence in a single go. Even though this is true, such an approach has limitations such as processing very long sentences. However, there is a new family of models known as Transformers that are processing the full sequences of data with fully-connected layers, and have been surpassing the performance of sequential models. We will have a separate chapter on these models later.</p>
    <h2 id="_idParaDest-135" class="heading-2">Modeling with RNNs</h2>
    <p class="normal">On the other hand, we can use an RNN<a id="_idIndexMarker526"/> to find a solution to this problem. We will start with the data we have: </p>
    <p class="center"><img src="../Images/B14070_06_008.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Assume that we have the following relationship:</p>
    <p class="center"><img src="../Images/B14070_06_009.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_06_010.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Now, let’s replace <em class="italic">g</em><sub class="subscript">1</sub> with a function approximator <img src="../Images/B14070_06_011.png" alt="" style="height: 1.25em !important; vertical-align: -0.33em !important;"/> parametrized by <img src="../Images/B14070_06_012.png" alt="" style="height: 1.05em !important; vertical-align: -0.19em !important;"/> that takes the current input <em class="italic">x</em><sub class="subscript">t</sub> and the previous state of the system <em class="italic">h</em><sub class="subscript">t-1</sub> as the input and produces the current state <em class="italic">h</em><sub class="subscript">t</sub>. Then, we will replace <em class="italic">g</em><sub class="subscript">2</sub> with <img src="../Images/B14070_06_013.png" alt="" style="height: 1.25em !important; vertical-align: -0.31em !important;"/>, which takes the current state of the system <em class="italic">h</em><sub class="subscript">t</sub> to produce <em class="italic">y</em><sub class="subscript">t</sub>. This gives us the following:</p>
    <p class="center"><img src="../Images/B14070_06_014.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_06_015.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">We can think of <img src="../Images/B14070_06_016.png" alt="" style="height: 1.15em !important; vertical-align: -0.26em !important;"/> as an approximation of the true model that generates <em class="italic">x</em> and <em class="italic">y</em>. To understand this more clearly, let’s now expand the equation as follows: </p>
    <p class="center"><img src="../Images/B14070_06_017.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">For example, we can represent <em class="italic">y</em><sub class="subscript">4</sub> as follows: </p>
    <p class="center"><img src="../Images/B14070_06_018.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Also, by expansion<a id="_idIndexMarker527"/> we get the following (omitting <img src="../Images/B14070_06_012.png" alt="" style="height: 1.05em !important; vertical-align: -0.19em !important;"/> and <img src="../Images/B14070_06_020.png" alt="" style="height: 1.15em !important; vertical-align: -0.13em !important;"/> for clarity): </p>
    <p class="center"><img src="../Images/B14070_06_021.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">This can be illustrated in a graph, as shown in <em class="italic">Figure 6.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_01.png" alt="Modeling with Recurrent Neural Networks"/></figure>
    <p class="packt_figref">Figure 6.1: The relationship between x<sub class="subscript">t</sub> and y<sub class="subscript">t</sub> expanded</p>
    <p class="normal">We can generally summarize the diagram, for any given time step <em class="italic">t</em>, as shown in <em class="italic">Figure 6.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_02.png" alt="Modeling with Recurrent Neural Networks"/></figure>
    <p class="packt_figref">Figure 6.2: A single-step calculation of an RNN structure</p>
    <p class="normal">However, it should be understood<a id="_idIndexMarker528"/> that <em class="italic">h</em><sub class="subscript">t-1</sub> in fact is what <em class="italic">h</em><sub class="subscript">t</sub> was before receiving <em class="italic">x</em><sub class="subscript">t</sub>. In other words, <em class="italic">h</em><sub class="subscript">t-1</sub> is <em class="italic">h</em><sub class="subscript">t</sub> before one time step. </p>
    <p class="normal">Therefore, we can represent the calculation of <em class="italic">h</em><sub class="subscript">t</sub> with a recurrent connection, as shown in <em class="italic">Figure 6.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_03.png" alt="Modeling with Recurrent Neural Networks"/></figure>
    <p class="packt_figref">Figure 6.3: A single-step calculation of an RNN with the recurrent connection</p>
    <p class="normal">The ability to summarize a chain of equations<a id="_idIndexMarker529"/> mapping <img src="../Images/B14070_06_022.png" alt="" style="height: 1.15em !important; vertical-align: -0.21em !important;"/> to  <img src="../Images/B14070_06_023.png" alt="" style="height: 1.25em !important; vertical-align: -0.25em !important;"/> as in <em class="italic">Figure 6.3</em> allows us to write any <em class="italic">y</em><sub class="subscript">t</sub> in terms of <em class="italic">x</em><sub class="subscript">t</sub>, <em class="italic">h</em><sub class="subscript">t-1</sub>, and <em class="italic">h</em><sub class="subscript">t</sub>. This is the key idea behind an RNN.</p>
    <h2 id="_idParaDest-136" class="heading-2">Technical description of an RNN</h2>
    <p class="normal">Let’s now have an even closer<a id="_idIndexMarker530"/> look at what makes an RNN and define the mathematical equations for the calculations taking place within an RNN. Let’s start with the two functions we derived as function approximators for learning <em class="italic">y</em><sub class="subscript">t</sub> from <em class="italic">x</em><sub class="subscript">t</sub>:</p>
    <p class="center"><img src="../Images/B14070_06_024.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_06_025.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">As we have seen, a neural network<a id="_idIndexMarker531"/> is composed of a set of weights and biases and some nonlinear activation function. Therefore, we can write the preceding relation as shown here: </p>
    <p class="center"><img src="../Images/B14070_06_026.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Here, tanh is the tanh activation function, and <em class="italic">U</em> is a weight matrix of size <img src="../Images/B14070_06_027.png" alt="" style="height: 1.05em !important; vertical-align: -0.12em !important;"/>, where <em class="italic">m</em> is the number of hidden units and <em class="italic">d</em> is the dimensionality of the input. Also, <em class="italic">W</em> is a weight matrix of size <img src="../Images/B14070_06_028.png" alt="" style="height: 1.05em !important; vertical-align: -0.13em !important;"/> that creates the recurrent link from <em class="italic">h</em><sub class="subscript">t-1</sub> to <em class="italic">h</em><sub class="subscript">t</sub>. The <em class="italic">y</em><sub class="subscript">t</sub> relation is given by the following equation: </p>
    <p class="center"><img src="../Images/B14070_06_029.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Here, <em class="italic">V</em> is a weight matrix of size <img src="../Images/B14070_06_030.png" alt="" style="height: 1.05em !important; vertical-align: -0.09em !important;"/> and <em class="italic">c</em> is the dimensionality of the output (this can be the number of output classes). <a href="Chapter_6.xhtml"/>In <em class="italic">Figure 6.4</em>, we illustrate how these weights form an RNN. The arrows represent the direction that the data flows in the network:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_04.png" alt="Technical description of a Recurrent Neural Network"/></figure>
    <p class="packt_figref">Figure 6.4: The structure of an RNN</p>
    <p class="normal">So far, we have seen how we can represent<a id="_idIndexMarker532"/> an RNN with a graph of computational nodes, with edges denoting computations. Also, we looked at the actual mathematics behind an RNN. Let’s now look at how to optimize (or train) the weights of an RNN to learn from sequential data.</p>
    <h1 id="_idParaDest-137" class="heading-1">Backpropagation Through Time</h1>
    <p class="normal">For training RNNs, a special form<a id="_idIndexMarker533"/> of <strong class="keyWord">backpropagation</strong>, known as <strong class="keyWord">Backpropagation Through Time</strong> (<strong class="keyWord">BPTT</strong>), is used. To understand BPTT, however, first<a id="_idIndexMarker534"/> we need to understand how <strong class="keyWord">BP</strong> works. Then we will discuss why BP cannot be directly applied to RNNs, but how BP can be adapted for RNNs, resulting in BPTT. Finally, we will discuss two major<a id="_idIndexMarker535"/> problems present in BPTT.</p>
    <h2 id="_idParaDest-138" class="heading-2">How backpropagation works</h2>
    <p class="normal">Backpropagation is the technique<a id="_idIndexMarker536"/> that is used to train a feed-forward neural network. In backpropagation, you do the following:</p>
    <ul>
      <li class="bulletList">Calculate a prediction for a given input</li>
      <li class="bulletList">Calculate an error, <em class="italic">E</em>, of the prediction by comparing it to the actual label of the input (for example, mean squared error and cross-entropy loss)</li>
      <li class="bulletList">Update the weights of the feed-forward network to minimize the loss calculated in <em class="italic">step 2</em>, by taking a small step in the opposite direction of the gradient <img src="../Images/B14070_06_031.png" alt="" style="height: 1.35em !important; vertical-align: -0.42em !important;"/> for all <em class="italic">w</em><sub class="subscript">ij</sub>, where <em class="italic">w</em><sub class="subscript">ij</sub> is the <em class="italic">j</em><sup class="superscript">th</sup> weight of the <em class="italic">i</em><sup class="superscript">th</sup> layer</li>
    </ul>
    <p class="normal">To understand the above computations more clearly, consider the feed-forward network depicted in <em class="italic">Figure 6.5</em>. This has two single weights, <em class="italic">w</em><sub class="subscript">1</sub> and <em class="italic">w</em><sub class="subscript">2</sub>, and calculates two outputs, <em class="italic">h</em> and <em class="italic">y</em>, as shown in the following figure. We assume no nonlinearities in the model for simplicity:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_05.png" alt="How backpropagation works"/></figure>
    <p class="packt_figref">Figure 6.5: Computations of a feed-forward network</p>
    <p class="normal">We can calculate <img src="../Images/B14070_06_032.png" alt="" style="height: 1.88em !important; vertical-align: -0.39em !important;"/> using the chain<a id="_idIndexMarker537"/> rule as follows: </p>
    <p class="center"><img src="../Images/B14070_06_033.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">This simplifies to the following: </p>
    <p class="center"><img src="../Images/B14070_06_034.png" alt="" style="height: 2.60em !important;"/></p>
    <p class="normal">Here, <em class="italic">l</em> is the correct label for the data point <em class="italic">x</em>. Also, we are assuming the mean squared error as the loss function. Everything<a id="_idIndexMarker538"/> here is defined, and it is quite straightforward to calculate <img src="../Images/B14070_06_035.png" alt="" style="height: 1.88em !important; vertical-align: -0.45em !important;"/>.</p>
    <h2 id="_idParaDest-139" class="heading-2">Why we cannot use BP directly for RNNs</h2>
    <p class="normal">Now, let’s try the same for the RNN in <a href="Chapter_6.xhtml"/><a id="_idIndexMarker539"/><em class="italic">Figure 6.6</em>. Now we have an additional recurrent weight <em class="italic">w</em><sub class="subscript">3</sub>. We have omitted the time components of inputs and outputs for the clarity of the problem we are trying to emphasize:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_06.png" alt="Why we cannot use BP directly for RNNs"/></figure>
    <p class="packt_figref">Figure 6.6: Computations of an RNN</p>
    <p class="normal">Let’s see what happens<a id="_idIndexMarker540"/> if we apply the chain rule to calculate <img src="../Images/B14070_06_036.png" alt="" style="height: 1.88em !important; vertical-align: -0.68em !important;"/>:</p>
    <p class="center"><img src="../Images/B14070_06_037.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">This becomes the following: </p>
    <p class="center"><img src="../Images/B14070_06_038.png" alt="" style="height: 2.60em !important;"/></p>
    <p class="normal">The term <img src="../Images/B14070_06_039.png" alt="" style="height: 1.88em !important; vertical-align: -0.32em !important;"/> here creates problems because it is a recursive term. You end up with an infinite number of derivative terms, as <em class="italic">h</em> is recursive (that is, calculating <em class="italic">h</em> includes <em class="italic">h</em> itself) and <em class="italic">h</em> is not a constant<a id="_idIndexMarker541"/> and dependent on <em class="italic">w</em><sub class="subscript">3</sub>. This is solved by unrolling the input sequence <em class="italic">x</em> over time, creating a copy of the RNN for each input <em class="italic">x</em><sub class="subscript">t</sub> and calculating derivatives for each copy separately, and collapsing those updates into one, by summing up the gradients, to calculate the weight update. We will discuss the details of this process next.</p>
    <h2 id="_idParaDest-140" class="heading-2">Backpropagation Through Time – training RNNs</h2>
    <p class="normal">The trick to calculating backpropagation<a id="_idIndexMarker542"/> for RNNs is to consider not a single input, but the full input sequence. Then, if we calculate <img src="../Images/B14070_06_040.png" alt="" style="height: 1.88em !important; vertical-align: -0.41em !important;"/> at time step 4, we will get the following: </p>
    <p class="center"><img src="../Images/B14070_06_041.png" alt="" style="height: 3.55em !important;"/></p>
    <p class="normal">This means that we need to calculate the sum of gradients for all the time steps up to the fourth time step. In other words, we will first unroll the sequence so that we can calculate <img src="../Images/B14070_06_042.png" alt="" style="height: 2.08em !important; vertical-align: -0.48em !important;"/> and <img src="../Images/B14070_06_043.png" alt="" style="height: 1.98em !important; vertical-align: -0.48em !important;"/> for each time step <em class="italic">j</em>. This is done by creating four copies of the RNN. So, to calculate <img src="../Images/B14070_06_044.png" alt="" style="height: 2.08em !important; vertical-align: -0.46em !important;"/>, we need <em class="italic">t-j+1</em> copies of the RNN. Then we will roll up the copies to a single RNN by summing up gradients with respect to all previous time steps to get the gradient, and update the RNN with the gradient <img src="../Images/B14070_06_045.png" alt="" style="height: 1.88em !important; vertical-align: -0.38em !important;"/>.</p>
    <p class="normal">However, this becomes costly as the number<a id="_idIndexMarker543"/> of time steps increases. For more computational efficiency, we can use <strong class="keyWord">Truncated Backpropagation Through Time</strong> (<strong class="keyWord">TBPTT</strong>) to optimize recurrent models, which is an approximation of BPTT.</p>
    <h2 id="_idParaDest-141" class="heading-2">Truncated BPTT – training RNNs efficiently</h2>
    <p class="normal">In TBPTT, we only calculate<a id="_idIndexMarker544"/> the gradients for a fixed number of <em class="italic">T</em> time steps (in contrast to calculating it up to the very beginning of the sequence as in BPTT). More specifically, when calculating <img src="../Images/B14070_06_040.png" alt="" style="height: 1.88em !important; vertical-align: -0.41em !important;"/>, for time step <em class="italic">t</em>, we only calculate derivatives down to <em class="italic">t-T</em> (that is, we do not compute derivatives up to the very beginning): </p>
    <p class="center"><img src="../Images/B14070_06_047.png" alt="" style="height: 3.55em !important;"/></p>
    <p class="normal">This is much more computationally efficient than standard BPTT. In standard BPTT, for each time step <em class="italic">t</em>, we calculate derivatives up to the very beginning of the sequence. But this gets computationally infeasible as the sequence length becomes larger and larger (for example, this could occur when processing a long text document word by word). However, in truncated BPTT, we only calculate the derivatives for a fixed number of steps backward, and as you can imagine, the computational cost does not change as the sequence becomes larger.</p>
    <h2 id="_idParaDest-142" class="heading-2">Limitations of BPTT – vanishing and exploding gradients</h2>
    <p class="normal">Having a way to calculate gradients<a id="_idIndexMarker545"/> for recurrent weights and having a computationally efficient approximation such as TBPTT does not enable us to train RNNs without trouble. Something else can go wrong with the calculations.</p>
    <p class="normal">To see why, let’s expand a single term in <img src="../Images/B14070_06_048.png" alt="" style="height: 1.88em !important; vertical-align: -0.51em !important;"/>, which is as follows: </p>
    <p class="center"><img src="../Images/B14070_06_049.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">Since we know that the issues of backpropagation arise from the recurrent connections, let’s ignore the <em class="italic">w</em><sub class="subscript">1</sub><em class="italic">x</em> terms and consider the following:</p>
    <p class="center"><img src="../Images/B14070_06_050.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">By simply expanding <em class="italic">h</em><sub class="subscript">3</sub> and doing simple arithmetic operations we can show this: </p>
    <p class="center"><img src="../Images/B14070_06_051.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">We see that for just four time steps we have a term <img src="../Images/B14070_06_052.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>. So at the <em class="italic">n</em><sup class="superscript">th</sup> time step, it would become <img src="../Images/B14070_06_053.png" alt="" style="height: 1.15em !important;"/>. Say we initialized <em class="italic">w</em><sub class="subscript">3</sub> to be very small (say 0.00001) at <em class="italic">n</em>=<em class="italic">100</em> time step; the gradient would<a id="_idIndexMarker546"/> be infinitesimally small (of scale 10<sup class="superscript">-500</sup>). Also, since computers have limited precision in representing a number, this update<a id="_idIndexMarker547"/> would be ignored (that is, arithmetic underflow). This is called the <strong class="keyWord">vanishing gradient</strong>. </p>
    <p class="normal">Solving the vanishing gradient is not very straightforward. There are no easy ways of rescaling the gradients so that they will properly propagate through time. A few techniques used in practice to solve the problem of vanishing gradients are to use careful initialization of weights (for example, the Xavier initialization), or to use momentum-based optimization methods (that is, in addition to the current gradient update, we add an<a id="_idIndexMarker548"/> additional term, which is the accumulation of all the past gradients known as the <strong class="keyWord">velocity term</strong>). However, more principled approaches to solving the vanishing gradient problem, such as different structural modifications to the standard RNN, have been introduced, as we will see in <em class="chapterRef">Chapter 7, Understanding Long</em><em class="italic"> Short-Term Memory Networks</em>.</p>
    <p class="normal">On the other hand, say that we initialized <em class="italic">w</em><sub class="subscript">3</sub> to be very large (say 1000.00). Then at the <em class="italic">n</em>=<em class="italic">100</em> time step, the gradients would be massive (of scale 10<sup class="superscript">300</sup>). </p>
    <p class="normal">This leads to numerical instabilities and you will<a id="_idIndexMarker549"/> get values such as <code class="inlineCode">Inf</code> or <code class="inlineCode">NaN</code> (that is, not a number) in Python. This is called the <strong class="keyWord">exploding gradient</strong>.</p>
    <p class="normal">Gradient explosion can also take place due to the complexity of the loss surface of a problem. Complex nonconvex loss surfaces are very common in deep neural networks due to both the dimensionality of inputs as well as the large number of parameters (weights) present in the models.</p>
    <p class="normal"><em class="italic">Figure</em> <em class="italic">6.7</em> illustrates the loss surface of an RNN and highlights the presence of walls with very high curvature. If the optimization method comes in contact with such a wall, then the gradients will explode or overshoot, as shown by the solid line in the image. This can either lead to very poor loss minimization, numerical instabilities, or both. A simple solution to avoid gradient explosion in such situations is to clip the gradients to a reasonably small value when it is larger than some threshold. The dashed line in the figure shows what happens when we clip the gradient at some small value. (Gradient clipping is covered in the paper <em class="italic">On the difficulty of training recurrent neural networks</em>, <em class="italic">Pascanu</em>, <em class="italic">Mikolov</em>, <em class="italic">and</em> <em class="italic">Bengio</em>, <em class="italic">International Conference on Machine Learning (2013): 1310-1318</em>.)</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_07.png" alt="Limitations of BPTT – vanishing and exploding gradients"/></figure>
    <p class="packt_figref">Figure 6.7: The gradient explosion phenomenon. Source: This figure is from the paper ‘On the difficulty of training recurrent neural networks’ by Pascanu, Mikolov, and Bengio</p>
    <p class="normal">Here we conclude<a id="_idIndexMarker550"/> our discussion about BPTT, which adapts backpropagation for RNNs. Next we will discuss various ways that RNNs can be used to solve applications. These applications include sentence classification, image captioning, and machine translation. We will categorize the RNNs into several different categories such as one-to-one, one-to-many, many-to-one, and many-to-many.</p>
    <h1 id="_idParaDest-143" class="heading-1">Applications of RNNs</h1>
    <p class="normal">So far, we have only talked <a id="_idIndexMarker551"/>about one-to-one-mapped RNNs, where the current output depends on the current input as well as the previously observed history of inputs. This means that there exists an output for the sequence of previously observed inputs and the current input. However, in the real word, there can be situations<a id="_idIndexMarker552"/> where there is only one output for a sequence of inputs, a sequence of outputs for a single input, and a sequence of outputs for a sequence of inputs where the sequence sizes are different. In this section, we will look at several different settings of RNN models and the applications they would be used in.</p>
    <h2 id="_idParaDest-144" class="heading-2">One-to-one RNNs</h2>
    <p class="normal">In one-to-one RNNs, the current <a id="_idIndexMarker553"/>input depends on the previously<a id="_idIndexMarker554"/> observed inputs (see <em class="italic">Figure 6.8</em>). Such RNNs are appropriate for problems where each input has an output, but the output depends both on the current input and the history of inputs that led to the current input. An example of such a task is stock market prediction, where we output a value for the current input, and this output also depends on how the previous inputs have behaved. Another example would be scene classification, where each pixel in an image is labeled (for example, labels such as car, road, and person). Sometimes <em class="italic">x</em><sub class="subscript">t+1</sub> can be the same as <em class="italic">y</em><sub class="subscript">t</sub> for some problems. For example, in text generation problems, the previously predicted word becomes an input to predict the next word. The following figure depicts a one-to-one RNN:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_08.png" alt="Diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.8: One-to-one RNNs having temporal dependencies</p>
    <h2 id="_idParaDest-145" class="heading-2">One-to-many RNNs</h2>
    <p class="normal">A one-to-many RNN would take a single input<a id="_idIndexMarker555"/> and output a sequence (see <em class="italic">Figure 6.9</em>). Here, we assume the inputs<a id="_idIndexMarker556"/> to be independent of each other. </p>
    <p class="normal">That is, we do not need information about previous inputs to make a prediction about the current input. However, the recurrent connections are needed because, although we process a single input, the output is a sequence of values that depends on the previous output values. An example task where such an RNN would be used is an image captioning task. For example, for a given input image, the text caption can consist of five or ten words. In other words, the RNN will keep predicting words until it outputs a meaningful phrase describing the image. The following figure depicts a one-to-many RNN:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_09.png" alt="One-to-many RNNs"/></figure>
    <p class="packt_figref">Figure 6.9: A one-to-many RNN</p>
    <h2 id="_idParaDest-146" class="heading-2">Many-to-one RNNs</h2>
    <p class="normal">Many-to-one RNNs<a id="_idIndexMarker557"/> take an input of arbitrary length and produce<a id="_idIndexMarker558"/> a single output for the sequence of inputs (see <em class="italic">Figure 6.10</em>). Sentence classification is one such task that can benefit from a many-to-one RNN. A sentence is represented to the model as a sequence of words of arbitrary length. The model takes it as the input and produces an output, classifying the sentence into one of a set of predefined classes. Some specific examples of sentence classification are as follows:</p>
    <ul>
      <li class="bulletList">Classifying movie reviews as positive or negative statements (that is, sentiment analysis)</li>
      <li class="bulletList">Classifying a sentence depending on what the sentence describes (for example, person, object, or location)</li>
    </ul>
    <p class="normal">Another application<a id="_idIndexMarker559"/> of many-to-one RNNs is classifying large-scale<a id="_idIndexMarker560"/> images by processing only a patch of images at a time and moving the window over the whole image.</p>
    <p class="normal">The following figure depicts a many-to-one RNN:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_10.png" alt="Many-to-one RNNs"/></figure>
    <p class="packt_figref">Figure 6.10: A many-to-one RNN</p>
    <h2 id="_idParaDest-147" class="heading-2">Many-to-many RNNs</h2>
    <p class="normal">Many-to-many RNNs (or Sequences-to-Sequence, seq2seq for short) often<a id="_idIndexMarker561"/> produce arbitrary-length<a id="_idIndexMarker562"/> outputs from arbitrary-length inputs (see <em class="italic">Figure 6.11</em>). In other words, inputs and outputs do not have to be of the same length. This is particularly useful in machine translation, where we translate a sentence from one language to another. As you can imagine, one sentence<a id="_idIndexMarker563"/> in a certain language does not always align with a sentence<a id="_idIndexMarker564"/> from another language. Another such example is chatbots, where the chatbot reads a sequence of words (that is, a user request) and outputs a sequence of words (that is, the answer). The following figure depicts a many-to-many RNN:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_11.png" alt="Many-to-many RNNs"/></figure>
    <p class="packt_figref">Figure 6.11: A many-to-many RNN</p>
    <p class="normal">We can summarize the different types of applications of feed-forward networks and RNNs as follows:</p>
    <table id="table001-3" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Algorithm</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Description</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Applications</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">One-to-one RNNs</p>
          </td>
          <td class="table-cell">
            <p class="normal">These take a single input and give a single output. Current input depends on the previously observed input(s).</p>
          </td>
          <td class="table-cell">
            <p class="normal">Stock market prediction, scene classification, and text generation</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">One-to-many RNNs</p>
          </td>
          <td class="table-cell">
            <p class="normal">These take a single input and give an output consisting of an arbitrary number of elements</p>
          </td>
          <td class="table-cell">
            <p class="normal">Image captioning</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Many-to-one RNNs</p>
          </td>
          <td class="table-cell">
            <p class="normal">These take a sequence of inputs and give a single output.</p>
          </td>
          <td class="table-cell">
            <p class="normal">Sentence classification (considering a single word as a single input)</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Many-to-many RNNs</p>
          </td>
          <td class="table-cell">
            <p class="normal">These take a sequence of arbitrary length as inputs and output a sequence of arbitrary length.</p>
          </td>
          <td class="table-cell">
            <p class="normal">Machine translation, chatbots</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Next, we will learn how to use RNNs to identify various entities mentioned in a text corpus.</p>
    <h1 id="_idParaDest-148" class="heading-1">Named Entity Recognition with RNNs</h1>
    <p class="normal">Now let’s look at our first task: using<a id="_idIndexMarker565"/> an RNN to identify named entities<a id="_idIndexMarker566"/> in a text corpus. This task<a id="_idIndexMarker567"/> is known as <strong class="keyWord">Named Entity Recognition</strong> (<strong class="keyWord">NER</strong>). We will be using a modified version of the well-known <strong class="keyWord">CoNLL 2003</strong> (which stands for <strong class="keyWord">Conference on Computational Natural Language Learning - 2003</strong>) dataset for NER. </p>
    <p class="normal">CoNLL 2003 is available for multiple languages, and the English data was generated from a Reuters Corpus that contains news stories published between August 1996 and August 1997. The database we’ll be using is found at <a href="https://github.com/ZihanWangKi/CrossWeigh"><span class="url">https://github.com/ZihanWangKi/CrossWeigh</span></a> and is called <strong class="keyWord">CoNLLPP</strong>. It is a more closely curated version<a id="_idIndexMarker568"/> than the original CoNLL, which contains errors in the dataset induced by incorrectly understanding the context of a word. For example, in the phrase <em class="italic">“Chicago won …”</em> Chicago was identified as a location, whereas it is in fact an organization. This exercise is available in <code class="inlineCode">ch06_rnns_for_named_entity_recognition.ipynb</code> in the <code class="inlineCode">Ch06-Recurrent-Neural-Networks</code> folder.</p>
    <h2 id="_idParaDest-149" class="heading-2">Understanding the data</h2>
    <p class="normal">We have defined<a id="_idIndexMarker569"/> a function called <code class="inlineCode">download_data()</code>, which can be used to download the data. We will not go into the details of it as it simply downloads several files and places them in a data folder. Once the download finishes, you’ll have three files:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">data\conllpp_train.txt</code> – Training set, contains 14041 sentences</li>
      <li class="bulletList"><code class="inlineCode">data\conllpp_dev.txt</code> – Validation set, contains 3250 sentences</li>
      <li class="bulletList"><code class="inlineCode">data\conllpp_test.txt</code> – Test set, contains 3452 sentences</li>
    </ul>
    <p class="normal">Next up, we will read the data and convert it into a specific format that suits our model. But before that, we need to see what our data looks like originally:</p>
    <pre class="programlisting code"><code class="hljs-code">-DOCSTART- -X- -X- O
EU NNP B-NP B-ORG
rejects VBZ B-VP O
German JJ B-NP B-MISC
call NN I-NP O
to TO B-VP O
boycott VB I-VP O
British JJ B-NP B-MISC
lamb NN I-NP O
. . O O
The DT B-NP O
European NNP I-NP B-ORG
Commission NNP I-NP I-ORG
said VBD B-VP O
...
to TO B-PP O
sheep NN B-NP O
. . O O
</code></pre>
    <p class="normal">As you can see, the document<a id="_idIndexMarker570"/> has a single word in each line along with the associated tags of that word. These tags are in the following order:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">The Part-of-speech (POS) tag (e.g. noun - <code class="inlineCode">NN</code>, verb - <code class="inlineCode">VB</code>, determinant - <code class="inlineCode">DT</code>, etc.)</li>
      <li class="numberedList">Chunk tag – A chunk is a segment of text made of one or more tokens (for example, <code class="inlineCode">NP</code> represents a noun phrase such as “The European Commission”)</li>
      <li class="numberedList">Named entity tag (e.g. Location, Organization, Person, etc.)</li>
    </ol>
    <p class="normal">Both chunk tags and named entity tags have a <code class="inlineCode">B-</code> and <code class="inlineCode">I-</code> prefix (e.g. <code class="inlineCode">B-ORG</code> or <code class="inlineCode">I-ORG</code>). These prefixes are there to differentiate the starting token of an entity/chunk from the continuing token of an entity/chunk. </p>
    <p class="normal">There are also five types of entities in the dataset:</p>
    <ul>
      <li class="bulletList">Location-based entities (<code class="inlineCode">LOC</code>)</li>
      <li class="bulletList">Person-based entities (<code class="inlineCode">PER</code>)</li>
      <li class="bulletList">Organization-based entities (<code class="inlineCode">ORG</code>)</li>
      <li class="bulletList">Miscellaneous entities (<code class="inlineCode">MISC</code>)</li>
      <li class="bulletList">Non-entities (<code class="inlineCode">O</code>) </li>
    </ul>
    <p class="normal">Finally, there’s an empty line between separate sentences. </p>
    <p class="normal">Now let’s look at the code that loads<a id="_idIndexMarker571"/> the data we downloaded into memory, so that we can start using it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">read_data</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-string">'''</span>
<span class="hljs-string">    Read data from a file with given filename</span>
<span class="hljs-string">    Returns a list of sentences (each sentence a string), </span>
<span class="hljs-string">    and list of ner labels for each string</span>
<span class="hljs-string">    '''</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Reading data ..."</span>)
    <span class="hljs-comment"># master lists - Holds sentences (list of tokens),</span>
<span class="hljs-comment">    # ner_labels (for each token an NER label)</span>
    sentences, ner_labels = [], [] 
    
    <span class="hljs-comment"># Open the file</span>
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename,<span class="hljs-string">'r'</span>,encoding=<span class="hljs-string">'latin-1'</span>) <span class="hljs-keyword">as</span> f:        
        <span class="hljs-comment"># Read each line</span>
        is_sos = <span class="hljs-literal">True</span> 
        <span class="hljs-comment"># We record at each line if we are seeing the beginning of a </span>
<span class="hljs-comment">        # sentence</span>
        
        <span class="hljs-comment"># Tokens and labels of a single sentence, flushed when encountered</span>
<span class="hljs-comment">        # a new one</span>
        sentence_tokens = []
        sentence_labels = []
        i = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> f:
        <span class="hljs-comment"># If we are seeing an empty line or -DOCSTART- that's a new line</span>
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(row.strip()) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> row.split(<span class="hljs-string">' '</span>)[<span class="hljs-number">0</span>] == <span class="hljs-string">'-</span>
            <span class="hljs-string">DOCSTART-'</span>:
                is_sos = <span class="hljs-literal">False</span>
            <span class="hljs-comment"># Otherwise keep capturing tokens and labels</span>
            <span class="hljs-keyword">else</span>:
                is_sos = <span class="hljs-literal">True</span>
                token, _, _, ner_label = row.split(<span class="hljs-string">' '</span>)
                sentence_tokens.append(token)
                sentence_labels.append(ner_label.strip())
            
            <span class="hljs-comment"># When we reach the end / or reach the beginning of next</span>
            <span class="hljs-comment"># add the data to the master lists, flush the temporary one</span>
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_sos <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(sentence_tokens)&gt;<span class="hljs-number">0</span>:
                sentences.append(<span class="hljs-string">' '</span>.join(sentence_tokens))
                ner_labels.append(sentence_labels)
                sentence_tokens, sentence_labels = [], []
    
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'\tDone'</span>)
    <span class="hljs-keyword">return</span> sentences, ner_labels
</code></pre>
    <p class="normal">Here, we will store all the<a id="_idIndexMarker572"/> sentences (as a list of strings in <code class="inlineCode">sentences</code>) and all the labels associated with each token in the sentences (as a list of lists in <code class="inlineCode">ner_labels</code>). We will read the file line by line. We will maintain a Boolean called <code class="inlineCode">is_sos</code> that indicates whether we are at the start of a sentence. We will also have two temporary lists (<code class="inlineCode">sentence_tokens</code> and <code class="inlineCode">sentence_labels</code>) that will accumulate the tokens and the NER labels of the current sentence. When we are at the start of a sentence, we reset these temporary lists. Otherwise, we keep writing each token and NER label we see in the file to these temporary lists. We can now run this function on the train, validation, and test corpora we have:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Train data</span>
train_sentences, train_labels = read_data(train_filepath) 
<span class="hljs-comment"># Validation data</span>
valid_sentences, valid_labels = read_data(dev_filepath) 
<span class="hljs-comment"># Test data</span>
test_sentences, test_labels = read_data(test_filepath)
</code></pre>
    <p class="normal">We will print a few samples and see what we have with:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Print some data</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">'\nSample data\n'</span>)
<span class="hljs-keyword">for</span> v_sent, v_labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(valid_sentences[:<span class="hljs-number">5</span>], valid_labels[:<span class="hljs-number">5</span>]):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Sentence: {}"</span>.<span class="hljs-built_in">format</span>(v_sent))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Labels: {}"</span>.<span class="hljs-built_in">format</span>(v_labels))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'\n'</span>)
</code></pre>
    <p class="normal">This produces:</p>
    <pre class="programlisting con"><code class="hljs-con">Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .
Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .
Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']
Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .
Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']
</code></pre>
    <p class="normal">One of the unique characteristics of NER tasks is the class imbalance. That is, not all classes will have a roughly equal<a id="_idIndexMarker573"/> number of samples. As you can probably guess, in a corpus, there are more non-named entities than named entities. This leads to a significant class imbalance among labels. Therefore, let’s have a look at the distribution of samples among different classes:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> chain
<span class="hljs-comment"># Print the value count for each label</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Training data label counts"</span>)
<span class="hljs-built_in">print</span>(pd.Series(chain(*train_labels)).value_counts())
</code></pre>
    <p class="normal">To analyze the data, we will first convert the NER labels into a pandas <code class="inlineCode">Series</code> object. This can be done by simply calling the <code class="inlineCode">pd.Series()</code> construct on <code class="inlineCode">train_labels</code>, <code class="inlineCode">valid_labels</code>, and <code class="inlineCode">test_labels</code>. But remember that these were lists of lists, where each inner list represents the NER tags for all the tokens in a sentence. To create a flat list, we can use the <code class="inlineCode">chain()</code> function from the built-in Python library <code class="inlineCode">itertools</code>. It will chain several lists together to form a single list. After that, we call the <code class="inlineCode">value_counts()</code> function on that pandas <code class="inlineCode">Series</code>. This will return a new list, where the indices are unique labels found in the original <code class="inlineCode">Series</code> and the values are the counts of occurrences of each label. This gives us:</p>
    <pre class="programlisting con"><code class="hljs-con">Training data label counts
O         169578
B-LOC       7140
B-PER       6600
B-ORG       6321
I-PER       4528
I-ORG       3704
B-MISC      3438
I-LOC       1157
I-MISC      1155
dtype: int64
</code></pre>
    <p class="normal">As you can see, O labels are several magnitudes higher than the volume of other labels. We need to keep this in mind<a id="_idIndexMarker574"/> when training the model. Subsequently, we will analyze the sequence length (i.e. number of tokens) of each sentence. We need this information later to pad our sentences to a fixed length.</p>
    <pre class="programlisting code"><code class="hljs-code">pd.Series(train_sentences).<span class="hljs-built_in">str</span>.split().<span class="hljs-built_in">str</span>.<span class="hljs-built_in">len</span>().describe(percentiles=[<span class="hljs-number">0.05</span>, <span class="hljs-number">0.95</span>])
</code></pre>
    <p class="normal">Here, we create a pandas <code class="inlineCode">Series</code>, where each item has the length of a sentence after splitting each sentence into a list of tokens. </p>
    <p class="normal">Then we will look at the 5% and 95% percentiles of those lengths. This produces:</p>
    <pre class="programlisting con"><code class="hljs-con">count    14041.000000
mean        14.501887
std         11.602756
min          1.000000
5%           2.000000
50%         10.000000
95%         37.000000
max        113.000000
dtype: float64
</code></pre>
    <p class="normal">We can see that 95% of our sentences have 37 tokens or less.</p>
    <h2 id="_idParaDest-150" class="heading-2">Processing data</h2>
    <p class="normal">Now it’s time to process<a id="_idIndexMarker575"/> the data. We will keep the sentences in the same format, i.e. a list of strings where each string represents a sentence. This is because we will integrate text processing right into our model (as opposed to doing it externally). For labels, we have to do several changes. Remember labels are a list of lists, where the inner lists represent labels for all the tokens in each sentence. Specifically we will do the following:</p>
    <ul>
      <li class="bulletList">Convert the class labels to class IDs</li>
      <li class="bulletList">Pad the sequences of labels to a specified maximum length</li>
      <li class="bulletList">Generate a mask that indicates the padded labels, so that we can use this information to disregard the padded labels during model training</li>
    </ul>
    <p class="normal">First let’s write a function<a id="_idIndexMarker576"/> to get a class label to class ID mapping. This function leverages pandas’ <code class="inlineCode">unique()</code> function to get the unique labels in the training set and generate a mapping of integers to unique labels found.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_label_id_map</span>(<span class="hljs-params">train_labels</span>):
    <span class="hljs-comment"># Get the unique list of labels</span>
    unique_train_labels = pd.Series(chain(*train_labels)).unique()
    <span class="hljs-comment"># Create a class label -&gt; class ID mapping</span>
    labels_map = <span class="hljs-built_in">dict</span>(
        <span class="hljs-built_in">zip</span>(unique_train_labels, 
    np.arange(unique_train_labels.shape[<span class="hljs-number">0</span>])))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"labels_map: {}"</span>.<span class="hljs-built_in">format</span>(labels_map))
    <span class="hljs-keyword">return</span> labels_map
</code></pre>
    <p class="normal">If you run this with:</p>
    <pre class="programlisting code"><code class="hljs-code">labels_map = get_label_id_map(train_labels)
</code></pre>
    <p class="normal">Then you will get:</p>
    <pre class="programlisting con"><code class="hljs-con">labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}
</code></pre>
    <p class="normal">We write a function called <code class="inlineCode">get_padded_int_labels()</code> that will take sequences of class labels and return sequences of padded class IDs, with the option to return a mask indicating padded labels. This function takes the following arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">labels</code> (<code class="inlineCode">List[List[str]]</code>) – A list of lists of strings, where each string is a class label of the string type</li>
      <li class="bulletList"><code class="inlineCode">labels_map</code> (<code class="inlineCode">Dict[str, int]</code>) – A dictionary mapping a string label to a class ID of type integer</li>
      <li class="bulletList"><code class="inlineCode">max_seq_length</code> (<code class="inlineCode">int</code>) – A maximum length to be padded to (longer sequences will be truncated at this length)</li>
      <li class="bulletList"><code class="inlineCode">return_mask</code> (<code class="inlineCode">bool</code>) – Whether to return the mask showing padded labels or not</li>
    </ul>
    <p class="normal">Let’s now look at the code that performs<a id="_idIndexMarker577"/> the aforementioned operations:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_padded_int_labels</span>(<span class="hljs-params">labels, labels_map, max_seq_length,</span>
<span class="hljs-params">return_mask=</span><span class="hljs-literal">True</span>):
    <span class="hljs-comment"># Convert string labels to integers </span>
    int_labels = [[labels_map[x] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> one_seq] <span class="hljs-keyword">for</span> one_seq <span class="hljs-keyword">in</span> 
    labels]
    
    <span class="hljs-comment"># Pad sequences</span>
    <span class="hljs-keyword">if</span> return_mask:
        <span class="hljs-comment"># If we return mask, we first pad with a special value (-1) and </span>
        <span class="hljs-comment"># use that to create the mask and later replace -1 with 'O'</span>
        padded_labels = np.array(
            tf.keras.preprocessing.sequence.pad_sequences(
                int_labels, maxlen=max_seq_length, padding=<span class="hljs-string">'post'</span>, 
                truncating=<span class="hljs-string">'post'</span>, value=-<span class="hljs-number">1</span>
            )
        )
        
        <span class="hljs-comment"># mask filter</span>
        mask_filter = (padded_labels != -<span class="hljs-number">1</span>)
        <span class="hljs-comment"># replace -1 with 'O' s ID</span>
        padded_labels[~mask_filter] = labels_map[<span class="hljs-string">'O'</span>]        
        <span class="hljs-keyword">return</span> padded_labels, mask_filter.astype(<span class="hljs-string">'int'</span>)
    
    <span class="hljs-keyword">else</span>:
        padded_labels = np.array(ner_pad_sequence_func(int_labels, 
        value=labels_map[<span class="hljs-string">'O'</span>]))
        <span class="hljs-keyword">return</span> padded_labels
</code></pre>
    <p class="normal">You can see the first step in the function converts all the string labels in <code class="inlineCode">labels</code> to integer labels using the <code class="inlineCode">labels_map</code>. Next we get the padded sequences with the <code class="inlineCode">tf.keras.preprocessing.sequence.pad_sequences()</code> function. We discussed this function in detail in the previous chapter. Essentially, it will pad (with a specified value) and truncate arbitrary-length sequences, to return fixed-length sequences. We are instructing the function<a id="_idIndexMarker578"/> to do both padding and truncating at the end of sequences, and to pad with a special value of <code class="inlineCode">-1</code>. Then we can simply generate the mask as a boolean filter where <code class="inlineCode">padded_labels</code> is not equal to <code class="inlineCode">-1</code>. Thus, the positions where original labels exist will have a value of <code class="inlineCode">1</code> and the rest will have <code class="inlineCode">0</code>. However, we have to convert the <code class="inlineCode">-1</code> values to a class ID found in the <code class="inlineCode">labels_map</code>. We will give them the class ID of the label <code class="inlineCode">O</code> (i.e. others).</p>
    <p class="normal">From our findings in the previous chapter, we will set the maximum sequence length to <code class="inlineCode">40</code>. Remember that the 95% percentile fell at the length of 37 words:</p>
    <pre class="programlisting code"><code class="hljs-code">max_seq_length = <span class="hljs-number">40</span>
</code></pre>
    <p class="normal">And now we will generate processed labels and masks for all of the training, validation, and testing data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Convert string labels to integers for all train/validation/test data</span>
<span class="hljs-comment"># Pad train/validation/test data</span>
padded_train_labels, train_mask = get_padded_int_labels(
    train_labels, labels_map, max_seq_length, return_mask=<span class="hljs-literal">True</span>
)
padded_valid_labels, valid_mask = get_padded_int_labels(
    valid_labels, labels_map, max_seq_length, return_mask=<span class="hljs-literal">True</span>
)
padded_test_labels, test_mask  = get_padded_int_labels(
    test_labels, labels_map, max_seq_length, return_mask=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">Finally, we will print the processed labels and masks of the first two sequences:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Print some labels IDs</span>
<span class="hljs-built_in">print</span>(padded_train_labels[:<span class="hljs-number">2</span>])
<span class="hljs-built_in">print</span>(train_mask[:<span class="hljs-number">2</span>])
</code></pre>
    <p class="normal">Which returns:</p>
    <pre class="programlisting con"><code class="hljs-con">[[0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
  1 1 1 1]
 [3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
  1 1 1 1]]
[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0]
 [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0]]
</code></pre>
    <p class="normal">You can see that the mask<a id="_idIndexMarker579"/> is indicating the true labels and padded ones clearly. Next, we will define some hyperparameters of the model.</p>
    <h2 id="_idParaDest-151" class="heading-2">Defining hyperparameters</h2>
    <p class="normal">Now let’s define<a id="_idIndexMarker580"/> several hyperparameters<a id="_idIndexMarker581"/> needed for our RNN, as shown here:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">max_seq_length</code> – Denotes the maximum length for a sequence. We infer this from our training data during data exploration. It is important to have a reasonable length for sequences, as otherwise, memory can explode, due to the unrolling of the RNN.</li>
      <li class="bulletList"><code class="inlineCode">emedding_size</code> – The dimensionality of token embeddings. Since we have a small corpus, a value &lt; 100 will suffice.</li>
      <li class="bulletList"><code class="inlineCode">rnn_hidden_size </code>– The dimensionality of hidden layers in the RNN. Increasing dimensionality of the hidden layer usually leads to better performance. However, note that increasing the size of the hidden layer causes all three sets of internal weights (that is, <em class="italic">U</em>, <em class="italic">W</em>, and <em class="italic">V</em>) to increase as well, thus resulting in a high computational footprint.</li>
      <li class="bulletList"><code class="inlineCode">n_classes</code> – Number of unique output classes present.</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code> – The batch size for training data, validation data, and test data. A higher batch size often leads to better results as we are seeing more data during each optimization step, but just like unrolling, this causes a higher memory requirement.</li>
      <li class="bulletList"><code class="inlineCode">epochs </code>– The number of epochs to train the model for.</li>
    </ul>
    <p class="normal">These are defined below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># The maximum length of sequences</span>
max_seq_length = <span class="hljs-number">40</span>
<span class="hljs-comment"># Size of token embeddings</span>
embedding_size = <span class="hljs-number">64</span>
<span class="hljs-comment"># Number of hidden units in the RNN layer</span>
rnn_hidden_size = <span class="hljs-number">64</span>
<span class="hljs-comment"># Number of output nodes in the last layer</span>
n_classes = <span class="hljs-number">9</span>
<span class="hljs-comment"># Number of samples in a batch</span>
batch_size = <span class="hljs-number">64</span>
<span class="hljs-comment"># Number of epochs to train</span>
epochs = <span class="hljs-number">3</span>
</code></pre>
    <p class="normal">Now we will define the model.</p>
    <h2 id="_idParaDest-152" class="heading-2">Defining the model</h2>
    <p class="normal">We will define the model<a id="_idIndexMarker582"/> here. Our model will have an embedding layer, followed by a simple RNN layer, and finally a dense prediction layer. One thing to note in the work we have done so far is that, unlike in previous chapters, we haven’t yet defined a <code class="inlineCode">Tokenizer</code> object. Although the <code class="inlineCode">Tokenizer</code> has been an important part of our NLP pipeline to convert each token (or word) into an ID, there’s a big downside to using an external tokenizer. After training the model, if you forget to save the tokenizer along with the model, your machine learning model becomes useless: to combat this, during inference, you would need to map each word to the exact ID it was mapped to during training. </p>
    <p class="normal">This is a significant risk the tokenizer poses. In this chapter, we will seek an alternative, where we will integrate the tokenization mechanism right into our model, so that we don’t need to worry about it later. <em class="italic">Figure 6.12</em> depicts the overall architecture of the model:</p>
    <figure class="mediaobject"><img src="../Images/B14070_06_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.12: Overall architecture of the model. The text vectorization layer tokenizes the text and converts it into word IDs. Next, each token is fed as an input at each timestep of the RNN. Finally, the RNN predicts a label for each token at every time step</p>
    <h3 id="_idParaDest-153" class="heading-3">Introduction to the TextVectorization layer</h3>
    <p class="normal">The <code class="inlineCode">TextVectorization</code> layer<a id="_idIndexMarker583"/> can be thought of as a modernized tokenizer that can be plugged into the model. Here, we will play around just with the <code class="inlineCode">TextVectorization</code> layer, without the overhead of the complexity from the rest of the model. First, we will import the <code class="inlineCode">TextVectorization</code> layer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers.experimental.preprocessing <span class="hljs-keyword">import</span> TextVectorization
</code></pre>
    <p class="normal">Now we will define a simple text corpus: </p>
    <pre class="programlisting code"><code class="hljs-code">toy_corpus = [<span class="hljs-string">"I went to the market on Sunday"</span>, <span class="hljs-string">"The Market was empty."</span>]
</code></pre>
    <p class="normal">We can instantiate a text vectorization layer as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">toy_vectorization_layer = TextVectorization()
</code></pre>
    <p class="normal">After instantiating, you need <a id="_idIndexMarker584"/>to fit this layer on some data. This way, just like the tokenizer we used previously, it can learn a word-to-numerical ID mapping. For this, we invoke the <code class="inlineCode">adapt()</code> method of the layer, by passing the corpus of text as an input:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Fit it on a corpus of data</span>
toy_vectorization_layer.adapt(toy_corpus)
</code></pre>
    <p class="normal">We can generate the tokenized output as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">toy_vectorized_output = toy_vectorization_layer(toy_corpus)
</code></pre>
    <p class="normal">Which will have:</p>
    <pre class="programlisting con"><code class="hljs-con">[[ 9  4  6  2  3  8  7]
 [ 2  3  5 10  0  0  0]]
</code></pre>
    <p class="normal">We can also see the vocabulary the layer has learned:</p>
    <pre class="programlisting con"><code class="hljs-con">Vocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty']
</code></pre>
    <p class="normal">We can see that the layer has done some pre-processing (e.g. turned words to lowercase and removed punctuation). Next let’s see how we can limit the size of the vocabulary. We can do this with the <code class="inlineCode">max_tokens</code> argument:</p>
    <pre class="programlisting code"><code class="hljs-code">toy_vectorization_layer = TextVectorization(max_tokens=<span class="hljs-number">5</span>)
toy_vectorization_layer.adapt(toy_corpus)
toy_vectorized_output = toy_vectorization_layer(toy_corpus)
</code></pre>
    <p class="normal">If you convert the <code class="inlineCode">toy_corpus</code> to word IDs, you will see:</p>
    <pre class="programlisting con"><code class="hljs-con">[[1 4 1 2 3 1 1]
 [2 3 1 1 0 0 0]]
</code></pre>
    <p class="normal">The vocabulary will be as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">Vocabulary: ['', '[UNK]', 'the', 'market', 'went']
</code></pre>
    <p class="normal">We can now see that there are only five elements in the vocabulary, just like we specified. Now if you need to skip the text pre-processing that happens within the layer, you can do so by setting the <code class="inlineCode">standardize</code> argument<a id="_idIndexMarker585"/> to <code class="inlineCode">None</code> in the layer:</p>
    <pre class="programlisting code"><code class="hljs-code">toy_vectorization_layer = TextVectorization(standardize=<span class="hljs-literal">None</span>)
toy_vectorization_layer.adapt(toy_corpus)
toy_vectorized_output = toy_vectorization_layer(toy_corpus)
</code></pre>
    <p class="normal">This will produce:</p>
    <pre class="programlisting con"><code class="hljs-con">[[12  2  4  5  7  6 10]
 [ 9 11  3  8  0  0  0]]
</code></pre>
    <p class="normal">The vocabulary will look as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">Vocabulary: ['', '[UNK]', 'went', 'was', 'to', 'the', 'on', 'market', 'empty.', 'The', 'Sunday', 'Market', 'I']
</code></pre>
    <p class="normal">Finally, we can also control the padding/truncation of sequences with the <code class="inlineCode">output_sequence_length</code> command. For example, the following command will pad/truncate sequences at length <code class="inlineCode">4:</code></p>
    <pre class="programlisting code"><code class="hljs-code">toy_vectorization_layer = TextVectorization(output_sequence_length=<span class="hljs-number">4</span>)
toy_vectorization_layer.adapt(toy_corpus)
toy_vectorized_output = toy_vectorization_layer(toy_corpus)
</code></pre>
    <p class="normal">This will produce:</p>
    <pre class="programlisting con"><code class="hljs-con">[[ 9  4  6  2]
 [ 2  3  5 10]]
</code></pre>
    <p class="normal">Here the vocabulary is: </p>
    <pre class="programlisting con"><code class="hljs-con">Vocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty']
</code></pre>
    <p class="normal">Now you have a good understanding of the arguments and what they do in the <code class="inlineCode">TextVectorization</code> layer. Let’s now discuss the model.</p>
    <h3 id="_idParaDest-154" class="heading-3">Defining the rest of the model</h3>
    <p class="normal">First we will import <a id="_idIndexMarker586"/>the necessary modules:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow.keras.layers <span class="hljs-keyword">as</span> layers
<span class="hljs-keyword">import</span> tensorflow.keras.backend <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">from</span> tensorflow.keras.layers.experimental.preprocessing <span class="hljs-keyword">import</span> TextVectorization
</code></pre>
    <p class="normal">We will define an input layer<a id="_idIndexMarker587"/> that has a single column (i.e. each sentence represented as a single unit) and has <code class="inlineCode">dtype=tf.string</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Input layer</span>
word_input = tf.keras.layers.Input(shape=(<span class="hljs-number">1</span>,), dtype=tf.string)
</code></pre>
    <p class="normal">Next, we will define a function that takes a corpus, a maximum sequence length, and a vocabulary size, and returns the trained <code class="inlineCode">TextVectorization</code> layer and the vocabulary size:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_fitted_token_vectorization_layer</span>(<span class="hljs-params">corpus, max_seq_length, vocabulary_size=</span><span class="hljs-literal">None</span>):
    <span class="hljs-string">""" Fit a TextVectorization layer on given data """</span>
    
    <span class="hljs-comment"># Define a text vectorization layer</span>
    vectorization_layer = TextVectorization(
        max_tokens=vocabulary_size, standardize=<span class="hljs-literal">None</span>,        
        output_sequence_length=max_seq_length, 
    )
    <span class="hljs-comment"># Fit it on a corpus of data</span>
    vectorization_layer.adapt(corpus)
    
    <span class="hljs-comment"># Get the vocabulary size</span>
    n_vocab = <span class="hljs-built_in">len</span>(vectorization_layer.get_vocabulary())
    <span class="hljs-keyword">return</span> vectorization_layer, n_vocab
</code></pre>
    <p class="normal">The function does what we have already described. However, pay attention to the various arguments we have set for the vectorization layer. We are passing the vocabulary size as <code class="inlineCode">max_tokens</code>; we are setting the <code class="inlineCode">standardize</code> to <code class="inlineCode">None</code>. This is an important setting. When performing NER, keeping the case of characters is very important. Typically, an entity starts with an uppercase letter (e.g. the name of a person or organization). Therefore, we should preserve the case in the text. </p>
    <p class="normal">Finally, we also set the <code class="inlineCode">output_sequence_length</code> to the sequence length we found during the analysis. With that, we create the text vectorization layer as follows: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Text vectorization layer</span>
vectorize_layer, n_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)
</code></pre>
    <p class="normal">Then pass the <code class="inlineCode">word_input</code> to the <code class="inlineCode">vectorize_layer</code> and get the output:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Vectorized output (each word mapped to an int ID)</span>
vectorized_out = vectorize_layer(word_input)
</code></pre>
    <p class="normal">The output from the <code class="inlineCode">vectorize_layer</code> (i.e <code class="inlineCode">vectorized_out</code>) will be sent to an embedding layer. This embedding<a id="_idIndexMarker588"/> layer is a randomly initialized embedding layer, which will have an output dimensionality of <code class="inlineCode">embedding_size</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Look up embeddings for the returned IDs</span>
embedding_layer = layers.Embedding(
    input_dim=n_vocab,
output_dim=embedding_size,
mask_zero=<span class="hljs-literal">True</span>
)(vectorized_out)
</code></pre>
    <p class="normal">Until now, we dealt with feed-forward networks. Outputs of feed-forward networks did not have a time dimension. But if you look at the output from the <code class="inlineCode">TextVectorization</code> layer, it will be a <code class="inlineCode">[batch size, sequence length]</code> - sized output. When this output goes through an embedding layer, the output would be a <code class="inlineCode">[batch size, sequence length, embedding size]</code>-shaped tensor. In other words, there is an additional time dimension included in the output of the embedding layer. </p>
    <p class="normal">Another difference is the introduction of the <code class="inlineCode">mask_true</code> argument. Masking is used to mask uninformative words added to sequences (e.g. the padding token added to make sentences a fixed length), as they do not contribute to the final outcome. Masking is a commonly used technique in sequence learning. To learn more about masking, please read the information box below.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Masking in sequence learning</strong></p>
      <p class="normal">Naturally, text has arbitrary<a id="_idIndexMarker589"/> lengths. For example, sentences in a corpus would have a wide variety of token lengths. But deep networks process tensors with fixed dimensions. To bring arbitrary-length sentences to constant length, we pad these sequences with some special value (e.g. 0). However, these padded values are synthetic, and only serve as a way to ensure the correct input shape. They should not contribute to the final loss or evaluation metrics. To ignore them during loss calculation and evaluation, “masking” is used. The idea is to multiply the loss resulting from padded timesteps with a zero, essentially cutting them off from the final loss.</p>
    </div>
    <p class="normal">It would be cumbersome to manually perform masking when training a model. But in TensorFlow, most layers support masking. For example, in the embedding layer, to ignore padded values (which will be zeros), all you need to do is set <code class="inlineCode">mask_true=True</code>. </p>
    <p class="normal">When you enable masking<a id="_idIndexMarker590"/> in a layer, it will propagate the mask to the downstream layers, flowing down until the loss computations. In other words, you only need to enable masking at the start of the model (as we have done at the embedding layer) and the rest is taken care of by TensorFlow. </p>
    <p class="normal">Following this, we will define the core layer of our model, the RNN:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define a simple RNN layer, it returns an output at each position</span>
rnn_layer = layers.SimpleRNN(
    units=rnn_hidden_size, return_sequences=<span class="hljs-literal">True</span>
)
rnn_out = rnn_layer(embedding_layer)
</code></pre>
    <p class="normal">You can implement a vanilla RNN by simply calling <code class="inlineCode">tf.keras.layers.SimpleRNN</code>. Here we pass two important arguments. There are other useful arguments besides the two discussed here, however, they will be covered in later chapters with more complex variants of RNNs:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">units</code> (<code class="inlineCode">int</code>) – This defines the hidden output size of the RNN model. The larger this is, the more representational power the model will have.</li>
      <li class="bulletList"><code class="inlineCode">return_sequences</code> (<code class="inlineCode">bool</code>) – Whether to return outputs from all the timesteps, or to return only the last output. For NER tasks, we need to label every single token. Therefore we need to return outputs for all the time steps.</li>
    </ul>
    <p class="normal">The <code class="inlineCode">rnn_layer</code> takes a <code class="inlineCode">[batch size, sequence length, embedding size]</code>-sized tensor and returns a <code class="inlineCode">[batch size, sequence length, rnn hidden size]</code>-sized tensor. Finally, the time-distributed output from the RNN will go to a Dense layer with <code class="inlineCode">n_classes</code> output nodes and a <code class="inlineCode">softmax</code> activation:</p>
    <pre class="programlisting code"><code class="hljs-code">dense_layer = layers.Dense(n_classes, activation=<span class="hljs-string">'softmax'</span>)
dense_out = dense_layer(rnn_out)
</code></pre>
    <p class="normal">Finally, we can define the final model as follows. It takes a batch of string sentences as the input, and returns a batch of sequences of labels as the output:</p>
    <pre class="programlisting code"><code class="hljs-code">model = tf.keras.Model(inputs=word_input, outputs=dense_out)
</code></pre>
    <p class="normal">We have now finished<a id="_idIndexMarker591"/> building the model. Next, we will discuss the loss function and the evaluation metrics.</p>
    <h2 id="_idParaDest-155" class="heading-2">Evaluation metrics and the loss function</h2>
    <p class="normal">During our previous discussion, we alluded<a id="_idIndexMarker592"/> to the fact that NER tasks carry<a id="_idIndexMarker593"/> a high class imbalance. It is quite normal for text to have more non-entity-related tokens than entity-related tokens. This leads to large amounts of other (<code class="inlineCode">O</code>) type labels and fewer of the remaining types. We need to take this into consideration when training the model and evaluating the model. We will address the class imbalance in two ways:</p>
    <ul>
      <li class="bulletList">We will create a new evaluation metric that is resilient to class imbalance</li>
      <li class="bulletList">We will use sample weights to penalize more frequent classes and boost the importance of rare classes</li>
    </ul>
    <p class="normal">In this section, we will only address the former. The latter will be addressed in the next section. We will define a modified version of the accuracy. This is called a macro-averaged accuracy. In macro averaging, we compute<a id="_idIndexMarker594"/> accuracies for each class separately, and then average it. Therefore, the class imbalance is ignored when computing the accuracy. When computing standard metrics like accuracy precision or recall, there are different types of averaging available. To learn more about these, read the information box below.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Different types of metric averaging</strong></p>
      <p class="normal">There are different types<a id="_idIndexMarker595"/> of averaging available for metrics. You can read one such example of these averaging available in scikit-learn explained at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html "><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html</span></a>. Consider a simple binary classification example with the following confusion matrix results:</p>
      <figure class="mediaobject"><img src="../Images/B14070_06_13.png" alt=""/></figure>
      <p class="packt_figref">Figure 6.13: Example confusion matrix results</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">micro</strong> – Computes a global metric, ignoring<a id="_idIndexMarker596"/> the differences in class distribution. e.g. 35/65 = ~54%</li>
        <li class="bulletList"><strong class="keyWord">macro</strong> – Computes the metric<a id="_idIndexMarker597"/> for each class separately and computes the mean. e.g. (35/40 + 0/25)/2 = ~43.7%</li>
        <li class="bulletList"><strong class="keyWord">weighted</strong> – Computes the metric<a id="_idIndexMarker598"/> for each class separately and weighs it by support (i.e. number of true labels for each class). e.g. (35/40)* 40 + (0/25) * 25 / 65 = ~54%</li>
      </ul>
      <p class="normal">Here you can see the micro and weighted return the same result. This is because the denominator of the accuracy computation is the same as the support. Therefore, they cancel out in the weighted averaging. However for other metrics such as precision and recall you will get different values.</p>
    </div>
    <p class="normal">Below we define the function<a id="_idIndexMarker599"/> to compute macro accuracy <a id="_idIndexMarker600"/>using a batch of true targets (<code class="inlineCode">y_true</code>) and predictions (<code class="inlineCode">y_pred</code>). <code class="inlineCode">y_true</code> will have the shape <code class="inlineCode">[batch_size, sequence length]</code> and <code class="inlineCode">y_pred</code> will have the shape <code class="inlineCode">[batch size, sequence length, n_classes]</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">macro_accuracy</span>(<span class="hljs-params">y_true, y_pred</span>):
    
    <span class="hljs-comment"># [batch size, time] =&gt; [batch size * time]</span>
    y_true = tf.cast(tf.reshape(y_true, [-<span class="hljs-number">1</span>]), <span class="hljs-string">'int32'</span>)
    <span class="hljs-comment"># [batch size, sequence length, n_classes] =&gt; [batch size * time]</span>
    y_pred = tf.cast(tf.reshape(tf.argmax(y_pred, axis=-<span class="hljs-number">1</span>), [-<span class="hljs-number">1</span>]), 
    <span class="hljs-string">'int32'</span>)
    
    sorted_y_true = tf.sort(y_true)
    sorted_inds = tf.argsort(y_true)
    
    sorted_y_pred = tf.gather(y_pred, sorted_inds)
    
    sorted_correct = tf.cast(tf.math.equal(sorted_y_true, 
    sorted_y_pred), <span class="hljs-string">'int32'</span>)
    
    <span class="hljs-comment"># We are adding one to make sure there are no division by zero</span>
    correct_for_each_label = 
    tf.cast(tf.math.segment_sum(sorted_correct, sorted_y_true), 
    <span class="hljs-string">'float32'</span>) + <span class="hljs-number">1</span>
    all_for_each_label = 
    tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true), 
    sorted_y_true), <span class="hljs-string">'float32'</span>) + <span class="hljs-number">1</span>
    
    mean_accuracy = 
    tf.reduce_mean(correct_for_each_label/all_for_each_label)
    
    <span class="hljs-keyword">return</span> mean_accuracy
</code></pre>
    <p class="normal">It is important to note<a id="_idIndexMarker601"/> that we have to write this function<a id="_idIndexMarker602"/> using TensorFlow operations, so that they are executed as a graph. Even though TensorFlow 2 has migrated toward more imperative style execution operations, there still are remnants of the declarative style introduced by TensorFlow 1.</p>
    <p class="normal">First we flatten <code class="inlineCode">y_true</code> so that it’s a vector. Next we get the predicted label from <code class="inlineCode">y_pred</code> using the <code class="inlineCode">tf.argmax()</code> function and flatten the predicted labels to a vector. The two flattened structures will have the same number of elements. Then we sort <code class="inlineCode">y_true</code>, so that same-labeled elements are close together. </p>
    <p class="normal">We take the indices of the original data after sorting and then use the <code class="inlineCode">tf.gather()</code> function to order <code class="inlineCode">y_pred</code> in the same order as <code class="inlineCode">y_true</code>. In other words, <code class="inlineCode">sorted_y_true</code> and <code class="inlineCode">sorted_y_pred</code> still have the same correspondence with each other. The <code class="inlineCode">tf.gather()</code> function takes a tensor and a set of indices and orders the passed tensor<a id="_idIndexMarker603"/> in the order of the indices. For more information about <code class="inlineCode">tf.gather()</code> refer to <a href="https://www.tensorflow.org/api_docs/python/tf/gather"><span class="url">https://www.tensorflow.org/api_docs/python/tf/gather</span></a>.</p>
    <p class="normal">Then we compute <code class="inlineCode">sorted_correct</code>, which is a simple indicator function that switches on if the corresponding element in <code class="inlineCode">sorted_y_true</code> and <code class="inlineCode">sorted_y_pred</code> are the same, and if not stays off. Then we use the <code class="inlineCode">tf.math.segment_sum()</code> function to compute a segmented sum of correctly predicted samples. Samples belonging to each class are considered a single segment (<code class="inlineCode">correct_for_each_label</code>). The <code class="inlineCode">segment_sum() </code>function takes two arguments: <code class="inlineCode">data</code> and <code class="inlineCode">segment_ids</code>. For example, if the <code class="inlineCode">data</code> is <code class="inlineCode">[0, 1, 2, 3, 4, 5, 6, 7]</code> and <code class="inlineCode">segment_ids</code> are <code class="inlineCode">[0, 0, 0, 1, 1, 2, 3, 3]</code>, then the segment sum would be <code class="inlineCode">[0+1+2, 3+4, 5, 6+7] = [3, 7, 5, 13]</code>.</p>
    <p class="normal">Then we do the same for a vector of 1s. In this case, we get the number of true samples present for each class in the batch of data (<code class="inlineCode">all_for_each_label</code>). Note that we are adding a 1 at the end. This is to avoid division by 0 in the next step. Finally, we divide <code class="inlineCode">correct_for_each_label</code> by <code class="inlineCode">all_for_each_label</code>, which gives us a vector containing the accuracy of each class. With that we compute the mean accuracy, which is the macro-averaged accuracy.</p>
    <p class="normal">Finally we wrap this function in a <code class="inlineCode">MeanMetricWrapper</code> that will produce a <code class="inlineCode">tf.keras.metrics.Metric</code> object that we can pass to the <code class="inlineCode">model.compile()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name=<span class="hljs-string">'macro_accuracy'</span>)
</code></pre>
    <p class="normal">Compile<a id="_idIndexMarker604"/> the model<a id="_idIndexMarker605"/> by calling:</p>
    <pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>, metrics=[mean_accuracy_metric])
</code></pre>
    <p class="normal">Next, we will train the model with the data prepared.</p>
    <h2 id="_idParaDest-156" class="heading-2">Training and evaluating RNN on NER task</h2>
    <p class="normal">Let’s train our model<a id="_idIndexMarker606"/> on the data we have prepared. But first, we need to define a function<a id="_idIndexMarker607"/> to tackle the class imbalance in our dataset. We will pass sample weights to the <code class="inlineCode">model.fit()</code> function. To compute sample weights, we will first define a function called <code class="inlineCode">get_class_weights()</code> that computes <code class="inlineCode">class_weights</code> for each class. Next we will pass the class weights to another function, <code class="inlineCode">get_sample_weights_from_class_weights()</code>, which will generate sample weights:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_class_weights</span>(<span class="hljs-params">train_labels</span>):
    
    label_count_ser = pd.Series(chain(*train_labels)).value_counts()
    label_count_ser = label_count_ser.<span class="hljs-built_in">min</span>()/label_count_ser
    
    label_id_map = get_label_id_map(train_labels)
    label_count_ser.index = label_count_ser.index.<span class="hljs-built_in">map</span>(label_id_map)
    <span class="hljs-keyword">return</span> label_count_ser.to_dict()
</code></pre>
    <p class="normal">The first function, <code class="inlineCode">get_class_weights()</code>, takes a <code class="inlineCode">train_labels</code> (a list of list of class IDs). Then we create a pandas <code class="inlineCode">Series</code> object with <code class="inlineCode">train_labels</code>. Note that we are using a function called <code class="inlineCode">chain</code> from the built-in <code class="inlineCode">itertools</code> library, which will flatten <code class="inlineCode">train_labels</code> to a list of class IDs. The <code class="inlineCode">Series</code> object contains frequency counts of each class label that appears in the train dataset. Next to compute weights, we divide the minimum frequency element-wise from other frequencies. In other words, if the frequency for class label <img src="../Images/B14070_06_054.png" alt="" style="height: 1.05em !important; vertical-align: -0.13em !important;"/> is denoted by <img src="../Images/B14070_06_055.png" alt="" style="height: 1.25em !important; vertical-align: -0.22em !important;"/>, and the total label set is denoted by <img src="../Images/B14070_06_056.png" alt="" style="height: 1.05em !important; vertical-align: -0.17em !important;"/>, the weight for class <img src="../Images/B14070_06_054.png" alt="" style="height: 1.05em !important; vertical-align: -0.13em !important;"/> is computed as:</p>
    <p class="center"><img src="../Images/B14070_06_058.png" alt="" style="height: 2.60em !important;"/></p>
    <p class="normal">Finally, the output<a id="_idIndexMarker608"/> is converted into a dictionary that has class IDs as keys and<a id="_idIndexMarker609"/> class weights as values. Next we need to convert the <code class="inlineCode">class_weights</code> to <code class="inlineCode">sample_weights</code>. We simply perform a dictionary lookup element-wise on each label to generate a sample weight from <code class="inlineCode">class_weights</code>. The <code class="inlineCode">sample_weights</code> will be the same shape as the <code class="inlineCode">train_labels</code> as there’s one weight for each sample:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_sample_weights_from_class_weights</span>(<span class="hljs-params">labels, class_weights</span>):
    <span class="hljs-string">""" From the class weights generate sample weights """</span>
    <span class="hljs-keyword">return</span> np.vectorize(class_weights.get)(labels)
</code></pre>
    <p class="normal">We can use NumPy’s <code class="inlineCode">np.vectorize()</code> function to achieve this. <code class="inlineCode">np.vectorize()</code> takes in a function (e.g. <code class="inlineCode">class_weights.get()</code> is the key lookup function provided by Python) and applies that on all elements, which gives us the sample weights. Call the functions we defined above to generate the actual weights:</p>
    <pre class="programlisting code"><code class="hljs-code">train_class_weights = get_class_weights(train_labels)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Class weights: {}"</span>.<span class="hljs-built_in">format</span>(train_class_weights))
<span class="hljs-comment"># Get sample weights (we cannot use class_weight with TextVectorization</span>
<span class="hljs-comment"># layer)</span>
train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)
</code></pre>
    <p class="normal">After we have the sample weights at our disposal, we can train our model. You can view the <code class="inlineCode">class_weights</code> by printing them out. This will give:</p>
    <pre class="programlisting con"><code class="hljs-con">labels_map: {
    'B-ORG': 0, 
    'O': 1, 
    'B-MISC': 2, 
    'B-PER': 3, 
    'I-PER': 4, 
    'B-LOC': 5, 
    'I-ORG': 6, 
    'I-MISC': 7, 
    'I-LOC': 8
}
Class weights: {
    1: 0.006811025015037328, 
    5: 0.16176470588235295, 
    3: 0.17500000000000002, 
    0: 0.18272425249169436, 
    4: 0.25507950530035334, 
    6: 0.31182505399568033, 
    2: 0.33595113438045376, 
    8: 0.9982713915298186, 
    7: 1.0
}
</code></pre>
    <p class="normal">You can see the class <code class="inlineCode">Other</code> has<a id="_idIndexMarker610"/> the lowest weight (because it’s the most frequent), and the<a id="_idIndexMarker611"/> class <code class="inlineCode">I-MISC</code> has the highest as it’s the least frequent. Now we will train our model using the prepared data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Make train_sequences an array</span>
train_sentences = np.array(train_sentences)
<span class="hljs-comment"># Training the model</span>
model.fit(
        train_sentences, padded_train_labels, 
        sample_weight=train_sample_weights,
        batch_size=batch_size,
        epochs=epochs, 
        validation_data=(np.array(valid_sentences), 
        padded_valid_labels)
)
</code></pre>
    <p class="normal">You should get an accuracy of around 78-79% without any special performance optimization tricks. Next you can evaluate the model on test data with:</p>
    <pre class="programlisting code"><code class="hljs-code">model.evaluate(np.array(test_sentences), padded_test_labels)
</code></pre>
    <p class="normal">This will give a test accuracy<a id="_idIndexMarker612"/> of around 77%. Since the validation accuracy and test accuracy<a id="_idIndexMarker613"/> are on par, we can say that the model has generalized well. But to make sure, let’s visually inspect a few samples from the test set.</p>
    <h2 id="_idParaDest-157" class="heading-2">Visually analyzing outputs</h2>
    <p class="normal">To analyze the output, we will<a id="_idIndexMarker614"/> use the first five sentences in the test set:</p>
    <pre class="programlisting code"><code class="hljs-code">n_samples = <span class="hljs-number">5</span>
visual_test_sentences = test_sentences[:n_samples]
visual_test_labels = padded_test_labels[:n_samples]
</code></pre>
    <p class="normal">Next predict using the model and convert those predictions to predicted class IDs:</p>
    <pre class="programlisting code"><code class="hljs-code">visual_test_predictions = model.predict(np.array(visual_test_sentences))
visual_test_pred_labels = np.argmax(visual_test_predictions, axis=-<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">We will create a reversed <code class="inlineCode">labels_map</code> that has a mapping from label ID to label string:</p>
    <pre class="programlisting code"><code class="hljs-code">rev_labels_map = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(labels_map.values(), labels_map.keys()))
</code></pre>
    <p class="normal">Finally, we will print out the results:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i, (sentence, sent_labels, sent_preds) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(visual_test_sentences, visual_test_labels, visual_test_pred_labels)):    
    n_tokens = <span class="hljs-built_in">len</span>(sentence.split())
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Sample:\t"</span>,<span class="hljs-string">"\t"</span>.join(sentence.split()))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"True:\t"</span>,<span class="hljs-string">"\t"</span>.join([rev_labels_map[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> 
<span class="hljs-built_in">    </span>sent_labels[:n_tokens]]))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Pred:\t"</span>,<span class="hljs-string">"\t"</span>.join([rev_labels_map[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> 
<span class="hljs-built_in">    </span>sent_preds[:n_tokens]]))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"\n"</span>)
</code></pre>
    <p class="normal">This will print out:</p>
    <pre class="programlisting con"><code class="hljs-con">Sample:     SOCCER    -    JAPAN    GET    LUCKY    WIN    ,    CHINA    IN    SURPRISE    DEFEAT    .
True:         O    O    B-LOC    O    O    O    O    B-LOC    O    O    O    O
Pred:         O    O    B-MISC    O    O    O    O    B-PER    O    B-LOC    O    O
Sample:     Nadim    Ladki
True:         B-PER    I-PER
Pred:         B-LOC    O
Sample:     AL-AIN    ,    United    Arab    Emirates    1996-12-06
True:         B-LOC    O    B-LOC    I-LOC    I-LOC    O
Pred:         B-LOC    O    B-LOC    I-LOC    I-LOC    I-ORG
Sample:     Japan    began    the    defence    of    their    Asian    Cup    title    with    a    lucky    2-1    win    against    Syria    in    a    Group    C    championship    match    on    Friday    .
True:         B-LOC    O    O    O    O    O    B-MISC    I-MISC    O    O    O    O    O    O    O    B-LOC    O    O    O    O    O    O    O    O    O
Pred:         B-LOC    I-LOC    O    O    O    O    B-MISC    I-MISC    I-MISC    O    O    O    O    O    O    B-LOC    O    O    O    O    O    O    O    O    O
</code></pre>
    <p class="normal">It can be seen that our model is doing a decent job. It is good at identifying locations but is struggling at identifying<a id="_idIndexMarker615"/> the names of people. Here we end our discussion about the basic RNN solution that performs NER. In the next section, we will make the model more complex, giving it the ability to understand text better by providing more fine-grained details. Let’s understand how we can improve our model.</p>
    <h1 id="_idParaDest-158" class="heading-1">NER with character and token embeddings</h1>
    <p class="normal">Nowadays, recurrent<a id="_idIndexMarker616"/> models used<a id="_idIndexMarker617"/> to solve<a id="_idIndexMarker618"/> the NER task are much more<a id="_idIndexMarker619"/> sophisticated than having just a single<a id="_idIndexMarker620"/> embedding layer and an RNN model. They involve <a id="_idIndexMarker621"/>using more advanced recurrent models like <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>), <strong class="keyWord">Gated Recurrent Units</strong> (<strong class="keyWord">GRUs</strong>), etc. We will set aside the discussion about these advanced models for several upcoming<a id="_idIndexMarker622"/> chapters. Here we will focus<a id="_idIndexMarker623"/> our discussion on a technique<a id="_idIndexMarker624"/> that provides the model embeddings at multiple scales, enabling<a id="_idIndexMarker625"/> it to understand language better. That is, instead of relying only on token embeddings, also use character embeddings. Then a token embedding is generated with the character embeddings by shifting a convolutional window over the characters in the token. Don’t worry if you don’t understand the details yet. The following sections will go into specific details of the solution. This exercise is available in <code class="inlineCode">ch06_rnns_for_named_entity_recognition.ipynb</code> in the <code class="inlineCode">Ch06-Recurrent-Neural-Networks</code> folder.</p>
    <h2 id="_idParaDest-159" class="heading-2">Using convolution to generate token embeddings</h2>
    <p class="normal">A combination of character embeddings<a id="_idIndexMarker626"/> and a convolutional kernel<a id="_idIndexMarker627"/> can be used to generate token embeddings (<em class="italic">Figure 6.14</em>). The method will be as follows:</p>
    <ul>
      <li class="bulletList">Pad each token (e.g. word) to a predefined length</li>
      <li class="bulletList">Look up the character embeddings for the characters in the token from an embedding layer</li>
      <li class="bulletList">Shift a convolutional kernel over the sequence of character embeddings to generate a token embedding</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B14070_06_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.14: How token embeddings are generated using character embeddings and the convolution operation</p>
    <p class="normal">The very first thing we need<a id="_idIndexMarker628"/> to do is analyze the statistics<a id="_idIndexMarker629"/> around how many characters there are for a token in our corpus. Similar to how we did it previously, we can do this with pandas:</p>
    <pre class="programlisting code"><code class="hljs-code">vocab_ser = pd.Series(
    pd.Series(train_sentences).<span class="hljs-built_in">str</span>.split().explode().unique()
)
vocab_ser.<span class="hljs-built_in">str</span>.<span class="hljs-built_in">len</span>().describe(percentiles=[<span class="hljs-number">0.05</span>, <span class="hljs-number">0.95</span>])
</code></pre>
    <p class="normal">In computing <code class="inlineCode">vocab_ser</code>, the first part (i.e. <code class="inlineCode">pd.Series(train_sentences).str.split()</code>) will result in a pandas <code class="inlineCode">Series</code>, whose elements are a list of tokens (each token in the sentence is an item of that list). Next, <code class="inlineCode">explode()</code> will convert the <code class="inlineCode">Series</code> of a list of tokens into a <code class="inlineCode">Series</code> of tokens, by converting each token into a separate item in the <code class="inlineCode">Series</code>. Finally we take only the unique tokens in that <code class="inlineCode">Series</code>. Here we end up with a pandas <code class="inlineCode">Series</code> where each item is a unique token.</p>
    <p class="normal">We will now use the <code class="inlineCode">str.len() </code>function to get the length of each token (i.e. the number of characters) and look at the 95% percentile in that. We will get the following:</p>
    <pre class="programlisting con"><code class="hljs-con">count    23623.000000
mean         6.832705
std          2.749288
min          1.000000
5%           3.000000
50%          7.000000
95%         12.000000
max         61.000000
dtype: float64
</code></pre>
    <p class="normal">We can see around 95% of our words<a id="_idIndexMarker630"/> have less than or equal to 12 characters. Next, we will<a id="_idIndexMarker631"/> write a function to pad shorter tokens:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_corpus_for_char_embeddings</span>(<span class="hljs-params">tokenized_sentences, max_seq_length</span>):
    <span class="hljs-string">""" Pads each sequence to a maximum length """</span>
    proc_sentences = []
    <span class="hljs-keyword">for</span> tokens <span class="hljs-keyword">in</span> tokenized_sentences:
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) &gt;= max_seq_length:
            proc_sentences.append([[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> 
            tokens[:max_seq_length]])
        <span class="hljs-keyword">else</span>:
            proc_sentences.append([[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> 
            tokens+[<span class="hljs-string">''</span>]*(max_seq_length-<span class="hljs-built_in">len</span>(tokens))])
            
    <span class="hljs-keyword">return</span> proc_sentences
</code></pre>
    <p class="normal">The function takes a set of tokenized sentences (i.e. each sentence as a list of tokens, not a string) and a maximum sequence length. Note that this is the maximum sequence length we used previously, not the new token length we discussed. This function would then do the following:</p>
    <ul>
      <li class="bulletList">For longer sentences, only return the <code class="inlineCode">max_seq_length</code> tokens</li>
      <li class="bulletList">For shorter sentences, append ‘‘ as a token until <code class="inlineCode">max_seq_length</code> is reached</li>
    </ul>
    <p class="normal">Let’s run this function on a small toy dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define sample data</span>
data = [<span class="hljs-string">'aaaa bb c'</span>, <span class="hljs-string">'d eee'</span>]
<span class="hljs-comment"># Pad sequences</span>
tokenized_sentences = prepare_corpus_for_char_embeddings([d.split() <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data], <span class="hljs-number">3</span>)
</code></pre>
    <p class="normal">This will return:</p>
    <pre class="programlisting con"><code class="hljs-con">Padded sequence: [[['aaaa'], ['bb'], ['c']], [['d'], ['eee'], ['']]]
</code></pre>
    <p class="normal">We will now define a new <code class="inlineCode">TextVectorization</code> layer that can cope with the changes we introduced to the data. Instead<a id="_idIndexMarker632"/> of tokenizing on the token level, the new <code class="inlineCode">TextVectorization</code> layer must tokenize<a id="_idIndexMarker633"/> on the character level. For this we need to make a few changes. We will again write a function to contain this vectorization layer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_fitted_char_vectorization_layer</span>(<span class="hljs-params">corpus, max_seq_length, max_token_length, vocabulary_size=</span><span class="hljs-literal">None</span>):
    <span class="hljs-string">""" Fit a TextVectorization layer on given data """</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">_split_char</span>(<span class="hljs-params">token</span>):
        <span class="hljs-keyword">return</span> tf.strings.bytes_split(token)
    <span class="hljs-comment"># Define a text vectorization layer</span>
    vectorization_layer = TextVectorization(
        standardize=<span class="hljs-literal">None</span>,      
        split=_split_char,
        output_sequence_length=max_token_length, 
    )
    tokenized_sentences = [sent.split() <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> corpus]
    padded_tokenized_sentences = 
    prepare_corpus_for_char_embeddings(tokenized_sentences, 
    max_seq_length)
    
    <span class="hljs-comment"># Fit it on a corpus of data</span>
    vectorization_layer.adapt(padded_tokenized_sentences)
    
    <span class="hljs-comment"># Get the vocabulary size</span>
    n_vocab = <span class="hljs-built_in">len</span>(vectorization_layer.get_vocabulary())
    <span class="hljs-keyword">return</span> vectorization_layer, n_vocab
</code></pre>
    <p class="normal">We first define a function called <code class="inlineCode">_split_char()</code> that takes a token (as a <code class="inlineCode">tf.Tensor</code>) and returns a char-tokenized tensor. For example, <code class="inlineCode">_split_char(tf.constant(['abcd']))</code> would return <code class="inlineCode">&lt;tf.RaggedTensor [[b'a', b'b', b'c', b'd']]&gt;</code>. Then we define a <code class="inlineCode">TextVectorization</code> layer that will use this newly defined function as the way to split the data it gets. We will also define <code class="inlineCode">output_sequence_length</code> as <code class="inlineCode">max_token_length</code>. Then we create <code class="inlineCode">tokenized_sentences</code>, a list of list of strings, and pad it using the <code class="inlineCode">prepare_corpus_for_char_embeddings()</code> function we defined earlier. Finally we use the <code class="inlineCode">TextVectorization</code> layer’s <code class="inlineCode">adapt()</code> function to fit it with the data we prepared. Two key differences between<a id="_idIndexMarker634"/> the previous token-based text vectorizer<a id="_idIndexMarker635"/> and this char-based text vectorizer are in the input dimensions and the final output dimensions:</p>
    <ul>
      <li class="bulletList">Token-based vectorizer – Takes in a <code class="inlineCode">[batch size, 1]</code>-sized input and produces a <code class="inlineCode">[batch size, sequence length]</code>-sized output</li>
      <li class="bulletList">Char-based vectorizer – Takes in a <code class="inlineCode">[batch size, sequence length, 1]</code>-sized input and produces a <code class="inlineCode">[batch size, sequence length, token length]</code>-sized output</li>
    </ul>
    <p class="normal">Now we are equipped with the ingredients to implement our new and improved NER classifier.</p>
    <h2 id="_idParaDest-160" class="heading-2">Implementing the new NER model</h2>
    <p class="normal">With a good conceptual understanding<a id="_idIndexMarker636"/> of the model, let’s implement the new NER model. We will first define some hyperparameters, followed by defining a text vectorizer as before. However, our <code class="inlineCode">TextVectorization</code> will be more complex in this section, as we have several different levels of tokenization taking place (e.g. char-level and token-level). Finally we define the RNN-based model that produces the output.</p>
    <h3 id="_idParaDest-161" class="heading-3">Defining hyperparameters</h3>
    <p class="normal">First, we will define the two hyperparameters<a id="_idIndexMarker637"/> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">max_seq_length = <span class="hljs-number">40</span>
max_token_length = <span class="hljs-number">12</span>
</code></pre>
    <h3 id="_idParaDest-162" class="heading-3">Defining the input layer</h3>
    <p class="normal">We then define an input layer<a id="_idIndexMarker638"/> with the data type <code class="inlineCode">tf.strings</code> as before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Input layer (tokens)</span>
word_input = tf.keras.layers.Input(shape=(<span class="hljs-number">1</span>,), dtype=tf.string)
</code></pre>
    <p class="normal">The inputs to this layer<a id="_idIndexMarker639"/> would be a batch of sentences, where each sentence is a string. </p>
    <h3 id="_idParaDest-163" class="heading-3">Defining the token-based TextVectorization layer</h3>
    <p class="normal">Then we define<a id="_idIndexMarker640"/> the token-level <code class="inlineCode">TextVectorization</code> layer just like<a id="_idIndexMarker641"/> we did above:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Text vectorize layer (token)</span>
token_vectorize_layer, n_token_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)
<span class="hljs-comment"># Vectorized output (each word mapped to an int ID)</span>
token_vectorized_out = token_vectorize_layer(word_input)
</code></pre>
    <h3 id="_idParaDest-164" class="heading-3">Defining the character-based TextVectorization layer</h3>
    <p class="normal">For the character-level<a id="_idIndexMarker642"/> vectorization layer<a id="_idIndexMarker643"/> we will employ the <code class="inlineCode">get_fitted_char_vectorization_layer()</code> function we defined above:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Text vectorize layer (char)</span>
char_vectorize_layer, n_char_vocab = get_fitted_char_vectorization_layer(train_sentences, max_seq_length, max_token_length)
</code></pre>
    <p class="normal">Next, we will discuss the inputs for this layer.</p>
    <h3 id="_idParaDest-165" class="heading-3">Processing the inputs for the char_vectorize_layer</h3>
    <p class="normal">We will use the same <code class="inlineCode">word_input</code> for this new<a id="_idIndexMarker644"/> vectorization layer as well. However, using <a id="_idIndexMarker645"/>the same input means we need to introduce some interim pre-processing to get the input to the correct format intended for this layer. Remember that the input to this layer needs to be a <code class="inlineCode">[batch size, sequence length, 1]</code>-sized tensor. </p>
    <p class="normal">This means the sentences need to be tokenized to a list of tokens. For that we will use the <code class="inlineCode">tf.keras.layers.Lambda()</code> layer and the <code class="inlineCode">tf.strings.split() </code>function:</p>
    <pre class="programlisting code"><code class="hljs-code">tokenized_word_input = layers.Lambda(
    <span class="hljs-keyword">lambda</span> x: tf.strings.split(x).to_tensor(default_value=<span class="hljs-string">''</span>, 
<span class="hljs-keyword">    </span>shape=[<span class="hljs-literal">None</span>, max_seq_length, <span class="hljs-number">1</span>])
)(word_input)
char_vectorized_out = char_vectorize_layer(tokenized_word_input)
</code></pre>
    <p class="normal">The <code class="inlineCode">Lambda</code> layer<a id="_idIndexMarker646"/> is used as a way to create a layer from a custom TensorFlow/Keras<a id="_idIndexMarker647"/> function, which may not be available as a standard layer in Keras. Here we are using a <code class="inlineCode">Lambda</code> layer to define a layer that will tokenize a passed input to a list of tokens. Furthermore, the <code class="inlineCode">tf.strings.split()</code> function returns a ragged tensor. In a typical tensor, all the dimensions need to have a constant size. A ragged tensor is a special tensor whose dimensions are not fixed. For example, since a list of sentences is highly unlikely to have the same number of tokens, this results in a ragged tensor. But TensorFlow will complain if you try to go forward with a <code class="inlineCode">tf.RaggedTensor</code> as most layers do not support these tensors. Therefore, we need to convert this to a standard tensor using the <code class="inlineCode">to_tensor()</code> function. We can pass a shape to this function and it will make sure the shape of the resulting tensor will be the defined shape (by means of padding and truncations). </p>
    <p class="normal">A key thing to pay attention to is how the shapes of the input-output tensors are transformed at each layer. For example, we started off with a <code class="inlineCode">[batch size, 1]</code>-sized tensor that went into the <code class="inlineCode">Lambda</code> layer to be transformed to a <code class="inlineCode">[batch size, sequence length, 1]</code>-sized layer. Finally, the <code class="inlineCode">char_vectorize_layer</code> transforms this into a <code class="inlineCode">[batch size, sequence length, token length]</code>-sized tensor.</p>
    <p class="normal">We will then define an embedding layer, with which we will look up embeddings for the resulting char IDs coming from the <code class="inlineCode">char_vectorize_layer</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Produces a [batch size, seq length, token_length, emb size]</span>
char_embedding_layer = layers.Embedding(input_dim=n_char_vocab, output_dim=<span class="hljs-number">32</span>, mask_zero=<span class="hljs-literal">True</span>)(char_vectorized_out)
</code></pre>
    <p class="normal">This layer produces a <code class="inlineCode">[batch size, sequence length, token length, 32]</code>-sized tensor, with a char embedding vector for each character in the tensor. Now it’s time to perform convolution on top of this output.</p>
    <h3 id="_idParaDest-166" class="heading-3">Performing convolution on the character embeddings</h3>
    <p class="normal">We will define<a id="_idIndexMarker648"/> a 1D convolution layer <a id="_idIndexMarker649"/>with a kernel size of 5 (i.e. convolutional window size), a stride of 1, <code class="inlineCode">'same'</code> padding, and a ReLU activation. We then feed the output from the previous section to this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># A 1D convolutional layer that will generate token embeddings by shifting # a convolutional kernel over the sequence of chars in each token (padded)</span>
char_token_output = layers.Conv1D(filters=<span class="hljs-number">1</span>, kernel_size=<span class="hljs-number">5</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>)(char_embedding_layer)
</code></pre>
    <p class="normal">This layer typically<a id="_idIndexMarker650"/> takes a <code class="inlineCode">[batch size, width, in channels]</code>-sized<a id="_idIndexMarker651"/> tensor. However, in our case, we have a four-dimensional input. This means, our Conv1D layer is going to behave in a time-distributed fashion. Put in another way, it will take an input with a temporal dimension (i.e. sequence length dimension) and produce an output with that dimension intact. In other words, it takes our input of shape <code class="inlineCode">[batch size, sequence length, token length, 32 (in channels)]</code> and produces a <code class="inlineCode">[batch size, sequence length, token length, 1 (out channels)]</code>-sized output. You can see that the convolution only operates on the last two dimensions, while keeping the first two as they are.</p>
    <p class="normal">Another way to think about this is, ignore the batch and sequence dimensions and visualize how convolution would work on the width and in channel dimensions. Then apply the same operation element-wise to other dimensions, while considering the operation on 2D <code class="inlineCode">[width, in channel] </code>tensors as a single unit of computation.</p>
    <p class="normal">Remember that we have a <code class="inlineCode">[batch size, sequence length, token length, 1]</code>-sized output. This has an extra dimension of 1 at the end. We will write a simple <code class="inlineCode">Lambda</code> layer to get rid of this dimension:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># There is an additional dimension of size 1 (out channel dimension) that</span>
<span class="hljs-comment"># we need to remove</span>
char_token_output = layers.Lambda(<span class="hljs-keyword">lambda</span> x: x[:, :, :, <span class="hljs-number">0</span>])(char_token_output)
</code></pre>
    <p class="normal">To get the final output embedding (i.e. a combination of token- and character-based embeddings), we concatenate the two embeddings on the last axis. This would result in a 48 element-long vector (i.e. 32 element-long token embedding + 12 element-long char-based token embedding):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Concatenate the token and char embeddings</span>
concat_embedding_out = layers.Concatenate()([token_embedding_out, char_token_output])
</code></pre>
    <p class="normal">The rest of the model, we will keep it the same. First define an RNN layer and pass the <code class="inlineCode">concat_embedding_out</code> as an input:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define a simple bidirectional RNN layer, it returns an output at each</span>
<span class="hljs-comment"># position</span>
rnn_layer_1 = layers.SimpleRNN(
    units=<span class="hljs-number">64</span>, activation=<span class="hljs-string">'tanh'</span>, use_bias=<span class="hljs-literal">True</span>, return_sequences=<span class="hljs-literal">True</span>
)
rnn_out_1 = rnn_layer_1(concat_embedding_out)
</code></pre>
    <p class="normal">Remember that we have set <code class="inlineCode">return_sequences=True</code>, which means it will produce an output at each time<a id="_idIndexMarker652"/> step, as opposed to only at the last<a id="_idIndexMarker653"/> time step. Next, we define the final Dense layer, which has <code class="inlineCode">n_classes</code> output nodes (i.e. 9) and a <code class="inlineCode">softmax</code> activation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Defines the final prediction layer</span>
dense_layer = layers.Dense(n_classes, activation=<span class="hljs-string">'softmax'</span>)
dense_out = dense_layer(rnn_out_1)
</code></pre>
    <p class="normal">We define the model and compile it like before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Defines the model</span>
char_token_embedding_rnn = tf.keras.Model(inputs=word_input, outputs=dense_out)
 
<span class="hljs-comment"># Define a macro accuracy measure</span>
mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name=<span class="hljs-string">'macro_accuracy'</span>)
<span class="hljs-comment"># Compile the model with a loss optimizer and metrics</span>
char_token_embedding_rnn.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>, metrics=[mean_accuracy_metric])
</code></pre>
    <p class="normal">This is our final model. The key difference in this model compared to the previous solution is that it used two different embedding types. A standard token-based embedding layer and a complex, char-based embedding that was leveraged to generate token embeddings using the convolution operation. Now let’s train the model.</p>
    <h2 id="_idParaDest-167" class="heading-2">Model training and evaluation</h2>
    <p class="normal">Model training<a id="_idIndexMarker654"/> is identical to the training we did for the standard RNN model, so we will not discuss it further:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Make train_sequences an array</span>
train_sentences = np.array(train_sentences)
<span class="hljs-comment"># Get sample weights (we cannot use class_weight with TextVectorization</span>
<span class="hljs-comment"># layer)</span>
train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)
<span class="hljs-comment"># Training the model</span>
char_token_embedding_rnn.fit(
    train_sentences, padded_train_labels,
    sample_weight=train_sample_weights,
    batch_size=<span class="hljs-number">64</span>,
    epochs=<span class="hljs-number">3</span>, 
    validation_data=(np.array(valid_sentences), padded_valid_labels)
)
</code></pre>
    <p class="normal">You should get around a ~2% validation<a id="_idIndexMarker655"/> accuracy and a ~1% test accuracy boost<a id="_idIndexMarker656"/> after these modifications.</p>
    <h2 id="_idParaDest-168" class="heading-2">Other improvements you can make</h2>
    <p class="normal">Here we will discuss several improvements<a id="_idIndexMarker657"/> you can make to uplift the model performance even further.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">More RNN layers</strong> – Adding more stacked<a id="_idIndexMarker658"/> RNN layers. By adding more hidden RNN layers, we can allow the model to learn more refined latent representations, leading to better performance. An example usage is shown below:
        <pre class="programlisting code"><code class="hljs-code">rnn_layer_1 = layers.SimpleRNN(
    units=<span class="hljs-number">64</span>, activation=<span class="hljs-string">'tanh'</span>, use_bias=<span class="hljs-literal">True</span>, return_sequences=<span class="hljs-literal">True</span>
)
rnn_out_1 = rnn_layer_1(concat_embedding_out)
rnn_layer_2 = layers.SimpleRNN(
    units=<span class="hljs-number">32</span>, activation=<span class="hljs-string">'tanh'</span>, use_bias=<span class="hljs-literal">True</span>, return_sequences=<span class="hljs-literal">True</span>
)
rnn_out_1 = rnn_layer_1(rnn_out_1)
</code></pre>
      </li>
      <li class="bulletList"><strong class="keyWord">Make the RNN layer bidirectional</strong> – The RNN models we discussed so far are uni-directional, i.e. looks <a id="_idIndexMarker659"/>at the sequence of text from forward to backward. However a different variant known as bi-directional RNNs looks at the sequence in both directions, i.e. forward to backward and backward to forward. This leads to better language understanding in models and inevitably better performance. We will discuss this variant in more detail in the upcoming chapters. An example usage is shown below:
        <pre class="programlisting code"><code class="hljs-code">rnn_layer_1 = layers.Bidreictional(layers.SimpleRNN(
    units=<span class="hljs-number">64</span>, activation=<span class="hljs-string">'tanh'</span>, use_bias=<span class="hljs-literal">True</span>, return_sequences=<span class="hljs-literal">True</span>
))
</code></pre>
      </li>
      <li class="bulletList"><strong class="keyWord">Incorporate regularization techniques</strong> – You can leverage L2 regularization and dropout techniques<a id="_idIndexMarker660"/> to avoid overfitting and improve generalization of the model.</li>
      <li class="bulletList"><strong class="keyWord">Use early stopping and learning rate reduction to reduce overfitting</strong> – During model training, use early stopping (i.e. training the model only until the validation accuracy is improving) and learning rate reduction (i.e. gradually reducing the learning rate over the epochs).</li>
    </ul>
    <p class="normal">We recommend experimenting<a id="_idIndexMarker661"/> with some of these techniques yourself to see how they can maximize the performance of your RNNs.</p>
    <h1 id="_idParaDest-169" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we looked at RNNs, which are different from conventional feed-forward neural networks and more powerful in terms of solving temporal tasks. </p>
    <p class="normal">Specifically, we discussed how to arrive at an RNN from a feed-forward neural network type structure. </p>
    <p class="normal">We assumed a sequence of inputs and outputs, and designed a computational graph that can represent the sequence of inputs and outputs. </p>
    <p class="normal">This computational graph resulted in a series of copies of functions that we applied to each individual input-output tuple in the sequence. Then, by generalizing this model to any given single time step <em class="italic">t</em> in the sequence, we were able to arrive at the basic computational graph of an RNN. We discussed the exact equations and update rules used to calculate the hidden state and the output.</p>
    <p class="normal">Next we discussed how RNNs are trained with data using BPTT. We examined how we can arrive at BPTT with standard backpropagation as well as why we can’t use standard backpropagation for RNNs. We also discussed two important practical issues that arise with BPTT—vanishing gradient and exploding gradient—and how these can be solved on the surface level.</p>
    <p class="normal">Then we moved on to the practical applications of RNNs. We discussed four main categories of RNNs. One-to-one architectures are used for tasks such as text generation, scene classification, and video frame labeling. Many-to-one architectures are used for sentiment analysis, where we process the sentences/phrases word by word (compared to processing a full sentence in a single go, as we saw in the previous chapter). One-to-many architectures are common in image captioning tasks, where we map a single image to an arbitrarily long sentence phrase describing the image. Many-to-many architectures are leveraged for machine translation tasks.</p>
    <p class="normal">We solved the task of NER with RNNs. In NER, the problem is to, given a sequence of tokens, predict a label for each token. The label represents an entity (e.g. organization, location, person, etc.). For this we used embeddings as well as an RNN to process each token while considering the sequence of tokens as a time-series input. We also used a text vectorization layer to convert tokens into word IDs. A key benefit of the text vectorization layer is that it is built as a part of the model, unlike the tokenizer we used before. </p>
    <p class="normal">Finally, we looked at how we can adopt character embeddings and the convolution operation to generate token embeddings. We used these new token embeddings along with standard word embeddings to improve model accuracy. </p>
    <p class="normal">In the next chapter, we will discuss a more powerful RNN model known as <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>) networks that further reduces the adverse effect of the vanishing gradient, and thus produces much better results.</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="https://packt.link/nlpgithub"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="https://packt.link/nlp"><span class="url">https://packt.link/nlp</span></a></p>
    <figure class="mediaobject"> <img src="../Images/QR_Code5143653472357468031.png" alt=""/></figure>
  </div>
</body></html>