- en: Recurrent-Type Neural Networks - Language Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '** Recurrent neural networks** (**RNNs**) are a class of deep learning architectures
    that are widely used for natural language processing. This set of architectures
    enables us to provide contextual information for current predictions and also
    have specific architecture that deals with long-term dependencies in any input
    sequence. In this chapter, we''ll demonstrate how to make a sequence-to-sequence
    model, which will be useful in many applications in NLP. We will demonstrate these
    concepts by building a character-level language model and see how our model generates
    sentences similar to original input sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The intuition behind RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of the language model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intuition behind RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the deep learning architectures that we have dealt with so far have no mechanism
    to memorize the input that they have received previously. For instance, if you
    feed a **feed-forward neural network** (**FNN**) with a sequence of characters
    such as **HELLO**, when the network gets to **E**, you will find that it didn't
    preserve any information/forgotten that it just read **H**. This is a serious
    problem for sequence-based learning. And since it has no memory of any previous
    characters it read, this kind of network will be very difficult to train to predict
    the next character. This doesn't make sense for lots of applications such as language
    modeling, machine translation, speech recognition, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: For this specific reason, we are going to introduce RNNs, a set of deep learning
    architectures that do preserve information and memorize what they have just encountered.
  prefs: []
  type: TYPE_NORMAL
- en: Let's demonstrate how RNNs should work on the same input sequence of characters, **HELLO**.
    When the RNN cell/unit receives **E** as an input, it also receives that character
    **H**, which it received earlier. This feeding of the present character along
    with the past one as an input to the RNN cell gives a great advantage to these
    architectures, which is short-term memory; it also makes these architectures usable
    for predicting/guessing the most likely character after **H**, which is **L**,
    in this specific sequence of characters.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that previous architectures assign weights to their inputs; RNNs
    follow the same optimization process of assigning weights to their multiple inputs,
    which is the present and past. So in this case, the network will assign two different
    matrices of weights to each one of them. In order to do that, we will be using
    **gradient descent** and a heavier version of backpropagation, which is called **backpropagation
    through time** (**BPTT**).
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on our background of using previous deep learning architectures,
    you will find out why RNNs are special. The previous architectures that we have
    learned about are not flexible in terms of their input or training. They accept
    a fixed-size sequence/vector/image as an input and produce another fixed-size
    one as an output. RNN architectures are somehow different, because they enable
    you to feed a sequence as input and get another sequence as output, or to have
    sequences in the input only/output only as shown in *Figure 1*. This kind of flexibility
    is very useful for multiple applications such as language modeling and sentiment
    analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/567e14af-e7cf-4439-bdf0-7a72a595a1f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Flexibility of RNNs in terms of shape of input or output (http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  prefs: []
  type: TYPE_NORMAL
- en: The intuition behind these set of architectures is to mimic the way humans process
    information. In any typical conversation your understanding of someone's words
    is totally dependent on what he said previously and you might even be able to
    predict what he's going to say next based on what he just said.
  prefs: []
  type: TYPE_NORMAL
- en: The exact same process should be followed in the case of RNNs. For example,
    imagine you want translate a specific word in a sentence. You can't use traditional
    FNNs for that, because they won't be able to use the translation of previous words
    as an input with the current word that we want to translate, and this may result
    in an incorrect translation because of the lack of contextual information around
    this word.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs do preserves information about the past and they have some kind of loops
    to allow the previously learned information to be used for the current prediction
    at any given point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5975b720-dd6b-44f2-b227-732b2aa481d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: RNNs architecture which has loop to persist information for past
    steps (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2*, we have some neural networks called *A* which receives an input
    *X[t]* and produces and output *h[t]*. Also, it receives information from past
    steps with the help of this loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'This loop seems to unclear, but if we used the unrolled version of *Figure
    2*, you will find out that it''s very simple and intuitive, and that the RNN is
    nothing but a repeated version of the same network (which could be normal FNN),
    as shown in *Figure 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fdae554-aebd-49fb-a8ef-24071579e7fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An unrolled version of the recurrent neural network architecture
    (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: This intuitive architecture of RNNs and its flexibility in terms of input/output
    shape make them a good fit for interesting sequence-based learning tasks such
    as machine translation, language modeling, sentiment analysis, image captioning,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we have an intuitive understanding of how RNNs work and how it's going
    to be useful in different interesting sequence-based examples. Let's have a closer
    look of some of these interesting examples.
  prefs: []
  type: TYPE_NORMAL
- en: Character-level language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language modeling is an essential task for many applications such as speech
    recognition, machine translation and more. In this section, we'll try to mimic
    the training process of RNNs and get a deeper understanding of how these networks
    work. We'll build a language model that operate over characters. So, we will feed
    our network with a chunk of text with the purpose of trying to build a probability
    distribution of the next character given the previous ones which will allow us
    to generate text similar to the one we feed as an input in the training process.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we have a language with only four letters as its vocabulary, **helo**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task is to train a recurrent neural network on a specific input sequence
    of characters such as **hello**. In this specific example, we have four training
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the character **e** should be calculated given the context
    of the first input character **h**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The probability of the character **l** should be calculated given the context
    of **he**,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The probability of the character **l** should be calculated given the context
    of **hel**, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally the probability of the character **o** should be calculated given the
    context of **hell**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we learned in previous chapters, machine learning techniques in general
    which deep learning is a part of, only accept real-value numbers as input. So,
    wee need somehow convert or encode or input character to a numerical form. To
    do this, we will use one-hot-vector encoding which is a way to encode text by
    have a vector of zeros except for a single entry in the vector, which is the index
    of the character in the vocabulary of this language that we are trying to model
    (in this case **helo**). After encoding our training samples, we will provide
    them to the RNN-type model one at a time. At each given character, the output
    of the RNN-type model will be a 4-dimensional vector (the size of the vector corresponds
    to the size of the vocab) which represents the probability of each character in
    the vocabulary being the next one after the given input character. *Figure 4*
    clarifies this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed9df466-611d-4550-aa61-c2ac49e16102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Example of RNN-type network with one-hot-vector encoded characters
    as an input and the output will be distribution over the vocab representing the
    most likely character after the current one (source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 4*, you can see that we fed the first character in our input
    sequence **h** to the model and the output was 4-dimensional vector representing
    the confidence about the next character. So it has a confidence of **1.0** of
    **h** being the next character after the input **h**, a confidence of **2.2**
    of **e** being the next character, a confidence of **-3.0** to **l** being the
    next character, and finally a confidence of **4.1** to **o** being the next character.
    In this specific example, we know the correct next character will be **e**,based
    on our training sequence **hello**. So our primary goal while training this RNN-type
    network is increase the confidence of **e** being the next character and decrease
    the confidence of other characters. To do this kind of optimization we will be
    using gradient descent and backpropagation algorithms to update the weights and
    influence the network to produce a higher confidence for our correct next character, **e**,
    and so on, for the other 3 training examples.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see the output of the RNN-type network produces a confidence distribution
    over all the characters of the vocab being the next one. We can turn this confidence
    distribution into a probability distribution such that the increase of one characters
    probability being the next one will result in decreasing the others probabilities
    because the probability needs to sum up to 1\. For this specific modification
    we can use a standard softmax layer to every output vector.
  prefs: []
  type: TYPE_NORMAL
- en: For generating text from these kind of networks, we can feed an initial character
    to the model and get a probability distribution over the characters that are likely
    to be next, and then we can sample from these characters and feed it back as an
    input to the model. We'll be able to get a sequence of characters by repeating
    this process over and over again as many times as we want to generate a text with
    a desired length.
  prefs: []
  type: TYPE_NORMAL
- en: Language model using Shakespeare data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the preceding example, we can get the model to generate text. But the network
    will surprise us, as it's not only going to generate text but also it's going
    to learn the style and the structure in training data. We can demonstrate this
    interesting process by training an RNN-type model on specific kind of text that
    has structure and style in it, such as the following Shakespeare work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at a generated output from the trained network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second Senator:'
  prefs: []
  type: TYPE_NORMAL
- en: They are away this miseries, produced upon my soul,
  prefs: []
  type: TYPE_NORMAL
- en: Breaking and strongly should be buried, when I perish
  prefs: []
  type: TYPE_NORMAL
- en: The earth and thoughts of many states.
  prefs: []
  type: TYPE_NORMAL
- en: In spite of the fact that the network only knows how to produce one single character
    at a time, it was able to generate a meaningful text and names that actually have
    the structure and style of Shakespeare work.
  prefs: []
  type: TYPE_NORMAL
- en: The vanishing gradient problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While training these sets of RNN-type architectures, we use gradient descent
    and backprogagation through time, which introduced some successes for lots of
    sequence-based learning tasks. But because of the nature of the gradient and due
    to using fast training strategies, it could be shown that the gradient values
    will tend to be too small and vanish. This process introduced the vanishing gradient
    problem that many practitioners fall into. Later on in this chapter, we will discuss
    how researchers approached these kind of problems and produced variations of the
    vanilla RNNs to overcome this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79e4dcaf-8767-4631-94d6-2f24a58024d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Vanishing gradient problem'
  prefs: []
  type: TYPE_NORMAL
- en: The problem of long-term dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another challenging problem faced by researchers is the long-term dependencies
    that one can find in text. For example, if someone feeds a sequence like *I used
    to live in France and I learned how to speak...* the next obvious word in the
    sequence is the word French.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these kind of situation vanilla RNNs will be able to handle it because it
    has short-term dependencies, as shown in *Figure 6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/568df409-a768-4696-ae1a-d6e371d6cb6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Showing short-term dependencies in the text (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example, if someone started the sequence by saying that *I used to
    live in France..* and then he/she start to describe the beauty of living there
    and finally he ended the sequence by *I learned to speak French*. So, for the
    model to predict the language that he/she learned at the end of the sequence,
    the model needs to have some information about the early words *live* and *France*.
    The model won''t be able to handle these kind of situation, if it doesn''t manage
    to keep track of long term dependencies in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12e35c64-5d9d-4ecc-b969-4e627147b77d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The challenge of long-term dependencies in text (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: To handle vanishing gradients and long-term dependencies in the text, researchers
    introduced a variation of the vanilla RNN network called **Long Short Term Networks **(**LSTM**).
  prefs: []
  type: TYPE_NORMAL
- en: LSTM networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LSTM, a variation of an RNN that is used to help learning long term dependencies
    in the text. LSTMs were initially introduced by Hochreiter & Schmidhuber (1997)
    (link: [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)),
    and many researchers worked on it and produced interesting results in many domains.'
  prefs: []
  type: TYPE_NORMAL
- en: These kind of architectures will be able to handle the problem of long-term
    dependencies in the text because of its inner architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs are similar to the vanilla RNN as it has a repeating module over time,
    but the inner architecture of this repeated module is different from the vanilla
    RNNs. It includes more layers for forgetting and updating information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ae7087a-fced-4f5c-86c8-84855ad8301a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The repeating module in a standard RNN containing a single layer
    (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned previously, the vanilla RNNs have a single NN layer, but the LSTMs
    have four different layers interacting in a special way. This special kind of
    interaction is what makes LSTM, work very well for many domains, which we''ll
    see while building our language model example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a385710b-f1db-4c91-af43-244a22b863b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The repeating module in an LSTM containing four interacting layers
    (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the mathematical details and how the four layers are
    actually interacting with each other, you can have a look at this interesting
    tutorial: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  prefs: []
  type: TYPE_NORMAL
- en: Why does LSTM work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in our vanilla LSTM architecture it to decide which information
    is not necessary and it will work by throwing it away to leave more room for more
    important information. For this, we have a layer called **forget gate layer**,
    which looks at the previous output *h[t-1]* and the current input *x[t]* and decides
    which information we are going to throw away.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step in the LSTM architecture is to decide which information is worth
    keeping/persisting and storing in the cell. This is done in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A layer called **input gate layer**, which decides which values of the previous
    state of the cell needs to be updated
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second step is to generate a set of new candidate values that will be added
    to the cell
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we need to decide what the LSTM cell is going to output. This output
    will be based on our cell state, but will be a filtered version.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of the language model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll build a language model that operates over characters.
    For this implementation, we will use an Anna Karenina novel and see how the network
    will learn to implement the structure and style of the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29c6dfef-8eee-49f1-a36c-b6826af3c065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: General architecture for the character-level RNN (source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This network is based off of Andrej Karpathy''s post on RNNs (link: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))
    and implementation in Torch (link: [https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)).
    Also, there''s some information here at r2rt (link: [http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html))
    and from Sherjil Ozairp (link: [https://github.com/sherjilozair/char-rnn-tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow))
    on GitHub. The following is the general architecture of the character-wise RNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll build a character-level RNN trained on the Anna Karenina novel (link:
    [https://en.wikipedia.org/wiki/Anna_Karenina](https://en.wikipedia.org/wiki/Anna_Karenina))[.](https://en.wikipedia.org/wiki/Anna_Karenina)
    It''ll be able to generate new text based on the text from the book. You will
    find the `.txt` file included with the assets of this implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the necessary libraries for this character-level implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To start off, we need to prepare the dataset by loading it and converting it
    in to integers. So, we will convert the characters into integers and then encode
    them as integers which makes it straightforward and easy to use as input variables
    for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s have look at the first 200 characters from the Anna Karenina text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We have also converted the characters to a convenient form for the network,
    which is integers. So, let''s have a look at the encoded version of the characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Since the network is working with individual characters, it's similar to a classification
    problem in which we are trying to predict the next character from the previous
    text. Here's how many classes our network has to pick from.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we will be feeding the model a character at a time, and the model will
    predict the next character by producing a probability distribution over the possible
    number of characters that could come next (vocab), which is equivalent to a number
    of classes the network needs to pick from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since we'll be using stochastic gradient descent to train our model, we need
    to convert our data into training batches.
  prefs: []
  type: TYPE_NORMAL
- en: Mini-batch generation for training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will divide our data into small batches to be used for
    training. So, the batches will consist of many sequences of desired number of
    sequence steps. So, let''s look at a visual example in *Figure 11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77f38b3c-ced1-4784-af60-897d92f968ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Illustration of how batches and sequences would look like (source:
    http://oscarmore2.github.io/Anna_KaRNNa_files/charseq.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now we need to define a function that will iterate through the encoded
    text and generate the batches. In this function we will be using a very nice mechanism
    of Python called **yield** (link: [https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/)).'
  prefs: []
  type: TYPE_NORMAL
- en: A typical batch will have *N × M* characters, where *N* is the number of sequences
    and *M* is, number of sequence steps. For getting the number of possible batches
    in our dataset, we can simply divide the length of the data by the desired batch
    size and after getting this number of possible batches, we can drive how many
    characters should be in each batch.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we need to split the dataset we have into a desired number of sequences
    (*N*). We can use `arr.reshape(size)`. We know we want *N* sequences (`num_seqs` is
    used in, following code), let's make that the size of the first dimension. For
    the second dimension, you can use -1 as a placeholder in the size; it'll fill
    up the array with the appropriate data for you. After this, you should have an
    array that is *N × (M * K)*, where *K* is the number of batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have this array, we can iterate through it to get the training
    batches, where each batch has *N × M* characters. For each subsequent batch, the
    window moves over by `num_steps`. Finally, we also want to create both the input
    and output arrays for ours to be used as the model input. This step of creating
    the output values is very easy; remember that the targets are the inputs shifted
    over one character. You''ll usually see the first input character used as the
    last target character, so something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b300aec-cfbe-4d7f-a835-c7ef2c9944a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *x* is the input batch and *y* is the target batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way I like to do this window is to use range to take steps of size `num_steps`,
    starting from 0 to `arr.shape[1]`, the total number of steps in each sequence.
    That way, the integers you get from the range always point to the start of a batch,
    and each window is `num_steps` wide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s demonstrate this using this function by generating a batch of 15
    sequences and 50 sequence steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next up, we'll be looking forward to building the core of this example, which
    is the LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into building the character-level model using LSTMs, it is worth
    mentioning something called **Stacked LSTM**.
  prefs: []
  type: TYPE_NORMAL
- en: Stacked LSTMs are useful for looking at your information at different time scales.
  prefs: []
  type: TYPE_NORMAL
- en: Stacked LSTMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Building a deep RNN by stacking multiple recurrent hidden states on top of
    each other. This approach potentially allows the hidden state at each level to
    operate at different timescale" How to Construct Deep Recurrent Neural Networks
    (link: https://arxiv.org/abs/1312.6026), 2013'
  prefs: []
  type: TYPE_NORMAL
- en: '"RNNs are inherently deep in time, since their hidden state is a function of
    all previous hidden states. The question that inspired this paper was whether
    RNNs could also benefit from depth in space; that is from stacking multiple recurrent
    hidden layers on top of each other, just as feedforward layers are stacked in
    conventional deep networks". Speech Recognition With Deep RNNs (link: [https://arxiv.org/abs/1303.5778](https://arxiv.org/abs/1303.5778)),
    2013'
  prefs: []
  type: TYPE_NORMAL
- en: Most researchers are using stacked LSTMs for challenging sequence prediction
    problems. A stacked LSTM architecture can be defined as an LSTM model comprised
    of multiple LSTM layers. The preceding LSTM layer provides a sequence output rather
    than a single-value output to the LSTM layer as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, it''s one output per input time step, rather than one output
    time step for all input time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fcf09a5d-b4e6-4c54-826b-58aed18b75a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Stacked LSTMs'
  prefs: []
  type: TYPE_NORMAL
- en: So in this example, we will be using this kind of stacked LSTM architecture,
    which gives better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is where you''ll build the network. We''ll break it into parts so that
    it''s easier to reason about each bit. Then, we can connect them with the whole
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b4a2032-77d5-4c59-a4de-0a4cf19bccfd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Character-level model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s start by defining the model inputs as placeholders. The inputs
    of the model will be training data and the targets. We will also use a parameter
    called `keep_probability` for the dropout layer, which helps the model avoid overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Building an LSTM cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will write a function for creating the LSTM cell, which
    will be used in the hidden layer. This cell will be the building block for our
    model. So, we will create this cell using TensorFlow. Let's have a look at how
    we can use TensorFlow to build a basic LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the following line of code to create an LSTM cell with the parameter
    `num_units` representing the number of units in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To prevent overfitting, we can use something called **dropout**, which is a
    mechanism for preventing the model from overfitting the data by decreasing the
    model''s complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned before, we will be using the stacked LSTM architecture; it
    will help us to look at the data from different angles and has been practically found
    to perform better. In order to define a stacked LSTM in TensorFlow, we can use
    the `tf.contrib.rnn.MultiRNNCell` function (link: [https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Initially for the first cell, there will be no previous information, so we
    need to initialize the cell state to be zeros. We can use the following function
    to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s put it all together and create our LSTM cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: RNN output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next up, we need to create the output layer, which is responsible for reading
    the output of the individual LSTM cells and passing them through a fully connected
    layer. This layer has a softmax output for producing a probability distribution
    over the likely character to be next after the input one.
  prefs: []
  type: TYPE_NORMAL
- en: As you know, we have generated input batches for the network with size *N ×
    M* characters, where *N* is the number of sequences in this batch and *M* is the
    number of sequence steps. We have also used *L* hidden units in the hidden layer
    while creating the model. Based on the batch size and number of hidden units,
    the output of the network will be a *3D* Tensor with size *N × M × L*, and that's
    because we call the LSTM cell *M* times, one for each sequence step. Each call
    to LSTM cell produces an output of size *L*. Finally, we need to do this as many
    as number of sequences *N* as the we have.
  prefs: []
  type: TYPE_NORMAL
- en: So we pass this *N × M × L* output to a fully connected layer (which is the
    same for all outputs with the same weights), but before doing this, we reshape
    the output to a *2D* tensor, which has a shape of *(M * N) × L*. This reshaping
    will make things easier for us when operating on the output, because the new shape
    will be more convenient; the values of each row represents the *L* outputs of
    the LSTM cell, and hence it's one row for each sequence and step.
  prefs: []
  type: TYPE_NORMAL
- en: After getting the new shape, we can connect it to the fully connected layer
    with the softmax by doing matrix multiplication with the weights. The weights
    created in the LSTM cells and the weight that we are about to create here have
    the same name by default, and TensorFlow will raise an error in such a case. To
    avoid this error, we can wrap the weight and bias variables created here in a
    variable scope using the TensorFlow function `tf.variable_scope()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After explaining the shape of the output and how we are going to reshape it,
    to make things easier, let''s go ahead and code this `build_model_output` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Training loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next up is the training loss. We get the logits and targets and calculate the
    softmax cross-entropy loss. First, we need to one-hot encode the targets; we're
    getting them as encoded characters. Then, we reshape the one-hot targets, so it's
    a *2D* tensor with size *(M * N) × C*, where *C* is the number of classes/characters
    we have. Remember that we reshaped the LSTM outputs and ran them through a fully
    connected layer with *C* units. So, our logits will also have size *(M * N) × C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we run the `logits` and `targets` through `tf.nn.softmax_cross_entropy_with_logits`
    and find the mean to get the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we need to use an optimization method that will help us learn something
    from the dataset. As we know, vanilla RNNs have exploding and vanishing gradient
    issues. LSTMs fix only one issue, which is the vanishing of the gradient values,
    but even after using LSTM, some gradient values explode and grow without bounds.
    In order to fix this problem, we can use something called **gradient clipping**,
    which is a technique to clip the gradients that explode to a specific threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s define our optimizer by using the Adam optimizer for the learning
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Building the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can put all the pieces together and build a class for the network.
    To actually run data through the LSTM cells, we will use `tf.nn.dynamic_rnn` (link:
    [https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn)).
    This function will pass the hidden and cell states across LSTM cells appropriately
    for us. It returns the outputs for each LSTM cell at each step for each sequence
    in the mini-batch. It also gives us the final LSTM state. We want to save this
    state as `final_state`, so we can pass it to the first LSTM cell in the the next
    mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state
    we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot
    encode the inputs before going into the RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Model hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with any deep learning architecture, there are a few hyperparameters that
    someone can use to control the model and fine-tune it. The following is the set
    of hyperparameters that we are using for this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch size is the number of sequences running through the network in one pass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of steps is the number of characters in the sequence the network
    is trained on. Larger is better typically; the network will learn more long-range
    dependencies, but will take longer to train. 100 is typically a good number here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LSTM size is the number of units in the hidden layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture number layers is the number of hidden LSTM layers to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate is the typical learning rate for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And finally, the new thing that we call keep probability is used by the dropout
    layer; it helps the network to avoid overfitting. So if your network is overfitting,
    try decreasing this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's kick off the training process by providing the inputs and outputs
    to the built model and then use the optimizer to train the network. Don't forget
    that we need to use the previous state while making predictions for the current
    state. Thus, we need to pass the output state back to the network so that it can
    be used during the prediction of the next input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s provide initial values for our hyperparameters (you can tune them afterwards
    depending on the dataset you are using to train this architecture):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the training process, you should get an error close to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Saving checkpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s load the checkpoints. For more about saving and loading checkpoints,
    you can check out the TensorFlow documentation ([https://www.tensorflow.org/programmers_guide/variables](https://www.tensorflow.org/programmers_guide/variables)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Generating text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have a trained model based on our input dataset. The next step is to use
    this trained model to generate text and see how this model learned the style and
    structure of the input data. To do this, we can start with some initial characters
    and then feed the new, predicted one as an input in the next step. We will repeat
    this process until we get a text with a specific length.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code, we have also added extra statements to the function to
    prime the network with some initial text and start from there.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network gives us predictions or probabilities for each character in the
    vocab. To reduce noise and only use the ones that the network is more confident
    about, we''re going to only choose a new character from the top *N* most probable
    characters in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start the sampling process using the latest checkpoint saved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to sample using this latest checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we were able to generate some meaningful words and some meaningless
    words. In order to get more results, you can run the model for more epochs and
    try to play with the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned about RNNs, how they work, and why they have become a big deal. We
    trained an RNN character-level language model on fun novel datasets and saw where
    RNNs are going. You can confidently expect a large amount of innovation in the
    space of RNNs, and I believe they will become a pervasive and critical component
    of intelligent systems.
  prefs: []
  type: TYPE_NORMAL
