["```\ninputs = Input(shape=[img_height * img_width])\n# Encoding layers:\nenc_1  = Dense(128, activation='relu')(inputs)\ncode   = Dense(64,  activation='relu')(enc_1)\n# Decoding layers:\ndec_1  = Dense(64,  activation='relu')(code)\npreds  = Dense(128, activation='sigmoid')(dec_1)\nautoencoder = Model(inputs, preds)\n# Training:\nautoencoder.compile(loss='binary_crossentropy')\nautoencoder.fit(x_train, x_train) # x_train as inputs and targets\n```", "```\nx_noisy = x_train + np.random.normal(loc=.0, scale=.5, size=x_train.shape)\nautoencoder.fit(x_noisy, x_train)\n```", "```\ninputs = Input(shape=(224, 224, 3))\n# Building a pretrained VGG-16 feature extractor as encoder:\nvgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=inputs)\n# We recover the feature maps returned by each of the 3 final blocks:\nf3 = vgg16.get_layer('block3_pool').output # shape: (28, 28, 256)\nf4 = vgg16.get_layer('block4_pool').output # shape: (14, 14, 512)\nf5 = vgg16.get_layer('block5_pool').output # shape: ( 7, 7, 512)\n# We replace the VGG dense layers by convs, adding the \"decoding\" layers instead after the conv/pooling blocks:\nf3 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f3)\nf4 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f4)\nf5 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f5)\n# We upscale `f5` to a 14x14 map so it can be merged with `f4`:\nf5x2 = Conv2DTranspose(filters=out_chh, kernel_size=4,strides=2, \n                       padding='same', activation='relu')(f5)\n# We merge the 2 feature maps with an element-wise addition:\nm1 = add([f4, f5x2])\n# We repeat the operation to merge `m1` and `f3` into a 28x28 map:\nm1x2 = Conv2DTranspose(filters=out_ch, kernel_size=4, strides=2,\n                       padding='same', activation='relu')(m1)\nm2 = add([f3, m1x2])\n# Finally, we use a transp-conv to recover the original shape:\noutputs = Conv2DTranspose(filters=out_ch, kernel_size=16, strides=8,\n                          padding='same', activation='sigmoid')(m2)\nfcn_8s = Model(inputs, outputs)\n```", "```\nx_noisy = bilinear_upscale(bilinear_downscale(x_train)) # pseudo-code\nfcn_8s.fit(x_noisy, x_train)\n```", "```\ninputs = Input(shape=(224, 224, 3))\nout_ch = num_classes = 19 # e.g., for object segmentation over Cityscapes\n# [...] building e.g. a FCN-8s architecture, c.f. previous snippet.\noutputs = Conv2DTranspose(filters=out_ch, kernel_size=16, strides=8,\n                          padding='same', activation=None)(m2)\nseg_fcn = Model(inputs, outputs)\nseg_fcn.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n# [...] training the network. Then we use it to predict label maps:\nlabel_map = np.argmax(seg_fcn.predict(image), axis=-1)\n```", "```\ndef dice_loss(labels, logits, num_classes, eps=1e-6, spatial_axes=[1, 2]):\n    # Transform logits in probabilities, and one-hot the ground truth:\n    pred_proba = tf.nn.softmax(logits, axis=-1)\n    gt_onehot  = tf.one_hot(labels, num_classes, dtype=tf.float32)\n    # Compute Dice numerator and denominator:\n    num_perclass = 2 * tf.reduce_sum(pred_proba * gt_onehot, axis=spatial_axes)\n    den_perclass = tf.reduce_sum(pred_proba + gt_onehot, axis=spatial_axes)\n    # Compute Dice and average over batch and classes:\n    dice = tf.reduce_mean((num_perclass + eps) / (den_perclass + eps))\n    return 1 - dice\n```"]