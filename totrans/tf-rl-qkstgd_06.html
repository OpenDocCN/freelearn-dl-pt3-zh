<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Asynchronous Methods - A3C and A2C</h1>
                </header>
            
            <article>
                
<p>We looked at the DDPG algorithm in the previous chapter. One of the main drawbacks of the DDPG algorithm (as well as the DQN algorithm that we saw earlier) is the use of a replay buffer to obtain independent and identically distributed samples of data for training. Using a replay buffer consumes a lot of memory, which is not desirable for robust RL applications. To overcome this problem, researchers at Google DeepMind came up with an on-policy algorithm called <strong>Asynchronous Advantage Actor Critic</strong> (<strong>A3C</strong>). A3C does not use a replay buffer; instead, it uses parallel worker processors, where different instances of the environment are created and the experience samples are collected. Once a finite and fixed number of samples are collected, they are used to compute the policy gradients, which are asynchronously sent to a central processor that updates the policy. This updated policy is then sent back to the worker processors. The use of parallel processors to experience different scenarios of the environment gives rise to independent and identically distributed samples that can be used to train the policy. This chapter will cover A3C, and will also briefly touch upon a variant of it called the <strong>Advantage Actor Critic</strong> (<strong>A2C</strong>).</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>The A3C algorithm</li>
<li>The A3C algorithm applied to CartPole</li>
<li>The A3C algorithm applied to LunarLander</li>
<li>The A2C algorithm</li>
</ul>
<p>In this chapter, you will learn about the A3C and A2C algorithms, as well as how to code them using Python and TensorFlow. We will also apply the A3C algorithm to solving two OpenAI Gym problems: CartPole and LunarLander.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To successfully complete this chapter, some knowledge of the following will be of great help:</p>
<ul>
<li>TensorFlow (version 1.4 or higher)</li>
<li>Python (version 2 or 3)</li>
<li>NumPy</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The A3C algorithm</h1>
                </header>
            
            <article>
                
<p>As we mentioned earlier, we have parallel workers in A3C, and each worker will compute the policy gradients and pass them on to the central (or master) processor. The A3C paper also uses the <kbd>advantage</kbd> function to reduce variance in the policy gradients. The <kbd>loss</kbd> functions consist of three losses, which are weighted and added; they include the value loss, the policy loss, and an entropy regularization term. The value loss, <em>L<sub>v</sub></em>, is an L2 loss of the state value and the target value, with the latter computed as a discounted sum of the rewards. The policy loss, <em>L<sub>p</sub></em>, is the product of the logarithm of the policy distribution and the <kbd>advantage</kbd> function, <em>A</em>. The entropy regularization, <em>L<sub>e</sub></em>, is the Shannon entropy, which is computed as the product of the policy distribution and its logarithm, with a minus sign included. The entropy regularization term is like a bonus for exploration;<span> the</span> higher the entropy, the better r<span>egularized</span> <span>the ensuing policy is. These three terms are weighted as 0.5, 1, and -0.005, respectively.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss functions</h1>
                </header>
            
            <article>
                
<p>The value loss is computed as the weighted sum of three loss terms: the value loss, <em>L<sub>v</sub></em>, the policy loss, <em>L<sub>p</sub></em>, and the entropy regularization term, <em>L<sub>e</sub></em>, which are evaluated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/24dfebf5-0c27-4d51-bdd1-fa075fbf0e84.png" style="width:45.00em;height:7.25em;"/></p>
<p><em>L</em> is the total loss, which has to be minimized. Note that we would like to maximize the <kbd>advantage</kbd> function, so we have a minus sign in <em>L<sub>p</sub></em>, as we are minimizing <em>L</em>. Likewise, we would like to maximize the entropy term, and since we are minimizing <em>L</em>, we have a minus sign in the term <em>-0.005 L<sub>e</sub></em> in <em>L.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CartPole and LunarLander</h1>
                </header>
            
            <article>
                
<p>In this section, we will apply A3C to OpenAI Gym's CartPole and LunarLander.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CartPole</h1>
                </header>
            
            <article>
                
<p>CartPole consists of a vertical pole on a cart that needs to be balanced by moving the cart either to the left or to the right. The state dimension is four and the action dimension is two for CartPole.</p>
<p>Check out the following link for more details on CartPole: <a href="https://gym.openai.com/envs/CartPole-v0/">https://gym.openai.com/envs/CartPole-v0/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LunarLander</h1>
                </header>
            
            <article>
                
<p>LunarLander, as the name suggests, involves the landing of a lander on the lunar surface. For example, when Apollo 11's Eagle lander touched down on the moon's surface in 1969, the astronauts Neil Armstrong and Buzz Aldrin had to control the rocket thrusters during the final phase of the descent and safely land the spacecraft on the surface. After this, of course, Armstrong walked on the moon and remarked the now famous sentence: "<em>One small step for a man, one giant leap for mankind</em>". In LunarLander, there are two yellow flags on the lunar surface, and the goal is to land the spacecraft between these flags. Fuel in the lander is infinite, unlike the case in Apollo 11's Eagle lander. The state dimension is eight and the action dimension is four for LunarLander, with the four actions being do nothing, fire the left thruster, fire the main thruster, or fire the right thruster.</p>
<p>Check out the following link for a schematic of the environment: <a href="https://gym.openai.com/envs/LunarLander-v2/">https://gym.openai.com/envs/LunarLander-v2/</a>.<a href="https://gym.openai.com/envs/LunarLander-v2/"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The A3C algorithm applied to CartPole</h1>
                </header>
            
            <article>
                
<p>Here, we will code A3C in TensorFlow and apply it so that we can train an agent to learn the CartPole problem. The following code files will be required to code:</p>
<ul>
<li><kbd>cartpole.py</kbd>: This will start the training or testing process</li>
<li><kbd>a3c.py</kbd>: This is where the A3C algorithm is coded</li>
<li><kbd>utils.py</kbd>: This includes utility functions</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding cartpole.py</h1>
                </header>
            
            <article>
                
<p>We will now code <kbd>cartpole.py</kbd>. Follow these steps to get started:</p>
<ol>
<li>First, we import the packages:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import tensorflow as tf<br/>import gym<br/>import os<br/>import threading<br/>import multiprocessing<br/><br/>from random import choice<br/>from time import sleep<br/>from time import time<br/><br/>from a3c import *<br/>from utils import *</pre>
<ol start="2">
<li>Next, we set the parameters for the problem. We only need to train for <kbd>200</kbd> episodes (yes, CartPole is an easy problem!). We set the discount factor gamma to <kbd>0.99</kbd>. The state and action dimensions are <kbd>4</kbd> and <kbd>2</kbd>, respectively, for CartPole. If you want to load a pre-trained model and resume training, set <kbd>load_model</kbd> to <kbd>True</kbd>; for fresh training from scratch, set this to <kbd>False</kbd>. We will also set the <kbd>model_path</kbd>:</li>
</ol>
<pre style="padding-left: 60px">max_episode_steps = 200<br/>gamma = 0.99<br/>s_size = 4 <br/>a_size = 2 <br/>load_model = False<br/>model_path = './model'</pre>
<ol start="3">
<li class="mce-root">We reset the TensorFlow graph and also create a directory for storing our model. We will refer to the master processor as CPU 0. Worker threads have non-zero CPU numbers. The master processor will undertake the following: first, it will create a count of global variables in the <kbd>global_episodes</kbd> object. The total number of worker threads will be stored in <kbd>num_workers</kbd>, and we can use Python's multiprocessing library to obtain the number of available processors in our system by calling <kbd>cpu_count()</kbd>. We will use the Adam optimizer and store it in an object called <kbd>trainer</kbd>, along with an appropriate learning rate. We will later define an actor critic class called <kbd>AC</kbd>, so we must first create a master network object of the type <kbd>AC</kbd> class, called <kbd>master_network</kbd>. We will also pass the appropriate arguments to the class' constructor. Then, for each worker thread, we will create a separate instance of the CartPole environment and an instance of a <kbd>Worker</kbd> class, which will soon be defined. Finally, for saving the model, we will also create a TensorFlow saver:</li>
</ol>
<pre style="padding-left: 60px">tf.reset_default_graph()<br/><br/>if not os.path.exists(model_path):<br/>    os.makedirs(model_path)<br/>    <br/><br/><br/>with tf.device("/cpu:0"): <br/><br/>    # keep count of global episodes<br/>    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)<br/><br/>    # number of worker threads<br/>    num_workers = multiprocessing.cpu_count() <br/><br/>    # Adam optimizer<br/>    trainer = tf.train.AdamOptimizer(learning_rate=2e-4, use_locking=True) <br/>   <br/>    # global network<br/>    master_network = AC(s_size,a_size,'global',None) <br/>    <br/>    workers = []<br/>    for i in range(num_workers):<br/>        env = gym.make('CartPole-v0')<br/>        workers.append(Worker(env,i,s_size,a_size,trainer,model_path,global_episodes))<br/> <br/>    # tf saver<br/>    saver = tf.train.Saver(max_to_keep=5)</pre>
<ol start="4">
<li>We then start the TensorFlow session. Inside it, we create a TensorFlow coordinator for the different workers. Then, we either load or restore a pre-trained model or run <kbd>tf.global_variables_initializer()</kbd> to assign initial values for all the weights and biases:</li>
</ol>
<pre style="padding-left: 60px">with tf.Session() as sess:<br/><br/>    # tf coordinator for threads<br/>    coord = tf.train.Coordinator()<br/><br/>    if load_model == True:<br/>        print ('Loading Model...')<br/>        ckpt = tf.train.get_checkpoint_state(model_path)<br/>        saver.restore(sess,ckpt.model_checkpoint_path)<br/>    else:<br/>        sess.run(tf.global_variables_initializer())</pre>
<ol start="5">
<li>Then, we start the <kbd>worker_threads</kbd>. Specifically, we call the <kbd>work()</kbd> function, which is part of the <kbd>Worker()</kbd> class (to be defined soon). <kbd>threading.Thread()</kbd> will assign one thread to each <kbd>worker</kbd>. By calling <kbd>start()</kbd>, we initiate the <kbd>worker</kbd> thread. In the end, we need to join the threads so that they wait until all the threads finish before they terminate:</li>
</ol>
<pre style="padding-left: 30px">    # start the worker threads<br/>    worker_threads = []<br/>    for worker in workers:<br/>        worker_work = lambda: worker.work(max_episode_steps, gamma, sess, coord,saver)<br/>        t = threading.Thread(target=(worker_work))<br/>        t.start()<br/>        worker_threads.append(t)<br/>    coord.join(worker_threads)</pre>
<div class="packt_infobox">You can find out more about TensorFlow coordinators at <a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator">https://www.tensorflow.org/api_docs/python/tf/train/Coordinator</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding a3c.py</h1>
                </header>
            
            <article>
                
<p>We will now code <kbd>a3c.py</kbd>. This involves the following steps:</p>
<ol>
<li>Import the packages</li>
<li>Set the initializers for weights and biases</li>
</ol>
<ol start="3">
<li>Define the <kbd>AC</kbd> class</li>
<li>Define the <kbd>Worker</kbd> class</li>
</ol>
<p>First, we need to import the necessary packages:</p>
<pre>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import tensorflow as tf<br/>import gym<br/>import threading<br/>import multiprocessing<br/><br/>from random import choice<br/>from time import sleep<br/>from time import time<br/>from threading import Lock<br/><br/>from utils import *</pre>
<p>Then, we need to set the initializers for the weights and biases; specifically, we use the Xavier initializer for the weights and zero bias. For the last output layer of the network, the weights are uniform random numbers within a specified range:</p>
<pre>xavier = tf.contrib.layers.xavier_initializer()<br/>bias_const = tf.constant_initializer(0.05)<br/>rand_unif = tf.keras.initializers.RandomUniform(minval=-3e-3,maxval=3e-3)<br/>regularizer = tf.contrib.layers.l2_regularizer(scale=5e-4)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The AC class</h1>
                </header>
            
            <article>
                
<p>We will now describe the <kbd>AC </kbd>class, which is also part of <kbd>a3c.py</kbd>. We define the constructor of the <kbd>AC </kbd>class with an input placeholder, two fully connected hidden layers with <kbd>256</kbd> and <kbd>128</kbd> neurons, respectively, and the <kbd>elu</kbd> activation function. This is followed by the policy network with the <kbd>softmax</kbd> activation, since our actions our discrete for CartPole. In addition, we also have a value network with no activation function. Note that we share the same hidden layers for both the policy and value, unlike in past examples:</p>
<pre>class AC():<br/>    def __init__(self,s_size,a_size,scope,trainer):<br/>        with tf.variable_scope(scope):<br/>           <br/>            self.inputs = tf.placeholder(shape=[None,s_size],dtype=tf.float32)<br/>           <br/>            # 2 FC layers <br/>            net = tf.layers.dense(self.inputs, 256, activation=tf.nn.elu, kernel_initializer=xavier, bias_initializer=bias_const, kernel_regularizer=regularizer)<br/>            net = tf.layers.dense(net, 128, activation=tf.nn.elu, kernel_initializer=xavier, bias_initializer=bias_const, kernel_regularizer=regularizer)<br/>                          <br/>            # policy<br/>            self.policy = tf.layers.dense(net, a_size, activation=tf.nn.softmax, kernel_initializer=xavier, bias_initializer=bias_const)<br/><br/>            # value<br/>            self.value = tf.layers.dense(net, 1, activation=None, kernel_initializer=rand_unif, bias_initializer=bias_const)</pre>
<p>For <kbd>worker</kbd> threads, we need to define the <kbd>loss</kbd> functions. Thus, when the TensorFlow scope is not <kbd>global</kbd>, we define an actions placeholder, as well as its one-hot representation; we also define placeholders for the <kbd>target</kbd> value and <kbd>advantage</kbd> functions. We then compute the product of the policy distribution and the one-hot actions, sum them, and store them in the <kbd>policy_times_a</kbd> object. Then, we combine these terms to construct the <kbd>loss</kbd> functions, as we mentioned previously. We compute the sum over the batch of the L2 loss for value; the Shannon entropy as the policy distribution multiplied with its logarithm, with a minus sign; the policy loss as the product of the logarithm of the policy distribution; and the <kbd>advantage</kbd> function, summed over the batch of samples. Finally, we use the appropriate weights to combine these losses to compute the total loss, which is stored in <kbd>self.loss</kbd>:</p>
<pre># only workers need tf operations for loss functions and gradient updating<br/>            if scope != 'global':<br/>                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)<br/>                self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)<br/>                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)<br/>                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)<br/><br/>                self.policy_times_a = tf.reduce_sum(self.policy * self.actions_onehot, [1])<br/><br/>                # loss <br/>                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))<br/>                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy + 1.0e-8))<br/>                self.policy_loss = -tf.reduce_sum(tf.log(self.policy_times_a + 1.0e-8) * self.advantages)</pre>
<pre>                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.005</pre>
<p>As you saw in the previous chapter, we use <kbd>tf.gradients()</kbd> to compute the policy gradients; specifically, we compute the gradients of the <kbd>loss</kbd> function with respect to the local network variables, with the latter obtained from <kbd>tf.get_collection()</kbd>. To mitigate the problem of exploding gradients, we clip the gradients to a magnitude of <kbd>40.0</kbd> using TensorFlow's <kbd>tf.clip_by_global_norm()</kbd> function. We can then collect the network parameters of the global network using <kbd>tf.get_collection()</kbd> with a scope of <kbd>global</kbd> and apply the gradients in the Adam optimizer by using <kbd>apply_gradients()</kbd>. This will compute the policy gradients:</p>
<pre># get gradients from local networks using local losses; clip them to avoid exploding gradients<br/>local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)<br/>self.gradients = tf.gradients(self.loss,local_vars)<br/>self.var_norms = tf.global_norm(local_vars)<br/>grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)<br/>                <br/># apply local gradients to global network using tf.apply_gradients()<br/>global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')<br/>self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Worker() class</h1>
                </header>
            
            <article>
                
<p>We will now describe the <kbd>Worker()</kbd> class, which each worker thread will use. First, we define the <kbd>__init__()</kbd> constructor for the class. Inside of it, we define the worker name, the number, the model path, the Adam optimizer, the count of global episodes, and the operator to increment it:</p>
<pre>class Worker():<br/>    def __init__(self,env,name,s_size,a_size,trainer,model_path,global_episodes):<br/>        self.name = "worker_" + str(name)<br/>        self.number = name <br/>        self.model_path = model_path<br/>        self.trainer = trainer<br/>        self.global_episodes = global_episodes<br/>        self.increment = self.global_episodes.assign_add(1)</pre>
<p>We also create the local instance of the <kbd>AC</kbd> class, with the appropriate arguments passed in. We then create a TensorFlow operation to copy the model parameters from global to local. We also create a NumPy identity matrix with ones on the diagonal, as well as an environment object:</p>
<pre># local copy of the AC network <br/>self.local_AC = AC(s_size,a_size,self.name,trainer)<br/><br/># tensorflow op to copy global params to local network<br/>self.update_local_ops = update_target_graph('global',self.name) <br/>        <br/>self.actions = np.identity(a_size,dtype=bool).tolist()<br/>self.env = env</pre>
<p>Next, we create the <kbd>train()</kbd> function, which is the most important part of the <kbd>Worker</kbd> class. The states, actions, rewards, next states, or observations and values are obtained from the experience list that's received as an argument by the function. We computed the discounted sum over the rewards by using a utility function called <kbd>discount()</kbd>, which we will define soon. Similarly, the <kbd>advantage</kbd> function is also discounted:</p>
<pre># train function<br/>    def train(self,experience,sess,gamma,bootstrap_value):<br/>        experience = np.array(experience)<br/>        observations = experience[:,0]<br/>        actions = experience[:,1]<br/>        rewards = experience[:,2]<br/>        next_observations = experience[:,3]<br/>        values = experience[:,5]<br/>        <br/>        # discounted rewards<br/>        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])<br/>        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]<br/><br/>        # value <br/>        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])<br/><br/>        # advantage function <br/>        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]<br/>        advantages = discount(advantages,gamma)</pre>
<p>We then update the global network parameters by calling the TensorFlow operations that we defined earlier, along with the required input to the placeholders that are passed using TensorFlow's <kbd>feed_dict</kbd> function. Note that since we have multiple worker threads performing this update on the master parameters, we need to avoid conflict. In other words, only one thread can update the master network parameters at a given time instant; two or more threads doing this update at the same time will not update the global parameters one after the other, and it can cause problems if one thread updates the global parameters while the other thread is in the process of updating the same. This means that the former's update will be overwritten by the latter, which we do not desire. This is accomplished using Python's threading library's <kbd>Lock()</kbd> function. We create an instance of <kbd>Lock()</kbd> called <kbd>lock</kbd>. <kbd>lock.acquire()</kbd> will grant access to the current thread only, which will perform the update, after which it will release the lock using <kbd>lock.release()</kbd>. Finally, we return the losses from the function:</p>
<pre># lock for updating global params<br/>lock = Lock()<br/>lock.acquire() <br/><br/># update global network params<br/>fd = {self.local_AC.target_v:discounted_rewards, self.local_AC.inputs:np.vstack(observations), self.local_AC.actions:actions, self.local_AC.advantages:advantages}<br/>value_loss, policy_loss, entropy, _, _, _ = sess.run([self.local_AC.value_loss, self.local_AC.policy_loss, self.local_AC.entropy, self.local_AC.grad_norms, self.local_AC.var_norms, self.local_AC.apply_grads], feed_dict=fd)<br/> <br/># release lock<br/>lock.release() <br/><br/>return value_loss / len(experience), policy_loss / len(experience), entropy / len(experience)</pre>
<p>Now, we need to define the workers' <kbd>work()</kbd> function. We first obtain the global episode count and set <kbd>total_steps</kbd> to zero. Then, inside a TensorFlow session, while the threads are still coordinated, we copy the global parameters to the local network using <kbd>self.update_local_ops</kbd>. We then start an episode. Since the episode hasn't been terminated, we obtain the policy distribution and store it in <kbd>a_dist</kbd>. We sample an action from this distribution using NumPy's <kbd>random.choice()</kbd> function. This action, <kbd>a</kbd>, is fed into the environment's <kbd>step()</kbd> function to obtain the new state, the reward, and the Terminal Boolean. We can shape the reward by dividing it by <kbd>100.0</kbd>.</p>
<p>The experience is stored in the local buffer, called <kbd>episode_buffer</kbd>. We also add the reward to <kbd>episode_reward</kbd>, and we increment the <kbd>total_steps</kbd> count, as well as <kbd>episode_step_count</kbd>:</p>
<pre># worker's work function<br/>def work(self,max_episode_steps, gamma, sess, coord, saver):<br/>    episode_count = sess.run(self.global_episodes)<br/>    total_steps = 0<br/>    print ("Starting worker " + str(self.number))<br/><br/>        with sess.as_default(), sess.graph.as_default(): <br/>            while not coord.should_stop():<br/>          <br/>                # copy global params to local network <br/>                sess.run(self.update_local_ops)<br/><br/>                # lists for book keeping<br/>                episode_buffer = []<br/>                episode_values = []<br/>                episode_frames = []<br/><br/>                episode_reward = 0<br/>                episode_step_count = 0<br/>                d = False<br/>                <br/>                s = self.env.reset()<br/>                episode_frames.append(s)<br/><br/>                <br/>                while not d:<br/>            <br/>                    # action and value<br/>                    a_dist, v = sess.run([self.local_AC.policy,self.local_AC.value], feed_dict={self.local_AC.inputs:[s]})<br/>                    a = np.random.choice(np.arange(len(a_dist[0])), p=a_dist[0])<br/><br/><br/>                    if (self.name == 'worker_0'):<br/>                       self.env.render()<br/>                      <br/>                    # step<br/>                    s1, r, d, info = self.env.step(a)<br/>                      <br/>                    # normalize reward<br/>                    r = r/100.0<br/><br/>                    if d == False:<br/>                        episode_frames.append(s1)<br/>                    else:<br/>                        s1 = s<br/>                        <br/>                    # collect experience in buffer <br/>                    episode_buffer.append([s,a,r,s1,d,v[0,0]])<br/><br/>                    episode_values.append(v[0,0])<br/><br/>                    episode_reward += r<br/>                    s = s1 <br/>                    total_steps += 1<br/>                    episode_step_count += 1</pre>
<p>If we have <kbd>25</kbd> entries in the buffer, it's time for an update. First, the value is computed and stored in <kbd>v1</kbd>, which is then passed to the <kbd>train()</kbd> function, which will output the three loss values: value, policy, and entropy. After this, the <kbd>episode_buffer</kbd> is reset. If the episode has terminated, we break from the loop. Finally, we print the episode count and reward on the screen. Note that we have used <kbd>25</kbd> entries as the time to do the update. Feel free to vary this and see how the training is affected by this hyperparameter:</p>
<pre># if buffer has 25 entries, time for an update <br/>if len(episode_buffer) == 25 and d != True and episode_step_count != max_episode_steps - 1:<br/>    v1 = sess.run(self.local_AC.value, feed_dict={self.local_AC.inputs:[s]})[0,0]<br/>    value_loss, policy_loss, entropy = self.train(episode_buffer,sess,gamma,v1)<br/>    episode_buffer = []<br/>    sess.run(self.update_local_ops)<br/> <br/># idiot check to ensure we did not miss update for some unforseen reason <br/>if (len(episode_buffer) &gt; 30):<br/>    print(self.name, "buffer full ", len(episode_buffer))<br/>    sys.exit()<br/><br/>if d == True:<br/>    break<br/><br/>                <br/>print("episode: ", episode_count, "| worker: ", self.name, "| episode reward: ", episode_reward, "| step count: ", episode_step_count)</pre>
<p>After exiting the episode loop, we use the remaining samples in the buffer to train the network. <kbd>worker _0</kbd> contains the global or master network, which we can save by using <kbd>saver.save</kbd>. We can also call the <kbd>self.increment</kbd> operation to increment the global episode count by one:</p>
<pre># Update the network using the episode buffer at the end of the episode<br/>if len(episode_buffer) != 0:<br/>    value_loss, policy_loss, entropy = self.train(episode_buffer,sess,gamma,0.0)<br/>                                <br/>                <br/>print("loss: ", self.name, value_loss, policy_loss, entropy)<br/><br/># write to file for worker_0<br/>if (self.name == 'worker_0'): <br/>    with open("performance.txt", "a") as myfile:<br/>        myfile.write(str(episode_count) + " " + str(episode_reward) + " " + str(episode_step_count) + "\n")<br/><br/># save model params for worker_0<br/>if (episode_count % 25 == 0 and self.name == 'worker_0' and episode_count != 0):<br/>        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')<br/>print ("Saved Model")<br/>                    <br/>if self.name == 'worker_0':<br/>    sess.run(self.increment)<br/>episode_count += 1</pre>
<p>That's it for <kbd>a3c.py</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding utils.py</h1>
                </header>
            
            <article>
                
<p>Last but not least, we will code the <kbd>utility</kbd> functions in <kbd>utils.py</kbd>. We will import the necessary packages, and we'll also define the <kbd>update_target_graph()</kbd> function that we used earlier. It takes the scope of the source and destination parameters as arguments, and it copies the parameters from the source to the destination:</p>
<pre>import numpy as np<br/>import tensorflow as tf<br/>from random import choice<br/><br/><br/><br/># copy model params <br/>def update_target_graph(from_scope,to_scope):<br/>    from_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)<br/>    to_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)<br/><br/>    copy_ops = []<br/>    for from_param,to_param in zip(from_params,to_params):<br/>        copy_ops.append(to_param.assign(from_param))<br/>    return copy_ops</pre>
<p>The other utility function that we need is the <kbd>discount()</kbd> function. It runs the input list, <kbd>x</kbd> backwards, and sums them with a weight of <kbd>gamma</kbd>, which is the discount factor. The discounted value is then returned from the function:</p>
<pre><br/># Discounting function used to calculate discounted returns.<br/>def discount(x, gamma):<br/>    dsr = np.zeros_like(x,dtype=np.float32)<br/>    running_sum = 0.0<br/>    for i in reversed(range(0, len(x))):<br/>       running_sum = gamma * running_sum + x[i]<br/>       dsr[i] = running_sum <br/>    return dsr</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training on CartPole</h1>
                </header>
            
            <article>
                
<p>The code for <kbd>cartpole.py</kbd> can be run using the following command:</p>
<pre><strong>python cartpole.py</strong></pre>
<p>The code stores the episode rewards in the <kbd>performance.txt</kbd> file. A plot of the episode rewards during training is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-520 image-border" src="assets/0d3de487-a0c6-42e0-8e2e-f899dc923ec4.png" style="width:27.67em;height:20.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: Episode rewards for CartPole, which was trained using A3C</div>
<p>Note that since we have shaped the reward, the episode reward that you can see in the preceding screenshot is different from the values that are typically reported by other researchers in papers and/or blogs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The A3C algorithm applied to LunarLander</h1>
                </header>
            
            <article>
                
<p>We will extend the same code to train an agent on the LunarLander problem, which is harder than CartPole. Most of the code is the same as before, so we will only describe the changes that need to be made to the preceding code. First, the reward shaping is different for the LunarLander problem. So, we will include a function called <kbd>reward_shaping()</kbd> in the <kbd>a3c.py</kbd> file. It will check if the lander has crashed on the lunar surface; if so, the episode will be terminated and there will be a <kbd>-1.0</kbd> penalty. If the lander is not moving, the episode will be terminated and a <kbd>-0.5</kbd> penalty will be paid:</p>
<pre>def reward_shaping(r, s, s1):<br/>     # check if y-coord &lt; 0; implies lander crashed<br/>     if (s1[1] &lt; 0.0):<br/>       print('-----lander crashed!----- ')<br/>       d = True <br/>       r -= 1.0<br/><br/>     # check if lander is stuck<br/>     xx = s[0] - s1[0]<br/>     yy = s[1] - s1[1]<br/>     dist = np.sqrt(xx*xx + yy*yy) <br/>     if (dist &lt; 1.0e-4):<br/>       print('-----lander stuck!----- ')<br/>       d = True <br/>       r -= 0.5<br/>     return r, d</pre>
<p>We will call this function after <kbd>env.step()</kbd>:</p>
<pre># reward shaping for lunar lander<br/>r, d = reward_shaping(r, s, s1)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding lunar.py</h1>
                </header>
            
            <article>
                
<p>The <kbd>cartpole.py</kbd> file from the previous exercise has been renamed to <kbd>lunar.py</kbd>. The changes that have been made are as follows. First, we set the maximum number of time steps per episode to <kbd>1000</kbd> for LunarLander, the discount factor to <kbd>gamma = 0.999</kbd>, and the state and action dimensions to <kbd>8</kbd> and <kbd>4</kbd>, respectively:</p>
<pre>max_episode_steps = 1000<br/>gamma = 0.999<br/>s_size = 8 <br/>a_size = 4</pre>
<p>The environment is set to <kbd>LunarLander-v2</kbd>:</p>
<pre>env = gym.make('LunarLander-v2')</pre>
<p>That's it for the code changes for training A3C on LunarLander.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training on LunarLander</h1>
                </header>
            
            <article>
                
<p>You can start the training by using the following command:</p>
<pre><strong>python lunar.py</strong></pre>
<p>This will train the agent and store the episode rewards in the <kbd>performance.txt</kbd> file, which we can plot as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-521 image-border" src="assets/d9abb1ac-a778-47d5-86bd-c02003be013a.png" style="width:24.75em;height:19.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2: Episode rewards for LunarLander using A3C</div>
<p>As you can see, the agent has learned to land the spacecraft on the lunar surface. Happy landings! Again, note that the episode reward is different from the values that have been reported in papers and blogs by other RL practitioners, since we have scaled the rewards.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The A2C algorithm</h1>
                </header>
            
            <article>
                
<p>The difference between A2C and A3C is that A2C performs synchronous updates. Here, all the workers will wait until they have completed the collection of experiences and computed the gradients. Only after this are the global (or master) network's parameters updated. This is different from A3C, where the update is performed asynchronously, that is, where the worker threads do not wait for the others to finish. A2C is easier to code than A3C, but that is not undertaken here. If you are interested in this, you are encouraged to take the preceding A3C code and convert it to A2C, after which the performance of both algorithms can be compared.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the A3C algorithm, which is an on-policy algorithm that's applicable to both discrete and continuous action problems. You saw how three different loss terms are combined into one and optimized. Python's threading library is useful for running multiple threads, with a copy of the policy network in each thread. These different workers compute the policy gradients and pass them on to the master to update the neural network parameters. We applied A3C to train agents for the CartPole and the LunarLander problems, and the agents learned them very well. A3C is a very robust algorithm and does not require a replay buffer, although it does require a local buffer for collecting a small number of experiences, after which it is used to update the networks. Lastly, a synchronous version of the algorithm, called A2C, was also introduced.</p>
<p>This chapter should have really improved your understanding of yet another deep RL algorithm. In the next chapter, we will study the last two RL algorithms in this book, TRPO and PPO.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Is A3C an on-policy or off-policy algorithm?</li>
<li>Why is the Shannon entropy term used?</li>
<li>What are the problems with using a large number of worker threads?</li>
<li>Why is softmax used in the policy neural network?</li>
<li>Why do we need an <kbd>advantage</kbd> function?</li>
<li>This is left as an exercise: For the LunarLander problem, repeat the training without reward shaping and see if the agent learns faster/slower than what we saw in this chapter.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Asynchronous Methods for Deep Reinforcement Learning</em>, by <em>Volodymyr Mnih</em>, <em>Adrià Puigdomènech Badia</em>, <em>Mehdi Mirza</em>, <em>Alex Graves</em>, <em>Timothy P. Lillicrap</em>, <em>Tim Harley</em>, <em>David Silver</em>, and <em>Koray Kavukcuoglu</em>, A3C paper from <em>DeepMind</em> arXiv:1602.01783: <a href="https://arxiv.org/abs/1602.01783" target="_blank">https://arxiv.org/abs/1602.01783</a></span></li>
<li><em>Deep Reinforcement Learning Hands-On</em>, by <em>Maxim Lapan</em>, <em>Packt Publishing</em>: <a href="https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands</a></li>
</ul>


            </article>

            
        </section>
    </body></html>