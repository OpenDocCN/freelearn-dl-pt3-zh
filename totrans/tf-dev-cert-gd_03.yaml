- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Regression with TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover the concept of linear regression and how we can
    implement it using TensorFlow. We will start by discussing what linear regression
    is, how it works, its underlying assumptions, and the type of problems that can
    be solved using it. Next, we will examine the various evaluation metrics used
    in regression modeling, such as mean squared error, mean absolute error, root
    mean squared error, and R-squared, and strive to understand how to interpret the
    results from these metrics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: To get hands-on, we will implement linear regression by building a real-world
    use case where we predict employees’ salaries using various attributes. Here,
    we will learn in a hands-on fashion how to load and pre-process data, covering
    important ideas such as handling missing values, encoding categorical variables,
    and normalizing the data for modeling. Then, we will explore the process of building,
    compiling, and fitting a linear regression model with TensorFlow, as well as examine
    concepts such as underfitting and overfitting and their impact on our model’s
    performance. Before we close this chapter, you will also learn how to save and
    load a trained model to make predictions on unseen data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating regression models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salary prediction with TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving and loading models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use a `python >= 3.8.0`, along with the following packages, which can
    be installed using the `pip` `install` command:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow>=2.7.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow-datasets==4.4.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pillow==8.4.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas==1.3.4`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy==1.21.4`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scipy==1.7.3`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression with TensorFlow
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Linear regression** is a supervised machine learning technique that models
    the linear relationship between the predicted output variable (dependent variable)
    and one or more independent variables. When one independent variable can be used
    to effectively predict the output variable, we have a case of *simple linear regression*,
    which can be represented by the equation *y = wX + b*, where *y* is the target
    variable, *X* is the input variable, *w* is the weight of the feature(s), and
    *b* is the bias.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – A plot showing simple linear regression](img/B18118_03_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – A plot showing simple linear regression
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.1*, the straight line, referred to as the regression line (the
    line of best fit), is the line that optimally models the relationship between
    *X* and *y*. Hence, we can use it to determine the dependent variable based on
    the current value of the independent variable at a certain point on the plot.
    The objective of linear regression is to find the best values of *w* and *b*,
    which model the underlying relationship between *X* and *y*. The closer the predicted
    value is to the ground truth, the smaller the error.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, when we have more than one input variable used to predict the output
    value, then we have a case of *multiple linear regression*, and we can represent
    it by the equation *y = b0 + b1X1 + b2X2 + .... + bnXn*, where *y* is the target
    variable, *X1*, *X2*, ... *Xn* are input variables, *b0* is the bias, and *b1*,
    *b2*, ... *bn* are the feature weights.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 反之，当我们有多个输入变量来预测输出值时，就出现了*多重线性回归*的情况，我们可以用方程 *y = b0 + b1X1 + b2X2 + .... +
    bnXn* 来表示，其中 *y* 是目标变量，*X1*、*X2*、... *Xn* 是输入变量，*b0* 是偏差项，*b1*、*b2*、... *bn* 是特征权重。
- en: Simple linear and multiple linear regression have lots of real-world applications,
    as they are simple to implement and computationally cheap. Hence, they can be
    easily applied to large datasets. However, linear regression may fail when we
    try to model nonlinear relationships between *X* and *y*, or when there are many
    irrelevant features in our input data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归和多重线性回归有很多实际应用，因为它们易于实现且计算成本低。因此，它们可以很容易地应用于大规模数据集。然而，当我们试图建立 *X* 和 *y*
    之间的非线性关系模型，或者输入数据中包含大量无关特征时，线性回归可能会失效。
- en: Linear regression is widely used to solve a wide range of real-world problems
    across different domains. For example, we can apply linear regression to predict
    the price of a house using factors such as the size, number of bedrooms, location,
    and proximity to social amenities. Similarly, in the field of **human resource**
    (**HR**), we can use linear regression to predict the salary of new hires, based
    on factors such as years of experience of the candidate and their level of education.
    These are a few examples of what is possible using linear regression. Next, let
    us see how we can evaluate linear regression models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归广泛应用于解决各个领域的实际问题。例如，我们可以应用线性回归预测房屋价格，考虑因素如房屋大小、卧室数量、位置以及距离社会设施的远近。同样，在**人力资源**（**HR**）领域，我们可以利用线性回归预测新员工的薪资，考虑因素包括候选人的工作经验年限和教育水平。这些都是使用线性回归可以实现的一些例子。接下来，让我们看看如何评估线性回归模型。
- en: Evaluating regression models
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: In our *hello world* example from [*Chapter 2*](B18118_02.xhtml#_idTextAnchor045),
    *Introduction to TensorFlow*, we tried to predict a student’s test score when
    the student spent 38 hours studying during the term. Our study model arrived at
    81.07 marks, while the true value was 81\. So, we were close but not completely
    correct. When we subtract the difference between our model’s prediction and the
    ground truth, we get a residual of 0.07\. The residual value could be either positive
    or negative, depending on whether our model overestimates or underestimates the
    predicted result. When we take the absolute value of the residual, we eliminate
    any negative signs; hence, the absolute error will always be a positive value,
    irrespective of whether the residual is positive or negative.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们来自 [*第2章*](B18118_02.xhtml#_idTextAnchor045) 的 *hello world* 示例中，*TensorFlow简介*，我们尝试预测学生在学期内学习了38小时后，考试成绩是多少。我们的模型得出了81.07分，而真实值是81分。因此，我们很接近，但并不完全正确。当我们减去模型预测值和真实值之间的差异时，我们得到了一个残差0.07。残差值可以是正数也可以是负数，具体取决于我们的模型是高估还是低估了预测结果。当我们取残差的绝对值时，就消除了任何负号；因此，绝对误差总是一个正值，无论残差是正还是负。
- en: 'The formula for absolute error is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差的公式如下：
- en: Absolute error = |Y pred − Y true|
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差 = |Y pred − Y true|
- en: where Y pred = the predicted value and Y true = the ground truth.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Y pred = 预测值，Y true = 真实值。
- en: 'The **mean absolute error** (**MAE**) of a model is the average of all absolute
    errors of the data points under consideration. MAE measures the average of the
    residuals and can be represented using the following equation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是模型的所有数据点的绝对误差的平均值。MAE 衡量的是残差的平均值，可以通过以下公式表示：'
- en: MAE =  1 _ n  ∑ i=1 n  |Y pred − Y true|
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: MAE = 1 _ n ∑ i=1 n |Y pred − Y true|
- en: 'where:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*n* = the number of data points under consideration'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* = 所考虑的数据点数量'
- en: ∑ = summation of the absolute errors of all the observations
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∑ = 所有观测值的绝对误差之和
- en: '|Y pred − Y true| = Absolute value'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|Y pred − Y true| = 绝对值'
- en: If the MAE = 0, it means that Y pred = Y true. This means the model is 100 percent
    accurate; although this is an ideal scenario, it is highly unlikely. On the flip
    side, if MAE= ∞, this means the model is completely off, as it fails to capture
    any relationship between the input and output variables. The larger the error,
    the larger the value of the MAE. For performance evaluation, we aim for low values
    of MAE, but because MAE is a relative metric whose value depends on the scale
    of the data you work with, it is difficult to compare MAE results across different
    datasets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果MAE = 0，这意味着Y_pred = Y_true。这意味着模型的预测完全准确；尽管这是一个理想的情况，但极不可能发生。相反，如果MAE = ∞，则表示模型完全失败，因为它未能捕捉到输入与输出变量之间的任何关系。误差越大，MAE的值也越大。在性能评估中，我们希望MAE的值较低，但由于MAE是一个相对度量，其值取决于所处理数据的规模，因此很难在不同数据集之间比较MAE的结果。
- en: 'Another important evaluation metric is the **mean squared error** (**MSE**).
    MSE, in contrast to MAE, squares the residuals, thus removing any negative values
    in the residuals. MSE is represented using the following equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的评估指标是**均方误差**（**MSE**）。与MAE不同，MSE对残差进行了平方处理，从而去除了残差中的负值。MSE用以下公式表示：
- en: MSE =  1 _ N  ∑ i=1 N ( Y pred − Y true) 2
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: MSE = 1 _ N ∑ i=1 N ( Y pred − Y true) 2
- en: Like MAE, when there are no residuals, we have a perfect model. So, the lower
    the MSE value, the better the performance of the model. Unlike MAE, where large
    or small errors have a proportional impact, MSE penalizes larger errors in comparison
    to smaller errors, and it has a higher order of units, since we square the residual
    in this instant.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与MAE类似，当没有残差时，我们有一个完美的模型。因此，MSE值越低，模型性能越好。与MAE不同，MAE中的大误差或小误差对结果有相同比例的影响，而MSE会对较大的误差进行惩罚，相较于小误差，它具有更高的单位阶数，因为我们在此情况下对残差进行了平方。
- en: 'Another useful metric in regression modeling is the **root mean square error**
    (**RMSE**). As the name suggests, it is the square root of MSE, as shown in the
    equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 回归建模中另一个有用的指标是**均方根误差**（**RMSE**）。顾名思义，它是MSE的平方根，如下所示的公式所示：
- en: MSE =  1 _ N  ∑ i=1 N ( Y pred − Y true) 2
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MSE = 1 _ N ∑ i=1 N ( Y pred − Y true) 2
- en: RMSE = √ _ MSE  = √ ________________   1 _ N  ∑ i=1 N ( Y pred − Y true) 2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE = √_ MSE = √ ________________ 1 _ N ∑ i=1 N ( Y pred − Y true) 2
- en: 'Lastly, let us look at the **coefficient of determination** (**R squared**).
    R2 measures how well the dependent variable is explained by the independent variables
    in a regression modeling task. We can calculate R2 with this equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看一下**决定系数**（**R平方**）。R²衡量回归建模任务中，因变量在多大程度上可以由自变量解释。我们可以通过以下公式计算R²：
- en: R 2 = 1 −  R res _ R tot
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: R 2 = 1 − R res _ R tot
- en: where Rres is the sum of the square of residuals and Rtot is the total sum of
    squares. The closer the value of R2 is to 1, the more accurate the model is, and
    the closer the R2 value of a model is to 0, the worse the model is. Also, it is
    possible for R² to take on a negative value. This happens when the model does
    not follow the trend of the data – in this instance, Rres is greater than Rtot.
    A negative R2 is a sign that our model requires significant improvement due to
    its poor performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Rres是残差平方和，Rtot是总平方和。R²值越接近1，模型越准确；R²值越接近0，模型越差。此外，R²值也可能是负数。这发生在模型未能遵循数据趋势的情况下——此时，Rres大于Rtot。负的R²是模型性能差的标志，说明我们的模型需要显著改进。
- en: We have looked at some regression evaluation metrics. The good news is that
    we will not work them out by hand; we will leverage the `tf.keras.metrics` module
    from TensorFlow to help us do the heavy lifting. We have breezed quickly through
    the theory at a high level. Now, let us examine a multiple linear regression case
    study to enable us to understand all the moving parts required to build a model
    with TensorFlow, as well as understand how to evaluate, save, load, and use our
    trained model to make predictions on new data. Let’s proceed to our case study.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了一些回归评估指标。好消息是，我们不需要手动计算它们；我们将利用TensorFlow中的`tf.keras.metrics`模块来帮助我们完成这些繁重的计算。我们已经从高层次快速浏览了理论。接下来，让我们通过一个多元线性回归的案例研究，帮助我们理解构建TensorFlow模型所需的各个部分，同时了解如何评估、保存、加载并使用训练好的模型来对新数据进行预测。让我们继续进行案例研究。
- en: Salary prediction with TensorFlow
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行薪资预测
- en: In this case study, you will assume the role of a new machine learning engineer
    at Tensor Limited, a rapidly growing start-up with over 200 employees. Now, the
    company wants to hire seven new employees, and the HR department is having a hard
    time coming up with the ideal salary based on varying qualifications, years of
    experience, the roles applied for, and the level of training of each of the potential
    new hires. Your job is to work with the HR unit to determine the optimal salary
    for each of these potential hires.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，你将假设自己是Tensor Limited（一个快速增长的初创公司，拥有200多名员工）的新晋机器学习工程师。目前，公司希望招聘7名新员工，人力资源部门难以根据不同的资质、工作经验、申请职位以及每个潜在新员工的培训水平来确定理想的薪资。你的任务是与人力资源部门合作，为这些潜在新员工确定最优薪资。
- en: Luckily, we went through the machine learning life cycle in [*Chapter 1*](B18118_01.xhtml#_idTextAnchor014),
    *Introduction to Machine Learning,* built our hello world case study in [*Chapter
    2*](B18118_02.xhtml#_idTextAnchor045), *Introduction to TensorFlow,* and have
    already covered some key evaluation metrics required for regression modeling in
    this chapter. So, you are well equipped theoretically to carry out the task. You
    have had a productive discussion with the HR manager, and now you have a better
    understanding of the task and the requirements. You defined your task as a supervised
    learning task (regression). Also, the HR unit allowed you to download employee
    records and their corresponding salaries for this task. Now that you have the
    dataset, let us proceed to load the data into our notebook.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们已经在[*第1章*](B18118_01.xhtml#_idTextAnchor014)《机器学习简介》中讲解了机器学习生命周期，[*第2章*](B18118_02.xhtml#_idTextAnchor045)《TensorFlow简介》里也构建了我们的Hello
    World案例研究，并且在本章中已经涵盖了回归建模所需的一些关键评估指标。因此，从理论上讲，你已经充分准备好执行这个任务。你已经与人力资源经理进行了富有成效的讨论，现在对任务和要求有了更清晰的理解。你将任务定义为一个监督学习任务（回归）。同时，人力资源部门允许你下载员工记录和相应的薪资数据来完成这个任务。现在你已经拥有数据集，让我们继续将数据加载到笔记本中。
- en: Loading the data
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'Perform the following steps to load the dataset:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以加载数据集：
- en: 'Open the notebook called `Linear_Regression_with_TensorFlow.ipynb`. We will
    start by importing all the necessary libraries for this project:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开名为`Linear_Regression_with_TensorFlow.ipynb`的笔记本。我们将从导入所有必要的库开始：
- en: '[PRE0]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will run this code block. If everything goes well, we will get to see the
    version of TensorFlow we are using:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行这段代码块。如果一切顺利，我们将看到我们正在使用的TensorFlow版本：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will import some additional libraries that will help us simplify our
    workflow:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将导入一些额外的库，这些库将帮助我们简化工作流程：
- en: '[PRE7]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We will run this cell, and everything should work perfectly. NumPy is a scientific
    computing library in Python that is used to perform mathematical operations on
    arrays, while pandas is a built-in Python library for data analysis and manipulation.
    Matplotlib and Seaborn are used to visualize data, and we will use sklearn for
    data preprocessing and splitting our data. We will apply these libraries in this
    case study, and you will get to understand what they do and also be able to apply
    them in your exam and beyond.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行这个单元格，应该一切正常。NumPy是一个Python中的科学计算库，用于对数组执行数学运算，而pandas是Python内置的用于数据分析和处理的库。Matplotlib和Seaborn用于数据可视化，我们将使用sklearn进行数据预处理和数据拆分。在这个案例研究中，我们将应用这些库，你将了解它们的作用，并且能够在考试中以及之后的工作中应用它们。
- en: 'Now, we will proceed to load the dataset, which we got from the HR team for
    this project:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将继续加载数据集，这个数据集是我们从人力资源团队获取的，用于这个项目：
- en: '[PRE13]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will use pandas to generate a DataFrame that holds the record in a tabular
    format, and we will use `df.head()` to print the first five entries in the data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用pandas生成一个DataFrame，以表格格式保存记录，并将使用`df.head()`打印数据中的前五个条目：
- en: '![Figure 3.2 – A DataFrame showing a snapshot of our dataset](img/B18118_03_002.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 显示数据集快照的DataFrame](img/B18118_03_002.jpg)'
- en: Figure 3.2 – A DataFrame showing a snapshot of our dataset
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 显示数据集快照的DataFrame
- en: We now have a sense of what data was collected, based on the details captured
    in each column. We will proceed to explore the data to see what we can learn and
    how we can effectively develop a solution to meet the business objective. Let
    us proceed by looking at data pre-processing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每一列捕捉的细节，我们现在对收集的数据有了初步了解。接下来，我们将继续探索数据，看看我们能学到什么，并且如何有效地开发一个解决方案来实现业务目标。让我们通过查看数据预处理部分继续。
- en: Data preprocessing
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'To be able to model our data, we need to ensure it is in the right form (i.e.,
    numerical values). Also, we will need to deal with missing values and remove irrelevant
    features. In the real world, data preprocessing takes a long time. You will hear
    this repeatedly, and it is true. Without correctly shaping the data, we cannot
    model it. Let’s jump in and see how we can do this for our current task. From
    the DataFrame, we can immediately see that there are some irrelevant columns,
    and they hold personally identifiable information about employees. So, we will
    remove and also inform HR about this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够对数据进行建模，我们需要确保数据是正确的格式（即数值型数据）。此外，我们还需要处理缺失值并去除无关的特征。在实际应用中，数据预处理通常需要很长时间。你会反复听到这一点，且这是真的。如果数据没有正确地整理，我们就无法进行建模。让我们深入了解一下，看看如何为当前任务做这些操作。从DataFrame中，我们可以立即看到一些无关的列，它们包含员工的个人身份信息。因此，我们将删除这些列，并告知人力资源部门：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will use the `drop` function in pandas to drop the name, phone number, and
    date of birth columns. We will now display the DataFrame again using `df.head()`
    to show the first five rows of the data:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用pandas中的`drop`函数删除姓名、电话号码和出生日期列。接下来，我们将再次使用`df.head()`显示DataFrame，展示数据的前五行：
- en: '![Figure 3.3 – The first five rows of the DataFrame after dropping the columns](img/B18118_03_003.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 删除列后的DataFrame前五行](img/B18118_03_003.jpg)'
- en: Figure 3.3 – The first five rows of the DataFrame after dropping the columns
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 删除列后的DataFrame前五行
- en: 'We have successfully removed the irrelevant columns, so we can now proceed
    and check for missing values in our dataset using the `isnull()` function in pandas:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功删除了无关的列，现在可以继续使用pandas中的`isnull()`函数检查数据集中的缺失值：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'When we run this code block, we can see that there are no missing values in
    the `University` and `Salary` columns. However, we have missing values for the
    `Role`, `Cert`, `Qualification`, and `Experience` columns:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个代码块时，可以看到`University`和`Salary`列没有缺失值。然而，`Role`、`Cert`、`Qualification`和`Experience`列存在缺失值：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'There are a number of ways to handle missing values – from simply asking HR
    to fix the omissions to simple imputations or replacements using mean, median,
    or mode. In this case study, we will drop the rows with missing values, since
    it’s a small subset of our data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值有多种方法——从简单地要求HR修复遗漏，到使用均值、中位数或众数进行简单的填补或替换。在这个案例研究中，我们将删除含有缺失值的行，因为它是我们数据的一个小子集：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We use the `dropna` function to drop all the missing values in the dataset,
    and then we save the new dataset in `df`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`dropna`函数删除数据集中的所有缺失值，然后将新的数据集保存为`df`。
- en: Note
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you want to learn more about how to handle missing values, check out this
    playlist by Data Scholar: [https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u](https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于如何处理缺失值的内容，可以查看Data Scholar的这个播放列表：[https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u](https://www.youtube.com/playlist?list=PLB9iiBW-oO9eMF45oEMB5pvC7fsqgQv7u)。
- en: 'Now, we need to check to ensure that there are no more missing values using
    the `isnull()` function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要检查确保没有更多的缺失值，使用`isnull()`函数：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run the code, and let’s see whether there are any missing values:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码，看看是否还有缺失值：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can see that there are no missing values in our dataset anymore. Our model
    requires us to pass in numerical values for it to be able to model our data and
    predict the target variable, so let us look at the data types:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据集中不再有缺失值。我们的模型要求输入数值型数据，才能对数据进行建模并预测目标变量，因此让我们来看看数据类型：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'When we run the code, we get an output showing the different columns and their
    data types:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们会得到一个输出，显示不同的列及其数据类型：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'From the output, we can see that experience and salary are numeric values,
    since they are `float` and `int`, respectively, while `Qualification`, `University`,
    `Role`, and `Cert` are categorical values. This means we cannot train our model
    yet; we have to find a way to convert our categorical values to numerical values.
    Luckily, this is possible via a process called one-hot encoding. `get_dummies`
    function in pandas to achieve this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到`experience`和`salary`是数值型数据，因为它们分别是`float`和`int`类型，而`Qualification`、`University`、`Role`和`Cert`是分类数据。这意味着我们还不能训练模型；我们必须找到一种方法将分类数据转换为数值数据。幸运的是，这可以通过一个叫做独热编码（one-hot
    encoding）的过程来实现。我们可以使用pandas中的`get_dummies`函数来完成这一任务：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When we run the code, we will get a DataFrame like the one displayed in *Figure
    3**.4*. We use the `drop_first` argument to drop the first category.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们将得到一个类似于*图 3.4*中显示的DataFrame。我们使用`drop_first`参数来删除第一类。
- en: '![Figure 3.4 – A DataFrame showing numerical values](img/B18118_03_004.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 显示数值的 DataFrame](img/B18118_03_004.jpg)'
- en: Figure 3.4 – A DataFrame showing numerical values
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 显示数值的 DataFrame
- en: If you are confused as to why we dropped one of the categorical columns, let’s
    look at the `Cert` column, which was made up of yes or no. values If we performed
    one hot encoding, without dropping any columns, we would have two `Cert` columns,
    as displayed in *Figure 3**.5*. In the `Cert_No` column, if the employee has a
    relevant certification, the column gets a value of `0`, and when the employee
    does not have a relevant certification, the column gets a value of `1`. Looking
    at the `Cert_Yes` column, we can see that when an employee has a certificate,
    the column gets a value of `1`; otherwise, it gets `0`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不理解为什么我们删除了其中一个分类列，让我们来看一下`Cert`列，它由“是”或“否”值组成。如果我们进行了独热编码，但没有删除任何列，那么我们将有两个`Cert`列，如*图
    3.5*所示。在`Cert_No`列中，如果员工有相关证书，该列的值为`0`，如果员工没有相关证书，该列的值为`1`。查看`Cert_Yes`列，我们可以看到，当员工有证书时，该列的值为`1`；否则，该列的值为`0`。
- en: '![Figure 3.5 – The dummy variables from the Cert column](img/B18118_03_005.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 来自 Cert 列的虚拟变量](img/B18118_03_005.jpg)'
- en: Figure 3.5 – The dummy variables from the Cert column
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 来自 Cert 列的虚拟变量
- en: From *Figure 3**.5*, we can see that both columns can be used to show whether
    an employee has a certificate or not. Using both dummy columns generated from
    our certificate column will lead to the *dummy variable trap*. This occurs when
    our one-hot encoded columns are strongly related or correlated, where one column
    can effectively explain the other column. Hence, we say both columns are multicollinear,
    and *multicollinearity* can lead to the overfitting of our model. We will talk
    more about overfitting in [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105)*, Image
    Classification with* *Neural Networks*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 3.5*中，我们可以看到两列都可以用来显示员工是否有证书。使用从证书列生成的两个虚拟列会导致*虚拟变量陷阱*。当我们的独热编码列之间存在强相关性时，就会发生这种情况，其中一列可以有效地解释另一列。因此，我们说这两列是多重共线性的，而*多重共线性*可能导致模型过拟合。我们将在[*第五章*](B18118_05.xhtml#_idTextAnchor105)中讨论更多关于过拟合的内容，标题为“使用神经网络进行图像分类”。
- en: For now, it is good enough to know that overfitting is a situation where our
    model performs very well on training data but poorly on test data. To avoid the
    dummy variable trap, we will drop one of the columns in *Figure 3**.5*. If there
    are three categories, we only need two columns to capture all three categories;
    if we have four categories, we will only need three columns to capture four categories,
    and so on. Hence, we can drop the extra columns for all the other categorical
    columns as well.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们知道过拟合是指我们的模型在训练数据上表现非常好，但在测试数据上表现不佳的情况。为了避免虚拟变量陷阱，我们将删除*图 3.5*中的一列。如果有三类，我们只需要两列就能捕捉到所有三类；如果有四类，我们只需要三列来捕捉所有四类，依此类推。因此，我们可以删除所有其他分类列的多余列。
- en: 'Now, we will use the `corr()` function to get the correlation of our refined
    dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`corr()`函数来获取我们精炼数据集的相关性：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can see that there is a strong correlation between salary and years of experience.
    Also, there is a strong correlation between `Role_Senior` and `Salary`, as shown
    in *Figure 3**.6*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，薪资与工作经验年数之间存在强相关性。同时，`Role_Senior`与`Salary`之间也有较强的相关性，如*图 3.6*所示。
- en: '![Figure 3.6 – The correlation values for our data](img/B18118_03_006.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 我们数据的相关性值](img/B18118_03_006.jpg)'
- en: Figure 3.6 – The correlation values for our data
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 我们数据的相关性值
- en: We have completed the preprocessing phase of our task, or at least for now.
    We have removed all irrelevant columns; we also removed the missing values by
    dropping rows with missing values and, finally, used one-hot encoding to convert
    our categorical values to numeric values. It is important to note that we are
    skipping some **exploratory data analysis** (**EDA**) steps here, such as visualizing
    the data; although that’s an essential step, our core focus for the exam is building
    models with TensorFlow. In our Colab notebook, you will find some additional EDA
    steps; although they are not directly relevant to the exams, they will give you
    a better understanding of your data and help you detect anomalies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了任务的预处理阶段，至少目前是这样。我们已移除所有不相关的列；还通过删除缺失值的行来去除缺失值，最后使用独热编码将分类值转换为数值。需要注意的是，我们在这里跳过了一些**探索性数据分析**（**EDA**）步骤，例如可视化数据；虽然这些步骤很重要，但我们在考试中的核心重点是使用TensorFlow构建模型。在我们的Colab笔记本中，你会找到一些额外的EDA步骤；虽然它们与考试内容无关，但它们将帮助你更好地理解数据，并帮助你检测异常值。
- en: Now, let us move on to the modeling phase.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入建模阶段。
- en: Model building
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型构建
- en: 'To build a model, we will have to sort our data into features (*X*) and the
    target (*y*). To do this, we will run this code block:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建模型，我们必须将数据分为特征（*X*）和目标（*y*）。为此，我们将运行以下代码块：
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We will use the `drop()` function to drop the `Salary` column from the `X` variable,
    and we will make the `y` variable the `Salary` column alone, since this is our
    target.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`drop()`函数从`X`变量中删除`Salary`列，同时将`y`变量设为仅包含`Salary`列，因为这就是我们的目标变量。
- en: 'With our features and target variable well defined, we can proceed to split
    our data into training and test sets. This step is important, as it enables our
    model to learn patterns from our data to effectively predict employees’ salaries.
    To achieve this, we train our model using the training set and then evaluate the
    model’s efficacy on the hold-out test set. We discussed this at a high level in
    [*Chapter 1*](B18118_01.xhtml#_idTextAnchor014)*, Introduction to Machine Learning,*
    when we talked about the ML life cycle. It is a very important process, as we
    will use the test set to evaluate our model’s generalization capability before
    we deploy it for real-world use. To split our data into training and testing sets,
    we will use the `sklearn` library:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征和目标变量定义清楚后，我们可以继续将数据分割为训练集和测试集。这一步很重要，因为它使我们的模型能够从数据中学习模式，从而有效地预测员工的薪资。为了实现这一点，我们使用训练集训练模型，然后在留出的测试集上评估模型的效果。我们在[*第1章*](B18118_01.xhtml#_idTextAnchor014)《机器学习导论》中讨论过这一点，介绍了机器学习的生命周期。这是一个非常重要的过程，因为我们将使用测试集来评估模型的泛化能力，然后再将其部署到实际应用中。为了将数据分割为训练集和测试集，我们将使用`sklearn`库：
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Using the `train_test_split` function from the `sklearn` library, we split our
    data into training and testing datasets, with a test size of `0.2`. We set the
    `random_state =10` to ensure reproducibility so that every time you use the same
    `random_state` value, you’ll get the same split, even if you run the code multiple
    times. For instance, in our code, we set `random_state` to `10`, which means every
    time we run the code, we will get the same split. If we change this value from
    `10` to, say, `50`, we will get a different shuffled split for our training and
    test set. Setting the `random_state` argument when splitting our data into training
    and test sets is very useful, as it allows us to effectively compare different
    models, since our training set and test sets are the same across all the models
    we experiment with.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`库中的`train_test_split`函数，我们将数据分割为训练集和测试集，测试集大小为`0.2`。我们设置`random_state
    = 10`以确保可重复性，这样每次使用相同的`random_state`值时，即使多次运行代码，也能得到相同的分割。例如，在我们的代码中，我们将`random_state`设置为`10`，这意味着每次运行代码时，我们都会得到相同的分割。如果我们将这个值从`10`改为`50`，那么我们的训练集和测试集就会得到不同的随机分割。在将数据分割为训练集和测试集时，设置`random_state`参数非常有用，因为它可以确保我们有效地比较不同的模型，因为我们在所有实验模型中使用的是相同的训练集和测试集。
- en: When modeling our data in machine learning, we usually use 80 percent of the
    data to train the model and 20 percent of the data to test the model’s generalization
    capability. That’s why we set `test_size` to `0.2` for our dataset. Now that we
    have everything in place, we will start the modeling process in earnest. When
    it comes to building models with TensorFlow, there are three essential steps,
    as illustrated in *Figure 3**.7* – building the model, compiling the model, and
    fitting it to our data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中建模数据时，通常使用80%的数据来训练模型，剩下的20%用来测试模型的泛化能力。这就是为什么我们将`test_size`设置为`0.2`来处理我们的数据集。现在我们已经准备就绪，我们将认真开始建模过程。当涉及使用TensorFlow构建模型时，有三个关键步骤，如*图3**.7*所示
    – 构建模型、编译模型和将其适应我们的数据。
- en: '![Figure 3.7 – The three-step modeling process](img/B18118_03_007.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7 – 三步建模过程](img/B18118_03_007.jpg)'
- en: Figure 3.7 – The three-step modeling process
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 – 三步建模过程
- en: 'Let us see how we can use this three-step approach to build our salary prediction
    model. We will begin by building our model:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用这种三步方法来构建我们的薪资预测模型。我们将从构建我们的模型开始：
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In *Figure 3**.8*, we can see the first line of code for our model. Here, we
    generated a single layer using the `Sequential` class as an array. The `Sequential`
    class is used for layer definition. The `Dense` function is used to generate a
    layer of fully connected neuron. In this case, we have just one unit. For our
    activation function here, we employ a linear activation function. Activation functions
    are used to determine the output of a neuron based on a given input or set of
    inputs. Here, the linear activation function simply outputs whatever the input
    is – that is, a direct relationship between the input and the output. Next, we
    pass in the input shape of our data, which in this case is 8, representing the
    features in data (columns) in `X_train`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3**.8*中，我们可以看到我们模型的第一行代码。在这里，我们使用`Sequential`类作为数组生成单个层。`Sequential`类用于定义层。`Dense`函数用于生成一个全连接神经元层。在这种情况下，我们只有一个单元。在这里，我们使用线性激活函数作为激活函数。激活函数用于根据给定的输入或一组输入确定神经元的输出。在线性激活函数中，输出与输入直接相关。接下来，我们传入我们数据的输入形状，在这种情况下是8，表示`X_train`中的数据特征（列）。
- en: '![Figure 3.8 – Building a model in TensorFlow](img/B18118_03_008.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8 – 在TensorFlow中构建模型](img/B18118_03_008.jpg)'
- en: Figure 3.8 – Building a model in TensorFlow
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 – 在TensorFlow中构建模型
- en: In the first step of our three-step process, we designed the model structure.
    Now, we will move on to the model compilation step. This step is equally important
    as it determines how the model will learn. Here, we specify parameters such as
    the loss function, the optimizer, and the metrics we want to use to evaluate our
    model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们三步骤过程的第一步中，我们设计了模型结构。现在，我们将进入模型编译步骤。这一步同样重要，因为它决定了模型的学习方式。在这里，我们指定参数，如损失函数、优化器以及我们想要用来评估模型的指标。
- en: 'The optimizer determines how our model will update its internal parameters,
    based on the information it gathers from the loss function and the data. The job
    of the loss function is to measure how well our model does on our training data.
    We then use our metrics to monitor the model’s performance on the training step
    and test steps. Here, we use **stochastic gradient descent** (**SGD**) as our
    optimizer and MAE for our loss and evaluation metric:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器决定了我们的模型将如何根据损失函数和数据更新其内部参数。损失函数的作用是衡量模型在训练数据上的表现。然后，我们使用指标来监测模型在训练步骤和测试步骤上的性能。在这里，我们使用**随机梯度下降**
    (**SGD**) 作为我们的优化器，MAE作为我们的损失和评估指标：
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, all we have to do is feed our model with training data and the corresponding
    labels, with which our model can learn to intelligently predict the target numerical
    values, which in our case is the expected salary. Every time the model makes a
    prediction, the loss function compares the difference between the model’s prediction
    and the ground truth. This information is passed to the optimizer, which uses
    the information to make an improved prediction until the model can fashion the
    right mathematical equation to accurately predict our employee’s salary.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需用训练数据和相应的标签来喂养我们的模型，我们的模型就能学会智能地预测目标数值，这在我们的情况下是预期的薪水。每次模型进行预测时，损失函数都会比较模型预测与实际值之间的差异。这些信息传递给优化器，优化器利用信息进行改进预测，直到模型能够制定正确的数学方程以准确预测我们员工的薪水。
- en: 'Now, let’s fit our training model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们适应我们的训练模型：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We use `model_1.fit` to fit our training data and labels and set the number
    of tries (epochs) to `50`. In just a few lines of code, we have generated a mini-brain
    that we can train over time to make sensible predictions. Let’s run the code and
    see what the output looks like:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have displayed the last five tries (epochs `46`–`50`). The error drops gradually;
    however, we end up with a very large error after `50` epochs. Perhaps we can train
    our model for more epochs, as we did in [*Chapter 2*](B18118_02.xhtml#_idTextAnchor045)*,
    Introduction to TensorFlow*. Why not?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we simply change the number of epochs to `500` using our single-layer
    model. The activation function, the loss, and optimizers are the same as our initial
    model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'From the last five lines of our output, we can see that the loss is still quite
    high after `500` epochs. You may wish to experiment with the model for longer
    epochs to see how it will fare. It is also a good idea to visualize your model’s
    loss curve to see how it performs. A lower loss indicates a better-performing
    model. With this in mind, let us explore the loss curve for `model_2`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We will generate a utility plotting function, `visualize_model`, which we will
    use in our experiments to plot the model’s loss over time as it trains. In this
    code, we generate a figure to plot the loss values stored in the `history` object.
    The `history` object is the output of the `fit` function in our three-step modeling
    process, and it holds the loss and metrics values at the end of each epoch.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'To plot `model_2`, we simply call the function to visualize the plot and pass
    in `history_2`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'When we run the code, we get the plot shown in *Figure 3**.9*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Model losses at 500](img/B18118_03_009.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Model losses at 500
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3**.9*, we can see the loss falling, and the rate at which it falls
    is too slow, as it takes **500** epochs to fall from around **97400** to **97000**.
    In your spare time, you can try to train the model for 2,000 or more epochs. It
    will not be able to generalize well, as the model is too simple to handle the
    complexity of our data. In machine learning lingo, we say the model is *underfitting*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'There are primarily two ways to build models with TensorFlow – the sequential
    API and the functional API. The sequential API is a simple way of building models
    by using a stack of layers, where data flows in a single direction, from the input
    layer to the output layer. Conversely, the functional API in TensorFlow allows
    us to build more complex models – this includes models with multiple inputs or
    outputs and models with shared layers. Here, we use the Sequential API. For more
    information about building models with the sequential API, check out the documentation:
    [https://www.tensorflow.org/guide/keras/sequential_model](https://www.tensorflow.org/guide/keras/sequential_model).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, let us try to build a more complex model and see whether we can push
    the loss lower and quicker than our initial model:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here, we have generated a new model. We stack a 64-neuron layer on top of our
    single-unit layer. We also use a **Rectified Linear Unit** (**ReLU**) activation
    function for this layer; its job is to help our model learn more complex patterns
    in our data and improve computational efficiency. The second layer is our output
    layer, made up of a single neuron because we have a regression task (predicting
    a continuous value). Let’s run it for 500 epochs and see whether this will make
    any difference:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: From the last five lines of our output, we can see that there is a significant
    drop in our loss to around `3686`. Let’s also plot the loss curve to get a visual
    understanding as well.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Model losses after 500 epochs](img/B18118_03_010.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Model losses after 500 epochs
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.10*, we can see that our model’s loss has fallen below our lowest
    recorded loss. This is a massive improvement in comparison to our previous model.
    However, this is not a desired result, nor does it look like the type of result
    we would like to present to the HR team. This is because, with this model, if
    an employee earns $50,000, the model could predict either around $46,300 as the
    employee’s salary, which would make them unhappy, or $53,700 as the employee’s
    salary, in which case the HR team will not be happy. So, we need to figure out
    how to improve our result.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s zoom into the plot to have a better understanding of what is happening
    with our model:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: When we run the code, it returns the plot shown in *Figure 3**.11*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Model losses after 500 epochs when we zoom into the plot](img/B18118_03_011.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Model losses after 500 epochs when we zoom into the plot
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: From the plot in *Figure 3**.11*, we can see that the loss falls sharply and
    settles at around the 100th epoch, and nothing significant seems to happen afterward.
    Hence, training the model for longer just as we did in our previous model may
    not be the optimal solution. What can we do to improve our model?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps we can add another layer? Let’s do that and see what our results look
    like. As we initially pointed out, our job requires a lot of experimentation;
    only then can we learn how to do things better and faster:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here, we added another dense layer of `64` neurons. Note that we also use ReLU
    as the activation function here:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We display only the last five epochs, and we can see the loss is around `97384`,
    which is worse than the results achieved in `model_3`. So, how do we know how
    many layers to use in our modeling process? The answer is by experimenting. We
    use trial and error, backed by our understanding of what the results look like.
    We can decide whether we need to add more layers, as we did initially when the
    model was underfitting. And should the model get so complex that it masters the
    training data well but does not generalize well on our test (hold-out) data, it
    is said to be *overfitting* in machine learning lingo.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have tried smaller and larger models, we cannot yet say we have
    achieved a suitable result, and the HR manager has checked in on us to find out
    how we are doing in terms of the prediction modeling task. So far, we did some
    research, as all ML engineers do, and we discovered a very important step that
    we can try out. What step? Let’s see.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Normalization** is a technique applied to input features to ensure they are
    of a consistent scale, usually between 0 and 1\. This process helps our model
    to converge faster and more accurately. It is worth noting that we should apply
    normalization after completing other data preprocessing steps, such as handling
    missing values.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s good practice to know that improving your model output can also rely strongly
    on your data preparation process. Hence, let us apply this here. We will take
    a step back from model building and look at our features after we converted all
    the columns to numerical values:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We will use the `describe` function to get vital statistics of our data. This
    information shows us that most of the columns have a minimum value of 0 and a
    maximum value of 1, but the experience column is of a different scale, as shown
    in *Figure 3**.12*:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – A statistical summary of the dataset (before normalization)](img/B18118_03_012.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – A statistical summary of the dataset (before normalization)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Why does this matter, you may ask? When the scale of our data is different,
    our model will arbitrarily attach more importance to columns with higher values,
    which could affect the model’s ability to predict our target correctly. To resolve
    this issue, we will use normalization to scale the data between 0 and 1 to bring
    all our features to the same scale, thereby giving every feature an equal chance
    when our model begins to learn how they relate to our target (*y*).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'To normalize our data, we will use the following equation to scale it:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: X norm =  X − X min _ X max − X min
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'where *X* is our data, X min is the minimum value of *X*, and X max is the
    maximum value of *X*. In our case study, the minimum value of *X* in the `Experience`
    column is 1, and the maximum value of *X* in the `Experience` column is 7\. The
    good news is that we can easily implement this step using the `MinMaxScaler` function
    from the `sklearn` library. Let’s see how to scale our data next:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Let’s use the `describe()` function to view the key statistics again, as shown
    in *Figure 3**.13*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – A statistical summary of the dataset (after normalization)](img/B18118_03_013.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – A statistical summary of the dataset (after normalization)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Now, all our data is of the same scale. So, we have successfully implemented
    normalization of our data in just a few lines of code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we split our data into training and testing sets, but this time, we use
    our normalized *X* (`X_norm`) in the code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, we use our best-performing model (`model_3`) from the initial experiments
    we have done so far. Let’s see how our model performs after normalization:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: From the results, we can see that MAE has reduced by more than half in comparison
    to the results we got without applying normalization.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – The zoomed-in loss curve for model_5](img/B18118_03_014.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – The zoomed-in loss curve for model_5
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, if you look at the loss plot for `model_5` in *Figure 3**.14*, you can
    see the loss fails to drop significantly after around the 100th epoch. Instead
    of guessing how many epochs are ideal to train the model, how about we set a rule
    to stop training when the model fails to improve its performance? Also, we can
    see that `model_5` doesn’t give us the result we want; perhaps now is a good time
    to try out a bigger model, in which we train it for longer and set a rule to stop
    training once it fails to improve its performance on the training data:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here, we use a three-layer model; the first two layers are made up of 64 neurons
    and the output layer has a single neuron. To set the rule to stop training, we
    use *early stopping*; this additional parameter is applied when we fit our model
    on the data to stop training when the model loss fails to improve after 10 epochs.
    This is achieved by specifying the metric to monitor loss and setting `patience`
    to `10`. Early stopping is also a great technique to prevent overfitting, as it
    stops training when the model fails to improve; we will discuss this further in
    [*Chapter 6*](B18118_06.xhtml#_idTextAnchor129)*, Improving the Model*. Let’s
    look at the result now:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Although we set our training for `1000` epochs, our `Earlystopping` callback
    halted the training process on the 29th epoch because it observed no meaningful
    drop in the loss. Although the result here isn’t great, we have used `EarlyStopping`
    to save a considerable amount of computational resources and time. Perhaps now
    is a good time to try out a different optimizer. For this next experiment, let’s
    use the Adam optimizer. Adam is another popular optimizer that is used in deep
    learning, due to its ability to adaptively control the learning rate for each
    parameter in a model, which accelerates the model’s convergence:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Note we only changed the optimizer to Adam in our compile step. Let’s see the
    result of this change in the optimizer:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'By just changing the optimizer, we have recorded an incredible drop in loss.
    Also, note that we did not need the entire `1000` epochs, as training ended on
    `901` epochs. Let us add another layer; perhaps we will see an improved performance:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here, we added an extra layer with `64` neurons, with ReLU as the activation
    function. Everything else is the same:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Training stops at 270 epochs; although our model is more complex, it doesn’t
    perform better than `model_7` on training. We have tried out different ideas while
    experimenting; now, let us try out all eight models on the test set and evaluate
    them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate our models, we will write a function to apply the `evaluate` metrics
    to all eight models:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will generate an `eval_testing(model)` function that takes a model as an
    argument and uses the `evaluate` method to evaluate the performance of the model
    on our test dataset. Looping through the list of models, the code returns the
    loss and MAE values for all eight models for our test data:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: After we evaluate the models, we can that see `model_7` has the lowest loss.
    Let’s see how it does on our test set by using it to make predictions.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we are done with experimenting and have evaluated the models, let’s
    use `model_7` to predict our test set salaries and see how they compare with the
    ground truth. To do this, we will use the `predict()` function:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'After we run this code block, we get the output in an array, as shown here:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'For clarity, let’s build a DataFrame with the model’s prediction and ground
    truth. This should be fun and somewhat magical when you see how good our model
    has become:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, we generate two columns and convert the model’s prediction from `float`
    to `int`, just to keep it in scope with the ground truth. Ready for the result?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `head` function to print out the first 10 values of the test
    set:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We then see our results, as shown in *Figure 3**.15*:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – A DataFrame showing the actual values, predictions made by
    the model, and the resulting residuals](img/B18118_03_015.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – A DataFrame showing the actual values, predictions made by the
    model, and the resulting residuals
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Our model has achieved something impressive; it is really close to the initial
    salaries in our test data. Now, you can show the HR manager your amazing result.
    We must save the model so that we can load it and make predictions anytime we
    want. Let’s learn how to do this next.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Saving and loading models
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The beauty of TensorFlow is the ease with which we can do complex stuff. To
    save a model, we just need one line of code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: You can save it as `your_model.h5` or `your_model`; either way works. TensorFlow
    recommends the `SavedModel` approach because it is language-agnostic, which makes
    it easy to deploy on various platforms. In this format, we can save the model
    and its individual components, such as the weights and variables. Conversely,
    the HDF5 format saves the complete model structure, its weights, and the training
    configurations as a single file. This approach gives us greater flexibility to
    share and distribute models; however, for deployment purposes, it’s not the preferred
    method. When we run the code, we can see the saved model on the left-hand panel
    in our Colab notebook, as shown in *Figure 3**.16*.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – A snapshot of our saved model](img/B18118_03_016.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – A snapshot of our saved model
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have saved the model, it is wise to test it out by reloading it
    and testing it. Let’s do that. Also, it’s just one line of code to load the model:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let’s try out our `saved_model` and see whether it will work as well as `model_7`.
    We will generate `y_pred` again and generate a DataFrame, using `y_test` and `y_pred`
    as we did earlier checking first the 10 random samples from our test data:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – A DataFrame showing the actual values and predictions made
    by the saved model](img/B18118_03_017.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – A DataFrame showing the actual values and predictions made by
    the saved model
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'From the results in *Figure 3**.17*, we can see that our saved model performs
    at a high level. Now, you can deliver your result to the HR manager, and they
    should be excited about your results. Let’s imagine that the HR manager wants
    you to use your model to predict the salary of the new hires. Let’s do that next:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We generate a function using our saved model. We simply wrap all the steps
    we’ve covered so far into the function, and we return a DataFrame. Now, let’s
    read in the data of our new hires:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: When we run the code block, we can see their data in a DataFrame, as shown in
    *Figure 3**.18*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – A DataFrame showing the new hires](img/B18118_03_018.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – A DataFrame showing the new hires
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we pass the data into the function we generated to get the predicted salaries
    for our new hires:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We pass `df_new` into the salary prediction function, and we get a new DataFrame,
    as shown in *Figure 3**.19*:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – A DataFrame showing the new hires with their predicted salaries](img/B18118_03_019.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – A DataFrame showing the new hires with their predicted salaries
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have achieved our goal. HR is happy, the new hires are happy, and
    everyone in the company thinks you are a magician. Perhaps a pay raise could be
    on the table, but while you bask in the euphoria around your first success, your
    manager returns with another task. This time, it is a classification task, which
    we will look at this in the next chapter. For now, good job!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a deeper dive into supervised learning, with a focus
    on regression modeling. Here, we discussed the difference between simple and multiple
    linear regression and looked at some important evaluation metrics for regression
    modeling. Then, we rolled up our sleeves on our case study, helping our company
    build a working regression model to predict the salaries of new employees. We
    carried out some data preprocessing steps and saw the importance of normalization
    in our modeling process.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the case study, we successfully built a salary prediction model,
    evaluated the model on our test set, and mastered how to save and load models
    for use at a later stage. Now, you can confidently build a regression model with
    TensorFlow.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll take a look at classification modeling.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s test what we learned in this chapter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: What is linear regression?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between simple and multiple linear regression?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What evaluation metric penalizes large errors in regression modeling?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the salary dataset to forecast salaries.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, you can check out the following resources:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Amr, T., 2020\. *Hands-On Machine Learning with scikit-learn and Scientific
    Python Toolkits*. [S.l.]: Packt Publishing.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raschka, S. and Mirjalili, V., 2019\. *Python Machine Learning*. 3rd ed. Packt
    Publishing.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow* *documen**t**ation*: [https://www.TensorFlow.org/guide](https://www.TensorFlow.org/guide).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
