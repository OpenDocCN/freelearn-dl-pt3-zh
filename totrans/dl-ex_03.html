<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Feature Engineering and Model Complexity – The Titanic Example Revisited</h1>
                </header>
            
            <article>
                
<p class="calibre2">Model complexity and assessment is a must-do step toward building a successful data science system. There are lots of tools that you can use to assess and choose your model. In this chapter, we are going to address some of the tools that can help you to increase the value of your data by adding more descriptive features and extracting meaningful information from existing ones. We are also going to address other tools related optimal number features and learn why it's a problem to have a large number of features and fewer training samples/observations.</p>
<p class="calibre2">The following are the topics that will be explained in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">Feature engineering</li>
<li class="calibre8">The curse of dimensionality</li>
<li class="calibre8">Titanic example revisited—all together</li>
<li class="calibre8">Bias-variance decomposition</li>
<li class="calibre8">Learning visibility</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature engineering</h1>
                </header>
            
            <article>
                
<p class="calibre2">Feature engineering is one of the key components that contribute to the model's performance. A simple model with the right features can perform better than a complicated one with poor features. You can think of the feature engineering process as the most important step in determining your predictive model's success or failure. Feature engineering will be much easier if you understand the data.</p>
<p class="calibre2">Feature engineering is used extensively by anyone who uses machine learning to solve only one question, which is: <strong class="calibre13">how do you get the most out of your data samples for predictive modeling</strong>? This is the problem that the process and practice of feature engineering solves, and the success of your data science skills starts by knowing how to represent your data well.</p>
<p class="calibre2">Predictive modeling is a formula or rule that transforms a list of features or input variables (<em class="calibre19">x</em><sub class="calibre28">1</sub>, <em class="calibre19">x</em><sub class="calibre28">2</sub>,..., <em class="calibre19">x</em><sub class="calibre28">n</sub>) into an output/target of interest (y). So, what is feature engineering? It's the process of creating new input variables or features (<em class="calibre19">z</em><sub class="calibre28">1</sub>, <em class="calibre19">z</em><sub class="calibre28">2</sub>, ..., <em class="calibre19">z</em><sub class="calibre28">n</sub>) from existing input variables (<em class="calibre19">x</em><sub class="calibre28">1</sub>, <em class="calibre19">x</em><sub class="calibre28">2</sub>,..., <em class="calibre19">x</em><sub class="calibre28">n</sub>). We don't just create any new features; the newly created features should contribute and be relevant to the model's output. Creating such features that will be relevant to the model's output will be an easy process with knowledge of the domain (such as marketing, medical, and so on). Even if machine learning practitioners interact with some domain experts during this process, the outcome of the feature engineering process will be much better.</p>
<p class="calibre2">An example where domain knowledge can be helpful is modeling the likelihood of rain, given a set of input variables/features (temperature, wind speed, and percentage of cloud cover). For this specific example, we can construct a new binary feature called <strong class="calibre13">overcast</strong>, where its value equals 1 or no whenever the percentage of cloud cover is less than 20%, and equals 0 or yes otherwise. In this example, domain knowledge was essential to specify the threshold or cut-off percentage. The more thoughtful and useful the inputs, the better the reliability and predictivity of your model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of feature engineering</h1>
                </header>
            
            <article>
                
<p class="calibre2">Feature engineering as a technique has three main subcategories. As a deep learning practitioner, you have the freedom to choose between them or combine them in some way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection</h1>
                </header>
            
            <article>
                
<p class="calibre2">Sometimes called <strong class="calibre13">feature importance</strong>, this is the process of ranking the input variables according to their contribution to the target/output variable. Also, this process can be considered a ranking process of the input variables according to their value in the predictive ability of the model.</p>
<p class="calibre2">Some learning methods do this kind of feature ranking or importance as part of their internal procedures (such as decision trees). Mostly, these kind of methods uses entropy to filter out the less valuable variables. In some cases, deep learning practitioners use such learning methods to select the most important features and then feed them into a better learning algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction</h1>
                </header>
            
            <article>
                
<p class="calibre2">Dimensionality reduction is sometimes feature extraction, and it is the process of combining the existing input variables into a new set of a much reduced number of input variables. One of the most used methods for this type of feature engineering is <strong class="calibre13">principle component analysis</strong> (<strong class="calibre13">PCA</strong>), which utilizes the variance in data to come up with a reduced number of input variables that don't look like the original input variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature construction</h1>
                </header>
            
            <article>
                
<p class="calibre2">Feature construction is a commonly used type of feature engineering, and <span class="calibre10"> people </span>usually refer to it when they talk about feature engineering. This technique is the process of handcrafting or constructing new features from raw data. In this type of feature engineering, domain knowledge is very useful to manually make up other features from existing ones. Like other feature engineering techniques, the purpose of feature construction is to increase the predictivity of your model. A simple example of feature construction is using the date stamp feature to generate two <span class="calibre10">new </span>features, such as AM and PM, which might be useful to distinguish between day and night. We can also transform/convert noisy numerical features into simpler, nominal ones by calculating the mean value of the noisy feature and then determining whether a given row is more than or less than that mean value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Titanic example revisited</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to go through the Titanic example again but from a different perspective while using the feature engineering tool. In case you skipped <a href="6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml" target="_blank" class="calibre11">Chapter 2</a>, <em class="calibre19">Data Modeling in Action - The Titanic Example</em>, the Titanic example is a Kaggle competition with the purpose of predicting weather a specific passenger survived or not.</p>
<p class="calibre2">During this revisit of the Titanic example, we are going to use the scikit-learn and pandas libraries. So first off, let's start by reading the train and test sets and get some statistics about the data:</p>
<pre class="calibre21"><span># reading the train and test sets using pandas<br class="title-page-name"/></span>train_data = pd.read_csv(<span>'data/train.csv'</span>, <span>header</span>=<span>0</span>)<br class="title-page-name"/>test_data = pd.read_csv(<span>'data/test.csv'</span>, <span>header</span>=<span>0</span>)<br class="title-page-name"/><br class="title-page-name"/><span># concatenate the train and test set together for doing the overall feature engineering stuff<br class="title-page-name"/></span>df_titanic_data = pd.concat([train_data, test_data])<br class="title-page-name"/><br class="title-page-name"/><span># removing duplicate indices due to coming the train and test set by re-indexing the data<br class="title-page-name"/></span>df_titanic_data.reset_index(<span>inplace</span>=<span>True</span>)<br class="title-page-name"/><br class="title-page-name"/><span># removing the index column the reset_index() function generates<br class="title-page-name"/></span>df_titanic_data.drop(<span>'index'</span>, <span>axis</span>=<span>1</span>, <span>inplace</span>=<span>True</span>)<br class="title-page-name"/><br class="title-page-name"/><span># index the columns to be 1-based index<br class="title-page-name"/></span>df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, <span>axis</span>=<span>1</span>)</pre>
<p class="calibre2">We need to point out a few things about the preceding code snippet:</p>
<ul class="calibre7">
<li class="calibre8">As shown, we have used the <kbd class="calibre12">concat</kbd> function of pandas to combine the data frames of the train and test sets. This is useful for the feature engineering task as we need a full view of the distribution of the input variables/features.</li>
<li class="calibre8">After combining both data frames, we need to do some modifications to the output data frame.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Missing values</h1>
                </header>
            
            <article>
                
<p class="calibre2">This step will be the first thing to think of after getting a new dataset from the customer, because there will be missing/incorrect data in nearly every dataset. In the next chapters, you will see that some learning algorithms are able to deal with missing values and others need you to handle missing data. During this example, we are going to use the random forest classifier from scikit-learn, which requires separate handling of missing data.</p>
<p class="calibre2">There are different approaches that you can use to handle missing data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing any sample with missing values in it</h1>
                </header>
            
            <article>
                
<p class="calibre2">This approach won't be a good choice if you have a small dataset with lots of missing values, as removing the samples with missing values will produce useless data. It could be a quick and easy choice if you have lots of data, and removing it won't affect the original dataset much.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Missing value inputting</h1>
                </header>
            
            <article>
                
<p class="calibre2">This approach is useful when you have categorical data. The intuition behind this approach is that missing values may correlate with other variables, and removing them will result in a loss of information that <span class="calibre10">can </span>affect the model <span class="calibre10">significantly</span>.<br class="calibre20"/>
For example, if we have a binary variable with two possible values, -1 and 1, we can add another value (0) to indicate a missing value. You can use the following code to replace the null values of the <strong class="calibre13">Cabin</strong> feature with <kbd class="calibre12">U0</kbd>:</p>
<pre class="calibre21"><span># replacing the missing value in cabin variable "U0"<br class="title-page-name"/></span>df_titanic_data[<span>'Cabin'</span>][df_titanic_data.Cabin.isnull()] = <span>'U0'</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assigning an average value</h1>
                </header>
            
            <article>
                
<p class="calibre2">This is also one of the common approaches because of its simplicity. In the case of a numerical feature, you can just replace the missing values with the mean or median. You can also use this approach in the case of categorical variables by assigning the mode (the value that has the highest occurrence) to the missing values.</p>
<p class="calibre2">The following code assigns the median of the non-missing values of the <kbd class="calibre12">Fare</kbd> feature to the missing values:</p>
<pre class="calibre21"><span># handling the missing values by replacing it with the median fare<br class="title-page-name"/></span>df_titanic_data[<span>'Fare'</span>][np.isnan(df_titanic_data[<span>'Fare'</span>])] = df_titanic_data[<span>'Fare'</span>].median()</pre>
<p class="calibre2">Or, you can use the following code to find the value that has the highest occurrence in the <kbd class="calibre12">Embarked</kbd> feature and assign it to the missing values:</p>
<pre class="calibre21"><span># replacing the missing values with the most common value in the variable<br class="title-page-name"/></span>df_titanic_data.Embarked[df_titanic_data.Embarked.isnull()] = df_titanic_data.Embarked.dropna().mode().values</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a regression or another simple model to predict the values of missing variables</h1>
                </header>
            
            <article>
                
<p class="calibre2">This is the approach that we will use for the <kbd class="calibre12">Age</kbd> feature of the Titanic example. The <kbd class="calibre12">Age</kbd> feature is an important step towards predicting the survival of passengers, and applying the previous approach by taking the mean will make us lose some information.</p>
<p class="calibre2">In order to predict the missing values, you need to use a supervised learning algorithm that takes the available features as input and the available values of the feature that you want to predict for its missing value as output. In the following code snippet, we are using the random forest classifier to predict the missing values of the <kbd class="calibre12">Age</kbd> feature:</p>
<pre class="calibre21"><span># Define a helper function that can use RandomForestClassifier for handling the missing values of the age variable<br class="title-page-name"/></span><span>def </span>set_missing_ages():<br class="title-page-name"/>    <span>global </span>df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    age_data = df_titanic_data[<br class="title-page-name"/>        [<span>'Age'</span>, <span>'Embarked'</span>, <span>'Fare'</span>, <span>'Parch'</span>, <span>'SibSp'</span>, <span>'Title_id'</span>, <span>'Pclass'</span>, <span>'Names'</span>, <span>'CabinLetter'</span>]]<br class="title-page-name"/>    input_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, <span>1</span>::]<br class="title-page-name"/>    target_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, <span>0</span>]<br class="title-page-name"/><br class="title-page-name"/>    <span># Creating an object from the random forest regression function of sklearn&lt;use the documentation for more details&gt;<br class="title-page-name"/></span><span>    </span>regressor = RandomForestRegressor(<span>n_estimators</span>=<span>2000</span>, <span>n_jobs</span>=-<span>1</span>)<br class="title-page-name"/><br class="title-page-name"/>    <span># building the model based on the input values and target values above<br class="title-page-name"/></span><span>    </span>regressor.fit(input_values_RF, target_values_RF)<br class="title-page-name"/><br class="title-page-name"/>    <span># using the trained model to predict the missing values<br class="title-page-name"/></span><span>    </span>predicted_ages = regressor.predict(age_data.loc[(df_titanic_data.Age.isnull())].values[:, <span>1</span>::])<br class="title-page-name"/><br class="title-page-name"/></pre>
<pre class="calibre21">    <span># Filling the predicted ages in the original titanic dataframe<br class="title-page-name"/></span><span>    </span>age_data.loc[(age_data.Age.isnull()), <span>'Age'</span>] = predicted_ages</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature transformations</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous two sections, we covered reading the train and test sets and combining them. We also handled some missing values. Now, we will use the random forest classifier of scikit-learn to predict the survival of passengers. Different implementations of the random forest algorithm accept different types of data. The scikit-learn implementation of random forest accepts <span class="calibre10">only </span>numeric data. So, we need to transform the categorical features into numerical ones.</p>
<p class="calibre2">There are two types of features:</p>
<ul class="calibre7">
<li class="calibre8">
<p class="calibre9"><strong class="calibre13">Quantitative</strong>: Quantitative features are measured in a numerical scale and can be meaningfully sorted. In the Titanic data samples, the <kbd class="calibre12">Age</kbd> feature is an example of a quantitative feature.</p>
</li>
<li class="calibre8"><strong class="calibre1">Qualitative</strong>: Qualitative variables, also called <strong class="calibre1">categorical variables</strong>, are variables that are not numerical. They describe data that fits into categories. In the Titanic data samples, the <kbd class="calibre12">Embarked</kbd> (indicates the name of the departure port) feature is an example of a qualitative feature.</li>
</ul>
<p class="calibre2">We can apply different kinds of transformations to different variables. The following are some approaches that one can use to transform qualitative/categorical features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dummy features</h1>
                </header>
            
            <article>
                
<p class="calibre2">These variables are also known as categorical or binary features. This approach will be a good choice if we have a small number of distinct values for the feature to be transformed. In the Titanic data samples, the <kbd class="calibre12">Embarked</kbd> feature has only three distinct values (<kbd class="calibre12">S</kbd>, <kbd class="calibre12">C</kbd>, and <kbd class="calibre12">Q</kbd>) that occur frequently. So, we can transform the <kbd class="calibre12">Embarked</kbd> feature into three dummy variables, (<kbd class="calibre12">'Embarked_S'</kbd>, <kbd class="calibre12">'Embarked_C'</kbd>, and <kbd class="calibre12">'Embarked_Q'</kbd>) to be able to use the random forest classifier.</p>
<p class="calibre2">The following code will show you how to do this kind of transformation:</p>
<pre class="calibre21"><span># constructing binary features<br class="title-page-name"/></span><span>def </span>process_embarked():<br class="title-page-name"/>    <span>global </span>df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    <span># replacing the missing values with the most common value in the variable<br class="title-page-name"/></span><span>    </span>df_titanic_data.Embarked[df.Embarked.isnull()] = df_titanic_data.Embarked.dropna().mode().values<br class="title-page-name"/><br class="title-page-name"/>    <span># converting the values into numbers<br class="title-page-name"/></span><span>    </span>df_titanic_data[<span>'Embarked'</span>] = pd.factorize(df_titanic_data[<span>'Embarked'</span>])[<span>0</span>]<br class="title-page-name"/><br class="title-page-name"/>    <span># binarizing the constructed features<br class="title-page-name"/></span><span>    </span><span>if </span>keep_binary:<br class="title-page-name"/>        df_titanic_data = pd.concat([df_titanic_data, pd.get_dummies(df_titanic_data[<span>'Embarked'</span>]).rename(<br class="title-page-name"/>            <span>columns</span>=<span>lambda </span>x: <span>'Embarked_' </span>+ <span>str</span>(x))], <span>axis</span>=<span>1</span>)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Factorizing</h1>
                </header>
            
            <article>
                
<p class="calibre2">This approach is used to create a numerical categorical feature from any other feature. In pandas, the <kbd class="calibre12">factorize()</kbd> function does that. This type of transformation is useful if your feature is an alphanumeric categorical variable. In the Titanic data samples, we can transform the <kbd class="calibre12">Cabin</kbd> feature into a categorical feature, representing the letter of the cabin:</p>
<pre class="calibre21"><span># the cabin number is a sequence of of alphanumerical digits, so we are going to create some features<br class="title-page-name"/></span><span># from the alphabetical part of it<br class="title-page-name"/></span>df_titanic_data[<span>'CabinLetter'</span>] = df_titanic_data[<span>'Cabin'</span>].map(<span>lambda </span>l: get_cabin_letter(l))<br class="title-page-name"/>df_titanic_data[<span>'CabinLetter'</span>] = pd.factorize(df_titanic_data[<span>'CabinLetter'</span>])[<span>0</span>]</pre>
<pre class="calibre21"><span>def </span>get_cabin_letter(cabin_value):<br class="title-page-name"/>    <span># searching for the letters in the cabin alphanumerical value<br class="title-page-name"/></span><span>    </span>letter_match = re.compile(<span>"([a-zA-Z]+)"</span>).search(cabin_value)<br class="title-page-name"/><br class="title-page-name"/>    <span>if </span>letter_match:<br class="title-page-name"/>        <span>return </span>letter_match.group()<br class="title-page-name"/>    <span>else</span>:<br class="title-page-name"/>        <span>return </span><span>'U'</span></pre>
<p class="calibre2">We can also apply transformations to quantitative features by using one of the following approaches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling</h1>
                </header>
            
            <article>
                
<p class="calibre2">This kind of transformation can be applied to numerical features <span class="calibre10">only</span>.</p>
<p class="calibre2">For example, in the Titanic data, the <kbd class="calibre12">Age</kbd> feature can reach 100, but the household income may be in millions. Some models are sensitive to the magnitude of values, so scaling such features will help those models perform better. Also, scaling can be used to squash a variable's values to be within a specific range.</p>
<p class="calibre2">The following code will scale the <kbd class="calibre12">Age</kbd> feature by removing its mean from each value and scale to the unit variance:</p>
<pre class="calibre21"># scale by subtracting the mean from each value</pre>
<pre class="calibre21">scaler_processing = preprocessing.StandardScaler()</pre>
<pre class="calibre21">df_titanic_data['Age_scaled'] = scaler_processing.fit_transform(df_titanic_data['Age'])</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Binning</h1>
                </header>
            
            <article>
                
<p class="calibre2">This kind of quantitative transformation is used to create quantiles. In this case, the quantitative feature values will be the transformed ordered variable. This approach is not a good choice for linear regression, but it might work well for learning algorithms that respond effectively when using ordered/categorical variables.</p>
<p class="calibre2">The following code applies this kind of transformation to the <kbd class="calibre12">Fare</kbd> feature:</p>
<pre class="calibre21"><span># Binarizing the features by binning them into quantiles<br class="title-page-name"/></span>df_titanic_data[<span>'Fare_bin'</span>] = pd.qcut(df_titanic_data[<span>'Fare'</span>], <span>4</span>)<br class="title-page-name"/><br class="title-page-name"/><span>if </span>keep_binary:<br class="title-page-name"/>    df_titanic_data = pd.concat(<br class="title-page-name"/>        [df_titanic_data, pd.get_dummies(df_titanic_data[<span>'Fare_bin'</span>]).rename(<span>columns</span>=<span>lambda </span>x: <span>'Fare_' </span>+ <span>str</span>(x))],<br class="title-page-name"/>        <span>axis</span>=<span>1</span>)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Derived features</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous section, we applied some transformations to the Titanic data in order to be able to use the random forest classifier of scikit-learn (which only accepts numerical data). In this section, we are going to define another type of variable, which is derived from one or more other features.</p>
<p class="calibre2">Under this definition, we can say that some of the transformations in the previous section are also called <strong class="calibre13">derived features</strong>. In this section, we will look into other, complex transformations.</p>
<p class="calibre2">In the previous sections, we mentioned that you need to use your feature engineering skills to derive new features to enhance the model's predictive power. We have also talked about the importance of feature engineering in the data science pipeline and why you should spend most of your time and effort coming up with useful features. Domain knowledge will be very helpful in this section.</p>
<p class="calibre2">Very simple examples of derived features will be something like extracting the country code and/or region code from a telephone number. You can also extract the country/region from the GPS coordinates.</p>
<p class="calibre2">The Titanic data is a very simple one and doesn't contain a lot of variables to work with, but we can try to derive some features from the text feature that we have in it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Name</h1>
                </header>
            
            <article>
                
<p class="calibre2">The <kbd class="calibre12">name</kbd> variable by itself is useless for most datasets, but it has two useful properties. The first one is the length of your name. For example, the length of your name may reflect something about your status and hence your ability to get on a lifeboat:</p>
<pre class="calibre21"># getting the different names in the names variable<br class="title-page-name"/>df_titanic_data['Names'] = df_titanic_data['Name'].map(lambda y: len(re.split(' ', y)))</pre>
<p class="calibre2">The second interesting property is the <kbd class="calibre12">Name</kbd> title, which can <span class="calibre10">also </span>be used to indicate status and/or gender:</p>
<pre class="calibre21"><span># Getting titles for each person<br class="title-page-name"/></span>df_titanic_data[<span>'Title'</span>] = df_titanic_data[<span>'Name'</span>].map(<span>lambda </span>y: re.compile(<span>", (.*?)\."</span>).findall(y)[<span>0</span>])<br class="title-page-name"/><br class="title-page-name"/><span># handling the low occurring titles<br class="title-page-name"/></span>df_titanic_data[<span>'Title'</span>][df_titanic_data.Title == <span>'Jonkheer'</span>] = <span>'Master'<br class="title-page-name"/></span>df_titanic_data[<span>'Title'</span>][df_titanic_data.Title.isin([<span>'Ms'</span>, <span>'Mlle'</span>])] = <span>'Miss'<br class="title-page-name"/></span>df_titanic_data[<span>'Title'</span>][df_titanic_data.Title == <span>'Mme'</span>] = <span>'Mrs'<br class="title-page-name"/></span>df_titanic_data[<span>'Title'</span>][df_titanic_data.Title.isin([<span>'Capt'</span>, <span>'Don'</span>, <span>'Major'</span>, <span>'Col'</span>, <span>'Sir'</span>])] = <span>'Sir'<br class="title-page-name"/></span>df_titanic_data[<span>'Title'</span>][df_titanic_data.Title.isin([<span>'Dona'</span>, <span>'Lady'</span>, <span>'the Countess'</span>])] = <span>'Lady'<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span># binarizing all the features<br class="title-page-name"/></span><span>if </span>keep_binary:<br class="title-page-name"/>    df_titanic_data = pd.concat(<br class="title-page-name"/>        [df_titanic_data, pd.get_dummies(df_titanic_data[<span>'Title'</span>]).rename(<span>columns</span>=<span>lambda </span>x: <span>'Title_' </span>+ <span>str</span>(x))],<br class="title-page-name"/>        <span>axis</span>=<span>1</span>)</pre>
<p class="calibre2">You can also try to come up with other interesting features from the <kbd class="calibre12">Name</kbd> feature. For example, you might think of using the last name feature to find out the size of family members on the Titanic ship.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cabin</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the Titanic data, the <kbd class="calibre12">Cabin</kbd> feature is represented by a letter, which indicates the deck, and a number, which indicates the room number. The room number increases towards the back of the boat, and this will provide some useful measure of the passenger's location. We can also get the status of the passenger from the different decks, and this will help to determine who gets on the lifeboats:</p>
<pre class="calibre21"><span># repllacing the missing value in cabin variable "U0"<br class="title-page-name"/></span>df_titanic_data[<span>'Cabin'</span>][df_titanic_data.Cabin.isnull()] = <span>'U0'<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span># the cabin number is a sequence of of alphanumerical digits, so we are going to create some features<br class="title-page-name"/></span><span># from the alphabetical part of it<br class="title-page-name"/></span>df_titanic_data[<span>'CabinLetter'</span>] = df_titanic_data[<span>'Cabin'</span>].map(<span>lambda </span>l: get_cabin_letter(l))<br class="title-page-name"/>df_titanic_data[<span>'CabinLetter'</span>] = pd.factorize(df_titanic_data[<span>'CabinLetter'</span>])[<span>0</span>]<br class="title-page-name"/><br class="title-page-name"/><span># binarizing the cabin letters features<br class="title-page-name"/></span><span>if </span>keep_binary:<br class="title-page-name"/>    cletters = pd.get_dummies(df_titanic_data[<span>'CabinLetter'</span>]).rename(<span>columns</span>=<span>lambda </span>x: <span>'CabinLetter_' </span>+ <span>str</span>(x))<br class="title-page-name"/>    df_titanic_data = pd.concat([df_titanic_data, cletters], <span>axis</span>=<span>1</span>)<br class="title-page-name"/><br class="title-page-name"/><span># creating features from the numerical side of the cabin<br class="title-page-name"/></span>df_titanic_data[<span>'CabinNumber'</span>] = df_titanic_data[<span>'Cabin'</span>].map(<span>lambda </span>x: get_cabin_num(x)).astype(<span>int</span>) + <span>1<br class="title-page-name"/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ticket</h1>
                </header>
            
            <article>
                
<p class="calibre2">The code of the <kbd class="calibre12">Ticket</kbd> feature is not immediately clear, but we can do some guesses and try to group them. After looking at the Ticket feature, you may get these clues:</p>
<ul class="calibre7">
<li class="calibre8">Almost a quarter of the tickets begin with a character while the rest consist of only numbers.</li>
<li class="calibre8">The number part of the ticket code seems to have some indications about the class of the passenger. For example, numbers starting with 1 are usually first class tickets, 2 are usually second, and 3 are third. I say <em class="calibre25">usually</em> because it holds for the majority of examples, but not all. There are also ticket numbers starting with 4-9, and those are rare and almost exclusively third class.</li>
<li class="calibre8">Several people can share a ticket number, which might indicate a family or close friends traveling together and acting like a family.</li>
</ul>
<p class="calibre2">The following code tries to analyze the ticket feature code to come up with preceding clues:</p>
<pre class="calibre21"><span># Helper function for constructing features from the ticket variable<br class="title-page-name"/></span><span>def </span>process_ticket():<br class="title-page-name"/>    <span>global </span>df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data[<span>'TicketPrefix'</span>] = df_titanic_data[<span>'Ticket'</span>].map(<span>lambda </span>y: get_ticket_prefix(y.upper()))<br class="title-page-name"/>    df_titanic_data[<span>'TicketPrefix'</span>] = df_titanic_data[<span>'TicketPrefix'</span>].map(<span>lambda </span>y: re.sub(<span>'[\.?\/?]'</span>, <span>''</span>, y))<br class="title-page-name"/>    df_titanic_data[<span>'TicketPrefix'</span>] = df_titanic_data[<span>'TicketPrefix'</span>].map(<span>lambda </span>y: re.sub(<span>'STON'</span>, <span>'SOTON'</span>, y))<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data[<span>'TicketPrefixId'</span>] = pd.factorize(df_titanic_data[<span>'TicketPrefix'</span>])[<span>0</span>]<br class="title-page-name"/><br class="title-page-name"/>    <span># binarzing features for each ticket layer<br class="title-page-name"/></span><span>    </span><span>if </span>keep_binary:<br class="title-page-name"/>        prefixes = pd.get_dummies(df_titanic_data[<span>'TicketPrefix'</span>]).rename(<span>columns</span>=<span>lambda </span>y: <span>'TicketPrefix_' </span>+ <span>str</span>(y))<br class="title-page-name"/>        df_titanic_data = pd.concat([df_titanic_data, prefixes], <span>axis</span>=<span>1</span>)<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data.drop([<span>'TicketPrefix'</span>], <span>axis</span>=<span>1</span>, <span>inplace</span>=<span>True</span>)<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data[<span>'TicketNumber'</span>] = df_titanic_data[<span>'Ticket'</span>].map(<span>lambda </span>y: get_ticket_num(y))<br class="title-page-name"/>    df_titanic_data[<span>'TicketNumberDigits'</span>] = df_titanic_data[<span>'TicketNumber'</span>].map(<span>lambda </span>y: <span>len</span>(y)).astype(np.int)<br class="title-page-name"/>    df_titanic_data[<span>'TicketNumberStart'</span>] = df_titanic_data[<span>'TicketNumber'</span>].map(<span>lambda </span>y: y[<span>0</span>:<span>1</span>]).astype(np.int)<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data[<span>'TicketNumber'</span>] = df_titanic_data.TicketNumber.astype(np.int)<br class="title-page-name"/><br class="title-page-name"/>    <span>if </span>keep_scaled:<br class="title-page-name"/>        scaler_processing = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data[<span>'TicketNumber_scaled'</span>] = scaler_processing.fit_transform(<br class="title-page-name"/>            df_titanic_data.TicketNumber.reshape(-<span>1</span>, <span>1</span>))<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/><span>def </span>get_ticket_prefix(ticket_value):<br class="title-page-name"/>    <span># searching for the letters in the ticket alphanumerical value<br class="title-page-name"/></span><span>    </span>match_letter = re.compile(<span>"([a-zA-Z\.\/]+)"</span>).search(ticket_value)<br class="title-page-name"/>    <span>if </span>match_letter:<br class="title-page-name"/>        <span>return </span>match_letter.group()<br class="title-page-name"/>    <span>else</span>:<br class="title-page-name"/>        <span>return </span><span>'U'<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>def </span>get_ticket_num(ticket_value):<br class="title-page-name"/>    <span># searching for the numbers in the ticket alphanumerical value<br class="title-page-name"/></span><span>    </span>match_number = re.compile(<span>"([\d]+$)"</span>).search(ticket_value)<br class="title-page-name"/>    <span>if </span>match_number:<br class="title-page-name"/>        <span>return </span>match_number.group()<br class="title-page-name"/>    <span>else</span>:<br class="title-page-name"/>        <span>return </span><span>'0'<br class="title-page-name"/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interaction features</h1>
                </header>
            
            <article>
                
<p class="calibre2">Interaction features are obtained by performing mathematical operations on sets of features and indicate the effect of the relationship between variables. We use basic mathematical operations on the numerical features and see the effects of the relationship between variables:</p>
<pre class="calibre21"><span># Constructing features manually based on  the interaction between the individual features<br class="title-page-name"/></span>numeric_features = df_titanic_data.loc[:,<br class="title-page-name"/>                   [<span>'Age_scaled'</span>, <span>'Fare_scaled'</span>, <span>'Pclass_scaled'</span>, <span>'Parch_scaled'</span>, <span>'SibSp_scaled'</span>,<br class="title-page-name"/>                    <span>'Names_scaled'</span>, <span>'CabinNumber_scaled'</span>, <span>'Age_bin_id_scaled'</span>, <span>'Fare_bin_id_scaled'</span>]]<br class="title-page-name"/><span>print</span>(<span>"</span><span>\n</span><span>Using only numeric features for automated feature generation:</span><span>\n</span><span>"</span>, numeric_features.head(<span>10</span>))<br class="title-page-name"/><br class="title-page-name"/>new_fields_count = <span>0<br class="title-page-name"/></span><span>for </span>i <span>in </span><span>range</span>(<span>0</span>, numeric_features.columns.size - <span>1</span>):<br class="title-page-name"/>    <span>for </span>j <span>in </span><span>range</span>(<span>0</span>, numeric_features.columns.size - <span>1</span>):<br class="title-page-name"/>        <span>if </span>i &lt;= j:<br class="title-page-name"/>            name = <span>str</span>(numeric_features.columns.values[i]) + <span>"*" </span>+ <span>str</span>(numeric_features.columns.values[j])<br class="title-page-name"/>            df_titanic_data = pd.concat(<br class="title-page-name"/>                [df_titanic_data, pd.Series(numeric_features.iloc[:, i] * numeric_features.iloc[:, j], <span>name</span>=name)],<br class="title-page-name"/>                <span>axis</span>=<span>1</span>)<br class="title-page-name"/>            new_fields_count += <span>1<br class="title-page-name"/></span><span>        </span><span>if </span>i &lt; j:<br class="title-page-name"/>            name = <span>str</span>(numeric_features.columns.values[i]) + <span>"+" </span>+ <span>str</span>(numeric_features.columns.values[j])<br class="title-page-name"/>            df_titanic_data = pd.concat(<br class="title-page-name"/>                [df_titanic_data, pd.Series(numeric_features.iloc[:, i] + numeric_features.iloc[:, j], <span>name</span>=name)],<br class="title-page-name"/>                <span>axis</span>=<span>1</span>)<br class="title-page-name"/>            new_fields_count += <span>1<br class="title-page-name"/></span><span>        </span><span>if not </span>i == j:<br class="title-page-name"/>            name = <span>str</span>(numeric_features.columns.values[i]) + <span>"/" </span>+ <span>str</span>(numeric_features.columns.values[j])<br class="title-page-name"/>            df_titanic_data = pd.concat(<br class="title-page-name"/>                [df_titanic_data, pd.Series(numeric_features.iloc[:, i] / numeric_features.iloc[:, j], <span>name</span>=name)],<br class="title-page-name"/>                <span>axis</span>=<span>1</span>)<br class="title-page-name"/>            name = <span>str</span>(numeric_features.columns.values[i]) + <span>"-" </span>+ <span>str</span>(numeric_features.columns.values[j])<br class="title-page-name"/>            df_titanic_data = pd.concat(<br class="title-page-name"/>                [df_titanic_data, pd.Series(numeric_features.iloc[:, i] - numeric_features.iloc[:, j], <span>name</span>=name)],<br class="title-page-name"/>                <span>axis</span>=<span>1</span>)<br class="title-page-name"/>            new_fields_count += <span>2<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>print</span>(<span>"</span><span>\n</span><span>"</span>, new_fields_count, <span>"new features constructed"</span>)</pre>
<p class="calibre2">This kind of feature engineering can produce lots of features. In the preceding code snippet, we used 9 features to generate 176 interaction features.</p>
<p class="calibre2">We can also remove highly correlated features as the existence of these features won't add any information to the model. We can use Spearman's correlation to identify and remove highly correlated features. The Spearman method has a rank coefficient in its output that can be used to identity the highly correlated features:</p>
<pre class="calibre21"><span># using Spearman correlation method to remove the feature that have high correlation<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span># calculating the correlation matrix<br class="title-page-name"/></span>df_titanic_data_cor = df_titanic_data.drop([<span>'Survived'</span>, <span>'PassengerId'</span>], <span>axis</span>=<span>1</span>).corr(<span>method</span>=<span>'spearman'</span>)<br class="title-page-name"/><br class="title-page-name"/><span># creating a mask that will ignore correlated ones<br class="title-page-name"/></span>mask_ignore = np.ones(df_titanic_data_cor.columns.size) - np.eye(df_titanic_data_cor.columns.size)<br class="title-page-name"/>df_titanic_data_cor = mask_ignore * df_titanic_data_cor<br class="title-page-name"/><br class="title-page-name"/>features_to_drop = []<br class="title-page-name"/><br class="title-page-name"/><span># dropping the correclated features<br class="title-page-name"/></span><span>for </span>column <span>in </span>df_titanic_data_cor.columns.values:<br class="title-page-name"/><br class="title-page-name"/>    <span># check if we already decided to drop this variable<br class="title-page-name"/></span><span>    </span><span>if </span>np.in1d([column], features_to_drop):<br class="title-page-name"/>        <span>continue<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>    </span><span># finding highly correlacted variables<br class="title-page-name"/></span><span>    </span>corr_vars = df_titanic_data_cor[<span>abs</span>(df_titanic_data_cor[column]) &gt; <span>0.98</span>].index<br class="title-page-name"/>    features_to_drop = np.union1d(features_to_drop, corr_vars)<br class="title-page-name"/><br class="title-page-name"/><span>print</span>(<span>"</span><span>\n</span><span>We are going to drop"</span>, features_to_drop.shape[<span>0</span>], <span>" which are highly correlated features...</span><span>\n</span><span>"</span>)<br class="title-page-name"/>df_titanic_data.drop(features_to_drop, <span>axis</span>=<span>1</span>, <span>inplace</span>=<span>True</span>)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The curse of dimensionality</h1>
                </header>
            
            <article>
                
<p class="calibre2">In order to better explain the curse of dimensionality and the problem of overfitting, we are going to go through an example in which we have a set of images. Each image has a cat or a dog in it. So, we would like to build a model that can distinguish between the images with cats and the ones with dogs. Like the fish recognition system in <em class="calibre19"><a href="c6be0d67-2ba9-45ac-b6dd-116518853f42.xhtml" target="_blank" class="calibre11">Chapter 1</a></em>, <em class="calibre19">Data science - Bird's-eye view</em>, we need to find an explanatory feature that the learning algorithm can use to distinguish between the two classes (cats and dogs). In this example, we can argue that color is a good descriptor to be used to differentiate between cats and dogs. So the average red, average blue, and average green colors <span class="calibre10">can </span>be used as explanatory features to distinguish between the two classes.</p>
<p class="calibre2">The algorithm will then combine these three features in some way to form a decision boundary between the two classes.</p>
<p class="calibre2">A simple linear combination of the three features <span class="calibre10">can </span>be something like the following:</p>
<pre class="calibre21">If 0.5*red + 0.3*green + 0.2*blue &gt; 0.6 : return cat;</pre>
<pre class="calibre21">else return dog;</pre>
<p class="calibre2">These descriptive features will not be enough to get a good performing classifie, so we <span class="calibre10">can </span>decide to add more features that will enhance the model predictivity to discriminate between cats and dogs. For example, we can consider adding some features such as the texture of the image by calculating the average edge or gradient intensity in both dimensions of the image, X and Y. After adding these two features, the model accuracy will improve. We can even make the model/classifier get more accurate classification power by adding more and more features that are based on color, texture histograms, statistical moments, and so on. We can easily add a few hundred of these features to enhance the model's predictivity. But the counter-intuitive results will be worse after increasing the features beyond some limit. You'll better understand this by looking at <em class="calibre19">Figure 1</em>:</p>
<div class="CDPAlignCenter"><img src="assets/c2fec23b-79b5-4122-bafc-90c2ce8c0df8.png" class="calibre50"/></div>
<div class="CDPAlignCenter1">Figure 1: Model performance versus number of features</div>
<p class="calibre2"><em class="calibre19">Figure 1</em> shows that as the number of features increases, the classifier's performance increases as well, until we reach the optimal number of features. Adding more features based on the same size of the training set will <span class="calibre10">then</span><span class="calibre10"> </span><span class="calibre10">degrade the classifier's performance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Avoiding the curse of dimensionality</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous sections, we showed that the classifier's performance will decrease when the number of features exceeds a certain optimal point. In theory, if you have infinite training samples, the curse of dimensionality won't exist. So, the optimal number of features is totally dependent on the size of your data.</p>
<p class="calibre2">An approach that will help you to avoid the harm of this curse is to subset <em class="calibre19">M</em> features from the large number of features <em class="calibre19">N</em>, where <em class="calibre19">M &lt;&lt; N</em>. Each feature from <em class="calibre19">M</em> can be a combination of some features in <em class="calibre19">N</em>. There are some algorithms that can do this for you. These algorithms somehow <span class="calibre10">try </span>to find useful, uncorrelated, and linear combinations of the original <em class="calibre19">N</em> features. A commonly used technique for this is <strong class="calibre13">principle component analysis</strong> (<strong class="calibre13">PCA</strong>). PCA tries to find a smaller number of features that capture the largest variance of the original data. You can find more insights and a full explanation of PCA at this interesting blog: <a href="http://www.visiondummy.com/2014/05/feature-extraction-using-pca/" class="calibre11">http://www.visiondummy.com/2014/05/feature-extraction-using-pca/</a>.</p>
<p class="calibre2">A useful and easy way to apply PCA over your original training features is by using the following code:</p>
<pre class="calibre21"><span># minimum variance percentage that should be covered by the reduced number of variables<br class="title-page-name"/></span>variance_percentage = <span>.99<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span># creating PCA object<br class="title-page-name"/></span>pca_object = PCA(<span>n_components</span>=variance_percentage)<br class="title-page-name"/><br class="title-page-name"/><span># trasforming the features<br class="title-page-name"/></span>input_values_transformed = pca_object.fit_transform(input_values, target_values)<br class="title-page-name"/><br class="title-page-name"/><span># creating a datafram for the transformed variables from PCA<br class="title-page-name"/></span>pca_df = pd.DataFrame(input_values_transformed)<br class="title-page-name"/><br class="title-page-name"/><span>print</span>(pca_df.shape[<span>1</span>], <span>" reduced components which describe "</span>, <span>str</span>(variance_percentage)[<span>1</span>:], <span>"% of the variance"</span>)</pre>
<p class="calibre2">In the Titanic example, we tried to build the classifier with and without applying PCA on the original features. Because we used the random forest classifier at the end, we found that applying PCA isn't very helpful; random forest works very well without any feature transformations, and even correlated features don't really affect the model much.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Titanic example revisited – all together</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to put all the bits and pieces of feature engineering and dimensionality reduction together:</p>
<pre class="calibre21">import re<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import random as rd<br class="title-page-name"/>from sklearn import preprocessing<br class="title-page-name"/>from sklearn.cluster import KMeans<br class="title-page-name"/>from sklearn.ensemble import RandomForestRegressor<br class="title-page-name"/>from sklearn.decomposition import PCA<br class="title-page-name"/><br class="title-page-name"/># Print options<br class="title-page-name"/>np.set_printoptions(precision=4, threshold=10000, linewidth=160, edgeitems=999, suppress=True)<br class="title-page-name"/>pd.set_option('display.max_columns', None)<br class="title-page-name"/>pd.set_option('display.max_rows', None)<br class="title-page-name"/>pd.set_option('display.width', 160)<br class="title-page-name"/>pd.set_option('expand_frame_repr', False)<br class="title-page-name"/>pd.set_option('precision', 4)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># constructing binary features<br class="title-page-name"/>def process_embarked():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    # replacing the missing values with the most common value in the variable<br class="title-page-name"/>    df_titanic_data.Embarked[df.Embarked.isnull()] = df_titanic_data.Embarked.dropna().mode().values<br class="title-page-name"/><br class="title-page-name"/>    # converting the values into numbers<br class="title-page-name"/>    df_titanic_data['Embarked'] = pd.factorize(df_titanic_data['Embarked'])[0]<br class="title-page-name"/><br class="title-page-name"/>    # binarizing the constructed features<br class="title-page-name"/>    if keep_binary:<br class="title-page-name"/>        df_titanic_data = pd.concat([df_titanic_data, pd.get_dummies(df_titanic_data['Embarked']).rename(<br class="title-page-name"/>            columns=lambda x: 'Embarked_' + str(x))], axis=1)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Define a helper function that can use RandomForestClassifier for handling the missing values of the age variable<br class="title-page-name"/>def set_missing_ages():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    age_data = df_titanic_data[<br class="title-page-name"/>        ['Age', 'Embarked', 'Fare', 'Parch', 'SibSp', 'Title_id', 'Pclass', 'Names', 'CabinLetter']]<br class="title-page-name"/>    input_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 1::]<br class="title-page-name"/>    target_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 0]<br class="title-page-name"/><br class="title-page-name"/>    # Creating an object from the random forest regression function of sklearn&lt;use the documentation for more details&gt;<br class="title-page-name"/>    regressor = RandomForestRegressor(n_estimators=2000, n_jobs=-1)<br class="title-page-name"/><br class="title-page-name"/>    # building the model based on the input values and target values above<br class="title-page-name"/>    regressor.fit(input_values_RF, target_values_RF)<br class="title-page-name"/><br class="title-page-name"/>    # using the trained model to predict the missing values<br class="title-page-name"/>    predicted_ages = regressor.predict(age_data.loc[(df_titanic_data.Age.isnull())].values[:, 1::])<br class="title-page-name"/><br class="title-page-name"/>    # Filling the predicted ages in the original titanic dataframe<br class="title-page-name"/>    age_data.loc[(age_data.Age.isnull()), 'Age'] = predicted_ages<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Helper function for constructing features from the age variable<br class="title-page-name"/>def process_age():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    # calling the set_missing_ages helper function to use random forest regression for predicting missing values of age<br class="title-page-name"/>    set_missing_ages()<br class="title-page-name"/><br class="title-page-name"/>    # # scale the age variable by centering it around the mean with a unit variance<br class="title-page-name"/>    # if keep_scaled:<br class="title-page-name"/>    # scaler_preprocessing = preprocessing.StandardScaler()<br class="title-page-name"/>    # df_titanic_data['Age_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Age.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/>    # construct a feature for children<br class="title-page-name"/>    df_titanic_data['isChild'] = np.where(df_titanic_data.Age &lt; 13, 1, 0)<br class="title-page-name"/><br class="title-page-name"/>    # bin into quartiles and create binary features<br class="title-page-name"/>    df_titanic_data['Age_bin'] = pd.qcut(df_titanic_data['Age'], 4)<br class="title-page-name"/><br class="title-page-name"/>    if keep_binary:<br class="title-page-name"/>        df_titanic_data = pd.concat(<br class="title-page-name"/>            [df_titanic_data, pd.get_dummies(df_titanic_data['Age_bin']).rename(columns=lambda y: 'Age_' + str(y))],<br class="title-page-name"/>            axis=1)<br class="title-page-name"/><br class="title-page-name"/>    if keep_bins:<br class="title-page-name"/>        df_titanic_data['Age_bin_id'] = pd.factorize(df_titanic_data['Age_bin'])[0] + 1<br class="title-page-name"/><br class="title-page-name"/>    if keep_bins and keep_scaled:<br class="title-page-name"/>        scaler_processing = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data['Age_bin_id_scaled'] = scaler_processing.fit_transform(<br class="title-page-name"/>            df_titanic_data.Age_bin_id.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/>    if not keep_strings:<br class="title-page-name"/>        df_titanic_data.drop('Age_bin', axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Helper function for constructing features from the passengers/crew names<br class="title-page-name"/>def process_name():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    # getting the different names in the names variable<br class="title-page-name"/>    df_titanic_data['Names'] = df_titanic_data['Name'].map(lambda y: len(re.split(' ', y)))<br class="title-page-name"/><br class="title-page-name"/>    # Getting titles for each person<br class="title-page-name"/>    df_titanic_data['Title'] = df_titanic_data['Name'].map(lambda y: re.compile(", (.*?)\.").findall(y)[0])<br class="title-page-name"/><br class="title-page-name"/>    # handling the low occurring titles<br class="title-page-name"/>    df_titanic_data['Title'][df_titanic_data.Title == 'Jonkheer'] = 'Master'<br class="title-page-name"/>    df_titanic_data['Title'][df_titanic_data.Title.isin(['Ms', 'Mlle'])] = 'Miss'<br class="title-page-name"/>    df_titanic_data['Title'][df_titanic_data.Title == 'Mme'] = 'Mrs'<br class="title-page-name"/>    df_titanic_data['Title'][df_titanic_data.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'<br class="title-page-name"/>    df_titanic_data['Title'][df_titanic_data.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'<br class="title-page-name"/><br class="title-page-name"/>    # binarizing all the features<br class="title-page-name"/>    if keep_binary:<br class="title-page-name"/>        df_titanic_data = pd.concat(<br class="title-page-name"/>            [df_titanic_data, pd.get_dummies(df_titanic_data['Title']).rename(columns=lambda x: 'Title_' + str(x))],<br class="title-page-name"/>            axis=1)<br class="title-page-name"/><br class="title-page-name"/>    # scaling<br class="title-page-name"/>    if keep_scaled:<br class="title-page-name"/>        scaler_preprocessing = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data['Names_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Names.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/>    # binning<br class="title-page-name"/>    if keep_bins:<br class="title-page-name"/>        df_titanic_data['Title_id'] = pd.factorize(df_titanic_data['Title'])[0] + 1<br class="title-page-name"/><br class="title-page-name"/>    if keep_bins and keep_scaled:<br class="title-page-name"/>        scaler = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data['Title_id_scaled'] = scaler.fit_transform(df_titanic_data.Title_id.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Generate features from the cabin input variable<br class="title-page-name"/>def process_cabin():<br class="title-page-name"/>    # refering to the global variable that contains the titanic examples<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    # repllacing the missing value in cabin variable "U0"<br class="title-page-name"/>    df_titanic_data['Cabin'][df_titanic_data.Cabin.isnull()] = 'U0'<br class="title-page-name"/><br class="title-page-name"/>    # the cabin number is a sequence of of alphanumerical digits, so we are going to create some features<br class="title-page-name"/>    # from the alphabetical part of it<br class="title-page-name"/>    df_titanic_data['CabinLetter'] = df_titanic_data['Cabin'].map(lambda l: get_cabin_letter(l))<br class="title-page-name"/>    df_titanic_data['CabinLetter'] = pd.factorize(df_titanic_data['CabinLetter'])[0]<br class="title-page-name"/><br class="title-page-name"/>    # binarizing the cabin letters features<br class="title-page-name"/>    if keep_binary:<br class="title-page-name"/>        cletters = pd.get_dummies(df_titanic_data['CabinLetter']).rename(columns=lambda x: 'CabinLetter_' + str(x))<br class="title-page-name"/>        df_titanic_data = pd.concat([df_titanic_data, cletters], axis=1)<br class="title-page-name"/><br class="title-page-name"/>    # creating features from the numerical side of the cabin<br class="title-page-name"/>    df_titanic_data['CabinNumber'] = df_titanic_data['Cabin'].map(lambda x: get_cabin_num(x)).astype(int) + 1<br class="title-page-name"/><br class="title-page-name"/>    # scaling the feature<br class="title-page-name"/>    if keep_scaled:<br class="title-page-name"/>        scaler_processing = preprocessing.StandardScaler() # handling the missing values by replacing it with the median feare<br class="title-page-name"/>    df_titanic_data['Fare'][np.isnan(df_titanic_data['Fare'])] = df_titanic_data['Fare'].median()<br class="title-page-name"/>    df_titanic_data['CabinNumber_scaled'] = scaler_processing.fit_transform(df_titanic_data.CabinNumber.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>def get_cabin_letter(cabin_value):<br class="title-page-name"/>    # searching for the letters in the cabin alphanumerical value<br class="title-page-name"/>    letter_match = re.compile("([a-zA-Z]+)").search(cabin_value)<br class="title-page-name"/><br class="title-page-name"/>    if letter_match:<br class="title-page-name"/>        return letter_match.group()<br class="title-page-name"/>    else:<br class="title-page-name"/>        return 'U'<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>def get_cabin_num(cabin_value):<br class="title-page-name"/>    # searching for the numbers in the cabin alphanumerical value<br class="title-page-name"/>    number_match = re.compile("([0-9]+)").search(cabin_value)<br class="title-page-name"/><br class="title-page-name"/>    if number_match:<br class="title-page-name"/>        return number_match.group()<br class="title-page-name"/>    else:<br class="title-page-name"/>        return 0<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># helper function for constructing features from the ticket fare variable<br class="title-page-name"/>def process_fare():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    # handling the missing values by replacing it with the median feare<br class="title-page-name"/>    df_titanic_data['Fare'][np.isnan(df_titanic_data['Fare'])] = df_titanic_data['Fare'].median()<br class="title-page-name"/><br class="title-page-name"/>    # zeros in the fare will cause some division problems so we are going to set them to 1/10th of the lowest fare<br class="title-page-name"/>    df_titanic_data['Fare'][np.where(df_titanic_data['Fare'] == 0)[0]] = df_titanic_data['Fare'][<br class="title-page-name"/>                                                                             df_titanic_data['Fare'].nonzero()[<br class="title-page-name"/>                                                                                 0]].min() / 10<br class="title-page-name"/><br class="title-page-name"/>    # Binarizing the features by binning them into quantiles<br class="title-page-name"/>    df_titanic_data['Fare_bin'] = pd.qcut(df_titanic_data['Fare'], 4)<br class="title-page-name"/><br class="title-page-name"/>    if keep_binary:<br class="title-page-name"/>        df_titanic_data = pd.concat(<br class="title-page-name"/>            [df_titanic_data, pd.get_dummies(df_titanic_data['Fare_bin']).rename(columns=lambda x: 'Fare_' + str(x))],<br class="title-page-name"/>            axis=1)<br class="title-page-name"/><br class="title-page-name"/>    # binning<br class="title-page-name"/>    if keep_bins:<br class="title-page-name"/>        df_titanic_data['Fare_bin_id'] = pd.factorize(df_titanic_data['Fare_bin'])[0] + 1<br class="title-page-name"/><br class="title-page-name"/>    # scaling the value<br class="title-page-name"/>    if keep_scaled:<br class="title-page-name"/>        scaler_processing = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data['Fare_scaled'] = scaler_processing.fit_transform(df_titanic_data.Fare.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/>    if keep_bins and keep_scaled:<br class="title-page-name"/>        scaler_processing = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data['Fare_bin_id_scaled'] = scaler_processing.fit_transform(<br class="title-page-name"/>            df_titanic_data.Fare_bin_id.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/>    if not keep_strings:<br class="title-page-name"/>        df_titanic_data.drop('Fare_bin', axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Helper function for constructing features from the ticket variable<br class="title-page-name"/>def process_ticket():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data['TicketPrefix'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_prefix(y.upper()))<br class="title-page-name"/>    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('[\.?\/?]', '', y))<br class="title-page-name"/>    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('STON', 'SOTON', y))<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data['TicketPrefixId'] = pd.factorize(df_titanic_data['TicketPrefix'])[0]<br class="title-page-name"/><br class="title-page-name"/>    # binarzing features for each ticket layer<br class="title-page-name"/>    if keep_binary:<br class="title-page-name"/>        prefixes = pd.get_dummies(df_titanic_data['TicketPrefix']).rename(columns=lambda y: 'TicketPrefix_' + str(y))<br class="title-page-name"/>        df_titanic_data = pd.concat([df_titanic_data, prefixes], axis=1)<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data.drop(['TicketPrefix'], axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data['TicketNumber'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_num(y))<br class="title-page-name"/>    df_titanic_data['TicketNumberDigits'] = df_titanic_data['TicketNumber'].map(lambda y: len(y)).astype(np.int)<br class="title-page-name"/>    df_titanic_data['TicketNumberStart'] = df_titanic_data['TicketNumber'].map(lambda y: y[0:1]).astype(np.int)<br class="title-page-name"/><br class="title-page-name"/>    df_titanic_data['TicketNumber'] = df_titanic_data.TicketNumber.astype(np.int)<br class="title-page-name"/><br class="title-page-name"/>    if keep_scaled:<br class="title-page-name"/>        scaler_processing = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data['TicketNumber_scaled'] = scaler_processing.fit_transform(<br class="title-page-name"/>            df_titanic_data.TicketNumber.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>def get_ticket_prefix(ticket_value):<br class="title-page-name"/>    # searching for the letters in the ticket alphanumerical value<br class="title-page-name"/>    match_letter = re.compile("([a-zA-Z\.\/]+)").search(ticket_value)<br class="title-page-name"/>    if match_letter:<br class="title-page-name"/>        return match_letter.group()<br class="title-page-name"/>    else:<br class="title-page-name"/>        return 'U'<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>def get_ticket_num(ticket_value):<br class="title-page-name"/>    # searching for the numbers in the ticket alphanumerical value<br class="title-page-name"/>    match_number = re.compile("([\d]+$)").search(ticket_value)<br class="title-page-name"/>    if match_number:<br class="title-page-name"/>        return match_number.group()<br class="title-page-name"/>    else:<br class="title-page-name"/>        return '0'<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># constructing features from the passenger class variable<br class="title-page-name"/>def process_PClass():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    # using the most frequent value(mode) to replace the messing value<br class="title-page-name"/>    df_titanic_data.Pclass[df_titanic_data.Pclass.isnull()] = df_titanic_data.Pclass.dropna().mode().values<br class="title-page-name"/><br class="title-page-name"/>    # binarizing the features<br class="title-page-name"/>    if keep_binary:<br class="title-page-name"/>        df_titanic_data = pd.concat(<br class="title-page-name"/>            [df_titanic_data, pd.get_dummies(df_titanic_data['Pclass']).rename(columns=lambda y: 'Pclass_' + str(y))],<br class="title-page-name"/>            axis=1)<br class="title-page-name"/><br class="title-page-name"/>    if keep_scaled:<br class="title-page-name"/>        scaler_preprocessing = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data['Pclass_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Pclass.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># constructing features based on the family variables subh as SibSp and Parch<br class="title-page-name"/>def process_family():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/><br class="title-page-name"/>    # ensuring that there's no zeros to use interaction variables<br class="title-page-name"/>    df_titanic_data['SibSp'] = df_titanic_data['SibSp'] + 1<br class="title-page-name"/>    df_titanic_data['Parch'] = df_titanic_data['Parch'] + 1<br class="title-page-name"/><br class="title-page-name"/>    # scaling<br class="title-page-name"/>    if keep_scaled:<br class="title-page-name"/>        scaler_preprocessing = preprocessing.StandardScaler()<br class="title-page-name"/>        df_titanic_data['SibSp_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.SibSp.reshape(-1, 1))<br class="title-page-name"/>        df_titanic_data['Parch_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Parch.reshape(-1, 1))<br class="title-page-name"/><br class="title-page-name"/>    # binarizing all the features<br class="title-page-name"/>    if keep_binary:<br class="title-page-name"/>        sibsps_var = pd.get_dummies(df_titanic_data['SibSp']).rename(columns=lambda y: 'SibSp_' + str(y))<br class="title-page-name"/>        parchs_var = pd.get_dummies(df_titanic_data['Parch']).rename(columns=lambda y: 'Parch_' + str(y))<br class="title-page-name"/>        df_titanic_data = pd.concat([df_titanic_data, sibsps_var, parchs_var], axis=1)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># binarzing the sex variable<br class="title-page-name"/>def process_sex():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/>    df_titanic_data['Gender'] = np.where(df_titanic_data['Sex'] == 'male', 1, 0)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># dropping raw original<br class="title-page-name"/>def process_drops():<br class="title-page-name"/>    global df_titanic_data<br class="title-page-name"/>    drops = ['Name', 'Names', 'Title', 'Sex', 'SibSp', 'Parch', 'Pclass', 'Embarked', \<br class="title-page-name"/>             'Cabin', 'CabinLetter', 'CabinNumber', 'Age', 'Fare', 'Ticket', 'TicketNumber']<br class="title-page-name"/>    string_drops = ['Title', 'Name', 'Cabin', 'Ticket', 'Sex', 'Ticket', 'TicketNumber']<br class="title-page-name"/>    if not keep_raw:<br class="title-page-name"/>        df_titanic_data.drop(drops, axis=1, inplace=True)<br class="title-page-name"/>    elif not keep_strings:<br class="title-page-name"/>        df_titanic_data.drop(string_drops, axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># handling all the feature engineering tasks<br class="title-page-name"/>def get_titanic_dataset(binary=False, bins=False, scaled=False, strings=False, raw=True, pca=False, balanced=False):<br class="title-page-name"/>    global keep_binary, keep_bins, keep_scaled, keep_raw, keep_strings, df_titanic_data<br class="title-page-name"/>    keep_binary = binary<br class="title-page-name"/>    keep_bins = bins<br class="title-page-name"/>    keep_scaled = scaled<br class="title-page-name"/>    keep_raw = raw<br class="title-page-name"/>    keep_strings = strings<br class="title-page-name"/><br class="title-page-name"/>    # reading the train and test sets using pandas<br class="title-page-name"/>    train_data = pd.read_csv('data/train.csv', header=0)<br class="title-page-name"/>    test_data = pd.read_csv('data/test.csv', header=0)<br class="title-page-name"/><br class="title-page-name"/>    # concatenate the train and test set together for doing the overall feature engineering stuff<br class="title-page-name"/>    df_titanic_data = pd.concat([train_data, test_data])<br class="title-page-name"/><br class="title-page-name"/>    # removing duplicate indices due to coming the train and test set by re-indexing the data<br class="title-page-name"/>    df_titanic_data.reset_index(inplace=True)<br class="title-page-name"/><br class="title-page-name"/>    # removing the index column the reset_index() function generates<br class="title-page-name"/>    df_titanic_data.drop('index', axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/>    # index the columns to be 1-based index<br class="title-page-name"/>    df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)<br class="title-page-name"/><br class="title-page-name"/>    # processing the titanic raw variables using the helper functions that we defined above<br class="title-page-name"/>    process_cabin()<br class="title-page-name"/>    process_ticket()<br class="title-page-name"/>    process_name()<br class="title-page-name"/>    process_fare()<br class="title-page-name"/>    process_embarked()<br class="title-page-name"/>    process_family()<br class="title-page-name"/>    process_sex()<br class="title-page-name"/>    process_PClass()<br class="title-page-name"/>    process_age()<br class="title-page-name"/>    process_drops()<br class="title-page-name"/><br class="title-page-name"/>    # move the survived column to be the first<br class="title-page-name"/>    columns_list = list(df_titanic_data.columns.values)<br class="title-page-name"/>    columns_list.remove('Survived')<br class="title-page-name"/>    new_col_list = list(['Survived'])<br class="title-page-name"/>    new_col_list.extend(columns_list)<br class="title-page-name"/>    df_titanic_data = df_titanic_data.reindex(columns=new_col_list)<br class="title-page-name"/><br class="title-page-name"/>    print("Starting with", df_titanic_data.columns.size,<br class="title-page-name"/>          "manually constructing features based on the interaction between them...\n", df_titanic_data.columns.values)<br class="title-page-name"/><br class="title-page-name"/>    # Constructing features manually based on the interaction between the individual features<br class="title-page-name"/>    numeric_features = df_titanic_data.loc[:,<br class="title-page-name"/>                       ['Age_scaled', 'Fare_scaled', 'Pclass_scaled', 'Parch_scaled', 'SibSp_scaled',<br class="title-page-name"/>                        'Names_scaled', 'CabinNumber_scaled', 'Age_bin_id_scaled', 'Fare_bin_id_scaled']]<br class="title-page-name"/>    print("\nUsing only numeric features for automated feature generation:\n", numeric_features.head(10))<br class="title-page-name"/><br class="title-page-name"/>    new_fields_count = 0<br class="title-page-name"/>    for i in range(0, numeric_features.columns.size - 1):<br class="title-page-name"/>        for j in range(0, numeric_features.columns.size - 1):<br class="title-page-name"/>            if i &lt;= j:<br class="title-page-name"/>                name = str(numeric_features.columns.values[i]) + "*" + str(numeric_features.columns.values[j])<br class="title-page-name"/>                df_titanic_data = pd.concat(<br class="title-page-name"/>                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] * numeric_features.iloc[:, j], name=name)],<br class="title-page-name"/>                    axis=1)<br class="title-page-name"/>                new_fields_count += 1<br class="title-page-name"/>            if i &lt; j:<br class="title-page-name"/>                name = str(numeric_features.columns.values[i]) + "+" + str(numeric_features.columns.values[j])<br class="title-page-name"/>                df_titanic_data = pd.concat(<br class="title-page-name"/>                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] + numeric_features.iloc[:, j], name=name)],<br class="title-page-name"/>                    axis=1)<br class="title-page-name"/>                new_fields_count += 1<br class="title-page-name"/>            if not i == j:<br class="title-page-name"/>                name = str(numeric_features.columns.values[i]) + "/" + str(numeric_features.columns.values[j])<br class="title-page-name"/>                df_titanic_data = pd.concat(<br class="title-page-name"/>                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] / numeric_features.iloc[:, j], name=name)],<br class="title-page-name"/>                    axis=1)<br class="title-page-name"/>                name = str(numeric_features.columns.values[i]) + "-" + str(numeric_features.columns.values[j])<br class="title-page-name"/>                df_titanic_data = pd.concat(<br class="title-page-name"/>                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] - numeric_features.iloc[:, j], name=name)],<br class="title-page-name"/>                    axis=1)<br class="title-page-name"/>                new_fields_count += 2<br class="title-page-name"/><br class="title-page-name"/>    print("\n", new_fields_count, "new features constructed")<br class="title-page-name"/><br class="title-page-name"/>    # using Spearman correlation method to remove the feature that have high correlation<br class="title-page-name"/><br class="title-page-name"/>    # calculating the correlation matrix<br class="title-page-name"/>    df_titanic_data_cor = df_titanic_data.drop(['Survived', 'PassengerId'], axis=1).corr(method='spearman')<br class="title-page-name"/><br class="title-page-name"/>    # creating a mask that will ignore correlated ones<br class="title-page-name"/>    mask_ignore = np.ones(df_titanic_data_cor.columns.size) - np.eye(df_titanic_data_cor.columns.size)<br class="title-page-name"/>    df_titanic_data_cor = mask_ignore * df_titanic_data_cor<br class="title-page-name"/><br class="title-page-name"/>    features_to_drop = []<br class="title-page-name"/><br class="title-page-name"/>    # dropping the correclated features<br class="title-page-name"/>    for column in df_titanic_data_cor.columns.values:<br class="title-page-name"/><br class="title-page-name"/>        # check if we already decided to drop this variable<br class="title-page-name"/>        if np.in1d([column], features_to_drop):<br class="title-page-name"/>            continue<br class="title-page-name"/><br class="title-page-name"/>        # finding highly correlacted variables<br class="title-page-name"/>        corr_vars = df_titanic_data_cor[abs(df_titanic_data_cor[column]) &gt; 0.98].index<br class="title-page-name"/>        features_to_drop = np.union1d(features_to_drop, corr_vars)<br class="title-page-name"/><br class="title-page-name"/>    print("\nWe are going to drop", features_to_drop.shape[0], " which are highly correlated features...\n")<br class="title-page-name"/>    df_titanic_data.drop(features_to_drop, axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/>    # splitting the dataset to train and test and do PCA<br class="title-page-name"/>    train_data = df_titanic_data[:train_data.shape[0]]<br class="title-page-name"/>    test_data = df_titanic_data[test_data.shape[0]:]<br class="title-page-name"/><br class="title-page-name"/>    if pca:<br class="title-page-name"/>        print("reducing number of variables...")<br class="title-page-name"/>        train_data, test_data = reduce(train_data, test_data)<br class="title-page-name"/>    else:<br class="title-page-name"/>        # drop the empty 'Survived' column for the test set that was created during set concatenation<br class="title-page-name"/>        test_data.drop('Survived', axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/>    print("\n", train_data.columns.size, "initial features generated...\n") # , input_df.columns.values<br class="title-page-name"/><br class="title-page-name"/>    return train_data, test_data<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># reducing the dimensionality for the training and testing set<br class="title-page-name"/>def reduce(train_data, test_data):<br class="title-page-name"/>    # join the full data together<br class="title-page-name"/>    df_titanic_data = pd.concat([train_data, test_data])<br class="title-page-name"/>    df_titanic_data.reset_index(inplace=True)<br class="title-page-name"/>    df_titanic_data.drop('index', axis=1, inplace=True)<br class="title-page-name"/>    df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)<br class="title-page-name"/><br class="title-page-name"/>    # converting the survived column to series<br class="title-page-name"/>    survived_series = pd.Series(df_titanic_data['Survived'], name='Survived')<br class="title-page-name"/><br class="title-page-name"/>    print(df_titanic_data.head())<br class="title-page-name"/><br class="title-page-name"/>    # getting the input and target values<br class="title-page-name"/>    input_values = df_titanic_data.values[:, 1::]<br class="title-page-name"/>    target_values = df_titanic_data.values[:, 0]<br class="title-page-name"/><br class="title-page-name"/>    print(input_values[0:10])<br class="title-page-name"/><br class="title-page-name"/>    # minimum variance percentage that should be covered by the reduced number of variables<br class="title-page-name"/>    variance_percentage = .99<br class="title-page-name"/><br class="title-page-name"/>    # creating PCA object<br class="title-page-name"/>    pca_object = PCA(n_components=variance_percentage)<br class="title-page-name"/><br class="title-page-name"/>    # trasforming the features<br class="title-page-name"/>    input_values_transformed = pca_object.fit_transform(input_values, target_values)<br class="title-page-name"/><br class="title-page-name"/>    # creating a datafram for the transformed variables from PCA<br class="title-page-name"/>    pca_df = pd.DataFrame(input_values_transformed)<br class="title-page-name"/><br class="title-page-name"/>    print(pca_df.shape[1], " reduced components which describe ", str(variance_percentage)[1:], "% of the variance")<br class="title-page-name"/><br class="title-page-name"/>    # constructing a new dataframe that contains the newly reduced vars of PCA<br class="title-page-name"/>    df_titanic_data = pd.concat([survived_series, pca_df], axis=1)<br class="title-page-name"/><br class="title-page-name"/>    # split into separate input and test sets again<br class="title-page-name"/>    train_data = df_titanic_data[:train_data.shape[0]]<br class="title-page-name"/>    test_data = df_titanic_data[test_data.shape[0]:]<br class="title-page-name"/>    test_data.reset_index(inplace=True)<br class="title-page-name"/>    test_data.drop('index', axis=1, inplace=True)<br class="title-page-name"/>    test_data.drop('Survived', axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/>    return train_data, test_data<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Calling the helper functions<br class="title-page-name"/>if __name__ == '__main__':<br class="title-page-name"/>    train, test = get_titanic_dataset(bins=True, scaled=True, binary=True)<br class="title-page-name"/>    initial_drops = ['PassengerId']<br class="title-page-name"/>    train.drop(initial_drops, axis=1, inplace=True)<br class="title-page-name"/>    test.drop(initial_drops, axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/>    train, test = reduce(train, test)<br class="title-page-name"/><br class="title-page-name"/>    print(train.columns.values)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bias-variance decomposition</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous section, we knew how to select the best hyperparameters for our model. This set of best hyperparameters was chosen based on the measure of minimizing the cross validated error. Now, we need to see how the model will perform over the unseen data, or the so-called out-of-sample data, which refers to new data samples that haven't been seen during the model training phase.</p>
<p class="calibre2">Consider the following example: we have a data sample of size 10,000, and we are going to train the same model with different train set sizes and plot the test error at each step. For example, we are going to take out 1,000 as a test set and use the other 9,000 for training. So for the first training round, we will randomly select a train set of size 100 out of those 9,000 items. We'll train the model based on the <em class="calibre19">best</em> selected set of hyperparameters, test the model with the test set, and finally plot the train (in-sample) error and the test (out-of-sample) error. We repeat this training, testing, and plotting operation for different train sizes (for example, repeat with 500 out of the 9,000, then 1,000 out of the 9,000, and so on).</p>
<p class="calibre2">After doing all this training, testing, and plotting, we will get a graph of two curves, representing the train and test errors with the same model but across different train set sizes. From this graph, we will get to know how good our model is.</p>
<p class="calibre2">The output graph, which will contain two curves representing the training and testing error, will be one of the four possible shapes shown in <em class="calibre19">Figure 2</em>. The source of this different shapes is Andrew Ng's Machine Learning course on Coursera (<a href="https://www.coursera.org/learn/machine-learning" class="calibre11">https://www.coursera.org/learn/machine-learning</a>). It's a great course with lots of insights and best practices for machine learning newbies:</p>
<div class="CDPAlignCenter"><img src="assets/9a88f3c0-8a00-4e65-816b-022e6450a072.png" class="calibre51"/></div>
<div class="CDPAlignCenter1">Figure 2: Possible shapes for plotting the training and testing error over different training set sizes</div>
<p class="calibre2">So, when should we accept our model and put it into production? And when do we know that our model is not performing well over the test set and hence won't have a bad generalization error? The answer to these questions depends on the shape that you get from plotting the train error versus the test error on different training set sizes:</p>
<ul class="calibre7">
<li class="calibre8">If your shape looks like the <em class="calibre25">top left</em> one, it represents a low training error and generalizes well over the test set. This shape is a winner and you should go ahead and use this model in production.</li>
<li class="calibre8">If your shape is similar to the <em class="calibre25">top right</em> one, it represents a high training error (the model didn't manage to learn from the training samples) and even has worse generalization performance over the test set. This shape is a complete failure and you need to go back and see what's wrong with your data, chosen learning algorithm, and/or selected hyperparameters.</li>
<li class="calibre8">If your shape is similar to the <em class="calibre25">bottom left</em> one, it represents a bad training error as the model didn't manage to capture the underlying structure of the data, which also fits the new test data.</li>
<li class="calibre8">If your shape is similar to the <em class="calibre25">bottom right</em> one, it represents high bias and variance. This means that your model hasn't figured out the training data very well and hence didn't generalize well over the testing set.</li>
</ul>
<p class="calibre2">Bias and variance are the components that we can use to figure out how good our model is. In supervised learning, there are two opposing sources of errors, and using the learning curves in <em class="calibre19">Figure 2</em>, we can figure out due to which component(s) our model is suffering. The problem of having high variance and low bias is called <strong class="calibre13">overfitting</strong>, which means that the model performed well over the training samples but didn't generalize well on the test set. On the other hand, the problem of having high bias and low variance is called <strong class="calibre13">underfitting</strong>, which means that the model didn't make use of the data and didn't manage to estimate the output/target from the input features. There are different approaches one can use to avoid getting into one of these problems. But usually, enhancing one of them will come at the expense of the second one.</p>
<p class="calibre2">We can solve the situation of high variance by adding more features from which the model can learn. This solution will most likely increase the bias, so you need to make some sort of trade-off between them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning visibility</h1>
                </header>
            
            <article>
                
<p class="calibre2">There are lots of great data science algorithms that one can use to solve problems in different domains, but the key component that makes the learning process visible is having enough data. You might ask how much data is needed for the learning process to be visible and worth doing. As a rule of thumb, researchers and machine learning practitioners agree that you need to have data samples at least 10 times the number of <strong class="calibre13">degrees of freedom</strong> in your model.</p>
<p class="calibre2">For example, in the case of linear models, the degree of freedom represents the number of features that you have in your dataset. If you have 50 explanatory features in your data, then you need at least 500 data samples/observations in your data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breaking the rule of thumb</h1>
                </header>
            
            <article>
                
<p class="calibre2">In practice, you can get away with this rule and do learning with less than 10 times the number of features in your data; this mostly happens if your model is simple and you are using something called <strong class="calibre13">regularization</strong> (addressed in the next chapter).</p>
<p class="calibre2">Jake Vanderplas wrote an article (<a href="https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/" class="calibre11">https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/</a>) to show that one can learn even if the data has more parameters than examples. To demonstrate this, he used regularization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we covered the most important tools that machine learning practitioners use in order to make sense of their data and get the learning algorithm to get the most out of their data.</p>
<p class="calibre2">Feature engineering was the first and commonly used tool in data science; it's a must-have component in any data science pipeline. The purpose of this tool is to make better representations for your data and increase the predictive power of your model.</p>
<p class="calibre2">We saw how a large number of features can be problematic and lead to worse classifier performance. We also saw that there is an optimal number of features that should be used to get the maximum model performance, and this optimal number of features is a function of the number of data samples/observations you got.</p>
<p class="calibre2">Subsequently, we introduced one of the most powerful tools, which is bias-variance decomposition. This tool is widely used to test how good the model is over the test set.</p>
<p class="calibre2">Finally, we went through learning visibility, which answers the question of how much data <span class="calibre10">we </span>should need in order to get in business and do machine learning. The rule of thumb showed that we need data samples/observations at least 10 times the number of features in your data. However, this rule of thumb <span class="calibre10">can </span>be broken by using another tool called regularization, which will be addressed in more detail in the next chapter.</p>
<p class="calibre2">Next up, we are going to continue to increase our data science tools that we can use to drive meaningful analytics from our data, and face some daily problems of applying machine learning.<br class="calibre20"/></p>


            </article>

            
        </section>
    </body></html>