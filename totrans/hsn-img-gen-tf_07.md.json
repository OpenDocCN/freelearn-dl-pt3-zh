["```\nvgg = tf.keras.applications.VGG19(include_top=False, \t\t\t\t\t\t weights='imagenet')\ncontent_layers = ['block4_conv2']\ncontent_outputs = [vgg.get_layer(x).output for x in  \t\t\t\t\tcontent_layers]\nmodel = Model(vgg.input, content_outputs)\n```", "```\ndef extract_features(image):\n    image = tf.keras.applications.vgg19.\\ \t\t\tpreprocess_input(image *255.)\n    content_ref = model(image)\n    return content_ref\ncontent_image = tf.reverse(content_image, axis=[-1])\ncontent_ref = extract_features(content_image) \n```", "```\nimage = tf.Variable(tf.random.normal( \t\t\t\t\tshape=content_image.shape))\n```", "```\ndef calc_loss(y_true, y_pred):\n    loss = [tf.reduce_sum((x-y)**2) for x, y in  \t\t\t\t\tzip(y_pred, y_true)]\n    return tf.reduce_mean(loss)\n```", "```\nfor i in range(1,steps+1):\n    with tf.GradientTape() as tape:\n        content_features = self.extract_features(image)\n        loss = calc_loss(content_features, content_ref)\n    grad = tape.gradient(loss, image)\n    optimizer.apply_gradients([(grad, image)])\n    image.assign(tf.clip_by_value(image, 0., 1.))\n```", "```\ndef gram_matrix(x):\n    x = tf.transpose(tf.squeeze(x), (2,0,1));\n    x = tf.keras.backend.batch_flatten(x)\n    num_points = x.shape[-1]\n    gram = tf.linalg.matmul(x, tf.transpose(x))/num_points\n    return gram\n```", "```\ndef extract_features(image):\n    image = tf.keras.applications.vgg19.\\ \t\t\t\tpreprocess_input(image *255.)\n    styles = self.model(image)\n    styles = [self.gram_matrix(s) for s in styles]\n    return styles\n```", "```\nvgg = tf.keras.applications.VGG19(include_top=False, \t\t\t\t\t\t  weights='imagenet')\ndefault_content_layers = ['block5_conv1']\ndefault_style_layers = ['block1_conv1',\n                        'block2_conv1',\n                        'block3_conv1', \n                        'block4_conv1', \n                        'block5_conv1']\ncontent_layers = content_layers if content_layers else default_content_layers\nstyle_layers = style_layers if style_layers else default_style_layers\nself.content_outputs = [vgg.get_layer(x).output for x in content_layers]\nself.style_outputs = [vgg.get_layer(x).output for x in style_layers]\nself.model = Model(vgg.input, [self.content_outputs, \t\t\t\t\t    self.style_outputs])\n```", "```\ncontent_ref, _ = self.extract_features(content_image)\n_, style_ref = self.extract_features(style_image)\n```", "```\ndef train_step(self, image, content_ref, style_ref):\n    with tf.GradientTape() as tape:\n        content_features, style_features = \\ \t\t\t\t\tself.extract_features(image)\n        content_loss = self.content_weight*self.calc_loss( \t\t\t\t\t  content_ref, content_features)\n        style_loss = self.style_weight*self.calc_loss( \t\t\t\t\t\tstyle_ref, style_features)\n        loss = content_loss + style_loss\n    grad = tape.gradient(loss, image)\n    self.optimizer.apply_gradients([(grad, image)])\n    image.assign(tf.clip_by_value(image, 0., 1.))\n    return content_loss, style_loss  \n```", "```\nclass AdaIN(layers.Layer):\n    def __init__(self, epsilon=1e-5):\n        super(AdaIN, self).__init__()\n        self.epsilon = epsilon        \n    def call(self, inputs):\n        x = inputs[0] # content\n        y = inputs[1] # style\n        mean_x, var_x = tf.nn.moments(x, axes=(1,2), \t\t\t\t\t\t\tkeepdims=True)\n        mean_y, var_y = tf.nn.moments(y, axes=(1,2), \t\t\t\t\t\t\tkeepdims=True)\n        std_x = tf.sqrt(var_x+self.epsilon)\n        std_y = tf.sqrt(var_y+self.epsilon)\n        output = std_y*(x – mean_x)/(std_x) + mean_y    \n        return output\n```", "```\ndef build_encoder(self, name='encoder'):\n    self.encoder_layers = ['block1_conv1',\n                           'block2_conv1',\n                           'block3_conv1', \n                           'block4_conv1']\n    vgg = tf.keras.applications.VGG19(include_top=False, \t\t\t\t\t\t\tweights='imagenet') \n    layer_outputs = [vgg.get_layer(x).output for x in  \t\t\t\t\tself.encoder_layers] \n    return Model(vgg.input, layer_outputs, name=name)\n```", "```\nclass Conv2D(layers.Layer):\n    @tf.function\n    def call(self, inputs):\n        padded = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], \t\t\t\t\t\t[0, 0]], mode='REFLECT')\n        # perform conv2d using low level API\n        output = tf.nn.conv2d(padded, self.w, strides=1, \t\t\t\t\t   padding=”VALID”) + self.b\n        if self.use_relu:\n            output = tf.nn.relu(output)\n        return output\n```", "```\ndef build_decoder(self):\n    block = tf.keras.Sequential([\\\n            Conv2D(512, 256, 3),\n            UpSampling2D((2,2)),\n            Conv2D(256, 256, 3),\n            Conv2D(256, 256, 3),\n            Conv2D(256, 256, 3),\n            Conv2D(256, 128, 3),\n            UpSampling2D((2,2)),\n            Conv2D(128, 128, 3),\n            Conv2D(128, 64, 3),\n            UpSampling2D((2,2)),\n            Conv2D(64, 64, 3),\n            Conv2D(64, 3, 3, use_relu=False)],\n                               name='decoder')\n    return block\n```", "```\ndef preprocess(self, image):\n    # rgb to bgr\n    image = tf.reverse(image, axis=[-1])\n    return tf.keras.applications.vgg19.preprocess_input(image)\n```", "```\ndef postprocess(self, image):\n    return tf.clip_by_value(image, 0., 255.)\n```", "```\ncontent_image = self.preprocess(content_image_input)\nstyle_image = self.preprocess(style_image_input) \nself.content_target = self.encoder(content_image)\nself.style_target = self.encoder(style_image) \nadain_output = AdaIN()([self.content_target[-1], \t\t\t\t  self.style_target[-1]]) \nself.stylized_image = self.postprocess( \t\t\t\t\tself.decoder(adain_output)) \nself.stn = Model([content_image_input,  \t\t\t  style_image_input],  \t\t\t  self.stylized_image)\n```", "```\ncontent_loss =  tf.reduce_sum((output_features[-1]-\\ \t\t\t\t\t    adain_output)**2)\n```", "```\ndef calc_style_loss(self, y_true, y_pred):\n    n_features = len(y_true)\n    epsilon = 1e-5\n    loss = [] \n    for i in range(n_features):\n        mean_true, var_true = tf.nn.moments(y_true[i], \t\t\t\t\t\taxes=(1,2), keepdims=True)\n        mean_pred, var_pred = tf.nn.moments(y_pred[i], \t\t\t\t\t\taxes=(1,2), keepdims=True)\n        std_true, std_pred = tf.sqrt(var_true+epsilon), \t\t\t\t\t\ttf.sqrt(var_pred+epsilon)\n        mean_loss = tf.reduce_sum(tf.square( \t\t\t\t\t\t    mean_true-mean_pred))\n        std_loss = tf.reduce_sum(tf.square( \t\t\t\t\t\t  std_true-std_pred))\n        loss.append(mean_loss + std_loss) \n    return tf.reduce_mean(loss)\n```", "```\ndef train_step(self, train_data):\n    with tf.GradientTape() as tape:\n        adain_output, output_features, style_target = \\ \t\t\t\t\tself.training_model(train_data) \n        content_loss = tf.reduce_sum( \t\t\t\t(output_features[-1]-adain_output)\\ \t\t\t\t\t\t\t\t\t**2)\n        style_loss = self.style_weight * \\ \t\t\t\t  self.calc_style_loss( \t\t\t\t\tstyle_target, output_features)\n        loss =  content_loss + style_loss \n        gradients = tape.gradient(loss, \t\t\t\t self.decoder.trainable_variables) \n        self.optimizer.apply_gradients(zip(gradients, \t\t\t\tself.decoder.trainable_variables)) \n    return content_loss, style_loss\n```"]