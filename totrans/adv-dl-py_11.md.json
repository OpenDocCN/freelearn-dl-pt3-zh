["```\nclass EncoderRNN(torch.nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Embedding for the input words\n        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n\n        # The actual rnn sell\n        self.rnn_cell = torch.nn.GRU(hidden_size, hidden_size)\n```", "```\ndef forward(self, input, hidden):\n    # Pass through the embedding\n    embedded = self.embedding(input).view(1, 1, -1)\n    output = embedded\n\n    # Pass through the RNN\n    output, hidden = self.rnn_cell(output, hidden)\n    return output, hidden\n```", "```\ndef init_hidden(self):\n    return torch.zeros(1, 1, self.hidden_size, device=device)\n```", "```\nclass DecoderRNN(torch.nn.Module):\n\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        # Embedding for the current input word\n        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n\n        # decoder cell\n        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n\n        # Current output word\n        self.out = torch.nn.Linear(hidden_size, output_size)\n        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n```", "```\n    def forward(self, input, hidden, _):\n        # Pass through the embedding\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = torch.nn.functional.relu(embedded)\n\n        # Pass through the RNN cell\n        output, hidden = self.rnn_cell(embedded, hidden)\n\n        # Produce output word\n        output = self.log_softmax(self.out(output[0]))\n        return output, hidden, _\n```", "```\nclass AttnDecoderRNN(torch.nn.Module):\n    def __init__(self, hidden_size, output_size, max_length=MAX_LENGTH,\n    dropout=0.1):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.max_length = max_length\n\n        # Embedding for the input word\n        self.embedding = torch.nn.Embedding(self.output_size,\n        self.hidden_size)\n\n        self.dropout = torch.nn.Dropout(dropout)\n\n        # Attention portion\n        self.attn = torch.nn.Linear(in_features=self.hidden_size,\n                                    out_features=self.hidden_size)\n\n        self.w_c = torch.nn.Linear(in_features=self.hidden_size * 2,\n                                   out_features=self.hidden_size)\n\n        # RNN\n        self.rnn_cell = torch.nn.GRU(input_size=self.hidden_size,\n                                     hidden_size=self.hidden_size)\n\n        # Output word\n        self.w_y = torch.nn.Linear(in_features=self.hidden_size,\n                                   out_features=self.output_size)\n```", "```\ndef forward(self, input, hidden, encoder_outputs):\n    embedded = self.embedding(input).view(1, 1, -1)\n    embedded = self.dropout(embedded)\n```", "```\n    rnn_out, hidden = self.rnn_cell(embedded, hidden)\n```", "```\n    alignment_scores = torch.mm(self.attn(hidden)[0], encoder_outputs.t())\n```", "```\n    attn_weights = torch.nn.functional.softmax(alignment_scores, dim=1)\n```", "```\n    c_t = torch.mm(attn_weights, encoder_outputs)\n```", "```\n    hidden_s_t = torch.cat([hidden[0], c_t], dim=1)\n    hidden_s_t = torch.tanh(self.w_c(hidden_s_t))\n```", "```\n    output = torch.nn.functional.log_softmax(self.w_y(hidden_s_t), dim=1)\n```", "```\n    return output, hidden, attn_weights\n```", "```\ndef train(encoder, decoder, loss_function, encoder_optimizer, decoder_optimizer, data_loader, max_length=MAX_LENGTH):\n    print_loss_total = 0\n\n    # Iterate over the dataset\n    for i, (input_tensor, target_tensor) in enumerate(data_loader):\n        input_tensor = input_tensor.to(device).squeeze(0)\n        target_tensor = target_tensor.to(device).squeeze(0)\n\n        encoder_hidden = encoder.init_hidden()\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        input_length = input_tensor.size(0)\n        target_length = target_tensor.size(0)\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n        loss = torch.Tensor([0]).squeeze().to(device)\n```", "```\nwith torch.set_grad_enabled(True):\n    # Pass the sequence through the encoder and store the hidden states\n    at each step\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    # Initiate decoder with the GO_token\n    decoder_input = torch.tensor([[GO_token]], device=device)\n\n    # Initiate the decoder with the last encoder hidden state\n    decoder_hidden = encoder_hidden\n\n    # Teacher forcing: Feed the target as the next input\n    for di in range(target_length):\n        decoder_output, decoder_hidden, decoder_attention = decoder(\n            decoder_input, decoder_hidden, encoder_outputs)\n        loss += loss_function(decoder_output, target_tensor[di])\n        decoder_input = target_tensor[di]  # Teacher forcing\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n```", "```\n# Initiate the decoder with the last encoder hidden state\ndecoder_input = torch.tensor([[GO_token]], device=device)  # GO\n\n# Initiate the decoder with the last encoder hidden state\ndecoder_hidden = encoder_hidden\n\ndecoded_words = []\ndecoder_attentions = torch.zeros(max_length, max_length)\n\n# Generate the output sequence (opposite to teacher forcing)\nfor di in range(max_length):\n    decoder_output, decoder_hidden, decoder_attention = decoder(\n        decoder_input, decoder_hidden, encoder_outputs)\n    decoder_attentions[di] = decoder_attention.data\n\n    # Obtain the output word index with the highest probability\n    _, topi = decoder_output.data.topk(1)\n    if topi.item() != EOS_token:\n        decoded_words.append(dataset.output_lang.index2word[topi.item()])\n    else:\n        break\n\n    # Use the latest output word as the next input\n    decoder_input = topi.squeeze().detach()\n```", "```\ndef clones(module: torch.nn.Module, n: int):\n    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n```", "```\ndef attention(query, key, value, mask=None, dropout=None):\n    \"\"\"Scaled Dot Product Attention\"\"\"\n    d_k = query.size(-1)\n\n    # 1) and 2) Compute the alignment scores with scaling\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n    # 3) Compute the attention scores (softmax)\n    p_attn = torch.nn.functional.softmax(scores, dim=-1)\n\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n\n    # 4) Apply the attention scores over the values\n    return torch.matmul(p_attn, value), p_attn\n```", "```\nclass MultiHeadedAttention(torch.nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"\"\"\n        :param h: number of heads\n        :param d_model: query/key/value vector length\n        \"\"\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n\n        # Create 4 fully connected layers\n        # 3 for the query/key/value projections\n        # 1 to concatenate the outputs of all heads\n        self.fc_layers = clones(torch.nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = torch.nn.Dropout(p=dropout)\n```", "```\ndef forward(self, query, key, value, mask=None):\n    if mask is not None:\n        # Same mask applied to all h heads.\n        mask = mask.unsqueeze(1)\n\n    batch_samples = query.size(0)\n\n    # 1) Do all the linear projections in batch from d_model => h x d_k\n    projections = list()\n    for l, x in zip(self.fc_layers, (query, key, value)):\n        projections.append(\n            l(x).view(batch_samples, -1, self.h, self.d_k).transpose(1, 2)\n        )\n\n    query, key, value = projections\n\n    # 2) Apply attention on all the projected vectors in batch.\n    x, self.attn = attention(query, key, value,\n                             mask=mask,\n                             dropout=self.dropout)\n\n    # 3) \"Concat\" using a view and apply a final linear.\n    x = x.transpose(1, 2).contiguous() \\\n        .view(batch_samples, -1, self.h * self.d_k)\n\n    return self.fc_layers[-1](x)\n```", "```\nl(x).view(batch_samples, -1, self.h, self.d_k).transpose(1, 2)\n```", "```\nclass Encoder(torch.nn.Module):\n    def __init__(self, block: EncoderBlock, N: int):\n        super(Encoder, self).__init__()\n        self.blocks = clones(block, N)\n        self.norm = LayerNorm(block.size)\n\n    def forward(self, x, mask):\n        \"\"\"Iterate over all blocks and normalize\"\"\"\n        for layer in self.blocks:\n            x = layer(x, mask)\n\n        return self.norm(x)\n```", "```\nclass EncoderBlock(torch.nn.Module):\n    def __init__(self,\n                 size: int,\n                 self_attn: MultiHeadedAttention,\n                 ffn: PositionwiseFFN,\n                 dropout=0.1):\n        super(EncoderBlock, self).__init__()\n        self.self_attn = self_attn\n        self.ffn = ffn\n\n        # Create 2 sub-layer connections\n        # 1 for the self-attention\n        # 1 for the FFN\n        self.sublayers = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask):\n        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayers[1](x, self.ffn)\n```", "```\nclass SublayerConnection(torch.nn.Module):\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))\n```", "```\nclass PositionwiseFFN(torch.nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout=0.1):\n        super(PositionwiseFFN, self).__init__()\n        self.w_1 = torch.nn.Linear(d_model, d_ff)\n        self.w_2 = torch.nn.Linear(d_ff, d_model)\n        self.dropout = torch.nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(torch.nn.functional.relu(self.w_1(x))))\n```", "```\nclass Decoder(torch.nn.Module):\n    def __init__(self, block: DecoderBlock, N: int, vocab_size: int):\n        super(Decoder, self).__init__()\n        self.blocks = clones(block, N)\n        self.norm = LayerNorm(block.size)\n        self.projection = torch.nn.Linear(block.size, vocab_size)\n\n    def forward(self, x, encoder_states, source_mask, target_mask):\n        for layer in self.blocks:\n            x = layer(x, encoder_states, source_mask, target_mask)\n\n        x = self.norm(x)\n\n        return torch.nn.functional.log_softmax(self.projection(x), dim=-1)\n```", "```\nclass DecoderBlock(torch.nn.Module):\n    def __init__(self,\n                 size: int,\n                 self_attn: MultiHeadedAttention,\n                 encoder_attn: MultiHeadedAttention,\n                 ffn: PositionwiseFFN,\n                 dropout=0.1):\n        super(DecoderBlock, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.encoder_attn = encoder_attn\n        self.ffn = ffn\n\n        # Create 3 sub-layer connections\n        # 1 for the self-attention\n        # 1 for the encoder attention\n        # 1 for the FFN\n        self.sublayers = clones(SublayerConnection(size, dropout), 3)\n\n    def forward(self, x, encoder_states, source_mask, target_mask):\n        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n        x = self.sublayers[1](x, lambda x: self.encoder_attn(x, encoder_states, encoder_states, source_mask))\n        return self.sublayers[2](x, self.ffn)\n```", "```\nclass EncoderDecoder(torch.nn.Module):\n    def __init__(self,\n                 encoder: Encoder,\n                 decoder: Decoder,\n                 source_embeddings: torch.nn.Sequential,\n                 target_embeddings: torch.nn.Sequential):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.source_embeddings = source_embeddings\n        self.target_embeddings = target_embeddings\n\n    def forward(self, source, target, source_mask, target_mask):\n        encoder_output = self.encoder(\n            x=self.source_embeddings(source),\n            mask=source_mask)\n\n        return self.decoder(\n            x=self.target_embeddings(target),\n            encoder_states=encoder_output,\n            source_mask=source_mask,\n            target_mask=target_mask)\n```", "```\ndef build_model(source_vocabulary: int,\n                target_vocabulary: int,\n                N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n    \"\"\"Build the full transformer model\"\"\"\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFFN(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n\n    model = EncoderDecoder(\n        encoder=Encoder(EncoderBlock(d_model, c(attn), c(ff), dropout), N),\n        decoder=Decoder(DecoderBlock(d_model, c(attn), c(attn),c(ff),\n                                    dropout), N, target_vocabulary),\n        source_embeddings=torch.nn.Sequential(\n            Embeddings(d_model, source_vocabulary), c(position)),\n        target_embeddings=torch.nn.Sequential(\n            Embeddings(d_model, target_vocabulary), c(position)))\n\n    # This was important from their code.\n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() > 1:\n            torch.nn.init.xavier_uniform_(p)\n\n    return model\n```", "```\nsource_embeddings=torch.nn.Sequential(Embeddings(d_model, source_vocabulary), c(position))\n```", "```\nimport torch\nfrom transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n```", "```\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate pre-trained model-specific tokenizer and the model itself\ntokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\nmodel = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103').to(device)\n```", "```\ntext = \"The company was founded in\"\ntokens_tensor = \\\n    torch.tensor(tokenizer.encode(text)) \\\n        .unsqueeze(0) \\\n        .to(device)\n```", "```\nmems = None  # recurrence mechanism\n\npredicted_tokens = list()\nfor i in range(50):  # stop at 50 predicted tokens\n    # Generate predictions\n    predictions, mems = model(tokens_tensor, mems=mems)\n\n    # Get most probable word index\n    predicted_index = torch.topk(predictions[0, -1, :], 1)[1]\n\n    # Extract the word from the index\n    predicted_token = tokenizer.decode(predicted_index)\n\n    # break if [EOS] reached\n    if predicted_token == tokenizer.eos_token:\n        break\n\n    # Store the current token\n    predicted_tokens.append(predicted_token)\n\n    # Append new token to the existing sequence\n    tokens_tensor = torch.cat((tokens_tensor, predicted_index.unsqueeze(1)), dim=1)\n```", "```\nprint('Initial sequence: ' + text)\nprint('Predicted output: ' + \" \".join(predicted_tokens))\n```", "```\nInitial sequence: The company was founded in\nPredicted output: the United States .\n```"]