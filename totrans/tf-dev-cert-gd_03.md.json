["```\n    # import tensorflow\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    from tensorflow import keras\n    ```", "```\n    from tensorflow.keras import Sequential\n    ```", "```\n    from tensorflow.keras.layers import Dense\n    ```", "```\n    print(tf.__version__)\n    ```", "```\n2.12.0\n```", "```\n    import numpy as np\n    ```", "```\n    import pandas as pd\n    ```", "```\n    import matplotlib.pyplot as plt\n    ```", "```\n    import seaborn as sns\n    ```", "```\n    from sklearn.model_selection import train_test_split\n    ```", "```\n    from sklearn.preprocessing import MinMaxScaler\n    ```", "```\n    #Loading from the course GitHub account\n    ```", "```\n    df=pd.read_csv('https://raw.githubusercontent.com/oluwole-packt/datasets/main/salary_dataset.csv')\n    ```", "```\n    df.head()\n    ```", "```\n#drop irrelevant columns\ndf =df.drop(columns =['Name', 'Phone_Number',\n    'Date_Of_Birth'])\ndf.head()\n```", "```\n#check the data for any missing values\ndf.isnull().sum()\n```", "```\nExperience       2\nQualification    1\nUniversity       0\nRole             3\nCert             2\nSalary           0\ndtype: int64\n```", "```\n#drop the null values\ndf=df.dropna()\n```", "```\n#check for null values\ndf.isnull().sum()\n```", "```\nExperience       0\nQualification    0\nUniversity       0\nRole             0\nCert             0\nSalary           0\ndtype: int64\n```", "```\ndf.dtypes\n```", "```\nExperience       float64\nQualification     object\nUniversity        object\nRole              object\nCert              object\nSalary             int64\ndtype: object\n```", "```\n#Converting categorical variables to numeric values\ndf = pd.get_dummies(df, drop_first=True)\ndf.head()\n```", "```\ndf.corr()\n```", "```\n# We split the attributes and labels into X and y variables\nX = df.drop(\"Salary\", axis=1)\ny = df[\"Salary\"]\n```", "```\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n    test_size=0.2, random_state=10)\n```", "```\n#create a model using the Keras API\nModel_1 = Sequential([Dense(units=1, activation='linear',\n    input_shape=[len(X_train.columns)])])\n```", "```\n#compile the model\nModel_1.compile(loss=tf.keras.losses.mae,\n    optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])\n```", "```\n#Fit the model\nmodel_1.fit(X_train, y_train, epochs =50)\n```", "```\nEpoch 46/50\n6/6 [==============================] - 0s 9ms/step - loss: 97378.0391 - mae: 97378.0391\nEpoch 47/50\n6/6 [==============================] - 0s 4ms/step - loss: 97377.2500 - mae: 97377.2500\nEpoch 48/50\n6/6 [==============================] - 0s 4ms/step - loss: 97376.4609 - mae: 97376.4609\nEpoch 49/50\n6/6 [==============================] - 0s 3ms/step - loss: 97375.6484 - mae: 97375.6484\nEpoch 50/50\n6/6 [==============================] - 0s 3ms/step - loss: 97374.8516 - mae: 97374.8516\n```", "```\n#create a model using the Keras API\nmodel_2 = Sequential([Dense(units=1, activation='linear',\n    input_shape=[len(X_train.columns)])])\n#compile the model\nmodel_2.compile(loss=tf.keras.losses.mae,\n    optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])\n#Fit the model\nhistory=model_2.fit(X_train, y_train, epochs =500)\n```", "```\nEpoch 496/500\n6/6 [==============================] - 0s 3ms/step - loss: 97014.8516 - mae: 97014.8516\nEpoch 497/500\n6/6 [==============================] - 0s 2ms/step - loss: 97014.0391 - mae: 97014.0391\nEpoch 498/500\n6/6 [==============================] - 0s 3ms/step - loss: 97013.2500 - mae: 97013.2500\nEpoch 499/500\n6/6 [==============================] - 0s 3ms/step - loss: 97012.4453 - mae: 97012.4453\nEpoch 500/500\n6/6 [==============================] - 0s 3ms/step - loss: 97011.6484 - mae: 97011.6484\n```", "```\ndef visualize_model(history, ymin=None, ymax=None):\n    # Lets visualize our model\n    print(history.history.keys())\n    # Lets plot the loss\n    plt.plot(history.history['loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Number of epochs')\n    plt.ylim([ymin,ymax]) # To zoom in on the y-axis\n    plt.legend(['loss plot'], loc='upper right')\n    plt.show()\n```", "```\nvisualize_model(history_2)\n```", "```\n#Set random set\ntf.random.set_seed(10)\n#create a model\nmodel_3 =Sequential([\n    Dense(units=64, activation='relu',\n    input_shape=[len(X_train.columns)]),\n    Dense(units=1)\n    ])\n#compile the model\nmodel_3.compile(loss=\"mae\", optimizer=\"SGD\",\n    metrics = ['mae'])\n#Fit the model\nhistory_3 =model_3.fit(X_train, y_train, epochs=500)\n```", "```\nEpoch 496/500\n6/6 [==============================] - 0s 3ms/step - loss: 3651.6785 - mae: 3651.6785\nEpoch 497/500\n6/6 [==============================] - 0s 3ms/step - loss: 3647.4753 - mae: 3647.4753\nEpoch 498/500\n6/6 [==============================] - 0s 3ms/step - loss: 3722.4863 - mae: 3722.4863\nEpoch 499/500\n6/6 [==============================] - 0s 3ms/step - loss: 3570.9023 - mae: 3570.9023\nEpoch 500/500\n6/6 [==============================] - 0s 3ms/step - loss: 3686.0293 - mae: 3686.0293\n```", "```\nvisualize_model(history_3, ymin=0, ymax=10000)\n```", "```\n#Set random set\ntf.random.set_seed(10)\n#create a model\nmodel_4 =Sequential([\n    Dense(units=64, activation='relu',\n        input_shape=[len(X_train.columns)]),\n    Dense(units=64, activation='relu'),\n        Dense(units=1)\n    ])\n#compile the model\nmodel_4.compile(loss=\"mae\", optimizer=\"SGD\",\n    metrics = \"mae\")\n#fit the model\nhistory_4 =model_4.fit(X_train, y_train, epochs=500)\n```", "```\nEpoch 496/500\n6/6 [==============================] - 0s 3ms/step - loss: 97384.4141 - mae: 97384.4141\nEpoch 497/500\n6/6 [==============================] - 0s 3ms/step - loss: 97384.3516 - mae: 97384.3516\nEpoch 498/500\n6/6 [==============================] - 0s 3ms/step - loss: 97384.3047 - mae: 97384.3047\nEpoch 499/500\n6/6 [==============================] - 0s 3ms/step - loss: 97384.2422 - mae: 97384.2422\nEpoch 500/500\n6/6 [==============================] - 0s 3ms/step - loss: 97384.1797 - mae: 97384.1797\n```", "```\nX.describe()\n```", "```\n# create a scaler object\nscaler = MinMaxScaler()\n# fit and transform the data\nX_norm = pd.DataFrame(scaler.fit_transform(X),\n    columns=X.columns)\nX_norm.describe()\n```", "```\n# Create training and test sets with the normalized data (X_norm)\nX_train, X_test, y_train, y_test = train_test_split(X_norm,\n    y,  test_size=0.2, random_state=10)\n```", "```\n#create a model\nmodel_5 =Sequential([\n    Dense(units=64, activation='relu',\n        input_shape=[len(X_train.columns)]),\n    Dense(units=64, activation =\"relu\"),\n        Dense(units=1)\n    ])\n#compile the model\nmodel_5.compile(loss=\"mae\",\n    optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])\nhistory_5 =model_5.fit(X_train, y_train, epochs=1000)\n```", "```\nEpoch 996/1000\n6/6 [==============================] - 0s 4ms/step - loss: 1459.2953 - mae: 1459.2953\nEpoch 997/1000\n6/6 [==============================] - 0s 4ms/step - loss: 1437.8248 - mae: 1437.8248\nEpoch 998/1000\n6/6 [==============================] - 0s 3ms/step - loss: 1469.3732 - mae: 1469.3732\nEpoch 999/1000\n6/6 [==============================] - 0s 4ms/step - loss: 1433.6071 - mae: 1433.6071\nEpoch 1000/1000\n6/6 [==============================] - 0s 3ms/step - loss: 1432.2891 - mae: 1432.2891\n```", "```\n#create a model\nmodel_6 =Sequential([\n    Dense(units=64, activation='relu',\n        input_shape=[len(X_train.columns)]),\n        Dense(units=64, activation =\"relu\"), Dense(units=1)\n    ])\n#compile the model\nmodel_6.compile(loss=\"mae\",\n    optimizer=tf.keras.optimizers.SGD(), metrics = ['mae'])\n#fit the model\nearly_stop=keras.callbacks.EarlyStopping(monitor='loss',\n    patience=10)\nhistory_6 =model_6.fit(\n    X_train, y_train, epochs=1000, callbacks=[early_stop])\n```", "```\nEpoch 25/1000\n6/6 [==============================] - 0s 3ms/step - loss: 84910.6953 - mae: 84910.6953\nEpoch 26/1000\n6/6 [==============================] - 0s 3ms/step - loss: 81037.8516 - mae: 81037.8516\nEpoch 27/1000\n6/6 [==============================] - 0s 3ms/step - loss: 72761.0078 - mae: 72761.0078\nEpoch 28/1000\n6/6 [==============================] - 0s 3ms/step - loss: 81160.6562 - mae: 81160.6562\nEpoch 29/1000\n6/6 [==============================] - 0s 3ms/step - loss: 70687.3125 - mae: 70687.3125\n```", "```\n#create a model\nmodel_7 =Sequential([\n    Dense(units=64, activation='relu',\n        input_shape=[len(X_train.columns)]),\n    Dense(units=64, activation =\"relu\"),\n    Dense(units=1)\n    ])\n#compile the model\nmodel_7.compile(loss=\"mae\", optimizer=\"Adam\",\n    metrics =\"mae\")\n#fit the model\nearly_stop=keras.callbacks.EarlyStopping(monitor='loss',\n    patience=10)\nhistory_7 =model_7.fit(\n    X_train, y_train, epochs=1000, callbacks=[early_stop])\n```", "```\nEpoch 897/1000\n6/6 [==============================] - 0s 4ms/step - loss: 30.4748 - mae: 30.4748\nEpoch 898/1000\n6/6 [==============================] - 0s 4ms/step - loss: 19.4643 - mae: 19.4643\nEpoch 899/1000\n6/6 [==============================] - 0s 3ms/step - loss: 17.0965 - mae: 17.0965\nEpoch 900/1000\n6/6 [==============================] - 0s 3ms/step - loss: 18.5009 - mae: 18.5009\nEpoch 901/1000\n6/6 [==============================] - 0s 3ms/step - loss: 15.5516 - mae: 15.5516\n```", "```\n#create a model\nmodel_8 =Sequential([\n    Dense(units=64, activation='relu',\n        input_shape=[len(X_train.columns)]),\n    Dense(units=64, activation =\"relu\"),\n    Dense(units=64, activation =\"relu\"),\n    Dense(units=1)\n    ])\n#compile the model\nmodel_8.compile(loss=\"mae\", optimizer=\"Adam\",\n    metrics =\"mae\")\n#fit the model\nearly_stop=keras.callbacks.EarlyStopping(monitor='loss',\n    patience=10)\nhistory_8 =model_8.fit(\n    X_train, y_train, epochs=1000, callbacks=[early_stop])\n```", "```\nEpoch 266/1000\n6/6 [==============================] - 0s 4ms/step - loss: 73.3237 - mae: 73.3237\nEpoch 267/1000\n6/6 [==============================] - 0s 4ms/step - loss: 113.9100 - mae: 113.9100\nEpoch 268/1000\n6/6 [==============================] - 0s 4ms/step - loss: 257.4851 - mae: 257.4851\nEpoch 269/1000\n6/6 [==============================] - 0s 4ms/step - loss: 149.9819 - mae: 149.9819\nEpoch 270/1000\n6/6 [==============================] - 0s 4ms/step - loss: 179.7796 - mae: 179.7796\n```", "```\ndef eval_testing(model):\n    return model.evaluate(X_test, y_test)\nmodels = [model_1, model_2, model_3, model_4, model_5,\n    model_6, model_7, model_8]\nfor x in models:\n    eval_testing(x)\n```", "```\n2/2 [==============================] - 0s 8ms/step - loss: 100682.4609 - mae: 100682.4609\n2/2 [==============================] - 0s 8ms/step - loss: 100567.9453 - mae: 100567.9453\n2/2 [==============================] - 0s 10ms/step - loss: 17986.0801 - mae: 17986.0801\n2/2 [==============================] - 0s 9ms/step - loss: 100664.0781 - mae: 100664.0781\n2/2 [==============================] - 0s 6ms/step - loss: 1971.4187 - mae: 1971.4187\n2/2 [==============================] - 0s 11ms/step - loss: 5831.1250 - mae: 5831.1250\n2/2 [==============================] - 0s 7ms/step - loss: 5.0099 - mae: 5.0099\n2/2 [==============================] - 0s 26ms/step - loss: 70.2970 - mae: 70.2970\n```", "```\n#Let's make predictions on our test data\ny_preds=model_7.predict(X_test).flatten()\ny_preds\n```", "```\n2/2 [==============================] - 0s 9ms/step\narray([ 64498.64 , 131504.89 , 116491.73 ,  72500.13 , 102983.836,\n        60504.645,  84503.36 , 119501.664, 112497.734,  63501.168,\n        77994.87 ,  84497.16 , 112497.734,  90980.625,  87499.88 ,\n       100502.234, 135498.88 , 112491.53 , 119501.664, 131504.89 ,\n       108990.31 , 117506.63 ,  80503.16 , 123495.66 , 112497.734,\n       117506.63 , 111994.03 ,  78985.125, 135498.88 , 129502.125,\n       117506.64 , 119501.664, 100502.234, 113506.43 , 101987.38 ,\n       113506.43 ,  93990.555,  65496.2  ,  61494.906, 107506.17 ,\n       105993.77 , 106502.5  ,  72493.94 , 135498.88 ,  67501.37 ,\n       107506.17 , 117506.63 ,  70505.1  ,  57500.906], dtype=float32)\n```", "```\n#Let's make a DataFrame to compare our prediction with the ground truth\ndf_predictions = pd.DataFrame({'Ground_Truth': y_test, \n    'Model_prediction': y_preds}, columns=['Ground_Truth', \n    'Model_prediction']) df_predictions[\n    'Model_prediction']= df_predictions[\n    'Model_prediction'].astype(int)\n```", "```\n#Let's look at the top 10 data points in the test set\ndf_predictions.sample(10)\n```", "```\n#Saving the model in one line of code\nModel7.save('salarypredictor.h5')\n#Alternate method is\n#model7.save('salarypredictor')\n```", "```\n#loading the model\nsaved_model =tf.keras.models.load_model(\"/content/salarypredictor.h5\")\n```", "```\n#Putting everything into a function for our big task\ndef salary_predictor(df):\n    df_hires= df.drop(columns=['Name', 'Phone_Number',\n        'Date_Of_Birth' ])\n    df_hires = pd.get_dummies(df_hires, drop_first=True)\n    X_norm = pd.DataFrame(scaler.fit_transform(df_hires),\n        columns=df.columns)\n    y_preds=saved_model.predict(X_norm).flatten()\n    df_predictions = pd.DataFrame({ 'Model_prediction':\n        y_preds}, columns=[ 'Model_prediction'])\n    df_predictions['Model_prediction']= df_predictions[\n        'Model_prediction'].astype(int)\n    df['Salary']=df_predictions['Model_prediction']\n    return df\n```", "```\n#Load the data\ndf_new=pd.read_csv('https://raw.githubusercontent.com/oluwole-packt/datasets/main/new_hires.csv')\ndf_new\n```", "```\n#Lets see how much\nsalary_predictor(df_new)\n```"]