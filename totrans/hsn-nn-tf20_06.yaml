- en: TensorFlow 2.0 Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 3](f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml), *TensorFlow Graph
    Architecture*, we introduced the TensorFlow graph definition and execution paradigm
    that, although powerful and has high expressive power, has some disadvantages,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A steep learning curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard to debug
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counter-intuitive semantics when it comes to certain operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python is only used to build the graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to work with computational graphs can be tough—defining the computation
    instead of executing the operations as the Python interpreter encounters them
    is a different way of thinking compared to what most programs do, especially the
    ones that only work with imperative languages.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is still recommended that you have a deep understanding of DataFlow
    graphs and how TensorFlow 1.x forced its users to think since it will help you
    understand many parts of the TensorFlow 2.0 architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging a DataFlow graph is not easy—TensorBoard helps in visualizing the
    graph, but it is not a debugging tool. Visualizing the graph only ascertains whether
    the graph has been built as defined in Python, but the peculiarities such as the
    parallel execution of the non-dependant operations (remember the exercise at the
    end of the previous chapter regarding `tf.control_dependencies`?) are hard to
    find and are not explicitly shown in the graph visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Python, the de facto data science and machine learning language, is only used
    to define the graph; the other Python libraries that could help solve the problem
    can't be used during the graph's definition since it is not possible to mix graph
    definition and session execution. Mixing graph definition, execution, and the
    usage of other libraries on graph generated data is difficult and makes the design
    of the Python application really ugly since it is nearly impossible to not rely
    on global variables, collections, and objects that are common to many different
    files. Organizing the code using classes and functions is not natural when using
    this graph definition and execution paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The release of TensorFlow 2.0 introduced several changes to the framework:
    from defaulting to eager execution to a complete cleanup of the APIs. The whole
    TensorFlow package, in fact, was full of duplicated and deprecated APIs that,
    in TensorFlow 2.0, have been finally removed. Moreover, by deciding to follow
    the Keras API specification, the TensorFlow developers decided to remove several
    modules that do not follow it: the most important removal was `tf.layers` (which
    we used in [Chapter 3](f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml), *TensorFlow
    Graph Architecture*) in favor of `tf.keras.layers`.'
  prefs: []
  type: TYPE_NORMAL
- en: Another widely used module, `tf.contrib`, has been completely removed. The `tf.contrib` module
    contained the community-added layers/software that used TensorFlow. From a software
    engineering point of view, having a module that contains several completely unrelated
    and huge projects in one package is a terrible idea. For this reason, they removed
    it from the main package and decided to move maintained and huge modules into
    separate repositories, while removing unused and unmaintained modules.
  prefs: []
  type: TYPE_NORMAL
- en: By defaulting on eager execution and removing (hiding) the graph definition
    and execution paradigm, TensorFlow 2.0 allows for better software design, thereby
    lowering the steepness of the learning curve and simplifying the debug phase.
    Of course, coming from a static graph definition and execution paradigm, you need
    to have a different way of thinking—this struggle is worth it since the advantages
    the version 2.0 brings in the long term will highly repay this initial struggle.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Relearning the TensorFlow framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras framework and its models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eager execution and new features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codebase migration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relearning the framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we introduced in [Chapter 3](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=27&action=edit#post_26), *TensorFlow
    Graph Architecture*, TensorFlow works by building a computational graph first
    and then executing it. In TensorFlow 2.0, this graph definition is hidden and
    simplified; the execution and the definition can be mixed, and the flow of execution
    is always the one that's found in the source code—there's no need to worry about
    the order of execution in 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior to the 2.0 release, developers had to design the graph and the source
    by following this pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: How can I define the graph? Is my graph composed of multiple layers that are
    logically separated? If so, I have to define every logical block inside a different `tf.variable_scope`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the training or inference phase, do I have to use a part of the graph
    more than once in the same execution step? If so, I have to define this part by
    wrapping it inside a `tf.variable_scope` and ensuring that the `reuse` parameter
    is correctly used. We do this the first time to define the block; any other time,
    we reuse it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the graph definition completed? If so, I have to initialize all the global
    and local variables, thereby defining the `tf.global_variables_initializer()`
    operation and executing it as soon as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you have to create the session, load the graph, and run the `sess.run`
    calls on the node you want to execute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After TensorFlow 2.0 was released, this reasoning completely changed, becoming
    more intuitive and natural for developers who are not used to working with DataFlow
    graphs. In fact, in TensorFlow 2.0, the following changes occurred:'
  prefs: []
  type: TYPE_NORMAL
- en: There are no more global variables. In 1.x, the graph is global; it doesn't
    matter if a variable has been defined inside a Python function—it is visible and
    separate from every other part of the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No more `tf.variable_scope`. A context manager can't change the behavior of
    a function by setting a `boolean` flag (`reuse`). In TensorFlow 2.0, variable
    sharing is made by **the model itself**. Every model is a Python object, every
    object has its own set of variables, and to share the variables, you just have
    to use the **same model** with different input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No more `tf.get_variable`. As we saw in [Chapter 3](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=27&action=edit#post_26), *TensorFlow
    Graph Architecture*, `tf.get_variable` allows you to declare variables that can
    be shared by using `tf.variable_scope`. Since every variable now matches 1:1 with
    a Python variable, the possibility of declaring global variables has been removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No more `tf.layers`. Every layer that's declared inside the `tf.layers` module
    uses `tf.get_variable` to define its own variables. Use `tf.keras.layers` instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No more global collections. Every variable was added to a collection of global
    variables that were accessible via `tf.trainable_variables()`—this was contradictory
    to every good software design principle. Now, the only way to access the variables
    of an object is by accessing its `trainable_variables` attribute, which returns
    the list of the trainable variables of that specific object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's no need to manually call an operation that initializes all the variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API cleanup and the removal of `tf.contrib` is now used in favor of the creation
    of several small and well-organized projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these changes have been made to simplify how TensorFlow is used, to organize
    the codebase better, to increase the expressive power of the framework, and to
    standardize its structure.
  prefs: []
  type: TYPE_NORMAL
- en: Eager execution, together with the adherence of TensorFlow to the Keras API,
    are the most important changes that came with TensorFlow's 2.0 release.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras framework and its models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to what people who already familiar with Keras usually think, Keras
    is not a high-level wrapper around a machine learning framework (TensorFlow, CNTK,
    or Theano); instead, it is an API specification that's used for defining and training
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow implements the specification in its `tf.keras` module. In particular,
    TensorFlow 2.0 itself is an implementation of the specification and as such, many
    first-level submodules are nothing but aliases of the `tf.keras` submodules; for
    example, `tf.metrics = tf.keras.metrics` and `tf.optimizers = tf.keras.optimizers`.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow 2.0 has, by far, the most complete implementation of the specification,
    making it the framework of choice for the vast majority of machine learning researchers. Any
    Keras API implementation allows you to build and train deep learning models. It
    is used for prototyping quick solutions that follow the natural human way of thinking
    due to its layer organization, as well as for advanced research due to its modularity
    and extendibility and for its ease of being deployed to production. The main advantages
    of the Keras implementation that are available in TensorFlow are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ease of use**: The Keras interface is standardized. Every model definition
    must follow a common interface; every model is composed of layers, and each of
    them must implement a well-defined interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Being standardized in every part—from the model definition to the training
    loop—makes learning to use a framework that implements the specification easy
    and extremely useful: any other framework that implements the Keras specification
    looks similar. This is a great advantage since it allows researchers to read code
    written in other frameworks without the struggle of learning about the details
    of the framework that was used.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Modular and extendible**: The Keras specification describes a set of building
    blocks that can be used to compose any kind of machine learning model. The TensorFlow
    implementation allows you to write custom building blocks, such as new layers,
    loss functions, and optimizers, and compose them to develop new ideas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Built-in**: Since TensorFlow 2.0''s release, there has been no need to download
    a separate Python package in order to use Keras. The `tf.keras` module is already
    built into the `tensorflow` package, and it has some TensorFlow-specific enhancements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eager execution is a first-class citizen, just like the high-performance input
    pipeline module known as `tf.data`. Exporting a model that's been created using
    Keras is even easier than exporting a model defined in plain TensorFlow. Being
    exported in a language-agnostic format means that its compatibility with any production
    environment has already been configured, and so it is guaranteed to work with
    TensorFlow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Keras, together with eager execution, are the perfect tools to prototype new
    ideas faster and design maintainable and well-organized software. In fact, you
    no longer need to think about graphs, global collections, and how to define the
    models in order to share their parameters across different runs; what's really
    important in TensorFlow 2.0 is to think in terms of Python objects, all of which
    carry their own variables.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0 lets you design the whole machine learning pipeline while just
    thinking about objects and classes, and not about graphs and session execution.
  prefs: []
  type: TYPE_NORMAL
- en: Keras was already present in TensorFlow 1.x, but without eager execution enabled
    by default, which allowed you to define, train, and evaluate models through *assembling
    layers.*In the next few sections, we will demonstrate three ways to build a model
    and train it using a standard training loop
  prefs: []
  type: TYPE_NORMAL
- en: In the *Eager execution and new features* section, you will be shown how to
    create a custom training loop. The rule of thumb is to use Keras to build the
    models and use a standard training loop if the task to solve is quite standard,
    and then write a custom training loop when Keras does not provide a simple and
    ready-to-use training loop.
  prefs: []
  type: TYPE_NORMAL
- en: The Sequential API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common type of model is a stack of layers. The `tf.keras.Sequential`
    model allows you to define a Keras model by stacking `tf.keras.layers`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNN that we defined in [Chapter 3](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=27&action=edit#post_26), *TensorFlow
    Graph Architecture*, can be recreated using a Keras sequential model in fewer
    lines and in an elegant way. Since we are training a classifier, we can use the
    Keras model''s `compile` and `fit` methods to build the training loop and execute
    it, respectively. At the end of the training loop, we can also evaluate the performance
    of the model on the test set using the `evaluate` method—Keras will take care
    of all the boilerplate code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Some things to take note of regarding the preceding code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras.Sequential` builds a `tf.keras.Model` object by stacking Keras layers.
    Every layer expects input and produces an output, except for the first one. The
    first layer uses the additional `input_shape` parameter, which is required to
    correctly build the model and print the summary before feeding in a real input.
    Keras allows you to specify the input shape of the first layer or leave it undefined.
    In the case of the former, every following layer knows its input shape and forward
    propagates its output shape to the next layer, making the input and output shape
    of every layer in the model known once the `tf.keras.Model` object has been created.
    In the case of the latter, the shapes are undefined and will be computed once
    the input has been fed to the model, making it impossible to generate the *summary*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model.summary()` prints a complete description of the model, which is really
    useful if you want to check whether the model has been correctly defined and thereby
    check whether there are possible typos in the model definition, which layer weighs
    the most (in terms of the number of parameters), and how many parameters the whole
    model has. The CNN summary is presented in the following code. As we can see,
    the vast majority of parameters are in the fully connected layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The dataset preprocessing step was made without the use of NumPy but, instead,
    using **eager execution**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.expand_dims(data, -1).numpy()` show how TensorFlow can be used as a replacement
    for NumPy (having a 1:1 API compatibility). By using `tf.expand_dims` instead
    of `np.expand_dims`, we obtained the same result (adding one dimension at the
    end of the input tensor), but created a `tf.Tensor` object instead of a `np.array`
    object. The `compile` method, however, requiresNumPy arrays as input, and so we
    need to use the `numpy()` method. Every `tf.Tensor` object has to get the corresponding NumPy
    value contained in the Tensor object.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the case of a standard classification task, Keras allows you to build the
    training loop in a single line using the `compile` method. To configure a training
    loop, the method only requires three arguments: the optimizer, the loss, and the
    metrics to monitor. In the preceding example, we can see that it is possible to
    use both strings and Keras objects as parameters to correctly build the training
    loop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model.fit` is the method that you call after the training loop has been built
    in order to effectively start the training phase on the data that''s passed for
    the desired number of epochs while measuring the metrics specified in the compile
    phase. The batch size can be configured by passing the `batch_size` parameter.
    In this case, we are using the default value of 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the training loop, the model's performance can be measured on
    some unseen data. In this case, it's testing the test set of the fashion-MNIST
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keras takes care of giving the user feedback while the model is being training,
    logging a progress bar for each epoch and the live value of loss and metrics in
    the standard output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The last line in the preceding code is the result of the `evaluate` call.
  prefs: []
  type: TYPE_NORMAL
- en: The Functional API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Sequential API is the simplest and most common way of defining models. However,
    it cannot be used to define arbitrary models. The Functional API allows you to
    define complex topologies without the constraints of the sequential layers.
  prefs: []
  type: TYPE_NORMAL
- en: The Functional API allows you to define multi-input, multi-output models, easily
    sharing layers, defines residual connections, and in general define models with
    arbitrary complex topologies.
  prefs: []
  type: TYPE_NORMAL
- en: Once built, a Keras layer is a callable object that accepts an input tensor
    and produces an output tensor. It knows that it is possible to compose the layers
    by treating them as functions and building a `tf.keras.Model` object just by passing
    the input and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how we can define a Keras model using the functional
    interface: the model is a fully connected neural network that accepts a 100-dimensional
    input and produces a single number as output (as we will see in [Chapter 9](66948c53-131c-43ef-a7fc-3d242d1e0664.xhtml), *Generative
    Adversarial Networks*, this will be our Generator architecture):'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Being a Keras model, `model` can be compiled and trained exactly like any other
    Keras model that's defined using the Sequential API.
  prefs: []
  type: TYPE_NORMAL
- en: The subclassing method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Sequential and Functional APIs cover almost any possible scenario. However,
    Keras offers another way of defining models that is object-oriented, more flexible,
    but error-prone and harder to debug. In practice, it is possible to subclass any `tf.keras.Model` by
    defining the layers in `__init__` and the forward passing in the `call` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The subclassing method is not recommended since it separates the layer definition
    from its usage, making it easy to make mistakes while refactoring the code. However,
    defining the forward pass using this kind of model definition is sometimes the
    only way to proceed, especially when working with recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Subclassing from a `tf.keras.Model` the `Generator` object is a `tf.keras.Model` itself,
    and as such, it can be trained using the `compile` and `fit` commands, as shown
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Keras can be used to train and evaluate models, but TensorFlow 2.0, with its
    eager execution, allows us to write our own custom training loop so that we have
    complete control of the training process and can debug easily.
  prefs: []
  type: TYPE_NORMAL
- en: Eager execution and new features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is stated in the eager execution official documentation ([https://www.tensorflow.org/guide/eager](https://www.tensorflow.org/guide/eager)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*TensorFlow''s eager execution is an imperative programming environment that
    evaluates operations immediately, without building graphs: operations return concrete
    values instead of constructing a computational graph to run later. This makes
    it easy to get started with TensorFlow and debug models, and it reduces boilerplate
    as well. To follow along with this guide, run the following code samples in an
    interactive Python interpreter.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Eager execution is a flexible machine learning platform for research and experimentation,
    providing the following:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*An intuitive interface: Structure your code naturally and use Python data
    structures. Quickly iterate on small models and small data.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Easier debugging: Call ops directly to inspect running models and test changes.
    Use standard Python debugging tools for immediate error reporting.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Natural control flow: Use Python control flow instead of graph control flow,
    simplifying the specification of dynamic models.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As shown in *The Sequential API* section, eager execution allows you to (among
    other features) use TensorFlow as a standard Python library that is executed immediately
    by the Python interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: As we explained in [Chapter 3](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=27&action=edit#post_26), *TensorFlow
    Graph Architecture*, the graph definition and session execution paradigm is no
    longer the default. Don't worry! Everything you learned in the previous chapter
    is of extreme importance if you wish to master TensorFlow 2.0, and it will help
    you understand why certain parts of the framework work in this way, especially
    when you're using AutoGraph and the Estimator API, which we will talk about next.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how the baseline example from the previous chapter works when eager
    execution is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s recall the baseline example from the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The session''s execution produces the NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting the baseline example into TensorFlow 2.0 is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry about the graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't worry about the session execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Just write what you want to be executed whenever you want it to be executed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces a different output with respect to the 1.x version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The numerical value is, of course, the same, but the object that's returned
    is no longer a NumPy array—instead, it's a `tf.Tensor` object.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow 1.x, a `tf.Tensor` object was only a symbolic representation of
    the output of a `tf.Operation`; in 2.0, this is no longer the case.
  prefs: []
  type: TYPE_NORMAL
- en: Since the operations are executed as soon as the Python interpreter evaluates
    them, every `tf.Tensor` object is not only a symbolic representation of the output
    of a `tf.Operation`, but also a concrete Python object that contains the result
    of the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that a `tf.Tensor` object is still a symbolic representation of
    the output of a `tf.Operation`. This allows it to support and use 1.x features
    in order to manipulate `tf.Tensor` objects, thereby building graphs of `tf.Operation` that
    produce `tf.Tensor`.
  prefs: []
  type: TYPE_NORMAL
- en: The graph is still present and the `tf.Tensor` objects are returned as a result
    of every TensorFlow method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `y` Python variable, being a `tf.Tensor` object, can be used as input for
    any other TensorFlow operation. If, instead, we are interested in extracting the
    value `tf.Tensor` holds so that we have the identical result of the `sess.run`
    call of the 1.x version, we can just invoke the `tf.Tensor.numpy` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow 2.0, with its focus on eager execution, allows the user to design
    better-engineered software. In its 1.x version, TensorFlow had the omnipresent
    concepts of global variables, collections, and sessions.
  prefs: []
  type: TYPE_NORMAL
- en: Variables and collections could be accessed from everywhere in the source code
    since a default graph was always present.
  prefs: []
  type: TYPE_NORMAL
- en: The session is required in order to organize the complete project structure
    since it knows that only a single session can be present. Every time a node had
    to be evaluated, the session object had to be instantiated and being accessibe
    in the current scope.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0 changed all of these aspects, increasing the overall quality
    of code that can be written using it. In practice, before 2.0, using TensorFlow
    to design a complex software system was tough, and many users just gave up and
    defined huge single file projects that had everything inside them. Now, it is
    possible to design software in a way better and cleaner way by following all the
    software engineering good practices.
  prefs: []
  type: TYPE_NORMAL
- en: Functions, not sessions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tf.Session` object has been removed from the TensorFlow API. By focusing
    on eager execution, you no longer need the concept of a session because the execution
    of the operation is immediate—we don't build a computational graph before running
    the computation.
  prefs: []
  type: TYPE_NORMAL
- en: This opens up a new scenario, in which the source code can be organized better.
    In TensorFlow 1.x, it was tough to design software by following object-oriented
    programming principles or even create modular code that used Python functions.
    However, in TensorFlow 2.0, this is natural and is highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the previous example, the baseline example can be easily converted
    into its eager execution counterpart. This source code can be improved by following
    some Python best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The two operations that could be executed singularly by calling `sess.run` (the
    matrix multiplication and the sum) have been moved to independent functions. Of
    course, the baseline example is simple, but just think about the training step
    of a machine learning model—it is easy to define a function that accepts the model
    and the input, and then executes a training step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through some of the advantages of this:'
  prefs: []
  type: TYPE_NORMAL
- en: Better software organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almost complete control over the execution flow of the program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to carry a `tf.Session` object around the source code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to use `tf.placeholder`. To feed the graph, you only need to pass the
    data to the function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can document the code! In 1.x, in order to understand what was happening
    in a certain part of the program, we had to read the complete source code, understand
    its organization, understand which operations were executed when a node was evaluated
    in a `tf.Session`, and only then did we have an idea of what was going on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using functions we can write self-contained and well-documented code that does
    exactly what the documentation states.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second and most important advantage that eager execution brings is that
    global graphs are no longer needed and, by extension, neither are its global collections
    and variables.
  prefs: []
  type: TYPE_NORMAL
- en: No more globals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Global variables are a bad software engineering practice—everyone agrees on
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow 1.x, there is a strong separation between the concept of Python
    variables and graph variables. A Python variable is a variable with a certain
    name and type that follows the Python language rules: it can be deleted using `del`
    and it is visible only in its scope and scopes that are at a lower level in the
    hierarchy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph variable, on the other hand, is a graph that''s declared in the computational
    graph and lives outside the Python language rules. We can declare a Graph variable
    by assigning it to a Python variable, but this bond is not tight: the Python variable
    gets destroyed as soon as it goes out of scope, while the graph variable is still
    present: it is a global and persistent object.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand the great advantages this change brings, we will take
    a look at what happens to the baseline operation definitions when the Python variables
    are garbage-collected:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The program fails on the second assertion and the output of `count_op` is the
    same in the invocation of `[A, x, b]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deleting Python variables is completely useless since all the operations defined
    in the graph are still there and we can access their output tensor, thus restoring
    the Python variables if needed or creating new Python variables that point to
    the graph nodes. We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Why is this behavior bad? Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The operations, once defined in the graph, are always there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any operation that's defined in the graph has a side effect (see the following
    example regarding variable initialization), deleting the corresponding Python
    variable is useless and the side effects will remain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, even if we declared the `A,x,b` variables inside a separate function
    that has its own Python scope, we can access them from every function by getting
    the tensor by name, which breaks every sort of encapsulation process out there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following example shows some of the side effects of not having global graph
    variables connected to Python variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fails to run and highlights several downsides of the global variables
    approach, alongside the downsides of the naming system used by Tensorfow 1.x:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sess.run(y)` triggers the execution of an operation that depends on the `z:0` tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When fetching a tensor using its name, we don't know whether the operation that
    generates it is an operation without side effects or not. In our case, the operation
    is a `tf.Variable` definition, which requires the variable's initialization to
    be executed before the `z:0` tensor can be evaluated; that's why the code fails
    to run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Python variable name means nothing to TensorFlow 1.x: `test` contains a
    graph variable named `z` first, and then `test` is destroyed and replaced with
    the graph constant we require, that is, `z`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, the call to `get_y` found a tensor named `z:0`, which refers
    to the `tf.Variable` operation (that has side effects) and not the constant node, `z`.
    Why? Even though we deleted the `test` variable in the graph variable, `z` is
    still defined. Therefore, when calling `tf.constant`, we have a name that conflicts
    with the graph that TensorFlow solves for us. It does this by adding the `_1` suffix
    to the output tensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these problems are gone in TensorFlow 2.0—we just have to write Python
    code that we are used to. There's no need to worry about graphs, global scopes,
    naming conflicts, placeholders, graph dependencies, and side effects. Even the
    control flow is Python-like, as we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Control flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Executing sequential operations in TensorFlow 1.x was not an easy task if the
    operations had no explicit order of execution constraints. Let''s say we want
    to use TensorFlow to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Declare and initialize two variables: `y` and `y`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increment the value of `y` by 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute `x*y`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this five times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first, non-working attempt, in TensorFlow 1.x is to just declare the code by
    following the preceding steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Those of you who completed the exercises that were provided in the previous
    chapter will have already noticed the problem in this code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output node, `out`, has no explicit dependency on the `assign_op` node,
    and so it never evaluates when `out` is executed, making the output just a sequence
    of 2\. In TensorFlow 1.x, we have to explicitly force the order of execution using `tf.control_dependencies`,
    conditioning the assignment operation so that it''s executed before the evaluation
    of `out`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, the output is the sequence of 3, 4, 5, 6, 7, which is what we wanted.
  prefs: []
  type: TYPE_NORMAL
- en: More complex examples, such as declaring and executing loops directly inside
    the graph where conditional execution (using `tf.cond`) could occur, are possible,
    but the point is the same—in TensorFlow 1.x, we have to worry about the side effects
    of our operations, we have to think about the graph's structure when writing Python
    code, and we can't even use the Python interpreter that we're used to. The conditions
    have to be expressed using `tf.cond` instead of a Python `if` statement and the
    loops have to be defined using `tf.while_loop` instead of using the Python `for`
    and `while` statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow 2.x, with its eager execution, makes it possible to use the Python
    interpreter to control the flow of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The previous example, which was developed using eager execution, is simpler
    to develop, debug, and understand—it's just standard Python, after all!
  prefs: []
  type: TYPE_NORMAL
- en: By simplifying the control flow, eager execution was possible, and is one of
    the main features that was introduced in TensorFlow 2.0—now, even users without
    any previous experience of DataFlow graphs or descriptive programming languages
    can start writing TensorFlow code. Eager execution reduces the overall framework's
    complexity and lowers the entry barrier.
  prefs: []
  type: TYPE_NORMAL
- en: Users coming from TensorFlow 1.x may start wondering how can we train machine
    learning models since, in order to compute gradients using automatic differentiation,
    we need to have a graph of the executed operations.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0 introduced the concept of GradienTape to efficiently combat this
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: GradientTape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tf.GradientTape()` invocation creates a context that records the operations
    for automatic differentiation. Every operation that's executed within the context
    manager is recorded on tape if at least one of their inputs is watchable and is
    being watched.
  prefs: []
  type: TYPE_NORMAL
- en: 'An input is watchable when the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: It's a trainable variable that's been created by using `tf.Variable`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's being explicitly watched by the tape, which is done by calling the `watch`
    method on the `tf.Tensor` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tape records every operation that''s executed within the context in order
    to build a graph of the forward pass that was executed; then, the tape can be
    unrolled in order to compute the gradients using reverse-mode automatic differentiation.
    It does this by calling the `gradient` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we explicitly asked `tape` to watch a constant value
    that, by its nature, is not watchable (since it is not a `tf.Variable` object).
  prefs: []
  type: TYPE_NORMAL
- en: 'A `tf.GradientTape` object such as `tape` releases the resources that it''s
    holding as soon as the `tf.GradientTape.gradient()` method is called. This is
    desirable for the most common scenarios, but there are cases in which we need
    to invoke `tf.GradientTape.gradient()` more than once. To do that, we need to
    create a persistent gradient tape that allows multiple calls to the gradient method
    without it releasing the resources. In this case, it is up to the developer to
    take care of releasing the resources when no more are needed. They do this by
    dropping the reference to the tape using Python''s `del` instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It is also possible to nest more than one `tf.GradientTape` object in higher-order
    derivatives (this should be easy for you to do now, so I'm leaving this as an
    exercise).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0 offers a new and easy way to build models using Keras and a highly
    customizable and efficient way to compute gradients using the concept of tape.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras models that we mentioned in the previous sections already come with
    methods to train and evaluate them; however, Keras can't cover every possible
    training and evaluation scenario. Therefore, TensorFlow 1.x can be used to build
    custom training loops so that you can train and evaluate the models and have complete
    control over what's going on. This gives you the freedom to experiment with controlling
    every part of the training. For instance, as shown in [Chapter 9](66948c53-131c-43ef-a7fc-3d242d1e0664.xhtml), *Generative
    Adversarial Networks*, the best way to define the adversarial training process
    is by defining a custom training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Custom training loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tf.keras.Model` object, through its `compile` and `fit` methods, allows
    you to train a great number of machine learning models, from classifiers to generative
    models. The Keras way of training can speed up the definition of the training
    phase of the most common models, but the customization of the training loop remains
    limited.
  prefs: []
  type: TYPE_NORMAL
- en: There are models, training strategies, and problems that require a different
    kind of model training. For instance, let's say we need to face the gradient explosion
    problem. It could happen that, during the training of a model using gradient descent,
    the loss function starts diverging until it becomes `NaN` because of the size
    of the gradient update, which becomes higher and higher until it overflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common strategy that you can use to face this problem is clipping the gradient
    or capping the threshold: the gradient update can''t have a magnitude greater
    than the threshold value. This prevents the network from diverging and usually
    helps us find a better local minima during the minimization process. There are
    several gradient clipping strategies, but the most common is L2 norm gradient
    clipping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this strategy, the gradient vector is normalized in order to make the L2
    norm less than or equal to a threshold value. In practice, we want to update the
    gradient update rule in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow has an API for this task: `tf.clip_by_norm`. We only need to access
    the gradients that have been computed, apply the update rule, and feed it to the
    chosen optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: In order to create a custom training loop using `tf.GradientTape` to compute
    the gradients and post-process them, the image classifier training script that
    we developed at the end of the previous chapter needs to be migrated to its TensorFlow
    2.0 version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please take the time to read the source code carefully: have a look at the
    new modular organization and compare the previous 1.x code with this new code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several differences between these APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: The optimizers are now Keras optimizers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The losses are now Keras losses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy is easily computed using the Keras metrics package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is always a TensorFlow 2.0 version of any TensorFlow 1.x symbol.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no more global collections. The tape needs a list of the variables
    it needs to use to compute the gradient and the `tf.keras.Model` object has to
    carry its own set of `trainable_variables`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While in version 1.x there was method invocation, in 2.0, there is a Keras method
    that returns a callable object. The constructor of almost every Keras object is
    used to configure it, and they use the `call` method to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the `tensorflow` library and then define the `make_model` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the `load_data` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward, we define the `train()` functions that instantiate the model, the
    input data, and the training parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To conclude, we need to define the `train_step` function inside the `train`
    function and use it inside the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The previous example does not include model saving, model selection, and TensorBoard
    logging. Moreover, the gradient clipping part has been left as an exercise for
    you (see the `TODO` section of the preceding code).
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, all of the missing functionalities will be included;
    in the meantime, take your time to read through the new version carefully and
    compare it with the 1.x version.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will focus on how to save the model parameters, restart the
    training process, and make model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and restoring the model's status
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow 2.0 introduced the concept of a checkpointable object: every object
    that inherits from `tf.train.Checkpointable` is automatically serializable, which
    means that it is possible to save it in a checkpoint. Compared to the 1.x version,
    where only the variables were checkpointable, in 2.0, whole Keras layers/models
    inherit from `tf.train.Checkpointable`. Due to this, it is possible to save whole
    layers/models instead of worrying about their variables; as usual, Keras introduced
    an additional abstraction layer that simplifies the usage of the framework. There
    are two ways of saving a model:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a checkpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a SavedModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we explained in [Chapter 3](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=27&action=edit#post_26), *TensorFlow
    Graph Architecture*, checkpoints do not contain any description of the model itself:
    they are just an easy way to store the model parameters and let the developer
    restore them correctly by defining the model that maps the checkpoint saved variables
    with Python `tf.Variable` objects or, at a higher level, with `tf.train.Checkpointable`
    objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The SavedModel format, on the other hand, is the serialized description of
    the computation, in addition to the parameter''s value. We can summarize these
    two objects as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Checkpoint**: An easy way to store variables on disk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SavedModel**: Model structure and checkpoint'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SavedModels are language-agnostic representations (Protobuf serialized graphs)
    that are suitable for deployment in other languages. The last chapter of this
    book, [Chapter 10](889170ef-f89d-4485-a111-6cd4e72f0daa.xhtml), *Bringing a Model
    to Production,* is dedicated to the SavedModel since it is the correct way to
    bring a model to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'While training a model, we have the model definition available in Python. Due
    to this, we are interested in saving the model status, which we can do as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Restart the training process in the case of failures, without wasting all the
    previous computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the model parameters at the end of the training loop so that we can test
    the trained model on the test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the model parameters in different locations so that we can save the status
    of the models that reached the best validation performance (model selection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To save and restore the model parameters in TensorFlow 2.0, we can use two
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.train.Checkpoint` is the object-based serializer/deserializer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.train.CheckpointManager` is an object that can use a `tf.train.Checkpoint`
    instance to save and manage checkpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to TensorFlow 1.x's `tf.train.Saver` method, the `Checkpoint.save`
    and `Checkpoint.restore` methods write and read object-based checkpoints; the
    former was only able to write and read `variable.name`-based checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Saving objects instead of variables is more robust when it comes to making changes
    in the Python program and it works correctly with the eager execution paradigm.
    In TensorFlow 1.x, saving only the `variable.name` was enough since the graph
    wouldn't change once defined and executed. In 2.0, where the graph is hidden and
    the control flow can make the objects and their variables appear/disappear, saving
    objects is the only way to preserve their status.
  prefs: []
  type: TYPE_NORMAL
- en: Using `tf.train.Checkpoint` is amazingly easy—do you want to store a checkpointable
    object? Just pass it to its constructor or create a new attribute for the object
    during its lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've defined the checkpoint object, use it to build a `tf.train.CheckpointManager`
    object, where you can specify where to save the model parameters and how many
    checkpoints to keep.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of this, the save and restore capabilities of the previous model''s
    training are as easy as adding the following lines, right after the model and
    optimizer definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Trainable and not-trainable variables are automatically added for the checkpoint
    variables to monitor, allowing you to restore the model and restart the training
    loop without introducing unwanted fluctuations in the loss functions. In fact,
    the optimizer object, which usually carries its own set of non-trainable variables
    (moving means and variances), is a checkpointable object that is added to the
    checkpoint, allowing you to restart the training loop in the same exact status
    as when it was interrupted.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a condition is met (`i % 10 == 0`, or when the validation metric is improved),
    is it possible to use the `manager.save` method invocation to checkpoint the model''s
    status:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The manager can save the model parameters in the directory that's specified
    during its construction; therefore, to perform model selection, you need to create
    a second manager object that is invoked when the model selection condition is
    met. This is left as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: Summaries and metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorBoard is still the default and recommended data logging and visualization
    tool for TensorFlow. The `tf.summary` package contains all the required methods
    to save scalar values, images, plot histograms, distributions, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Together with the `tf.metrics` package, it is possible to log aggregated data.
    Metrics are usually measured on mini-batches and not on the whole training/validation/test
    set: aggregating data while looping on the complete dataset split allows us to
    measure the metrics correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: The objects in the `tf.metrics` package are stateful, which means they are able
    to accumulate/aggregate values and return a cumulative result when calling `.result()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way as TensorFlow 1.x, to save a summary to disk, you need a File/Summary
    writer object. You can create one by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This new object doesn't work like it does in 1.x—its usage is now simplified
    and more powerful. Instead of using a session and executing the `sess.run(summary)` line
    to get the line to write inside the summary, the new `tf.summary.*` objects are
    able to detect the context they are used within and log the correct summary inside
    the writer once the summary line has been computed.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the summary writer object defines a context manager by calling `.as_default()`;
    every `tf.summary.*` method that's invoked within this context will add its result
    to the default summary writer.
  prefs: []
  type: TYPE_NORMAL
- en: Combining `tf.summary` with `tf.metrics` allows us to measure and log the training/validation/test
    metrics correctly and in an easier way with respect to TensorFlow 1.x. In fact,
    if we decide to log every 10 training steps for the computed metric, we have to
    visualize the mean value that's computed over those 10 training steps and not
    just the last one.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, at the end of every training step, we have to invoke the metric object's `.update_state`
    method to aggregate and save the computed value inside the object status and then
    invoke the `.result()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.result()` method takes care of correctly computing the metric over the
    aggregated values. Once computed, we can reset the internal states of the metric
    by calling `reset_states()`. Of course, the same reasoning holds for every value
    that''s computed during the training phase because the loss is quite common:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This defines the metric's `Mean`, which is the mean of the input that's passed
    during the training phase. In this case, this is the loss value, but the same
    metric can be used to compute the mean of every scalar value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tf.summary` package also contains methods that you can use to log images
    (`tf.summary.image`), therefore extending the previous example to log both scalar
    metrics and batches of images on TensorBoard in an extremely easy. The following
    code shows how the previous example can be extended to log the training loss,
    accuracy, and three training images—please take the time to analyze the structure,
    see how metrics and logging are performed, and try to understand how the code
    structure can be improved by defining more functions in order to make it more
    modular and easy to maintain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define the `train_step` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'On TensorBoard, at the end of the first epoch, is it possible to see the loss
    value measured every 10 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47307d71-c49a-4ca7-828a-fab262d72662.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss value, measured every 10 steps, as visualized in TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the training accuracy, measured at the same time as the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/320f3747-b934-4d9e-9973-555f29e29176.png)'
  prefs: []
  type: TYPE_IMG
- en: The training accuracy, as visualized in TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we can also see the images sampled for the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12bba95e-bd24-4be9-a049-c93f8fa2a787.png)'
  prefs: []
  type: TYPE_IMG
- en: Three image samples from the training set—a dress, a sandal, and a pullover
    from the fashion-MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: Eager execution allows you to create and execute models on the fly, without
    explicitly creating a graph. However, working in eager mode does not mean that
    a graph can't be built from TensorFlow code. In fact, as we saw in the previous
    section, by using `tf.GradientTape`, is it possible to register what happens during
    a training step, build a computational graph by tracing the operations that are
    executed, and use this graph to automatically compute the gradient using automatic
    differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing what happens during a function's execution allows us to analyze what
    operations are executed at runtime. Knowing the operations, their input relations,
    and their output relation makes it possible to build graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is of extreme importance since it can be exploited to execute a function
    once, trace its behavior, convert its body into its graph representation, and
    fall back to the more efficient graph definition and session execution, which
    has a huge performance boost. It does all of this automatically: this is the concept
    of AutoGraph.'
  prefs: []
  type: TYPE_NORMAL
- en: AutoGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatically converting Python code into its graphical representation is done
    with the use of **AutoGraph**. In TensorFlow 2.0, AutoGraph is automatically applied
    to a function when it is decorated with `@tf.function`. This decorator creates
    callable graphs from Python functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A function, once decorated correctly, is processed by `tf.function` and the `tf.autograph`
    module in order to convert it into its graphical representation. The following
    diagram shows a schematic representation of what happens when a decorated function
    is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba5dc45f-0079-4085-b76d-2f6fb034fae8.png)'
  prefs: []
  type: TYPE_IMG
- en: Schematic representation of what happens when a function, f, decorated with
    @tf.function, which is called on the first call and on any other subsequent call
  prefs: []
  type: TYPE_NORMAL
- en: 'On the first call of the annotated function, the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: The function is executed and traced. Eager execution is disabled in this context,
    and so every `tf.*` method defines a `tf.Operation` node that produces a `tf.Tensor`
    output, exactly like it does in TensorFlow 1.x.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `tf.autograph` module is used to detect Python constructs that can be converted
    into their graph equivalent. The graph representation is built from the function
    trace and AutoGraph information. This is done in order to preserve the execution
    order that's defined in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `tf.Graph` object has now been built.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the function name and the input parameters, a unique ID is created
    and associated with the graph. The graph is then cached into a map so that it
    can be reused when a second invocation occurs and the ID matches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting a function into its graph representation usually requires us to think;
    in TensorFlow 1.x, not every function that works in eager mode can be converted
    painlessly into its graph version.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a variable in eager mode is a Python object that follows the Python
    rules regarding its scope. In graph mode, as we found out in the previous chapter,
    a variable is a persistent object that will continue to exist, even if its associated
    Python variable goes out of scope and is garbage-collected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, special attention has to be placed on software design: if a function
    has to be graph-accelerated and it creates a status (using `tf.Variable` and similar
    objects), it is up to the developer to take care of avoiding having to recreate
    the variable every time the function is called.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, `tf.function` parses the function body multiple times while
    looking for the `tf.Variable` definition. If, at the second invocation, it finds
    out that a variable object is being recreated, it raises an exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In practice, if we have defined a function that performs a simple operation
    that uses a `tf.Variable` inside it, we have to ensure that the object is only
    created once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function works correctly in eager mode, but it fails to execute
    if it is decorated with `@tf.function` and is raising the preceding exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Handling functions that create a state means that we have to rethink our usage
    of graph-mode. A state is a persistent object, such as a variable, and the variable
    can''t be redeclared more than once. Due to this, the function definition can
    be changed in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By passing the variable as an input parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By breaking the function scope and inheriting a variable from the external scope
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first option requires changing the function definition that''s making it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`f` now accepts a Python input variable, `b`. This variable can be a `tf.Variable`,
    a `tf.Tensor`, and also a NumPy object or a Python type. Every time the input
    type changes, a new graph is created in order to make an accelerated version of
    the function that works for any required input type (this is required because
    of how a TensorFlow graph is statically typed).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option, on the other hand, requires breaking down the function scope,
    making the variable available outside the scope of the function itself. In this
    case, there are two paths we can follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not recommended**: Use global variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommended**: Use Keras-like objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first path, which is **not recommended**, consists of declaring the variable
    outside the function body and using it inside, ensuring that it will only be declared
    once:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The second path, which is **recommended**, is to use an object-oriented approach
    and declare the variable as a private attribute of a class. Then, you need to
    make the objects that were instantiated callable by putting the function body
    inside the `__call__` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: AutoGraph and the graph acceleration process work best when it comes to optimizing
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, the most computationally-intensive part of the training is the forward
    pass, followed by gradient computation and parameter updates. In the previous
    example, following the new structure that the absence of `tf.Session` allows us
    to follow, we separate the training step from the training loop. The training
    step is a function without a state that uses variables inherited from the outer
    scope. Therefore, it can be converted into its graph representation and accelerated
    just by decorating it with the `@tf.function` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You are invited to measure the speedup that was introduced by the graph conversion
    of the `train_step` function.
  prefs: []
  type: TYPE_NORMAL
- en: The speedup is not guaranteed since eager execution is already fast and there
    are simple scenarios in which eager execution is as fast as its graphical counterpart.
    However, the performance boost is visible when the models become more complex
    and deeper.
  prefs: []
  type: TYPE_NORMAL
- en: AutoGraph automatically converts Python constructs into their `tf.*` equivalent,
    but since converting source code that preserves semantics is not an easy task,
    there are scenarios in which it is better to help AutoGraph perform source code
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, there are constructs that work in eager execution that are already
    drop-in replacements for Python constructs. In particular, `tf.range` replaces `range`, `tf.print`
    replaces `print`, and `tf.assert` replaces `assert`.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, AutoGraph is not able to automatically convert `print` into `tf.print` in
    order to preserve its semantic. Therefore, if we want a graph-accelerated function
    to print something when executed in graph mode, we have to write the function
    using `tf.print` instead of `print`.
  prefs: []
  type: TYPE_NORMAL
- en: You are invited to define simple functions that use `tf.range` instead of  `range` and `print`
    instead of `tf.print`, and then visualize how the source code is converted using
    the `tf.autograph` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This produces `0,1,2, ..., 10` when `f` is called—does this happens every time
    `f` is invoked, or only the first time?
  prefs: []
  type: TYPE_NORMAL
- en: 'You are invited to carefully read through the following AutoGraph-generated
    function (this is machine-generated, and so it is hard to read) in order to understand
    why `f` behaves in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Migrating an old codebase from Tensorfow 1.x to 2.0 can be a time-consuming
    process. This is why the TensorFlow authors created a conversion tool that allows
    us to automatically migrate the source code (it even works on Python notebooks!).
  prefs: []
  type: TYPE_NORMAL
- en: Codebase migration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already seen, TensorFlow 2.0 brings a lot of breaking changes, which
    means that we have to relearn how to use the framework. TensorFlow 1.x is the
    most widely used machine learning framework and so there is a lot of existing
    code that needs to be upgraded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TensorFlow engineers developed a conversion tool that can help in the conversion
    process: unfortunately, it relies on the `tf.compat.v1` module, and it does not
    remove the graph nor the session execution. Instead, it just rewrites the code,
    prefixing it using `tf.compat.v1`, and applies some source code transformations
    to fix some easy API changes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is a good starting point to migrate a whole codebase. In fact,
    the suggested migration process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the migration script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manually remove every `tf.contrib` symbol, looking for the new location of the
    project that was used in the `contrib` namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manually switch the models to their Keras equivalent. Remove the sessions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the training loop in eager execution mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accelerate the computationally-intensive parts using `tf.function`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The migration tool, `tf_upgrade_v2`, is installed automatically when TensorFlow
    2.0 is installed via `pip`. The upgrade script works on single Python files, notebooks,
    or complete project directories.
  prefs: []
  type: TYPE_NORMAL
- en: 'To migrate a single Python file (or notebook), use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To run it on a directory tree, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, the script will print errors if it cannot find a fix for the
    input code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, it always reports a list of detailed changes in the `report.txt`
    file, which can help us understand why certain changes have been applied by the
    tool; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Migrating the codebase, even using the conversion tool, is a time-consuming
    process since most of the work is manual. Converting a codebase into TensorFlow
    2.0 is worth it since it brings many advantages, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy debugging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased code quality using an object-oriented approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fewer lines of code to maintain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future-proof—TensorFlow 2.0 follows the Keras standard and the standard will
    last the test of time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, all the major changes that were introduced in TensorFlow 2.0
    have been presented, including the standardization of the framework on the Keras
    API specification, the way models are defined using Keras, and how to train them
    using a custom training loop. We even looked at graph acceleration, which was
    introduced by AutoGraph, and `tf.function`.
  prefs: []
  type: TYPE_NORMAL
- en: AutoGraph, in particular, still requires us to know how the TensorFlow graph
    architecture works since the Python function that's defined and used in eager
    mode needs to be re-engineered if there is the need to graph-accelerate them.
  prefs: []
  type: TYPE_NORMAL
- en: The new API is more modular, object-oriented, and standardized; these groundbreaking
    changes have been made to make the usage of the framework easier and more natural,
    although the subtleties from the graph architecture are still present and always
    will be.
  prefs: []
  type: TYPE_NORMAL
- en: Those of you who have years of experience working with TensorFlow 1.0 may find
    it really difficult to change your way of thinking to the new object-based and
    no more graph- and session-based approach; however, it is a struggle that's worth
    it since the overall quality of the written software increases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about efficient data input pipelines and
    the Estimator API.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please go through the following exercises and answer all questions carefully.
    This is the only way (by making exercises, via trial and error, and with a lot
    of struggle) you will be able to master the framework and become an expert:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a classifier using the Sequential, Functional, and Subclassing APIs so
    that you can classify the fashion-MNIST dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model using the Keras model's built-in methods and measure the prediction
    accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a class that accepts a Keras model in its constructor and that it trains
    and evaluates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The API should work as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Accelerate the training method using the `@tf.function` annotation. Create a
    private method called `_train_step` to accelerate only the most computationally-intensive
    part of the training loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the training and measure the performance boost in milliseconds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Define a Keras model with multiple (2) inputs and multiple (2) outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model must accept a grayscale 28 x 28 x 1 image as input, as well as a second
    grayscale image that's 28 x 28 x 1 in size. The first layer should be a concatenation
    on the depth of these two images (28 x 28 x 1).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The architecture should be an autoencoder-like structure of convolutions that
    will reduce the input to a vector of 1 x 1 x 128 first, and then in its decoding
    part will upsample (using the `tf.keras.layer.UpSampling2D` layer) the layers
    until it gets back to 28 x 28 x D, where D is the depth of your choice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, two unary convolutional layers should be added on top of this last layer,
    each of them producing a 28 x 28 x 1 image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Define a training loop using the fashion-MNIST dataset that generates `(image,
    condition)` pairs, where `condition` is a 28 x 28 x 1 image completely white if
    the label associated with `image` is 6; otherwise, it needs to be a black image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before feeding the network, scale the input images in the `[-1, 1]` range.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train the network using the sum of two losses. The first loss is the L2 between
    the first input and the first output of the network. The second loss is the L1
    between the `condition` and the second output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Measure the L1 reconstruction error on the first pair during the training. Stop
    the training when the value is less than 0.5.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use the TensorFlow conversion tool to convert all the scripts in order to solve
    the exercises that were presented in [Chapter 3](f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml),
    *TensorFlow Graph Architecture*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Analyze the result of the conversion: does it uses Keras? If not, manually
    migrate the models by getting rid of every `tf.compat.v1` reference. Is this always
    possible?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pick a training loop you wrote for one of the preceding exercises: the gradients
    can be manipulated before applying the updates. Constraints should be the norm
    of the gradients and in the range of [-1, 1] before the updates are applied. Use
    the TensorFlow primitives to do that: it should be compatible with `@tf.function`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Does the following function produce any output if it''s decorated with `@tf.function`?
    Describe what happens under the hood:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Does the following function produce any output if it''s decorated with `@tf.function`?
    Describe what happens under the hood:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Does the following function produce any output if it''s decorated with `@tf.function`?
    Describe what happens under the hood:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Given ![](img/439eb476-406c-4946-93df-996c26702aad.png), compute the first and
    second-order partial derivatives using `tf.GradientTape` in ![](img/1b1e52b5-6cb9-4b6d-9049-3ae39657a945.png) and ![](img/f976628f-3388-4ab6-b144-e94851e5648c.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the side effects from the example that fails to execute in the *No more
    globals* section and use the constant instead of the variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extend the custom training loop defined in the *Custom training loop* section
    in order to measure the accuracy of the whole training set, of the whole validation
    set, and at the end of each training epoch. Then, perform model selection using
    two `tf.train.CheckpointManager` objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the validation accuracy stops increasing (with a variation of +/-0.2 at most)
    for 5 epochs, stop the training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following training functions, has the `step` variable been converted
    into a `tf.Variable` object? If not, what are the cons of this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Keep working on all of these exercises throughout this book.
  prefs: []
  type: TYPE_NORMAL
