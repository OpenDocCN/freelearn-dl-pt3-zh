- en: Imitation Learning with the DAgger Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ability of an algorithm to learn only from rewards is a very important characteristic
    that led us to develop reinforcement learning algorithms. This enables an agent
    to learn and improve its policy from scratch without additional supervision. Despite
    this, there are situations where other expert agents are already employed in a
    given environment. **Imitation learning** (**IL**) algorithms leverage the expert
    by imitating their actions and learning the policy from them.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on imitation learning. Although different to reinforcement
    learning, imitation learning offers great opportunities and capabilities, especially
    in environments with very large state spaces and sparse rewards. Obviously, imitation
    learning is possible only when a more expert agent to imitate is available.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will focus on the main concepts and features of imitation learning
    methods. We'll implement an imitation learning algorithm called DAgger, and teach
    an agent to play Flappy Bird. This will help you to master this new family of
    algorithms and appreciate their basic principles.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section of this chapter, we'll introduce **inverse reinforcement
    learning** (**IRL**). IRL is a method that extracts and learns the behaviors of
    another agent in terms of values and rewards; that is, IRL learns the reward function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The imitation approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing with Flappy Bird
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the dataset aggregation algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IRL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After a brief theoretical introduction to grasp the core concepts behind the
    imitation learning algorithms, we'll implement a real IL algorithm. However, we'll
    provide only the main and most interesting parts. Thus, if you are interested
    in the full implementation, you can find it in the GitHub repository of this book: [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: Installation of Flappy Bird
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Later, we'll run our IL algorithm on a revisited version of a famous game called Flappy
    Bird ([https://en.wikipedia.org/wiki/Flappy_Bird](https://en.wikipedia.org/wiki/Flappy_Bird)).
    In this section, we'll give you all the commands needed to install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before installing the environment of the game, we need to take care of
    a few additional libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Ubuntu, the procedure is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are a Mac user, you can install the libraries with the following commands:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for both Ubuntu and Mac users, the procedure is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, you have to clone PLE. The cloning is done with the following line of
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: PLE is a set of environments that also includes Flappy Bird. Thus, by installing
    PLE, you'll obtain Flappy Bird.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you have to enter the `PyGame-Learning-Environment` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, run the installation with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, you should be able to use Flappy Bird.
  prefs: []
  type: TYPE_NORMAL
- en: The imitation approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'IL is the art of acquiring a new skill by emulating an expert. This property
    of learning from imitation is not strictly necessary for learning sequential decision-making
    policies but nowadays, it is essential in plenty of problems. Some tasks cannot
    be solved through mere reinforcement learning, and bootstrapping a policy from
    the enormous spaces of complex environments is a key factor. The following diagram
    represents a high-level view of the core components involved in the imitation
    learning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c59df7c9-96f1-4662-9cda-5da6e9588fce.png)'
  prefs: []
  type: TYPE_IMG
- en: If intelligent agents (the experts) already exist in an environment, they can
    be used to provide a huge amount of information to a new agent (the learner) about
    the behaviors needed to accomplish the task and navigate the environment. In this
    situation, the newer agent can learn much faster without the need to learn from
    scratch. The expert agent can also be used as a teacher to instruct and feed back
    to the new agent on its performing. Note the difference here. The expert can be
    used both as a guide to follow and as a supervisor to correct the mistakes of
    the student.
  prefs: []
  type: TYPE_NORMAL
- en: If either the model of the guide, or the supervisor, is available, an imitation
    learning algorithm can leverage them. You can now understand why imitation learning
    plays such an important role and why we cannot leave it out of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The driving assistant example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To grasp these key concepts better, we can use the example of a teenager learning
    to drive. Let''s assume that they have never been in a car, that this is the first
    time they are seeing one, and that they don''t have any knowledge of how it works.
    There are three approaches to learning:'
  prefs: []
  type: TYPE_NORMAL
- en: They are given the keys and have to learn all by themselves, with no supervision
    at all.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before being given the keys, they sit in the passenger seat for 100 hours and
    look at the expert driving in different weather conditions and on different roads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They observe the expert driving but, most importantly, they have sessions where
    the expert provides feedback while driving. For example, the expert can give real-time
    instructions on how to park the car, and give direct feedback on how to stay in
    a lane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you may have guessed, the first case is a reinforcement learning approach
    where the agent has only sparse rewards from not breaking the car, pedestrians
    not yelling at them, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the second case, this is a passive IL approach with the competence
    that is acquired from the pure reproduction of the expert's actions. Overall,
    it's very close to a supervised learning approach.
  prefs: []
  type: TYPE_NORMAL
- en: The third and final case is an active IL approach that gives rise to a *real*
    imitation learning approach. In this case, it is required that, during the training
    phase, the expert instructs the learner on every move the learner makes.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing IL and RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go more in-depth with the IL approach by highlighting the differences
    vis-à-vis RL. This contrast is very important. In imitation learning, the learner
    is not aware of any reward. This constraint can have very big implications.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our example, the apprentice can only replicate the expert's moves
    as closely as possible, be it in a passive or an active way. Not having objective
    rewards from the environment, they are constrained to the subjective supervision
    of the expert. Thus, even if they wanted to, they aren't able to improve and understand
    the teacher's reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: So, IL should be seen as a way to copy the moves of the expert but without knowing
    its main goal. In our example, it's as if the young driver assimilates the trajectories
    of the teacher very well but, still, they don't know the motivations that made
    the teacher choose them. Without being aware of the reward, an agent trained with
    imitation learning cannot maximize the total reward as executed in RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'This highlights the main differences between IL and RL. The former lacks the
    understanding of the main objective, and thus cannot surpass the teacher. The
    latter instead lacks a direct supervision signal and, in most cases, has access only
    to a sparse reward. This situation is clearly depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09a97c8b-02fd-4770-94cd-ecb16c86a8c3.png)'
  prefs: []
  type: TYPE_IMG
- en: The diagram on the left represents the usual RL cycle, while on the right, the
    imitation learning cycle is represented. Here, the learner doesn't receive any
    reward; just the state and action given by the expert.
  prefs: []
  type: TYPE_NORMAL
- en: The role of the expert in imitation learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The terms *expert*, *teacher*, and *supervisor* refer to the same concept when
    speaking of imitation learning algorithms. They express a figure from which the
    new agent (the learner) can learn.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, the expert can be of any form, from a real human expert to an
    expert system. The first case is more obvious and adopted. What you are doing
    is teaching an algorithm to perform a task that a human is already able to do.
    The advantages are evident and it can be employed in a vast number of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The second case may not be so common. One of the valid motivations behind choosing
    a new algorithm trained with IL can be attributed to a slow expert system that,
    due to technical limitations, cannot be improved. For example, the teacher could
    be an accurate, but slow, tree search algorithm that is not able to perform at
    a decent speed at inference time. A deep neural network could be employed in its
    place. The training of the neural network under the supervision of the tree search
    algorithm could take some time but, once trained, it could perform much faster
    during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: By now, it should be clear that the quality of the policy coming from the learner
    is largely due to the quality of the information provided by the expert. The performance
    of the teacher is an upper limit to the final performances of the scholar. A poor
    teacher will always provide bad data to the learner. Thus, the expert is a key
    component that sets the bar for the quality of the final agent. With a weak teacher,
    we cannot pretend to obtain good policies.
  prefs: []
  type: TYPE_NORMAL
- en: The IL structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that all the ingredients of imitation learning have been tackled, we can
    elaborate on the algorithms and approaches that can be used in order to design
    a full imitation learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward way to tackle the imitation problem is shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d182866-bc4e-4b9d-84b4-b882be2af54f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram can be summarized in two main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: An expert collects data from the environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A policy is learned through supervised learning on the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, despite supervised learning being the imitation algorithm for
    excellence, most of the time, it doesn't work.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand why the supervised learning approach isn''t a good alternative,
    we have to recall the foundations of supervised learning. We are mostly interested
    in two basic principles: the training and test set should belong to the same distribution,
    and the data should be independent and identically distributed (i.i.d). However,
    a policy should be tolerant of different trajectories and be robust to eventual
    distribution shifts.'
  prefs: []
  type: TYPE_NORMAL
- en: If an agent is trained using only a supervised learning approach to drive a
    car, whenever it shifts a little bit from the expert trajectories, it will be
    in a new state never seen before, and that will create a distribution mismatch.
    In this new state, the agent will be uncertain about the next action to take.
    In a usual supervised learning problem, it doesn't matter too much. If a prediction
    is missed, this will not have an influence on the next prediction. However, in
    an imitation learning problem, the algorithm is learning a policy and the i.i.d
    property is no longer valid because subsequent actions are strictly correlated
    to each other. Thus, they will have consequences and a compounding effect on all
    the others.
  prefs: []
  type: TYPE_NORMAL
- en: In our example of the self-driving car, once the distribution has changed from
    that of the expert, the correct path will be very difficult to recover, since
    bad actions will accumulate and lead to dramatic consequences. The longer the
    trajectory, the worse the effect of imitation learning. To clarify, supervised
    learning problems with i.i.d. data can be seen as having a trajectory of length
    1\. No consequences on the next actions are found. The paradigm we have just presented
    is what we referred to previously as *passive* learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the distributional shift that can have catastrophic effects on
    policies learned using *passive* imitation, different techniques can be adopted.
    Some are hacks*,* while others are more algorithmic variations. Two of these strategies
    that work well are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning a model that generalizes very well on the data without overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an active imitation in addition to the passive one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the first is more of a broad challenge, we will concentrate on the second
    strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing active with passive imitation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the term *active imitation* in the previous example, with the
    teenager learning to drive a car. Specifically, we referred to it in the situation
    in which the learner was driving with additional feedback from the expert. In
    general, for active imitation, we mean learning from on-policy data with the actions
    assigned by the expert.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking in terms of input *s* (the state or observation) and output *a* (the
    action), in passive learning, s and a both come from the expert. In active learning,
    s is sampled from the learner, and a is the action that the expert would have
    taken in state s. The objective of the newbie agent is to learn a mapping, [![](img/6d071f93-e760-4783-ba8a-76a29b1a221d.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Active learning with on-policy data allows the learner to fix small deviations
    from the expert trajectory that the learner wouldn't know how to correct with
    only passive imitation.
  prefs: []
  type: TYPE_NORMAL
- en: Playing Flappy Bird
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Later in this chapter, we'll develop and test an IL algorithm called DAgger
    on a new environment. The environment named Flappy Bird emulates the famous Flappy
    Bird game. Here, our mission is to give you the tools needed to implement code
    using this environment, starting from the explanation of the interface.
  prefs: []
  type: TYPE_NORMAL
- en: Flappy Bird belongs to the **PyGame Learning Environment** (**PLE**), a set
    of environments that mimic the **Arcade Learning Environment** (**ALE**) interface.
    This is similar to the **Gym** interface, and later we'll see the differences,
    although it's simple to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of Flappy Bird is to make the bird fly through vertical pipes without
    hitting them. It is controlled by only one action that makes it flap its wings.
    If it doesn''t fly, it progresses in a decreasing trajectory determined by gravity.
    A screenshot of the environment is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dab636b8-a119-4ae9-91f0-4ca19952eccf.png)'
  prefs: []
  type: TYPE_IMG
- en: How to use the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following steps, we will see how to use the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use Flappy Bird in our Python scripts, firstly, we need to import
    PLE and Flappy Bird:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we instance a `FlappyBird` object and pass it to `PLE` with a few parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, with `display_screen`, you can choose whether to display the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The environment is initialized by calling the `init()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To interact and get the state of the environment, we primarily use four functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`p.act(act)`, to execute the `act` action in the game. `act(act)` returns the
    reward obtained from the action performed.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p.game_over()`, to check whether the game reached a final state.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p.reset_game()`, to reset the game to the initial conditions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p.getGameState()`, to obtain the current state of the environment. We could
    also use `p.getScreenRGB()` if we want to obtain the RGB observations (that is,
    the full screen) of the environment.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Putting everything together, a simple script that plays Flappy Bird for five
    games can be designed as in the following code snippet. Note that in order to
    make it work, you still have to define the `get_action(state)` function that returns
    an action given a state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A couple of things to point out here are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getGameState()` returns a dictionary with the position, velocity, and the
    distance of the player, as well as the position of the next pipe and the following
    one. Before giving the state to the policymaker that we represented here with
    the `get_action` function, the dictionary is converted to a NumPy array and normalized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`act(action)` expects `None` as input if no action has to be performed, or
    `119` if the bird has to flap its wings in order to fly higher.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the dataset aggregation algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most successful algorithms that learns from demonstrations is **Dataset
    Aggregation** (**DAgger**). This is an iterative policy meta-algorithm that performs
    well under the distribution of states induced. The most notable feature of DAgger
    is that it addresses the distribution mismatch by proposing an active method in
    which the expert teaches the learner how to recover from the learner's mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: A classic IL algorithm learns a classifier that predicts expert behaviors. This
    means that the model fits a dataset consisting of training examples, observed
    by an expert. The inputs are the observations, and the actions are the desired
    output values. However, following the previous reasoning, the predictions of the
    learner affect the future state or observation visited, violating the i.i.d assumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'DAgger deals with the change in distribution by iterating a pipeline of aggregation
    of new data sampled from the learner multiple times, and training with the aggregated
    dataset. A simple diagram of the algorithm is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe1faf16-a462-463c-a02b-3e0531dd25b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The expert populates the dataset used by the classifier, but, depending on the
    iteration, the action performed in the environment may come from the expert or
    the learner.
  prefs: []
  type: TYPE_NORMAL
- en: The DAgger algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Specifically, DAgger proceeds by iterating the following procedure. At the first
    iteration, a dataset *D* of trajectories is created from the expert policy and
    used to train a first policy ![](img/dd21f2a8-7117-44ca-96d1-97e70f418dfc.png)
    that best fits those trajectories without overfitting them. Then, during iteration
    *i*, new trajectories are collected with the learned policy ![](img/f9898e60-eb25-48ed-a2df-4a73bbc2b431.png) and
    added to the dataset *D*. After that, the aggregated dataset *D* with the new
    and old trajectories is used to train a new policy, ![](img/34d844e4-4668-46b2-b95e-6621b653c169.png).
  prefs: []
  type: TYPE_NORMAL
- en: As per the report in the Dagger paper ([https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf)), there
    is an active on-policy learning that outperforms many other imitation learning
    algorithms, and it's also able to learn very complex policies with the help of
    deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, at iteration *i*, the policy can be modified so that the expert
    takes control of a number of actions. This technique better leverages the expert
    and lets the learner gradually assume control over the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudocode of the algorithm can clarify this further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Implementation of DAgger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code is divided into three main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the expert inference function to predict an action given a state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a computational graph for the learner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the DAgger iterations to build the dataset and train the new policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we'll explain the most interesting parts, leaving the others for your
    personal interest. You can check the remaining code and the complete version in
    the book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the expert inference model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The expert should be a policy that takes a state as input and returns the best
    action. Despite this, it can be anything. In particular, for these experiments,
    we used an agent trained with Proximal Policy Optimization (PPO) as the expert.
    In principle, this doesn't make any sense, but we adopted this solution for academic
    purposes, to facilitate integration with the imitation learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expert''s model trained with PPO has been saved on file so that we can easily
    restore it with its trained weights. Three steps are required to restore the graph
    and make it usable:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the meta graph. The computational graph can be restored with `tf.train.import_meta_graph`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore the weights. Now, we have to load the pretrained weights on the computational
    graph we have just imported. The weights have been saved in the latest checkpoint
    and they can be restored with `tf.train.latest_checkpoint(session, checkpoint)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access the output tensors. The tensors of the restored graph are accessed with
    `graph.get_tensor_by_name(tensor_name)`, where `tensor_name` is the tensor's name
    in the graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following lines of code summarize the entire process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, because we are only interested in a simple function that returns an expert
    action given a state, we can design the `expert` function in such a way that it
    returns that function. Thus, inside `expert()`, we define an inner function called `expert_policy(state)` and
    return it as output of `expert()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Creating the learner's computational graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the following code is located inside a function called `DAgger`, which takes some
    hyperparameters that we'll see throughout the code as arguments.
  prefs: []
  type: TYPE_NORMAL
- en: The learner's computational graph is simple as its only goal is to build a classifier.
    In our case, there are only two actions to predict, one for doing nothing, and
    the other to make the bird flap its wings. We can instantiate two placeholders,
    one for the input state, and one for the *ground-truth* actions that are those
    of the expert. The actions are an integer corresponding to the action taken. In
    the case of two possible actions, they are just 0 (do nothing) or 1 (fly).
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to build such a computational graph are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a deep neural network, specifically, a fully connected multilayer perceptron
    with a ReLu activations function in the hidden layers and a linear function on
    the final layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every input state, take the action with the highest value. This is done
    using the `tf.math.argmax(tensor,axis)` function with `axis=1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the action's placeholders in a one-hot tensor. This is needed because
    the logits and labels that we'll use in the loss function should have dimensions, `[batch_size,
    num_classes]`. However, our labels named `act_ph` have shapes, `[batch_size]`.
    Therefore, we convert them to the desired shape with one-hot encoding. `tf.one_hot` is
    the TensorFlow function that does just that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the loss function. We use the softmax cross-entropy loss function. This
    is a standard loss function used for discrete classification with mutually exclusive
    classes, just like in our case. The loss function is computed using `softmax_cross_entropy_with_logits_v2(labels,
    logits)` between the logits and the labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, the mean of the softmax cross-entropy is computed across the batch and
    minimized using Adam.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These five steps are implemented in the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then initialize a session, the global variables, and define a function, `learner_policy(state)`.
    This function, given a state, returns the action with a higher probability chosen
    by the learner (this is the same thing we did for the expert):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Creating a DAgger loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s now time to set up the core of the DAgger algorithm. The outline has
    already been defined in the pseudocode in *The DAgger algorithm* section, but
    let''s take a more in-depth look at how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the dataset composed of two lists, `X` and `y`, where we''ll put
    the states visited and the expert target actions. We also initialize the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate across all the DAgger iterations. At the beginning of every DAgger
    iteration, we have to reinitialize the learner computational graph (because we
    retrain the learner on every iteration on the new dataset), reset the environment,
    and run a number of random actions. At the start of each game, we run a few random
    actions to add a stochastic component to the deterministic environment. The result
    will be a more robust policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Collect new data by interacting with the environment. As we said previously,
    the first iteration contains the expert that has to choose the actions by calling
    `expert_policy`, but, in the following iterations, the learner progressively takes
    control. The learned policy is executed by the `learner_policy` function. The
    dataset is collected by appending to `X` (the input variable) the current state
    of the game, and by appending to `y` (the output variable) the actions that the
    expert would have taken in that state. When the game is over, the game is reset
    and `game_rew` is set to `0`. The code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that the actions are performed twice. This is done to reduce the number
    of actions every second to 15 instead of 30, as required by the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the new policy on the aggregated dataset. The pipeline is standard. The
    dataset is shuffled and divided into mini-batches of length `batch_size`. Then,
    the optimization is repeated by running `p_opt` for a number of epochs equals
    to `train_epochs` on each mini-batch. This is done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`test_agent` tests `learner_policy` on a few games to understand how well the
    learner is performing.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the results on Flappy Bird
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before showing the results of the imitation learning approach, we want to provide
    some numbers so that you can compare these with those of a reinforcement learning
    algorithm. We know that this is not a fair comparison (the two algorithms work
    on very different conditions), but nevertheless, they underline why imitation
    learning can be rewarding when an expert is available.
  prefs: []
  type: TYPE_NORMAL
- en: The expert has been trained with proximal policy optimization for about 2 million
    steps and, after about 400,000 steps, reached a plateau score of about 138.
  prefs: []
  type: TYPE_NORMAL
- en: 'We tested DAgger on Flappy Bird with the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hyperparameter** | **Variable name** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| Learner hidden layers | hidden_sizes | 16,16 |'
  prefs: []
  type: TYPE_TB
- en: '| DAgger iterations | dagger_iterations | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Learning rate | p_lr | 1e-4 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of steps for every DAgger iteration | step_iterations | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| Mini-batch size | batch_size | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| Training epochs | train_epochs | 2000 |'
  prefs: []
  type: TYPE_TB
- en: 'The plot in the following screenshot shows the trend of the performance of
    DAgger with respect to the number of steps taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48524175-156e-43d5-aa99-ff195df2dec0.png)'
  prefs: []
  type: TYPE_IMG
- en: The horizontal line represents the average performance reached by the expert.
    From the results, we can see that a few hundred steps are sufficient to reach
    the performance of the expert. However, compared with the experience required
    by PPO to train the expert, this represents about a 100-fold increase in sample
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is not a fair comparison as the methods are in different contexts,
    but it highlights that whenever an expert is available, it is suggested that you
    use an imitation learning approach (perhaps at least to learn a starting policy).
  prefs: []
  type: TYPE_NORMAL
- en: IRL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest limitations of IL lies in its inability to learn other trajectories
    to reach a goal, except those learned from the expert. By imitating an expert,
    the learner is constrained to the range of behaviors of its teacher. They are
    not aware of the end goal that the expert is trying to reach. Thus, these methods
    are only useful when there's no intention to perform better than the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: IRL is an RL algorithm, such as IL, that uses an expert to learn. The difference
    is that IRL uses the expert to learn its reward function. Therefore, instead of
    copying the demonstrations, as is done in imitation learning, IRL figures out
    the goal of the expert. Once the reward function is learned, the agent uses it
    to learn the policy.
  prefs: []
  type: TYPE_NORMAL
- en: With the demonstrations used only to understand the goal of the expert, the
    agent is not bound to the actions of the teacher and can finally learn better
    strategies. For example, a self-driving car that learns by IRL would understand
    that the goal is to go from point A to point B in the minimum amount of time,
    while reducing the damage to things and people. The car would then learn a policy
    by itself (for example, with an RL algorithm) that maximizes this reward function.
  prefs: []
  type: TYPE_NORMAL
- en: However, IRL also has a number of challenges that limit its applicability. The
    expert's demonstration may not be optimal, and, as a result, the learner may not
    be able to achieve its full potential and may remain stuck in the wrong reward
    function. The other challenge lies in the evaluation of the learned reward function.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a break from reinforcement learning algorithms and
    explored a new type of learning called imitation learning. The novelty of this
    new paradigm lies in the way in which the learning takes place; that is, the resulting
    policy imitates the behavior of an expert. This paradigm differentiates from reinforcement
    learning in the absence of a reward signal and in its ability to leverage the
    incredible source of information brought by the expert entity.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that the dataset from which the learner learns can be expanded with additional
    state action pairs to increase the confidence of the learner in new situations.
    This process is called data aggregation. Moreover, new data could come from the
    new learned policy and, in this case, we talked about on-policy data (as it comes
    from the same policy learned). This integration of on-policy states with expert
    feedback is a very valuable approach that increases the quality of the learner.
  prefs: []
  type: TYPE_NORMAL
- en: We then explored and developed one of the most successful imitation learning
    algorithms, called DAgger, and applied it to learn the Flappy Bird game.
  prefs: []
  type: TYPE_NORMAL
- en: However, because imitation learning algorithms only copy the behavior of an
    expert, these systems cannot do better than the expert. Therefore, we introduced
    inverse reinforcement learning, which overcomes this problem by inferring the
    reward function from the expert. In this way, the policy can be learned independently
    of the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll take a look at another set of algorithms for solving
    sequential tasks; namely, evolutionary algorithms. You'll learn the mechanisms
    and advantages of these black-box optimization algorithms so that you'll be able
    to adopt them in challenging environments. Furthermore, we'll delve into an evolutionary
    algorithm called evolution strategy in greater depth and implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is imitation learning considered a reinforcement learning technique?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Would you use imitation learning to build an unbitable agent in Go?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the full name of DAgger?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the main strength of DAgger?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where would you use IRL instead of IL?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To read the original paper that introduced DAgger, checkout the following paper, *A
    Reduction of Imitation Learning and Structured Prediction to No-Regret Online
    Learning*: [https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about imitation learning algorithms, checkout the following paper,
    *Global Overview of Imitation Learning*: [https://arxiv.org/pdf/1801.06503.pdf](https://arxiv.org/pdf/1801.06503.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn more about inverse reinforcement learning, checkout the following
    survey, *A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress*: [https://arxiv.org/pdf/1806.06877.pdf](https://arxiv.org/pdf/1806.06877.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
