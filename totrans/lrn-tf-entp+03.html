<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer062">&#13;
			<p id="_idParaDest-30" class="chapter-number"><a id="_idTextAnchor061"/>Chapter 2:</p>&#13;
			<h1 id="_idParaDest-31"><a id="_idTextAnchor062"/>Running TensorFlow Enterprise in Google AI Platform</h1>&#13;
			<p><a id="_idTextAnchor063"/>Currently, the TensorFlow Enterprise distribution is only available through Google Cloud AI Platform. This chapter will demonstrate how to launch AI Platform for use with TensorFlow Enterprise. In AI Platform, TensorFlow Enterprise can interact with Cloud Storage and BigQuery via their respective command-line tools as well as simple APIs to load data from the source. In this chapter, we are going to take a look at how to launch AI Platform and how easy it is to start using the TensorFlow Enterprise distribution. </p>&#13;
			<p>We'll cover the following main topics:</p>&#13;
			<ul>&#13;
				<li>Setting up a notebook environment</li>&#13;
				<li>Easy parameterized data extraction from BigQuery</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-32"><a id="_idTextAnchor064"/>Setting up a notebook environment</h1>&#13;
			<p>TensorFlow <a id="_idIndexMarker071"/>Enterprise is exclusively available in the JupyterLab environment hosted by Google Cloud. There are three ways to consume the JupyterLab with <a id="_idIndexMarker072"/>this TensorFlow distribution: <strong class="bold">Google Cloud</strong> <strong class="bold">AI Platform Notebook</strong>, <strong class="bold">Google Cloud</strong> <strong class="bold">Deep Learning Virtual Machine Images (DLVM)</strong>, and <strong class="bold">Google Cloud </strong><strong class="bold"><a id="_idIndexMarker073"/></strong><strong class="bold">Deep Learning Containers</strong> (<strong class="bold">Docker image</strong>) running on your local machine. No matter which one you choose, you will see the same interface of a standard JupyterLab environment like this: </p>&#13;
			<div>&#13;
				<div id="_idContainer039" class="IMG---Figure">&#13;
					<img src="Images/Figure_2.1.jpg" alt="Figure 2.1 – JupyterLab portal"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 2.1 – JupyterLab portal</p>&#13;
			<p>So let's take a look at how to get started.<a id="_idTextAnchor065"/> </p>&#13;
			<h2 id="_idParaDest-33"><a id="_idTextAnchor066"/>AI Platform Notebook</h2>&#13;
			<p>This is the <a id="_idIndexMarker074"/>easiest and least complicated way to start using TensorFlow Enterprise and get it running in Google Cloud:</p>&#13;
			<ol>&#13;
				<li>Simply go to the Google Cloud portal, select <strong class="bold">AI Platform</strong> in the left panel, then select the <strong class="bold">Notebooks</strong> option:<div id="_idContainer040" class="IMG---Figure"><img src="Images/Figure_2.2.jpg" alt="Figure 2.2 – AI Platform starting portal&#13;&#10;"/></div><p class="figure-caption">Figure 2.2 – AI Platform starting portal</p></li>&#13;
				<li>Then <a id="_idIndexMarker075"/>click on <strong class="bold">NEW INSTANCE</strong>, and you'll be offered choices for TensorFlow Enterprise, which is available for <strong class="bold">1.15</strong> as well as <strong class="bold">2.1</strong> and <strong class="bold">2.3</strong>. You also have the option to use one <strong class="bold">Tesla K4</strong> GPU:<div id="_idContainer041" class="IMG---Figure"><img src="Images/Figure_2.3.jpg" alt="Figure 2.3 – Creating a new notebook instance in AI Platform&#13;&#10;"/></div><p class="figure-caption">Figure 2.3 – Creating a new notebook instance in AI Platform</p><p>For our examples in <a id="_idIndexMarker076"/>this chapter, we don't need to use a GPU. Selecting <strong class="bold">Without GPUs</strong> will suffice.</p></li>&#13;
				<li>Then click on <strong class="bold">CREATE</strong> to accept the default node choice, or <strong class="bold">CUSTOMIZE</strong> to see all the setup options available:<div id="_idContainer042" class="IMG---Figure"><img src="Images/Figure_2.4.jpg" alt="Figure 2.4 – Customizing a compute instance&#13;&#10;"/></div><p class="figure-caption">Figure 2.4 – Customizing a compute instance</p><p>These are the <a id="_idIndexMarker077"/>available machine configuration choices when using the notebook option in AI Platform:</p><div id="_idContainer043" class="IMG---Figure"><img src="Images/Figure_2.5.jpg" alt="Figure 2.5 – Available options for the machine instance&#13;&#10;"/></div><p class="figure-caption">Figure 2.5 – Available options for the machine instance</p><p>The Notebook instance will be available within a few minutes after clicking <strong class="bold">CREATE</strong>:</p><div id="_idContainer044" class="IMG---Figure"><img src="Images/Figure_2.6.jpg" alt="Figure 2.6 – Instance going live and ready&#13;&#10;"/></div><p class="figure-caption">Figure 2.6 – Instance going live and ready</p></li>&#13;
				<li>When <a id="_idIndexMarker078"/>the instance is ready, <strong class="bold">OPEN JUPYTERLAB</strong> will be activated and you may click on it. Clicking on it will lead you to a JupyterLab notebook:</li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer045" class="IMG---Figure">&#13;
					<img src="Images/Figure_2.7.jpg" alt="Figure 2.7 – JupyterLab environment&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 2.7 – JupyterLab environment</p>&#13;
			<p>We will use Python 3 for all our examples. </p>&#13;
			<h2 id="_idParaDest-34"><a id="_idTextAnchor067"/>Deep Learning Virtual Machine Image</h2>&#13;
			<p>If you wish to have <a id="_idIndexMarker079"/>more options, such as different GPU choices, then DLVM is a better choice. You may find these references helpful:</p>&#13;
			<ul>&#13;
				<li><a href="https://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-cli">https://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-cli</a></li>&#13;
				<li><a href="https://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-marketplace">https://cloud.google.com/ai-platform/deep-learning-vm/docs/quickstart-marketplace</a></li>&#13;
			</ul>&#13;
			<p>Follow these steps to choose DLVM:</p>&#13;
			<ol>&#13;
				<li value="1">Click <strong class="bold">Marketplace</strong> in the left panel:<div id="_idContainer046" class="IMG---Figure"><img src="Images/Figure_2.8.jpg" alt="Figure 2.8 – Google Cloud Platform Marketplace&#13;&#10;"/></div><p class="figure-caption">Figure 2.8 – Google Cloud Platform Marketplace</p></li>&#13;
				<li>Search <a id="_idIndexMarker080"/>for <strong class="source-inline">Deep Learning VM</strong> in the query box and you will see the following:<div id="_idContainer047" class="IMG---Figure"><img src="Images/Figure_2.9.jpg" alt="Figure 2.9 – Enabling DLVM&#13;&#10;"/></div><p class="figure-caption">Figure 2.9 – Enabling DLVM</p><p>This is where <a id="_idIndexMarker081"/>you can launch a DLVM deployment.</p></li>&#13;
				<li>Click on <strong class="bold">LAUNCH</strong>, and you will see many options available, including <strong class="bold">Machine Type</strong>, <strong class="bold">GPU type</strong>, and <strong class="bold">Number of GPUs</strong>:<div id="_idContainer048" class="IMG---Figure"><img src="Images/Figure_2.10.jpg" alt="Figure 2.10 – DLVM configuration portal&#13;&#10;"/></div><p class="figure-caption">Figure 2.10 – DLVM configuration portal</p><p>Also, DLVM has <a id="_idIndexMarker082"/>many more frameworks besides TensorFlow Enterprise:</p><div id="_idContainer049" class="IMG---Figure"><img src="Images/Figure_2.11.jpg" alt="Figure 2.11 – DLVM and options for frameworks&#13;&#10;"/></div><p class="figure-caption">Figure 2.11 – DLVM and options for frameworks</p></li>&#13;
				<li>If you choose one <a id="_idIndexMarker083"/>of the two TensorFlow Enterprise frameworks, then click <p><strong class="bold">CREATE</strong>, you will be able to reach JupyterLab as you did previously:</p></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer050" class="IMG---Figure">&#13;
					<img src="Images/Figure_2.12.jpg" alt="Figure 2.12 – JupyterLab entry point&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 2.12 – JupyterLab entry point</p>&#13;
			<p>Here's a suggestion. In <a id="_idIndexMarker084"/>order to minimize your cost, it is important to stop your instances after you are done. The quickest way to see what you have running is to choose <strong class="bold">Compute Engine</strong> in the left panel, and then select <strong class="bold">VM instances</strong>:</p>&#13;
			<div>&#13;
				<div id="_idContainer051" class="IMG---Figure">&#13;
					<img src="Images/Figure_2.13.jpg" alt="Figure 2.13 – Compute instances in a subscription&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 2.13 – Compute instances in a subscription</p>&#13;
			<p>From there you <a id="_idIndexMarker085"/>will see all the instances you have created. Stop them when you are done:</p>&#13;
			<div>&#13;
				<div id="_idContainer052" class="IMG---Figure">&#13;
					<img src="Images/Figure_2.14.jpg" alt="Figure 2.14 – Listing VM instances and managing their use&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 2.14 – Listing VM instances and managing their use</p>&#13;
			<p>It is the user's responsibility to be aware of the instances that are running. As a good practice, when you <a id="_idIndexMarker086"/>are finished with your work, download or check in your notebook to save your work, and delete the instance when not in use<a id="_idTextAnchor068"/>.</p>&#13;
			<h2 id="_idParaDest-35"><a id="_idTextAnchor069"/>Deep Learning Container (DLC)</h2>&#13;
			<p>This is a relatively <a id="_idIndexMarker087"/>more complicated way of using TensorFlow Enterprise. An important reason for using this approach is for cases where data is not stored in Google Cloud, and you wish to run TensorFlow Enterprise on-premises or in your local machine. Another reason is that for enterprise use, you may want to use DLC as a base Docker image to build your own Docker image for a specific use or distribution amongst your team. This is the way to run TensorFlow Enterprise outside of Google Cloud. Since it is a Docker image, it requires the Docker Engine installed, and the daemon running. It would be extremely helpful to have some basic understanding of Docker. You will find a full list of currently available DLCs at <a href="https://console.cloud.google.com/gcr/images/deeplearning-platform-release">https://console.cloud.google.com/gcr/images/deeplearning-platform-release</a>.</p>&#13;
			<p>The goal is running a TensorFlow Enterprise JupyterLab. But since it is in a local machine, the URL to the JupyterLab is in the following format:</p>&#13;
			<p><strong class="source-inline">localhost:&lt;LOCAL_PORT&gt;</strong></p>&#13;
			<p>Here's how we can accomplish this (for reference, see <a href="https://cloud.google.com/ai-platform/deep-learning-containers/docs/getting-started-local">https://cloud.google.com/ai-platform/deep-learning-containers/docs/getting-started-local</a>):</p>&#13;
			<ol>&#13;
				<li value="1">Assuming a Docker daemon is running, we will execute the following command to run the TensorFlow Enterprise container:<p class="source-code">docker run -d -p &lt;LOCAL_PORT&gt;:8080 -v &lt;LOCAL_DIR&gt;:/home &lt;CONTAINER_REGISTRY&gt;</p><p>Let's understand the parts of the preceding command with the following table:</p><div id="_idContainer053" class="IMG---Figure"><img src="Images/Figure_2.15.jpg" alt="Figure 2.15 – Explaining the objects of the command to run the TensorFlow Enterprise container&#13;&#10;"/></div><p class="figure-caption">Figure 2.15 – Explaining the objects of the command to run the TensorFlow Enterprise container</p><p>In the preceding <a id="_idIndexMarker088"/>table, note the following:</p><ul><li><strong class="source-inline">&lt;LOCAL_PORT&gt;</strong> refers to the port number in the local machine to host this Docker image instance. It may be <strong class="source-inline">8080</strong>, or any other available port number that you wish to use, should <strong class="source-inline">8080</strong> be in use by another program or process already.</li><li><strong class="source-inline">&lt;LOCAL_DIR&gt;</strong> is the path to the top-level directory where the training data and assets can be found. For a Windows machine, it may be <strong class="source-inline">C:\Users\XXXX\Documents</strong>. For Linux or Mac machine, it may be <strong class="source-inline">/home/XXXX/Documents</strong>.</li><li><strong class="source-inline">&lt;CONTAINER_REGISTRY&gt;</strong> is where the Docker image can be found on the internet, and for the Docker container of our interest, it is in <strong class="source-inline">gcr.io/deeplearning-platform-release/tf2-cpu.2-1</strong>.</li></ul></li>&#13;
				<li>Put these together in a command and run it from a terminal of a local machine (such as Windows Command Prompt):<p class="source-code"><strong class="bold">docker run -d -p 8080:8080 -v C:\Users\xxxx\Documents:/home/jupyter gcr.io/deeplearning-platform-release/tf2-cpu.2-3</strong></p><p class="source-code"><strong class="bold">C:\Users\xxxx&gt;docker run -d -p 8080:8080 -v    C:\Users\xxxx\Documents:/home/jupyter gcr.io/deeplearning-platform-release/tf2-cpu.2-3</strong></p><p>A local port must be mapped to the Docker image's JupyterLab port. JupyterLab uses port <strong class="source-inline">8080</strong> inside the <a id="_idIndexMarker089"/>Docker image. Notice that the preceding command maps local port <strong class="source-inline">8080</strong> to the Docker image's port <strong class="source-inline">8080</strong>. The first <strong class="source-inline">8080</strong> is your local port, the second <strong class="source-inline">8080</strong> is the port number used by JupyterLab inside the Docker image environment. Again, the local port number doesn't have to be <strong class="source-inline">8080</strong>.</p></li>&#13;
				<li>Now you may access the local port through your browser:<p><strong class="source-inline">localhost:8080</strong></p><p>And you will see the JupyterLab running as a Docker container, as follows:</p><div id="_idContainer054" class="IMG---Figure"><img src="Images/Figure_2.16.jpg" alt="Figure 2.16 – Docker image of JupyterLab running in a local or on-premises environment"/></div><p class="figure-caption">Figure 2.16 – Docker image of JupyterLab running in a local or on-premises environment</p><p>Let's take a look at <a id="_idIndexMarker090"/>the left panel first. The left panel shows all the local files and folders that you designated as <strong class="source-inline">&lt;LOCAL_DIR&gt;</strong>. In this case, it is <strong class="source-inline">/temp/chapter2</strong></p><p>The <strong class="source-inline">-v</strong> (or <strong class="source-inline">--volume</strong>) option maps the local directory to the <strong class="source-inline">/home</strong> directory of your Docker container instance. This is how local contents become accessible to your Docker container.</p></li>&#13;
				<li>You can click on the <strong class="bold">Python 3</strong> icon in launcher to start using JupyterLab to read any file in <strong class="source-inline">/home</strong>:<div id="_idContainer055" class="IMG---Figure"><img src="Images/Figure_2.17.jpg" alt="Figure 2.17 – Docker image of JupyterLab reading local data&#13;&#10;"/></div><p class="figure-caption">Figure 2.17 – Docker image of JupyterLab reading local data</p></li>&#13;
				<li>You can also write data to the local directory:<div id="_idContainer056" class="IMG---Figure"><img src="Images/Figure_2.18.jpg" alt="Figure 2.18 – Docker image of JupyterLab writing data locally&#13;&#10;"/></div><p class="figure-caption">Figure 2.18 – Docker image of JupyterLab writing data locally</p><p>The following <a id="_idIndexMarker091"/>command uses <strong class="source-inline">/home</strong> as the file path:</p><p class="source-code">iris.to_csv('/home/iris-write-from-docker.csv', index=False)</p><p>Since you mapped <strong class="source-inline">/home</strong> with a local directory, you will also find the file in the local file explorer:</p><div id="_idContainer057" class="IMG---Figure"><img src="Images/Figure_2.19.jpg" alt="Figure 2.19 – Local data written by JupyterLab running in a Docker image&#13;&#10;"/></div><p class="figure-caption">Figure 2.19 – Local data written by JupyterLab running in a Docker image</p></li>&#13;
				<li>Once you are <a id="_idIndexMarker092"/>done, in order to shut down the Docker image, you need to know the container ID assigned to this instance by your local Docker daemon. The command is as follows:<p class="source-code"><strong class="bold">docker ps</strong></p><p>And it will return an output similar to this:</p><p class="source-code">CONTAINER ID        IMAGE                                              COMMAND                  CREATED             STATUS              PORTS                    NAMES</p><p class="source-code">553cfd198067        gcr.io/deeplearning-platform-release/tf2-cpu.2-1   '/entrypoint.sh /run…'   44 minutes ago      Up 44 minutes       0.0.0.0:8080-&gt;8080/tcp   intelligent_goldwasser</p></li>&#13;
				<li>Make a note of the <strong class="source-inline">CONTAINER ID</strong> value. Then use the following command to shut it down:<p class="source-code"><strong class="bold">docker stop 553cfd198067</strong></p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-36"><a id="_idTextAnchor070"/>Suggestions for selecting workspaces</h2>&#13;
			<p>All three methods <a id="_idIndexMarker093"/>discussed in the previous section lead you to a JupyterLab that runs TensorFlow Enterprise. There are some differences and consequences to consider for each method:</p>&#13;
			<ul>&#13;
				<li>The Docker image running locally is preferred for local data access.</li>&#13;
				<li>The DLC can serve as a base image for creating a new enterprise-specific image.<p>With the Docker image running locally, its advantage lies in its direct access to the local environment or data sources. We have seen how it can easily read and write data on a local node. This obviously cannot be easily achieved with the AI Platform environment. Therefore, if the training data and output are to stay on-premises or in the local environment, then this is the most sensible choice. The downside of this method is the overhead of setting up and managing your own Docker environment. Another reason for using the DLC is that big enterprises often need to have customizable environments. They may want to create their own Docker container on top of the DLC and later ask everyone in the company to use that container with Cloud AI Platform Notebook. Notebook supports the custom container mode, as long as that container is based on the DLC.</p></li>&#13;
				<li>Use DLVM if you want to customize compute instance cores, memory, and disk resources.<p>If you want to configure the CPU, GPU, memory, or disk resources for the workload, then DLVM is the method of choice.</p></li>&#13;
				<li>Use the default notebook environment for most general needs.<p>With AI Platform, the notebook environment obviously has direct access to cloud storage such as bucket containers or BigQuery tables. If it is not essential to pick and choose your CPU or GPU configurations, then the AI Platform Notebook would definitely suffice.</p></li>&#13;
			</ul>&#13;
			<p>What we have learned so far are the three different environments for users to start using Google's AI Platform and consume the TensorFlow Enterprise distribution. By and large, these methods all provide a consistent user experience and runtime distribution of the TensorFlow Enterprise library. The rationale for choosing a method is grounded in your need for data access and compute resource configurations. If the <a id="_idIndexMarker094"/>training data is on-premises or on your local disks, then the Docker image is the preferred method. If compute resources and speed are the primary concerns, then DLVM is the preferred choice. </p>&#13;
			<p>Now, having arrived at AI Platform and its notebook environment, as a starter, we are going to take a closer look at a common example of using AI Platform to access data in BigQuery and build your own training da<a id="_idTextAnchor071"/>ta. </p>&#13;
			<h1 id="_idParaDest-37"><a id="_idTextAnchor072"/>Easy parameterized data extraction from BigQuery  </h1>&#13;
			<p>Very often, your <a id="_idIndexMarker095"/>enterprise data warehouse <a id="_idIndexMarker096"/>contains the sources for you to build your own training data, and simple SQL query commands would meet your requirements for row and column selection and feature transformation. So let's take a look at a convenient, flexible, and fast way of selecting and manipulating original data through SQL queries, where the result of the query is a pandas DataFrame. We have already seen how to use the <strong class="source-inline">%%bigquery</strong> interpreter to execute a query and return the result as a pandas DataFrame. We now will look at how to pass in query parameters so users may explore and select data suitable for model training. The following example uses one of the public datasets, <strong class="source-inline">covid19_juh_csse</strong>, and its <strong class="source-inline">summary</strong> table. </p>&#13;
			<p>This table has the following structure:</p>&#13;
			<div>&#13;
				<div id="_idContainer058" class="IMG---Figure">&#13;
					<img src="Images/Figure_2.20.jpg" alt="Figure 2.20 – A table’s schema as shown using BigQuery&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 2.20 – A table's schema using BigQuery</p>&#13;
			<p>In the <a id="_idIndexMarker097"/>JupyterLab provided by any of the three methods discussed <a id="_idIndexMarker098"/>earlier, you may execute the following steps to perform parameterized queries:</p>&#13;
			<ol>&#13;
				<li value="1">Define a set of parameters in a JSON-compatible format, that is, a key-value pair as in a Python dictionary:<p class="source-code">params = {'min_death': 1000,</p><p class="source-code">         		'topn': 10}</p></li>&#13;
				<li>Construct <a id="_idIndexMarker099"/>the query and assign it to a DataFrame by <a id="_idIndexMarker100"/>name. Notice how each key in the parameter is referenced in the query with a preceding <strong class="source-inline">@</strong>:<p class="source-code">%%bigquery myderiveddata  --params $params</p><p class="source-code">SELECT country_region,  MAX(confirmed) as total_confirmed, MAX(deaths) AS total_deaths </p><p class="source-code">FROM `bigquery-public-data.covid19_jhu_csse.summary`</p><p class="source-code">GROUP BY country_region </p><p class="source-code">HAVING (total_deaths &gt; @min_death) </p><p class="source-code">ORDER BY total_deaths DESC </p><p class="source-code">LIMIT @topn </p></li>&#13;
				<li>Examine the data:<p class="source-code">myderiveddata</p></li>&#13;
			</ol>&#13;
			<p>And the following is the output of the aggregation command, which demonstrates the total results by country:</p>&#13;
			<div>&#13;
				<div id="_idContainer059" class="IMG---Figure">&#13;
					<img src="Images/Figure_2.21.jpg" alt="Figure 2.21 – Output of the aggregation command from the notebook&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 2.21 – Output of the aggregation command from the notebook</p>&#13;
			<p>We may <a id="_idIndexMarker101"/>confirm the data structure of our data object using <a id="_idIndexMarker102"/>Python's <strong class="source-inline">type</strong> command:</p>&#13;
			<p class="source-code">type(myderiveddata) </p>&#13;
			<p>And it confirms the object being a pandas DataFrame:</p>&#13;
			<p class="figure-caption"><img src="Images/B16070_02_021.png" alt="Figure 2.22 – Output from type(myderiveddata) &#13;&#10;"/></p>&#13;
			<p class="figure-caption">Figure 2.22 – Output from type(myderiveddata) </p>&#13;
			<p>The DataFrame may be serialized as a pickle file for future use. Once converted to the pickle format, you may persist it in the cloud storage as demonstrated in the previous <a id="_idTextAnchor073"/>chapter.</p>&#13;
			<p>Here are the key takeaways:</p>&#13;
			<ul>&#13;
				<li>Parameterized querying enables quick and easy data selection and manipulation for building training data as a pandas DataFrame.</li>&#13;
				<li>Parameters are wrapped in a Python dictionary and can be passed into the query string during execution.</li>&#13;
				<li>The <a id="_idIndexMarker103"/>query string can refer to the parameter with <a id="_idIndexMarker104"/>the <strong class="source-inline">@<a id="_idTextAnchor074"/></strong> operator.</li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-38"><a id="_idTextAnchor075"/>Putting it together</h2>&#13;
			<p>The following is the complete code snippet for the quick example we just worked with:</p>&#13;
			<p class="source-code">params = {'min_death': 1000,</p>&#13;
			<p class="source-code">          'topn': 10}</p>&#13;
			<p class="source-code">%%bigquery myderiveddata --params $params</p>&#13;
			<p class="source-code">SELECT country_region,  MAX(confirmed) as total_confirmed, MAX(deaths) AS total_deaths </p>&#13;
			<p class="source-code">FROM `bigquery-public-data.covid19_jhu_csse.summary`</p>&#13;
			<p class="source-code">GROUP BY country_region </p>&#13;
			<p class="source-code">HAVING (total_deaths &gt; @min_death) </p>&#13;
			<p class="source-code">ORDER BY total_deaths DESC </p>&#13;
			<p class="source-code">LIMIT @topn</p>&#13;
			<p class="source-code">print(myderiveddata)</p>&#13;
			<p>The output is shown in <em class="italic">Figure 2.23</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer061" class="IMG---Figure">&#13;
					<img src="Images/Figure_2.22.jpg" alt="Figure 2.23 – Output from BigQuery and compatibility with pandas DataFrame format &#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 2.23 – Output from BigQuery and compatibility with pandas DataFr<a id="_idTextAnchor076"/>ame format </p>&#13;
			<p>As the preceding steps demonstrate, the notebook environment is integrated closely with BigQuery. As a result, an inline query with SQL produces a DataFrame that is ready for use in Python. This further demonstrates the flexibility of the Google Cloud AI Platform Notebook en<a id="_idTextAnchor077"/>vironment.</p>&#13;
			<h1 id="_idParaDest-39"><a id="_idTextAnchor078"/>Summary</h1>&#13;
			<p>In this chapter, you have learned how to launch the JupyterLab environment to run TensorFlow Enterprise. TensorFlow Enterprise is available in three different forms: AI Platform Notebook, DLVM, and a Docker container. The computing resources used by these methods can be found in the Google Cloud Compute Engine panel. These compute nodes do not shut down on their own, therefore it is important to stop or delete them once you are done using them. </p>&#13;
			<p>The BigQuery command tool is seamlessly integrated with the TensorFlow Enterprise environment. Parameterized data extraction via the use of a SQL query string enables the quick and easy creation of a derived dataset and feature selection. </p>&#13;
			<p>TensorFlow Enterprise works even when your data is not yet in Google Cloud storage. By pulling and running the TensorFlow Enterprise Docker container, you can use it with on-premises or local data sources. </p>&#13;
			<p>Now that you have seen how to leverage data availability and accessibility for TensorFlow Enterprise consumption, in the next chapter, we are going to examine some common data transformation, serialization, and storage techniques optimized for TensorFlow Enterprise consumption and model training p<a id="_idTextAnchor079"/>ipelines. </p>&#13;
		</div>&#13;
	</div></body></html>