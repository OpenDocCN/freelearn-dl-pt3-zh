<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Enhancing and Segmenting Images</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have just learned how to create neural networks that output predictions that are more complex than just a single class. In this chapter, we will push this concept further and introduce <strong>encoders-decoders</strong>, which are models used to edit or generate full images. We will present how encoder-decoder networks can be applied to a wide range of applications, from image denoising to object and instance segmentation. This chapter comes with several concrete examples, such as the application of encoders-decoders to semantic segmentation for self-driving cars.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>What encoders-decoders are, and how they are trained for pixel-level prediction</li>
<li>Which novel layers they use to output high-dimensional data (unpooling, transposed, and atrous convolutions)</li>
<li>How the FCN and U-Net architectures are tackling semantic segmentation</li>
<li>How the models we have covered so far can be extended to deal with instance segmentation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Jupyter notebooks illustrating the concepts presented in this chapter can be found in the following Git folder: <a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter06">github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter06</a><a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-Tensorflow/tree/master/ch3"><span>.</span></a></p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-Tensorflow/tree/master/ch3"/></p>
<p>Later in this chapter, we introduce the <kbd>pydensecrf</kbd> library to improve segmentation results. As detailed on its GitHub page (refer to the documentation at <a href="https://github.com/lucasb-eyer/pydensecrf#installation">https://github.com/lucasb-eyer/pydensecrf#installation</a>), this Python module can be installed through <kbd>pip</kbd> (<kbd>pip install git+https://github.com/lucasb-eyer/pydensecrf.git</kbd>) and requires a recent version of Cython (<kbd>pip install -U cython</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming images with encoders-decoders</h1>
                </header>
            
            <article>
                
<p>As presented in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, multiple typical tasks in computer vision require pixel-level results. For example, semantic segmentation methods classify each pixel of an image, and smart editing tools return images with some pixels altered (for example, to remove unwanted elements). In this section, we will present encoders-decoders, and how <strong><span>convolutional neural networks</span></strong> (<strong>CNNs</strong>) following this paradigm can be applied to such applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to encoders-decoders</h1>
                </header>
            
            <article>
                
<p>Before tackling complex applications, let's first introduce what encoders-decoders are and what purpose they fulfill.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding and decoding</h1>
                </header>
            
            <article>
                
<p>The encoder-decoder architecture is a very generic framework, with applications in communications, cryptography, electronics, and beyond. According to this framework, the <strong>encoder</strong> is a function that maps input samples into a <strong>latent space</strong>, that is, a hidden structured set of values defined by the encoder. The <strong>decoder</strong> is the complementary function that maps elements from this latent space into a predefined target domain. For example, an encoder can be built to parse media files (with their content represented as elements in its latent space), and it can be paired with a decoder defined, for instance, to output the media contents in a different file format. Well-known examples are the image and audio compression formats we commonly use nowadays. JPEG tools encode our media, compressing them into lighter binary files; they then decode them to recover the pixel values at display time.</p>
<p class="mce-root"/>
<p>In machine learning, encoder-decoder networks have been used for a long time now (for instance, for text translation). An encoder network would take sentences from the source language as input (for instance, French sentences) and learn to project them into a latent space where the meaning of the sentence would be encoded as a feature vector. A decoder network would be trained alongside the encoder to convert the encoded vectors into sentences in the target language (for instance, English).</p>
<div class="packt_infobox">Vectors from the latent space in encoder-decoder models are commonly called <strong>codes</strong>.</div>
<p>Note that a common property of encoders-decoders is for their latent space to be smaller than the input and target latent spaces, as shown in <em>Figure 6-1</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2500985a-3cd3-4bc5-8ef6-0a0fd5799262.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-1: Example of an auto-encoder trained on the MNIST dataset (copyright owned by Yann LeCun and Corinna Cortes)</div>
<p>In <em>Figure 6-1</em>, the encoder is trained to convert the <em>28</em> × <em>28</em> images into vectors (codes) of <em>32</em> values here, and the decoder is trained to recover the images. These codes can be plotted with their class labels to highlight similarities/structures in the dataset (the <em>32</em>-dimensional vectors are projected on a 2D plane using <strong>t-SNE</strong>, a method developed by Laurens van der Maatens and Geoffrey Hinton and detailed in the notebooks).</p>
<p>Encoders are designed or trained to extract/compress the semantic information contained in the samples (for example, the meaning of a French sentence, without the grammatical particularities of this language). Then, decoders apply their knowledge of the target domain to decompress/complete the information accordingly (for instance, converting the encoded information into a proper English sentence).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auto-encoding</h1>
                </header>
            
            <article>
                
<p><strong>Auto-encoders</strong> (<strong>AEs</strong>) are a special type of encoders-decoders. As shown in <em>Figure 6-1</em>, their input and target domains are the same, so their goal is to properly encode and then decode images without impacting their quality, despite their <em>bottleneck</em> (their latent space of lower dimensionality). The inputs are reduced to a compressed representation (as feature vectors). If an original input is requested later on, it can be reconstructed from its compressed representation by the decoder.</p>
<p>JPEG tools can thus be called AEs, as their goal is to encode images and then decode them back without losing too much of their quality. The distance between the input and output data is the typical loss to minimize for auto-encoding algorithms. For images, this distance can simply be computed as the cross-entropy loss, or as the L1/L2 loss (Manhattan and Euclidean distances, respectively) between the input images and resulting images (as illustrated in <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>).</p>
<p>In machine learning, auto-encoding networks are really convenient to train, not only because their loss is straightforward to express, as we just described, but also because their training does not require any labels. The input images are the targets used to compute the loss.</p>
<div class="packt_infobox">There is a schism among machine learning experts regarding AEs. Some claim that these models are <strong>unsupervised</strong> since they do not require any additional labels for their training. Others affirm that, unlike purely unsupervised methods (which typically use complex loss functions to discover patterns in unlabeled datasets), AEs have clearly defined targets (that is, their input images). Therefore, it is also common for these models to be called <strong>self-supervised</strong> (that is, their targets can be directly derived from their inputs).</div>
<p>Given the smaller latent space of AEs, their encoding sub-network must learn to properly compress the data, whereas the decoder must learn a proper mapping to decompress it back.</p>
<div class="packt_infobox">Without the bottleneck condition, this identity mapping would be straightforward for networks with shortcut paths, such as ResNet (refer to <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>)<em>.</em> They could simply forward the complete input information from encoder to decoder. With a lower-dimensional latent space (bottleneck), they are forced to learn a properly compressed representation.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Purpose</h1>
                </header>
            
            <article>
                
<p>Regarding the more generic encoders-decoders, their applications are numerous. They are used to convert images, to map them from one domain or modality to another. For example, such models are often applied to <strong>depth regression</strong>, that is, the estimation of the distance between the camera and the image content (the depth) for each pixel. This is an important operation for augmented-reality applications, for example, since it allows them to build a 3D representation of the surroundings, and thus to better interact with the environment.</p>
<p>Similarly, encoders-decoders are commonly used for <strong>semantic segmentation</strong> (refer to <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, for its definition). In this case, the networks are trained not to return the depth, but the estimated class for each pixel (refer to <em>Figure 6-2-c</em>). This important application will be detailed in the second part of this chapter. Finally, encoders-decoders are also famous for their more <em>artistic use cases</em>, such as transforming doodle art into pseudo-realistic images or estimating the daytime equivalent of pictures taken at night:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dad5686d-2a67-4c74-97fd-dc41265ac87e.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6-2: Examples of applications for encoders-decoders. These three applications are covered in the Jupyter notebooks for this chapter, with additional explanations and implementation details</div>
<div class="packt_infobox">The urban scene images and their labels for semantic segmentation in <em>Figure 6-2</em>, <em>Figure 6-10</em>, and <em>Figure 6-11</em> come from the <em>Cityscapes</em> dataset (<a href="https://www.cityscapes-dataset.com">https://www.cityscapes-dataset.com</a>). <em>Cityscapes</em> is an awesome dataset and a benchmark for recognition algorithms applied to autonomous driving. Marius Cordts et al., the researchers behind this dataset, kindly gave us the authorization to use some of their images to illustrate this book and to demonstrate some algorithms presented later in this chapter (refer to Jupyter notebooks).</div>
<p>Let's now consider AEs. Why should a network be trained to return its input images? The answer lies once again in the bottleneck property of AEs. While the encoding and decoding components are trained as a whole, they are applied separately depending on the use cases.</p>
<p>Because of the bottleneck, the encoder has to compress the data while preserving as much information as possible. Therefore, in case the training dataset has recurring patterns, the network will try to uncover these correlations to improve the encoding. The encoder part of an AE can thus be used to obtain low-dimensional representations of images from the domain it was trained for. The low-dimensional representations they provide are often good at preserving the content similarity between images, for instance. Therefore, they are sometimes used for dataset visualization, to highlight clusters and patterns (refer to <em>Figure 6-1</em>).</p>
<div class="packt_infobox">AEs are not as good as algorithms, such as JPEG for generic image compression. Indeed, AEs are <em>data-specific</em>; that is, they can only efficiently compress images from the domain they know (for example, an AE trained on images of natural landscapes would work poorly on portraits since the visual features would be too different). However, unlike traditional compression methods, AEs have a better understanding of the images they were trained for, their recurring features, semantic information, and more).</div>
<p>In some cases, AEs are trained for their decoders, which can be used for <strong>generative tasks</strong>. Indeed, if the latent space has been appropriately structured during training, then any vector randomly picked from this space can be turned into a picture by the decoder! As we will briefly explain later in this chapter and in <a href="337ec077-c215-4782-b56c-beae4d94d718.xhtml">Chapter 7</a>, <em>Training on Complex and Scarce Datasets</em>, training a decoder for the generation of new images is actually not that easy, and requires some careful engineering for the resulting images to be realistic (this is especially true for the training of <strong>generative adversarial networks</strong> (<strong>GANs</strong>), as explained in the next chapter).</p>
<p>However, <strong>denoising AEs</strong> are the most common AE instances found in practice. These models have the particularity that their input images undergo a lossy transformation before being passed to the networks. Since these models are still trained to return the original images (before transformation), they will learn to cancel the lossy operation and recover some of the missing information (refer to <em>Figure 6-2-a</em>). Typical models are trained to cancel white or Gaussian noise, or to recover missing content (such as occluded/removed image patches). Such AEs are also used for <strong>smart image upscaling</strong>, also called <strong>image super-resolution</strong>. Indeed, these networks can learn to partially remove the artifacts (that is, noise) caused by traditional upscaling algorithms such as bilinear interpolation (refer to <em>Figure 6-2-b</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic example – image denoising</h1>
                </header>
            
            <article>
                
<p>We will illustrate the usefulness of AEs on a simple example—the denoising of corrupted MNIST images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simplistic fully connected AE</h1>
                </header>
            
            <article>
                
<p>To demonstrate how simple, yet efficient, these models can be, we will opt for a shallow, fully connected architecture, which we will implement with Keras:</p>
<pre>inputs = Input(<span>shape</span>=[img_height * img_width])<br/><span># Encoding layers:<br/></span>enc_1  = Dense(<span>128</span><span>, </span><span>activation</span>=<span>'relu'</span>)(inputs)<br/>code   = Dense(<span>64</span><span>,  </span><span>activation</span>=<span>'relu'</span>)(enc_1)<br/><span># Decoding layers:<br/></span>dec_1  = Dense(<span>64</span><span>,  </span><span>activation</span>=<span>'relu'</span>)(code)<br/>preds  = Dense(<span>128</span><span>, </span><span>activation</span>=<span>'sigmoid'</span>)(dec_1)<span><br/></span>autoencoder = Model(inputs<span>, </span>preds)<br/># Training:<br/>autoencoder.compile(<span>loss</span>=<span>'binary_crossentropy'</span>)<br/>autoencoder.fit(<strong>x_train</strong><span>, </span><strong>x_train</strong>) # x_train as inputs and targets</pre>
<p class="mce-root">We have highlighted here the usual symmetrical architecture of encoders-decoders, with their lower-dimensional bottleneck. To train our AE, we use the images (<kbd>x_train</kbd>) both as inputs and as targets. Once trained, this simple model can be used to embed datasets, as shown in <em>Figure 6-1</em>.</p>
<div class="packt_tip">We opted for <em>sigmoid</em> as the last activation function, in order to get output values between 0 and 1, like the input values.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application to image denoising</h1>
                </header>
            
            <article>
                
<p class="mce-root">Training our previous model for image denoising is as simple as creating a noisy copy of the training images and passing it as input to our network instead:</p>
<pre>x_noisy = x_train + np.random.normal(loc=.0, scale=.5, size=x_train.shape)<br/>autoencoder.fit(x_noisy, x_train)</pre>
<div class="packt_infobox">The first two notebooks dedicated to this chapter detail the training process, providing illustrations and additional tips (for instance, to visualize the images predicted during training).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional encoders-decoders</h1>
                </header>
            
            <article>
                
<p>Like other <strong>neural network</strong> (<strong>NN</strong>)-based systems, encoders-decoders benefited a lot from the introduction of convolutional and pooling layers. <strong>Deep auto-encoders</strong> (<strong>DAEs</strong>) and other architectures soon became widely used for increasingly complex tasks.</p>
<p>In this section, we will first introduce new layers developed for convolutional encoders-decoders. We will then present some significant architectures based on these operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unpooling, transposing, and dilating</h1>
                </header>
            
            <article>
                
<p>As we saw in previous chapters, such as <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>, and <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml"/><a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>, CNNs are great <em>feature extractors</em>. Their convolutional layers convert their input tensors into more and more high-level feature maps, while their pooling layers gradually down-sample the data, leading to compact and semantically rich features. Therefore, CNNs make for performant encoders.</p>
<p>However, how could this process be reversed to decode these low-dimensional features into full images? As we will present in the following paragraphs, the same way convolutions and pooling operations replaced dense layers for the encoding of images, reverse operations—such as <strong>transposed convolution</strong> (also known as <strong>deconvolutions</strong>), <strong>dilated convolutions</strong>, and <strong>unpooling</strong>—were developed to better decode features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transposed convolution (deconvolution)</h1>
                </header>
            
            <article>
                
<p>Back in <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>, we introduced convolutional layers, the operations they perform, and how their hyperparameters (kernel size <em>k</em>, input depth <em>D</em>, number of kernels <em>N</em>, padding <em>p</em>, and stride <em>s</em>) affect the dimensions of their output (<em>Figure 6-3</em> serves as a reminder). For an input tensor of shape (<em>H</em><span class="ui_qtext_rendered_qtext">, <em>W</em>, <em>D</em></span>), we presented the following equations to evaluate the output shape (<span class="ui_qtext_rendered_qtext"><em>H<sub>o</sub></em>, <em>W<sub>o</sub></em>, <em>N</em></span>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f8aa2ec8-6584-443a-b480-9c97145b58f0.png" style="width:24.50em;height:2.50em;"/></p>
<p>Now, let's assume that we want to develop a layer to reverse the spatial transformation of convolutions. In other words, given a feature map of shape (<span class="ui_qtext_rendered_qtext"><em>H<sub>o</sub></em>, <em>W<sub>o</sub></em>, <em>N</em></span>) and the same hyperparameters, <em>k</em>, <span class="ui_qtext_rendered_qtext"><em>D</em></span>, <span class="ui_qtext_rendered_qtext"><em>N</em></span>, <em>p</em>, and <em>s</em>, we would like a <em>convolution-like</em> operation to recover a tensor of shape (<em>H</em><span class="ui_qtext_rendered_qtext">, <em>W</em>, <em>D</em></span>). Isolating <span class="ui_qtext_rendered_qtext"><em>H</em> and <em>W</em> in the previous equations, we thus want an operation upholding the following properties:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1d436c0a-3be4-40bf-ad90-52ef801a8561.png" style="width:24.50em;height:1.25em;"/></p>
<p>This is how <strong>transposed convolutions</strong> were defined. As we briefly mentioned in <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>, this new type of layer was proposed by Zeiler and Fergus, the researchers behind ZFNet, the winning methods at ILSVRC 2013 (<em>Visualizing and understanding convolutional networks</em>, <em>Springer, 2014</em>).</p>
<p>With a <em>k</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <em>k</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <span class="ui_qtext_rendered_qtext"><em>D</em> <span class="js-about-item-abstr">×</span> <em>N</em> stack of</span> kernels, these layers convolve an <span class="ui_qtext_rendered_qtext"><em>H<sub>o</sub></em></span> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <span class="ui_qtext_rendered_qtext"><em>W<sub>o</sub></em> <span class="js-about-item-abstr">×</span> <em>N</em></span> tensor into an <span class="ui_qtext_rendered_qtext"><em>H</em></span> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <span class="ui_qtext_rendered_qtext"><em>W</em> <span class="js-about-item-abstr">×</span> <em>D</em> map. To achieve this, the input tensor first undergoes <strong>dilation</strong>. The dilation operation, defined by a rate, <em>d</em>, consists of inserting <em>d</em> – 1 zeroed rows and columns</span> between each couple of rows and columns (respectively) of the input tensor, as shown in <em>Figure 6-4</em>. In a transposed convolution, the dilation rate is set to <em>s</em> (the stride used for the standard convolution it is reversing). After this resampling, the tensor is then padded by <em>p</em>' = <em>k</em> – <em>p</em> – 1. Both the dilation and padding parameters are defined in this way in order to recover the original shape, (<em>H</em><span class="ui_qtext_rendered_qtext">, <em>W</em>, <em>D</em></span>). The tensor is then finally convolved with the layer's filters using a stride of <em>s</em>' = 1, finally resulting in an <em>H</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <span class="ui_qtext_rendered_qtext"><em>W</em> <span class="js-about-item-abstr">×</span> <em>D</em> map. Normal and transposed convolutions are compared in <em>Figures 6-3</em> and <em>6-4</em>.<br/></span></p>
<p>The following is a normal convolution:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c33cac43-87ea-4a18-8a78-d962bc44b7c3.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-3: Reminder of the operations performed by a convolutional layer (defined here by a 3 × 3 kernel w, padding p = 1, and stride s = 2)</div>
<div class="packt_infobox"><span>Note that in <em>Figure 6-3</em>, the mathematical operation between the patches and the kernel is actually a cross-correlation (refer to <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>).</span></div>
<p>The following is a transposed convolution:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/849cef3b-5538-4a9e-befe-17b6f6e7b6ed.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-4: Operations performed by a transposed convolution layer to reverse the spatial transformation of a standard convolution (defined here by a 3 × 3 kernel <em>w</em>, padding <em>p</em> = 1, and dilation <em>d</em> = 2, as in Figure 6-3)</div>
<div class="packt_infobox"><span>Note that this time, in <em>Figure 6-4</em>, the operation between the patches and the kernel is mathematical convolution.</span></div>
<p>If this process seems a bit abstract, it is enough to remember that transposed convolutional layers are commonly used to mirror standard convolutions in order to increase the spatial dimensionality of feature maps while convolving their content with trainable filters. This makes these layers quite suitable for decoder architectures. They can be instantiated using <kbd>tf.layers.conv2d_transpose()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/layers/conv2d_transpose">https://www.tensorflow.org/api_docs/python/tf/layers/conv2d_transpose</a>) and <kbd>tf.keras.layers.Conv2DTranspose()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose</a>), which have the same signatures as the standard <kbd>conv2d</kbd> ones.</p>
<div class="mce-root packt_infobox">There is another subtle difference between standard convolutions and transposed ones, which does not have any real impact in practice, but which is still good to know. Going back to <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>, we mentioned that convolutional layers in CNNs actually perform cross-correlation. As shown in <em>Figure 6-4</em>, transposed convolutional layers actually use mathematical convolution, flipping the indices of the kernels.<br/>
<br/>
Transposed convolutions are also popularly, yet wrongly, called <strong>deconvolutions</strong>. While there is a mathematical operation named <em>deconvolution</em>, it performs differently than transposed convolution. Deconvolutions actually fully revert convolutions, returning the original tensors. Transposed convolutions only approximate this process and return tensors with the original shapes. As we can see in <em>Figures 6-3</em> and <em>6-4</em>, the shapes of the original and final tensors match, but not their values.<br/>
<br/>
Transposed convolutions are also sometimes called <strong>fractionally strided convolutions</strong>. Indeed, the dilation of the input tensors can somehow be seen as the equivalent of using a <em>fractional</em> stride for the convolution.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unpooling</h1>
                </header>
            
            <article>
                
<p>Although strided convolutions are often used in CNN architectures, average-pooling and max-pooling are the most common operations when it comes to reducing the spatial dimensions of images. Therefore, Zeiler and Fergus also proposed a <strong>max-unpooling</strong> operation (often simply referred to as <strong>unpooling</strong>) to pseudo-reverse max-pooling. They used this operation within a network they called a <strong>deconvnet</strong>, to decode and visualize the features of their <em>convnet</em> (that is, a CNN). In the paper describing their solution after winning ILSVRC 2013 (in <em>Visualizing and understanding convolutional networks</em>, <em>Springer, 2014</em>), they explain that, even though max-pooling is not invertible (that is, we cannot mathematically recover all the non-maximum values the operation discards), it is possible to define an operation approximating its inversion, at least in terms of spatial sampling.</p>
<p>To implement this pseudo-inverse operation, they first modified each max-pooling layer so that it outputs the pooling mask along with the resulting tensor. In other words, this mask indicates the original positions of the selected maxima. The max-unpooling operation takes for inputs the pooled tensor (which may have undergone other shape-preserving operations in-between the operation) and the pooling mask. It uses the latter to scatter the input values into a tensor upscaled to its pre-pooling shape. A picture is worth a thousand words, so <em>Figure 6-5</em> may help you to understand the operation:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7dbfd6f3-6a7f-4ccc-aa49-df760cfec394.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-5: Example of a max-unpooling operation, following a max-pooling layer edited to also output its pooling mask</div>
<p>Note that, like pooling layers, unpooling operations are fixed/untrainable operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upsampling and resizing</h1>
                </header>
            
            <article>
                
<p>Similarly, an <strong>average-unpooling</strong> operation was developed to mirror average-pooling. The latter operation takes a pooling region of <em>k</em> × <em>k</em> elements and averages them into a single value. Therefore, an average-unpooling layer takes each value of a tensor and duplicates it into a <em>k</em> × <em>k</em> region, as illustrated in <em>Figure 6-6</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fffece53-111c-4993-b031-1220992eb61e.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-6: Example of an average-unpooling operation (also known as upsampling)</div>
<p>This operation is nowadays <span>used </span>more often than max-unpooling, and is more commonly known as <strong>upsampling</strong>. For instance, this operation can be instantiated through <kbd>tf.keras.layers.UpSampling2D()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D">https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D</a>). This method is itself nothing more than a wrapper for <kbd>tf.image.resize()</kbd> (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/image/resize">https://www.tensorflow.org/api_docs/python/tf/image/resize</a>) when called with the <kbd>method=tf.image.ResizeMethod.NEAREST_NEIGHBOR</kbd> argument, used to resize images using nearest-neighbor interpolation (as its name implies). Finally, note that bilinear interpolation is also sometimes used to upscale feature maps without adding any parameters to train, for example, by instantiating <kbd>tf.keras.layers.UpSampling2D()</kbd> with the <kbd>interpolation="bilinear"</kbd> argument (instead of the default <kbd>"nearest"</kbd> value), which is equivalent to calling <kbd>tf.image.resize()</kbd> with the default <kbd>method=tf.image.ResizeMethod.BILINEAR</kbd> attribute.</p>
<p>In decoder architecture, each <span>nearest-neighbor</span> or bilinear upscaling is commonly followed by a convolution with stride <em>s</em> = 1 and padding <kbd>"SAME"</kbd> (to preserve the new shape). These combinations of predefined upscaling and convolutional operations mirror the convolutional and pooling layers composing encoders, and allow the decoder to learn its own features to better recover the target signals.</p>
<div class="packt_infobox">Some researchers, such as Augustus Odena, favor these operations over transposed convolutions, especially for tasks such as image super-resolution. Indeed, transposed convolutions tend to cause some checkerboard artifacts (due to feature overlapping when the kernel size is not a multiple of the stride), impacting the output quality (<em>Deconvolution and Checkerboard artifacts, Distill, 2016</em>).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dilated/atrous convolution</h1>
                </header>
            
            <article>
                
<p>The last operation we will introduce in this chapter is a bit different from the previous ones, as it is not meant to upsample a feature map <span>provided</span>. Instead, it was proposed to artificially increase the receptive field of convolutions without further sacrificing the spatial dimensionality of the data. To achieve this, <strong>dilation</strong> is applied here too (refer to the <em>Transposed convolutions (deconvolution)</em> section), though quite differently.</p>
<p class="mce-root"/>
<p>Indeed, <strong>dilated convolutions</strong> are similar to standard convolutions, with an additional hyperparameter, <em>d</em>, <span class="ui_qtext_rendered_qtext">defining the</span> dilation applied to their kernels<span class="ui_qtext_rendered_qtext">. <em>Figure 6-7</em> illustrates how this process does artificially increase the layer's receptive field:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6d123b71-98b4-41cd-83c9-a11928744340.png" style="width:51.50em;height:20.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6-7: Operations performed by a dilated-convolutional layer (defined here by a 2 × 2 kernel w, padding p = 1, stride s = 1, and dilation d = 2<span>)</span></div>
<div class="packt_infobox">These layers are also called <strong>atrous convolutions</strong>, from the French expression <em>à trous</em> (<em>with holes</em>). Indeed, while the kernel dilation increases the receptive field, it does so by carving holes in it.</div>
<p>With such properties, this operation is frequently used in modern encoders-decoders, to map images from one domain to another. In TensorFlow and Keras, instantiating dilated convolutions is just a matter of providing a value above the default 1 for the <kbd>dilation_rate</kbd> parameter of <kbd>tf.layers.conv2d()</kbd> and <kbd>tf.keras.layers.Conv2D()</kbd>.</p>
<p>These various operations developed to preserve or increase the spatiality of feature maps led to multiple CNN architectures for pixel-wise dense prediction and data generation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example architectures – FCN and U-Net</h1>
                </header>
            
            <article>
                
<p>Most convolutional encoders-decoders follow the same template as their fully connected counterparts, but leverage the spatial properties of their locally connected layers for higher-quality results. A typical convolutional AE is presented in one of the Jupyter notebooks. In this subsection, we will cover two more advanced architectures derived from this basic template. Both released in 2015, the FCN and U-Net models are still popular, and are commonly used as components for more complex systems (in semantic segmentation, domain adaptation, and others).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully convolutional networks</h1>
                </header>
            
            <article>
                
<p>As briefly presented in <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>, <strong>fully convolutional networks</strong> (<strong>FCNs</strong>) are based on the VGG-16 architecture, with the final dense layers replaced by <em>1 </em>× <em>1</em> convolutions. What we did not mention was that these networks are commonly extended with upsampling blocks and used as encoders-decoders. Proposed by <span>Jonathan Long</span><span>,</span> <span>Evan Shelhamer</span><span>, and</span> <span>Trevor Darrell from the</span> University of California, <span>Berkeley, the FCN architecture perfectly illustrates the notions developed in the previous subsection:</span></p>
<ul>
<li><span>How CNNs for feature extraction can be used as efficient encoders</span></li>
<li><span>How their feature maps can then be effectively upsampled and decoded by the operations we just introduced</span></li>
</ul>
<p>Indeed, Jonathan Long et al. suggested reusing a pretrained VGG-16 as a feature extractor (refer to <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>). With its five convolutional blocks, VGG-16 efficiently transforms images into feature maps, albeit dividing their spatial dimensions by two after each block. To decode the feature maps from the last block (for instance, into semantic masks), the fully connected layers used for classification are replaced by convolutional ones. The final layer is then applied – a transposed convolution to upsample the data back to the input shape (that is, with a stride of <em>s</em> = 32, since the spatial dimensions are divided by 32 through VGG).</p>
<p>However, Long et al. quickly noticed that this architecture, named <strong>FCN-32s</strong>, was yielding overly <em>coarse</em> results. As explained in their paper (<em>Fully convolutional networks for semantic segmentation</em>, <em>Proceedings of the IEEE CVPR conference</em>, <em>2015</em>), the large stride at the final layer indeed <span>limits the scale of detail</span>. Though the features from the last VGG block contain rich contextual information, too much of their spatial definition is already lost. Therefore, the authors had the idea to fuse the feature maps from the last block with those larger ones from previous blocks.</p>
<p>In FCN-16s, the last layer of FCN-32s is thus replaced by a transposed layer with a stride of <em>s</em> = 2 only, so the resulting tensor has the same dimensions as the feature map from the fourth block. Using a skip connection, features from both tensors are merged together (element-wise addition). The result is finally scaled back to the input shape with another transposed convolution with <em>s</em> = 16. In FCN-8s, the same procedure is repeated instead with features from the third block, before the final transposed convolution with <em>s</em> = 8. For clarity, the complete architecture is presented in <em>Figure 6-8</em>, and a Keras implementation is provided in the next example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6bf5d945-e3fe-4a50-831b-85475c77e8f6.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-8: FCN-8s architecture. The data dimensions are shown after each block, supposing an <em>H</em> × <em>W</em> input. <em>D</em><sub>o</sub> represents the desired number of output channels</div>
<p><em>Figure 6-8</em> illustrates how VGG-16 serves as a feature extractor/encoder, and how the transposed convolutions are used for decoding. The figure also highlights that FCN-32s and FCN-16s are simpler, lighter architectures, with only one skip connection, or none at all.</p>
<p class="mce-root">With its use of transfer learning and its fusion of multi-scale feature maps, FCN-8s can output images with fine details. Furthermore, because of its fully convolutional nature, it can be applied to encode/decode images of different sizes. Performant and versatile, FCN-8s is still commonly used in many applications, while inspiring multiple other architectures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">U-Net</h1>
                </header>
            
            <article>
                
<p>Among the solutions inspired by FCNs, the U-Net architecture is not only one of the first; it is probably the most popular (proposed by Olaf Ronneberger, Philipp Fischer, and Thomas Brox in a paper entitled <em>U-Net: Convolutional networks for biomedical image segmentation</em>, published by Springer).</p>
<p>Also developed for semantic segmentation (applied to medical imaging), it shares multiple properties with FCNs. It is also composed of a multi-block contractive encoder that increases the features' depth while reducing their spatial dimensions, and of an expansive decoder that recovers the image resolution. Moreover, like in FCNs, skip connections are used to connect encoding blocks to their decoding counterparts. The decoding blocks are thus provided with both the contextual information from the preceding block and the location information from the encoding path.</p>
<p>U-Net also differs from FCN in two main ways. Unlike FCN-8s, U-Net is <strong>symmetrical</strong>, going back to the traditional U-shaped encoder-decoder structure (hence the name). Furthermore, the merging with the feature maps from the skip connection is done through <strong>concatenation</strong> (along the channel axis) instead of addition. The U-Net architecture is depicted in <em>Figure 6-9</em>. As for the FCN, a Jupyter Notebook is dedicated to its implementation from scratch:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ca653a8b-5c09-41be-972e-3245084e067d.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-9: U-Net architecture</div>
<p>Note also that while the original decoding blocks have transposed convolutions with <em>s = 2</em> for upsampling, it is common to find implementations using nearest-neighbor scaling instead (refer to the discussion in the previous subsection). Given its popularity, U-Net has known many variations and still inspires numerous architectures (for example, replacing its blocks with residual ones, and densifying the intra-block and extra-block connectivity).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intermediary example – image super-resolution</h1>
                </header>
            
            <article>
                
<p>Let's briefly apply one of these models to a new problem – image super-resolution (complete implementation and additional tips are found in the related notebook).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">FCN implementation</h1>
                </header>
            
            <article>
                
<p class="mce-root">Remembering the architecture we just presented, a simplified version of FCN-8s can be implemented as follows (note that the real model has additional convolutions before each transposed one):</p>
<pre>inputs = Input(<span>shape</span>=(<span>224</span><span>, </span><span>224</span><span>, </span><span>3</span>))<span><br/></span><span># Building a pretrained VGG-16 feature extractor as encoder:<br/></span>vgg16 = VGG16(<span>include_top</span>=<span>False, </span><span>weights</span>=<span>'imagenet'</span><span>, </span><span>input_tensor</span>=inputs)<br/><span># We recover the feature maps returned by each of the 3 final blocks:<br/></span>f3 = vgg16.get_layer(<span>'block3_pool'</span>).output <span># shape: (28, 28, 256)<br/></span>f4 = vgg16.get_layer(<span>'block4_pool'</span>).output <span># shape: (14, 14, 512)<br/></span>f5 = vgg16.get_layer(<span>'block5_pool'</span>).output <span># shape: ( 7, 7, 512)<br/># We replace the VGG dense layers by convs, adding the "decoding" layers instead after the conv/pooling blocks:<br/></span><span>f3 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f3)<br/>f4 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f4)<br/>f5 = Conv2D(filters=out_ch, kernel_size=1, padding='same')(f5)</span><span><br/># We upscale `f5` to a 14x14 map </span><span>so it can be merged with `f4`:<br/></span>f5x2 = Conv2DTranspose(<span>filters</span>=<span>out_chh</span><span>, </span><span>kernel_size</span>=<span>4</span><span>,</span><span>strides=2, <br/>                       </span><span>padding</span>=<span>'same'</span><span>, </span><span>activation</span>=<span>'relu'</span>)(f5)<br/><span># We merge the 2 feature maps with an element-wise addition:<br/></span>m1 = add([f4<span>, </span>f5x2])<br/><span># We repeat the operation to merge `m1` and `f3` into a 28x28 map:<br/></span>m1x2 = Conv2DTranspose(<span>filters</span>=<span>out_ch</span><span>, </span><span>kernel_size</span>=<span>4</span><span>, </span><span>strides</span>=<span>2</span><span>,<br/>                       </span><span>padding</span>=<span>'same'</span><span>, </span><span>activation</span>=<span>'relu'</span>)(m1)<br/>m2 = add([f3<span>, </span>m1x2])<br/><span># Finally, we use a transp-conv to recover t</span><span>he original shape:<br/></span>outputs = Conv2DTranspose(<span>filters</span>=out_ch<span>, </span><span>kernel_size</span>=<span>16</span><span>, </span><span>strides</span>=<span>8</span><span>,<br/></span><span>                          </span><span>padding</span>=<span>'same'</span><span>, </span><span>activation</span>=<span>'sigmoid'</span>)(m2)<br/>fcn_8s = Model(inputs<span>, </span>outputs)</pre>
<p>Reusing the Keras implementation of VGG and the Functional API, an FCN-8s model can be created with minimal effort.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application to upscaling images</h1>
                </header>
            
            <article>
                
<p>A simple trick to train a network for super-resolution is to use a traditional upscaling method (such as bilinear interpolation) to scale the images to the target dimensions, before feeding them to the model. This way, the network can be trained as a denoising AE, whose task is to clear the upsampling artifacts and to recover lost details:</p>
<pre>x_noisy = bilinear_upscale(bilinear_downscale(x_train)) # pseudo-code<br/>fcn_8s.fit(x_noisy, x_train)</pre>
<div class="mce-root packt_infobox">Proper code and complete demonstration on images can be found in the notebooks.</div>
<p>As mentioned earlier, the architectures we just covered are commonly applied to a wide range of tasks, such as depth estimation from color images, next-frame prediction (that is, predicting what the content of the next image could be, taking for input a series of video frames), and image segmentation. In the second part of this chapter, we will develop the latter task, which is essential in many real-life applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding semantic segmentation</h1>
                </header>
            
            <article>
                
<p><strong>Semantic segmentation</strong> is a more generic term for the task of segmenting images into meaningful parts. It covers both object segmentation and instance segmentation, which were introduced in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>. Unlike image classification and object detection, covered in the previous chapters, segmentation tasks require the methods to return pixel-level dense predictions, that is, to assign a label to each pixel in the input images.</p>
<p>After explaining in more detail why encoders-decoders are thus great at object segmentation, and how their results can be further refined, we will present some solutions for the more complicated task of instance segmentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object segmentation with encoders-decoders</h1>
                </header>
            
            <article>
                
<p>As we saw in the first part of this chapter, encoding-decoding networks are trained to map data samples from one domain to another (for example, from noisy to noiseless, or from color to depth). Object segmentation can be seen as one such operation – the mapping of images from the color domain to the class domain. Given its value and context, we want to assign one of the target classes to each pixel of a picture, returning a <strong>label map</strong> with the same height and width.</p>
<p>Teaching encoders-decoders to take an image and return a label map still requires some consideration, which we will now discuss.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview</h1>
                </header>
            
            <article>
                
<p>In the following paragraphs, we will present how networks such as U-Net are used for object segmentation, and how their outputs can be further processed into refined label maps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decoding as label maps</h1>
                </header>
            
            <article>
                
<p>Building encoders-decoders to directly output label maps—where each pixel value represents a class (for instance, <kbd>1</kbd> for <em>dog</em>, and <kbd>2</kbd> for <em>cat</em>)—would yield poor results. As with classifiers, we need a better way to output categorical values.</p>
<p>To classify images among <em>N</em> categories, we learned to build networks with the final layers outputting <em>N</em> logits, representing the predicted per-class scores. We also learned how to convert these scores into probabilities using the <strong>softmax</strong><em> </em>operation, and how to return the most probable class(es) by picking the highest values (for instance, using <strong>argmax</strong>). The same mechanism can be applied to semantic segmentation, at the pixel level instead of the image level. Instead of outputting a column vector of <em>N</em> logits containing the per-class scores for each full image, our network is built to return an <em>H</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <em>W</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <em>N</em> tensor with scores for each pixel (refer to <em>Figure 6-10</em>):</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/041ed823-c5b3-47b2-850f-645a0916f564.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-10: Given an input image of dimensions <em>H</em> × <em>W</em>, the network returns an <em>H</em> × <em>W</em> × <em>N</em> probability map, with <em>N</em> being the number of classes. Using argmax, the predicted label map can then be obtained</div>
<p>For the architectures we presented in this chapter, obtaining such an output tensor is simply a matter of setting <em>D</em><em><sub>o</sub></em> = <em>N</em>, that is, setting the number of output channels equal to the number of classes when building the models (refer to <em>Figures 6-8</em> and <em>6-9</em>). They can then be trained as classifiers. The <strong>cross-entropy loss</strong> is used to compare the softmax values with the one-hot-encoded ground truth label maps (the fact that the compared tensors have more dimensions for classification <span>that </span>do not impact the calculations). Also, the <em>H</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <em>W</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span></span> <em>N</em> predictions can be similarly transformed into per-pixel labels by selecting the indices of the highest values along the channel axis (that is, <kbd>argmax</kbd> over the channel axis). For instance, the FCN-8s code presented earlier can be adapted to train a model for object segmentation, as follows:</p>
<pre>inputs = Input(<span>shape</span>=(<span>224</span><span>, </span><span>224</span><span>, </span><span>3</span>))<br/>out_ch = <span>num_classes = 19 # e.g., for object segmentation over Cityscapes</span><span><br/></span><span># [...] building e.g. a FCN-8s architecture, c.f. previous snippet.<br/></span>outputs = Conv2DTranspose(<span>filters</span>=out_ch<span>, </span><span>kernel_size</span>=<span>16</span><span>, </span><span>strides</span>=<span>8</span><span>,<br/></span><span>                          </span><span>padding</span>=<span>'same'</span><span>, </span><span>activation</span>=<span>None</span>)(m2)<br/>seg_fcn = Model(inputs<span>, </span>outputs)<br/>seg_fcn.compile(<span>optimizer</span>='adam'<span>, </span><span>loss</span>=<span>'sparse_categorical_crossentropy'</span>)<br/># [...] training the network. Then we use it to predict label maps:<br/>label_map = np.argmax(seg_fcn.predict(image), axis=-1)</pre>
<p>The Git repository contains a complete example of an FCN-8s model built and trained for semantic segmentation, as well as a U-Net model.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training with segmentation losses and metrics</h1>
                </header>
            
            <article>
                
<p>The use of state-of-the-art architectures, such as FCN-8s and U-Net, is key to building performant systems for semantic segmentation. However, the most advanced models still need a proper loss to converge optimally. While cross-entropy is the default loss to train models both for coarse and dense classification, precautions should be taken for the latter cases.</p>
<p>For image-level and pixel-level classification tasks, <strong>class imbalance</strong> is a common problem. Imagine training models over a dataset of 990 cat<em> </em>pictures and 10 dog<em> </em>pictures. A model that would learn to always output cat would achieve 99% training accuracy, but would not be really useful in practice. For image classification, this can be avoided by adding or removing pictures so that all classes appear in the same proportions. The problem is trickier for pixel-level classification. Some classes may appear in every image but span only a handful of pixels, while other classes may cover most of the images (such as <em>traffic sign</em> versus <em>road </em>classes for our self-driving car application). The dataset cannot be edited to compensate for such an imbalance.</p>
<p>To prevent the segmentation models from developing a bias toward larger classes, their loss functions should instead be adapted. For instance, it is common practice to weigh the contribution of each class to the cross-entropy loss. As presented in our notebook on semantic segmentation for self-driving cars and in <em>Figure 6-11</em>, the less a class appears in training images, the more it should weigh on the loss. This way, the network would be heavily penalized if it starts ignoring smaller classes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/578eba97-9363-4de3-b9d0-5a32c60dd13d.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 6-11: Examples of pixel weighing strategies for semantic segmentation (the lighter the pixels, the greater their weight on the loss)</div>
<p><span>The weight maps are usually computed from the ground truth label maps. It should be noted that, as shown in <em>Figure 6-11</em>, the weight applied to each pixel can be set not only according to the class, but also according to the pixel's position relative to other elements, and more.</span></p>
<p>Another solution is to replace the cross-entropy with another cost function that's not affected by the class proportions. After all, cross-entropy is a surrogate accuracy function, adopted because it is nicely differentiable. However, this function does not really express the actual objective of our models—to properly segment the different classes, whatever their areas. Therefore, several loss functions and metrics that are specific to semantic segmentation have been proposed by researchers to more explicitly capture this objective.</p>
<p><strong>Intersection-over-Union</strong> (<strong>IoU</strong>), presented in <a href="593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml">Chapter 5</a>, <em>Object Detection Models</em>, is one of these common metrics. The <strong>Sørensen–Dice coefficient</strong> (often simply named the <strong>Dice coefficient</strong>) is another. Like IoU, it measures how well two sets overlap:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b7d499fd-2d60-4663-9f4e-a6af1a01abd8.png" style="width:9.25em;height:2.33em;"/></p>
<p>Here, <em>|</em><em>A|</em> and <em>|B|</em> represent the cardinality of each set (refer to the explanations in the previous chapter), and <img class="fm-editor-equation" src="assets/72c18afa-cb39-4f19-b2ee-9b7f0743feca.png" style="width:3.08em;height:1.17em;"/> represents the number of elements they have in common (cardinality of their intersection). IoU and Dice share several properties, and one can actually help calculate the other:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6c5e9c55-7c4d-4de2-980e-e15be36d1799.png" style="width:27.33em;height:2.50em;"/></p>
<p>In semantic segmentation, <em>Dice</em> is, therefore, used to measure how well the predicted mask for each class overlaps the ground truth mask. For one class, the numerator then represents the number of correctly classified pixels, and the denominator represents the total number of pixels belonging to this class in both the predicted and ground truth masks. As a metric, the <em>Dice</em> coefficient thus does not depend on the relative number of pixels one class takes in images. For multi-class tasks, scientists usually compute the <em>Dice</em> coefficient for each class (comparing each pair of predicted and ground truth masks), and then average the results.</p>
<p>From the equation, we can see the <em>Dice</em> coefficient is defined between 0 and 1—its value reaches 0 if <em>A</em> and <em>B</em> do not overlap at all, and it reaches 1 if they <span>do </span>perfectly. Therefore, to use it as a loss function that a network should minimize, we need to reverse this scoring. All in all, for semantic segmentation applied to <em>N</em> classes, the <em>Dice</em> loss is commonly defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3920f224-4661-4428-9ce8-ec9610b4bbe3.png" style="width:34.58em;height:2.92em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's clarify this equation a bit. If <em>a</em> and <em>b</em> are two one-hot tensors, then the <em>Dice</em> numerator (that is, their intersection) can be approximated by applying the element-wise multiplication between them (refer to <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>), then by summing together all the values in the resulting tensor. The denominator is obtained by summing all the elements, <em>a</em> and <em>b</em>. Finally, a small value, <img class="fm-editor-equation" src="assets/11068ff7-140a-42e3-a3f0-c49ed876d062.png" style="width:0.58em;height:0.83em;"/> (for instance, below <em>1e-6</em>), is usually added to the denominator to avoid dividing by zero if the tensors contain nothing, and added to the numerator to smooth the result.</p>
<div class="packt_infobox">Note that, in practice, unlike the ground truth one-hot tensors, the predictions do not contain binary values. They are composed of the softmax probabilities ranging continuously from 0 to 1. This loss is therefore often named <strong>soft Dice</strong>.</div>
<p>In TensorFlow, this loss can be implemented as follows:</p>
<pre>def dice_loss(labels, logits, num_classes, eps=1e-6, spatial_axes=[1, 2]):<br/>    # Transform logits in probabilities, and one-hot the ground truth:<br/>    pred_proba = tf.nn.softmax(logits, axis=-1)<br/>    gt_onehot  = tf.one_hot(labels, num_classes, dtype=tf.float32)<br/>    # Compute Dice numerator and denominator:<br/>    num_perclass = 2 * tf.reduce_sum(pred_proba * gt_onehot, axis=spatial_axes)<br/>    den_perclass = tf.reduce_sum(pred_proba + gt_onehot, axis=spatial_axes)<br/>    # Compute Dice and average over batch and classes:<br/>    dice = tf.reduce_mean((num_perclass + eps) / (den_perclass + eps))<br/>    return 1 - dice</pre>
<p>Both <em>Dice</em> and <em>IoU</em> are important tools for segmentation tasks, and their usefulness is further demonstrated in the related Jupyter notebook.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Post-processing with conditional random fields</h1>
                </header>
            
            <article>
                
<p class="mce-root">Labeling every pixel properly is a complex task, and it is common to obtain predicted label maps with poor contours and small incorrect areas. Thankfully, there are some methods that post-process the results, correcting some obvious defects. Among these methods, the <strong>conditional random fields</strong> (<strong>CRFs</strong>) methods are the most popular because of their overall efficiency.</p>
<p>The theory behind this is beyond the scope of this book, but CRFs are able to improve pixel-level predictions by taking into account the context of each pixel back in the original image. If the color gradient between two neighboring pixels is small (that is, no abrupt change of color), chances are that they belong to the same class. Taking into account this spatial and color-based model, as well as the probability maps provided by the predictors (in our case, the softmax tensors from CNNs), CRF methods return refined label maps, which are better with respect to visual contours.</p>
<p>Several ready-to-use implementations are available, such as <kbd>pydensecrf</kbd> by Lucas Beyer (<a href="https://github.com/lucasb-eyer/pydensecrf">https://github.com/lucasb-eyer/pydensecrf</a>), a Python wrapper for dense CRFs with Gaussian edge potentials proposed by Philipp Krähenbühl and Vladlen Koltun (refer to <em>Efficient inference in fully connected CRFs with gaussian edge potentials</em>, <em>Advances in neural information processing systems</em>, <em>2011</em>). In the last notebook for this chapter, we explain how to use this framework.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced example – image segmentation for self-driving cars</h1>
                </header>
            
            <article>
                
<p class="mce-root">As suggested at the beginning of this chapter, we will apply this new knowledge to a complex real-life use case—the segmentation of traffic images for self-driving cars.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Task presentation</h1>
                </header>
            
            <article>
                
<p class="mce-root">Like human drivers, self-driving cars need to understand their environment and be aware of the elements around them. Applying semantic segmentation to the video images from a front camera would allow the system to know whether other cars are around, to know whether pedestrians or bikes are crossing the road, to follow traffic lines and signs, and more. </p>
<p class="mce-root">This is, therefore, a critical process, and researchers are putting in lots of effort into refining the models. For that reason, multiple related datasets and benchmarks are available. The <em>Cityscapes</em> dataset (<a href="https://www.cityscapes-dataset.com">https://www.cityscapes-dataset.com</a>) we chose for our demonstration is one of the most famous. Shared by Marius Cordts et al. (refer to <em>The Cityscapes Dataset for Semantic Urban Scene Understanding</em>, <em>Proceedings of the IEEE CVPR Conference</em>), it contains video sequences from multiple cities, with semantic labels for more than 19 classes <em>(</em>road, car, plant, and so on). A notebook is specifically dedicated to getting started with this benchmark.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exemplary solution</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the two final Jupyter notebooks for this chapter, FCN and U-Net models are trained to tackle this task, using several of the tricks presented in this section. We demonstrate how to properly weigh each class when computing the loss, we present how to post-process the label maps, and more besides.</p>
<p class="mce-root">As the whole solution is quite long and notebooks are better suited to the present code, we invite you to pursue the reading there, if you're interested in this use case. This way, we can dedicate the rest of this chapter to another fascinating problem—instance segmentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The more difficult case of instance segmentation</h1>
                </header>
            
            <article>
                
<p class="mce-root">With models trained for object segmentation, the <em>softmax</em> output represents for each pixel the probability that it belongs to one of <em>N</em> classes. However, it does not express whether two pixels or blobs of pixels belong to the same instance of a class. For example, given the predicted label map shown in <em>Figure 6-10</em>, we have no way of counting the number of <em>tree</em> or <em>building</em> instances.</p>
<p>In the following subsection, we will present two different ways of achieving instance segmentation by extending solutions for two related tasks that we've tackled already—object segmentation and object detection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From object segmentation to instance segmentation</h1>
                </header>
            
            <article>
                
<p class="mce-root">First, we will present some tools that we can use to obtain instance masks from the segmentation models we just covered. The U-Net authors popularized the idea of tuning encoders-decoders so that their output can be used for instance segmentation. This idea was pushed further by Alexander Buslaev, Victor Durnov, and Selim Seferbekov, who famously won Kaggle's 2018 Data Science Bowl (<a href="https://www.kaggle.com/c/data-science-bowl-2018">https://www.kaggle.com/c/data-science-bowl-2018</a>), a sponsored competition to advance instance segmentation for medical applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Respecting boundaries</h1>
                </header>
            
            <article>
                
<p class="mce-root">If elements captured by a semantic mask are well-separated/non-overlapping, splitting the masks to distinguish each instance is not too complicated a task. Plenty of algorithms are available to estimate the contours of distinct blobs in binary matrices and/or to provide a separate mask for each blob. For multi-class instance segmentation, this process can just be repeated for each class mask returned by object segmentation methods, splitting them further into instances.</p>
<p class="mce-root"/>
<p>But precise semantic masks should first be obtained, or elements too close to each other may be returned as a single blob. So, how can we ensure that segmentation models put enough attention into generating masks with precise contours, at least for non-overlapping elements? We know the answer already—the only way to teach networks to do something specific is to adapt their training loss accordingly.</p>
<p>U-Net was developed for biomedical applications, to segment neuronal structures in microscope images. To teach their network to separate nearby cells properly, the authors decided to weight their loss function to more heavily penalize misclassified pixels at the boundaries of several instances. Also illustrated in <em>Figure 6-11</em>, this strategy is quite similar to the per-class loss weighting we presented in the previous subsection, although here, the weighting is specifically computed for each pixel. The U-Net authors present a formula to compute these weight maps based on the ground truth class mask. For each pixel and for each class, this formula takes into account the pixel's distance to the two nearest class instances. The smaller the two distances, the higher the weight. The weight maps can be precomputed and stored along the ground truth masks to be used together during training.</p>
<p>Note that this per-pixel weighting can be combined with the per-class weighting in multi-class scenarios. The idea to penalize the networks more heavily for certain regions of the images can also be adapted to other applications (for example, to better segment critical parts of manufactured objects).</p>
<p>We mentioned the winners of Kaggle's 2018 Data Science Bowl, who put a noteworthy spin on this idea. For each class, their custom U-Net was outputting two masks: the usual mask predicting the per-pixel class probability, and a second mask capturing the class boundaries. The ground truth boundary masks were precomputed based on the class masks. After proper training, the information from the two predicted masks can be used to obtain well-separated elements for each class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Post-processing into instance masks</h1>
                </header>
            
            <article>
                
<p class="mce-root">As discussed earlier in the previous section, once precise masks are obtained, non-overlapping instances can be identified from them by applying proper algorithms. This post-processing is usually done using <strong>morphological functions</strong>, such as <strong>mask</strong> <strong>erosion</strong> and <strong>dilation</strong>.</p>
<p class="mce-root"/>
<p><strong>Watershed transforms</strong> are another common family of algorithms that further segment the class masks into instances. These algorithms take a one-channel tensor and consider it as a topographic surface, where each value represents an elevation. Using various methods that we won't go into, they then extract the ridges' tops, representing the instance boundaries. Several implementations of these transforms are available, some of which are CNN-based, such as the <em>Deep watershed transform for instance segmentation</em> (<em>Proceedings of the IEEE CVPR conference</em>, <em>2017</em>), by Min Bai and Raquel Urtasun from the University of Toronto. Inspired by the FCN architecture, their network takes for input both the predicted semantic mask and the original RGB image, and returns an energy map that can be used to identify the ridges. Thanks to the RGB information, this solution can even separate overlapping instances with good accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From object detection to instance segmentation – Mask R-CNN</h1>
                </header>
            
            <article>
                
<p>A second way of addressing instance segmentation is from the angle of object detection. In <a href="593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml">Chapter 5</a>, <em>Object Detection Models</em>, we presented solutions to return the bounding boxes for object instances appearing in images. In the following paragraphs, we will demonstrate how these results can be turned into more refined instance masks. More precisely, we will present <strong>Mask R-CNN</strong>, a network extending <strong>Faster R-CNN</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying semantic segmentation to bounding boxes</h1>
                </header>
            
            <article>
                
<p>When we introduced object detection in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, we explained that this process is often used as a preliminary step, providing image patches containing a single instance for further analysis. With this in mind, instance segmentation becomes a matter of two steps:</p>
<ol>
<li>Using an object detection model to return bounding boxes for each instance of target classes</li>
<li>Feeding each patch to a semantic segmentation model to obtain the instance mask</li>
</ol>
<p>If the predicted bounding boxes are accurate (each capturing a whole, single element), then the task of the segmentation network is straightforward—to classify which pixels in the corresponding patch belong to the captured class, and which pixels are part of the background/belong to another class.</p>
<p>This way of solving instance segmentation is advantageous, as we already have all the necessary tools to implement it (object detection and semantic segmentation models)!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an instance segmentation model with Faster-RCNN</h1>
                </header>
            
            <article>
                
<p>While we could simply use a pretrained detection network followed by a pretrained segmentation network, the whole pipeline would certainly work better if the two networks were stitched together and trained in an end-to-end manner. Backpropagating the segmentation loss through the common layers would better ensure that the features extracted are meaningful both for the detection and the segmentation tasks. This is pretty much the original idea behind <em>Mask R-CNN</em> by Kaiming He et al. from <strong>Facebook AI Research</strong> (<strong>FAIR</strong>) in 2017 (<em>Mask R-CNN</em>, <em>Proceedings of the IEEE CVPR conference</em>).</p>
<div class="packt_tip">If the name rings a bell, Kaiming He was also among the main authors of ResNet and Faster R-CNN.</div>
<p>Mask R-CNN is mostly based on Faster R-CNN. Like Faster R-CNN, Mask R-CNN is composed of a region-proposal network, followed by two branches predicting the class and the box offset for each proposed region (refer to <a href="593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml">Chapter 5</a>, <em>Object Detection Models</em>). However, the authors extended this model with a <em>third parallel branch</em>, outputting a binary mask for the element in each region (as shown in <em>Figure 6-12</em>). Note that this additional branch is only composed of a couple of standard and transposed convolutions. As the authors highlighted in their paper, this parallel processing follows the spirit of Faster R-CNN, and contrasts with other instance segmentation methods, which are usually sequential:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f5c8d229-30e0-4f17-8de2-bb52d744aefc.png" style="width:41.25em;height:21.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6-12: Mask R-CNN architecture, based on Faster R-CNN</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Thanks to this parallelism, He et al. could decouple the classification and segmentation. While the segmentation branch is defined to output <em>N</em> binary masks (one for each class, like any usual semantic segmentation model), only the mask corresponding to the class predicted by the other branch will be considered for the final prediction and for the training loss. In other words, only the mask of the instance class contributes to the cross-entropy loss applied to the segmentation branch. As <span><span>explained </span></span>by the authors, this lets the segmentation branch predict label maps without competition among the classes, thereby simplifying its task.</p>
<div class="packt_infobox">Another famous contribution of the Mask R-CNN authors is the <strong>RoI</strong> <strong>a</strong><strong>lign</strong> <strong>layer</strong>, replacing the <strong>RoI pooling</strong> of Faster R-CNN. The difference between the two is actually quite subtle, but provides a non-negligible accuracy boost. RoI pooling causes quantization, for instance, by discretizing the coordinates of the subwindow cells (refer to <a href="593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml">Chapter 5</a>, <em>Object Detection Models,</em> and <em>Figure 5-13</em>). While this does not really impact the predictions of the classification branch (it's robust to such small misalignments), this would affect the quality of the pixel-level prediction of the segmentation branch. To avoid this, He et al. simply <em>removed the discretization</em> and <em>used bilinear interpolation</em> instead to obtain the cells' content.</div>
<p>Mask R-CNN distinguished itself at the COCO 2017 challenges, and is widely used nowadays. Multiple implementations can be found online, for instance, in the folder of the <kbd>tensorflow/models</kbd> repository dedicated to object detection and instance segmentation (<a href="https://github.com/tensorflow/models/tree/master/research/object_detection">https://github.com/tensorflow/models/tree/master/research/object_detection</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered several paradigms for pixel-precise applications. We introduced encoders-decoders and some specific architectures and applied them to multiple tasks from image denoising to semantic segmentation. We also demonstrated how different solutions can be combined to tackle more advanced problems, such as instance segmentation.</p>
<p>As we tackle more and more complex tasks, new challenges arise. For example, in semantic segmentation, precisely annotating images to train models is a time-consuming task. Available datasets are thus usually scarce, and specific measures should be taken to avoid overfitting. Furthermore, because the training images and their ground truths are heavier, well-engineered data pipelines are needed for efficient training.</p>
<p>In the following chapter, we will, therefore, provide in-depth details of how TensorFlow can be used to effectively augment and serve training batches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the particularity of AEs?</li>
<li>Which classification architecture are FCNs based on?</li>
<li>How can a semantic segmentation model be trained so that it does not ignore small classes?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p><span><em>Mask R-CNN</em> (</span><a href="http://openaccess.thecvf.com/content_iccv_2017/html/He_Mask_R-CNN_ICCV_2017_paper.html">http://openaccess.thecvf.com/content_iccv_2017/html/He_Mask_R-CNN_ICCV_2017_paper.html</a><span>)</span> by Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick: This nicely written conference paper mentioned in the chapter presents Mask R-CNN, providing additional illustrations and details that may help you to understand this model.</p>


            </article>

            
        </section>
    </body></html>