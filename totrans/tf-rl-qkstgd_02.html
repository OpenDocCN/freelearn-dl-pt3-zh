<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Temporal Difference, SARSA, and Q-Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we looked at the basics of RL. In this chapter, we will cover <strong>temporal difference</strong> (<strong>TD</strong>) learning, SARSA, and Q-learning, which were very widely used algorithms in RL before deep RL became more common. Understanding these older-generation algorithms is essential if you want to master the field, and will also lay the foundation for delving into deep RL. We will therefore spend this chapter looking at examples using these older generation algorithms. In addition, we will also code some of these algorithms using Python. We will not be using TensorFlow for this chapter, as the problems do not involve any deep neural networks <span>under study</span>. However, this chapter will lay the groundwork for more advanced topics that we will cover in the subsequent chapters, and will also be our first coding experience of an RL algorithm. Specifically, this chapter will be our first deep dive into a standard RL algorithm, and how you can use it to train RL agents for a specific task. It will also be our first hands-on effort at RL, including both theory and practice. </p>
<p>Some of the topics that will be covered in this chapter are as follows:</p>
<ul>
<li>Understanding TD learning</li>
<li>Learning SARSA</li>
<li>Understanding Q-learning</li>
<li>Cliff walking with SARSA and Q-learning</li>
<li>Grid world with SARSA</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Knowledge of the following will help you to better understand the concepts presented in this chapter:</p>
<ul>
<li>Python (version 2 or 3)</li>
<li>NumPy </li>
<li>TensorFlow (version 1.4 or higher)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding TD learning</h1>
                </header>
            
            <article>
                
<p>We will first learn about TD learning. This is a very fundamental concept in RL. In TD learning, the learning of the agent is attained by experience. Several trial episodes are undertaken of the environment, and the rewards accrued are used to update the value functions. Specifically, the agent will keep an update of the state-action value functions as it experiences new states/actions. The Bellman equation is used to update this state-action value function, and the goal is to minimize the TD error. This essentially means the agent is reducing its uncertainty of which action is the optimal action in a given state; it gains confidence on the optimal action in a given state by lowering the TD error. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Relation between the value functions and state</h1>
                </header>
            
            <article>
                
<p>The value function is an agent's estimate of how good a given state is. For instance, if a robot is near the edge of a cliff and may fall, that state is bad and must have a low value. On the other hand, if the robot/agent is near its final goal, that state is a good state to be in, as the rewards they will soon receive are high, and so that state will have a higher value.</p>
<p>The value function, <em>V</em>, is updated after reaching a <em>s<sub>t</sub><span> </span></em><span>state </span>and receiving a <em>r<sub>t</sub></em> <span>reward </span>from the environment. The simplest TD learning algorithm is called <em>TD(0)</em> and performs an update using the following equation <span>where </span><em>α</em><span> is the learning rate and </span><em>0 ≤ α ≤ 1</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5af87999-0b2c-43b2-bb23-aaff79a6b0e9.png" style="width:20.00em;height:1.25em;"/></p>
<p>Note that in some reference papers or books, the preceding formula will have <em>r<sub><span>t</span></sub></em> instead of <em>r<sub>t+1</sub></em>. This is just a difference in convention and is not an error; <em>r<sub>t+1</sub></em><span> </span>here denotes the reward received from <em>s<sub>t</sub></em><span> state<strong> </strong></span>and transitioning to <em>s<sub>t+1</sub></em>. </p>
<p>There is also another TD learning variant called <em>TD(λ)</em> that used eligibility traces <em>e(s),</em> which are a record of visiting a state. More formally, we perform a <em>TD(λ)</em> update as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/79783f9a-d161-4dae-9041-68ded414b74a.png" style="width:20.67em;height:1.17em;"/></p>
<p>The eligibility traces are given by the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dcb459a2-8c15-4304-8038-4473ebaef015.png" style="width:14.50em;height:2.67em;"/></p>
<p>Here, <em>e(s) = 0</em> at <em>t = 0</em>. For each step the agent takes, the eligibility trace decreases by <em>γλ</em> for all states, and is incremented by <em>1</em> for the state visited in the current time step. Here, <em>0 ≤ λ ≤ 1</em>, and it is a parameter that decides how much of the credit from a reward is to be assigned to distant states. Next, we will look at the theory behind our next two RL algorithms, SARSA and Q-learning, both of which are quite popular in the RL community.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding SARSA and Q-Learning </h1>
                </header>
            
            <article>
                
<p>In this section, we will learn about SARSA and Q-Learning and how can they are coded with Python. Before we go further, let's find out what SARSA and Q-Learning are. SARSA is an algorithm that uses the state-action Q values to update. These concepts are derived from the computer science field of dynamic programming, while <span>Q-learning is an off-policy algorithm that was first proposed by Christopher Watkins in 1989, and is a widely used RL algorithm. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning SARSA </h1>
                </header>
            
            <article>
                
<p><strong>SARSA</strong> is another on-policy algorithm that was very popular, particularly in the 1990s. It is an extension of TD-learning, which we saw previously, and is an on-policy algorithm. SARSA keeps an update of the state-action value function, and as new experiences are encountered, this state-action value function is updated using the Bellman equation of dynamic programming. We extend the preceding<span> </span>TD algorithm to state-action value function, <em>Q(s<sub>t</sub>,a<sub>t</sub>)</em>, and this approach is called SARSA: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/54c7d075-6c67-403c-8d81-bb5c4e19cb39.png" style="width:24.92em;height:1.17em;"/></p>
<p class="mce-root">Here, from a given state <em>s<sub>t</sub></em>, we take action <em>a<sub>t</sub></em>, receive a reward <em>r<sub>t+1</sub></em>, transition to a new state <em>s<sub>t+1</sub></em>, and thereafter take an action <em>a<sub>t+1</sub></em> that then continues on and on. This quintuple (<em>s<sub>t</sub></em>, <em>a<sub>t</sub></em>, <em>r<sub>t+1</sub></em>, <em>s<sub>t+1</sub></em>, <em>a<sub>t+1</sub></em>) gives the algorithm the name SARSA. SARSA is an on-policy algorithm, as the same policy is updated as was used to estimate Q. For exploration, you can use, say, <em>ε</em>-greedy. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding Q-learning</h1>
                </header>
            
            <article>
                
<p>Q-learning is an off-policy algorithm that was first proposed by Christopher Watkins in<span> </span>1989,<span> </span>and is a widely used RL algorithm. Q-learning, such as SARSA, keeps an update of the state-action value function for each state-action pair, and recursively updates it using the Bellman equation of dynamic programming as new experiences are collected. Note that it is an off-policy algorithm as it uses the state-action value function evaluated at the action, which will maximize the value. Q-learning is used for problems where the actions are discrete – for example, if we have the actions move north, move south, move east, move west, and we are to decide the optimum action in a given state, then Q-learning is applicable in such settings.</p>
<p>In the classical Q-learning approach, the update is given as follows, <span>where the max is performed over actions, that is, we choose the action</span><span> </span><span>a corresponding</span><span> </span><span>to the maximum value of Q at state </span><em>s<sub>t+1</sub></em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3536e18c-a044-4baa-a15b-c7703069b46a.png" style="width:32.67em;height:1.42em;"/></p>
<p>The <em>α</em> is the learning rate, which is a hyper-parameter that the user can specify.</p>
<p>Before we code the algorithms in Python, let's find out what kind of problems will be considered.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cliff walking and grid world problems</h1>
                </header>
            
            <article>
                
<p>Let's consider cliff walking and grid world problems. First, we will introduce these problems to you, then we will proceed on to the coding part. For both problems, we consider a rectangular grid with <kbd>nrows</kbd> (number of rows) and <kbd>ncols</kbd> (number of columns). We start from one cell to the south of the bottom left cell, and the goal is to reach the destination, which is one cell to the south of the bottom right cell.</p>
<p>Note that the start and destination cells are not part of the <kbd>nrows</kbd> x <kbd>ncols</kbd> grid of cells. For the cliff walking problem, the cells to the south of the bottom row of cells, except for the start and destination cells, form a cliff where, if the agent enters, the episode ends with catastrophic fall into the cliff. Likewise, if the agent tries to leave the left, top, or right boundaries of the grid of cells, it is placed back in the same cell, that is, it is equivalent to taking no action.</p>
<p>For the grid world problem, we do not have a cliff, but we have obstacles inside the grid world. If the agent tries to enter any of these obstacle cells, it is bounced back to the same cell from which it came. In both these problems, the goal is to find the optimum path from the start to the destination.</p>
<p>So, let's dive on in!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cliff walking with SARSA</h1>
                </header>
            
            <article>
                
<p>We will now learn how to code the aforementioned equations in Python and implement the cliff walking problem with SARSA. First, let's import the <kbd>numpy</kbd>, <kbd>sys</kbd>, and <kbd>matplotlib</kbd> packages in Python. If you have not used these packages in the past, there are several Packt books on these topics to help you come up to speed. Type the following command to install the required packages in a Linux Terminal:</p>
<pre><strong><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">python</span><span class="o">-</span><span class="n">numpy</span> <span class="n">python</span><span class="o">-</span><span class="n">scipy</span> <span class="n">python</span><span class="o">-</span><span class="n">matplotlib</span></strong></pre>
<p>We will now summarize the code involved to solve the grid world problem. In a Terminal, use your favorite editor (for example, gedit, emacs, or vi) to code the following:</p>
<pre class="mce-root">import numpy as np <br/>import sys <br/>import matplotlib.pyplot as plt</pre>
<p class="mce-root">We will use a 3 x 12 grid for the cliff walking problem, that is, <kbd>3</kbd> rows and <kbd>12</kbd> columns. We also have <kbd>4</kbd> actions to take at any cell. You can go north, east, south, or west:</p>
<pre class="mce-root">nrows = 3<br/>ncols = 12<br/>nact = 4 </pre>
<p>We will consider <kbd>100000</kbd> episodes in total. For exploration, we will use ε-greedy with a value of <em>ε</em> = <kbd>0.1.</kbd> We will consider a constant value for <em>ε</em>, although the interested reader is encouraged to consider variable values for <em>ε</em> as well with its value slowly annealed to zero over the course of the episodes.</p>
<p>The learning rate, <em>α</em>, is chosen as <kbd>0.1</kbd>, and the discount factor <em>γ</em> = <kbd>0.95</kbd> is used, which are typical values for this problem:</p>
<pre class="mce-root">nepisodes = 100000<br/>epsilon = 0.1<br/>alpha = 0.1<br/>gamma = 0.95</pre>
<p class="mce-root">We will next assign values for the rewards. For any normal action that does not fall into the cliff, the reward is <kbd>-1</kbd>; if the agent falls down the cliff, the reward is <kbd>-100</kbd>; for reaching the destination, the reward is also <kbd>-1</kbd>. Feel free to explore other values for these rewards later, and investigate its effect on the final Q values and path taken from start to destination:</p>
<pre class="mce-root">reward_normal = -1<br/>reward_cliff = -100<br/>reward_destination = -1</pre>
<p class="mce-root">The Q values for state-action pairs are initialized to zero. We will use a NumPy array for Q, which is <kbd>nrows</kbd> x <kbd>ncols</kbd> x <kbd>nact</kbd>, that is, we have a <kbd>nact</kbd> number of entries for each cell, and we have <kbd>nrows</kbd> x <kbd>ncols</kbd> total number of cells:</p>
<pre class="mce-root">Q = np.zeros((nrows,ncols,nact),dtype=np.float)</pre>
<p>We will define a function to make the agent go to the start location, which has (<em>x, y</em>) co-ordinates of (<kbd>x=0</kbd>, <kbd>y=nrows</kbd>):</p>
<pre class="mce-root">def go_to_start():<br/>    # start coordinates<br/>    y = nrows<br/>    x = 0<br/>    return x, y</pre>
<p>Next, we define a function to take a random action, where we define <kbd>a = 0</kbd> for moving <kbd>top/north</kbd>, <kbd>a = 1</kbd> for moving <kbd>right/east</kbd>, <kbd>a = 2</kbd> for moving <kbd>bottom/south</kbd>, and <kbd>a = 4</kbd> for moving <kbd>left/west</kbd>. Specifically, we will use NumPy's <kbd>random.randint()</kbd> function, as follows:</p>
<pre class="mce-root">def random_action():<br/>    # a = 0 : top/north<br/>    # a = 1 : right/east<br/>    # a = 2 : bottom/south<br/>    # a = 3 : left/west<br/>    a = np.random.randint(nact)<br/>    return a</pre>
<p class="mce-root">We will now define the <kbd>move</kbd> function, which will take a given (<em>x, y</em>) location of the agent and the current action, <kbd>a</kbd>, and then will perform the action. It will return the new location of the agent after taking the action, (<em>x1, y1</em>), as well as the state of the agent, which we define as <kbd>state = 0</kbd> for the agent to be <kbd>OK</kbd> after taking the action; <kbd>state = 1</kbd> for reaching the destination; and <kbd>state = 2</kbd> for falling into the cliff. If the agent leaves the domain through the left, top, or right, it is sent back to the same grid (equivalent to taking no action):</p>
<pre class="mce-root">def move(x,y,a):<br/>    # state = 0: OK<br/>    # state = 1: reached destination<br/>    # state = 2: fell into cliff<br/>    state = 0<br/><br/>   if (x == 0 and y == nrows and a == 0):<br/>     # start location<br/>     x1 = x<br/>     y1 = y - 1<br/>     return x1, y1, state<br/>   elif (x == ncols-1 and y == nrows-1 and a == 2):<br/>     # reached destination<br/>     x1 = x<br/>     y1 = y + 1<br/>     state = 1<br/>     return x1, y1, state<br/>   else:<br/>     # inside grid; perform action<br/>     if (a == 0):<br/>       x1 = x<br/>       y1 = y - 1<br/>     elif (a == 1):<br/>       x1 = x + 1<br/>       y1 = y<br/>     elif (a == 2):<br/>       x1 = x<br/>       y1 = y + 1<br/>     elif (a == 3):<br/>       x1 = x - 1<br/>       y1 = y<br/><br/>     # don't allow agent to leave boundary<br/>     if (x1 &lt; 0):<br/>       x1 = 0<br/>     if (x1 &gt; ncols-1):<br/>       x1 = ncols-1<br/>     if (y1 &lt; 0):<br/>       y1 = 0<br/>     if (y1 &gt; nrows-1):<br/>       state = 2<br/><br/>   return x1, y1, state</pre>
<p>We will next define the <kbd>exploit</kbd> function, which will take the (<em>x, y</em>) location of the agent and take the greedy action based on the Q values, that is, it will take the <kbd>a</kbd> action that has the highest Q value at that (<em>x, y</em>) location. We will do this using NumPy's <kbd>np.argmax()</kbd>. If we are at the start location, we go north (<kbd>a = 0</kbd>); if we are one step away from the destination, we go south (<kbd>a = 2</kbd>):</p>
<pre class="mce-root">def exploit(x,y,Q):<br/>   # start location<br/>   if (x == 0 and y == nrows):<br/>     a = 0<br/>     return a<br/><br/>   # destination location<br/>   if (x == ncols-1 and y == nrows-1):<br/>     a = 2<br/>     return a<br/><br/>   if (x == ncols-1 and y == nrows):<br/>     print("exploit at destination not possible ")<br/>     sys.exit()<br/><br/>   # interior location<br/>   if (x &lt; 0 or x &gt; ncols-1 or y &lt; 0 or y &gt; nrows-1):<br/>     print("error ", x, y)<br/>     sys.exit()<br/><br/>   a = np.argmax(Q[y,x,:])<br/>   return a</pre>
<p class="mce-root">Next, we will perform the Bellman update using the following <kbd>bellman()</kbd> function:</p>
<pre class="mce-root">def bellman(x,y,a,reward,Qs1a1,Q):<br/>  if (y == nrows and x == 0):<br/>    # at start location; no Bellman update possible<br/>    return Q<br/><br/>  if (y == nrows and x == ncols-1):<br/>    # at destination location; no Bellman update possible<br/>    return Q<br/><br/>  # perform the Bellman update <br/>  Q[y,x,a] = Q[y,x,a] + alpha*(reward + gamma*Qs1a1 - Q[y,x,a])<br/>  return Q</pre>
<p class="mce-root">We will then define a function to explore or exploit, depending on a random number less than <em>ε</em>, the parameter we use in the ε-greedy exploration strategy. For this, we will use NumPy's <kbd>np.random.uniform()</kbd>, which will output a real random number between zero and one:</p>
<pre class="mce-root">def explore_exploit(x,y,Q):<br/>  # if we end up at the start location, then exploit<br/>  if (x == 0 and y == nrows):<br/>     a = 0<br/>     return a<br/><br/>  # call a uniform random number<br/>  r = np.random.uniform()<br/><br/>  if (r &lt; epsilon):<br/>    # explore<br/>    a = random_action()<br/>  else:<br/>    # exploit<br/>    a = exploit(x,y,Q)<br/>  return a</pre>
<p class="mce-root">We now have all the functions required for the cliff walking problem. So, we will loop over the episodes and for each episode we start at the starting location, then explore or exploit, then we move the agent one step depending on the action. Here is the Python code for this:</p>
<pre class="mce-root">for n in range(nepisodes+1):<br/><br/>  # print every 1000 episodes <br/>  if (n % 1000 == 0):<br/>    print("episode #: ", n)<br/> <br/>  # start<br/>  x, y = go_to_start()<br/><br/>  # explore or exploit<br/>  a = explore_exploit(x,y,Q)<br/><br/>  while(True):<br/>   # move one step<br/>   x1, y1, state = move(x,y,a)</pre>
<p>We perform the Bellman update based on the rewards received; note that this is based on the equations presented earlier in this chapter in the theory section. We stop the episode if we reach the destination or fall down the cliff; if not, we continue the exploration or exploitation strategy for one more step, and this goes on and on. The <kbd>state</kbd> variable in the following code takes the <kbd>1</kbd> value for reaching the destination, takes the value <kbd>2</kbd> for falling down the cliff, and is <kbd>0</kbd> otherwise:</p>
<pre class="mce-root">   # Bellman update<br/>   if (state == 1):<br/>      reward = reward_destination<br/>      Qs1a1 = 0.0<br/>      Q = bellman(x,y,a,reward,Qs1a1,Q)<br/>      break<br/>   elif (state == 2):<br/>      reward = reward_cliff<br/>      Qs1a1 = 0.0<br/>      Q = bellman(x,y,a,reward,Qs1a1,Q)<br/>      break<br/>   elif (state == 0):<br/>      reward = reward_normal<br/>      # Sarsa<br/>      a1 = explore_exploit(x1,y1,Q)<br/>      if (x1 == 0 and y1 == nrows):<br/>        # start location<br/>        Qs1a1 = 0.0<br/>      else:<br/>        Qs1a1 = Q[y1,x1,a1]<br/> <br/>      Q = bellman(x,y,a,reward,Qs1a1,Q)<br/>      x = x1<br/>      y = y1<br/>      a = a1</pre>
<p class="mce-root">The preceding code will complete all the episodes, and we now have the converged values of Q. We will now plot this using <kbd>matplotlib</kbd> for each of the actions:</p>
<pre class="mce-root">for i in range(nact):<br/>  plt.subplot(nact,1,i+1)<br/>  plt.imshow(Q[:,:,i])<br/>  plt.axis('off')<br/>  plt.colorbar()<br/>  if (i == 0):<br/>    plt.title('Q-north')<br/>  elif (i == 1):<br/>    plt.title('Q-east')<br/>  elif (i == 2):<br/>    plt.title('Q-south')<br/>  elif (i == 3):<br/>    plt.title('Q-west')<br/>  plt.savefig('Q_sarsa.png')<br/>  plt.clf()<br/>  plt.close()</pre>
<p class="mce-root">Finally, we will do a path planning using the preceding converged Q values. That is, we will plot the exact path the agent will take from start to finish using the final converged Q values. For this, we will create a variable called <kbd>path</kbd>, and store values for it tracing the <kbd>path</kbd>. We will then plot it using <kbd>matplotlib</kbd> as follows:</p>
<pre class="mce-root">path = np.zeros((nrows,ncols,nact),dtype=np.float)<br/>x, y = go_to_start()<br/>while(True):<br/> a = exploit(x,y,Q)<br/> print(x,y,a)<br/> x1, y1, state = move(x,y,a)<br/> if (state == 1 or state == 2):<br/> print("breaking ", state)<br/> break<br/> elif (state == 0):<br/> x = x1<br/> y = y1<br/> if (x &gt;= 0 and x &lt;= ncols-1 and y &gt;= 0 and y &lt;= nrows-1):<br/> path[y,x] = 100.0<br/>plt.imshow(path)<br/>plt.savefig('path_sarsa.png')</pre>
<p>That's it. We have completed the coding required for the cliff walking problem with SARSA. We will now view the results. In the following screenshot, we present the Q values for each of the actions (going north, east, south, or west) at each of the locations in the grid. As per the legend, yellow represents high Q values and violet represents low Q values.</p>
<p>SARSA clearly tries to avoid the cliff by choosing to not go south when the agent is just one step to the north of the cliff, as is evident from the large negative Q values for the south action:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-525 image-border" src="assets/505d9930-d182-4645-a9a4-014be72a8096.png" style="width:25.00em;height:24.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: Q values for the cliff walking problem using SARSA</div>
<p>We will next plot the path taken by the agent from start to finish in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-526 image-border" src="assets/65270e16-dedb-4ed8-88e7-d5c04a3c5497.png" style="width:27.75em;height:9.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2: Path taken by the agent using SARSA</div>
<p><span>The same cliff walking problem will now be investigated using Q-learning.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cliff walking with Q-learning</h1>
                </header>
            
            <article>
                
<p><span>We will now repeat the same cliff walking problem, albeit using Q-learning in lieu of SARSA. Most of the code is the same as was used for SARSA, except for a few differences that will be summarized here. Since Q-learning uses a greedy action selection strategy, we will use a function for this as follows to compute the value of the maximum value of Q at a given location. Most of the code is the same as in the previous section, so we will only specify the changes to be made.<br/></span></p>
<p>Now, let's code cliff walking with Q-learning.</p>
<p>The <kbd>max_Q()</kbd> function is defined as follows:</p>
<pre>def max_Q(x,y,Q):<br/>  a = np.argmax(Q[y,x,:]) <br/>  return Q[y,x,a]</pre>
<p>We will compute the Q value at the new state using the <kbd>max_Q()</kbd> function defined previously:</p>
<pre>Qs1a1 = max_Q(x1,y1,Q)</pre>
<p>In addition, the action choosing whether it is exploration or exploitation is done inside the <kbd>while</kbd> loop as we choose actions greedily when exploiting:</p>
<pre># explore or exploit<br/>a = explore_exploit(x,y,Q)</pre>
<p>That's it for coding Q-learning. We will now apply this to solve the cliff walking problem and present the Q values for each of the actions, and the path traced by the agent to go from start to finish, which are shown in the following screenshots:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-528 image-border" src="assets/f5369a08-d1bf-4c24-a6cd-402953bf2d65.png" style="width:25.58em;height:25.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3: Q values for the cliff walking problem using Q-learning</div>
<p>As evident, the path traced is now different for Q-learning vis-<span>à</span>-vis SARSA. Since Q-learning is a greedy strategy, the agent now takes a path close to the cliff at the bottom of the following screenshot (<em>Figure 4</em>), as it is the shortest path:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-529 image-border" src="assets/1129e881-0d54-4ac3-9dc7-bf086e65ba38.png" style="width:28.50em;height:9.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4: Path traced by the agent for the cliff walking problem using<span> </span>Q-learning</div>
<p>On the other hand, since SARSA is more far-sighted, and so chooses the safe but longer path that is the top row of cells (see <em>Figure 2</em>).</p>
<p>Our next problem is the grid world problem, where we must navigate a grid. We will code this in SARSA.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grid world with SARSA</h1>
                </header>
            
            <article>
                
<p>We will next consider the grid world problem, and we will use SARSA to solve it. We will introduce obstacles in place of a cliff. The goal of the agent is to navigate the grid world from start to destination by avoiding the obstacles. We will store the coordinates of the obstacle cells in the <kbd>obstacle_cells</kbd> list, where each entry is the (<em>x, y</em>) coordinate of the obstacle cell. </p>
<p>Here is a summary of the steps involved in this task:</p>
<ol>
<li>Most of the code is the same as previously used, the differences will be summarized here</li>
<li>Place obstacles in the grid</li>
<li>The <kbd>move()</kbd> function has to also look for obstacles in the grid</li>
<li>Plot Q values and the path traced by the agent</li>
</ol>
<p>Here, we will start coding the algorithm in Python:</p>
<pre>import numpy as np<br/>import sys<br/>import matplotlib.pyplot as plt<br/><br/><br/>nrows = 3<br/>ncols = 12<br/>nact = 4<br/><br/>nepisodes = 100000<br/>epsilon = 0.1<br/>alpha = 0.1<br/>gamma = 0.95<br/><br/><br/>reward_normal = -1<br/>reward_destination = -1<br/><br/><br/># obstacles<br/>obstacle_cells = [(4,1), (4,2), (8,0), (8,1)]</pre>
<p>The <kbd>move()</kbd> function will now change, as we have to also look for obstacles. If the agent ends up in one of the obstacle cells it is pushed back to where it came from, as shown in the following code snippet:</p>
<pre>def move(x,y,a):<br/>  # state = 0: OK<br/>  # state = 1: reached destination<br/>  state = 0 <br/><br/>  if (x == 0 and y == nrows and a == 0):<br/>    # start location<br/>    x1 = x<br/>    y1 = y - 1 <br/>    return x1, y1, state <br/>  elif (x == ncols-1 and y == nrows-1 and a == 2):<br/>    # reached destination<br/>    x1 = x<br/>    y1 = y + 1<br/>    state = 1<br/>    return x1, y1, state<br/>  else: <br/><br/>    if (a == 0):<br/>      x1 = x<br/>      y1 = y - 1<br/>    elif (a == 1):<br/>      x1 = x + 1<br/>      y1 = y<br/>    elif (a == 2):<br/>      x1 = x<br/>      y1 = y + 1<br/>    elif (a == 3):<br/>      x1 = x - 1 <br/>      y1 = y<br/><br/>    if (x1 &lt; 0):<br/>     x1 = 0<br/>    if (x1 &gt; ncols-1):<br/>     x1 = ncols-1<br/>    if (y1 &lt; 0):<br/>     y1 = 0<br/>    if (y1 &gt; nrows-1):<br/>     y1 = nrows-1<br/><br/>    # check for obstacles; reset to original (x,y) if inside obstacle<br/>    for i in range(len(obstacle_cells)):<br/>      if (x1 == obstacle_cells[i][0] and y1 == obstacle_cells[i][1]):<br/>         x1 = x<br/>         y1 = y <br/>            <br/>    return x1, y1, state </pre>
<p>That's it for coding grid world with SARSA. The Q-values and the path taken are shown in following diagrams: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-530 image-border" src="assets/6fea0ff6-2606-4c75-9d97-0e2b04560463.png" style="width:24.42em;height:23.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5: Q-values for each of the actions for the grid world problem using SARSA</div>
<p>As we can see in the following diagram, the agent navigates around the obstacles to reach its destination:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-531 image-border" src="assets/ce653ae1-9920-48d1-87d8-7de42b1930c7.png" style="width:28.75em;height:10.17em;"/><br/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6: Path traced by the agent for the grid world problem using SARSA</div>
<p>Grid world with Q-learning is not a straightforward problem to attempt, as the greedy strategy used will not enable the agent to avoid repeated actions easily at a given state. Convergence is typically very slow, and so it will be avoided for now. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the concept of TD. We also learned about our first two RL algorithms: Q-learning and SARSA. We saw how you can code these two algorithms in Python and use them to solve the cliff walking and grid world problems. These two algorithms give us a good understanding of the basics of RL and how to transition from theory to code. These two algorithms were very popular in the 1990s and early 2000s, before deep RL gained prominence. Despite that, Q-learning and SARSA still find use in the RL community today.</p>
<p>In the next chapter, we will look at the use of deep neural networks in RL that gives rise to deep RL. We will see a variant of Q-learning called <strong>Deep Q-Networks</strong> (<strong>DQNs</strong>) that will use a neural network instead of a tabular state-action value function, which we saw in this chapter. Note that only problems with small number of states and actions are suited to Q-learning and SARSA. When we have a large number of states and/or actions, we encounter what is called as the Curse of Dimensionality, where a tabular approach will be unfeasible due to excessive memory use; in these problems, DQN is better suited, and will be the crux of the next chapter. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Reinforcement Learning: an Introduction</em> by </span><em>Richard Sutton and Andrew Barto</em>, 2018</li>
</ul>


            </article>

            
        </section>
    </body></html>