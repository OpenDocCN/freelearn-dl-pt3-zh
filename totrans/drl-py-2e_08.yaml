- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: A Primer on TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 入门
- en: TensorFlow is one of the most popular deep learning libraries. In upcoming chapters,
    we will use TensorFlow to build deep reinforcement models. So, in this chapter,
    we will get ourselves familiar with TensorFlow and its functionalities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是最受欢迎的深度学习库之一。在接下来的章节中，我们将使用 TensorFlow 构建深度强化学习模型。因此，在本章中，我们将熟悉
    TensorFlow 及其功能。
- en: We will learn about what computational graphs are and how TensorFlow uses them.
    We will also explore TensorBoard, which is a visualization tool provided by TensorFlow
    used for visualizing models. Going forward, we will understand how to build a
    neural network with TensorFlow to perform handwritten digit classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将了解什么是计算图，以及 TensorFlow 如何使用它们。我们还将探讨 TensorBoard，这是 TensorFlow 提供的一个可视化工具，用于可视化模型。接下来，我们将理解如何使用
    TensorFlow 构建神经网络来执行手写数字分类。
- en: Moving on, we will learn about TensorFlow 2.0, which is the latest version of
    TensorFlow. We will understand how TensorFlow 2.0 differs from its previous versions
    and how it uses Keras as its high-level API.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习 TensorFlow 2.0，它是 TensorFlow 的最新版本。我们将了解 TensorFlow 2.0 与其先前版本的不同之处，以及它如何使用
    Keras 作为其高级 API。
- en: 'In this chapter, we will learn about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: TensorFlow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: Computational graphs and sessions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算图和会话
- en: Variables, constants, and placeholders
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量、常量和占位符
- en: TensorBoard
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorBoard
- en: Handwritten digit classification in TensorFlow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 中的手写数字分类
- en: Math operations in TensorFlow
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 中的数学运算
- en: TensorFlow 2.0 and Keras
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 和 Keras
- en: What is TensorFlow?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: TensorFlow is an open source software library from Google, which is extensively
    used for numerical computation. It is one of the most used libraries for building
    deep learning models. It is highly scalable and runs on multiple platforms, such
    as Windows, Linux, macOS, and Android. It was originally developed by the researchers
    and engineers of the Google Brain team.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是 Google 的一个开源软件库，广泛用于数值计算。它是构建深度学习模型时使用最广泛的库之一，具有高度的可扩展性，并能在多个平台上运行，例如
    Windows、Linux、macOS 和 Android。最初由 Google Brain 团队的研究人员和工程师开发。
- en: TensorFlow supports execution on everything, including CPUs, GPUs, TPUs, which
    are tensor processing units, and mobile and embedded platforms. Due to its flexible
    architecture and ease of deployment, it has become a popular choice of library
    among many researchers and scientists for building deep learning models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 支持在各种设备上执行，包括 CPU、GPU、TPU（张量处理单元）、移动平台和嵌入式平台。由于其灵活的架构和易于部署，它已成为许多研究人员和科学家构建深度学习模型时的热门选择。
- en: In TensorFlow, every computation is represented by a data flow graph, also known
    as a **computational graph**, where nodes represent operations, such as addition
    or multiplication, and edges represent tensors. Data flow graphs can also be shared
    and executed on many different platforms. TensorFlow provides a visualization
    tool, called TensorBoard, for visualizing data flow graphs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，每个计算都是通过数据流图表示的，也称为 **计算图**，其中节点代表操作（如加法或乘法），边代表张量。数据流图也可以在许多不同的平台上共享和执行。TensorFlow
    提供了一种可视化工具，叫做 TensorBoard，用于可视化数据流图。
- en: TensorFlow 2.0 is the latest version of TensorFlow. In the upcoming chapters,
    we will use TensorFlow 2.0 for building deep reinforcement learning models. However,
    it is important to understand how TensorFlow 1.x works. So, first, we will learn
    to use TensorFlow 1.x and then we will look into TensorFlow 2.0\.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 是 TensorFlow 的最新版本。在接下来的章节中，我们将使用 TensorFlow 2.0 构建深度强化学习模型。然而，理解
    TensorFlow 1.x 的工作原理也很重要。所以，首先，我们将学习如何使用 TensorFlow 1.x，然后再深入了解 TensorFlow 2.0。
- en: 'You can install TensorFlow easily through `pip` by just typing the following
    command in your terminal:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在终端输入以下命令，轻松通过 `pip` 安装 TensorFlow：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can check the successful installation of TensorFlow by running the following
    simple `Hello TensorFlow!` program:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下简单的 `Hello TensorFlow!` 程序来检查 TensorFlow 是否安装成功：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding program should print `Hello TensorFlow!`. If you get any errors,
    then you probably have not installed TensorFlow correctly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的程序应该打印出`Hello TensorFlow!`。如果你遇到任何错误，那么可能是你没有正确安装 TensorFlow。
- en: Understanding computational graphs and sessions
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解计算图和会话
- en: As we have learned, every computation in TensorFlow is represented by a computational
    graph. They consist of several nodes and edges, where nodes are mathematical operations,
    such as addition and multiplication, and edges are tensors. Computational graphs
    are very efficient at optimizing resources and promote distributed computing.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所学，TensorFlow 中的每一个计算都由一个计算图表示。计算图由多个节点和边组成，其中节点是数学运算，如加法和乘法，边是张量。计算图在资源优化方面非常高效，并且促进了分布式计算。
- en: A computational graph consists of several TensorFlow operations, arranged in
    a graph of nodes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图由多个 TensorFlow 操作组成，排列成一个节点图。
- en: 'A computational graph helps us to understand the network architecture when
    we work on building a really complex neural network. For instance, let''s consider
    a simple layer, *h* = Relu(*WX* + *b*). Its computational graph would be represented
    as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图帮助我们理解网络架构，尤其在构建复杂神经网络时。例如，假设我们考虑一个简单的层，*h* = Relu(*WX* + *b*)。其计算图将如下所示：
- en: '![](img/B15558_08_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_08_01.png)'
- en: 'Figure 8.1: Computational graph'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：计算图
- en: 'There are two types of dependency in the computational graph, called direct
    and indirect dependency. Say we have node `b`, the input of which is dependent
    on the output of node `a`; this type of dependency is called **direct dependency**,
    as shown in the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图中有两种依赖关系，分别称为直接依赖和间接依赖。假设我们有节点`b`，其输入依赖于节点`a`的输出；这种依赖关系称为**直接依赖**，如以下代码所示：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When node `b` doesn''t depend on node `a` for its input, it is called **indirect
    dependency**, as shown in the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点 `b` 的输入不依赖于节点 `a` 时，这称为**间接依赖**，如以下代码所示：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'So, if we can understand these dependencies, we can distribute the independent
    computations in the available resources and reduce the computation time. Whenever
    we import TensorFlow, a default graph is created automatically and all of the
    nodes we create are associated with the default graph. We can also create our
    own graphs instead of using the default graph, and this is useful when building
    multiple models that do not depend on one another. A TensorFlow graph can be created
    using `tf.Graph()`, as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能够理解这些依赖关系，就能在可用资源中分配独立的计算，减少计算时间。每当我们导入 TensorFlow 时，默认图会自动创建，我们创建的所有节点都会与该默认图关联。我们还可以创建自己的图，而不是使用默认图，这在构建相互独立的多个模型时非常有用。可以使用`tf.Graph()`创建一个
    TensorFlow 图，如下所示：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If we want to clear the default graph (that is, if we want to clear the previously
    defined variables and operations in the graph), then we can do this using `tf.reset_default_graph()`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想清除默认图（即清除图中先前定义的变量和操作），则可以使用`tf.reset_default_graph()`来实现。
- en: Sessions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 会话
- en: As mentioned in the previous section, a computational graph with operations
    on its nodes and tensors to its edges is created, and in order to execute the
    graph, we use a TensorFlow session.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，创建了一个计算图，其节点上包含操作，边缘上连接张量。为了执行该图，我们使用一个 TensorFlow 会话。
- en: 'A TensorFlow session can be created using `tf.Session()`, as shown in the following
    code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`tf.Session()`来创建 TensorFlow 会话，如以下代码所示：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After creating the session, we can execute our graph, using the `sess.run()`
    method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 创建会话后，我们可以使用`sess.run()`方法来执行我们的计算图。
- en: Every computation in TensorFlow is represented by a computational graph, so
    we need to run a computational graph for everything. That is, in order to compute
    anything on TensorFlow, we need to create a TensorFlow session.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的每一个计算都由一个计算图表示，因此我们需要运行计算图来进行所有计算。也就是说，要在 TensorFlow 中进行任何计算，我们需要创建一个
    TensorFlow 会话。
- en: 'Let''s execute the following code to multiply two numbers:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下代码以乘法运算两个数字：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Instead of printing `9`, the preceding code will print a TensorFlow object,
    `Tensor("Mul:0", shape=(), dtype=int32)`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与打印 `9` 不同，前面的代码将打印一个 TensorFlow 对象，`Tensor("Mul:0", shape=(), dtype=int32)`。
- en: As we discussed earlier, whenever we import TensorFlow, a default computational
    graph is automatically created and all nodes are attached to the graph. Hence,
    when we print `a`, it just returns the TensorFlow object because the value for
    `a` is not computed yet, as the computation graph has not been executed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每当我们导入 TensorFlow 时，默认的计算图会自动创建，所有节点都会附加到该图上。因此，当我们打印 `a` 时，它只会返回 TensorFlow
    对象，因为 `a` 的值尚未计算，计算图尚未执行。
- en: 'In order to execute the graph, we need to initialize and run the TensorFlow
    session, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行计算图，我们需要初始化并运行 TensorFlow 会话，如下所示：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding code prints `9`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码打印出 `9`。
- en: Now that we have learned about sessions, in the next section, we will learn
    about variables, constants, and placeholders.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了会话，接下来的部分将学习变量、常量和占位符。
- en: Variables, constants, and placeholders
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量、常量和占位符
- en: Variables, constants, and placeholders are fundamental elements of TensorFlow.
    However, there is always confusion between these three. Let's look at each element,
    one by one, and learn the difference between them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 变量、常量和占位符是 TensorFlow 的基础元素。然而，这三者之间经常会产生混淆。让我们逐一查看每个元素，并了解它们之间的区别。
- en: Variables
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量
- en: 'Variables are containers used to store values. Variables are used as input
    to several other operations in a computational graph. A variable can be created
    using the `tf.Variable()` function, as shown in the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 变量是用来存储值的容器。变量作为输入用于计算图中的多个其他操作。可以使用 `tf.Variable()` 函数来创建变量，如下列代码所示：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s create a variable called `W`, using `tf.Variable()`, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `tf.Variable()` 创建一个名为 `W` 的变量，如下所示：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see in the preceding code, we create a variable, `W`, by randomly
    drawing values from a normal distribution with a standard deviation of `0.35`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的代码所示，我们通过从标准差为 `0.35` 的正态分布中随机抽取值来创建一个变量 `W`。
- en: What is that `name` parameter in `tf.Variable()`?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Variable()` 中的 `name` 参数是什么？'
- en: It is used to set the name of the variable in the computational graph. So, in
    the preceding code, Python saves the variable as `W` but in the TensorFlow graph,
    it will be saved as `weights`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它用于设置计算图中变量的名称。所以，在前面的代码中，Python 将变量保存为 `W`，但在 TensorFlow 图中，它将被保存为 `weights`。
- en: After defining a variable, we need to initialize all of the variables in the
    computational graph. That can be done using `tf.global_variables_initializer()`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义变量后，我们需要初始化计算图中的所有变量。可以使用 `tf.global_variables_initializer()` 来完成这一步。
- en: 'Once we create a session, we run the initialization operation, which initializes
    all of the defined variables, and only then can we run the other operations, as
    shown in the following code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了会话，我们运行初始化操作，初始化所有已定义的变量，只有在此之后才能运行其他操作，如下列代码所示：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Constants
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常量
- en: 'Constants, unlike variables, cannot have their values changed. That is, constants
    are immutable. Once they are assigned values, they cannot be changed throughout
    the program. We can create constants using `tf.constant()`, as the following code
    shows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 常量与变量不同，常量的值不能改变。也就是说，常量是不可变的。一旦赋值，它们在整个程序中都无法更改。我们可以使用 `tf.constant()` 创建常量，如下列代码所示：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Placeholders and feed dictionaries
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 占位符和馈送字典
- en: We can think of placeholders as variables, where we only define the type and
    dimension, but do not assign a value. Values for the placeholders will be fed
    at runtime. We feed data to computational graphs using placeholders. Placeholders
    are defined with no values.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把占位符看作变量，在其中只定义类型和维度，但不分配值。占位符的值将在运行时输入。我们使用占位符将数据传递给计算图。占位符在定义时没有值。
- en: 'A placeholder can be defined using `tf.placeholder()`. It takes an optional
    argument called `shape`, which denotes the dimensions of the data. If `shape`
    is set to `None`, then we can feed data of any size at runtime. A placeholder
    can be defined as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 占位符可以通过 `tf.placeholder()` 来定义。它有一个可选参数 `shape`，表示数据的维度。如果 `shape` 设置为 `None`，则可以在运行时输入任何大小的数据。占位符可以这样定义：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To put it in simple terms, we use `tf.Variable` to store data and `tf.placeholder`
    to feed in external data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们使用 `tf.Variable` 来存储数据，而使用 `tf.placeholder` 来输入外部数据。
- en: 'Let''s consider a simple example to better understand placeholders:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解占位符，我们考虑一个简单的例子：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If we run the preceding code, then it will return an error because we are trying
    to compute `y`, where `y= x+3` and `x` is a placeholder whose value is not assigned.
    As we have learned, values for the placeholders will be assigned at runtime. We
    assign the values of the placeholder using the `feed_dict` parameter. The `feed_dict`
    parameter is basically a dictionary where the key represents the name of the placeholder,
    and the value represents the value of the placeholder.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following code, we set `feed_dict = {x:5}`, which implies
    that the value for the `x` placeholder is `5`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding code returns `8.0`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: That's it. In the next section, we will learn about TensorBoard.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorBoard
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorBoard is TensorFlow's visualization tool, which can be used to visualize
    a computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it becomes confusing when we have to debug the network. So,
    if we can visualize the computational graph in TensorBoard, we can easily understand
    such complex models, debug them, and optimize them. TensorBoard also supports
    sharing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 8.2*, the TensorBoard panel consists of several tabs—**SCALARS**,
    **IMAGES**, **AUDIO**, **GRAPHS**, **DISTRIBUTIONS**, **HISTOGRAMS**, and **EMBEDDINGS**:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_02.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: TensorBoard'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The tabs are pretty self-explanatory. The **SCALARS** tab shows useful information
    about the scalar variables we use in our program. For example, it shows how the
    value of a scalar variable called `loss` changes over several iterations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The **GRAPHS** tab shows the computational graph. The **DISTRIBUTIONS** and
    **HISTOGRAMS** tabs show the distribution of a variable. For example, our model's
    weight distribution and histogram can be seen under these tabs. The **EMBEDDINGS**
    tab is used for visualizing high-dimensional vectors, such as word embeddings.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a basic computational graph and visualize it in TensorBoard. Let''s
    say we have four constants, shown as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s multiply `x` and `y` and `a` and `b` and save them as `prod1` and `prod2`,
    as shown in the following code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add `prod1` and `prod2` and store them in `sum`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, we can visualize all of these operations in TensorBoard. To visualize in
    TensorBoard, we first need to save our event files. That can be done using `tf.summary.FileWriter()`.
    It takes two important parameters, `logdir` and `graph`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, `logdir` specifies the directory where we want to store
    the graph, and `graph` specifies which graph we want to store:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, `./graphs` is the directory where we are storing our
    event file, and `sess.graph` specifies the current graph in our TensorFlow session.
    So, we are storing the current graph of the TensorFlow session in the `graphs`
    directory.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'To start TensorBoard, go to your Terminal, locate the working directory, and
    type the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `logdir` parameter indicates the directory where the event file is stored
    and `port` is the port number. Once you run the preceding command, open your browser
    and type `http://localhost:8000/`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'In the TensorBoard panel, under the **GRAPHS** tab, you can see the computational
    graph:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Computational graph'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: As you may notice, all of the operations we have defined are clearly shown in
    the graph.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Creating a name scope
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scoping is used to reduce complexity and helps us to better understand a model
    by grouping related nodes together. Having a name scope helps us to group similar
    operations in a graph. This comes in handy when we are building a complex architecture.
    Scoping can be created using `tf.name_scope()`. In the previous example, we performed
    two operations, `Product` and `sum`. We can simply group them into two different
    name scopes as `Product` and `sum`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how `prod1` and `prod2` perform multiplication
    and compute the result. We''ll define a name scope called `Product`, and group
    the `prod1` and `prod2` operations, as shown in the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, define the name scope for `sum`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Store the file in the `graphs` directory:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Visualize the graph in TensorBoard:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you may notice, now, we have only two nodes, **sum** and **Product**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: A computational graph'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we double-click on the nodes, we can see how the computation is happening.
    As you can see, the **prod1** and **prod2** nodes are grouped under the **Product**
    scope, and their results are sent to the **sum** node, where they will be added.
    You can see how the **prod1** and **prod2** nodes compute their value:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_05.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: A computational graph in detail'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The preceding graph is just a simple example. When we are working on a complex
    project with a lot of operations, name scoping helps us to group similar operations
    together and enables us to understand the computational graph better.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about TensorFlow, in the next section, let's see how
    to build handwritten digit classification using TensorFlow.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Handwritten digit classification using TensorFlow
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Putting together all the concepts we have learned so far, we will see how we
    can use TensorFlow to build a neural network to recognize handwritten digits.
    If you have been playing around with deep learning of late, then you must have
    come across the MNIST dataset. It has been called the *hello world* of deep learning.
    It consists of 55,000 data points of handwritten digits (0 to 9).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will see how we can use our neural network to recognize
    these handwritten digits, and we will get the hang of TensorFlow and TensorBoard.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Importing the required libraries
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a first step, let''s import all of the required libraries:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Loading the dataset
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load the dataset, using the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding code, `data/mnist` implies the location where we store the
    MNIST dataset, and `one_hot=True` implies that we are one-hot encoding the labels
    (0 to 9).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see what we have in our data by executing the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have `55000` images in the training set, each image is of size `784`, and
    we have `10` labels, which are actually 0 to 9\. Similarly, we have `10000` images
    in the test set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll plot an input image to see what it looks like:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Thus, our input image looks like the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Image of the digit 7 from the training set'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of neurons in each layer
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll build a four-layer neural network with three hidden layers and one output
    layer. As the size of the input image is `784`, we set `num_input` to `784`, and
    since we have 10 handwritten digits (0 to 9), we set `10` neurons in the output
    layer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the number of neurons in each layer as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Defining placeholders
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have learned, we first need to define the placeholders for `input` and
    `output`. Values for the placeholders will be fed in at runtime through `feed_dict`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Since we have a four-layer network, we have four weights and four biases. We
    initialize our weights by drawing values from the truncated normal distribution
    with a standard deviation of `0.1`. Remember, the dimensions of the weight matrix
    should be *the* *number of neurons in the previous layer* x *the number of neurons
    in the current layer*. For instance, the dimensions of weight matrix `w3` should
    be *the* *number of neurons in hidden layer 2* x *the number of neurons in hidden
    layer 3*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'We often define all of the weights in a dictionary, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The shape of the bias should be the number of neurons in the current layer.
    For instance, the dimension of the `b2` bias is the number of neurons in hidden
    layer 2\. We set the bias value as a constant; `0.1` in all of the layers:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Forward propagation
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we''ll define the forward propagation operation. We''ll use ReLU activations
    in all layers. In the last layers, we''ll apply `sigmoid` activation, as shown
    in the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Computing loss and backpropagation
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we''ll define our loss function. We''ll use softmax cross-entropy as
    our loss function. TensorFlow provides the `tf.nn.softmax_cross_entropy_with_logits()`
    function for computing softmax cross-entropy loss. It takes two parameters as
    inputs, `logits` and `labels`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: The `logits` parameter specifies the `logits` predicted by our network; for
    example, `y_hat`
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `labels` parameter specifies the actual labels; for example, true labels,
    `Y`
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We take the mean of the `loss` function using `tf.reduce_mean()`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we need to minimize the loss using backpropagation. Don''t worry! We don''t
    have to calculate the derivatives of all the weights manually. Instead, we can
    use TensorFlow''s `optimizer`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Computing accuracy
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We calculate the accuracy of our model as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The `y_hat` parameter denotes the predicted probability for each class of our
    model. Since we have `10` classes, we will have `10` probabilities. If the probability
    is high at position `7`, then it means that our network predicts the input image
    as digit `7` with high probability. The `tf.argmax()` function returns the index
    of the largest value. Thus, `tf.argmax(y_hat,1)` gives the index where the probability
    is high. Thus, if the probability is high at index `7`, then it returns `7`.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Y` parameter denotes the actual labels, and they are the one-hot encoded
    values. That is, the `Y` parameter consists of zeros everywhere except at the
    position of the actual image, where it has a value of `1`. For instance, if the
    input image is `7`, then `Y` has a value of 0 at all indices except at index `7`,
    where it has a value of `1`. Thus, `tf.argmax(Y,1)` returns `7` because that is
    where we have a high value, `1`.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, `tf.argmax(y_hat,1)` gives the predicted digit, and `tf.argmax(Y,1)` gives
    us the actual digit.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.equal(x, y)` function takes `x` and `y` as inputs and returns the truth
    value of *(x == y)* element-wise. Thus, `correct_pred = tf.equal(predicted_digit,actual_digit)`
    consists of `True` where the actual and predicted digits are the same, and `False`
    where the actual and predicted digits are not the same. We convert the Boolean
    values in `correct_pred` into float values using TensorFlow's cast operation,
    `tf.cast(correct_pred, tf.float32)`. After converting them into float values,
    we take the average using `tf.reduce_mean()`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, `tf.reduce_mean(tf.cast(correct_pred, tf.float32))` gives us the average
    correct predictions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Creating a summary
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also visualize how the loss and accuracy of our model changes during
    several iterations in TensorBoard. So, we use `tf.summary()` to get the summary
    of the variable. Since the loss and accuracy are scalar variables, we use `tf.summary.scalar()`,
    as shown in the following code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we merge all of the summaries we use in our graph, using `tf.summary.merge_all()`.
    We do this because when we have many summaries, running and storing them would
    become inefficient, so we run them once in our session instead of running them
    multiple times:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Training the model
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, it is time to train our model. As we have learned, first, we need to initialize
    all of the variables:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the batch size, number of iterations, and learning rate, as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Start the TensorFlow session:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Initialize all the variables:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Save the event files:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Train the model for a number of iterations:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Get a batch of data according to the batch size:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Train the network:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Print `loss` and `accuracy` for every 100^(th) iteration:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As you may notice from the following output, the loss decreases and the accuracy
    increases over various training iterations:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Visualizing graphs in TensorBoard
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After training, we can visualize our computational graph in TensorBoard, as
    shown in *Figure 8.7*. As you can see, our **Model** takes **input**, **weights**,
    and **biases** as input and returns the output. We compute **Loss** and **Accuracy**
    based on the output of the model. We minimize the loss by calculating **gradients**
    and updating **weights**:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_07.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Computational graph'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'If we double-click and expand **Model**, we can see that we have three hidden
    layers and one output layer:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_08.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Expanding the Model node'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can double-click and see every node. For instance, if we open
    **weights**, we can see how the four weights are initialized using truncated normal
    distribution, and how it is updated using the Adam optimizer:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_09.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Expanding the weights node'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: As we have learned, the computational graph helps us to understand what is happening
    on each node.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how the accuracy is being calculated by double-clicking on the **Accuracy**
    node:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_10.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: Expanding the Accuracy node'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that we also stored a summary of our `loss` and `accuracy` variables.
    We can find them under the **SCALARS** tab in TensorBoard. *Figure 8.11* shows
    how the loss decreases over iterations:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_11.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Plot of the loss function'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.12* shows how accuracy increases over iterations:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_12.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 Plot of accuracy
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: That's it. In the next section, we will learn about another interesting feature
    in TensorFlow called eager execution.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Introducing eager execution
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Eager execution in TensorFlow is more Pythonic and allows rapid prototyping.
    Unlike graph mode, where we need to construct a graph every time we want to perform
    any operation, eager execution follows the imperative programming paradigm, where
    any operation can be performed immediately, without having to create a graph,
    just like we do in Python. Hence, with eager execution, we can say goodbye to
    sessions and placeholders. It also makes the debugging process easier with an
    immediate runtime error, unlike graph mode. For instance, in graph mode, to compute
    anything, we run the session. As shown in the following code, to evaluate the
    value of `z`, we have to run the TensorFlow session:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'With eager execution, we don''t need to create a session; we can simply compute
    `z`, just like we do in Python. In order to enable eager execution, just call
    the `tf.enable_eager_execution()` function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It will return the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In order to get the output value, we can print the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Although eager execution enables the imperative programming paradigm, in this
    book, we will investigate most of the examples in non-eager mode to better understand
    the algorithms from scratch. In the next section, we will see how to perform math
    operations using TensorFlow.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Math operations in TensorFlow
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will explore some of the operations in TensorFlow using eager execution
    mode:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Let's start with some basic arithmetic operations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `tf.add` to add two numbers:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `tf.subtract` function is used for finding the difference between two numbers:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `tf.multiply` function is used for multiplying two numbers:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Divide two numbers using `tf.divide`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The dot product can be computed as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, let''s find the index of the minimum and maximum elements:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The index of the minimum value is computed using `tf.argmin()`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The index of the maximum value is computed using `tf.argmax()`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Run the following code to find the squared difference between `x` and `y`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Let's try typecasting; that is, converting from one data type into another.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the type of `x`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can convert the type of `x`, which is `tf.int32`, into `tf.float32`, using
    `tf.cast`, as shown in the following code:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, check the `x` type. It will be `tf.float32`, as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Concatenate the two matrices:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Concatenate the matrices row-wise:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Use the following code to concatenate the matrices column-wise:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Stack the `x` matrix using the `stack` function:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, let''s see how to perform the `reduce_mean` operation:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Compute the mean value of `x`; that is, (*1.0* + *5.0* + *2.0* + *3.0*) / *4*:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Compute the mean across the row; that is, (*1.0*+*5.0*)/*2,* (*2.0*+*3.0*)/*2*:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Compute the mean across the column; that is, (*1.0*+*5.0*)/*2.0,* (*2.0*+*3.0*)/*2.0*:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Draw random values from the probability distributions:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Compute the softmax probabilities:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Now, we'll look at how to compute the gradients.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `square` function:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The gradients can be computed for the preceding `square` function using `tf.GradientTape`,
    as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: TensorFlow 2.0 and Keras
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow 2.0 has got some really cool features. It sets the eager execution
    mode by default. It provides a simplified workflow and uses Keras as the main
    API for building deep learning models. It is also backward compatible with TensorFlow
    1.x versions.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'To install TensorFlow 2.0, open your Terminal and type the following command:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Since TensorFlow 2.0 uses Keras as a high-level API, we will look at how Keras
    works in the next section.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Bonjour Keras
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras is another popularly used deep learning library. It was developed by François
    Chollet at Google. It is well known for its fast prototyping, and it makes model
    building simple. It is a high-level library, meaning that it does not perform
    any low-level operations on its own, such as convolution. It uses a backend engine
    for doing that, such as TensorFlow. The Keras API is available in `tf.keras`,
    and TensorFlow 2.0 uses it as the primary API.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Building a model in Keras involves four important steps:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compiling the model
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fitting the model
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating the model
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the model
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step is defining the model. Keras provides two different APIs to
    define the model:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The sequential API
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The functional API
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a sequential model
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In a sequential model, we stack each layer, one above another:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'First, let''s define our model as a `Sequential()` model, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now, define the first layer, as shown in the following code:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: In the preceding code, `Dense` implies a fully connected layer, `input_dim`
    implies the dimension of our input, and `activation` specifies the activation
    function that we use. We can stack up as many layers as we want, one above another.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the next layer with the `relu` activation function, as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Define the output layer with the `sigmoid` activation function:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The final code block of the sequential model is shown as follows. As you can
    see, the Keras code is much simpler than the TensorFlow code:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Defining a functional model
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A functional model provides more flexibility than a sequential model. For instance,
    in a functional model, we can easily connect any layer to another layer, whereas,
    in a sequential model, each layer is in a stack, one above another. A functional
    model comes in handy when creating complex models, such as directed acyclic graphs,
    models with multiple input values, multiple output values, and shared layers.
    Now, we will see how to define a functional model in Keras.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to define the input dimensions:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now, we''ll define our first fully connected layer with `10` neurons and `relu`
    activation, using the `Dense` class, as shown:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'We defined `layer1`, but where is the input to `layer1` coming from? We need
    to specify the input to `layer1` in a bracket notation at the end, as shown:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We define the next layer, `layer2`, with `13` neurons and `relu` activation.
    The input to `layer2` comes from `layer1`, so that is added in the bracket at
    the end, as shown in the following code:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now, we can define the output layer with the `sigmoid` activation function.
    Input to the output layer comes from `layer2`, so that is added in bracket at
    the end:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'After defining all of the layers, we define the model using a `Model` class,
    where we need to specify `inputs` and `outputs`, as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The complete code for the functional model is shown here:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Compiling the model
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have defined the model, the next step is to compile it. In this
    phase, we set up how the model should learn. We define three parameters when compiling
    the model:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'The `optimizer` parameter: This defines the optimization algorithm we want
    to use; for example, the gradient descent.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `loss` parameter: This is the objective function that we are trying to
    minimize; for example, the mean squared error or cross-entropy loss.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `metrics` parameter: This is the metric through which we want to assess
    the model''s performance; for example, `accuracy`. We can also specify more than
    one metric.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run the following code to compile the model:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Training the model
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We defined and also compiled the model. Now, we will train the model. Training
    the model can be done using the `fit` function. We specify our features, `x`;
    labels, `y`; the number of `epochs` we want to train the model for; and `batch_size`,
    as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Evaluating the model
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After training the model, we will evaluate the model on the test set:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We can also evaluate the model on the same train set, and that will help us
    to understand the training accuracy:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: That's it. Let's see how to use TensorFlow for the MNIST digit classification
    task in the next section.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: MNIST digit classification using TensorFlow 2.0
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will see how we can perform MNIST handwritten digit classification,
    using TensorFlow 2.0\. It requires only a few lines of code compared to TensorFlow
    1.x. As we have learned, TensorFlow 2.0 uses Keras as its high-level API; we just
    need to add `tf.keras` to the Keras code.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by loading the dataset:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Create a training set and a test set with the following code:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Normalize the train and test sets by dividing the values of `x` by the maximum
    value of `x`; that is, `255.0`:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Define the sequential model as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Now, let''s add layers to the model. We use a three-layer network with the
    ReLU function in the hidden layer and softmax in the final layer:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Compile the model by running the following line of code:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Train the model:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Evaluate the model:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: That's it! Writing code with the Keras API is that simple.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off this chapter by understanding TensorFlow and how it uses computational
    graphs. We learned that computation in TensorFlow is represented by a computational
    graph, which consists of several nodes and edges, where nodes are mathematical
    operations, such as addition and multiplication, and edges are tensors.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned that variables are containers used to store values, and they
    are used as input to several other operations in a computational graph. We also
    learned that placeholders are like variables, where we only define the type and
    dimension but do not assign the values, and the values for the placeholders are
    fed at runtime. Moving forward, we learned about TensorBoard, which is TensorFlow's
    visualization tool and can be used to visualize a computational graph. We also
    explored eager execution, which is more Pythonic and allows rapid prototyping.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: We understood that, unlike graph mode, where we need to construct a graph every
    time to perform any operation, eager execution follows the imperative programming
    paradigm, where any operation can be performed immediately, without having to
    create a graph, just like we do in Python.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we begin our **deep reinforcement learning** (**DRL**)
    journey by understanding one of the popular DRL algorithms, called the **Deep
    Q Network** (**DQN**).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s put our knowledge of TensorFlow to the test by answering the following
    questions:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: What is a TensorFlow session?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a placeholder.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is TensorBoard?
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is eager execution mode useful?
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are all the steps involved in building a model using Keras?
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does Keras's functional model differ from its sequential model?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can learn more about TensorFlow by checking its official documentation at
    [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
