- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Primer on TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is one of the most popular deep learning libraries. In upcoming chapters,
    we will use TensorFlow to build deep reinforcement models. So, in this chapter,
    we will get ourselves familiar with TensorFlow and its functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about what computational graphs are and how TensorFlow uses them.
    We will also explore TensorBoard, which is a visualization tool provided by TensorFlow
    used for visualizing models. Going forward, we will understand how to build a
    neural network with TensorFlow to perform handwritten digit classification.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we will learn about TensorFlow 2.0, which is the latest version of
    TensorFlow. We will understand how TensorFlow 2.0 differs from its previous versions
    and how it uses Keras as its high-level API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computational graphs and sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variables, constants, and placeholders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwritten digit classification in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Math operations in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow 2.0 and Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is an open source software library from Google, which is extensively
    used for numerical computation. It is one of the most used libraries for building
    deep learning models. It is highly scalable and runs on multiple platforms, such
    as Windows, Linux, macOS, and Android. It was originally developed by the researchers
    and engineers of the Google Brain team.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow supports execution on everything, including CPUs, GPUs, TPUs, which
    are tensor processing units, and mobile and embedded platforms. Due to its flexible
    architecture and ease of deployment, it has become a popular choice of library
    among many researchers and scientists for building deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow, every computation is represented by a data flow graph, also known
    as a **computational graph**, where nodes represent operations, such as addition
    or multiplication, and edges represent tensors. Data flow graphs can also be shared
    and executed on many different platforms. TensorFlow provides a visualization
    tool, called TensorBoard, for visualizing data flow graphs.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0 is the latest version of TensorFlow. In the upcoming chapters,
    we will use TensorFlow 2.0 for building deep reinforcement learning models. However,
    it is important to understand how TensorFlow 1.x works. So, first, we will learn
    to use TensorFlow 1.x and then we will look into TensorFlow 2.0\.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install TensorFlow easily through `pip` by just typing the following
    command in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the successful installation of TensorFlow by running the following
    simple `Hello TensorFlow!` program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding program should print `Hello TensorFlow!`. If you get any errors,
    then you probably have not installed TensorFlow correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding computational graphs and sessions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have learned, every computation in TensorFlow is represented by a computational
    graph. They consist of several nodes and edges, where nodes are mathematical operations,
    such as addition and multiplication, and edges are tensors. Computational graphs
    are very efficient at optimizing resources and promote distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: A computational graph consists of several TensorFlow operations, arranged in
    a graph of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A computational graph helps us to understand the network architecture when
    we work on building a really complex neural network. For instance, let''s consider
    a simple layer, *h* = Relu(*WX* + *b*). Its computational graph would be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Computational graph'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of dependency in the computational graph, called direct
    and indirect dependency. Say we have node `b`, the input of which is dependent
    on the output of node `a`; this type of dependency is called **direct dependency**,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When node `b` doesn''t depend on node `a` for its input, it is called **indirect
    dependency**, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So, if we can understand these dependencies, we can distribute the independent
    computations in the available resources and reduce the computation time. Whenever
    we import TensorFlow, a default graph is created automatically and all of the
    nodes we create are associated with the default graph. We can also create our
    own graphs instead of using the default graph, and this is useful when building
    multiple models that do not depend on one another. A TensorFlow graph can be created
    using `tf.Graph()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If we want to clear the default graph (that is, if we want to clear the previously
    defined variables and operations in the graph), then we can do this using `tf.reset_default_graph()`.
  prefs: []
  type: TYPE_NORMAL
- en: Sessions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous section, a computational graph with operations
    on its nodes and tensors to its edges is created, and in order to execute the
    graph, we use a TensorFlow session.
  prefs: []
  type: TYPE_NORMAL
- en: 'A TensorFlow session can be created using `tf.Session()`, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: After creating the session, we can execute our graph, using the `sess.run()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Every computation in TensorFlow is represented by a computational graph, so
    we need to run a computational graph for everything. That is, in order to compute
    anything on TensorFlow, we need to create a TensorFlow session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute the following code to multiply two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Instead of printing `9`, the preceding code will print a TensorFlow object,
    `Tensor("Mul:0", shape=(), dtype=int32)`.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed earlier, whenever we import TensorFlow, a default computational
    graph is automatically created and all nodes are attached to the graph. Hence,
    when we print `a`, it just returns the TensorFlow object because the value for
    `a` is not computed yet, as the computation graph has not been executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to execute the graph, we need to initialize and run the TensorFlow
    session, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code prints `9`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about sessions, in the next section, we will learn
    about variables, constants, and placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Variables, constants, and placeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variables, constants, and placeholders are fundamental elements of TensorFlow.
    However, there is always confusion between these three. Let's look at each element,
    one by one, and learn the difference between them.
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Variables are containers used to store values. Variables are used as input
    to several other operations in a computational graph. A variable can be created
    using the `tf.Variable()` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a variable called `W`, using `tf.Variable()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, we create a variable, `W`, by randomly
    drawing values from a normal distribution with a standard deviation of `0.35`.
  prefs: []
  type: TYPE_NORMAL
- en: What is that `name` parameter in `tf.Variable()`?
  prefs: []
  type: TYPE_NORMAL
- en: It is used to set the name of the variable in the computational graph. So, in
    the preceding code, Python saves the variable as `W` but in the TensorFlow graph,
    it will be saved as `weights`.
  prefs: []
  type: TYPE_NORMAL
- en: After defining a variable, we need to initialize all of the variables in the
    computational graph. That can be done using `tf.global_variables_initializer()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we create a session, we run the initialization operation, which initializes
    all of the defined variables, and only then can we run the other operations, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Constants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Constants, unlike variables, cannot have their values changed. That is, constants
    are immutable. Once they are assigned values, they cannot be changed throughout
    the program. We can create constants using `tf.constant()`, as the following code
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Placeholders and feed dictionaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can think of placeholders as variables, where we only define the type and
    dimension, but do not assign a value. Values for the placeholders will be fed
    at runtime. We feed data to computational graphs using placeholders. Placeholders
    are defined with no values.
  prefs: []
  type: TYPE_NORMAL
- en: 'A placeholder can be defined using `tf.placeholder()`. It takes an optional
    argument called `shape`, which denotes the dimensions of the data. If `shape`
    is set to `None`, then we can feed data of any size at runtime. A placeholder
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To put it in simple terms, we use `tf.Variable` to store data and `tf.placeholder`
    to feed in external data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a simple example to better understand placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If we run the preceding code, then it will return an error because we are trying
    to compute `y`, where `y= x+3` and `x` is a placeholder whose value is not assigned.
    As we have learned, values for the placeholders will be assigned at runtime. We
    assign the values of the placeholder using the `feed_dict` parameter. The `feed_dict`
    parameter is basically a dictionary where the key represents the name of the placeholder,
    and the value represents the value of the placeholder.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following code, we set `feed_dict = {x:5}`, which implies
    that the value for the `x` placeholder is `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code returns `8.0`.
  prefs: []
  type: TYPE_NORMAL
- en: That's it. In the next section, we will learn about TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorBoard is TensorFlow's visualization tool, which can be used to visualize
    a computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it becomes confusing when we have to debug the network. So,
    if we can visualize the computational graph in TensorBoard, we can easily understand
    such complex models, debug them, and optimize them. TensorBoard also supports
    sharing.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 8.2*, the TensorBoard panel consists of several tabs—**SCALARS**,
    **IMAGES**, **AUDIO**, **GRAPHS**, **DISTRIBUTIONS**, **HISTOGRAMS**, and **EMBEDDINGS**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: The tabs are pretty self-explanatory. The **SCALARS** tab shows useful information
    about the scalar variables we use in our program. For example, it shows how the
    value of a scalar variable called `loss` changes over several iterations.
  prefs: []
  type: TYPE_NORMAL
- en: The **GRAPHS** tab shows the computational graph. The **DISTRIBUTIONS** and
    **HISTOGRAMS** tabs show the distribution of a variable. For example, our model's
    weight distribution and histogram can be seen under these tabs. The **EMBEDDINGS**
    tab is used for visualizing high-dimensional vectors, such as word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a basic computational graph and visualize it in TensorBoard. Let''s
    say we have four constants, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s multiply `x` and `y` and `a` and `b` and save them as `prod1` and `prod2`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Add `prod1` and `prod2` and store them in `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can visualize all of these operations in TensorBoard. To visualize in
    TensorBoard, we first need to save our event files. That can be done using `tf.summary.FileWriter()`.
    It takes two important parameters, `logdir` and `graph`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name suggests, `logdir` specifies the directory where we want to store
    the graph, and `graph` specifies which graph we want to store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `./graphs` is the directory where we are storing our
    event file, and `sess.graph` specifies the current graph in our TensorFlow session.
    So, we are storing the current graph of the TensorFlow session in the `graphs`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start TensorBoard, go to your Terminal, locate the working directory, and
    type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `logdir` parameter indicates the directory where the event file is stored
    and `port` is the port number. Once you run the preceding command, open your browser
    and type `http://localhost:8000/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the TensorBoard panel, under the **GRAPHS** tab, you can see the computational
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Computational graph'
  prefs: []
  type: TYPE_NORMAL
- en: As you may notice, all of the operations we have defined are clearly shown in
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a name scope
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scoping is used to reduce complexity and helps us to better understand a model
    by grouping related nodes together. Having a name scope helps us to group similar
    operations in a graph. This comes in handy when we are building a complex architecture.
    Scoping can be created using `tf.name_scope()`. In the previous example, we performed
    two operations, `Product` and `sum`. We can simply group them into two different
    name scopes as `Product` and `sum`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how `prod1` and `prod2` perform multiplication
    and compute the result. We''ll define a name scope called `Product`, and group
    the `prod1` and `prod2` operations, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define the name scope for `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the file in the `graphs` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Visualize the graph in TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may notice, now, we have only two nodes, **sum** and **Product**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: A computational graph'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we double-click on the nodes, we can see how the computation is happening.
    As you can see, the **prod1** and **prod2** nodes are grouped under the **Product**
    scope, and their results are sent to the **sum** node, where they will be added.
    You can see how the **prod1** and **prod2** nodes compute their value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: A computational graph in detail'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding graph is just a simple example. When we are working on a complex
    project with a lot of operations, name scoping helps us to group similar operations
    together and enables us to understand the computational graph better.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about TensorFlow, in the next section, let's see how
    to build handwritten digit classification using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Handwritten digit classification using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Putting together all the concepts we have learned so far, we will see how we
    can use TensorFlow to build a neural network to recognize handwritten digits.
    If you have been playing around with deep learning of late, then you must have
    come across the MNIST dataset. It has been called the *hello world* of deep learning.
    It consists of 55,000 data points of handwritten digits (0 to 9).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will see how we can use our neural network to recognize
    these handwritten digits, and we will get the hang of TensorFlow and TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the required libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a first step, let''s import all of the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Loading the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load the dataset, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `data/mnist` implies the location where we store the
    MNIST dataset, and `one_hot=True` implies that we are one-hot encoding the labels
    (0 to 9).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see what we have in our data by executing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We have `55000` images in the training set, each image is of size `784`, and
    we have `10` labels, which are actually 0 to 9\. Similarly, we have `10000` images
    in the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll plot an input image to see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, our input image looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Image of the digit 7 from the training set'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the number of neurons in each layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll build a four-layer neural network with three hidden layers and one output
    layer. As the size of the input image is `784`, we set `num_input` to `784`, and
    since we have 10 handwritten digits (0 to 9), we set `10` neurons in the output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the number of neurons in each layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Defining placeholders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have learned, we first need to define the placeholders for `input` and
    `output`. Values for the placeholders will be fed in at runtime through `feed_dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Since we have a four-layer network, we have four weights and four biases. We
    initialize our weights by drawing values from the truncated normal distribution
    with a standard deviation of `0.1`. Remember, the dimensions of the weight matrix
    should be *the* *number of neurons in the previous layer* x *the number of neurons
    in the current layer*. For instance, the dimensions of weight matrix `w3` should
    be *the* *number of neurons in hidden layer 2* x *the number of neurons in hidden
    layer 3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We often define all of the weights in a dictionary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the bias should be the number of neurons in the current layer.
    For instance, the dimension of the `b2` bias is the number of neurons in hidden
    layer 2\. We set the bias value as a constant; `0.1` in all of the layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Forward propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we''ll define the forward propagation operation. We''ll use ReLU activations
    in all layers. In the last layers, we''ll apply `sigmoid` activation, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Computing loss and backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we''ll define our loss function. We''ll use softmax cross-entropy as
    our loss function. TensorFlow provides the `tf.nn.softmax_cross_entropy_with_logits()`
    function for computing softmax cross-entropy loss. It takes two parameters as
    inputs, `logits` and `labels`:'
  prefs: []
  type: TYPE_NORMAL
- en: The `logits` parameter specifies the `logits` predicted by our network; for
    example, `y_hat`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `labels` parameter specifies the actual labels; for example, true labels,
    `Y`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We take the mean of the `loss` function using `tf.reduce_mean()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to minimize the loss using backpropagation. Don''t worry! We don''t
    have to calculate the derivatives of all the weights manually. Instead, we can
    use TensorFlow''s `optimizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Computing accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We calculate the accuracy of our model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `y_hat` parameter denotes the predicted probability for each class of our
    model. Since we have `10` classes, we will have `10` probabilities. If the probability
    is high at position `7`, then it means that our network predicts the input image
    as digit `7` with high probability. The `tf.argmax()` function returns the index
    of the largest value. Thus, `tf.argmax(y_hat,1)` gives the index where the probability
    is high. Thus, if the probability is high at index `7`, then it returns `7`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Y` parameter denotes the actual labels, and they are the one-hot encoded
    values. That is, the `Y` parameter consists of zeros everywhere except at the
    position of the actual image, where it has a value of `1`. For instance, if the
    input image is `7`, then `Y` has a value of 0 at all indices except at index `7`,
    where it has a value of `1`. Thus, `tf.argmax(Y,1)` returns `7` because that is
    where we have a high value, `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, `tf.argmax(y_hat,1)` gives the predicted digit, and `tf.argmax(Y,1)` gives
    us the actual digit.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.equal(x, y)` function takes `x` and `y` as inputs and returns the truth
    value of *(x == y)* element-wise. Thus, `correct_pred = tf.equal(predicted_digit,actual_digit)`
    consists of `True` where the actual and predicted digits are the same, and `False`
    where the actual and predicted digits are not the same. We convert the Boolean
    values in `correct_pred` into float values using TensorFlow's cast operation,
    `tf.cast(correct_pred, tf.float32)`. After converting them into float values,
    we take the average using `tf.reduce_mean()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, `tf.reduce_mean(tf.cast(correct_pred, tf.float32))` gives us the average
    correct predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Creating a summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also visualize how the loss and accuracy of our model changes during
    several iterations in TensorBoard. So, we use `tf.summary()` to get the summary
    of the variable. Since the loss and accuracy are scalar variables, we use `tf.summary.scalar()`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we merge all of the summaries we use in our graph, using `tf.summary.merge_all()`.
    We do this because when we have many summaries, running and storing them would
    become inefficient, so we run them once in our session instead of running them
    multiple times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, it is time to train our model. As we have learned, first, we need to initialize
    all of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the batch size, number of iterations, and learning rate, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the event files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model for a number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Get a batch of data according to the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Print `loss` and `accuracy` for every 100^(th) iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may notice from the following output, the loss decreases and the accuracy
    increases over various training iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing graphs in TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After training, we can visualize our computational graph in TensorBoard, as
    shown in *Figure 8.7*. As you can see, our **Model** takes **input**, **weights**,
    and **biases** as input and returns the output. We compute **Loss** and **Accuracy**
    based on the output of the model. We minimize the loss by calculating **gradients**
    and updating **weights**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Computational graph'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we double-click and expand **Model**, we can see that we have three hidden
    layers and one output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Expanding the Model node'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can double-click and see every node. For instance, if we open
    **weights**, we can see how the four weights are initialized using truncated normal
    distribution, and how it is updated using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Expanding the weights node'
  prefs: []
  type: TYPE_NORMAL
- en: As we have learned, the computational graph helps us to understand what is happening
    on each node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how the accuracy is being calculated by double-clicking on the **Accuracy**
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: Expanding the Accuracy node'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that we also stored a summary of our `loss` and `accuracy` variables.
    We can find them under the **SCALARS** tab in TensorBoard. *Figure 8.11* shows
    how the loss decreases over iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Plot of the loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.12* shows how accuracy increases over iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_08_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 Plot of accuracy
  prefs: []
  type: TYPE_NORMAL
- en: That's it. In the next section, we will learn about another interesting feature
    in TensorFlow called eager execution.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing eager execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Eager execution in TensorFlow is more Pythonic and allows rapid prototyping.
    Unlike graph mode, where we need to construct a graph every time we want to perform
    any operation, eager execution follows the imperative programming paradigm, where
    any operation can be performed immediately, without having to create a graph,
    just like we do in Python. Hence, with eager execution, we can say goodbye to
    sessions and placeholders. It also makes the debugging process easier with an
    immediate runtime error, unlike graph mode. For instance, in graph mode, to compute
    anything, we run the session. As shown in the following code, to evaluate the
    value of `z`, we have to run the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'With eager execution, we don''t need to create a session; we can simply compute
    `z`, just like we do in Python. In order to enable eager execution, just call
    the `tf.enable_eager_execution()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'It will return the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to get the output value, we can print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Although eager execution enables the imperative programming paradigm, in this
    book, we will investigate most of the examples in non-eager mode to better understand
    the algorithms from scratch. In the next section, we will see how to perform math
    operations using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Math operations in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will explore some of the operations in TensorFlow using eager execution
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Let's start with some basic arithmetic operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `tf.add` to add two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tf.subtract` function is used for finding the difference between two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tf.multiply` function is used for multiplying two numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide two numbers using `tf.divide`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The dot product can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s find the index of the minimum and maximum elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The index of the minimum value is computed using `tf.argmin()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The index of the maximum value is computed using `tf.argmax()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following code to find the squared difference between `x` and `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Let's try typecasting; that is, converting from one data type into another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the type of `x`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We can convert the type of `x`, which is `tf.int32`, into `tf.float32`, using
    `tf.cast`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, check the `x` type. It will be `tf.float32`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate the two matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate the matrices row-wise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to concatenate the matrices column-wise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Stack the `x` matrix using the `stack` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see how to perform the `reduce_mean` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the mean value of `x`; that is, (*1.0* + *5.0* + *2.0* + *3.0*) / *4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the mean across the row; that is, (*1.0*+*5.0*)/*2,* (*2.0*+*3.0*)/*2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the mean across the column; that is, (*1.0*+*5.0*)/*2.0,* (*2.0*+*3.0*)/*2.0*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw random values from the probability distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the softmax probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Now, we'll look at how to compute the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `square` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The gradients can be computed for the preceding `square` function using `tf.GradientTape`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow 2.0 and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow 2.0 has got some really cool features. It sets the eager execution
    mode by default. It provides a simplified workflow and uses Keras as the main
    API for building deep learning models. It is also backward compatible with TensorFlow
    1.x versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install TensorFlow 2.0, open your Terminal and type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Since TensorFlow 2.0 uses Keras as a high-level API, we will look at how Keras
    works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Bonjour Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras is another popularly used deep learning library. It was developed by François
    Chollet at Google. It is well known for its fast prototyping, and it makes model
    building simple. It is a high-level library, meaning that it does not perform
    any low-level operations on its own, such as convolution. It uses a backend engine
    for doing that, such as TensorFlow. The Keras API is available in `tf.keras`,
    and TensorFlow 2.0 uses it as the primary API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building a model in Keras involves four important steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step is defining the model. Keras provides two different APIs to
    define the model:'
  prefs: []
  type: TYPE_NORMAL
- en: The sequential API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The functional API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a sequential model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In a sequential model, we stack each layer, one above another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s define our model as a `Sequential()` model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define the first layer, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `Dense` implies a fully connected layer, `input_dim`
    implies the dimension of our input, and `activation` specifies the activation
    function that we use. We can stack up as many layers as we want, one above another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the next layer with the `relu` activation function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the output layer with the `sigmoid` activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The final code block of the sequential model is shown as follows. As you can
    see, the Keras code is much simpler than the TensorFlow code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Defining a functional model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A functional model provides more flexibility than a sequential model. For instance,
    in a functional model, we can easily connect any layer to another layer, whereas,
    in a sequential model, each layer is in a stack, one above another. A functional
    model comes in handy when creating complex models, such as directed acyclic graphs,
    models with multiple input values, multiple output values, and shared layers.
    Now, we will see how to define a functional model in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to define the input dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll define our first fully connected layer with `10` neurons and `relu`
    activation, using the `Dense` class, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined `layer1`, but where is the input to `layer1` coming from? We need
    to specify the input to `layer1` in a bracket notation at the end, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the next layer, `layer2`, with `13` neurons and `relu` activation.
    The input to `layer2` comes from `layer1`, so that is added in the bracket at
    the end, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define the output layer with the `sigmoid` activation function.
    Input to the output layer comes from `layer2`, so that is added in bracket at
    the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining all of the layers, we define the model using a `Model` class,
    where we need to specify `inputs` and `outputs`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code for the functional model is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Compiling the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have defined the model, the next step is to compile it. In this
    phase, we set up how the model should learn. We define three parameters when compiling
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `optimizer` parameter: This defines the optimization algorithm we want
    to use; for example, the gradient descent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `loss` parameter: This is the objective function that we are trying to
    minimize; for example, the mean squared error or cross-entropy loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `metrics` parameter: This is the metric through which we want to assess
    the model''s performance; for example, `accuracy`. We can also specify more than
    one metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run the following code to compile the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We defined and also compiled the model. Now, we will train the model. Training
    the model can be done using the `fit` function. We specify our features, `x`;
    labels, `y`; the number of `epochs` we want to train the model for; and `batch_size`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After training the model, we will evaluate the model on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also evaluate the model on the same train set, and that will help us
    to understand the training accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: That's it. Let's see how to use TensorFlow for the MNIST digit classification
    task in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST digit classification using TensorFlow 2.0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will see how we can perform MNIST handwritten digit classification,
    using TensorFlow 2.0\. It requires only a few lines of code compared to TensorFlow
    1.x. As we have learned, TensorFlow 2.0 uses Keras as its high-level API; we just
    need to add `tf.keras` to the Keras code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a training set and a test set with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the train and test sets by dividing the values of `x` by the maximum
    value of `x`; that is, `255.0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the sequential model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add layers to the model. We use a three-layer network with the
    ReLU function in the hidden layer and softmax in the final layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model by running the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Writing code with the Keras API is that simple.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off this chapter by understanding TensorFlow and how it uses computational
    graphs. We learned that computation in TensorFlow is represented by a computational
    graph, which consists of several nodes and edges, where nodes are mathematical
    operations, such as addition and multiplication, and edges are tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned that variables are containers used to store values, and they
    are used as input to several other operations in a computational graph. We also
    learned that placeholders are like variables, where we only define the type and
    dimension but do not assign the values, and the values for the placeholders are
    fed at runtime. Moving forward, we learned about TensorBoard, which is TensorFlow's
    visualization tool and can be used to visualize a computational graph. We also
    explored eager execution, which is more Pythonic and allows rapid prototyping.
  prefs: []
  type: TYPE_NORMAL
- en: We understood that, unlike graph mode, where we need to construct a graph every
    time to perform any operation, eager execution follows the imperative programming
    paradigm, where any operation can be performed immediately, without having to
    create a graph, just like we do in Python.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we begin our **deep reinforcement learning** (**DRL**)
    journey by understanding one of the popular DRL algorithms, called the **Deep
    Q Network** (**DQN**).
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s put our knowledge of TensorFlow to the test by answering the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a TensorFlow session?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a placeholder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is TensorBoard?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is eager execution mode useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are all the steps involved in building a model using Keras?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does Keras's functional model differ from its sequential model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can learn more about TensorFlow by checking its official documentation at
    [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials).
  prefs: []
  type: TYPE_NORMAL
