<html><head></head><body>
		<div>
			<div id="_idContainer169" class="Content">
			</div>
		</div>
		<div id="_idContainer170" class="Content">
			<h1 id="_idParaDest-92"><a id="_idTextAnchor093"/>5. Classification Models</h1>
		</div>
		<div id="_idContainer227" class="Content">
			<p class="callout-heading"><a id="_idTextAnchor094"/>Overview</p>
			<p class="callout">In this chapter, you will explore different types of classification models. You will gain hands-on experience of using TensorFlow to build binary, multi-class, and multi-label classifiers. Finally, you will learn the concepts of model evaluation and how you can use different metrics to assess the performance of a model. </p>
			<p class="callout">By the end of this chapter, you will have a good understanding of what classification models are and how programming with TensorFlow works. </p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor095"/>Introduction</h1>
			<p>In the previous chapter, you learned about regression problems where the target variable is continuous. A continuous variable can take any value between a minimum and maximum value. You learned how to train such models with TensorFlow. </p>
			<p>In this chapter, you will look at another type of supervised learning problem called classification, where the target variable is discrete — meaning it can only take a finite number of values. In industry, you will most likely encounter such projects where variables are aggregated into groups such as product tiers, or classes of users, customers, or salary ranges. The objective of a classifier is to learn the patterns from the data and predict the right class for observation. </p>
			<p>For instance, in the case of a loan provider, a classification model will try to predict whether a customer is most likely to default in the coming year based on their profile and financial position. This outcome can only take two possible values (<strong class="source-inline">yes</strong> or <strong class="source-inline">no</strong>), which is a binary classification. Another classifier model could predict the ratings from 1 to 5 of a new movie for a user given their previous ratings and the information about this movie. When the outcome can be more than two possible values, you are dealing with a multi-class classification. Finally, there is a third type of classifier called multi-label where the model will predict more than a class. For example, a model will analyze an input image and predict whether there is a cat, a dog, or a mouse in the image. In such a case, the model will predict three different binary outputs (or labels).</p>
			<p>You will go through each type of classifier in this chapter, detail their specificities, and explore how to measure the performance of these models.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor096"/>Binary Classification </h1>
			<p>As mentioned previously, binary classification refers to a type of supervised learning where the target variable can only take two possible values (or classes) such as true/false or yes/no. For instance, in the medical industry, you may want to predict whether a patient is more likely to have a disease based on their personal information such as age, height, weight, and/or medical measurements. Similarly, in marketing, advertisers might utilize similar information to optimize email campaigns. </p>
			<p>Machine learning algorithms such as the random forest classifier, support vector classifier, or logistic regression work well for classification. Neural networks can also achieve good results for binary classification. It is extremely easy to turn a regression model such as those in the previous chapter into a binary classifier. There are only two key changes required: the activation function for the last layer and the loss function.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor097"/>Logistic Regression</h2>
			<p><strong class="bold">Logistic regression</strong> is one of the most popular algorithms for dealing with binary classification. As its name implies, it is an extension of the linear regression algorithm. A linear regression model predicts an output that can take an infinite number of values within a range. For logistic regression, you want your model to predict values between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. The value <strong class="source-inline">0</strong> usually corresponds to <strong class="source-inline">false</strong> (or <strong class="source-inline">no</strong>) while the value <strong class="source-inline">1</strong> refers to <strong class="source-inline">true</strong> (or <strong class="source-inline">yes</strong>). </p>
			<p>In other terms, the output of logistic regression will be the probability of it being true. For example, if the output is <strong class="source-inline">0.3</strong>, you can say there is a probability of 30% that the result should be true (or yes). But as there are only two possible values, this will also mean there is a probability of 70% (100% – 30%) of having the outcome of false (or no):</p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/B16341_05_01.jpg" alt="Figure 5.1: Output of logistic regression&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1: Output of logistic regression</p>
			<p>Now that you know what the output of logistic regression is, you just need to find a function that can transform an input value that is continuous into a value between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. Luckily, such a mathematical function does exist, and it is called the <strong class="bold">sigmoid function</strong>. The formula for this function is as follows:</p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/B16341_05_02.jpg" alt="Figure 5.2: Formula of the sigmoid function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2: Formula of the sigmoid function</p>
			<p><img src="image/B16341_05_02a.png" alt="Text&#10;&#10;Description automatically generated with medium confidence"/>corresponds to the exponential function applied to <strong class="source-inline">x</strong>. The exponential function ranges from 0 to positive infinity. So, if <strong class="source-inline">x</strong> has a value close to positive infinite, the value of sigmoid will tend to <strong class="source-inline">1</strong>. On the other hand, if <strong class="source-inline">x</strong> is very close to negative infinite, then the value of sigmoid will tend to <strong class="source-inline">0</strong>:</p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/B16341_05_03.jpg" alt="Figure 5.3: Curve of the sigmoid function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3: Curve of the sigmoid function</p>
			<p>So, applying the sigmoid function on the output of a linear regression model turns it into logistic regression. The same logic holds for neural networks: if you apply the sigmoid function on a perceptron model (linear regression), you will get a binary classifier. To do so, you just need to specify sigmoid as the activation function of the last fully connected layer of a perceptron model. In TensorFlow, you specify the <strong class="source-inline">activation</strong> parameter as:</p>
			<p class="source-code">from tensorflow.keras.layers import Dense</p>
			<p class="source-code">Dense(1, activation='sigmoid')</p>
			<p>The preceding code snippet shows how to define a fully connected layer with a single unit that can output any value and apply the sigmoid activation function to it. The result will then be within <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. Now that you know how to modify a neural network's regression model to turn it into a binary classifier, you need to specify the relevant loss function.</p>
			<h2 id="_idParaDest-96">Bina<a id="_idTextAnchor098"/>ry Cross-Entropy</h2>
			<p>In the previous section, you learned how to turn a linear regression model into a binary classifier. With neural networks, it is as simple as adding sigmoid as the activation function for the last fully connected layer. But there is another consideration that will impact the training of this model: the choice of the loss function.</p>
			<p>For linear regression, the most frequently used loss functions are <strong class="bold">mean squared error</strong> and <strong class="bold">mean absolute error</strong> as seen in <em class="italic">Chapter 4</em>, <em class="italic">Regression and Classification Models</em>. These functions will calculate the difference between the predicted and the actual values, and the neural network model will update all its weights accordingly during backpropagation. For a binary classification, the typical loss function is <strong class="bold">binary cross-entropy</strong> (also called <strong class="bold">log loss</strong>). The formula for this function is as follows:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/B16341_05_04.jpg" alt="Figure 5.4: Formula of binary cross-entropy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4: Formula of binary cross-entropy</p>
			<p><img src="image/B16341_05_04a.png" alt="A picture containing text&#10;&#10;Description automatically generated"/> represents the actual value for the observation <strong class="source-inline">i</strong>.</p>
			<p><img src="image/B16341_05_04b.png" alt="Formula"/> represents the predicted probability for the observation <strong class="source-inline">i</strong>.</p>
			<p><strong class="source-inline">N</strong> represents the total number of observations.</p>
			<p>This formula looks quite complicated, but its logic is quite simple. Consider the following example of a single observation: the actual value is <strong class="source-inline">1</strong> and the predicted probability is <strong class="source-inline">0.8</strong>. If the preceding formula is applied, the result will be as follows:</p>
			<p><img src="image/B16341_05_04c.png" alt="Formula"/></p>
			<p>Notice that the right-hand side of the equation is approximately zero:</p>
			<p><img src="image/B16341_05_04d.png" alt="Formula"/></p>
			<p>So, the loss value will be very small as the predicted value is very close to the actual one.</p>
			<p>Now consider another example where the actual value is <strong class="source-inline">0</strong> and the predicted probability is <strong class="source-inline">0.99</strong>. The result will be as follows:</p>
			<p><img src="image/B16341_05_04e.png" alt="Formula"/></p>
			<p><img src="image/B16341_05_04f.png" alt="Formula"/></p>
			<p>The loss will be high in this case as the prediction is very different from the actual value. </p>
			<p>The <strong class="bold">binary cross-entropy function</strong> is a good fit for assessing the difference between predicted and actual values for a binary classification. TensorFlow provides a class called <strong class="source-inline">BinaryCrossentropy</strong> that computes this loss:</p>
			<p class="source-code">from tensorflow.keras.losses import BinaryCrossentropy</p>
			<p class="source-code">bce = BinaryCrossentropy()</p>
			<h2 id="_idParaDest-97">Binary Clas<a id="_idTextAnchor099"/>sification Architecture</h2>
			<p>The architecture for binary classifiers is extremely similar to that of linear regression as seen in <em class="italic">Chapter 4</em>, <em class="italic">Regression and Classification Models</em>. It is composed of an input layer that reads each observation of the input dataset, an output layer responsible for predicting the response variable, and some hidden layers that learn the patterns leading to the correct predictions. The following diagram shows an example of such an architecture:</p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/B16341_05_05.jpg" alt="Figure 5.5: Architecture of the binary classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5: Architecture of the binary classifier</p>
			<p>The only difference compared to linear regression is the output, which is a probability value between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. This probability indicates the likelihood of the occurrence for one of the two possible values. As seen previously, this is achieved using the sigmoid activation function and binary cross-entropy for backpropagation.</p>
			<p>Now that you have seen all the elements to build a binary classifier, you can put this into practice with an exercise.</p>
			<h2 id="_idParaDest-98">Exercise 5.0<a id="_idTextAnchor100"/>1: Building a Logistic Regression Model</h2>
			<p>In this exercise, you will build and train a logistic regression model in TensorFlow that will predict the winning team in a game of Dota 2 using some information about the game, such as the mode and type used.</p>
			<p>You will be working on the Dota 2 dataset. Dota 2 is a popular computer game. The dataset contains information related to the game and the target variable indicates which team won. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The training dataset can be accessed here: <a href="https://packt.link/Tdvdj">https://packt.link/Tdvdj</a>.</p>
			<p class="callout">The test dataset can be accessed here: <a href="https://packt.link/4PsPN">https://packt.link/4PsPN</a>.</p>
			<p class="callout">The original dataset can be found here: <a href="https://archive.ics.uci.edu/ml/datasets/Dota2+Games+Results">https://archive.ics.uci.edu/ml/datasets/Dota2+Games+Results</a>.</p>
			<ol>
				<li>Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable called <strong class="source-inline">train_url</strong> that contains the URL to the training set:<p class="source-code">train_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">            '/The-TensorFlow-Workshop/master/Chapter05'\</p><p class="source-code">            '/dataset/dota2Train.csv'</p></li>
				<li>Load the training dataset into a <strong class="source-inline">DataFrame()</strong> function called <strong class="source-inline">X_train</strong> using <strong class="source-inline">read_csv()</strong> method, provide the URL to the CSV file, and set <strong class="source-inline">header=None</strong> as the dataset doesn't provide column names. Print the first five rows of the DataFrame using <strong class="source-inline">head()</strong>method:<p class="source-code">X_train = pd.read_csv(train_url, header=None)</p><p class="source-code">X_train.head()</p><p>The expected output will be as follows:</p><div id="_idContainer183" class="IMG---Figure"><img src="image/B16341_05_06.jpg" alt="Figure 5.6: The first five rows of the Dota 2 training set&#13;&#10;"/></div><p class="figure-caption">Figure 5.6: The first five rows of the Dota 2 training set</p><p>You can see that the dataset contains 117 columns, and they are all numeric. Note also that the target variable (column <strong class="source-inline">0</strong>) contains two different values: <strong class="source-inline">-1</strong> and <strong class="source-inline">1</strong>. As you will train a logistic regression model, the possible values should be <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. You will need to replace the <strong class="source-inline">-1</strong> values with <strong class="source-inline">0</strong>.</p></li>
				<li>Extract the target variable (column 0) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">y_train</strong>:<p class="source-code">y_train = X_train.pop(0)</p></li>
				<li>Replace all values with <strong class="source-inline">-1</strong> with <strong class="source-inline">0</strong> from the target variable using <strong class="source-inline">replace()</strong>, and print the first five rows using <strong class="source-inline">head()</strong> method:<p class="source-code">y_train = y_train.replace(-1,0)</p><p class="source-code">y_train.head()</p><p>The expected output will be as follows:</p><div id="_idContainer184" class="IMG---Figure"><img src="image/B16341_05_07.jpg" alt="Figure 5.7: The first five rows of the Dota 2 target variable from the training set&#13;&#10;"/></div><p class="figure-caption">Figure 5.7: The first five rows of the Dota 2 target variable from the training set</p><p>Now all the values from the target variable of the training set are either <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>.</p></li>
				<li>Create a variable called <strong class="source-inline">test_url</strong> that contains the URL to the test set:<p class="source-code">test_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">           '/The-TensorFlow-Workshop/master/Chapter05/dataset'\</p><p class="source-code">           '/dota2Test.csv'</p></li>
				<li>Load the test dataset into a <strong class="source-inline">DataFrame()</strong> function called <strong class="source-inline">X_test</strong> using <strong class="source-inline">read_csv()</strong> method, provide the URL to the CSV file, and set <strong class="source-inline">header=None</strong> as the dataset doesn't provide column names. Print the first five rows using <strong class="source-inline">head()</strong> method:<p class="source-code">X_test = pd.read_csv(test_url, header=None)</p><p class="source-code">X_test.head()</p><p>The expected output will be as follows:</p><div id="_idContainer185" class="IMG---Figure"><img src="image/B16341_05_08.jpg" alt="Figure 5.8: The first five rows of the Dota 2 test set&#13;&#10;"/></div><p class="figure-caption">Figure 5.8: The first five rows of the Dota 2 test set</p><p>The test set is very similar to the training one, and you will need to perform the same transformation on it.</p></li>
				<li>Extract the target variable (column 0) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">y_test</strong>:<p class="source-code">y_test = X_test.pop(0)</p></li>
				<li>Replace all values with <strong class="source-inline">-1</strong> with <strong class="source-inline">0</strong> from the target variable using <strong class="source-inline">replace()</strong> method and print the first five rows using <strong class="source-inline">head()</strong> method:<p class="source-code">y_test = y_test.replace(-1,0)</p><p class="source-code">y_test.head()</p><p>The expected output will be as follows:</p><div id="_idContainer186" class="IMG---Figure"><img src="image/B16341_05_09.jpg" alt="Figure 5.9: The first five rows of the Dota 2 target variable from the test set&#13;&#10;"/></div><p class="figure-caption">Figure 5.9: The first five rows of the Dota 2 target variable from the test set</p></li>
				<li>Import TensorFlow library and use <strong class="source-inline">tf</strong> as the alias:<p class="source-code">import tensorflow as tf</p></li>
				<li>Set the seed for TensorFlow as <strong class="source-inline">8</strong>, using <strong class="source-inline">tf.random.set_seed()</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong> and store it in a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Import the <strong class="source-inline">Dense()</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function and the input shape as <strong class="source-inline">(116,)</strong>, which corresponds to the number of features from the dataset. Save it in a variable called <strong class="source-inline">fc1</strong>:<p class="source-code">fc1 = Dense(512, input_shape=(116,), activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc2</strong>:<p class="source-code">fc2 = Dense(512, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc3</strong>:<p class="source-code">fc3 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc4</strong>:<p class="source-code">fc4 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify sigmoid as the activation function. Save it in a variable called <strong class="source-inline">fc5</strong>:<p class="source-code">fc5 = Dense(1, activation='sigmoid')</p></li>
				<li>Sequentially add all five fully connected layers to the model using <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(fc2)</p><p class="source-code">model.add(fc3)</p><p class="source-code">model.add(fc4)</p><p class="source-code">model.add(fc5)</p></li>
				<li>Print the summary of the model using <strong class="source-inline">summary()</strong> method:<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer187" class="IMG---Figure"><img src="image/B16341_05_10.jpg" alt="Figure 5.10: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 5.10: Summary of the model architecture</p><p>The preceding output shows that there are five layers in your model (as expected) and displays the number of parameters at each layer. For example, the first layer contains 59,904 parameters, and the total number of parameters for this model is 404,855. All these parameters will be trained while fitting the model.</p></li>
				<li>Instantiate a <strong class="source-inline">BinaryCrossentropy()</strong> function from <strong class="source-inline">tf.keras.losses</strong> and save it in a variable called <strong class="source-inline">loss</strong>:<p class="source-code">loss = tf.keras.losses.BinaryCrossentropy()</p></li>
				<li>Instantiate <strong class="source-inline">Adam()</strong> from <strong class="source-inline">tf.keras.optimizers</strong> with <strong class="source-inline">0.001</strong> as the learning rate and save it in a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the model using the <strong class="source-inline">compile()</strong> function and specify the optimizer and loss you just created in previous steps:<p class="source-code">model.compile(optimizer=optimizer, loss=loss)</p></li>
				<li>Start the model training process using <strong class="source-inline">fit()</strong> method on the training set for five epochs:<p class="source-code">model.fit(X_train, y_train, epochs=5)</p><p>The expected output will be as follows:</p><div id="_idContainer188" class="IMG---Figure"><img src="image/B16341_05_11.jpg" alt="Figure 5.11: Logs of the training process&#13;&#10;"/></div><p class="figure-caption">Figure 5.11: Logs of the training process</p><p>The preceding output shows the logs of each epoch during the training of the model. Note that it took around 15 seconds to process a single epoch and the loss value decreased from <strong class="source-inline">0.6923</strong> (first epoch) to <strong class="source-inline">0.6650</strong> (fifth epoch), so the model is slowly improving its performance by reducing the binary cross-entropy loss.</p></li>
				<li>Predict the results of the test set using <strong class="source-inline">predict()</strong> method. Save it in a variable called <strong class="source-inline">preds</strong> and display its first five values:<p class="source-code">preds = model.predict(X_test)</p><p class="source-code">preds[:5]</p><p>The expected output will be as follows:</p><div id="_idContainer189" class="IMG---Figure"><img src="image/B16341_05_12.jpg" alt="Figure 5.12: Predictions of the first five rows of the test set&#13;&#10;"/></div><p class="figure-caption">Figure 5.12: Predictions of the first five rows of the test set</p><p>The preceding output shows the probability of each prediction. Each value below <strong class="source-inline">0.5</strong> will be classified as <strong class="source-inline">0</strong> (first and last observation in this output) and all values greater than or equal to <strong class="source-inline">0.5</strong> will be <strong class="source-inline">1</strong> (second to fourth observations).</p></li>
				<li>Display the first five true labels of the test set:<p class="source-code">y_test[:5]</p><p>The expected output will be as follows:</p><div id="_idContainer190" class="IMG---Figure"><img src="image/B16341_05_13.jpg" alt="Figure 5.13: True labels of the first five rows of the test set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.13: True labels of the first five rows of the test set</p>
			<p>Comparing this output with the model predictions on the first five rows of the test set, there are some incorrect values: the third prediction (index <strong class="source-inline">2</strong>) should be a value of <strong class="source-inline">0</strong> and the last one should be <strong class="source-inline">0</strong>. So, out of these five observations, your binary classifiers made two mistakes. </p>
			<p>In the section ahead, you will see how to properly evaluate the performance of a model with different metrics.</p>
			<h1 id="_idParaDest-99">Metrics for Classifi<a id="_idTextAnchor101"/>ers</h1>
			<p>In the previous section, you learned how to train a binary classifier to predict the right output: either <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. In <em class="italic">Exercise 5.01</em>, <em class="italic">Building a Logistic Regression Model</em>, you looked at a few samples to assess the performance of the models that were built. Usually, you would evaluate a model not just on a small subset but on the whole dataset using a performance metric such as accuracy or F1 score.</p>
			<h2 id="_idParaDest-100">Accuracy and Null Ac<a id="_idTextAnchor102"/>curacy</h2>
			<p>One of the most widely used metrics for classification problems is accuracy. Its formula is quite simple:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B16341_05_14.jpg" alt="Figure 5.14: Formula of the accuracy metric&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14: Formula of the accuracy metric</p>
			<p>The maximum value for accuracy is <strong class="source-inline">1</strong>, which means the model correctly predicts 100% of the cases. Its minimum value is <strong class="source-inline">0</strong>, where the model can't predict any case correctly.</p>
			<p>For a binary classifier, the number of correct predictions is the number of observations with a value of <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong> as the correctly predicted value:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B16341_05_15.jpg" alt="Figure 5.15: Formula of the accuracy metric for a binary classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15: Formula of the accuracy metric for a binary classifier</p>
			<p>Say you are assessing the performance of two different binary classifiers predicting the outcome on 10,000 observations on the test set. The first model correctly predicted 5,000 instances of value <strong class="source-inline">0</strong> and 3,000 instances of value <strong class="source-inline">1</strong>. Its accuracy score will be as follows:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B16341_05_16.jpg" alt="Figure 5.16: Formula for the accuracy of model1&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16: Formula for the accuracy of model1</p>
			<p>The second model correctly predicted the value <strong class="source-inline">0</strong> for 500 cases and the value <strong class="source-inline">1</strong> for 1,500 observations. Its accuracy score will be as follows:</p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/B16341_05_17.jpg" alt="Figure 5.17: Formula for the accuracy of model2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.17: Formula for the accuracy of model2</p>
			<p>The first model predicts accurately 80% of the time, while the second model is only 20% accurate. In this case, you can say that model 1 is better than model 2.</p>
			<p>Even though <strong class="source-inline">0.8</strong> is usually a relatively good score, this does not necessarily mean your model is performing well. For instance, say your dataset contains 9,000 cases of value <strong class="source-inline">0</strong> and 1,000 cases of value <strong class="source-inline">1</strong>. A very simple model that always predicts value <strong class="source-inline">0</strong> will achieve an accuracy score of 0.9. In this case, the first model is performing even less well than this extremely simple model. This characteristic of such a model that always predicts the most frequent value of a dataset is called the <strong class="bold">null accuracy</strong>. It is used as a baseline to compare with other trained models. In the preceding example, the null accuracy is <strong class="source-inline">0.9</strong> since the simple model predicts <strong class="source-inline">0</strong>, which is correct 90% of the time.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The accuracy and null accuracy metrics are not specific to binary classification but can also be applied to other types of classification.</p>
			<p>TensorFlow provides a class, <strong class="source-inline">tf.keras.metrics.Accuracy</strong>, that can calculate the accuracy score from tensors. This class has a method called <strong class="source-inline">update_state()</strong> that takes two tensors as input parameters and will compute the accuracy score between them. You can access this score by calling the <strong class="source-inline">result()</strong> method. The output result will be a tensor. You can use the <strong class="source-inline">numpy()</strong> method to convert it into a NumPy array. Here is an example of how to calculate the accuracy score:</p>
			<p class="source-code">from tensorflow.keras.metrics import Accuracy</p>
			<p class="source-code">preds = [1, 1, 1, 1, 0, 0]</p>
			<p class="source-code">target = [1, 0, 1, 0, 1, 0]</p>
			<p class="source-code">acc = Accuracy()</p>
			<p class="source-code">acc.update_state(preds, target)</p>
			<p class="source-code">acc.result().numpy()</p>
			<p>This will result in the following accuracy score:</p>
			<p class="source-code">0.5</p>
			<p class="callout-heading">Note</p>
			<p class="callout">TensorFlow doesn't provide a class for the null accuracy metric, but you can easily compute it using <strong class="source-inline">Accuracy()</strong> and provide a tensor with only <strong class="source-inline">1</strong> (or <strong class="source-inline">0</strong>) as the predictions.</p>
			<h2 id="_idParaDest-101">Precision, Recall, and t<a id="_idTextAnchor103"/>he F1 Score</h2>
			<p>In the previous section, you learned how to use the accuracy metric to assess the performance of a model and compare it against a baseline called the null accuracy. The accuracy score is widely used as it is well known to non-technical audiences, but it does have some limitations. Consider the following example.</p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/B16341_05_18.jpg" alt="Figure 5.18: Example of model predictions versus actual values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.18: Example of model predictions versus actual values</p>
			<p>This model achieves an accuracy score of 0.981 <img src="image/B16341_05_18a.png" alt="Diagram&#10;&#10;Description automatically generated"/>, which is quite high. But if this model is used to predict whether a person has a disease, it will only predict correctly in a single case. It incorrectly predicted in nine cases that these people are not sick while they actually have the given disease. At the same time, it incorrectly predicted sickness for 10 people who were actually healthy. This model's performance, then, is clearly unsatisfactory. Unfortunately, the accuracy score is simply an overall score, and it doesn't tell you where the model is performing badly. </p>
			<p>Luckily, other metrics provide a better assessment of a model, such as precision, recall, or F1 score. All three of these metrics have the same range of values as the accuracy score: <strong class="source-inline">1</strong> is the perfect score, wherein all observations are predicted correctly, and <strong class="source-inline">0</strong> is the worst, wherein there is no correct prediction at all.</p>
			<p> But before looking at them, you need to be familiar with the following definitions:</p>
			<ul>
				<li><strong class="bold">True Positive (TP)</strong>: All the observations where the actual value and the corresponding prediction are both true</li>
				<li><strong class="bold">True Negative (TN)</strong>: All the observations where the actual value and the corresponding prediction are both false</li>
				<li><strong class="bold">False Positive (FP)</strong>: All the observations where the prediction is true, but the values are actually false</li>
				<li><strong class="bold">False Negative (FN)</strong>: All the observations where the prediction is false, but the values are actually true</li>
			</ul>
			<p>Taking the same example as <em class="italic">Figure 5.18</em>, you will get the following:</p>
			<ul>
				<li>TP = 1</li>
				<li>TN = 980</li>
				<li>FP = 10</li>
				<li>FN = 9</li>
			</ul>
			<p>This is seen in the following table:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B16341_05_19.jpg" alt="Figure 5.19: Example of TP, TN, FP, and FN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.19: Example of TP, TN, FP, and FN</p>
			<p>The precision score is a metric that assesses whether a model has predicted a lot of FPs. Its formula is as follows:</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B16341_05_20.jpg" alt="Figure 5.20: Formula of precision&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.20: Formula of precision</p>
			<p>In the preceding example, the precision score will be <img src="image/B16341_05_20a.png" alt="A picture containing text&#10;&#10;Description automatically generated"/>. You can see this model is making a lot of mistakes and has predicted a lot of FPs compared to the actual TP.</p>
			<p>Recall is used to assess the number of FNs compared to TPs. Its formula is as follows:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B16341_05_21.jpg" alt="Figure 5.21: Formula of recall&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.21: Formula of recall</p>
			<p>In the preceding example, the recall score will be <img src="image/B16341_05_21a.png" alt="Text&#10;&#10;Description automatically generated with medium confidence"/>. With this metric, you can see that the model is not performing well and is predicting a lot of FNs. </p>
			<p>Finally, the F1 score is a metric that combines both precision and recall (it is the harmonic mean of precision and recall). Its formula is as follows:</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/B16341_05_22.jpg" alt="Figure 5.22: Formula for the F1 score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.22: Formula for the F1 score</p>
			<p>Taking the same example as the preceding, the F1 score will be <img src="image/B16341_05_22a.png" alt="Formula"/></p>
			<p>The model has achieved an F1 score of <strong class="source-inline">0.095</strong>, which is very different from its accuracy score of <strong class="source-inline">0.981</strong>. So, the F1 score is a good performance metric when you want to emphasize the incorrect predictions—the score considers the number of FNs and FPs in the score, as well as the TPs and TNs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">As with accuracy, precision, and recall performance metrics, the F1 score can also be applied to other types of classification.</p>
			<p>You can easily calculate precision and recall with TensorFlow by using the respective classes of <strong class="source-inline">Precision()</strong> and <strong class="source-inline">Recall()</strong>:</p>
			<p class="source-code">from tensorflow.keras.metrics import Precision, Recall</p>
			<p class="source-code">preds = [1, 1, 1, 1, 0, 0]</p>
			<p class="source-code">target = [1, 0, 1, 0, 1, 0]</p>
			<p class="source-code">prec = Precision()</p>
			<p class="source-code">prec.update_state(preds, target)</p>
			<p class="source-code">print(f"Precision: {prec.result().numpy()}")</p>
			<p class="source-code">rec = Recall()</p>
			<p class="source-code">rec.update_state(preds, target)</p>
			<p class="source-code">print(f"Recall: {rec.result().numpy()}")</p>
			<p>This results in the following output:</p>
			<p> </p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B16341_05_23.jpg" alt="Figure 5.23: Precision and recall scores of the provided example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.23: Precision and recall scores of the provided example</p>
			<p class="callout-heading">Note</p>
			<p class="callout">TensorFlow doesn't provide a class to calculate the F1 score, but this can easily be done by creating a custom metric. This will be covered in <em class="italic">Exercise 5.02</em>, <em class="italic">Classification Evaluation Metrics</em>.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor104"/>Confusion Matrices</h2>
			<p>A confusion mat<a id="_idTextAnchor105"/>rix is not a performance metric <em class="italic">per se</em>, but more a graphical tool used to visualize the predictions of a model against the actual values. You have actually already seen an example of this in the previous section with <em class="italic">Figure 5.18</em>. </p>
			<p>A confusion matrix will show all the possible values of the predictions on one axis (for example, the horizontal axis) and the actual values on the other axis (the vertical axis). At the intersection of each combination of predicted and actual values, you will record the number of observations that fall under this case.</p>
			<p>For a binary classification, the confusion matrix will look like the following:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/B16341_05_24.jpg" alt="Figure 5.24: Confusion matrix for a binary classification&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.24: Confusion matrix for a binary classification</p>
			<p>The ideal situation will be that all the values sit on the diagonal of this matrix. This will mean your model is correctly predicting all possible values. All values outside of this diagonal are where your model made some mistakes.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Confusion matrices can also be used for multi-class classification and are not specific to binary classification only.</p>
			<p>Run the code below to see the confusion matrix:</p>
			<p class="source-code">from tensorflow.math import confusion_matrix</p>
			<p class="source-code">preds = [1, 1, 1, 1, 0, 0]</p>
			<p class="source-code">target = [1, 0, 1, 0, 1, 0]</p>
			<p class="source-code">print(confusion_matrix(target, preds))</p>
			<p>This will display the following output:</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B16341_05_25.jpg" alt="Figure 5.25: TensorFlow confusion matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.25: TensorFlow confusion matrix</p>
			<p>The preceding output shows the confusion matrix. From it, you can see that the model has predicted the following results: two TPs, one TN, two FPs, and one FN.</p>
			<p>In the next exercise, you will apply these performance metrics to the same logistic regression model that you created in <em class="italic">Exercise 5.01</em>, <em class="italic">Building a Logistic Regression Model</em>.</p>
			<h2 id="_idParaDest-103">Exercise 5.02: Classification Evalua<a id="_idTextAnchor106"/>tion Metrics</h2>
			<p>In this exercise, you will reuse the same logistic regression model as in <em class="italic">Exercise 5.01</em>, <em class="italic">Building a Logistic Regression Model</em>, and assess its performance by looking at different performance metrics: accuracy, precision, recall, and F1 score.</p>
			<p>The original dataset was shared by Stephen Tridgell from the University of Sydney.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The training dataset can be accessed here: <a href="https://packt.link/QJGpA">https://packt.link/QJGpA</a>.</p>
			<p class="callout">The test dataset can be accessed here: <a href="https://packt.link/ix5rW">https://packt.link/ix5rW</a>.</p>
			<p class="callout">The model from <em class="italic">Exercise 5.01</em>, <em class="italic">Building a Logistic Regression Model</em>, can be found here: <a href="https://packt.link/sSRQL">https://packt.link/sSRQL</a>.</p>
			<p>Now, run the following instructions:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable called <strong class="source-inline">train_url</strong> that contains the URL to the training set:<p class="source-code">train_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">            '/The-TensorFlow-Workshop/master/Chapter05/dataset'\</p><p class="source-code">            '/dota2PreparedTrain.csv'</p></li>
				<li>Load the training dataset into a <strong class="source-inline">DataFrame()</strong> function called <strong class="source-inline">X_train</strong> using <strong class="source-inline">read_csv()</strong> method, provide the URL to the CSV file, and set <strong class="source-inline">header=None</strong> as the dataset doesn't provide column names:<p class="source-code">X_train = pd.read_csv(train_url, header=None)</p></li>
				<li>Extract the target variable (column <strong class="source-inline">0</strong>) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">y_train</strong>:<p class="source-code">y_train = X_train.pop(0)</p></li>
				<li>Create a variable called <strong class="source-inline">test_url</strong> that contains the URL to the test set:<p class="source-code">test_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">           '/The-TensorFlow-Workshop/master/Chapter05/dataset'\</p><p class="source-code">           '/dota2PreparedTest.csv'</p></li>
				<li>Load the test dataset into a <strong class="source-inline">DataFrame()</strong> function called <strong class="source-inline">X_test</strong> using <strong class="source-inline">read_csv()</strong> method, provide the URL to the CSV file, and set <strong class="source-inline">header=None</strong> as the dataset doesn't provide column names:<p class="source-code">X_test = pd.read_csv(test_url, header=None)</p></li>
				<li>Extract the target variable (column <strong class="source-inline">0</strong>) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">y_test</strong>:<p class="source-code">y_test = X_test.pop(0)</p></li>
				<li>Import the <strong class="source-inline">tensorflow</strong> library using <strong class="source-inline">tf</strong> as the alias and import the <strong class="source-inline">get_file()</strong> method from <strong class="source-inline">tensorflow.keras.utils</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.utils import get_file</p></li>
				<li>Create a variable called <strong class="source-inline">model_url</strong> that contains the URL to the model:<p class="source-code">model_url = 'https://github.com/PacktWorkshops'\</p><p class="source-code">            '/The-TensorFlow-Workshop/blob/master/Chapter05'\</p><p class="source-code">            'model/exercise5_01_model.h5?raw=true'</p></li>
				<li>Download the model locally using the <strong class="source-inline">get_file()</strong> method by providing the name (<strong class="source-inline">exercise5_01_model.h5</strong>) of the file and its URL. Save the output to a variable called <strong class="source-inline">model_path</strong>:<p class="source-code">model_path = get_file('exercise5_01_model.h5', model_url)</p></li>
				<li>Load the model with <strong class="source-inline">tf.keras.models.load_model()</strong> and specify the local path to the model:<p class="source-code">model = tf.keras.models.load_model(model_path)</p></li>
				<li>Print the model summary using the <strong class="source-inline">summary()</strong> method:<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer207" class="IMG---Figure"><img src="image/B16341_05_26.jpg" alt="Figure 5.26: Summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 5.26: Summary of the model</p><p>The preceding output shows the same architecture as the model from <em class="italic">Exercise 5.01</em>, <em class="italic">Building a Logistic Regression Model</em>.</p></li>
				<li>Predict the results of the test set using <strong class="source-inline">predict()</strong> method. Save it in a variable called <strong class="source-inline">preds_proba</strong> and display its first five values:<p class="source-code">preds_proba = model.predict(X_test)</p><p class="source-code">preds_proba[:5]</p><p>The expected output will be as follows:</p><div id="_idContainer208" class="IMG---Figure"><img src="image/B16341_05_27.jpg" alt="Figure 5.27: Predicted probabilities of the test set&#13;&#10;"/></div><p class="figure-caption">Figure 5.27: Predicted probabilities of the test set</p><p>The outputs are the predicted probabilities of being <strong class="source-inline">1</strong> (or true) for each observation. You need to convert these probabilities into <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> only. To do so, you will need to consider all cases with a probability greater than or equal to <strong class="source-inline">0.5</strong> to be <strong class="source-inline">1</strong> (or true), and <strong class="source-inline">0</strong> (or false) for the records with a probability lower than <strong class="source-inline">0.5</strong>.</p></li>
				<li>Convert the predicted probabilities into <strong class="source-inline">1</strong> when the probability is greater than or equal to <strong class="source-inline">0.5</strong>, and <strong class="source-inline">0</strong> when below <strong class="source-inline">0.5</strong>. Save the results in a variable called <strong class="source-inline">preds</strong> and print its first five rows:<p class="source-code">preds = preds_proba &gt;= 0.5</p><p class="source-code">preds[:5]</p><p>The expected output will be as follows:</p><div id="_idContainer209" class="IMG---Figure"><img src="image/B16341_05_28.jpg" alt="Figure 5.28: Predictions of the test set&#13;&#10;"/></div><p class="figure-caption">Figure 5.28: Predictions of the test set</p><p>Now the predictions have been converted to binary values: true (which equals <strong class="source-inline">1</strong>) and false (which equals <strong class="source-inline">0</strong>).</p></li>
				<li>Import <strong class="source-inline">Accuracy</strong>, <strong class="source-inline">Precision</strong>, and <strong class="source-inline">Recall</strong> from <strong class="source-inline">tensorflow.keras.metrics</strong>:<p class="source-code">from tensorflow.keras.metrics import Accuracy, Precision, Recall</p></li>
				<li>Instantiate <strong class="source-inline">Accuracy</strong>, <strong class="source-inline">Precision</strong>, and <strong class="source-inline">Recall</strong> objects and save them in variables called <strong class="source-inline">acc</strong>, <strong class="source-inline">pres</strong>, and <strong class="source-inline">rec</strong>, respectively:<p class="source-code">acc = Accuracy()</p><p class="source-code">prec = Precision()</p><p class="source-code">rec = Recall()</p></li>
				<li>Calculate the accuracy score on the test set with the <strong class="source-inline">update_state()</strong>, <strong class="source-inline">result()</strong>, and <strong class="source-inline">numpy()</strong> methods. Save the results in a variable called <strong class="source-inline">acc_results</strong> and print its content:<p class="source-code">acc.update_state(preds, y_test)</p><p class="source-code">acc_results = acc.result().numpy()</p><p class="source-code">acc_results</p><p>The expected output will be as follows:</p><p class="source-code">0.59650314</p><p>This model achieved an accuracy score of <strong class="source-inline">0.597</strong>.</p></li>
				<li>Calculate the precision score on the test set with the <strong class="source-inline">update_state()</strong>, <strong class="source-inline">result()</strong>, and <strong class="source-inline">numpy()</strong> methods. Save the results in a variable called <strong class="source-inline">prec_results</strong> and print its content:<p class="source-code">prec.update_state(preds, y_test)</p><p class="source-code">prec_results = prec.result().numpy()</p><p class="source-code">prec_results</p><p>The expected output will be as follows:</p><p class="source-code">0.59578335</p><p>This model achieved a precision score of <strong class="source-inline">0.596</strong>.</p></li>
				<li>Calculate the recall score on the test set with the <strong class="source-inline">update_state()</strong>, <strong class="source-inline">result()</strong>, and <strong class="source-inline">numpy()</strong> methods. Save the results in a variable called <strong class="source-inline">rec_results</strong> and print its content:<p class="source-code">rec.update_state(preds, y_test)</p><p class="source-code">rec_results = rec.result().numpy()</p><p class="source-code">rec_results</p><p>The expected output will be as follows:</p><p class="source-code">0.6294163</p><p>This model achieved a recall score of <strong class="source-inline">0.629</strong>.</p></li>
				<li>Calculate the F1 score by applying the formula shown in the previous section. Save the result in a variable called <strong class="source-inline">f1</strong> and print its content:<p class="source-code">f1 = 2*(prec_results * rec_results) / (prec_results + rec_results)</p><p class="source-code">f1</p><p>The expected output will be as follows:</p><p class="source-code">0.6121381493171637</p><p>Overall, the model has achieved quite a low score close to <strong class="source-inline">0.6</strong> for all four different metrics: accuracy, precision, recall, and F1 score. So, this model is making almost as many correct predictions as bad ones. You may try on your own to build another model and see whether you can improve its performance.</p></li>
			</ol>
			<p>In the section ahead, you will be looking at expanding classification to more than two possible values with multi-class classification.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor107"/>Multi-Class Classification</h1>
			<p>With binary <a id="_idTextAnchor108"/>classification, you were limited to dealing with target variables that can only take two possible values: <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> (false or true). Multi-class classification can be seen as an extension of this and allows the target variable to have more than two values (or you can say binary classification is just a subset of multi-class classification). For instance, a model that predicts different levels of disease severity for a patient or another one that classifies users into different groups based on their past shopping behaviors will be multi-class classifiers.</p>
			<p>In the next section, you will dive into the softmax function, which is used for multi-class classification.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor109"/>The Softmax Function</h2>
			<p>Binary classifiers<a id="_idTextAnchor110"/> require a specific activation function for the last fully connected layer of a neural network, which is sigmoid. The activation function specific to multi-class classifiers is different. It is softmax. Its formula is as follows:</p>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<img src="image/B16341_05_29.jpg" alt="Figure 5.29: Formula of softmax function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.29: Formula of softmax function</p>
			<p><img src="image/B16341_05_29a.png" alt="Formula 1"/> corresponds to the predicted value for class <strong class="source-inline">i</strong>.</p>
			<p><img src="image/B16341_05_29b.png" alt="Formula 1"/> corresponds to the predicted value for class <strong class="source-inline">j</strong>.</p>
			<p>This formula will be applied to each possible value of the target variable. If you have 10 possible values, then this activation function will calculate 10 different softmax values. </p>
			<p>Note that softmax exponentiates the predicted values on both the numerator and the denominator. The reason behind this is that the exponential function magnifies small changes between predicted values and makes probabilities lie closer to <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong> for the purpose of interpreting the resulting output. For instance, <strong class="source-inline">exp(2) = 7.39</strong> while <strong class="source-inline">exp(2.2) = 9.03</strong>. So, if two classes have predicted values close to each other, the difference between their exponentiated values will be much bigger and therefore it will be easier to select the higher one.</p>
			<p>The result of the softmax function is between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> as the method divides the value for one class by the sum of all the classes. So, the actual output of a softmax function is the probability of the relevant class being the final prediction:</p>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="image/B16341_05_30.jpg" alt="Figure 5.30: Example of softmax transformation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.30: Example of softmax transformation</p>
			<p>In the preceding example, the target variable has five different values, and the softmax function transforms them into probabilities. The first class (<strong class="source-inline">0</strong>) is the one with the highest probability, and this will be the final prediction.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor111"/>Categorical Cross-Entropy</h2>
			<p>Multi-class classification also requires a specific loss function that is different from the binary cross-entropy for binary classifiers. For multi-class classification, the loss function is categorical cross-entropy. Its formula is as follows:</p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/B16341_05_31.jpg" alt="Figure 5.31: Formula of categorical cross-entropy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.31: Formula of categorical cross-entropy</p>
			<p><img src="image/B16341_05_31a.png" alt="Formula"/> represents the probability of the actual value for the observation <strong class="source-inline">i</strong> to be of class <strong class="source-inline">j</strong>.</p>
			<p><img src="image/B16341_05_31b.png" alt="Formula 1"/>represents the predicted probability for the observation <strong class="source-inline">i</strong> to be of class <strong class="source-inline">j</strong>.</p>
			<p>TensorFlow provides two different classes for this loss function: <strong class="source-inline">CategoricalCrossentropy()</strong> and <strong class="source-inline">SparseCategoricalCrossentropy()</strong>:</p>
			<p class="source-code">from tensorflow.keras.losses import CategoricalCrossentropy, </p>
			<p class="source-code">                                    SparseCategoricalCrossentropy</p>
			<p class="source-code">cce = CategoricalCrossentropy()</p>
			<p class="source-code">scce = SparseCategoricalCrossentropy()</p>
			<p>The difference between them lies in the format of the target variable. If the actual values are stored as a one-hot encoding representing the actual class, then you will need to use <strong class="source-inline">CategoricalCrossentropy()</strong>. On the other hand, if the response variable is stored as integers for representing the actual classes, you will have to use <strong class="source-inline">SparseCategoricalCrossentropy()</strong>:</p>
			<div>
				<div id="_idContainer217" class="IMG---Figure">
					<img src="image/B16341_05_32.jpg" alt="Figure 5.32: Loss function to be used depending on the format of the target variable&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.32: Loss function to be used depending on the format of the target variable</p>
			<p>The output of a multi-class model will be a vector containing probabilities for each class of the target variable, such as the following:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">preds_proba = np.array([0.54, 0.16, 0.09, 0.15, 0.06])</p>
			<p>The first value (<strong class="source-inline">0.54</strong>) corresponds to the probability of having the class at index 0, <strong class="source-inline">0.016</strong> is the probability of the class at index 1, while <strong class="source-inline">0.09</strong> corresponds to the probability for the class of index 2, and so on.</p>
			<p>In order to get the final prediction (that is, the class with the highest probability), you need to use the <strong class="source-inline">argmax()</strong> function, which will look at all the values from a vector, find the maximum one, and return the index associated with it:</p>
			<p class="source-code">preds_proba.argmax()</p>
			<p>This will display the following output:</p>
			<p class="source-code">0</p>
			<p>In the preceding example, the final prediction is <strong class="source-inline">class 0</strong>, which corresponds to the vector index with the highest probability (<strong class="source-inline">0.54</strong>).</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor112"/>Multi-Class Classification Architecture</h2>
			<p>The arc<a id="_idTextAnchor113"/>hitecture for a multi-class classifier is very similar to logistic regression, except that the last layer will contain more units. Each of them corresponds to a class of the target variable. For instance, if you are building a model that takes as input a vector of size 6 and predicts a response with three different values with a single hidden layer, its architecture will look like the following:</p>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="image/B16341_05_33.jpg" alt="Figure 5.33: Architecture of a multi-class classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.33: Architecture of a multi-class classifier</p>
			<p>The softmax activation function at the last layer provides a probability of occurrence for each of the possible classes: <strong class="source-inline">A</strong>, <strong class="source-inline">B</strong>, and <strong class="source-inline">C</strong>. These probabilities are dependent on each other as there should be only one class predicted at the end. If class <strong class="source-inline">A</strong> is more likely to be the prediction (as in the preceding example), then the probabilities for the remaining classes (<strong class="source-inline">B</strong> and <strong class="source-inline">C</strong>) should be lower. Note that the sum of all the class probabilities equals <strong class="source-inline">1</strong>. So, they are indeed dependent on one another.</p>
			<p>Now that you know all the building blocks, you can build a multi-class classifier in the following exercise.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor114"/>Exercise 5.03: Building a Multi-Class Model</h2>
			<p>In t<a id="_idTextAnchor115"/>his exercise, you will build and train a multi-class classifier in TensorFlow that will predict the radiator position of a space shuttle from eight different values using the nine different numerical features provided in this dataset.</p>
			<p>The target variable (last column) contains seven different levels: <strong class="source-inline">Rad.Flow</strong>, <strong class="source-inline">Fpv.Close</strong>, <strong class="source-inline">Fpv.Open</strong>, <strong class="source-inline">High</strong>, <strong class="source-inline">Bypass</strong>, <strong class="source-inline">Bpv.Close</strong>, and <strong class="source-inline">Bpv.Open</strong>. Your goal is to accurately predict one of these seven levels using the nine features from the dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The training dataset can be accessed here: <a href="https://packt.link/46iMY">https://packt.link/46iMY</a>.</p>
			<p class="callout">The test dataset can be accessed here: <a href="https://packt.link/dcNPt">https://packt.link/dcNPt</a>.</p>
			<p class="callout">The original dataset can be found here: <a href="http://archive.ics.uci.edu/ml/datasets/Statlog+%28Shuttle%29">http://archive.ics.uci.edu/ml/datasets/Statlog+%28Shuttle%29</a>.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable called <strong class="source-inline">train_url</strong> that contains the URL to the training set:<p class="source-code">train_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">            '/The-TensorFlow-Workshop/master/Chapter05'\</p><p class="source-code">            '/dataset/shuttle.trn'</p></li>
				<li>Load the training dataset into a DataFrame called <strong class="source-inline">X_train</strong> using the <strong class="source-inline">read_table()</strong> method, provide the URL to the CSV file, use <strong class="source-inline">header=None</strong> as the dataset doesn't provide column names, and use <strong class="source-inline">sep=' '</strong> as each column is separated by spaces in this dataset. Print the first five rows using <strong class="source-inline">head()</strong> method:<p class="source-code">X_train = pd.read_table(train_url, header=None, sep=' ')</p><p class="source-code">X_train.head()</p><p>The expected output will be as follows:</p><div id="_idContainer219" class="IMG---Figure"><img src="image/B16341_05_34.jpg" alt="Figure 5.34: The first five rows of the training set&#13;&#10;"/></div><p class="figure-caption">Figure 5.34: The first five rows of the training set</p><p>You can see that the dataset contains 10 columns, and they are all numeric. Also, note that the target variable (column <strong class="source-inline">9</strong>) contains different class values.</p></li>
				<li>Extract the target variable (column <strong class="source-inline">9</strong>) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">y_train</strong>:<p class="source-code">y_train = X_train.pop(9)</p></li>
				<li>Create a variable called <strong class="source-inline">test_url</strong> that contains the URL to the test set:<p class="source-code">test_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">           '/The-TensorFlow-Workshop/master/Chapter05/dataset'\</p><p class="source-code">           '/shuttle.tst'</p></li>
				<li>Load the test dataset into a DataFrame called <strong class="source-inline">X_test</strong> using <strong class="source-inline">read_table()</strong>, provide the URL to the CSV file, set <strong class="source-inline">header=None</strong> as the dataset doesn't provide column names, and use <strong class="source-inline">sep=' '</strong> as each column is separated by a space in this dataset. Print the first five rows using <strong class="source-inline">head()</strong> method.<p class="source-code">X_test = pd.read_table(test_url, header=None, sep=' ')</p><p class="source-code">X_test.head()</p><p>The expected output will be as follows:</p><div id="_idContainer220" class="IMG---Figure"><img src="image/B16341_05_35.jpg" alt="Figure 5.35: The first five rows of the test set&#13;&#10;"/></div><p class="figure-caption">Figure 5.35: The first five rows of the test set</p><p>You can see that the test set is very similar to the training one.</p></li>
				<li>Extract the target variable (column <strong class="source-inline">9</strong>) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">y_test</strong>:<p class="source-code">y_test = X_test.pop(9)</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias:<p class="source-code">import tensorflow as tf</p></li>
				<li>Set the seed for TensorFlow as <strong class="source-inline">8</strong> using <strong class="source-inline">tf.random.set_seed()</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong> and store it in a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Import the <strong class="source-inline">Dense()</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function and the input shape as <strong class="source-inline">(9,)</strong>, which corresponds to the number of features from the dataset. Save it in a variable called <strong class="source-inline">fc1</strong>:<p class="source-code">fc1 = Dense(512, input_shape=(9,), activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc2</strong>:<p class="source-code">fc2 = Dense(512, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc3</strong>:<p class="source-code">fc3 = Dense(128, activation='relu')</p></li>
				<li>Again, create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc4</strong>:<p class="source-code">fc4 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of 128 units with <strong class="source-inline">Dense()</strong> and specify softmax as the activation function. Save it in a variable called <strong class="source-inline">fc5</strong>:<p class="source-code">fc5 = Dense(8, activation='softmax')</p></li>
				<li>Sequentially add all five fully connected layers to the model using <strong class="source-inline">add()</strong> method.<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(fc2)</p><p class="source-code">model.add(fc3)</p><p class="source-code">model.add(fc4)</p><p class="source-code">model.add(fc5)</p></li>
				<li>Print the summary of the model using <strong class="source-inline">summary()</strong> method:<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer221" class="IMG---Figure"><img src="image/B16341_05_36.jpg" alt="Figure 5.36: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 5.36: Summary of the model architecture</p><p>The preceding output shows that there are five layers in your model (as expected) and tells you the number of parameters at each layer. For example, the first layer contains <strong class="source-inline">5,120</strong> parameters and the total number of parameters for this model is <strong class="source-inline">350,984</strong>. All these parameters will be trained while fitting the model.</p></li>
				<li>Instantiate <strong class="source-inline">SparseCategoricalCrossentropy()</strong> from <strong class="source-inline">tf.keras.losses</strong> and save it in a variable called <strong class="source-inline">loss</strong>:<p class="source-code">loss = tf.keras.losses.SparseCategoricalCrossentropy()</p></li>
				<li>Instantiate <strong class="source-inline">Adam()</strong> from <strong class="source-inline">tf.keras.optimizers</strong> with <strong class="source-inline">0.001</strong> as the learning rate and save it in a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the model using the <strong class="source-inline">compile()</strong> method and specify the optimizer and loss parameters, with accuracy as the metric to be reported:<p class="source-code">model.compile(optimizer=optimizer, loss=loss, \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Start the model training process using <strong class="source-inline">fit()</strong> method on the training set for five epochs:<p class="source-code">model.fit(X_train, y_train, epochs=5)</p><p>The expected output will be as follows:</p><div id="_idContainer222" class="IMG---Figure"><img src="image/B16341_05_37.jpg" alt="Figure 5.37: Logs of the training process&#13;&#10;"/></div><p class="figure-caption">Figure 5.37: Logs of the training process</p><p>The preceding output shows the logs of each epoch during the training of the model. Note that it took around 7 seconds to process a single epoch, and the loss value decreased from <strong class="source-inline">0.5859</strong> (first epoch) to <strong class="source-inline">0.0351</strong> (fifth epoch).</p></li>
				<li>Evaluate the performance of the model on the test set using the <strong class="source-inline">evaluate()</strong> method:<p class="source-code">model.evaluate(X_test, y_test)</p><p>The expected output will be as follows:</p><div id="_idContainer223" class="IMG---Figure"><img src="image/B16341_05_38.jpg" alt="Figure 5.38: Performance of the model on the test set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.38: Performance of the model on the test set</p>
			<p>In this exercise, you learned how to build and train a multi-class classifier to predict an outcome composed of eight different classes. Your model achieved an accuracy score close to <strong class="source-inline">0.997</strong> on both the training and test sets, which is quite remarkable. This implies that your model correctly predicts the right class in the majority of cases.</p>
			<p>Now, let's consolidate your learning in the following activity.</p>
			<h2 id="_idParaDest-109">Activity 5.01: Building a Character Recognition Model<a id="_idTextAnchor116"/><a id="_idTextAnchor117"/> with TensorFlow</h2>
			<p>In this activity, you are tasked with building and training a multi-class classifier that will recognize the 26 letters of the alphabet from images. In this dataset, the images have been converted into 16 different statistical measures that will constitute our features. The goal of this model is to determine which of the 26 characters each observation belongs to.</p>
			<p>The original dataset was shared by David J. Slate of the Odesta Corporation, and can be found here: <a href="http://archive.ics.uci.edu/ml/datasets/Letter+Recognition">http://archive.ics.uci.edu/ml/datasets/Letter+Recognition</a>.</p>
			<p>The dataset can be accessed from here: <a href="https://packt.link/j8m3L">https://packt.link/j8m3L</a>.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Load the data with <strong class="source-inline">read_csv()</strong> from pandas.</li>
				<li>Extract the target variable with <strong class="source-inline">pop()</strong> method from pandas.</li>
				<li>Split the data into training (the first 15,000 rows) and test (the last 5,000 rows) sets.</li>
				<li>Build the multi-class classifier with five fully connected layers of <strong class="source-inline">512</strong>, <strong class="source-inline">512</strong>, <strong class="source-inline">128</strong>, <strong class="source-inline">128</strong>, and <strong class="source-inline">26</strong> units, respectively.</li>
				<li>Train this model on the training set.</li>
				<li>Evaluate its performance on the test set with <strong class="source-inline">evaluate()</strong> method from TensorFlow.</li>
				<li>Print the confusion matrix with <strong class="source-inline">confusion_matrix()</strong> from TensorFlow.<p>The expected output is as follows:</p><p> </p><div id="_idContainer224" class="IMG---Figure"><img src="image/B16341_05_39.jpg" alt="Figure 5.39: Confusion matrix of the test set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.39: Confusion matrix of the test set</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor265">this link</a>.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor118"/>Multi-Label Classification </h1>
			<p>Multi-label classification<a id="_idTextAnchor119"/> is another type of classification where you predict not only one target variable as in binary or multi-class classification, but several response variables at the same time. For instance, you can predict multiple outputs for the different objects present in an image (for instance, a model will predict whether there is a cat, a man, and a car in a given picture) or you can predict multiple topics for an article (such as whether the article is about the economy, international news, and manufacturing).</p>
			<p>Implementing a multi-label classification with neural networks is extremely easy, and you have already learned everything required to build one. In TensorFlow, a multi-label classifier's architecture will look the same as for multi-class, with a final output layer with multiple units corresponding to the number of target variables you want to predict. But instead of using softmax as the activation function and categorical cross-entropy as the loss function, you will use sigmoid and binary cross-entropy as the activation and loss functions, respectively.</p>
			<p>The sigmoid function will predict the probability of occurrence for each target variable:</p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/B16341_05_40.jpg" alt="Figure 5.40: Architecture of the multi-label classifier&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.40: Architecture of the multi-label classifier</p>
			<p>In the preceding example, you have three target variables and each of them has a probability of occurrence that is independent of the others (their sum will not equal 1). This model predicts that targets <strong class="source-inline">2</strong> and <strong class="source-inline">3</strong> are very likely to be the outputs for this observation. </p>
			<p>Conceptually, multi-label classification combines several logistic regression models. They will share the same parameters (weights and biases) but with independent binary outputs. The last layer of the example of a multi-class classifier in TensorFlow will look like this:</p>
			<p class="source-code">from tensorflow.keras.layers import Dense</p>
			<p class="source-code">Dense(3, activation='sigmoid')</p>
			<p>The loss function to be used will be binary cross-entropy:</p>
			<p class="source-code">from tensorflow.keras.losses import BinaryCrossentropy</p>
			<p class="source-code">bce = BinaryCrossentropy()</p>
			<p>Now, put into action what you have learned so far in th<a id="_idTextAnchor120"/><a id="_idTextAnchor121"/>e following activity.</p>
			<h2 id="_idParaDest-111">Activity 5.02: Building a Movie Genre Tagging a Model w<a id="_idTextAnchor122"/>ith TensorFlow</h2>
			<p>In this activity, you are tasked with building and training a multi-label classifier that will predict the genre of a movie from 28 possible values. Each movie can be assigned to multiple genres at a time. The features are the top keywords extracted from its synopsis. The dataset used for this activity is a subset of the original one and contains only 20,000 rows.</p>
			<p>The original dataset was shared by IMDb and can be found here: <a href="http://www.uco.es/kdis/mllresources/#ImdbDesc">http://www.uco.es/kdis/mllresources/#ImdbDesc</a>.</p>
			<p>The features of the dataset can be accessed from here: <a href="https://packt.link/yW5ru">https://packt.link/yW5ru</a>.</p>
			<p>The targets of the dataset can be accessed from here: <a href="https://packt.link/8f1mb">https://packt.link/8f1mb</a>.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Load the features and targets with <strong class="source-inline">read_csv()</strong> from pandas.</li>
				<li>Split the data into training (the first 15,000 rows) and test (the last 5,000 rows) sets.</li>
				<li>Build the multi-class classifier with five fully connected layers of <strong class="source-inline">512</strong>, <strong class="source-inline">512</strong>, <strong class="source-inline">128</strong>, <strong class="source-inline">128</strong>, and <strong class="source-inline">28</strong> units, respectively.</li>
				<li>Train this model on the training set.</li>
				<li>Evaluate its performance on the test set with <strong class="source-inline">evaluate()</strong> method from TensorFlow.<p>The expected output is as follows:</p><div id="_idContainer226" class="IMG---Figure"><img src="image/B16341_05_41.jpg" alt="Formula"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.41: Expected output of Activity 5.02</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor267">this link</a>.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor123"/>Summary</h1>
			<p>You started your journey in this chapter with an<a id="_idTextAnchor124"/> introduction to classification models and their differences compared with regression models. You learned that the target variable for classifiers can only contain a limited number of possible values.</p>
			<p>You then explored binary classification, wherein the response variable can only be from two possible values: <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. You uncovered the specificities for building a logistic regression model with TensorFlow using the sigmoid activation function and binary cross-entropy as the loss function, and you built your own binary classifier for predicting the winning team on the video game Dota 2.</p>
			<p>After this, you went through the different performance metrics that can be used to assess the performance of classifier models. You practiced calculating accuracy, precision, recall, and F1 scores with TensorFlow, and also plotted a confusion matrix, which is a visual tool to see where the model made correct and incorrect predictions.</p>
			<p>Then you dove into the topic of multi-class classification. The difference between such models and binary classifiers is that their response variables can take more than two possible values. You looked at the softmax activation function and the categorical cross-entropy loss function, which are used for training such models in TensorFlow.</p>
			<p>Finally, in the last section, you learned about multi-label classification, wherein the output can be multiple classes at the same time. In TensorFlow, such models can be easily built by constructing an architecture similar to multi-class classification but using sigmoid and binary cross-entropy, respectively, as the activation and loss functions.</p>
			<p>In the next chapter, you will learn how to prevent model overfitting by applying some regularization techniques, which will help models to better generalize unseen data.</p>
		</div>
	</body></html>