["```\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import matplotlib.pyplot as plt\n    import numpy as np\n    data_dir = tf.keras.utils.get_file(\n        'flower_photos',\n        'https://storage.googleapis.com/download.tensorflow.\n         org/example_images/flower_photos.tgz',\n        untar=True)\n    print(data_dir)\n    ```", "```\n    !ls -lrt {data_dir}\n    ```", "```\n    -rw-r----- 1 jupyter jupyter 418049 Feb  9  2016 LICENSE.txt\n    drwx------ 2 jupyter jupyter  45056 Feb 10  2016 tulips\n    drwx------ 2 jupyter jupyter  36864 Feb 10  2016 sunflowers\n    drwx------ 2 jupyter jupyter  36864 Feb 10  2016 roses\n    drwx------ 2 jupyter jupyter  45056 Feb 10  2016 dandelion\n    drwx------ 2 jupyter jupyter  36864 Feb 10  2016 daisy\n    ```", "```\n    pixels =224\n    BATCH_SIZE = 32 \n    IMAGE_SIZE = (pixels, pixels)  \n    ```", "```\n    datagen_kwargs = dict(rescale=1./255, \n                          validation_split=.20)\n    dataflow_kwargs = dict(target_size=IMAGE_SIZE, \n                           batch_size=BATCH_SIZE,\n                           interpolation='bilinear')\n    ```", "```\n    valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        **datagen_kwargs)\n    valid_generator = valid_datagen.flow_from_directory(\n    data_dir, subset='validation', shuffle=False, **dataflow_kwargs)\n    ```", "```\n    Found 731 images belonging to 5 classes.\n    ```", "```\n    rotation_range\n    horizontal_flip\n    Width_shift_range\n    height_shift_range\n    Shear_range\n    Zoom_range\n    ```", "```\n    do_data_augmentation = False \n    if do_data_augmentation:\n      train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n          rotation_range=40,\n          horizontal_flip=True,\n          width_shift_range=0.2, height_shift_range=0.2,\n          shear_range=0.2, zoom_range=0.2,\n          **datagen_kwargs)\n    else:\n      train_datagen = valid_datagen\n    train_generator = \n    \ttrain_datagen.flow_from_directory(\n    \t\tdata_dir, subset='training', shuffle=True, \t\t\t**dataflow_kwargs)\n    ```", "```\n    Found 731 images belonging to 5 classes.\n    Found 2939 images belonging to 5 classes.\n    ```", "```\n    labels_idx = (train_generator.class_indices)\n    ```", "```\n    idx_labels = dict((v,k) for k,v in labels_idx.items())\n    print(idx_labels)\n    {0: 'daisy', 1: 'dandelion', 2: 'roses', 3: 'sunflowers', 4: 'tulips'}\n    ```", "```\n    FINE_TUNING_CHOICE = True\n    NUM_CLASSES = len(idx_labels)\n    ```", "```\n    mdl = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + \n                                   (3,)),\n        hub.KerasLayer('https://tfhub.dev/google/imagenet/    \n        resnet_v2_50/feature_vector/4', \n        trainable = FINE_TUNING_CHOICE),\n        tf.keras.layers.Dense(NUM_CLASSES, \n        activation='softmax', name = 'custom_class')\n    ])\n    ```", "```\n    mdl.build([None, 224, 224, 3])\n    ```", "```\n    mdl.summary()\n    ```", "```\n    Model: 'sequential_1'\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    keras_layer_1 (KerasLayer)   (None, 2048)              23564800  \n    _________________________________________________________________\n    custom_class (Dense)         (None, 5)                 10245     \n    =================================================================\n    Total params: 23,575,045\n    Trainable params: 23,529,605\n    Non-trainable params: 45,440\n    _________________________________________________________________\n    ```", "```\nmy_optimizer = tf.keras.optimizers.SGD(lr=0.005, momentum=0.9)\n```", "```\nmy_loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n```", "```\nmdl.compile(\n```", "```\n  optimizer=my_optimizer,\n```", "```\n  loss=my_loss_function,\n```", "```\n  metrics=['accuracy'])\n```", "```\nsteps_per_epoch = train_generator.samples // train_generator.batch_size\n```", "```\nvalidation_steps = valid_generator.samples // valid_generator.batch_size\n```", "```\nhist = mdl.fit(\n```", "```\n    train_generator,\n```", "```\n    epochs=5, steps_per_epoch=steps_per_epoch,\n```", "```\n    validation_data=valid_generator,\n```", "```\n    validation_steps=validation_steps).history\n```", "```\nEpoch 1/5\n```", "```\n91/91 [==============================] - 404s 4s/step - loss: 1.4899 - accuracy: 0.7348 - val_loss: 1.3749 - val_accuracy: 0.8565\n```", "```\nEpoch 2/5\n```", "```\n91/91 [==============================] - 404s 4s/step - loss: 1.3083 - accuracy: 0.9309 - val_loss: 1.3359 - val_accuracy: 0.8963\n```", "```\nEpoch 3/5\n```", "```\n91/91 [==============================] - 405s 4s/step - loss: 1.2723 - accuracy: 0.9704 - val_loss: 1.3282 - val_accuracy: 0.9077\n```", "```\nEpoch 4/5\n```", "```\n91/91 [==============================] - 1259s 14s/step - loss: 1.2554 - accuracy: 0.9869 - val_loss: 1.3302 - val_accuracy: 0.9020\n```", "```\nEpoch 5/5\n```", "```\n91/91 [==============================] - 403s 4s/step - loss: 1.2487 - accuracy: 0.9935 - val_loss: 1.3307 - val_accuracy: 0.8963\n```", "```\n    !wget https://dataverse.harvard.edu/api/access/datafile/4159750\n    ```", "```\n    /flower_photos/small_test directory available in the left panel of your notebook instance. \n    ```", "```\n    working_dir = ‘flower_photos/small_test’\n    test_generator =     \n    \ttrain_datagen.flow_from_directory\n    \t\t(directory=working_dir,\n                batch_size = 5,\n                target_size = [224, 224],\n                shuffle=False,\n                classes = list(labels_idx))\n    ```", "```\n    print(test_generator.class_indices)\n    ```", "```\n    {'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}\n    ```", "```\n    def plotImages(images_arr):\n        fig, axes = plt.subplots(1, 5, figsize=(10,10))\n        axes = axes.flatten()\n        for img, ax in zip( images_arr, axes):\n            ax.imshow(img)\n            ax.axis('off')\n        plt.tight_layout()\n        plt.show()\n    ```", "```\n    sample_test_images, ground_truth_labels = next(test_generator)\n    print(ground_truth_labels)\n    ```", "```\n    [[1\\. 0\\. 0\\. 0\\. 0.]\n     [1\\. 0\\. 0\\. 0\\. 0.]\n     [1\\. 0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0\\. 1.]\n     [0\\. 0\\. 0\\. 0\\. 1.]]\n    ```", "```\n    plotImages(sample_test_images[:5])\n    ```", "```\n    prediction = mdl.predict(sample_test_images[:5])\n    ```", "```\n    array([[9.9985600e-01, 3.2907694e-05, 2.3326173e-05,            \n            6.8752386e-05, 1.8940274e-05],\n           [9.9998152e-01, 7.6931758e-07, 9.4449973e-07, \n            1.6520202e-05, 2.8859478e-07],\n           [9.9977893e-01, 2.0959340e-05, 6.2238797e-07,         \n            1.8358800e-04, 1.6017557e-05],\n           [6.7357789e-04, 5.8116650e-05, 3.0710772e-04, \n            6.2863214e-04, 9.9833256e-01],\n           [1.9417066e-04, 1.3316995e-04, 6.2624150e-04, \n            1.4169540e-04, 9.9890471e-01]], dtype=float32)\n    ```", "```\n    labelings = tf.math.argmax(prediction, axis = -1)\n    label_reference = np.asarray(list(labels_idx))\n    ```", "```\n    def find_label(idx):\n        return label_reference[idx]\n    ```", "```\n    predicted_idx = tf.math.argmax(prediction, axis = -1)\n    ```", "```\n    <tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 4, 4])>\n    ```", "```\n    import pandas as pd\n    predicted_label = list(map(find_label, predicted_idx))\n    file_name = test_generator.filenames\n    results=pd.DataFrame({'File':file_name,\n                          'Prediction':predicted_label})\n    results\n    ```", "```\n    DATASET_GCP_PROJECT_ID = 'bigquery-public-data'\n    DATASET_ID = 'covid19_geotab_mobility_impact'\n    TABLE_ID = 'us_border_volumes'\n    ```", "```\n    SELECT * FROM `bigquery-public-data.covid19_geotab_mobility_impact.us_border_volumes` ORDER BY RAND() LIMIT 1000\n    ```", "```\n    import tensorflow as tf\n    from tensorflow import feature_column\n    from tensorflow_io.bigquery import BigQueryClient\n    import numpy as np\n    from google.cloud import bigquery\n    client = BigQueryClient()\n    PROJECT_ID = 'project1-XXXXX' \n    # A project ID in your GCP subscription.\n    DATASET_GCP_PROJECT_ID = 'bigquery-public-data'\n    DATASET_ID = 'covid19_geotab_mobility_impact'\n    TABLE_ID = 'us_border_volumes'\n    ```", "```\n    read_session3 = client.read_session(\n       'projects/' + PROJECT_ID,\n       DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,\n       ['trip_direction',\n        'day_type',\n        'day_of_week',\n        'avg_crossing_duration',\n        'percent_of_normal_volume',\n        'avg_crossing_duration_truck',\n        'percent_of_normal_volume_truck'\n\n        ],\n       [tf.string,\n        tf.string,\n        tf.int64,\n        tf.double,\n        tf.int64,\n        tf.double,\n        tf.int64\n        ],\n         requested_streams=10\n    )\n    dataset3 = read_session3.parallel_read_rows()\n    ```", "```\n    def transfrom_row(row_dict):\n    \t# Identify column names for features.\n    \tfeature_dict = { column:\n            (tf.strings.strip(tensor) if tensor.dtype ==   \n                'string' else tensor)\n                    \tfor (column,tensor) in row_dict.items()\n                    \t}\n    \t# Remove target column from data\n    \ttarget = feature_dict.pop\n                              ('avg_crossing_duration_truck')\n    \t# Return a tuple of features and target\n    \treturn (feature_dict, target)\n    ```", "```\n    transformed_ds = dataset3.map(transfrom_row)\n    ```", "```\n    BATCH_SIZE = 32\n    SHUFFLE_BUFFER = 1024\n    training_dataset3 = transformed_ds.shuffle\n                           (SHUFFLE_BUFFER).batch(BATCH_SIZE)\n    ```", "```\n    def get_categorical_feature_values(column):\n        query = 'SELECT DISTINCT TRIM({}) FROM `{}`.{}.{}'. \t        format(column, DATASET_GCP_PROJECT_ID, \n                    DATASET_ID, TABLE_ID)\n        client = bigquery.Client(project=PROJECT_ID)\n        dataset_ref = client.dataset(DATASET_ID)\n        job_config = bigquery.QueryJobConfig()\n        query_job = client.query(query, \n                                 job_config=job_config)\n        result = query_job.to_dataframe()\n        return result.values[:,0]\n    ```", "```\n    feature_columns = []\n    # Numeric columns\n    for header in ['day_of_week',     \n                 'avg_crossing_duration',\n                 'percent_of_normal_volume',\n                 'percent_of_normal_volume_truck']:\n     feature_columns.append\n                      (feature_column.numeric_column(header))\n    # Categorical columns\n    for header in ['trip_direction', 'day_type']:\n     categorical_feature = feature_column.categorical_column_with_vocabulary_list(\n           header, get_categorical_feature_values(header))\n     categorical_feature_one_hot = feature_column.indicator_column(categorical_feature)\n     feature_columns.append(categorical_feature_one_hot)\n    ```", "```\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n    Dense = tf.keras.layers.Dense\n    model = tf.keras.Sequential(\n     [\n       feature_layer,\n       Dense(100, activation=tf.nn.relu, \n       kernel_initializer='uniform'),\n       Dense(75, activation=tf.nn.relu),\n       Dense(50, activation=tf.nn.relu),\n       Dense(25, activation=tf.nn.relu),\n       Dense(1)\n     ])  \n    ```", "```\n    model.compile(\n       loss='mse',\n       metrics=['mae', 'mse'])\n    ```", "```\n    model.fit(training_dataset3, epochs=5)\n    ```", "```\n    test_samples = {\n       'trip_direction' : np.array(['Mexico to US', \n                                    'US to Canada']),\n       'day_type' : np.array(['Weekdays', 'Weekends']),\n       'day_of_week' : np.array([4, 7]),\n       'avg_crossing_duration' : np.array([32.8, 10.4]),\n       'percent_of_normal_volume' : np.array([102, 89]),\n       'percent_of_normal_volume_truck' : np.array([106, 84])\n    }\n    ```", "```\n    model.predict(test_samples)\n    ```", "```\n    array([[29.453201],\n           [10.395596]], dtype=float32)\n    ```", "```\nDATASET_GCP_PROJECT_ID = 'bigquery-public-data'\n```", "```\nDATASET_ID = 'covid19_geotab_mobility_impact'\n```", "```\nTABLE_ID = 'us_border_volumes'\n```", "```\nlinear_est = tf.estimator.LinearRegressor(feature_columns=feature_columns, model_dir=MODEL_DIR)\n```", "```\nlinear_est.train(input_fn)\n```", "```\n    import tensorflow as tf\n    from tensorflow_io.bigquery import BigQueryClient\n    from tensorflow import feature_column\n    from google.cloud import bigquery\n    import pandas as pd\n    import numpy as np\n    import datetime, os\n    import itertools\n    ```", "```\n    PROJECT_ID = '<YOUR_PROJECT_ID>'\n    DATASET_GCP_PROJECT_ID = 'bigquery-public-data'\n    DATASET_ID = 'covid19_geotab_mobility_impact'\n    TABLE_ID = 'us_border_volumes'\n    ```", "```\n    def input_fn():\n     PROJECT_ID = 'project1-190517' # This is from what you created in your Google Cloud Account.\n     DATASET_GCP_PROJECT_ID = 'bigquery-public-data'\n     TABLE_ID = 'us_border_volumes'\n     DATASET_ID = 'covid19_geotab_mobility_impact'  \n     client = BigQueryClient()\n     read_session = client.read_session(\n       'projects/' + PROJECT_ID,\n       DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,\n       ['trip_direction',\n        'day_type',\n        'day_of_week',\n        'avg_crossing_duration',\n        'percent_of_normal_volume',\n        'avg_crossing_duration_truck',\n        'percent_of_normal_volume_truck'\n\n        ],\n       [tf.string,\n        tf.string,\n        tf.int64,\n        tf.double,\n        tf.int64,\n        tf.double,\n        tf.int64\n        ],\n         requested_streams=10\n       )\n     dataset = read_session.parallel_read_rows()\n    ```", "```\n    def transform_row(row_dict):\n       # Trim all string tensors\n       feature_dict = { column:\n           (tf.strings.strip(tensor) if tensor.dtype == \t \t           'string' else tensor)\n           for (column,tensor) in row_dict.items()\n       }\n       # Extract target from features\n       target = feature_dict.pop(\n                               'avg_crossing_duration_truck')\n       # return a tuple of features and target\n       return (feature_dict, target)\n     transformed_ds = dataset.map(transfrom_row)\n     transformed_ds = transformed_ds.batch(32)\n     return transformed_ds  \n    ```", "```\n    feature_columns = []\n    # Numeric columns\n    for header in ['day_of_week',     \n                 'avg_crossing_duration',\n                 'percent_of_normal_volume',\n                 'percent_of_normal_volume_truck']:\n     feature_columns.append(\n                       feature_column.numeric_column(header))\n    # Categorical columns\n    for header in ['trip_direction', 'day_type']:\n     categorical_feature = feature_column.categorical_column_with_vocabulary_list(\n           header, get_categorical_feature_values(header))\n     categorical_feature_one_hot = feature_column.indicator_column(categorical_feature)\n     feature_columns.append(categorical_feature_one_hot)\n    ```", "```\n    MODEL_DIR = os.path.join('models', datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n    ```", "```\n    %mkdir models\n    %mkdir {MODEL_DIR}\n    ```", "```\n    linear_est = tf.estimator.LinearRegressor(feature_columns=feature_columns, model_dir=MODEL_DIR)\n    linear_est.train(input_fn)\n    ```", "```\n    test_samples = {\n       'trip_direction' : np.array(['Mexico to US', \n                                    'US to Canada']),\n       'day_type' : np.array(['Weekdays', 'Weekends']),\n       'day_of_week' : np.array([4, 7]),\n       'avg_crossing_duration' : np.array([32.8, 10.4]),\n       'percent_of_normal_volume' : np.array([102, 89]),\n       'percent_of_normal_volume_truck' : np.array([106, 84])\n    }\n    ```", "```\n    def scoring_input_fn():\n     return tf.data.Dataset.from_tensor_slices(test_samples).batch(2)\n    ```", "```\n    y = linear_est.predict(   \n            input_fn=scoring_input_fn)\n    ```", "```\n    predictions = list(p['predictions'] for p in itertools.islice(y, 2))\n    print('Predictions: {}'.format(str(predictions)))\n    Above code prints the output:\n    Predictions: [array([23.875168], dtype=float32), array([13.621282], dtype=float32)]   \n    ```"]