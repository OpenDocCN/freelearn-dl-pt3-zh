["```\nimport numpy as np\n\nclass Neuron(object):\n    \"\"\"A simple feed-forward artificial neuron.\n    Args:\n        num_inputs (int): The input vector size / number of input values.\n        activation_fn (callable): The activation function.\n    Attributes:\n        W (ndarray): The weight values for each input.\n        b (float): The bias value, added to the weighted sum.\n        activation_fn (callable): The activation function.\n    \"\"\"\n    def __init__(self, num_inputs, activation_fn):\n        super().__init__()\n        # Randomly initializing the weight vector and bias value:\n        self.W = np.random.rand(num_inputs)\n        self.b = np.random.rand(1)\n        self.activation_fn = activation_fn\n\n    def forward(self, x):\n        \"\"\"Forward the input signal through the neuron.\"\"\"\n        z = np.dot(x, self.W) + self.b\n        return self.activation_function(z)\n```", "```\n# Fixing the random number generator's seed, for reproducible results:\nnp.random.seed(42)\n# Random input column array of 3 values (shape = `(1, 3)`)\nx = np.random.rand(3).reshape(1, 3)\n# > [[0.37454012 0.95071431 0.73199394]]\n\n# Instantiating a Perceptron (simple neuron with step function):\nstep_fn = lambda y: 0 if y <= 0 else 1\nperceptron = Neuron(num_inputs=x.size, activation_fn=step_fn)\n# > perceptron.weights    = [0.59865848 0.15601864 0.15599452]\n# > perceptron.bias       = [0.05808361]\n\nout = perceptron.forward(x)\n# > 1\n```", "```\nimport numpy as np\n\nclass FullyConnectedLayer(object):\n    \"\"\"A simple fully-connected NN layer.\n    Args:\n        num_inputs (int): The input vector size/number of input values.\n        layer_size (int): The output vector size/number of neurons.\n        activation_fn (callable): The activation function for this layer.\n    Attributes:\n        W (ndarray): The weight values for each input.\n        b (ndarray): The bias value, added to the weighted sum.\n        size (int): The layer size/number of neurons.\n        activation_fn (callable): The neurons' activation function.\n    \"\"\"\n    def __init__(self, num_inputs, layer_size, activation_fn):\n        super().__init__()\n        # Randomly initializing the parameters (using a normal distribution this time):\n        self.W = np.random.standard_normal((num_inputs, layer_size))\n        self.b = np.random.standard_normal(layer_size)\n        self.size = layer_size\n        self.activation_fn = activation_fn\n\n    def forward(self, x):\n        \"\"\"Forward the input signal through the layer.\"\"\"\n        z = np.dot(x, self.W) + self.b\n        return self.activation_fn(z)\n```", "```\nnp.random.seed(42)\n# Random input column-vectors of 2 values (shape = `(1, 2)`):\nx1 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n# > [[-0.25091976  0.90142861]]\nx2 = np.random.uniform(-1, 1, 2).reshape(1, 2)    \n# > [[0.46398788 0.19731697]]\n\nrelu_fn = lambda y: np.maximum(y, 0)    # Defining our activation function\nlayer = FullyConnectedLayer(2, 3, relu_fn)\n\n# Our layer can process x1 and x2 separately...\nout1 = layer.forward(x1)\n# > [[0.28712364 0\\.         0.33478571]]\nout2 = layer.forward(x2)\n# > [[0\\.         0\\.         1.08175419]]\n# ... or together:\nx12 = np.concatenate((x1, x2))  # stack of input vectors, of shape `(2, 2)`\nout12 = layer.forward(x12)\n# > [[0.28712364 0\\.         0.33478571]\n#    [0\\.         0\\.         1.08175419]]\n\n```", "```\nimport numpy as np\nimport mnist\nnp.random.seed(42)\n\n# Loading the training and testing data:\nX_train, y_train = mnist.train_images(), mnist.train_labels()\nX_test,  y_test  = mnist.test_images(), mnist.test_labels()\nnum_classes = 10    # classes are the digits from 0 to 9\n\n# We transform the images into column vectors (as inputs for our NN):\nX_train, X_test = X_train.reshape(-1, 28*28), X_test.reshape(-1, 28*28)\n# We \"one-hot\" the labels (as targets for our NN), for instance, transform label `4` into vector `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`:\ny_train = np.eye(num_classes)[y_train]\n```", "```\nimport numpy as np\nfrom layer import FullyConnectedLayer\n\ndef sigmoid(x): # Apply the sigmoid function to the elements of x.\n    return 1 / (1 + np.exp(-x)) # y\n\nclass SimpleNetwork(object):\n    \"\"\"A simple fully-connected NN.\n    Args:\n        num_inputs (int): The input vector size / number of input values.\n        num_outputs (int): The output vector size.\n        hidden_layers_sizes (list): A list of sizes for each hidden layer to be added to the network\n    Attributes:\n        layers (list): The list of layers forming this simple network.\n    \"\"\"\n\n    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32)):\n        super().__init__()\n        # We build the list of layers composing the network:\n        sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n        self.layers = [\n            FullyConnectedLayer(sizes[i], sizes[i + 1], sigmoid)\n            for i in range(len(sizes) - 1)]\n\n    def forward(self, x):\n        \"\"\"Forward the input vector `x` through the layers.\"\"\"\n        for layer in self.layers: # from the input layer to the output one\n            x = layer.forward(x)\n        return x\n\n    def predict(self, x):\n        \"\"\"Compute the output corresponding to `x`, and return the index of the largest output value\"\"\"\n        estimations = self.forward(x)\n        best_class = np.argmax(estimations)\n        return best_class\n\n    def evaluate_accuracy(self, X_val, y_val):\n        \"\"\"Evaluate the network's accuracy on a validation dataset.\"\"\"\n        num_corrects = 0\n        for i in range(len(X_val)):\n            if self.predict(X_val[i]) == y_val[i]:\n                num_corrects += 1\n        return num_corrects / len(X_val)\n```", "```\n# Network for MNIST images, with 2 hidden layers of size 64 and 32:\nmnist_classifier = SimpleNetwork(X_train.shape[1], num_classes, [64, 32])\n\n# ... and we evaluate its accuracy on the MNIST test set:\naccuracy = mnist_classifier.evaluate_accuracy(X_test, y_test)\nprint(\"accuracy = {:.2f}%\".format(accuracy * 100))\n# > accuracy = 12.06%\n```", "```\nclass FullyConnectedLayer(object):\n    # [...] (code unchanged)\n    def __init__(self, num_inputs, layer_size, activation_fn, d_activation_fn):\n        # [...] (code unchanged)\n        self.d_activation_fn = d_activation_fn # Deriv. activation function \n        self.x, self.y, self.dL_dW, self.dL_db = 0, 0, 0, 0 # Storage attr.\n\n    def forward(self, x):\n        z = np.dot(x, self.W) + self.b\n        self.y = self.activation_fn(z)\n        self.x = x  # we store values for back-propagation\n        return self.y\n\n    def backward(self, dL_dy):\n        \"\"\"Back-propagate the loss.\"\"\"\n        dy_dz = self.d_activation_fn(self.y)  # = f'\n        dL_dz = (dL_dy * dy_dz) # dL/dz = dL/dy * dy/dz = l'_{k+1} * f'\n        dz_dw = self.x.T\n        dz_dx = self.W.T\n        dz_db = np.ones(dL_dy.shape[0]) # dz/db = \"ones\"-vector\n        # Computing and storing dL w.r.t. the layer's parameters:\n        self.dL_dW = np.dot(dz_dw, dL_dz)\n        self.dL_db = np.dot(dz_db, dL_dz)\n        # Computing the derivative w.r.t. x for the previous layers:\n        dL_dx = np.dot(dL_dz, dz_dx)\n        return dL_dx\n\n    def optimize(self, epsilon):\n        \"\"\"Optimize the layer's parameters w.r.t. the derivative values.\"\"\"\n        self.W -= epsilon * self.dL_dW\n        self.b -= epsilon * self.dL_db\n```", "```\ndef derivated_sigmoid(y):  # sigmoid derivative function\n    return y * (1 - y)\n\ndef loss_L2(pred, target): # L2 loss function\n    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. for results not depending on the batch size (pred.shape[0]), we divide the loss by it\n\ndef derivated_loss_L2(pred, target):    # L2 derivative function\n    return 2 * (pred - target) # we could add the batch size division here too, but it wouldn't really affect the training (just scaling down the derivatives).\n\nclass SimpleNetwork(object):\n # [...] (code unchanged)\n def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32), loss_fn=loss_L2, d_loss_fn=derivated_loss_L2):\n        # [...] (code unchanged, except for FC layers new params.)\n        self.loss_fn, self.d_loss_fn = loss_fn, d_loss_fn\n\n    # [...] (code unchanged)\n\n    def backward(self, dL_dy):\n        \"\"\"Back-propagate the loss derivative from last to 1st layer.\"\"\"\n        for layer in reversed(self.layers):\n            dL_dy = layer.backward(dL_dy)\n        return dL_dy\n\n def optimize(self, epsilon):\n        \"\"\"Optimize the parameters according to the stored gradients.\"\"\"\n        for layer in self.layers:\n            layer.optimize(epsilon)\n\n    def train(self, X_train, y_train, X_val, y_val, batch_size=32, num_epochs=5, learning_rate=5e-3):\n        \"\"\"Train (and evaluate) the network on the provided dataset.\"\"\"\n        num_batches_per_epoch = len(X_train) // batch_size\n        loss, accuracy = [], []\n        for i in range(num_epochs): # for each training epoch\n            epoch_loss = 0\n            for b in range(num_batches_per_epoch): # for each batch\n                # Get batch:\n                b_idx = b * batch_size\n                b_idx_e = b_idx + batch_size\n                x, y_true = X_train[b_idx:b_idx_e], y_train[b_idx:b_idx_e]\n                # Optimize on batch:\n                y = self.forward(x) # forward pass\n                epoch_loss += self.loss_fn(y, y_true) # loss \n                dL_dy = self.d_loss_fn(y, y_true) # loss derivation\n                self.backward(dL_dy) # back-propagation pass\n                self.optimize(learning_rate) # optimization\n\n            loss.append(epoch_loss / num_batches_per_epoch)\n            # After each epoch, we \"validate\" our network, i.e., we measure its accuracy over the test/validation set:\n            accuracy.append(self.evaluate_accuracy(X_val, y_val))\n            print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(i, loss[i], accuracy[i] * 100))\n```", "```\nlosses, accuracies = mnist_classifier.train(\n    X_train, y_train, X_test, y_test, batch_size=30, num_epochs=500)\n# > Epoch    0: training loss = 1.096978 | val accuracy = 19.10%\n# > Epoch    1: training loss = 0.886127 | val accuracy = 32.17%\n# > Epoch    2: training loss = 0.785361 | val accuracy = 44.06%\n# [...]\n# > Epoch  498: training loss = 0.046022 | val accuracy = 94.83%\n# > Epoch  499: training loss = 0.045963 | val accuracy = 94.83%\n```"]