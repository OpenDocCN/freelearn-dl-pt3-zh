- en: 'Chapter 9:'
  prefs: []
  type: TYPE_NORMAL
- en: Serving a TensorFlow Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, after learning all the previous chapters, you have seen many facets
    of a model building process in **TensorFlow Enterprise (TFE)**. Now it is time
    to wrap up what we have done and look at how we can serve the model we have built.
    In this chapter, we are going to look at the fundamentals of serving a TensorFlow
    model, which is through a RESTful API in localhost. The easiest way to get started
    is by using **TensorFlow Serving (TFS)**. Out of the box, TFS is a system for
    serving machine learning models built with TensorFlow. Although it is not yet
    officially supported by TFE, you will see that it works with models built by TFE
    2\. It can run as either a server or as a Docker container. For our ease, we are
    going to use a Docker container, as it is really the easiest way to start using
    TFS, regardless of your local environment, as long as you have a Docker engine
    available. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Running Local Serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding TFS with Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading TFS Docker images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow along with this chapter, and for trying the example code here: https://github.com/PacktPublishing/learn-tensorflow-enterprise,
    you will need to clone the GitHub repository for this book, and navigate to the
    folder in `chapter_09`. You may clone the repository with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will work from the folder named `chapter_09`. Inside this folder, there is
    a Jupyter notebook containing source code. You will also find the `flowerclassifier/001`
    directory, which contains a `saved_model.pb` file ready for your use. In the `raw_images`
    directory, you will find a few raw JPG images for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Running Local Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A prerequisite to serving the model is serialization of the model structure
    and its assets, such as weights and biases matrices. A trained TensorFlow model
    is typically saved in a `SavedModel` format. A `SavedModel` format consists of
    the complete TensorFlow program with weights, biases, and computation ops. This
    is done through the low-level `tf.saved_model` API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, when you execute a model training process using Fit, you end up
    with something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After you''ve executed the preceding code, you have a model object, `mdl`,
    that can be saved via the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you take a look at the current directory, you will find a `saved_model.pb`
    file there.
  prefs: []
  type: TYPE_NORMAL
- en: 'For your convenience, a `saved_model` file is provided for this exercise. In
    the `flowerclassifier/001` directory, you will find the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `save_model_path` is defined as `null`. This indicates that the
    model is to be saved in the current directory. If you have another directory that
    you want to use, you need to specify the full or relative path for that directory.
  prefs: []
  type: TYPE_NORMAL
- en: '`saved_model.pb` is the Protobuf format of the model structure. The `assets`
    folder contains objects such as a vocabulary list or any lookup table, which are
    necessary for model execution. It may be empty if no such objects are created
    or required. The `variables` folder contains the weights and bias values as the
    result of training. These items constitute `SavedModel`. We are going to take
    a look at how to invoke `SavedModel` for scoring test data. Now let''s turn our
    attention to the Jupyter notebook in this chapter''s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you simply want to use a Python script to invoke `SavedModel`, it is very
    simple. All you need to do is load the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each `SavedModel` has a default model signature that describes model inputs
    and outputs structures. This signature also has a name associated with it. We
    need to find out what this name is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since the signature name is not specified during the save process, the output
    of the signature name is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to create an inference object, `infer`, and then find the name
    for the model output as well as its shape, which are required when using this
    model to score test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is named `custom_class`, and it is a tensor with a floating-point
    NumPy array of `shape=(None, 5)`. This indicates that the output is an array of
    probabilities for each of the five flower types. And the position index of the
    array with the highest probability is what we need to map to the flower type.
    We have seen this map in [*Chapter 7*](B16070_07_Final_JM_ePub.xhtml#_idTextAnchor200),
    *Model Optimization*, where we learned how to process TFRecord to build and train
    this model. This is the map:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the highest probability is in the first position in the `custom_class` output's
    array, then the prediction is mapped to `roses`. If it is in the fifth position,
    then the prediction is mapped to `tulips`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Another thing we need to confirm is the shape of the input expected by the
    model. We may use `save_model_cli` to give us this information. We may execute
    this inline command in the Jupyter notebook cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will observe that the output of this command includes the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice the `shape` requirement. We know that (`224`, `224`, `3`) refers to the
    image dimensions. `-1` in the first dimension indicates this input is set up to
    handle multiple (batches) of (`224`, `224`, `3`) image arrays. Therefore, if we
    want to score one image, we need to expand that image by a dimension.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s use a test image in the `raw_image` directory and read the image with
    the `nvision` library''s `imread`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we only need to provide the height and width for resizing images
    to the correct pixel count in each dimension.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `infer` object to score this image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the prediction for each of the five flower types, given `img1_np`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The highest probability occurs in the fifth position with a value of `9.9995959e-01`.
    Therefore, based on the aforementioned map in *step 3*, this image is mapped to
    `tulips`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have seen how to use `SavedModel` for inference. This requires us to work
    in a Python runtime to load the model, read the image, and pass it to the model
    for scoring. In a production or application environment, however, the call to
    model is usually through TFS. In the next section, we are going to see how to
    make this model work with this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding TensorFlow Serving with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the core of TFS is actually a TensorFlow model server that runs a model Protobuf
    file. Installing the model server is not straightforward, as there are many dependencies.
    As a convenience, the TensorFlow team also provides this model server in a Docker
    container, which is a platform that uses virtualization at the operating system
    level, and it is self-contained with all the necessary dependencies (that is,
    libraries or modules) to run in an isolated environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the easiest way to deploy a TensorFlow `SavedModel` is by means
    of TFS with a Docker container. To install Docker, you can refer to the Docker
    site ([https://docs.docker.com/install/](https://docs.docker.com/install/)), along
    with the instructions for Mac, Windows, or Linux installations. For our chapter,
    a community version will suffice. We will be using Docker Desktop 2.4 running
    in macOS Catalina 10.15.6 with specs as indicated in *Figure 9.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – The Docker version used for this chapter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – The Docker version used for this chapter
  prefs: []
  type: TYPE_NORMAL
- en: It is assumed that you have installed Docker Desktop properly and that it is
    running. At a high level, we are going to download a TFS Docker image, add our
    model to it, and build a new Docker image on top of the base image, which is TFS.
    The final image is exposed through a TCP/IP port, which handles a RESTful API
    call from a client.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading TensorFlow Serving Docker images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the Docker engine is up and running, you are ready to perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may pull the latest TFS Docker image with this Docker command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is now our base image. In order to add our model on top of this image,
    we need to run this base image first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding command, we invoked the `tensorflow/serving` image and now
    it is running as a Docker container. We also name this container `serv_base_img`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new image with the model and serving it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now take a look at the file directory here. For this example, the directory
    structure is as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Directory structure for creating a custom Docker container'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Directory structure for creating a custom Docker container
  prefs: []
  type: TYPE_NORMAL
- en: We will execute the following commands from the same directory as `Tensorflow_Serving.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have the TFS base Docker image up and running as a container, we are
    ready to put our own `SavedModel` into this container:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, we have to copy our model into the TFS container''s `model` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we commit our change to the base image and give the container a name that
    matches our model directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We no longer require the base image to be running. Now we can just kill it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To serve the image and score on our test image, we will run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At your command terminal, you will observe output such as that shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Docker container running with a custom-built model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0033.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – Docker container running with a custom-built model
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the following in the preceding screenshot: **Successfully loaded servable
    version {name: flowerclassifier version: 1}**.'
  prefs: []
  type: TYPE_NORMAL
- en: This line indicates that the container successfully found our model, `flowerclassifier`,
    and the next directory in the naming hierarchy, `001`. This is a general pattern
    that TFS needs in order to make TFS work with a custom model that you have built.
  prefs: []
  type: TYPE_NORMAL
- en: Now the model is served. In the next section, we will see how to build our client
    that calls this model using the Python JSON library.
  prefs: []
  type: TYPE_NORMAL
- en: Scoring through the RESTful API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let''s return to our Jupyter environment. We will see how to pick up from
    what we did in Local Serving and continue from there. Recall that we used the
    `nvision` library to normalize and standardize our test image. We also need to
    expand the image dimension because the model expects to have a batch dimension
    in our input. After we have performed these steps as in Local Serving, we will
    have `img1_np` as the properly shaped NumPy array. Let''s build this array into
    a JSON payload, and pass the payload to our model through the RESTful API with
    the help of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build the JSON payload with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we converted the test image into a JSON payload format
    and we defined a header for our RESTful API call to indicate that the payload
    is in JSON format for the application to consume.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As per TFS, the payload must encode the scoring data with a key-value pair by
    the key name of the instances and the NumPy array to be converted to a list as
    the input. We also need a header to be defined for the JSON payload as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will score our test image, `img1_np`, for this data with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding command will produce a `response` payload back from our TFS container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will examine the `response` payload by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a dictionary containing the key prediction and the probability values
    for each of the five flower types. These values are identical to what we saw in
    Local Serving. Therefore, we know that the model is correctly served via TFS using
    a Docker container.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to deploy a TensorFlow `SavedModel`. This is
    by no means the most common method to use in enterprise deployment. In an enterprise
    deployment scenario, many factors determine how the deployment pipeline should
    be built, and depending on the use cases, it can quickly diverge in terms of deployment
    patterns and choices from there. For example, some organizations use AirFlow as
    their orchestration tool, and some may prefer KubeFlow, while many others still
    use Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this book is to show you how to leverage the latest and most reliable
    implementation of TensorFlow Enterprise from a data scientist/machine learning
    model builder's perspective.
  prefs: []
  type: TYPE_NORMAL
- en: From here, depending on your interests or priorities, you may take up what you
    learned in this book and pursue many other topics, such as MLOps, model orchestration,
    drift monitoring, and redeployment. These are some of the important topics in
    any enterprise machine learning discussions from a use case perspective. Use cases,
    IT infrastructure, and business considerations typically determine how a model
    is actually served. Further considerations include what kind of service-level
    agreement the serving pipeline has to meet, and the security and compliance issues
    related to authentication and model safety.
  prefs: []
  type: TYPE_NORMAL
