["```\nclass Encoder(K.layers.Layer):\n    def __init__(self, hidden_dim):\n        super(Encoder, self).__init__()\n        self.hidden_layer = K.layers.Dense(units=hidden_dim, activation=tf.nn.relu)\n    def call(self, input_features):\n        activation = self.hidden_layer(input_features)\n        return activation \n```", "```\nclass Decoder(K.layers.Layer):\n    def __init__(self, hidden_dim, original_dim):\n        super(Decoder, self).__init__()\n        self.output_layer = K.layers.Dense(units=original_dim, activation=tf.nn.relu)\n    def call(self, encoded):\n        activation = self.output_layer(encoded)\n        return activation \n```", "```\nclass Autoencoder(K.Model):\n    def __init__(self, hidden_dim, original_dim):\n        super(Autoencoder, self).__init__()\n        self.loss = []\n        self.encoder = Encoder(hidden_dim=hidden_dim)\n        self.decoder = Decoder(hidden_dim=hidden_dim, original_dim=original_dim)\n    def call(self, input_features):\n        encoded = self.encoder(input_features)\n        reconstructed = self.decoder(encoded)\n        return reconstructed \n```", "```\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as K\nimport matplotlib.pyplot as plt \n```", "```\nnp.random.seed(11)\ntf.random.set_seed(11)\nbatch_size = 256\nmax_epochs = 50\nlearning_rate = 1e-3\nmomentum = 8e-1\nhidden_dim = 128\noriginal_dim = 784 \n```", "```\n(x_train, _), (x_test, _) = K.datasets.mnist.load_data()\nx_train = x_train / 255.\nx_test = x_test / 255.\nx_train = x_train.astype(np.float32)\nx_test = x_test.astype(np.float32)\nx_train = np.reshape(x_train, (x_train.shape[0], 784))\nx_test = np.reshape(x_test, (x_test.shape[0], 784))\ntraining_dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size) \n```", "```\nautoencoder = Autoencoder(hidden_dim=hidden_dim, original_dim=original_dim)\nopt = tf.keras.optimizers.Adam(learning_rate=1e-2)\ndef loss(preds, real):\n    return tf.reduce_mean(tf.square(tf.subtract(preds, real))) \n```", "```\ndef train(loss, model, opt, original):\n    with tf.GradientTape() as tape:\n        preds = model(original)\n        reconstruction_error = loss(preds, original)\n        gradients = tape.gradient(reconstruction_error, model.trainable_variables)\n        gradient_variables = zip(gradients, model.trainable_variables)\n    opt.apply_gradients(gradient_variables)\n    return reconstruction_error \n```", "```\ndef train_loop(model, opt, loss, dataset, epochs=20):\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for step, batch_features in enumerate(dataset):\n            loss_values = train(loss, model, opt, batch_features)\n            epoch_loss += loss_values\n        model.loss.append(epoch_loss)\n        print('Epoch {}/{}. Loss: {}'.format(epoch + 1, epochs, epoch_loss.numpy())) \n```", "```\ntrain_loop(autoencoder, opt, loss, training_dataset, epochs=max_epochs) \n```", "```\nplt.plot(range(max_epochs), autoencoder.loss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show() \n```", "```\nnumber = 10  # how many digits we will display\nplt.figure(figsize=(20, 4))\nfor index in range(number):\n    # display original\n    ax = plt.subplot(2, number, index + 1)\n    plt.imshow(x_test[index].reshape(28, 28), cmap='gray')\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    # display reconstruction\n    ax = plt.subplot(2, number, index + 1 + number)\n    plt.imshow(autoencoder(x_test)[index].numpy().reshape(28, 28), cmap='gray')\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show() \n```", "```\nclass SparseEncoder(K.layers.Layer):\n    def __init__(self, hidden_dim):\n        # encoder initializer\n        super(SparseEncoder, self).__init__()\n        self.hidden_layer = K.layers.Dense(units=hidden_dim, activation=tf.nn.relu, activity_regularizer=regularizers.l1(10e-5))\n    def call(self, input_features):\n        # forward function\n        activation = self.hidden_layer(input_features)\n        return activation \n```", "```\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\nx_train_noisy = x_train + noise\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\nx_test_noisy = x_test + noise\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.) \n```", "```\n    import numpy as np\n    import tensorflow as tf\n    import tensorflow.keras as K\n    import matplotlib.pyplot as plt \n    ```", "```\n    np.random.seed(11)\n    tf.random.set_seed(11)\n    batch_size = 256\n    max_epochs = 50\n    learning_rate = 1e-3\n    momentum = 8e-1\n    hidden_dim = 128\n    original_dim = 784 \n    ```", "```\n    (x_train, _), (x_test, _) = K.datasets.mnist.load_data()\n    x_train = x_train / 255.\n    x_test = x_test / 255.\n    x_train = x_train.astype(np.float32)\n    x_test = x_test.astype(np.float32)\n    x_train = np.reshape(x_train, (x_train.shape[0], 784))\n    x_test = np.reshape(x_test, (x_test.shape[0], 784))\n    # Generate corrupted MNIST images by adding noise with normal dist\n    # centered at 0.5 and std=0.5\n    noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n    x_train_noisy = x_train + noise\n    noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n    x_test_noisy = x_test + noise \n    ```", "```\n    # Encoder\n    class Encoder(K.layers.Layer):\n        def __init__(self, hidden_dim):\n            super(Encoder, self).__init__()\n            self.hidden_layer = K.layers.Dense(units=hidden_dim, activation=tf.nn.relu)\n        def call(self, input_features):\n            activation = self.hidden_layer(input_features)\n            return activation\n    # Decoder\n    class Decoder(K.layers.Layer):\n        def __init__(self, hidden_dim, original_dim):\n            super(Decoder, self).__init__()\n            self.output_layer = K.layers.Dense(units=original_dim, activation=tf.nn.relu)\n        def call(self, encoded):\n            activation = self.output_layer(encoded)\n            return activation\n    class Autoencoder(K.Model):\n        def __init__(self, hidden_dim, original_dim):\n            super(Autoencoder, self).__init__()\n            self.loss = []\n            self.encoder = Encoder(hidden_dim=hidden_dim)\n            self.decoder = Decoder(hidden_dim=hidden_dim, original_dim=original_dim)\n        def call(self, input_features):\n            encoded = self.encoder(input_features)\n            reconstructed = self.decoder(encoded)\n            return reconstructed \n    ```", "```\n    model = Autoencoder(hidden_dim=hidden_dim, original_dim=original_dim)\n    model.compile(loss='mse', optimizer='adam')\n    loss = model.fit(x_train_noisy,\n                x_train,\n                validation_data=(x_test_noisy, x_test),\n                epochs=max_epochs,\n                batch_size=batch_size) \n    ```", "```\n    plt.plot(range(max_epochs), loss.history['loss'])\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show() \n    ```", "```\nnumber = 10  # how many digits we will display\nplt.figure(figsize=(20, 4))\nfor index in range(number):\n    # display original\n    ax = plt.subplot(2, number, index + 1)\n    plt.imshow(x_test_noisy[index].reshape(28, 28), cmap='gray')\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    # display reconstruction\n    ax = plt.subplot(2, number, index + 1 + number)\n    plt.imshow(model(x_test_noisy)[index].numpy().reshape(28, 28), cmap='gray')\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show() \n```", "```\n    import numpy as np\n    import tensorflow as tf\n    import tensorflow.keras as K\n    import matplotlib.pyplot as plt\n    from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D \n    ```", "```\n    np.random.seed(11)\n    tf.random.set_seed(11)\n    batch_size = 128\n    max_epochs = 50\n    filters = [32,32,16] \n    ```", "```\n    (x_train, _), (x_test, _) = K.datasets.mnist.load_data()\n    x_train = x_train / 255.\n    x_test = x_test / 255.\n    x_train = np.reshape(x_train, (len(x_train),28, 28, 1))\n    x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n    noise = 0.5\n    x_train_noisy = x_train + noise * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n    x_test_noisy = x_test + noise * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n    x_train_noisy = np.clip(x_train_noisy, 0, 1)\n    x_test_noisy = np.clip(x_test_noisy, 0, 1)\n    x_train_noisy = x_train_noisy.astype('float32')\n    x_test_noisy = x_test_noisy.astype('float32')\n    #print(x_test_noisy[1].dtype) \n    ```", "```\n    class Encoder(K.layers.Layer):\n        def __init__(self, filters):\n            super(Encoder, self).__init__()\n            self.conv1 = Conv2D(filters=filters[0], kernel_size=3, strides=1, activation='relu', padding='same')\n            self.conv2 = Conv2D(filters=filters[1], kernel_size=3, strides=1, activation='relu', padding='same')\n            self.conv3 = Conv2D(filters=filters[2], kernel_size=3, strides=1, activation='relu', padding='same')\n            self.pool = MaxPooling2D((2, 2), padding='same')\n\n        def call(self, input_features):\n            x = self.conv1(input_features)\n            x = self.pool(x)\n            x = self.conv2(x)\n            x = self.pool(x)\n            x = self.conv3(x)\n            x = self.pool(x)\n            return x \n    ```", "```\n    class Decoder(K.layers.Layer):\n        def __init__(self, filters):\n            super(Decoder, self).__init__()\n            self.conv1 = Conv2D(filters=filters[2], kernel_size=3, strides=1, activation='relu', padding='same')\n            self.conv2 = Conv2D(filters=filters[1], kernel_size=3, strides=1, activation='relu', padding='same')\n            self.conv3 = Conv2D(filters=filters[0], kernel_size=3, strides=1, activation='relu', padding='valid')\n            self.conv4 = Conv2D(1, 3, 1, activation='sigmoid', padding='same')\n            self.upsample = UpSampling2D((2, 2))\n        def call(self, encoded):\n            x = self.conv1(encoded)\n            #print(\"dx1\", x.shape)\n            x = self.upsample(x)\n            #print(\"dx2\", x.shape)\n            x = self.conv2(x)\n            x = self.upsample(x)\n            x = self.conv3(x)\n            x = self.upsample(x)\n            return self.conv4(x) \n    ```", "```\n    class Autoencoder(K.Model):\n        def __init__(self, filters):\n            super(Autoencoder, self).__init__()\n            self.encoder = Encoder(filters)\n            self.decoder = Decoder(filters)\n        def call(self, input_features):\n            #print(input_features.shape)\n            encoded = self.encoder(input_features)\n            #print(encoded.shape)\n            reconstructed = self.decoder(encoded)\n            #print(reconstructed.shape)\n            return reconstructed \n    ```", "```\n    model = Autoencoder(filters)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    loss = model.fit(x_train_noisy,\n                x_train,\n                validation_data=(x_test_noisy, x_test),\n                epochs=max_epochs,\n                batch_size=batch_size) \n    ```", "```\n    plt.plot(range(max_epochs), loss.history['loss'])\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show() \n    ```", "```\n    number = 10  # how many digits we will display\n    plt.figure(figsize=(20, 4))\n    for index in range(number):\n        # display original\n        ax = plt.subplot(2, number, index + 1)\n        plt.imshow(x_test_noisy[index].reshape(28, 28), cmap='gray')\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        # display reconstruction\n        ax = plt.subplot(2, number, index + 1 + number)\n        plt.imshow(tf.reshape(model(x_test_noisy)[index], (28, 28)), cmap='gray')\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    plt.show() \n    ```", "```\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing import sequence\nfrom scipy.stats import describe\nimport collections\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport os\nfrom time import gmtime, strftime\nfrom tensorflow.keras.callbacks import TensorBoard\nimport re\n# Needed to run only once\nnltk.download('punkt')\nnltk.download('reuters')\nfrom nltk.corpus import reuters \n```", "```\n%%capture\n!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora \n```", "```\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove*.zip \n```", "```\ndef is_number(n):\n    temp = re.sub(\"[.,-/]\", \"\",n)\n    return temp.isdigit()\n# parsing sentences and building vocabulary\nword_freqs = collections.Counter()\ndocuments = reuters.fileids()\n#ftext = open(\"text.tsv\", \"r\")\nsents = []\nsent_lens = []\nnum_read = 0\nfor i in range(len(documents)):\n    # periodic heartbeat report\n    if num_read % 100 == 0:\n        print(\"building features from {:d} docs\".format(num_read))\n    # skip docs without specified topic\n    title_body = reuters.raw(documents[i]).lower()\n    if len(title_body) == 0:\n        continue\n    num_read += 1\n    # convert to list of word indexes\n    title_body = re.sub(\"\\n\", \"\", title_body)\n    for sent in nltk.sent_tokenize(title_body):\n        for word in nltk.word_tokenize(sent):\n            if is_number(word):\n                word = \"9\"\n            word = word.lower()\n            word_freqs[word] += 1\n        sents.append(sent)\n        sent_lens.append(len(sent)) \n```", "```\nprint(\"Total number of sentences are: {:d} \".format(len(sents)))\nprint (\"Sentence distribution min {:d}, max {:d} , mean {:3f}, median {:3f}\".format(np.min(sent_lens), np.max(sent_lens), np.mean(sent_lens), np.median(sent_lens)))\nprint(\"Vocab size (full) {:d}\".format(len(word_freqs))) \n```", "```\nTotal number of sentences are: 50470 \nSentence distribution min 1, max 3688 , mean 167.072657, median 155.000000\nVocab size (full) 33748 \n```", "```\nVOCAB_SIZE = 5000\nSEQUENCE_LEN = 50 \n```", "```\nword2id = {}\nword2id[\"PAD\"] = 0\nword2id[\"UNK\"] = 1\nfor v, (k, _) in enumerate(word_freqs.most_common(VOCAB_SIZE - 2)):\n    word2id[k] = v + 2\nid2word = {v:k for k, v in word2id.items()} \n```", "```\nEMBED_SIZE = 50\ndef lookup_word2id(word):\n    try:\n        return word2id[word]\n    except KeyError:\n        return word2id[\"UNK\"]\ndef load_glove_vectors(glove_file, word2id, embed_size):\n    embedding = np.zeros((len(word2id), embed_size))\n    fglove = open(glove_file, \"rb\")\n    for line in fglove:\n        cols = line.strip().split()\n        word = cols[0].decode('utf-8')\n        if embed_size == 0:\n            embed_size = len(cols) - 1\n        if word in word2id:\n            vec = np.array([float(v) for v in cols[1:]])\n        embedding[lookup_word2id(word)] = vec\n    embedding[word2id[\"PAD\"]] = np.zeros((embed_size))\n    embedding[word2id[\"UNK\"]] = np.random.uniform(-1, 1, embed_size)\n    return embedding \n```", "```\nsent_wids = [[lookup_word2id(w) for w in s.split()] for s in sents]\nsent_wids = sequence.pad_sequences(sent_wids, SEQUENCE_LEN)\n# load glove vectors into weight matrix\nembeddings = load_glove_vectors(\"glove.6B.{:d}d.txt\".format(EMBED_SIZE), word2id, EMBED_SIZE) \n```", "```\nBATCH_SIZE = 64\ndef sentence_generator(X, embeddings, batch_size):\n    while True:\n        # loop once per epoch\n        num_recs = X.shape[0]\n        indices = np.random.permutation(np.arange(num_recs))\n        num_batches = num_recs // batch_size\n        for bid in range(num_batches):\n            sids = indices[bid * batch_size : (bid + 1) * batch_size]\n            Xbatch = embeddings[X[sids, :]]\n            yield Xbatch, Xbatch\ntrain_size = 0.7\nXtrain, Xtest = train_test_split(sent_wids, train_size=train_size)\ntrain_gen = sentence_generator(Xtrain, embeddings, BATCH_SIZE)\ntest_gen = sentence_generator(Xtest, embeddings, BATCH_SIZE) \n```", "```\nLATENT_SIZE = 512\nEMBED_SIZE = 50\nBATCH_SIZE = 64\nNUM_EPOCHS = 20\ninputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name=\"input\")\nencoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode=\"sum\", name=\"encoder_lstm\")(inputs)\ndecoded = RepeatVector(SEQUENCE_LEN, name=\"repeater\")(encoded)\ndecoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences=True), merge_mode=\"sum\", name=\"decoder_lstm\")(decoded)\nautoencoder = Model(inputs, decoded) \n```", "```\nautoencoder.compile(optimizer=\"adam\", loss=\"mse\") \n```", "```\nnum_train_steps = len(Xtrain) // BATCH_SIZE\nnum_test_steps = len(Xtest) // BATCH_SIZE\nsteps_per_epoch=num_train_steps,\nepochs=NUM_EPOCHS,\nvalidation_data=test_gen,\nvalidation_steps=num_test_steps,\nhistory = autoencoder.fit_generator(train_gen,\n                                steps_per_epoch=num_train_steps,\n                                epochs=NUM_EPOCHS,\n                                validation_data=test_gen,\n                                validation_steps=num_test_steps) \n```", "```\nencoder = Model(autoencoder.input, autoencoder.get_layer(\"encoder_lstm\").output) \n```", "```\ndef compute_cosine_similarity(x, y):\n    return np.dot(x, y) / (np.linalg.norm(x, 2) * np.linalg.norm(y, 2))\nk = 500\ncosims = np.zeros((k))\ni= 0\nfor bid in range(num_test_steps):\n    xtest, ytest = next(test_gen)\n    ytest_ = autoencoder.predict(xtest)\n    Xvec = encoder.predict(xtest)\n    Yvec = encoder.predict(ytest_)\n    for rid in range(Xvec.shape[0]):\n        if i >= k:\n            break\n        cosims[i] = compute_cosine_similarity(Xvec[rid], Yvec[rid])\n        if i <= 10:\n            print(cosims[i])\n        i += 1\n    if i >= k:\n        break \n```", "```\n0.9765363335609436\n0.9862152338027954\n0.9831727743148804\n0.977733314037323\n0.9851642847061157\n0.9849132895469666\n0.9831638932228088\n0.9843543767929077\n0.9825796484947205\n0.9877195954322815\n0.9820773601531982 \n```", "```\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt \n```", "```\nnp.random.seed(333)\ntf.random.set_seed(333)\nassert tf.__version__.startswith('2.'), \"TensorFlow Version Below 2.0\" \n```", "```\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train.astype(np.float32)/255., x_test.astype(np.float32)/255.\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape) \n```", "```\n--------------------------------------------------\n(60000, 28, 28) (60000,)\n(10000, 28, 28) (10000,) \n```", "```\nnumber = 10  # how many digits we will display\nplt.figure(figsize=(20, 4))\nfor index in range(number):\n    # display original\n    ax = plt.subplot(2, number, index + 1)\n    plt.imshow(x_train[index], cmap='gray')\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show() \n```", "```\nimage_size = x_train.shape[1]*x_train.shape[2]\nhidden_dim = 512\nlatent_dim = 10\nnum_epochs = 80\nbatch_size = 100\nlearning_rate = 0.001 \n```", "```\nclass VAE(tf.keras.Model):\n    def __init__(self,dim,**kwargs):\n        h_dim = dim[0]\n        z_dim = dim[1]\n        super(VAE, self).__init__(**kwargs)\n        self.fc1 = tf.keras.layers.Dense(h_dim)\n        self.fc2 = tf.keras.layers.Dense(z_dim)\n        self.fc3 = tf.keras.layers.Dense(z_dim)\n        self.fc4 = tf.keras.layers.Dense(h_dim)\n        self.fc5 = tf.keras.layers.Dense(image_size) \n```", "```\ndef encode(self, x):\n    h = tf.nn.relu(self.fc1(x))\n    return self.fc2(h), self.fc3(h)\ndef reparameterize(self, mu, log_var):\n    std = tf.exp(log_var * 0.5)\n    eps = tf.random.normal(std.shape)\n    return mu + eps * std\ndef decode_logits(self, z):\n    h = tf.nn.relu(self.fc4(z))\n    return self.fc5(h)\ndef decode(self, z):\n    return tf.nn.sigmoid(self.decode_logits(z)) \n```", "```\ndef call(self, inputs, training=None, mask=None):\n    mu, log_var = self.encode(inputs)\n    z = self.reparameterize(mu, log_var)\n    x_reconstructed_logits = self.decode_logits(z)\n    return x_reconstructed_logits, mu, log_var \n```", "```\nmodel = VAE([hidden_dim, latent_dim])\nmodel.build(input_shape=(4, image_size))\nmodel.summary()\noptimizer = tf.keras.optimizers.Adam(learning_rate) \n```", "```\nModel: \"vae\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               multiple                  401920    \n\n dense_1 (Dense)             multiple                  5130      \n\n dense_2 (Dense)             multiple                  5130      \n\n dense_3 (Dense)             multiple                  5632      \n\n dense_4 (Dense)             multiple                  402192    \n\n=================================================================\nTotal params: 820,004\nTrainable params: 820,004\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\ndataset = tf.data.Dataset.from_tensor_slices(x_train)\ndataset = dataset.shuffle(batch_size * 5).batch(batch_size)\nnum_batches = x_train.shape[0] // batch_size\nfor epoch in range(num_epochs):\n    for step, x in enumerate(dataset):\n        x = tf.reshape(x, [-1, image_size])\n        with tf.GradientTape() as tape:\n            # Forward pass\n            x_reconstruction_logits, mu, log_var = model(x)\n            # Compute reconstruction loss and kl divergence\n            # Scaled by 'image_size' for each individual pixel.\n            reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_reconstruction_logits)\n            reconstruction_loss = tf.reduce_sum(reconstruction_loss) / batch_size\n\n            kl_div = - 0.5 * tf.reduce_sum(1\\. + log_var - tf.square(mu) - tf.exp(log_var), axis=-1)\n            kl_div = tf.reduce_mean(kl_div)\n            # Backprop and optimize\n            loss = tf.reduce_mean(reconstruction_loss) + kl_div\n        gradients = tape.gradient(loss, model.trainable_variables)\n        for g in gradients:\n            tf.clip_by_norm(g, 15)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        if (step + 1) % 50 == 0:\n            print(\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\"\n            .format(epoch + 1, num_epochs, step + 1, num_batches, float(reconstruction_loss), float(kl_div))) \n```", "```\nz = tf.random.normal((batch_size, latent_dim))\nout = model.decode(z)  # decode with sigmoid\nout = tf.reshape(out, [-1, 28, 28]).numpy() * 255\nout = out.astype(np.uint8) \n```"]