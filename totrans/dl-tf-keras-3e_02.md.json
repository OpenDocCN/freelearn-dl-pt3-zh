["```\n    import tensorflow as tf\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd \n    ```", "```\n    #Generate a random data\n    np.random.seed(0)\n    area = 2.5 * np.random.randn(100) + 25\n    price = 25 * area + 5 + np.random.randint(20,50, size = len(area))\n    data = np.array([area, price])\n    data = pd.DataFrame(data = data.T, columns=['area','price'])\n    plt.scatter(data['area'], data['price'])\n    plt.show() \n    ```", "```\n    W = sum(price*(area-np.mean(area))) / sum((area-np.mean(area))**2)\n    b = np.mean(price) - W*np.mean(area)\n    print(\"The regression coefficients are\", W,b) \n    ```", "```\n    -----------------------------------------------\n    The regression coefficients are 24.815544052284988 43.4989785533412 \n    ```", "```\n    y_pred = W * area + b \n    ```", "```\n    plt.plot(area, y_pred, color='red',label=\"Predicted Price\")\n    plt.scatter(data['area'], data['price'], label=\"Training Data\")\n    plt.xlabel(\"Area\")\n    plt.ylabel(\"Price\")\n    plt.legend() \n    ```", "```\n    import tensorflow as tf\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import tensorflow.keras as K\n    from tensorflow.keras.layers import Dense \n    ```", "```\n    #Generate a random data\n    np.random.seed(0)\n    area = 2.5 * np.random.randn(100) + 25\n    price = 25 * area + 5 + np.random.randint(20,50, size = len(area))\n    data = np.array([area, price])\n    data = pd.DataFrame(data = data.T, columns=['area','price'])\n    plt.scatter(data['area'], data['price'])\n    plt.show() \n    ```", "```\n    data = (data - data.min()) / (data.max() - data.min())  #Normalize \n    ```", "```\n    model = K.Sequential([\n                          Dense(1, input_shape = [1,], activation=None)\n    ])\n    model.summary() \n    ```", "```\n    Model: \"sequential\"\n    ____________________________________________________________\n     Layer (type)           Output Shape              Param #   \n    ============================================================\n     dense (Dense)          (None, 1)                 2         \n\n    ============================================================\n    Total params: 2\n    Trainable params: 2\n    Non-trainable params: 0\n    ____________________________________________________________ \n    ```", "```\n    model.compile(loss='mean_squared_error', optimizer='sgd') \n    ```", "```\n    model.fit(x=data['area'],y=data['price'], epochs=100, batch_size=32, verbose=1, validation_split=0.2) \n    ```", "```\n    model.fit(x=data['area'],y=data['price'], epochs=100, batch_size=32, verbose=1, validation_split=0.2)\n    Epoch 1/100\n    3/3 [==============================] - 0s 78ms/step - loss: 1.2643 - val_loss: 1.4828\n    Epoch 2/100\n    3/3 [==============================] - 0s 13ms/step - loss: 1.0987 - val_loss: 1.3029\n    Epoch 3/100\n    3/3 [==============================] - 0s 13ms/step - loss: 0.9576 - val_loss: 1.1494\n    Epoch 4/100\n    3/3 [==============================] - 0s 16ms/step - loss: 0.8376 - val_loss: 1.0156\n    Epoch 5/100\n    3/3 [==============================] - 0s 15ms/step - loss: 0.7339 - val_loss: 0.8971\n    Epoch 6/100\n    3/3 [==============================] - 0s 16ms/step - loss: 0.6444 - val_loss: 0.7989\n    Epoch 7/100\n    3/3 [==============================] - 0s 14ms/step - loss: 0.5689 - val_loss: 0.7082\n    .\n    .\n    .\n    Epoch 96/100\n    3/3 [==============================] - 0s 22ms/step - loss: 0.0827 - val_loss: 0.0755\n    Epoch 97/100\n    3/3 [==============================] - 0s 17ms/step - loss: 0.0824 - val_loss: 0.0750\n    Epoch 98/100\n    3/3 [==============================] - 0s 14ms/step - loss: 0.0821 - val_loss: 0.0747\n    Epoch 99/100\n    3/3 [==============================] - 0s 21ms/step - loss: 0.0818 - val_loss: 0.0740\n    Epoch 100/100\n    3/3 [==============================] - 0s 15ms/step - loss: 0.0815 - val_loss: 0.0740\n    <keras.callbacks.History at 0x7f7228d6a790> \n    ```", "```\n    y_pred = model.predict(data['area']) \n    ```", "```\n    plt.plot(data['area'], y_pred, color='red',label=\"Predicted Price\")\n    plt.scatter(data['area'], data['price'], label=\"Training Data\")\n    plt.xlabel(\"Area\")\n    plt.ylabel(\"Price\")\n    plt.legend() \n    ```", "```\n    [<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.33806288]], dtype=float32)>,\n    <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.68142694], dtype=float32)>] \n    ```", "```\n    import tensorflow as tf\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import tensorflow.keras as K\n    from tensorflow.keras.layers import Dense, Normalization\n    import seaborn as sns \n    ```", "```\n    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n    column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin']\n    data = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True) \n    ```", "```\n    data = data.drop('origin', 1)\n    print(data.isna().sum())\n    data = data.dropna() \n    ```", "```\n    train_dataset = data.sample(frac=0.8, random_state=0)\n    test_dataset = data.drop(train_dataset.index) \n    ```", "```\n    sns.pairplot(train_dataset[['mpg', 'cylinders', 'displacement','horsepower', 'weight', 'acceleration', 'model_year']], diag_kind='kde') \n    ```", "```\n    train_features = train_dataset.copy()\n    test_features = test_dataset.copy() \n    train_labels = train_features.pop('mpg')\n    test_labels = test_features.pop('mpg') \n    ```", "```\n    #Normalize\n    data_normalizer = Normalization(axis=1)\n    data_normalizer.adapt(np.array(train_features)) \n    ```", "```\n    model = K.Sequential([\n        data_normalizer,\n        Dense(64, activation='relu'),\n        Dense(32, activation='relu'),\n        Dense(1, activation=None)\n    ])\n    model.summary() \n    ```", "```\n    model.compile(optimizer='adam', loss='mean_squared_error') \n    ```", "```\n    history = model.fit(x=train_features,y=train_labels, epochs=100, verbose=1, validation_split=0.2) \n    ```", "```\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Error [MPG]')\n    plt.legend()\n    plt.grid(True) \n    ```", "```\n    y_pred = model.predict(test_features).flatten()\n    a = plt.axes(aspect='equal')\n    plt.scatter(test_labels, y_pred)\n    plt.xlabel('True Values [MPG]')\n    plt.ylabel('Predictions [MPG]')\n    lims = [0, 50]\n    plt.xlim(lims)\n    plt.ylim(lims)\n    plt.plot(lims, lims) \n    ```", "```\n    error = y_pred - test_labels\n    plt.hist(error, bins=30)\n    plt.xlabel('Prediction Error [MPG]')\n    plt.ylabel('Count') \n    ```", "```\n    import tensorflow as tf\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import tensorflow.keras as K\n    from tensorflow.keras.layers import Dense, Flatten \n    ```", "```\n    ((train_data, train_labels),(test_data, test_labels)) = tf.keras.datasets.mnist.load_data() \n    ```", "```\n    train_data = train_data/np.float32(255)\n    train_labels = train_labels.astype(np.int32)  \n    test_data = test_data/np.float32(255)\n    test_labels = test_labels.astype(np.int32) \n    ```", "```\n    model = K.Sequential([\n        Flatten(input_shape=(28, 28)),\n        Dense(10, activation='sigmoid')\n    ])\n    model.summary() \n    ```", "```\n    Model: \"sequential\"\n    ____________________________________________________________\n     Layer (type)           Output Shape              Param #   \n    ============================================================\n     flatten (Flatten)      (None, 784)               0         \n\n     dense (Dense)          (None, 10)                7850      \n\n    ============================================================\n    Total params: 7,850\n    Trainable params: 7,850\n    Non-trainable params: 0\n    ____________________________________________________________ \n    ```", "```\n    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    history = model.fit(x=train_data,y=train_labels, epochs=50, verbose=1, validation_split=0.2) \n    ```", "```\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True) \n    ```", "```\n    def plot_image(i, predictions_array, true_label, img):\n        true_label, img = true_label[i], img[i]\n        plt.grid(False)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(img, cmap=plt.cm.binary)\n        predicted_label = np.argmax(predictions_array)\n        if predicted_label == true_label:\n          color ='blue'\n        else:\n          color ='red'\n        plt.xlabel(\"Pred {} Conf: {:2.0f}% True ({})\".format(predicted_label,\n                                      100*np.max(predictions_array),\n                                      true_label),\n                                      color=color)\n    def plot_value_array(i, predictions_array, true_label):\n        true_label = true_label[i]\n        plt.grid(False)\n        plt.xticks(range(10))\n        plt.yticks([])\n        thisplot = plt.bar(range(10), predictions_array,\n        color\"#777777\")\n        plt.ylim([0, 1])\n        predicted_label = np.argmax(predictions_array)\n        thisplot[predicted_label].set_color('red')\n        thisplot[true_label].set_color('blue') \n    ```", "```\n    predictions = model.predict(test_data)\n    i = 56\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plot_image(i, predictions[i], test_labels, test_data)\n    plt.subplot(1,2,2)\n    plot_value_array(i, predictions[i],  test_labels)\n    plt.show() \n    ```", "```\n    better_model = K.Sequential([\n        Flatten(input_shape=(28, 28)),\n        Dense(128,  activation='relu'),\n        Dense(10, activation='softmax')\n    ])\n    better_model.summary() \n    ```"]