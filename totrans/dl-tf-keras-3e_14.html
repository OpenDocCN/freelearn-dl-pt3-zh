<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer777">
<h1 class="chapterNumber">14</h1>
<h1 class="chapterTitle" id="_idParaDest-367">The Math Behind Deep Learning</h1>
<p class="normal">In this chapter, we discuss the math behind deep learning. This topic is quite advanced and not necessarily required for practitioners. However, it is recommended reading if you are interested in understanding what is going on <em class="italic">under the hood</em> when you play with neural networks. </p>
<p class="normal">Here is what you will learn:</p>
<ul>
<li class="bulletList">A historical introduction</li>
<li class="bulletList">The concepts of derivatives and gradients</li>
<li class="bulletList">Gradient descent and backpropagation algorithms commonly used to optimize deep learning networks</li>
</ul>
<p class="normal">Let’s begin!</p>
<h1 class="heading-1" id="_idParaDest-368">History</h1>
<p class="normal">The basics of continuous backpropagation were proposed by Henry J. Kelley [1] in 1960 using dynamic programming. Stuart Dreyfus proposed using the chain rule in 1962 [2]. Paul Werbos was the first to use<a id="_idIndexMarker1386"/> backpropagation (backprop for short) for neural nets in his 1974 PhD thesis [3]. However, it wasn’t until 1986 that backpropagation gained success with the work of David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams published in Nature [4]. In 1987, Yann LeCun described the modern version of backprop currently used for training neural networks [5]. </p>
<p class="normal">The basic intuition of <strong class="keyWord">Stochastic Gradient Descent</strong> (<strong class="keyWord">SGD</strong>) was introduced by Robbins and Monro in 1951 in a context different from<a id="_idIndexMarker1387"/> neural networks [6]. In 2012 – or 52 years after the first time backprop was first introduced – AlexNet [7] achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge using GPUs. According to The Economist [8], <em class="italic">Suddenly people started to pay attention, not just within the AI community but across the technology industry as a whole.</em> Innovation in this field was not something that happened overnight. Instead, it was a long walk lasting more than 50 years!</p>
<h1 class="heading-1" id="_idParaDest-369">Some mathematical tools</h1>
<p class="normal">Before introducing backpropagation, we need to review some mathematical tools from calculus. Don’t worry too much; we’ll briefly review a few areas, all of which are commonly covered in high school-level mathematics.</p>
<h2 class="heading-2" id="_idParaDest-370">Vectors</h2>
<p class="normal">We will review two basic<a id="_idIndexMarker1388"/> concepts of geometry and algebra that are quite useful for<a id="_idIndexMarker1389"/> machine learning: vectors and the cosine of angles. We start by giving an explanation of vectors. Fundamentally, a vector is a list of numbers. Given a vector, we can interpret it as a direction in space. Mathematicians most often write vectors as either a column <em class="italic">x</em> or row vector <em class="italic">x</em><sup class="italic">T</sup>. Given two column vectors <em class="italic">u</em> and <em class="italic">v</em>, we can form their dot product by computing <img alt="" height="46" src="../Images/B18331_14_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="358"/>. It can be easily proven that <img alt="" height="50" src="../Images/B18331_14_002.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="404"/> where <img alt="" height="42" src="../Images/B18331_10_024.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> is the angle between the two vectors. </p>
<p class="normal">Here are two easy questions for you. What is the result when the two vectors are very close? And what is the result when the two vectors are the same? </p>
<h2 class="heading-2" id="_idParaDest-371">Derivatives and gradients everywhere</h2>
<p class="normal">Derivatives are a powerful <a id="_idIndexMarker1390"/>mathematical tool. We are going to use<a id="_idIndexMarker1391"/> derivatives and gradients to optimize our network. Let’s look at the <a id="_idIndexMarker1392"/>definition. The derivative of a function <em class="italic">y</em> = <em class="italic">f</em>(<em class="italic">x</em>) of a variable <em class="italic">x</em> is a measure of the rate at which the value <em class="italic">y</em> of the function changes with respect to the change of the variable <em class="italic">x</em>.</p>
<p class="normal">If <em class="italic">x</em> and <em class="italic">y</em> are real numbers, and if the graph of <em class="italic">f</em> is plotted against <em class="italic">x</em>, the derivative is the “slope” of this graph at each point. </p>
<p class="normal">If the function is linear <img alt="" height="50" src="../Images/B18331_14_004.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="317"/>, the slope is <img alt="" height="71" src="../Images/B18331_14_005.png" style="height: 1.77em !important; vertical-align: -0.58em !important;" width="113"/>. This is a simple result of calculus, which can be derived by considering that:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_14_006.png" style="height: 1.25em !important;" width="1138"/></p>
<p class="center"><img alt="" height="50" src="../Images/B18331_14_007.png" style="height: 1.25em !important;" width="233"/></p>
<p class="center"><img alt="" height="92" src="../Images/B18331_14_008.png" style="height: 2.30em !important; vertical-align: 0.02em !important;" width="121"/></p>
<p class="normal">In <em class="italic">Figure 14.1</em>, we show the<a id="_idIndexMarker1393"/> geometrical<a id="_idIndexMarker1394"/> meaning of <img alt="" height="42" src="../Images/B18331_14_009.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="46"/>, <img alt="" height="50" src="../Images/B18331_14_010.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="46"/>, and the angle <img alt="" height="42" src="../Images/B18331_10_024.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> between the linear function and the <em class="italic">x</em>-cartesian axis:</p>
<figure class="mediaobject"><img alt="Chart  Description automatically generated" height="369" src="../Images/B18331_14_01.png" width="342"/></figure>
<p class="packt_figref">Figure 14.1: An example of a linear function and rate of change</p>
<p class="normal">If the function is not linear, then computing the rate of change as the mathematical limit value of the ratio of the differences <img alt="" height="71" src="../Images/B18331_14_012.png" style="height: 1.77em !important; vertical-align: -0.58em !important;" width="42"/> as <img alt="" height="46" src="../Images/B18331_14_013.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="75"/> becomes infinitely small. Geometrically, this is the tangent line at <img alt="" height="54" src="../Images/B18331_14_014.png" style="height: 1.35em !important; vertical-align: -0.34em !important;" width="225"/> as shown in <em class="italic">Figure 14.2</em>:</p>
<figure class="mediaobject"><img alt="A picture containing chart  Description automatically generated" height="289" src="../Images/B18331_14_02.png" width="369"/></figure>
<p class="packt_figref">Figure 14.2: Rate of change for <img alt="" height="50" src="../Images/B18331_14_015.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="167"/> and the tangential line as <img alt="" height="42" src="../Images/B18331_14_016.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="121"/></p>
<p class="normal">For<a id="_idIndexMarker1395"/> instance, considering <img alt="" height="50" src="../Images/B18331_14_015.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="167"/> and the<a id="_idIndexMarker1396"/> derivative <img alt="" height="50" src="../Images/B18331_14_018.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="188"/> in a given point, say <em class="italic">x</em> = 2, we can see that the derivative is positive <img alt="" height="50" src="../Images/B18331_14_019.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="163"/>, as shown in <em class="italic">Figure 14.3</em>:</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="437" src="../Images/B18331_14_03.png" width="401"/></figure>
<p class="packt_figref">Figure 14.3: <img alt="" height="50" src="../Images/B18331_14_015.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="167"/> and <img alt="" height="50" src="../Images/B18331_14_018.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="188"/></p>
<p class="normal">A gradient is a generalization of the derivative for multiple variables. Note that the derivative of a function of a single variable is a scalar-valued function, whereas the gradient of a function of several variables is a vector-valued function. The gradient is denoted with an<a id="_idIndexMarker1397"/> upside-down delta <img alt="" height="42" src="../Images/B18331_14_022.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="25"/>, and called “del” or <em class="italic">nabla</em> from the Greek <a id="_idIndexMarker1398"/>alphabet. This makes sense as delta indicates the change in one variable, and the gradient is the change in all variables. Suppose <img alt="" height="42" src="../Images/B18331_14_023.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="125"/> (e.g. the space of real numbers with <em class="italic">m</em> dimensions) and <em class="italic">f</em> maps from <img alt="" height="42" src="../Images/B18331_14_024.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="50"/> to <img alt="" height="42" src="../Images/B18331_14_025.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="33"/>; the gradient is defined as follows:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_14_026.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="375"/></p>
<p class="normal">In math, a partial derivative <img alt="" height="79" src="../Images/B18331_14_027.png" style="height: 1.98em !important; vertical-align: -0.73em !important;" width="50"/> of a function of several variables is its derivative with respect to one of those variables, with the others held constant. </p>
<p class="normal">Note that it is possible to show that the gradient is a vector (a direction to move) that:</p>
<ul>
<li class="bulletList">Points in the direction of the greatest increase of a function.</li>
<li class="bulletList">Is 0 at a local maximum or local minimum. This is because if it is 0, it cannot increase or decrease further.</li>
</ul>
<p class="normal">The proof is left as an exercise to the interested reader. (Hint: consider <em class="italic">Figure 14.2</em> and <em class="italic">Figure 14.3</em>.)</p>
<h2 class="heading-2" id="_idParaDest-372">Gradient descent</h2>
<p class="normal">If the gradient points in the direction<a id="_idIndexMarker1399"/> of the greatest increase for a function, then it is <a id="_idIndexMarker1400"/>possible to move toward a local minimum for the function by simply moving in a direction opposite to the gradient. That’s the key observation used for gradient descent algorithms, which will be used shortly. </p>
<p class="normal">An example is provided in <em class="italic">Figure 14.4</em>:</p>
<figure class="mediaobject"><img alt="Chart, radar chart  Description automatically generated" height="384" src="../Images/B18331_14_04.png" width="351"/></figure>
<p class="packt_figref">Fig.14.4: Gradient descent for a function in 3 variables</p>
<h2 class="heading-2" id="_idParaDest-373">Chain rule</h2>
<p class="normal">The chain rule says<a id="_idIndexMarker1401"/> that if we have a function <em class="italic">y</em> = <em class="italic">g</em>(<em class="italic">x</em>) and <img alt="" height="54" src="../Images/B18331_14_028.png" style="height: 1.35em !important; vertical-align: -0.34em !important;" width="338"/>, then<a id="_idIndexMarker1402"/> the derivative is defined as follows:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_14_029.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="200"/></p>
<p class="normal">This chaining can be generalized beyond the scalar case. Suppose <img alt="" height="42" src="../Images/B18331_14_023.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="125"/> and <img alt="" height="50" src="../Images/B18331_14_031.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="117"/> with <em class="italic">g</em>, which maps from <img alt="" height="42" src="../Images/B18331_14_032.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="58"/> to <img alt="" height="42" src="../Images/B18331_14_024.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="50"/>, and <em class="italic">f</em>, which maps from <img alt="" height="42" src="../Images/B18331_14_024.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="50"/> to <img alt="" height="42" src="../Images/B18331_14_025.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="33"/>. With <em class="italic">y</em> = <em class="italic">g</em>(<em class="italic">x</em>) and <em class="italic">z</em> = <em class="italic">f</em>(<em class="italic">y</em>), we can deduce:</p>
<p class="center"><img alt="" height="121" src="../Images/B18331_14_036.png" style="height: 3.02em !important; vertical-align: 0.03em !important;" width="292"/></p>
<p class="normal">The generalized chain rule using partial derivatives will be used as a basic tool for the backpropagation algorithm when dealing with functions in multiple variables. Stop for a second and make sure that you fully understand it.</p>
<h2 class="heading-2" id="_idParaDest-374">A few differentiation rules</h2>
<p class="normal">It might be useful to remind<a id="_idIndexMarker1403"/> ourselves of a few additional differentiation<a id="_idIndexMarker1404"/> rules that will be used later:</p>
<ul>
<li class="bulletList">Constant differentiation: <em class="italic">c’ = 0</em>, where <em class="italic">c</em> is a constant.</li>
<li class="bulletList">Variable differentiation: <img alt="" height="71" src="../Images/B18331_14_037.png" style="height: 1.77em !important; vertical-align: -0.58em !important;" width="133"/>, when deriving the differentiation of a variable.</li>
<li class="bulletList">Linear differentiation: <img alt="" height="50" src="../Images/B18331_14_038.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="608"/> </li>
<li class="bulletList">Reciprocal differentiation: <img alt="" height="83" src="../Images/B18331_14_039.png" style="height: 2.08em !important; vertical-align: -0.67em !important;" width="254"/></li>
<li class="bulletList">Exponential differentiation: <img alt="" height="50" src="../Images/B18331_14_040.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="375"/></li>
</ul>
<h2 class="heading-2" id="_idParaDest-375">Matrix operations</h2>
<p class="normal">There are many books about matrix<a id="_idIndexMarker1405"/> calculus. Here we focus only on only a<a id="_idIndexMarker1406"/> few basic operations used for neural networks. Recall that a matrix <img alt="" height="42" src="../Images/B18331_14_041.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="104"/> can be used to represent the weights <em class="italic">w</em><sub class="italic">ij</sub>, with <img alt="" height="42" src="../Images/B18331_14_042.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="175"/>, <img alt="" height="50" src="../Images/B18331_14_043.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="163"/> associated with the arcs between two adjacent layers. Note that by adjusting the weights we can control the “behavior” of the network and that a small change in a specific <em class="italic">w</em><sub class="italic">ij</sub> will be propagated through the network following its topology (see <em class="italic">Figure 14.5</em>, where the edges in bold are the ones impacted by the small change in a specific <em class="italic">w</em><sub class="italic">ij</sub>):</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="327" src="../Images/B18331_14_05.png" width="793"/></figure>
<p class="packt_figref">Figure 14.5: Propagating <em class="italic">w</em><sub class="italic">ij</sub> changes through the network via the edges in bold</p>
<p class="normal">Now that we have reviewed some basic concepts of calculus, let’s start applying them to deep learning. The first question is<a id="_idIndexMarker1407"/> how to optimize activation functions. Well, I am<a id="_idIndexMarker1408"/> pretty sure that you are thinking about computing the derivative, so let’s do it!</p>
<h1 class="heading-1" id="_idParaDest-376">Activation functions</h1>
<p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, we saw a few activation functions including sigmoid, tanh, and ReLU. In the<a id="_idIndexMarker1409"/> section below, we compute the derivative of these activation functions.</p>
<h2 class="heading-2" id="_idParaDest-377">Derivative of the sigmoid</h2>
<p class="normal">Remember that the sigmoid is<a id="_idIndexMarker1410"/> defined as <img alt="" height="71" src="../Images/B18331_14_044.png" style="height: 1.77em !important; vertical-align: -0.58em !important;" width="213"/> (see <em class="italic">Figure 14.6</em>):</p>
<figure class="mediaobject"><img alt="" height="230" src="../Images/B18331_14_06.png" width="793"/></figure>
<p class="packt_figref">Figure 14.6: Sigmoid activation function</p>
<p class="normal">The derivative can<a id="_idIndexMarker1411"/> be computed as follows:</p>
<p class="center"><img alt="" height="242" src="../Images/B18331_14_045.png" style="height: 6.05em !important; vertical-align: -2.66em !important;" width="1067"/></p>
<p class="normal">Therefore the derivative of <img alt="" height="46" src="../Images/B18331_14_046.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="75"/> can be computed as a very simple form: <img alt="" height="54" src="../Images/B18331_14_047.png" style="height: 1.35em !important; vertical-align: -0.34em !important;" width="396"/>.</p>
<h2 class="heading-2" id="_idParaDest-378">Derivative of tanh</h2>
<p class="normal">Remember <a id="_idIndexMarker1412"/>that the arctan <a id="_idIndexMarker1413"/>function is defined as <img alt="" height="75" src="../Images/B18331_14_048.png" style="height: 1.88em !important; vertical-align: -0.56em !important;" width="279"/> as seen in <em class="italic">Figure 14.7</em>:</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="240" src="../Images/B18331_14_07.png" width="706"/></figure>
<p class="packt_figref">Figure 14.7: Tanh activation function</p>
<p class="normal">If you remember that <img alt="" height="71" src="../Images/B18331_14_049.png" style="height: 1.77em !important; vertical-align: -0.58em !important;" width="175"/> and <img alt="" height="71" src="../Images/B18331_14_050.png" style="height: 1.77em !important; vertical-align: -0.58em !important;" width="242"/>, then the derivative is computed as:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_051.png" style="height: 2.60em !important;" width="1588"/></p>
<p class="normal">Therefore the derivative of <img alt="" height="46" src="../Images/B18331_14_052.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="129"/> can be computed as a very simple form: <img alt="" height="46" src="../Images/B18331_14_053.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="413"/>.</p>
<h2 class="heading-2" id="_idParaDest-379">Derivative of ReLU</h2>
<p class="normal">The <a id="_idIndexMarker1414"/>ReLU function is <a id="_idIndexMarker1415"/>defined as <img alt="" height="50" src="../Images/B18331_14_054.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="292"/> (see <em class="italic">Figure 14.8</em>). The derivative of ReLU is:</p>
<p class="center"><img alt="" height="92" src="../Images/B18331_14_055.png" style="height: 2.30em !important;" width="442"/></p>
<p class="normal">Note that ReLU is non-differentiable at zero. However, it is differentiable anywhere else, and the value of the <a id="_idIndexMarker1416"/>derivative at zero can be arbitrarily chosen to be a 0 or 1, as demonstrated in <em class="italic">Figure 14.8</em>:</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="390" src="../Images/B18331_14_08.png" width="488"/></figure>
<p class="packt_figref">Figure 14.8: ReLU activation function</p>
<h1 class="heading-1" id="_idParaDest-380">Backpropagation</h1>
<p class="normal">Now that we have<a id="_idIndexMarker1417"/> computed the derivative of the activation functions, we can describe the backpropagation algorithm — the mathematical core of deep learning. Sometimes, backpropagation<a id="_idIndexMarker1418"/> is called <em class="italic">backprop</em> for short.</p>
<p class="normal">Remember that a neural network can have multiple hidden layers, as well as one input layer and one output layer.</p>
<p class="normal">In addition to that, recall from <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, that backpropagation can be described as a way of progressively correcting mistakes as soon as they are detected. In order to reduce the errors made by a neural network, we must train the network. The training needs a dataset including input values and the corresponding true output value. We want to use the network for predicting output as close as possible to the true output value. The key intuition of the backpropagation algorithm is to update the weights of the connections based on the measured error at the output neuron(s). In the<a id="_idIndexMarker1419"/> remainder of this section, we will explain how to formalize this intuition.</p>
<p class="normal">When backpropagation starts, all the weights have some random assignment. Then the net is activated for each input in the training set; values are propagated forward from the input stage through the hidden stages to the output stage where a prediction is made (note that we keep the figure below simple by only representing a few values with green dotted lines, but in reality, all the values are propagated forward through the network):</p>
<figure class="mediaobject"><img alt="Diagram, schematic  Description automatically generated" height="363" src="../Images/B18331_14_09.png" width="453"/></figure>
<p class="packt_figref">Figure 14.9: Forward step in backpropagation</p>
<p class="normal">Since we know the true observed value in the training set, it is possible to calculate the error made in the prediction. The easiest way to think about backtracking is to propagate the error back (see <em class="italic">Figure 14.10</em>), using an appropriate optimizer algorithm such as gradient descent to adjust the neural network weights, with the goal of reducing the error (again, for the sake of simplicity only a few error values are represented here):</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="325" src="../Images/B18331_14_10.png" width="444"/></figure>
<p class="packt_figref">Figure 14.10: Backward step in backpropagation</p>
<p class="normal">The process of forward propagation<a id="_idIndexMarker1420"/> from input to output and backward propagation of errors is repeated several times until the error goes below a predefined threshold. The whole process is represented in <em class="italic">Figure 14.11</em>. A set of features is selected as input to a machine learning model that produces predictions. </p>
<p class="normal">The predictions are compared with the (true) label, and the resulting loss function is minimized by the optimizer, which updates the weights of the model:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="224" src="../Images/B18331_14_11.png" width="615"/></figure>
<p class="packt_figref">Figure 14.11: Forward propagation and backward propagation</p>
<p class="normal">Let’s see in detail how the forward and backward steps are realized. It might be useful to have a look back at <em class="italic">Figure 14.5</em> and recall that a small change in a specific <em class="italic">w</em><sub class="italic">ij</sub> will be propagated through the network following its topology (see <em class="italic">Figure 14.5</em>, where the edges in bold are the ones impacted by the small change in specific weights).</p>
<h2 class="heading-2" id="_idParaDest-381">Forward step</h2>
<p class="normal">During the forward steps, the inputs <a id="_idIndexMarker1421"/>are multiplied with the weights and then all summed together. Then the activation function is applied (see <em class="italic">Figure 14.12</em>). This step is repeated for each layer, one after another. The first layer takes the input features as input and it produces its output. Then, each subsequent layer takes as input the output of the previous layer:</p>
<figure class="mediaobject"><img alt="Arrow  Description automatically generated with medium confidence" height="286" src="../Images/B18331_14_12.png" width="560"/></figure>
<p class="packt_figref">Figure 14.12: Forward propagation </p>
<p class="normal">If we look at one single layer, mathematically we have two equations:</p>
<ul>
<li class="bulletList">The transfer equation <img alt="" height="46" src="../Images/B18331_14_056.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="258"/>, where <em class="italic">x</em><sub class="italic">i</sub> are the input values, <em class="italic">w</em><sub class="italic">i</sub> are the weights, and <em class="italic">b</em> is the bias. In vector notation <img alt="" height="42" src="../Images/B18331_14_057.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="146"/>. Note that <em class="italic">b</em> can be <em class="italic">absorbed</em> in the summatory by setting <img alt="" height="46" src="../Images/B18331_14_058.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="117"/> and <img alt="" height="46" src="../Images/B18331_14_059.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="108"/>.</li>
<li class="bulletList">The activation function: <img alt="" height="50" src="../Images/B18331_14_060.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="150"/>, where <img alt="" height="42" src="../Images/B18331_07_010.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="25"/> is the chosen activation function.</li>
</ul>
<p class="normal">An artificial neural network consists of an input layer <em class="italic">I</em>, an output layer <em class="italic">O</em>, and any number of hidden layers <em class="italic">H</em><sub class="italic">i</sub> situated between the input and the output layers. For the sake of simplicity, let’s assume that there is only one hidden layer, since the results can be easily generalized.</p>
<p class="normal">As shown in <em class="italic">Figure 14.12</em>, the features <em class="italic">x</em><sub class="italic">i</sub> from the input layer are multiplied by a set of fully connected weights <em class="italic">w</em><sub class="italic">ij</sub> connecting the input layer to the hidden layer (see the left side of <em class="italic">Figure 14.12</em>). The weighted signals are summed together and with the bias to calculate the result <img alt="" height="54" src="../Images/B18331_14_062.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="275"/> (see the center of <em class="italic">Figure 14.12</em>). The result is passed through the activation function <img alt="" height="58" src="../Images/B18331_14_063.png" style="height: 1.45em !important; vertical-align: -0.38em !important;" width="183"/>, which leaves the hidden layer to the output layer (see the right side of <em class="italic">Figure 14.12</em>).</p>
<p class="normal">In summary, during the<a id="_idIndexMarker1422"/> forward step we need to run the following operations:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">For each neuron in a layer, multiply each input by its corresponding weight.</li>
<li class="numberedList">Then for each neuron in the layer, sum all input weights together.</li>
<li class="numberedList">Finally, for each neuron, apply the activation function on the result to compute the new output.</li>
</ol>
<p class="normal">At the end of the forward step, we get a predicted vector <img alt="" height="50" src="../Images/B18331_14_064.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/> from the output layer <em class="italic">o</em> given the input vector <em class="italic">x</em> presented at the input layer. Now the question is: how close is the predicted vector <img alt="" height="50" src="../Images/B18331_14_064.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/> to the true value vector <em class="italic">t</em>?</p>
<p class="normal">That’s where the backstep comes in.</p>
<h2 class="heading-2" id="_idParaDest-382">Backstep</h2>
<p class="normal">To understand how <a id="_idIndexMarker1423"/>close the predicted vector <img alt="" height="50" src="../Images/B18331_14_064.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/> is to the true value vector <em class="italic">t</em>, we need a function<a id="_idIndexMarker1424"/> that measures the error at the output layer <em class="italic">o</em>. That is the <em class="italic">loss function </em>defined earlier in the book. There are many choices for loss function. For instance, we can define the mean squared error defined as follows:</p>
<p class="center"><img alt="" height="117" src="../Images/B18331_14_067.png" style="height: 2.92em !important;" width="325"/></p>
<p class="normal">Note that <em class="italic">E</em> is a quadratic function and, therefore, the difference is quadratically larger when <em class="italic">t</em> is far away from <img alt="" height="50" src="../Images/B18331_14_064.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/>, and the sign is not important. Note that this quadratic error (loss) function is not the only one that we can use. Later in this chapter, we will see how to deal with cross-entropy.</p>
<p class="normal">Now, remember that the key point is that during the training, we want to adjust the weights of the network to minimize the final error. As discussed, we can move toward a local minimum by moving in the opposite direction to the gradient <img alt="" height="42" src="../Images/B18331_14_069.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="83"/>. Moving in the opposite direction to the gradient is the<a id="_idIndexMarker1425"/> reason why this algorithm is called <em class="italic">gradient descent</em>. Therefore, it is reasonable to define the equation for updating the weight <em class="italic">w</em><sub class="italic">ij</sub> as follows:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_14_070.png" style="height: 1.35em !important;" width="283"/></p>
<p class="normal">For a function in <a id="_idIndexMarker1426"/>multiple variables, the gradient is computed using partial derivatives. We introduce the hyperparameter <img alt="" height="46" src="../Images/B18331_01_025.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> – or, in ML lingo, the learning<a id="_idIndexMarker1427"/> rate – to account for how large a step should be in the direction opposite to the gradient.</p>
<p class="normal">Considering the error, <em class="italic">E</em>, we have the equation: </p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_072.png" style="height: 2.60em !important; vertical-align: 0.05em !important;" width="242"/></p>
<p class="normal">The preceding equation is simply capturing the fact that a slight change will impact the final error, as seen in <em class="italic">Figure 14.13</em>:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="286" src="../Images/B18331_14_13.png" width="717"/></figure>
<p class="packt_figref">Figure 14.13: A small change in <em class="italic">w</em><sub class="italic">ij</sub> will impact the final error <em class="italic">E</em></p>
<p class="normal">Let’s define the notation used throughout our equations in the remaining section:</p>
<ul>
<li class="bulletList"><img alt="" height="54" src="../Images/B18331_14_073.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="29"/> is the input to node <em class="italic">j</em> for layer <em class="italic">l</em>.</li>
<li class="bulletList"><img alt="" height="54" src="../Images/B18331_14_074.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="29"/> is the activation function for node <em class="italic">j</em> in layer <em class="italic">l</em> (applied to <img alt="" height="54" src="../Images/B18331_14_073.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="29"/>).</li>
<li class="bulletList"><img alt="" height="58" src="../Images/B18331_14_076.png" style="height: 1.45em !important; vertical-align: -0.38em !important;" width="183"/> is the output of the activation of node <em class="italic">j</em> in layer <em class="italic">l</em>.</li>
<li class="bulletList"><img alt="" height="54" src="../Images/B18331_14_077.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="50"/> is the matrix of weights connecting the neuron <em class="italic">i</em> in layer <img alt="" height="42" src="../Images/B18331_14_078.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="83"/> to the neuron <em class="italic">j</em> in layer <em class="italic">l</em>.</li>
<li class="bulletList"><img alt="" height="54" src="../Images/B18331_14_079.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="29"/> is the bias for unit <em class="italic">j</em> in layer <em class="italic">l</em>.</li>
<li class="bulletList"><img alt="" height="46" src="../Images/B18331_14_080.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/> is the target value for node <em class="italic">o</em> in the output layer.</li>
</ul>
<p class="normal">Now we need to compute the <a id="_idIndexMarker1428"/>partial derivative for the error at the output<a id="_idIndexMarker1429"/> layer <img alt="" height="42" src="../Images/B18331_14_081.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="46"/> when the weights change by <img alt="" height="54" src="../Images/B18331_14_082.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="75"/>. There are two different cases: </p>
<ul>
<li class="bulletList"><strong class="keyWord">Case 1:</strong> Weight update equation for a neuron from hidden (or input) layer to output layer. </li>
<li class="bulletList"><strong class="keyWord">Case 2:</strong> Weight update equation for a neuron from hidden (or input) layer to hidden layer.</li>
</ul>
<p class="normal">We’ll begin with Case 1.</p>
<h3 class="heading-3" id="_idParaDest-383">Case 1: From hidden layer to output layer</h3>
<p class="normal">In this case, we need to<a id="_idIndexMarker1430"/> consider the equation for a neuron from hidden layer <em class="italic">j</em> to output layer <em class="italic">o</em>. Applying the definition of <em class="italic">E</em> and differentiating we have:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_14_083.png" style="height: 3.33em !important; vertical-align: 0.07em !important;" width="783"/></p>
<p class="normal">Here the summation disappears because when we take the partial derivative with respect to the <em class="italic">j</em>-th dimension, the only term that is not zero in the error is the <em class="italic">j</em>-th. Considering that differentiation is a linear operation and that <img alt="" height="83" src="../Images/B18331_14_084.png" style="height: 2.08em !important; vertical-align: -0.83em !important;" width="142"/> – because the true <img alt="" height="46" src="../Images/B18331_14_085.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="29"/> value does not depend on <img alt="" height="54" src="../Images/B18331_14_086.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="54"/> – we have:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_087.png" style="height: 2.70em !important;" width="375"/></p>
<p class="normal">Applying the chain rule again and remembering that <img alt="" height="50" src="../Images/B18331_14_088.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="196"/>, we have:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_089.png" style="height: 2.70em !important;" width="1117"/></p>
<p class="normal">Remembering that <img alt="" height="58" src="../Images/B18331_14_090.png" style="height: 1.45em !important; vertical-align: -0.38em !important;" width="371"/>, we have <img alt="" height="83" src="../Images/B18331_14_091.png" style="height: 2.08em !important; vertical-align: -0.83em !important;" width="217"/> again because when we take the partial derivative with<a id="_idIndexMarker1431"/> respect to the <em class="italic">j</em>-th dimension the only term that is not zero in the error is the <em class="italic">j</em>-th. By definition <img alt="" height="58" src="../Images/B18331_14_092.png" style="height: 1.45em !important; vertical-align: -0.38em !important;" width="183"/>, so putting everything together we have:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_093.png" style="height: 2.60em !important; vertical-align: 0.05em !important;" width="438"/></p>
<p class="normal">The gradient of the error <em class="italic">E</em> with respect to the weights <em class="italic">w</em><sub class="italic">j</sub> from the hidden layer <em class="italic">j</em> to the output layer <em class="italic">o</em> is therefore simply the product of three terms: the difference between the prediction <img alt="" height="50" src="../Images/B18331_14_064.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/> and the true value <img alt="" height="46" src="../Images/B18331_14_080.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/>, the derivative <img alt="" height="46" src="../Images/B18331_14_096.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="121"/> of the output layer activation function, and the activation output <img alt="" height="54" src="../Images/B18331_14_097.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="33"/> of node <em class="italic">j</em> in the hidden layer. For simplicity we can also define <img alt="" height="50" src="../Images/B18331_14_098.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="358"/> and get:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_099.png" style="height: 2.60em !important; vertical-align: 0.05em !important;" width="204"/></p>
<p class="normal">In short, for Case 1, the weight update equation for each of the hidden-output connections is: </p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_100.png" style="height: 2.60em !important; vertical-align: 0.05em !important;" width="325"/></p>
<p class="normal">Note: if we want to explicitly compute the gradient with respect to the output layer biases, the steps to follow are similar to the ones above with only one difference:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_101.png" style="height: 2.70em !important; vertical-align: 0.05em !important;" width="504"/></p>
<p class="normal">so in this case <img alt="" height="75" src="../Images/B18331_14_102.png" style="height: 1.88em !important; vertical-align: -0.68em !important;" width="138"/>.</p>
<p class="normal">Next, we’ll look at <a id="_idIndexMarker1432"/>Case 2.</p>
<h3 class="heading-3" id="_idParaDest-384">Case 2: From hidden layer to hidden layer</h3>
<p class="normal">In this case, we need to consider the equation for a neuron from a hidden layer (or the input layer) to a hidden layer. <em class="italic">Figure 14.13 </em>showed that there is an indirect relationship between the hidden layer weight change <a id="_idIndexMarker1433"/>and the output error. This makes the computation of the gradient a bit more challenging. In this case, we need to consider the equation for a neuron from hidden layer <em class="italic">i</em> to hidden layer <em class="italic">j</em>. </p>
<p class="normal">Applying the definition of <em class="italic">E</em> and differentiating we have:</p>
<p class="center"><img alt="" height="146" src="../Images/B18331_14_103.png" style="height: 3.65em !important; vertical-align: 0.04em !important;" width="1175"/></p>
<p class="normal">In this case, the sum will not disappear because the change of weights in the hidden layer is directly affecting the output. Substituting <img alt="" height="50" src="../Images/B18331_14_104.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="196"/> and applying the chain rule we have:</p>
<p class="center"><img alt="" height="121" src="../Images/B18331_14_105.png" style="height: 3.02em !important;" width="933"/></p>
<p class="normal">The indirect relation between <img alt="" height="46" src="../Images/B18331_14_106.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/> and the internal weights <em class="italic">w</em><sub class="italic">ij</sub> (<em class="italic">Figure 14.13</em>) is mathematically expressed by the expansion:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_14_107.png" style="height: 3.33em !important;" width="946"/></p>
<p class="normal">since <img alt="" height="54" src="../Images/B18331_14_108.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="288"/>.</p>
<p class="normal">This suggests applying the chain rule again:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_109.png" style="height: 2.60em !important; vertical-align: 0.05em !important;" width="117"/></p>
<p class="normal">Applying the chain rule:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_110.png" style="height: 2.70em !important;" width="225"/></p>
<p class="normal">Substituting <img alt="" height="46" src="../Images/B18331_14_106.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/>:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_112.png" style="height: 2.70em !important;" width="275"/></p>
<p class="normal">Deriving:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_113.png" style="height: 2.70em !important;" width="217"/></p>
<p class="normal">Substituting <img alt="" height="58" src="../Images/B18331_14_076.png" style="height: 1.45em !important; vertical-align: -0.38em !important;" width="183"/>:</p>
<p class="center"><img alt="" height="113" src="../Images/B18331_14_115.png" style="height: 2.83em !important; vertical-align: 0.06em !important;" width="263"/></p>
<p class="normal">Applying the chain rule:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_116.png" style="height: 2.70em !important;" width="325"/></p>
<p class="normal">Substituting <img alt="" height="54" src="../Images/B18331_14_117.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="292"/>:</p>
<p class="center"><img alt="" height="113" src="../Images/B18331_14_118.png" style="height: 2.83em !important; vertical-align: 0.06em !important;" width="517"/></p>
<p class="normal">Deriving:</p>
<p class="center"><img alt="" height="58" src="../Images/B18331_14_119.png" style="height: 1.45em !important; vertical-align: 0.01em !important;" width="200"/></p>
<p class="normal">Now we can combine the above two results:</p>
<p class="center"><img alt="" height="117" src="../Images/B18331_14_120.png" style="height: 2.92em !important; vertical-align: 0.03em !important;" width="542"/></p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_121.png" style="height: 2.60em !important; vertical-align: 0.05em !important;" width="325"/></p>
<p class="normal">and get:</p>
<p class="center"><img alt="" height="117" src="../Images/B18331_14_122.png" style="height: 2.92em !important; vertical-align: 0.03em !important;" width="1238"/></p>
<p class="normal">Remembering the definition: <img alt="" height="50" src="../Images/B18331_14_098.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="358"/>, we get:</p>
<p class="center"><img alt="" height="117" src="../Images/B18331_14_124.png" style="height: 2.92em !important; vertical-align: 0.03em !important;" width="1013"/></p>
<p class="normal">This last substitution with <img alt="" height="46" src="../Images/B18331_14_125.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/> is particularly interesting because it backpropagates the signal <img alt="" height="46" src="../Images/B18331_14_125.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/> computed in<a id="_idIndexMarker1434"/> the subsequent layer. The rate of change <img alt="" height="42" src="../Images/B18331_14_081.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="46"/> with respect to the rate of change of the weights <em class="italic">w</em><sub class="italic">ij</sub> is therefore the multiplication of three factors: the output activations <em class="italic">y</em><sub class="italic">i</sub> from the layer below, the derivative of hidden layer activation function <img alt="" height="54" src="../Images/B18331_14_128.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="42"/>, and the sum of the backpropagated signal <img alt="" height="46" src="../Images/B18331_14_125.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/> previously computed in the subsequent layer weighted by <img alt="" height="54" src="../Images/B18331_14_086.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="54"/>. We can use this idea of backpropagating the error signal by defining <img alt="" height="58" src="../Images/B18331_14_131.png" style="height: 1.45em !important; vertical-align: -0.38em !important;" width="346"/> and therefore <img alt="" height="83" src="../Images/B18331_14_132.png" style="height: 2.08em !important; vertical-align: -0.83em !important;" width="179"/>. This suggests that in order to calculate the gradients at any layer <img alt="" height="42" src="../Images/B18331_14_133.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="13"/> in a deep neural network, we can simply multiply the backpropagated error signal <img alt="" height="54" src="../Images/B18331_14_134.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="33"/> and multiply it by the feed-forward signal <img alt="" height="50" src="../Images/B18331_14_135.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="67"/>, arriving at the layer <em class="italic">l</em>. Note that the math is a bit complex but the result is indeed very very simple! The intuition is given in <em class="italic">Figure 14.14</em>. Given a function <img alt="" height="50" src="../Images/B18331_14_136.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="183"/>, computed locally to a neuron with the input <img alt="" height="42" src="../Images/B18331_14_137.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="21"/>, and <img alt="" height="50" src="../Images/B18331_14_138.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="25"/>, the gradients <img alt="" height="71" src="../Images/B18331_14_139.png" style="height: 1.77em !important; vertical-align: -0.51em !important;" width="38"/> are backpropagated. Then, they are combined via the chain rule with the local gradients <img alt="" height="71" src="../Images/B18331_14_140.png" style="height: 1.77em !important; vertical-align: -0.51em !important;" width="42"/> and <img alt="" height="75" src="../Images/B18331_14_141.png" style="height: 1.88em !important; vertical-align: -0.62em !important;" width="42"/> for further backpropagation. </p>
<p class="normal">Here, <em class="italic">L</em> denotes the error from the generic previous layer:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="377" src="../Images/B18331_14_14.png" width="560"/></figure>
<p class="packt_figref">Figure 14.14: An example of the math behind backpropagation</p>
<p class="normal">Note: if we want to explicitly compute the gradient with respect to the output layer biases, it can be proven that <img alt="" height="79" src="../Images/B18331_14_142.png" style="height: 1.98em !important; vertical-align: -0.73em !important;" width="129"/>. We leave this as an exercise for you.</p>
<p class="normal">In short, for Case 2 (hidden-to-hidden connection) the weight delta is <img alt="" height="54" src="../Images/B18331_14_143.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="192"/> and the weight update<a id="_idIndexMarker1435"/> equation for each of the hidden-hidden connections is simply:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_144.png" style="height: 2.60em !important; vertical-align: 0.05em !important;" width="317"/></p>
<p class="normal">We have arrived at the end of this section and all the mathematical tools are defined to make our final statement. The essence of the backstep is nothing more than applying the weight update rule one layer after another, starting from the last output layer and moving back toward the first input layer. Difficult to derive, to be sure, but extremely easy to apply once defined. The whole forward-backward algorithm at the core of deep learning is then the following:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Compute the feedforward signals from the input to the output.</li>
<li class="numberedList">Compute the output error <em class="italic">E</em> based on the predictions <img alt="" height="50" src="../Images/B18331_14_064.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/> and the true value <img alt="" height="46" src="../Images/B18331_14_080.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/>.</li>
<li class="numberedList">Backpropagate the error signals; multiply them with the weights in previous layers and with the gradients of the associated activation functions.</li>
<li class="numberedList">Compute the gradients <img alt="" height="71" src="../Images/B18331_14_147.png" style="height: 1.77em !important; vertical-align: -0.51em !important;" width="42"/> for all of the parameters <img alt="" height="42" src="../Images/B18331_10_024.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> based on the backpropagated error signal and the feedforward signals from the inputs.</li>
<li class="numberedList">Update the parameters using the computed gradients <img alt="" height="71" src="../Images/B18331_14_149.png" style="height: 1.77em !important; vertical-align: -0.51em !important;" width="217"/>.</li>
</ol>
<p class="normal">Note that the above algorithm will work for any choice of differentiable error function <em class="italic">E</em> and for any choice of differentiable activation <img alt="" height="46" src="../Images/B18331_14_150.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="29"/> function. The only requirement is that both must be differentiable.</p>
<p class="normal">Gradient descent with <a id="_idIndexMarker1436"/>backpropagation is not guaranteed to find the global minimum of the loss function, but only a local minimum. However, this is not necessarily a problem observed in practical application.</p>
<h2 class="heading-2" id="_idParaDest-385">Cross entropy and its derivative</h2>
<p class="normal">Gradient descent can be used when cross-entropy is<a id="_idIndexMarker1437"/> adopted as the loss function. As discussed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, the logistic loss function is defined as:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_151.png" style="height: 2.70em !important;" width="846"/></p>
<p class="normal">Where <em class="italic">c</em> refers to one-hot-encoded classes (or labels) whereas <em class="italic">p</em> refers to softmax-applied probabilities. Since <a id="_idIndexMarker1438"/>cross-entropy is applied to softmax-applied probabilities and to one-hot-encoded classes, we need to take into account the chain rule for computing the gradient with respect to the final weights <em class="italic">score</em><sub class="italic">i</sub>. Mathematically, we have:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_14_152.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="375"/></p>
<p class="normal">Computing each part separately, let’s start from <img alt="" height="79" src="../Images/B18331_14_153.png" style="height: 1.98em !important; vertical-align: -0.73em !important;" width="50"/>:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_154.png" style="height: 2.60em !important;" width="1383"/></p>
<p class="normal">(noting that for a fixed <img alt="" height="46" src="../Images/B18331_14_155.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="54"/> all the terms in the sum are constant except the chosen one).</p>
<p class="normal">Therefore, we have:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_156.png" style="height: 2.60em !important;" width="1013"/></p>
<p class="normal">(applying the partial derivative to the sum and considering that <img alt="" height="71" src="../Images/B18331_14_157.png" style="height: 1.77em !important; vertical-align: -0.58em !important;" width="167"/>)</p>
<p class="normal">Therefore, we have:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_158.png" style="height: 2.60em !important;" width="367"/></p>
<p class="normal">Now let’s compute the <a id="_idIndexMarker1439"/>other part <img alt="" height="79" src="../Images/B18331_14_159.png" style="height: 1.98em !important; vertical-align: -0.73em !important;" width="104"/> where <em class="italic">p</em><sub class="italic">i</sub> is the softmax function defined as:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_14_160.png" style="height: 2.60em !important;" width="242"/></p>
<p class="normal">The derivative is:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_161.png" style="height: 2.70em !important; vertical-align: 0.05em !important;" width="696"/></p>
<p class="normal">and </p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_162.png" style="height: 2.70em !important; vertical-align: 0.05em !important;" width="642"/></p>
<p class="normal">Using the Kronecker delta <img alt="" height="92" src="../Images/B18331_14_163.png" style="height: 2.30em !important; vertical-align: -0.83em !important;" width="338"/> we have:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_14_164.png" style="height: 2.70em !important; vertical-align: 0.05em !important;" width="488"/></p>
<p class="normal">Therefore, considering that we are computing the partial derivative, all the components are zeroed with the exception of only one, and we have:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_14_165.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="350"/></p>
<p class="normal">Combining the results, we have:</p>
<p class="center"><img alt="" height="204" src="../Images/B18331_14_166.png" style="height: 5.10em !important; vertical-align: -2.19em !important;" width="1050"/></p>
<p class="normal">Where <em class="italic">c</em><sub class="italic">i</sub> denotes<a id="_idIndexMarker1440"/> the one-hot-encoded classes and <em class="italic">p</em><sub class="italic">i</sub> refers to the softmax probabilities. In short, the derivative is both elegant and easy to compute:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_14_167.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="292"/></p>
<h2 class="heading-2" id="_idParaDest-386">Batch gradient descent, stochastic gradient descent, and mini-batch</h2>
<p class="normal">If we generalize the previous discussion, then we can state that the problem of optimizing a neural network consists of adjusting the weights <em class="italic">w</em> of the network in such a way that the loss function is minimized. Conveniently, we can think about the loss function in the form of a sum, as in this form it’s indeed representing all the loss functions commonly used:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_14_168.png" style="height: 3.33em !important; vertical-align: -0.03em !important;" width="333"/></p>
<p class="normal">In this case, we can perform a derivation using steps very similar to those discussed previously, following the update rule, where <img alt="" height="46" src="../Images/B18331_01_025.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> is the learning rate and <img alt="" height="42" src="../Images/B18331_14_022.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="25"/> is the gradient:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_14_171.png" style="height: 3.33em !important; vertical-align: -0.03em !important;" width="638"/></p>
<p class="normal">In many cases, evaluating the above gradient might require an expensive evaluation of the gradients from all summand functions. When the training set is very large, this can be extremely expensive. If we have three million samples, we have to loop through three million times or use the dot product. That’s a lot! How can we simplify this? There are three types of gradient descent, each different in the way they handle the training dataset.</p>
<h3 class="heading-3" id="_idParaDest-387">Batch gradient descent</h3>
<p class="normal"><strong class="keyWord">Batch Gradient Descent</strong> (<strong class="keyWord">BGD</strong>) computes the change of error but updates the whole model only once the entire dataset has<a id="_idIndexMarker1441"/> been evaluated. Computationally it is very efficient, but it requires that the results for the whole dataset be held in the memory.</p>
<h3 class="heading-3" id="_idParaDest-388">Stochastic gradient descent</h3>
<p class="normal">Instead of updating the model <a id="_idIndexMarker1442"/>once the dataset has been evaluated, <strong class="keyWord">Stochastic Gradient Descent</strong> (<strong class="keyWord">SGD</strong>) does so after every single training example. The key idea is very simple: SGD samples a subset of summand functions at every step.</p>
<h3 class="heading-3" id="_idParaDest-389">Mini-batch gradient descent</h3>
<p class="normal"><strong class="keyWord">Mini-Batch Gradient Descent</strong> (<strong class="keyWord">MBGD</strong>) is very frequently used in deep learning. MBGD (or mini-batch) combines BGD and <a id="_idIndexMarker1443"/>SGD in one single heuristic. The dataset is divided into small batches of about size <em class="italic">bs</em>, generally 64 to 256. Then each of the batches is evaluated separately.</p>
<p class="normal">Note that <em class="italic">bs</em> is another hyperparameter to fine-tune during training. MBGD lies between the extremes of BGD and SGD – by adjusting the batch size and the learning rate parameters, we sometimes find a solution that descends closer to the global minimum than what can be achieved by either of the extremes. </p>
<p class="normal">In contrast with gradient descent, where the cost function is minimized more smoothly, the mini-batch gradient has a bit more of a noisy and bumpy descent, but the cost function still trends downhill. The<a id="_idIndexMarker1444"/> reason for the noise is that mini-batches are a sample of all the examples and this sampling can cause the loss function to oscillate.</p>
<h2 class="heading-2" id="_idParaDest-390">Thinking about backpropagation and ConvNets</h2>
<p class="normal">In this section, we will <a id="_idIndexMarker1445"/>examine backprop and ConvNets. For the sake of simplicity, we will focus on an example of convolution with input <em class="italic">X</em> of size 3x3, one single filter <em class="italic">W</em> of size 2x2 with no padding, stride 1, and no dilation (see <em class="chapterRef">Chapter 3</em>, <em class="italic">Convolutional Neural Networks</em>). The generalization is left as an exercise.</p>
<p class="normal">The standard convolution operation is represented in <em class="italic">Figure 14.15</em>. Simply put, the convolutional operation is the forward pass:</p>
<table class="table-container" id="table001-7">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Input</strong></p>
<p class="normal"><strong class="keyWord">X11</strong></p>
<p class="normal"><strong class="keyWord">X12</strong></p>
<p class="normal"><strong class="keyWord">X13</strong></p>
<p class="normal"><strong class="keyWord">X21</strong></p>
<p class="normal"><strong class="keyWord">X22</strong></p>
<p class="normal"><strong class="keyWord">X23</strong></p>
<p class="normal"><strong class="keyWord">X31</strong></p>
<p class="normal"><strong class="keyWord">X32</strong></p>
<p class="normal"><strong class="keyWord">X33</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Weights</strong></p>
<p class="normal"><strong class="keyWord">W11</strong></p>
<p class="normal"><strong class="keyWord">W12</strong></p>
<p class="normal"><strong class="keyWord">W21</strong></p>
<p class="normal"><strong class="keyWord">W22</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Convolution</strong></p>
<p class="normal"><strong class="keyWord">W11X11+W12X12+W21X21+W22X22</strong></p>
<p class="normal"><strong class="keyWord">W11X12+W12X13+W21X21+W22X23</strong></p>
<p class="normal"><strong class="keyWord">W11X21+W12X22+W21X31+W22X32</strong></p>
<p class="normal"><strong class="keyWord">W11X22+W12X23+W21X32+W22X33</strong></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Figure 14.15: Forward pass for our ConvNet toy example </p>
<p class="normal">Following the examination of <em class="italic">Figure 14.15</em>, we can now focus our attention on the backward pass for the current layer. The key assumption is that we receive a backpropagated signal <img alt="" height="83" src="../Images/B18331_14_172.png" style="height: 2.08em !important; vertical-align: -0.83em !important;" width="63"/> as input, and we need to compute <img alt="" height="83" src="../Images/B18331_14_173.png" style="height: 2.08em !important; vertical-align: -0.83em !important;" width="67"/> and <img alt="" height="83" src="../Images/B18331_14_174.png" style="height: 2.08em !important; vertical-align: -0.83em !important;" width="63"/>. This computation is left as an exercise, but please note that each weight in the filter contributes to each pixel in the output map or, in other words, any change in a weight of a filter affects all the output pixels. </p>
<h2 class="heading-2" id="_idParaDest-391">Thinking about backpropagation and RNNs</h2>
<p class="normal">Remember from <em class="chapterRef">Chapter 5</em>, <em class="italic">Recurrent Neural Networks</em>, the basic equation for an RNN is <img alt="" height="58" src="../Images/B18331_14_175.png" style="height: 1.45em !important; vertical-align: -0.38em !important;" width="383"/>, the final prediction is <img alt="" height="50" src="../Images/B18331_14_176.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="329"/> at step <em class="italic">t</em>, the correct value is <em class="italic">y</em><sub class="italic">t</sub>, and the error <em class="italic">E</em> is the <a id="_idIndexMarker1446"/>cross-entropy. Here <em class="italic">U</em>, <em class="italic">V</em>, and <em class="italic">W</em> are learning parameters used for the RNN’s equations. These equations can be visualized as shown in <em class="italic">Figure 14.16</em>, where we unroll the recurrency. The core idea is that total error is just the sum of the errors at each time step. </p>
<p class="normal">If we used SGD, we need to sum the errors and the gradients at each time step for one given training example:</p>
<figure class="mediaobject"><img alt="" height="305" src="../Images/B18331_14_16.png" width="577"/></figure>
<p class="packt_figref">Figure 14.16: RNN unrolled with equations</p>
<p class="normal">We are not going to write all the tedious math behind all the gradients but rather focus only on a few peculiar cases. For instance, with math computations similar to the ones made in the previous sections, it can be proven by using the chain rule that the gradient for <em class="italic">V</em> depends only on the value at the current time step <em class="italic">s</em><sub class="subscript">3</sub>, <em class="italic">y</em><sub class="subscript">3</sub> and <img alt="" height="50" src="../Images/B18331_14_177.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="38"/>:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_14_178.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="738"/></p>
<p class="normal">However, <img alt="" height="71" src="../Images/B18331_14_179.png" style="height: 1.77em !important; vertical-align: -0.51em !important;" width="54"/> has dependencies carried across time steps because, for instance, <img alt="" height="58" src="../Images/B18331_14_180.png" style="height: 1.45em !important; vertical-align: -0.38em !important;" width="358"/> depends on <em class="italic">s</em><sub class="subscript">2</sub>, which depends on <em class="italic">W</em><sub class="subscript">2</sub> and <em class="italic">s</em><sub class="subscript">1</sub>. As a consequence, the gradient is a bit more complicated because we need to sum up the contributions of each time step:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_14_181.png" style="height: 3.33em !important; vertical-align: 0.07em !important;" width="442"/></p>
<p class="normal">To understand the <a id="_idIndexMarker1447"/>preceding equation, imagine that we are using the standard backpropagation algorithm used for traditional feedforward neural networks but for RNNs. We need to additionally add the gradients of <em class="italic">W</em> across time steps. That’s because we can effectively make the dependencies across time explicit by unrolling the RNN. This is the reason why backprop for RNNs is frequently called <strong class="keyWord">Backpropagation Through Time</strong> (<strong class="keyWord">BPTT</strong>). </p>
<p class="normal">The<a id="_idIndexMarker1448"/> intuition is shown in <em class="italic">Figure 14.17</em>, where the backpropagated signals are represented:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="302" src="../Images/B18331_14_17.png" width="589"/> </figure>
<p class="packt_figref">Figure 14.17: RNN equations and backpropagated signals </p>
<p class="normal">I hope that you have been following up to this point because now the discussion will be slightly more difficult. If we consider:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_14_182.png" style="height: 3.33em !important; vertical-align: 0.07em !important;" width="442"/></p>
<p class="normal">then we notice that <img alt="" height="79" src="../Images/B18331_14_183.png" style="height: 1.98em !important; vertical-align: -0.79em !important;" width="54"/> should be again computed with the chain rule, producing a number of multiplications. In this case, we take the derivative of a vector function with respect to a vector, so we need a matrix whose elements are all the pointwise derivatives (in math, this <a id="_idIndexMarker1449"/>matrix is called a Jacobian). Mathematically, it can be proven that:</p>
<p class="center"><img alt="" height="142" src="../Images/B18331_14_184.png" style="height: 3.55em !important;" width="292"/></p>
<p class="normal">Therefore, we have: </p>
<p class="center"><img alt="" height="154" src="../Images/B18331_14_185.png" style="height: 3.85em !important; vertical-align: 0.04em !important;" width="617"/></p>
<p class="normal">The multiplication in the above equation is particularly problematic since both the sigmoid and tanh get saturated at both ends and their derivative goes to 0. When this happens, they drive other <a id="_idIndexMarker1450"/>gradients in previous layers toward 0. This makes the gradient vanish completely after a few time steps and the network stops learning from “far away.”</p>
<p class="normal"><em class="chapterRef">Chapter 5</em>, <em class="italic">Recurrent Neural Networks</em>, discussed<a id="_idIndexMarker1451"/> how to use <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>) and <strong class="keyWord">Gated Recurrent Units</strong> (<strong class="keyWord">GRUs</strong>) to deal with the problem of vanishing <a id="_idIndexMarker1452"/>gradients and efficiently learn long-range dependencies. In a similar way, the gradient can explode when one single term in the multiplication of the Jacobian matrix becomes large. <em class="chapterRef">Chapter 5</em> discussed how to use gradient clipping to deal with this problem.</p>
<p class="normal">We have now concluded this journey, and you should now understand how backpropagation works and how it is applied in neural networks for dense networks, CNNs, and RNNs. In the next section, we will discuss how TensorFlow computes gradients, and why this is useful for backpropagation.</p>
<h1 class="heading-1" id="_idParaDest-392">A note on TensorFlow and automatic differentiation</h1>
<p class="normal">TensorFlow can automatically <a id="_idIndexMarker1453"/>calculate derivatives, a feature called automatic differentiation. This is achieved by using the chain rule. Every node in the computational graph has an attached gradient operation for calculating the derivatives of input with respect to output. After that, the <a id="_idIndexMarker1454"/>gradients with respect to parameters are automatically computed during backpropagation.</p>
<p class="normal">Automatic differentiation is a very important feature because you do not need to hand-code new variations of backpropagation for each new model of a neural network. This allows for quick iteration and running many experiments faster.</p>
<h1 class="heading-1" id="_idParaDest-393">Summary</h1>
<p class="normal">In this chapter, we discussed the math behind deep learning. Put simply, a deep learning model computes a function given an input vector to produce the output. The interesting part is that it can literally have billions of parameters (weights) to be tuned. Backpropagation is a core mathematical algorithm used by deep learning for efficiently training artificial neural networks, following a gradient descent approach that exploits the chain rule. The algorithm is based on two steps repeated alternatively: the forward step and the backstep. </p>
<p class="normal">During the forward step, inputs are propagated through the network to predict the outputs. These predictions might be different from the true values given to assess the quality of the network. In other words, there is an error and our goal is to minimize it. This is where the backstep plays a role, by adjusting the weights of the network to minimize the error. The error is computed via loss functions such as <strong class="keyWord">Mean Squared Error</strong> (<strong class="keyWord">MSE</strong>), or cross-entropy for non-continuous values such as Boolean (<em class="chapterRef">Chapter 1</em>,<em class="italic"> Neural Network Foundations with TF</em>). A gradient-descent-optimization algorithm is used to adjust the weight of neurons by calculating the gradient of the loss function. Backpropagation computes the gradient, and gradient descent uses the gradients for training the model. A reduction in the error rate of predictions increases accuracy, allowing machine learning models to improve. SGD is the simplest thing you could possibly do by taking one step in the direction of the gradient. This chapter does not cover the math behind other optimizers such as Adam and RMSProp (<em class="chapterRef">Chapter 1</em>). However, they involve using the first and the second moments of the gradients. The first moment involves the exponentially decaying average of the previous gradients, and the second moment involves the exponentially decaying average of the previous squared gradients.</p>
<p class="normal">There are three big properties of our data that justify using deep learning; otherwise, we might as well use regular machine learning: </p>
<ul>
<li class="bulletList">Very-high-dimensional input (text, images, audio signals, videos, and temporal series are frequently a good example).</li>
<li class="bulletList">Dealing with complex decision surfaces that cannot be approximated with a low-order polynomial function.</li>
<li class="bulletList">Having a large amount of training data available. </li>
</ul>
<p class="normal">Deep learning models can be thought of as a computational graph made up of stacking together several basic components such as dense networks (<em class="chapterRef">Chapter 1</em>), CNNs (<em class="chapterRef">Chapter 3</em>), embeddings (<em class="chapterRef">Chapter 4</em>), RNNs (<em class="chapterRef">Chapter 5</em>), GANs (<em class="chapterRef">Chapter 9</em>), autoencoders (<em class="chapterRef">Chapter 8</em>) and, sometimes, adopting shortcut connections such as “peephole,” “skip,” and “residual” because they help data flow a bit more smoothly. Each node in the graph takes tensors as input and produces tensors as output. As discussed, training happens by adjusting the weights in each node with backprop, where the key idea is to reduce the error in the final output node(s) via gradient descent. GPUs and TPUs (<em class="chapterRef">Chapter 15</em>) can significantly accelerate the optimization process since it is essentially based on (hundreds of) millions of matrix computations.</p>
<p class="normal">There are a few other mathematical tools that might be helpful to improve your learning process. Regularization (L1, L2, and Lasso (<em class="chapterRef">Chapter 1</em>)) can significantly improve learning by keeping weights normalized. Batch normalization (<em class="chapterRef">Chapter 1</em>) helps to basically keep track of the mean and the standard deviation of your dataset across multiple deep layers. The key idea is to have data resembling a normal distribution while it flows through the computational graph. Dropout (Chapters <em class="chapterRef">1</em>, <em class="chapterRef">3</em>, <em class="chapterRef">5</em>, <em class="chapterRef">6</em>, <em class="chapterRef">9</em>, and <em class="chapterRef">20</em>) helps by introducing some elements of redundancy in your computation; this prevents overfitting and allows better generalization.</p>
<p class="normal">This chapter has presented the mathematical foundation behind intuition. As discussed, this topic is quite advanced and not necessarily required for practitioners. However, it is recommended reading if you are interested in understanding what is going on “under the hood” when you play with neural networks.</p>
<p class="normal">The next chapter will introduce the <strong class="keyWord">Tensor Processing Unit</strong> (<strong class="keyWord">TPU</strong>), a special chip developed at Google for ultra-fast execution of many mathematical operations described in this chapter.</p>
<h1 class="heading-1" id="_idParaDest-394">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Kelley, Henry J. (1960). <em class="italic">Gradient theory of optimal flight paths</em>. ARS Journal. 30 (10): 947–954. Bibcode:1960ARSJ...30.1127B. doi:10.2514/8.5282.</li>
<li class="numberedList">Dreyfus, Stuart. (1962). <em class="italic">The numerical solution of variational problems</em>. Journal of Mathematical Analysis and Applications. 5 (1): 30–45. doi:10.1016/0022-247x(62)90004-5.</li>
<li class="numberedList">Werbos, P. (1974). <em class="italic">Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences</em>. PhD thesis, Harvard University.</li>
<li class="numberedList">Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (1986-10-09). <em class="italic">Learning representations by back-propagating errors</em>. Nature. 323 (6088): 533–536. Bibcode:1986Natur.323..533R. doi:10.1038/323533a0.</li>
<li class="numberedList">LeCun, Y. (1987). <em class="italic">Modèles Connexionnistes de l’apprentissage (Connectionist Learning Models)</em>, Ph.D. thesis, Université P. et M. Curie.</li>
<li class="numberedList">Herbert Robbins and Sutton Monro. (1951). <em class="italic">A Stochastic Approximation Method. The Annals of Mathematical Statistics, Vol. 22, No. 3</em>. pp. 400–407.</li>
<li class="numberedList">Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (June 2017). <em class="italic">ImageNet classification with deep convolutional neural networks</em> (PDF). Communications of the ACM. 60 (6): 84–90. doi:10.1145/3065386. ISSN 0001-0782. </li>
<li class="numberedList"><em class="italic">From not working to neural networking</em>. The Economist. (25 June 2016)</li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>