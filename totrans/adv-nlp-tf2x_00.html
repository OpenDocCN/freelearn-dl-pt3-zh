<html><head></head><body>
  <div id="_idParaDest-5">
    <h1 class="Introduction-Title--PACKT-">Preface</h1>
    <p class="normal">2017 was a watershed moment for <strong class="keyword">Natural Language Processing</strong> (<strong class="keyword">NLP</strong>), with Transformer-and attention-based networks coming to the fore. The past few years have been as transformational for NLP as AlexNet was for computer vision in 2012. Tremendous advances in NLP have been made, and we are now moving from research labs into applications. </p>
    <p class="normal">These advances span the domains of <strong class="keyword">Natural Language Understanding</strong> (<strong class="keyword">NLU</strong>), <strong class="keyword">Natural Language Generation</strong> (<strong class="keyword">NLG</strong>), and <strong class="keyword">Natural Language Interaction</strong> (<strong class="keyword">NLI</strong>). With so much research in all of these domains, it can be a daunting task to understand the exciting developments in NLP.</p>
    <p class="normal">This book is focused on cutting-edge applications in the fields of NLP, language generation, and dialog systems. It covers the concepts of pre-processing text using techniques such as tokenization, <strong class="keyword">parts-of-speech</strong> (<strong class="keyword">POS</strong>) tagging, and lemmatization using popular libraries such as Stanford NLP and spaCy. <strong class="keyword">Named Entity Recognition</strong> (<strong class="keyword">NER</strong>) models are built from scratch using <strong class="keyword">Bi-directional Long Short-Term Memory networks</strong> (<strong class="keyword">BiLSTMs</strong>), <strong class="keyword">Conditional Random Fields</strong> (<strong class="keyword">CRFs</strong>), and Viterbi decoding. Taking a very practical, application-focused perspective, the book covers key emerging areas such as generating text for use in sentence completion and text summarization, multi-modal networks that bridge images and text by generating captions for images, and managing the dialog aspects of chatbots. It covers one of the most important reasons behind recent advances of NLP – transfer learning and fine tuning. Unlabeled textual data is easily available but labeling this data is costly. This book covers practical techniques that can simplify the labeling of textual data. </p>
    <p class="normal">By the end of the book, I hope you will have advanced knowledge of the tools, techniques, and deep learning architectures used to solve complex NLP problems. The book will cover encoder-decoder networks, <strong class="keyword">Long Short-Term Memory networks</strong> (<strong class="keyword">LSTMs</strong>) and BiLSTMs, CRFs, BERT, GPT-2, GPT-3, Transformers, and other key technologies using TensorFlow. </p>
    <p class="normal">Advanced TensorFlow techniques required for building advanced models are also covered: </p>
    <ul>
      <li class="bullet">Building custom models and layers</li>
      <li class="bullet">Building custom loss functions</li>
      <li class="bullet">Implementing learning rate annealing</li>
      <li class="bullet">Using <code class="Code-In-Text--PACKT-">tf.data</code> for loading data efficiently</li>
      <li class="bullet">Checkpointing models to enable long training times (usually several days)</li>
    </ul>
    <p class="normal">This book contains working code that can be adapted to your own use cases. I hope that you will even be able to do novel state-of-the-art research using the skills you'll gain as you progress through the book.</p>
    <h1 id="_idParaDest-6" class="title">Who this book is for</h1>
    <p class="normal">This book assumes that the reader has some familiarity with the basics of deep learning and the fundamental concepts of NLP. This book focuses on advanced applications and building NLP systems that can solve complex tasks. All kinds of readers will be able to follow the content of the book, but readers who can benefit the most from this book include:</p>
    <ul>
      <li class="bullet">Intermediate <strong class="keyword">Machine Learning</strong> (<strong class="keyword">ML</strong>) developers who are familiar with the basics of supervised learning and deep learning techniques </li>
      <li class="bullet">Professionals who already use TensorFlow/Python for purposes such as data science, ML, research, analysis, etc., and can benefit from a more solid understanding of advanced NLP techniques</li>
    </ul>
    <h1 id="_idParaDest-7" class="title">What this book covers</h1>
    <p class="normal"><em class="chapterRef">Chapter 1</em>, <em class="italic">Essentials of NLP</em>, provides an overview of various topics in NLP such as tokenization, stemming, lemmatization, POS tagging, vectorization, etc. An overview of common NLP libraries like spaCy, Stanford NLP, and NLTK, with their key capabilities and use cases, will be provided. We will also build a simple classifier for spam.</p>
    <p class="normal"><em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>, covers the NLU use case of sentiment analysis with an overview of <strong class="keyword">Recurrent Neural Networks</strong> (<strong class="keyword">RNNs</strong>), LSTMs, and BiLSTMs, which are the basic building blocks of modern NLP models. We will also use <code class="Code-In-Text--PACKT-">tf.data</code> for efficient use of CPUs and GPUs to speed up data pipelines and model training.</p>
    <p class="normal"><em class="chapterRef">Chapter 3</em>, <em class="italic">Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding</em>, focuses on the key NLU problem of NER, which is a basic building block of task-oriented chatbots. We will build a custom layer for CRFs for improving the accuracy of NER and the Viterbi decoding scheme, which is often applied to a deep model to improve the quality of the output.</p>
    <p class="normal"><em class="chapterRef">Chapter 4</em>, <em class="italic">Transfer Learning with BERT</em>, covers a number of important concepts in modern deep NLP such as types of transfer learning, pre-trained embeddings, an overview of Transformers, and BERT and its application in improving the sentiment analysis task introduced in <em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>.</p>
    <p class="normal"><em class="chapterRef">Chapter 5</em>, <em class="italic">Generating Text with RNNs and GPT-2</em>, focuses on generating text with a custom character-based RNN and improving it with Beam Search. We will also cover the GPT-2 architecture and touch upon GPT-3.</p>
    <p class="normal"><em class="chapterRef">Chapter 6</em>, <em class="italic">Text Summarization with Seq2seq Attention and Transformer Networks</em>, takes on the challenging task of abstractive text summarization. BERT and GPT are two halves of the full encoder-decoder model. We put them together to build a seq2seq model for summarizing news articles by generating headlines for them. How ROUGE metrics are used for the evaluation of summarization is also covered.</p>
    <p class="normal"><em class="chapterRef">Chapter 7</em>, <em class="italic">Multi-Modal Networks and Image Captioning with ResNets and Transformers</em>, combines computer vision and NLP together to see if a picture is indeed worth a thousand words! We will build a custom Transformer model from scratch and train it to generate captions for images. </p>
    <p class="normal"><em class="chapterRef">Chapter 8</em>, <em class="italic">Weakly Supervised Learning for Classification with Snorkel</em>, focuses on a key problem – labeling data. While NLP has a lot of unlabeled data, labeling it is quite an expensive task. This chapter introduces the <code class="Code-In-Text--PACKT-">snorkel</code> library and shows how massive amounts of data can be quickly labeled.</p>
    <p class="normal"><em class="chapterRef">Chapter 9</em>, <em class="italic">Building Conversational AI Applications with Deep Learning</em>, combines the various techniques covered throughout the book to show how different types of chatbots, such as question-answering or slot-filling bots, can be built.</p>
    <p class="normal"><em class="chapterRef">Chapter 10</em>, <em class="italic">Installation and Setup Instructions for Code</em>, walks through all the instructions required to install and configure a system for running the code supplied with the book.</p>
    <h1 id="_idParaDest-8" class="title">To get the most out of this book</h1>
    <ul>
      <li class="bullet">It would be a good idea to get a background on the basics of deep learning models and TensorFlow.</li>
      <li class="bullet">The use of a GPU is highly recommended. Some of the models, especially in the later chapters, are pretty big and complex. They may take hours or days to fully train on CPUs. RNNs are very slow to train without the use of GPUs. You can get access to free GPUs on Google Colab, and instructions for doing so are provided in the first chapter.</li>
    </ul>
    <h2 id="_idParaDest-9" class="title">Download the example code files</h2>
    <p class="normal">The code bundle for the book is hosted on GitHub at <a href="https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2"><span class="url">https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2</span></a>. We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/"><span class="url">https://github.com/PacktPublishing/</span></a>. Check them out!</p>
    <h2 id="_idParaDest-10" class="title">Download the color images</h2>
    <p class="normal">We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://static.packt-cdn.com/downloads/9781800200937_ColorImages.pdf"><span class="url">https://static.packt-cdn.com/downloads/9781800200937_ColorImages.pdf</span></a>.</p>
    <h2 id="_idParaDest-11" class="title">Conventions used</h2>
    <p class="normal">There are a number of text conventions used throughout this book.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">CodeInText</code>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. For example: "In the <code class="Code-In-Text--PACKT-">num_capitals()</code> function, substitutions are performed for the capital letters in English."</p>
    <p class="normal">A block of code is set as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">en = snlp.Pipeline(lang=<span class="hljs-string">'en'</span>)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">word_counts</span><span class="hljs-function">(</span><span class="hljs-params">x, pipeline=en</span><span class="hljs-function">):</span>
  doc = pipeline(x)
  count = <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">len</span>(sentence.tokens) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> doc.sentences])
  <span class="hljs-keyword">return</span> count
</code></pre>
    <p class="normal">When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
    <pre class="programlisting code"><code class="hljs-code">en = snlp.Pipeline(lang=<span class="hljs-string">'en'</span>)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">word_counts</span><span class="hljs-function">(</span><span class="hljs-params">x, pipeline=en</span><span class="hljs-function">):</span>
  <span class="code-highlight"><strong class="hljs-slc">doc = pipeline(x)</strong></span>
  count = <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">len</span>(sentence.tokens) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> doc.sentences])
  <span class="hljs-keyword">return</span> count
</code></pre>
    <p class="normal">Any command-line input or output is written as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">!pip install gensim
</code></pre>
    <p class="normal"><strong class="keyword">Bold</strong>: Indicates a new term, an important word, or words that you see on the screen, for example, in menus or dialog boxes, also appear in the text like this. For example: "Select <strong class="keyword">System info</strong> from the <strong class="keyword">Administration</strong> panel."</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Warnings or important notes appear like this.</p>
    </div>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Tips and tricks appear like this.</p>
    </div>
    <h1 id="_idParaDest-12" class="title">Get in touch</h1>
    <p class="normal">Feedback from our readers is always welcome.</p>
    <p class="normal"><strong class="keyword">General feedback</strong>: If you have questions about any aspect of this book, mention the book title in the subject of your message and email Packt at <code class="Code-In-Text--PACKT-">customercare@packtpub.com</code>.</p>
    <p class="normal"><strong class="keyword">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you could report this to us. Please visit <a href="http://www.packtpub.com/support/errata"><span class="url">www.packtpub.com/support/errata</span></a>, select your book, click on the <strong class="screenText">Errata Submission Form</strong> link, and enter the details.</p>
    <p class="normal"><strong class="keyword">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <code class="Code-In-Text--PACKT-">copyright@packtpub.com</code> with a link to the material.</p>
    <p class="normal"><strong class="keyword">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com"><span class="url">http://authors.packtpub.com</span></a>.</p>
    <h2 id="_idParaDest-13" class="title">Reviews</h2>
    <p class="normal">Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
    <p class="normal">For more information about Packt, please visit <a href="http://packtpub.com"><span class="url">packtpub.com</span></a>.</p>
  </div>
</body></html>