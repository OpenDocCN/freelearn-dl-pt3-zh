- en: Audio/Speech/Voice Recognition in IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic audio/speech/voice recognition is becoming a common, convenient way
    for people to interact with their devices, including smartphones, wearables, and
    other smart devices. Machine learning and DL algorithms are useful for audio/speech/voice
    recognition and decision making. Consequently, they are very promising for IoT
    applications, which rely on audio/speech/voice recognition for their activity
    and decisions. This chapter will present DL-based speech/voice data analysis and
    recognition in IoT applications in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of this chapter will briefly describe different IoT applications
    and their speech/voice recognition-based decision making. In addition, it will
    briefly discuss two IoT applications and their speech/voice recognition-based
    implementations in a real-world scenario. In the second part of the chapter, we
    will present a hands-on speech/voice detection implementation of the applications
    using DL algorithms. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: IoT applications and audio/speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case one – voice-controlled smart light
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a voice-controlled smart light
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case two – voice-controlled home access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing voice-controlled home access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL for audio/speech recognition in IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL algorithms for audio/speech recognition in IoT applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different deployment options for DL-based audio/speech recognition in IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection and preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech/voice recognition for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like image recognition, the speech/voice recognition landscape in IoT applications
    is rapidly changing. In recent years, consumers have become depending on voice
    command features and this has been fueled by Amazon, Google, Xiomi, and other
    companies'' voice-enabled search and/or devices. This technology is becoming an
    extremely useful technology for users. Statistics show that around 50% of households
    ([https://techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/](https://techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/))
    in the United States use voice-activated commands for accessing online content.
    Thus, IoT, machine learning, and DL-supported speech/voice recognition has revolutionized
    the focus of businesses and consumer expectations. Many industries—including home
    automation, healthcare, automobiles, and entertainment—are adopting voice-enabled
    IoT applications. As shown in the following diagram, these applications use one
    or more of the following speech/voice recognition services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8cdd501c-ab06-4b16-b800-936f0e920c28.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Speech/command Recognition:** Voice-controlled IoT applications are gaining
    popularity in many application domains, such as smart home/office, smart hospital,
    and smart cars, because of their convenience. For example, a mobility disabled
    person may find difficulty in switching on their TV or light. A voice-controlled/commanded
    TV/light can ease this difficulty by turning on the TV/light simply by listening
    to a voice. This will offer independent living to many disabled individuals and/or
    people with special needs. Voice-activated smart microwave ovens can revolutionize
    cooking. Moreover, a voice enabled smart speaker can assist with and answer many
    common questions in many public service areas, such as hospitals, airports, and
    train stations. For example, a smart voice-enabled speaker can answer patients''
    common questions in hospital, such as when the visiting time is and who the ward
    doctor is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Person/Speaker Identification:** Speaker/person recognition is the second
    important service provided by IoT applications that has received the spotlight
    in recent years. The key applications that are utilizing DL/machine learning-based
    speaker recognition services include personalized voice-controlled assistants,
    smart home appliances, biometric authentication in security services, criminal
    investigations, and smart cars [1,2]. Voice-controlled home/office access is an
    example of biometric authentication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment Analysis/Emotion Detection:** User emotion detection or sentiment
    analysis can be useful in providing personalized and effective services to the
    user. IoT applications, such as smart healthcare [3], smart education, and security
    and safety, can improve their services through DL-based emotion detection or sentiment
    analysis. For example, in a smart classroom, a teacher can analyze the students''
    sentiments in real time or quasi real time to offer personalized and/or group-wise
    teaching. This will improve their learning experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language Translation:** There are 6,500 ([https://www.infoplease.com/askeds/how-many-spoken-languages](https://www.infoplease.com/askeds/how-many-spoken-languages))
    active spoken languages worldwide, and this is a challenge to effective communication
    and interoperability. Many public services, such as the immigration office, can
    use a translator instead of a paid interpreter. Tourists can use smart devices,
    such as **ILI** ([https://iamili.com/us/](https://iamili.com/us/)), to effectively
    communicate with others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case one – voice-controlled smart light
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the **World Health Organisation** (**WHO**), more than one billion
    people in the world live with some form of disability. Almost 20% of them are
    experiencing considerable difficulties in functioning and living independently.
    In the future, disability will be an even bigger concern because of its increasing
    prevalence. IoT applications, such as smart home, with the support of machine
    learning/DL, can offer support to this community and improve their quality of
    life through independence. One of these applications is a voice-activated smart
    light/fan control.
  prefs: []
  type: TYPE_NORMAL
- en: An individual facing a disability such as mobility impairment faces various
    difficulties in living their day-to-day life. One of these difficulties is switching
    on/off home or office lights/fans/other devices. Voice-activated smart control
    of home/office lights/fans/other devices is an IoT application. However, voice
    recognition and the correct detection of a given command is not an easy job. A
    person's accent, pronunciation, and ambient noises can make the person's voice
    recognition difficult. An appropriate DL algorithm trained on a significantly
    large voice dataset can be useful in addressing these issues and can make a working
    voice-controlled smart light application.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing use case one
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram presents the key components needed for the implementation
    of a voice-activated light (in a room):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98aa3b44-3195-4593-a2c6-56f5c37c2678.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding diagram, the implementation of the use case will
    need the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensors and a Computing Platform**: For this use case, we are considering
    two omnidirectional microphones that are installed on the walls of the room. These
    microphones are wirelessly connected to a computing platform. In this use case,
    we are using a Raspberry Pi 3 as the computing platform, and this can work as
    the smart home’s edge-computing device to control the IoT devices deployed in
    the home. We need two more devices: a 433 MHz wireless transmitter, connected
    to the Raspberry Pi, to transmit the processed commands to the switch, and a 433
    MHz remote control or wirelessly controlled mains socket to control the light
    or target device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Voice-Activated Command Detection and Control**: In this phase, the edge-computing
    device will be installed with one app. The installed app on the Raspberry Pi will
    be loaded with a pre-trained voice command detection and classification model.
    Once one of the microphones receives a “switch off the light” command or similar,
    it sends the received commands to the Raspberry Pi for processing and detection
    using the DL model. Finally, the Raspberry Pi transmits detected commands to the
    wirelessly controlled mains socket for the necessary action to be taken on the
    light.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Desktop or Server for Model Learning**: We also need a desktop/server or
    access to a cloud computing platform in order to learn the model for voice detection
    and classification using reference datasets. This learned model will be preinstalled
    in the Raspberry Pi.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second part (in the sections starting from *DL for Sound/Audio Recognition
    in IoT*) of the chapter will describe the implementation of the DL-based anomaly
    detection of the preceding use case. All the necessary code is available in the
    chapter's code folder.
  prefs: []
  type: TYPE_NORMAL
- en: Use case two – voice-controlled home access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating secure and friendly access to homes, offices, and any other premises
    is a challenging task, as it may need keys or an access card (such as a hotel
    room access card) that a user may not always remember to carry with them. The
    use of smart devices, including IoT solutions, can offer secure and friendly access
    to many premises. A potential approach to smart and secure access to homes/offices
    is image recognition-based identification of people and the opening of a door/gate
    accordingly. However, one problem with this approach is that any intruder can
    collect a photograph of one or more permitted persons and present the photo to
    the installed camera to access the office/home. One solution to this problem is
    to use a combination of image recognition and voice recognition or only voice
    recognition to allow access to the home/office.
  prefs: []
  type: TYPE_NORMAL
- en: A voice biometric (or voiceprint) is unique to every individual, and mimicking
    this is a challenging task. However, detection of this unique property is not
    an easy job. DL-based speech recognition can identify unique properties and the
    corresponding person, and allow access only to that person.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing use case two
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, the implementation of the voice-activated
    light (in a room) use case consists of three main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a239f740-81e4-43d7-ac66-b7eefd4c15b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Sensors and computing platform**: For this use case, we are considering one
    omnidirectional microphone installed in the entrance of the home and connected
    to the computing platform wirelessly or concealed in the walls. For the computing
    platform, we are using a Raspberry Pi , and this will work as the smart home''s
    edge-computing device to control the IoT devices deployed in the home. Also, the
    door is installed with a digital lock system that can be controlled through a
    computer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V****oice-activated command detection and control**: In this phase, the edge-computing
    device will be installed with one app. The installed app on the Raspberry Pi will
    be loaded with a pre-trained speaker or person detection and classification model.
    Once an authentic user talks to the door microphone, it gathers the audio signals
    and sends the received speech signal to the Raspberry Pi for processing and person
    detection using the DL model. If the detected person is on the **white list**
    (the list of occupants of the home) of the smart home controller (Raspberry Pi,
    in this case), the controller will command the door to be unlocked, otherwise
    it won''t.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Desktop or server for model learning:** We also need a desktop/server or
    access to a cloud computing platform in order to learn the model for voice detection
    and classification using reference datasets. This learned model will be preinstalled
    in the Raspberry Pi.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the following sections describe the implementation of the DL-based command/speaker
    recognition needed for the aforementioned use cases. All the necessary code is
    available in the chapter's code folder.
  prefs: []
  type: TYPE_NORMAL
- en: DL for sound/audio recognition in IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to understand the working principle of an **Automatic Speech
    Recognition** (**ASR**) system before discussing the useful DL models.
  prefs: []
  type: TYPE_NORMAL
- en: ASR system model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **Automatic Speech Recognition** (**ASR**) system needs three main sources
    of knowledge. These sources are known as an **acoustic model**, a **phonetic lexicon**,
    and a **language model** [4]. Generally, an acoustic model deals with the sounds
    of language, including the phonemes and extra sounds (such as pauses, breathing,
    background noise, and so on). On the other hand, a phonetic lexicon model or dictionary
    includes the words that can be understood by the system, with their possible pronunciations.
    Finally, a language model includes knowledge about the potential word sequences
    of a language. In recent years, DL approaches have been extensively used in acoustic
    and language models of ASR.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram presents a system model for **automatic speech recognition**
    (**ASR**). The model consists of three main stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Data gathering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signal analysis and feature extraction (also known as **preprocessing**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decoding/identification/classification. As shown in the following diagram,
    DL will be used in the identification stage:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eff5f6b3-cac2-47d8-9b2c-897050ff357f.png)'
  prefs: []
  type: TYPE_IMG
- en: Features extraction in ASR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Features extraction** is an important preprocessing stage in a DL pipeline
    of ASR. This stage consists of an analyzer and the extraction of audio fingerprints
    or features. This stage also mainly computes a sequence of feature vectors, which
    provides a compact representation of a gathered speech signal. Generally, this
    task can be performed in three key steps. The first step is known as speech analysis.
    This step carries out a spectra-temporal analysis of the speech signal and generates
    raw features describing the envelope of the power spectrum of short speech intervals.
    The second step extracts an extended feature vector that consists of static and
    dynamic features. The final step converts these extended feature vectors into
    more compact and robust vectors. Importantly, these vectors are the input for
    a DL-based command/speaker/language recognizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A number of feature extraction methods are available for ASR, and **Linear
    Predictive Codes** (**LPC**), **Perceptual Linear Prediction** (**PLP**), and
    **Mel Frequency Cepstral Coefficients** (**MFCC**) are widely used ones. MFCC
    is the most widely used method for feature extraction. The following diagram presents
    the key components of MFCC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1502bfa-3932-49b9-a2b3-bffe7a5d0e4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The key steps of the MFCC are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputting sound files and converting them to original sound data (a time domain
    signal).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting time domain signals into frequency domain signals through short-time
    Fourier transforms, windowing, and framing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Turning frequency into a linear relationship that humans can perceive through
    Mel spectrum transformation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separating the DC component from the sine component by adopting DCT Transform
    through Mel cepstrum analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracting sound spectrum feature vectors and converting them into images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DL models for ASR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A number of DL algorithms or models have been used in ASR. A **Deep Belief Network**
    (**DBN**) is one of the early implementations of DL in ASR. Generally, it has
    been used as a pre-training layer with a single supervised layer of a **Deep Neural
    Network** (**DNN**). **Long Short-Term Memory** (**LSTM**) has been used for large-scale
    acoustic modeling. **Time Delay Neural Network** (**TDNN**) architectures have
    been used for audio signal processing. CNN, which has popularized DL, is also
    used as DL architecture for ASR. Use of DL architectures has significantly improved
    the speech recognition accuracy of ASRs. However, not all DL architectures have
    shown improvements, especially in different types of audio signals and environments,
    such as noisy and reverberant environments. CNNs can be used to reduce spectral
    variations and model the spectral correlation that exists in a speech signal.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) and LSTM are widely used in continuous
    and/or natural language processing because of the capability to incorporate temporal
    features of input during evolution. On the contrary, CNNs are good for short and
    non-continuous audio signals because of their translation invariance, such as
    the skill of discovering structure patterns, regardless of the position. In addition,
    CNNs show the best performance for speech recognition in noisy and reverberant
    environments, and LSTMs are better in clean conditions. The reason for this could
    be CNNs'' emphasis on local correlations as opposed to global ones. In this context,
    we will use CNNs for the implementation of use cases, as voices used for light
    control and speech used for door access are short and non-continuous. In addition,
    their environments can be noisy and reflective.'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs and transfer learning for speech recognition in IoT applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A CNN is a very widely used DL algorithm for image recognition. Recently, this
    has become popular in audio/speech/speaker recognition, as these signals can be
    converted into images. A CNN has different implementations, including two versions
    of Mobilenets, and Incentive V3\. An overview of Mobilenets and Incentive V3 are
    presented in [Chapter 3](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml), *Image Recognition
    in IoT*.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data collection for ASR is a challenging task for many reasons, including privacy.
    Consequently, open source datasets are limited in number. Importantly, these datasets
    may not be easy to access, may have insufficient data/speakers, or may be noisy.
    In this context, we decided to use two different datasets for the two use cases.
    For the voice-driven controlled smart light, we are using Google’s speech command
    datasets, and for use case two, we can scrap data from one of three popular open
    data sources, LibriVox, LibriSpeech ASR, corpus, voxceleb, and YouTube.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google''s speech command dataset includes 65,000 one-second long utterances
    of 30 short words, contributed to by thousands of different members of the public
    through the AIY website. The dataset offers basic audio data on common words such
    as `On`, `Off`, `Yes`, digits, and directions, but this can be useful in testing
    the first use case. For example, the `switch on the light` command can be represented
    by `On` while `switch off the light` can be represented by `Off` data in the dataset.
    Similarly, data gathered on an individual''s speech through scrapping can represent
    the occupants of a home. The second use case will consider a typical home with
    three to five occupants. These occupants will be the white list for the home and
    will be granted access if they are identified. Any people other than the listed
    ones will not be granted automated access to the home. We tested CNN on Google’s
    speech commands dataset and a smaller version of it. The following screenshots
    show a hierarchical view of the smaller dataset used for use case one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5af7802d-765a-4872-b10b-a641843da7b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For use case two, we scrapped data from LibriVox and also downloaded audio
    files from the LibriSpeech ASR corpus. We wrote a web scrapper using BeautifulSoup
    and Selenium for the scrapping. You can write a similar scrapper using other Python
    modules or even other languages, such as Node.js, C, C++, and PHP. The scrapper
    will parse the LibriVox website or any other given link and download the listed
    audio books/files we want. In the following code, we briefly present the scrapper''s
    script, which consists of three main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 1**: Import the necessary Python modules for audio file scrapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Part 2**: Prepare the links for the audio books to be downloaded. Please
    note that the links may include repeated readers, which will be cleaned up to
    produce a non-repeated reader/speaker/home occupants dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Part 3**: Download the audio files from the listed books and form a dataset
    of non-repeatable readers/speakers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After downloading the desired number of reader's/speaker's audio files or `.mp3`
    files (such as five speakers or home occupants), we process the `.mp3` files and
    convert them into fixed-size five-second audio files (`.wav`). We can do this
    through a shell script using tools such as ffmpeg, sox, and mp3splt, or we can
    do it manually (if there are not many readers/occupants and files).
  prefs: []
  type: TYPE_NORMAL
- en: 'As the implementations are based on CNNs, we need to convert the WAV audio
    files into images. The process of converting audio files into images varies according
    to the input data format. We can use `convert_wav2spect.sh` (available in the [Chapter
    4](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml), *Audio/Speech/Voice Recognition
    in IoT* code folder) to convert the WAV files into fixed-size (503 x 800) spectrogram
    color images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Generally, sox, the tool in the preceding script, supports the `.png` format,
    and if we need to convert the images, we can do this through the batch renaming
    of the files from Windows or Command Prompt. The following screenshot shows a
    hierarchical view of the dataset used for use case 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3b3cb1e-f0da-434b-a61a-65db268db505.png)'
  prefs: []
  type: TYPE_IMG
- en: Exploring data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is essential to explore a dataset before applying DL algorithms on the data.
    To explore, firstly, we can run the audio signal (`.wav`) to the image converter, `wav2image.py`
    (available in [Chapter 4](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml), *Audio/Speech/Voice
    Recognition in IoT* code directory), to see how the spectrum image looks. This
    will produce images, as shown. The following screenshot shows converted images
    for an `on` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3981384-dab4-49e0-ba8b-649b84db6b39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows converted images for an `off` command. As we
    can see from the screenshots, their color distributions are different, which will
    be exploited by the DL algorithms in order to recognize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ba84f18-1ca5-4fb4-b862-2fd84139a358.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also carry out group-wise exploration of data, and for this we can run
    `image_explorer.py` on the dataset we want to explore, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot presents a snapshot of the data exploration process
    of the spectrum image data in the speech commands dataset. Interestingly, the
    colors of the images are different than the individual images presented earlier.
    This could be because of the tools we used for them. For the group ones, we used
    the sox tool; while we used `ffmpegf` for the individual ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfcc2321-4038-4716-a4c9-5466e3f98088.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding screenshot of data exploration, the differences between
    four different speech commands in spectrum images may not always be significant.
    This is a challenge in audio signal recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot presents a snapshot of the data exploration process
    of the spectrum image data based on a speaker’s/occupant’s speech (5-second) dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7842bb0-a9af-45ce-8275-ac00b08b795a.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding screenshot, each occupant’s short speech spectrum
    images present a pattern that will help to classify the occupants and grant access
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data preprocessing** is an essential step for a DL pipeline. The speech commands
    dataset consists of 1-second `.wav` files for each short speech command, and these
    files only need to be converted into a spectrum image. However, the downloaded
    audio files for the second use case are not uniform in length; hence, they require
    two-step preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.mp3` to uniform length (such as a 5-second length) WAV file conversion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.wav` file to spectrum image conversion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preprocessing of the datasets is discussed in the data collection section.
    A few issues to be noted during the training image set preparation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Size**: We need to collect at least a hundred images for each class
    in order to train a model that works well. The more we can gather, the better
    the accuracy of the trained model is likely to be. Each of the categories in the
    use case one dataset has more than 3,000 sample images. However, one-shot learning
    (learning with fewer samples) works well with fewer than 100 training samples.
    We also made sure that the images are a good representation of what our application
    will actually face in a real implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data heterogeneity**: Data collected for training should be heterogeneous.
    For example, audio or speech signals about a speaker need to be taken in as wide
    a variety of situations as possible, at different conditions of their voice, and
    with different devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, we are using transfer learning for both use cases, which
    does not require training from scratch; retraining the models with a new dataset
    will sufficiently work in many cases. In addition, in [Chapter 3](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml),
    *Image Recognition in IoT* , we found that Mobilenet V1 is a lightweight (low-memory
    footprint and lower training time) CNN architecture. Consequently, we are implementing
    both uses using the Mobilenet V1 network. Importantly, we will use TensorFlow's
    `retrain.py` module as it is specially designed for CNNs (such as Mobilenet V1)
    based transfer learning).
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to understand the list of key arguments of `retrain.py` before retraining
    Mobilenet V1 on the datasets. For the retraining, if we type in our Terminal (in
    Linux or macOS) or Command Prompt (Windows) `python retrain.py -h`, we will see
    a window like the following screenshot with additional information (such as an
    overview of each argument):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac1fd4ce-9554-46c5-93dd-f976765ca767.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding screenshot, the compulsory argument is the `-–image
    directory`, and it needs to be a dataset directory in which we want to train or
    retrain the models. In the case of Mobilenet V1, we must explicitly mention the
    CNN architecture, such as `--architecture mobilenet_1.0_224`. For the rest of
    the arguments, including data split ratio among training, validation, and test,
    we used the default values. The default split of data is to put 80% of the images
    into the main training set, keep 10% aside to run frequently as validation during
    training, and the final 10% of the data is for testing the real-world performance
    of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the command for running the retraining model for the Mobilenet
    v1 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we run the preceding commands, they will generate the retrain models (`retrained_graph.pb`)
    and labels text (`retrained_labels.txt`) in the given directory and the summary
    directory consists of training and validation summary information for the models.
    The summary information (`--summaries_dir argument with default value retrain_logs)`)
    can be used by TensorBoard to visualize different aspects of the models, including
    the networks and their performance graphs. If we type the following command in
    the Terminal or Command Prompt, it will run `tensorboard`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once TensorBoard is running, navigate your web browser to `localhost:6006` to
    view TensorBoard and view the network of the corresponding model. The following
    diagram presents the network of the Mobilnet V1 used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9529247b-18f8-40bb-83c5-6ef78a80144d.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can evaluate the models from three different aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning/(re)training time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage requirement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance (accuracy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mobilnet V1's retraining and validation process using the `retrain.py` module
    took less than an hour on a desktop (Intel Xenon CPU E5-1650 v3@3.5GHz and 32
    GB RAM) with GPU support.
  prefs: []
  type: TYPE_NORMAL
- en: 'The storage/memory requirement of a model is an essential consideration for
    resource-constrained IoT devices. To evaluate the storage/memory footprint of
    the Mobilenet V1, we compared its storage requirement to another two similar networks''
    (the Incentive V3 and CIFAR-10 CNN) storage requirements. The following screenshot
    presents the storage requirements for the three models. As shown, Mobilenet V1
    requires only 17.1 MB, less than one-fifth of the Incentive V3 (87.5 MB) and CIFAR-10
    CNN (91.1 MB). In terms of storage requirements, Mobilenet V1 is a better choice
    for many resource-constrained IoT devices, including the Raspberry Pi and smartphones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df8c7fd0-c88c-4399-9848-540d8e283d4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we have evaluated the performance of the models. Two levels of performance
    evaluation have been carried out for the use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset-wide evaluation or testing has been done during the retraining phase
    on the desktop PC platform/server side
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Individual audio and a group of home occupants, samples were tested or evaluated
    in the Raspberry Pi 3 environment. All the evaluation performances are presented
    in the following figures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance (use case 1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following screenshots present the evaluation results of Mobilenet V1 on
    a speech command dataset (customized to only five commands, including `on`, `no`,
    `off`, `yes`, and `stop`). Note that `on` is considered to be *switch on the light*
    for use case one due to the lack of a real dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95f50be3-73bc-4066-b89a-f6e152d10def.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot was generated from the TensorBoard log files. The
    orange line represents the training and the blue one represents the validation
    accuracy of the Mobilenet V1 on the command dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9e9d8c2-8965-43e4-b3ee-f0fbce31e3b2.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see from the preceding two screenshots, the performance of the Mobilenet
    V1 is not great, but it will be sufficient for detecting commands by adding more
    information to the commands, such as *switch on the main light* instead of only
    *on*. Furthermore, we can use a better audio file to image converter to improve
    the image quality and recognition accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance (use case 2)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following screenshots represents the evaluation results of Mobilenet V1
    on a `three occupants` dataset. As we can see, the performance of the dataset
    is reasonably good. It can successfully detect occupants more than 90% of the
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5dbedef-8761-444e-9534-31fbcfefaf1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot was generated from the TensorBoard log files. The
    orange line represents the training and the blue one represents the validation
    accuracy of the Mobilenet V1 on the `three occupants` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbed2591-7daf-4640-9ed3-0653a6c3d26e.png)'
  prefs: []
  type: TYPE_IMG
- en: We also tested the Mobilenet V1 on a `five occupants` dataset, and this consistently
    showed accuracy in the range of 85-94%. Finally, we can export the trained model
    detail (such as `retrained_mobilenet_graph.pb` and `retrained_labels.txt`) to
    an IoT device, including a smartphone or Raspberry Pi, and we can test the model
    on new data from both use cases using the provided `label_image.py` code or something
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic audio/speech/voice recognition is becoming a popular means for people
    to interact with their devices, including smartphones, wearables, and other smart
    devices. Machine learning and DL algorithms are essential in audio/speech/voice-based
    decision making.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this chapter, we briefly described different IoT applications
    and their audio/speech/voice detection-based decision making. We also briefly
    discussed two potential use cases of IoT where DL algorithms can be useful in
    speech/command-based decision making. The first use case considered an IoT application
    to make a home smart using voice-controlled lighting. The second use case also
    made a home or office smart, where a DL-based IoT solution offered automated access
    control to the smart home or office. In the second part of the chapter, we briefly
    discussed the data collection process for the use cases, and discussed the rationale
    behind selecting a CNN, especially the Mobilenet V1\. The rest of the sections
    of the chapter describe all the necessary components of the DL pipeline for these
    models and their results.
  prefs: []
  type: TYPE_NORMAL
- en: Many IoT devices and/or users are mobile. Localization of the devices and users
    is essential for offering them services when they are on the move. GPS can support
    outdoor localization, but it does not work in indoor environments. Consequently,
    alternative technologies are necessary for indoor localization. Different indoor
    technologies, including WiFi-fingerprinting, are available, and generally they
    work based on a device's communication signal analysis. In the next chapter ([Chapter
    5](3426d6f2-8913-4585-b04b-f0b3a8bd235d.xhtml), *Indoor Localization in IoT*),
    we will discuss and demonstrate how DL models can be used for indoor localization
    in IoT applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assistive technology: [http://www.who.int/en/news-room/fact-sheets/detail/assistive-technology](http://www.who.int/en/news-room/fact-sheets/detail/assistive-technology)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Smart and Robust Speaker Recognition for Context-Aware In-Vehicle Applications*,
    I Bisio, C Garibotto, A Grattarola, F Lavagetto, and A Sciarrone, in IEEE Transactions
    on Vehicular Technology, vol. 67, no. 9, pp. 8,808-8,821, September, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Emotion-Aware Connected Healthcare Big Data Towards 5G*, M S Hossain and G
    Muhammad, in IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2,399-2,406,
    August, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning Paradigms for Speech Recognition*, L Deng, X Li (2013). IEEE
    Transactions on Audio, Speech, and Language Processing, vol. 2, # 5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*On Comparison of Deep Learning Architectures for Distant Speech Recognition*,
    R Sustika, A R Yuliani, E Zaenudin, and H F Pardede, *2017 Second International
    Conferences on Information Technology, Information Systems and Electrical Engineering
    (ICITISEE)*, Yogyakarta, 2017, pp. 17-21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Neural Networks for Acoustic Modeling in Speech Recognition*, G Hinton,
    L Deng, D Yu, G E Dahl, A R Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen,
    T N Sainath, and B Kingsbury, IEEE Signal Processing Magazine, vol. 29, # 6, pp.
    82–97, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale
    Acoustic Modeling*, H Sak, A Senior, and F Beaufays, in Fifteenth Annual Conference
    of the International Speech Communication Association, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Phoneme recognition using time delay neural network*, IEEE Transaction on
    Acoustics, Speech, and Signal Processing, G. H. K. S. K. J. L. Alexander Waibel,
    Toshiyuki Hanazawa, vol. 37, # 3, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Time Delay Neural Network Architecture for Efficient Modeling of Long Temporal
    Contexts*, V Peddinti, D Povey, and S Khudanpur, in Proceedings of Interspeech.
    ISCA, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Convolutional Neural Network for lvcsr*, B. K. B. R. Tara N Sainath and
    Abdel Rahman Mohamed, in International Conference on Acoustics, Speech and Signal
    Processing. IEEE, 2013, pp. 8614–8618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mel Frequency Cepstral Coefficients for Music Modeling*, Logan, Beth and others,
    ISMIR,vol. 270, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Launching the Speech Commands Dataset*, Pete Warden: [https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
