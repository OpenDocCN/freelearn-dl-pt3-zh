<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer174">
<h1 class="chapterNumber">3</h1>
<h1 class="chapterTitle" id="_idParaDest-66">Convolutional Neural Networks</h1>
<p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, we discussed dense networks, in which each layer is fully connected to the adjacent layers. We looked at one application of those dense networks in classifying the MNIST handwritten characters dataset. In that context, each pixel in the input image has been assigned to a neuron for a total of 784 (28 x 28 pixels) input neurons. However, this strategy does not leverage the spatial structure and relationships between each image. In particular, this piece of code is a dense network that transforms the bitmap representing each written digit into a flat vector where the local spatial structure is removed. Removing the spatial structure is a problem because important information is lost:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#X_train is 60000 rows of 28x28 values --&gt; reshaped in 60000 x 784</span>
X_train = X_train.reshape(<span class="hljs-number">60000</span>, <span class="hljs-number">784</span>)
X_test = X_test.reshape(<span class="hljs-number">10000</span>, <span class="hljs-number">784</span>)
</code></pre>
<p class="normal">Convolutional neural networks leverage spatial information, and they are therefore very well-suited for classifying images. These nets use an ad hoc architecture inspired by biological data taken from physiological experiments performed on the visual cortex. Biological studies show that our vision is based on multiple cortex levels, each one recognizing more and more structured information. First, we see single pixels, then from that, we recognize simple geometric forms and then more and more sophisticated elements such as objects, faces, human bodies, animals, and so on.</p>
<p class="normal">Convolutional neural networks are a fascinating subject. Over a short period of time, they have shown themselves to be a disruptive technology, breaking performance records in multiple domains from text, to video, to speech, going well beyond the initial image processing domain where they were originally conceived. In this chapter, we will introduce the idea of convolutional neural networks (also known as CNNs, DCNNs, and ConvNets), a particular type of neural network that has large importance for deep learning.</p>
<p class="normal">This chapter covers the following topics:</p>
<ul>
<li class="bulletList">Deep convolutional neural networks</li>
<li class="bulletList">An example of a deep convolutional neural network</li>
<li class="bulletList">Recognizing CIFAR-10 images with deep learning</li>
<li class="bulletList">Very deep convolutional networks for large-scale image recognition</li>
<li class="bulletList">Deep Inception V3 networks for transfer learning</li>
<li class="bulletList">Other CNN architectures</li>
<li class="bulletList">Style transfer</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp3"><span class="url">https://packt.link/dltfchp3</span></a>.</p>
</div>
<p class="normal">Let’s begin with deep convolutional neural networks.</p>
<h1 class="heading-1" id="_idParaDest-67">Deep convolutional neural networks</h1>
<p class="normal">A <strong class="keyWord">Deep Convolutional Neural Network</strong> (<strong class="keyWord">DCNN</strong>) consists of many neural network layers. Two different types of layers, convolutional<a id="_idIndexMarker251"/> and pooling (i.e., subsampling), are typically alternated. The depth of each filter increases from left to right in the network. The last stage is typically made of one or more fully connected layers.</p>
<figure class="mediaobject"><img alt="Typical_cnn.png" height="271" src="../Images/B18331_03_01.png" width="877"/></figure>
<p class="packt_figref">Figure 3.1: An example of a DCNN</p>
<p class="normal">There are three key underlying concepts for ConvNets: local receptive fields, shared weights, and pooling. Let’s review them together.</p>
<h2 class="heading-2" id="_idParaDest-68">Local receptive fields</h2>
<p class="normal">If we want to preserve the spatial information<a id="_idIndexMarker252"/> of an image or other form of data, then it is convenient to represent each image with a matrix of pixels. Given this, a simple way to encode the local structure is to connect a submatrix of adjacent input neurons into one single hidden neuron belonging to the next layer. That single hidden neuron represents one local receptive field. Note that this operation is named convolution, and this is where the name for this type of network<a id="_idIndexMarker253"/> is derived. You can think about convolution as the treatment of a matrix by another matrix, referred to as a kernel.</p>
<p class="normal">Of course, we can encode more information by having overlapping submatrices. For instance, let’s suppose that the size of every single submatrix is 5 x 5 and that those submatrices are used with MNIST images of 28 x 28 pixels. Then we will be able to generate 24 x 24 local receptive field neurons in the hidden layer. In fact, it is possible to slide the submatrices by only 23 positions before touching the borders of the images. In TensorFlow, the number of pixels along one edge of the kernel, or submatrix, is the kernel size, and the stride length is the number of pixels by which the kernel is moved at each step in the convolution.</p>
<p class="normal">Let’s define the feature map from one layer to another. Of course, we can have multiple feature maps that learn independently from each hidden layer. For example, we can start with 28 x 28 input neurons for processing MNIST images, and then define <em class="italic">k</em> feature maps of size 24 x 24 neurons each (again with shape of 5 x 5) in the next hidden layer.</p>
<h2 class="heading-2" id="_idParaDest-69">Shared weights and bias</h2>
<p class="normal">Let’s suppose that <a id="_idIndexMarker254"/>we want to move away from the pixel representation in a raw image, by gaining the ability to detect the same feature independently from the location where it is placed in the input image. A simple approach is to use the same set of weights and biases for all the neurons in the hidden layers. In this way, each layer will learn a set of position-independent latent features derived from the image, bearing in mind that a layer consists of a set of kernels in parallel, and each kernel only learns one feature.</p>
<h2 class="heading-2" id="_idParaDest-70">A mathematical example</h2>
<p class="normal">One simple way to <a id="_idIndexMarker255"/>understand convolution is to think about a sliding window function applied to a matrix. In the following example, given the input matrix <strong class="keyWord">I</strong> and the kernel <strong class="keyWord">K</strong>, we get the convolved output. The 3 x 3 kernel <strong class="keyWord">K</strong> (sometimes called the filter or feature detector) is multiplied elementwise with the input matrix to get one cell in the output matrix. All the other cells are obtained by sliding the window over <strong class="keyWord">I</strong>:</p>
<table class="table-container" id="table001-1">
<tbody>
<tr>
<td class="table-cell">
<p>J</p>
<table class="table-container" id="table002">
<tbody>
<tr>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
</tr>
</tbody>
</table>
</td>
<td class="table-cell">
<p>K</p>
<table class="table-container" id="table003">
<tbody>
<tr>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
<td class="table-cell">
<p class="normal">1</p>
</td>
</tr>
</tbody>
</table>
</td>
<td class="table-cell">
<p>Convolved</p>
<table class="table-container" id="table004">
<tbody>
<tr>
<td class="table-cell">
<p class="normal">4</p>
</td>
<td class="table-cell">
<p class="normal">3</p>
</td>
<td class="table-cell">
<p class="normal">4</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">2</p>
</td>
<td class="table-cell">
<p class="normal">4</p>
</td>
<td class="table-cell">
<p class="normal">3</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">2</p>
</td>
<td class="table-cell">
<p class="normal">3</p>
</td>
<td class="table-cell">
<p class="normal">4</p>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p class="normal">In this example, we decided to stop the sliding window as soon as we touch the borders of <strong class="keyWord">I</strong> (so the output is 3 x 3). Alternatively, we could have chosen to pad the input with zeros (so that the output would have been 5 x 5). This decision relates to the padding choice adopted. Note that kernel depth is equal to input depth (channel).</p>
<p class="normal">Another choice is<a id="_idIndexMarker256"/> about how far along we slide our sliding windows with each step. This is called the stride and it can be one or more. A larger stride generates fewer applications of the kernel and a smaller output size, while a smaller stride generates more output and retains more information.</p>
<p class="normal">The size of the filter, the stride, and the type of padding are hyperparameters that can be fine-tuned during the training of the network.</p>
<h2 class="heading-2" id="_idParaDest-71">ConvNets in TensorFlow</h2>
<p class="normal">In TensorFlow, if<a id="_idIndexMarker257"/> we want to <a id="_idIndexMarker258"/>add a convolutional layer with 32 parallel features and a filter size of 3x3, we write:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models
model = models.Sequential()
model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)))
</code></pre>
<p class="normal">This means that we are <a id="_idIndexMarker259"/>applying a 3x3 convolution on 28x28 images with 1 input channel (or input filters) resulting in 32 output <a id="_idIndexMarker260"/>channels (or output filters).</p>
<p class="normal">An example of convolution is provided in <em class="italic">Figure 3.2</em>:</p>
<figure class="mediaobject"><img alt="Screen Shot 2016-12-04 at 8.10.51 PM.png" height="283" src="../Images/B18331_03_02.png" width="625"/></figure>
<p class="packt_figref">Figure.3.2: An example of convolution</p>
<h2 class="heading-2" id="_idParaDest-72">Pooling layers</h2>
<p class="normal">Let’s suppose that we<a id="_idIndexMarker261"/> want to summarize the output of a feature map. Again, we can use the spatial contiguity of the output produced from <a id="_idIndexMarker262"/>a single feature map and aggregate the values of a sub-matrix into one single output value synthetically describing the “meaning” associated with that physical region.</p>
<h3 class="heading-3" id="_idParaDest-73">Max pooling</h3>
<p class="normal">One easy and <a id="_idIndexMarker263"/>common choice is the so-called max pooling operator, which simply outputs the maximum <a id="_idIndexMarker264"/>activation as observed in the region. In Keras, if we want to define a max pooling layer of size 2 x 2, we write:</p>
<pre class="programlisting code"><code class="hljs-code">model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
</code></pre>
<p class="normal">An example of the max-pooling operation is given in <em class="italic">Figure 3.3</em>:</p>
<figure class="mediaobject"><img alt="Screen Shot 2016-12-04 at 7.49.01 PM.png" height="213" src="../Images/B18331_03_03.png" width="370"/></figure>
<p class="packt_figref">Figure 3.3: An example of max pooling</p>
<h3 class="heading-3" id="_idParaDest-74">Average pooling</h3>
<p class="normal">Another choice is average pooling, which simply aggregates a region into the average values of the<a id="_idIndexMarker265"/> activations observed in that region.</p>
<p class="normal">Note that Keras implements a large number of pooling layers, and a complete list is available online (see <a href="https://keras.io/layers/pooling/"><span class="url">https://keras.io/layers/pooling/</span></a>). In short, all the pooling operations are<a id="_idIndexMarker266"/> nothing more than a summary operation on a given region.</p>
<h2 class="heading-2" id="_idParaDest-75">ConvNets summary</h2>
<p class="normal">So far, we have described the basic concepts of ConvNets. CNNs apply convolution and pooling operations in <a id="_idIndexMarker267"/>one dimension for audio and text data along the time dimension, in two dimensions for images along the (height x width) dimensions, and in three dimensions for videos along the (height x width x time) dimensions. For images, sliding the filter over an input volume produces a map that provides the responses of the filter for each spatial position. </p>
<p class="normal">In other words, a ConvNet has multiple filters stacked together that learn to recognize specific visual features independently from the location in the image itself. Those visual features are simple in the initial layers of the network and become more and more sophisticated deeper in the network. Training of a CNN requires the identification of the right values for each filter so that an input, when passed through multiple layers, activates certain neurons of the last layer so that it will predict the correct values.</p>
<h1 class="heading-1" id="_idParaDest-76">An example of DCNN: LeNet</h1>
<p class="normal">Yann LeCun, who <a id="_idIndexMarker268"/>won the<a id="_idIndexMarker269"/> Turing Award, proposed [1] a family of ConvNets named LeNet, trained for recognizing MNIST handwritten characters with robustness to simple geometric transformations and distortion. The core idea of LeNet is to have lower layers alternating convolution operations with max-pooling operations. The convolution operations are based on <a id="_idIndexMarker270"/>carefully chosen local receptive fields with shared weights for multiple feature maps. Then, higher levels are fully connected based on a traditional MLP with hidden layers and softmax as the output layer.</p>
<h2 class="heading-2" id="_idParaDest-77">LeNet code in TF</h2>
<p class="normal">To define a LeNet in code, we<a id="_idIndexMarker271"/> use a convolutional 2D module (note that <code class="inlineCode">tf.keras.layers.Conv2D</code> is an alias of <code class="inlineCode">tf.keras.layers.Convolution2D</code>, so the two can be used in an interchangeable way – see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D</span></a>):</p>
<pre class="programlisting code"><code class="hljs-code">layers.Convolution2D(<span class="hljs-number">20</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), activation=<span class="hljs-string">'relu'</span>, input_shape=input_shape)
</code></pre>
<p class="normal">where the first parameter is the number of output filters in the convolution and the next tuple is the extension of each filter. An interesting optional parameter is padding. There are two options: <code class="inlineCode">padding='valid'</code> means that the convolution is only computed where the input and the filter fully overlap and therefore the output is smaller than the input, while <code class="inlineCode">padding='same'</code> means that we have an output that is the <code class="inlineCode">same</code> size as the input, for which the area around the input is padded with zeros.</p>
<p class="normal">In addition, we use a <code class="inlineCode">MaxPooling2D</code> module:</p>
<pre class="programlisting code"><code class="hljs-code">layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
</code></pre>
<p class="normal">where <code class="inlineCode">pool_size=(2, 2)</code> is a tuple of 2 integers representing the factors by which the image is vertically and horizontally downscaled. So (2, 2) will halve the image in each dimension, and <code class="inlineCode">strides=(2, 2)</code> is the stride used for processing.</p>
<p class="normal">Now, let us review the code. First, we import a number of modules:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models, optimizers
<span class="hljs-comment"># network and training</span>
EPOCHS = <span class="hljs-number">5</span>
BATCH_SIZE = <span class="hljs-number">128</span>
VERBOSE = <span class="hljs-number">1</span>
OPTIMIZER = tf.keras.optimizers.Adam()
VALIDATION_SPLIT=<span class="hljs-number">0.90</span>
IMG_ROWS, IMG_COLS = <span class="hljs-number">28</span>, <span class="hljs-number">28</span> <span class="hljs-comment"># input image dimensions</span>
INPUT_SHAPE = (IMG_ROWS, IMG_COLS, <span class="hljs-number">1</span>)
NB_CLASSES = <span class="hljs-number">10</span>  <span class="hljs-comment"># number of outputs = number of digits</span>
</code></pre>
<p class="normal">Then we define the LeNet network:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#define the convnet </span>
<span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">input_shape, classes</span>):
    model = models.Sequential()
</code></pre>
<p class="normal">We have a first convolutional <a id="_idIndexMarker272"/>stage with ReLU activations followed by max pooling. Our network will learn 20 convolutional filters, each one of which has a size of 5x5. The output dimension is the same as the input shape, so it will be 28 x 28. Note that since <code class="inlineCode">Convolutional2D</code> is the first stage of our pipeline, we are also required to define its <code class="inlineCode">input_shape</code>. </p>
<p class="normal">The max pooling operation implements a sliding window which slides over the layer and takes the maximum of each region with a step of two pixels both vertically and horizontally:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># CONV =&gt; RELU =&gt; POOL</span>
model.add(layers.Convolution2D(<span class="hljs-number">20</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), activation=<span class="hljs-string">'relu'</span>,
            input_shape=input_shape))
model.add(layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
</code></pre>
<p class="normal">Then there is a second convolutional stage with ReLU activations, followed again by a max pooling layer. In this case, we increase the number of convolutional filters learned to 50 from the previous 20. Increasing the number of filters in deeper layers is a common technique in deep learning:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># CONV =&gt; RELU =&gt; POOL</span>
model.add(layers.Convolution2D(<span class="hljs-number">50</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), activation=<span class="hljs-string">'relu'</span>))
model.add(layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
</code></pre>
<p class="normal">Then we have a pretty standard flattening and a dense network of 500 neurons, followed by a softmax classifier with 10 classes:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Flatten =&gt; RELU layers</span>
model.add(layers.Flatten())
model.add(layers.Dense(<span class="hljs-number">500</span>, activation=<span class="hljs-string">'relu'</span>))
<span class="hljs-comment"># a softmax classifier</span>
model.add(layers.Dense(classes, activation=<span class="hljs-string">"softmax"</span>))
<span class="hljs-keyword">return</span> model
</code></pre>
<p class="normal">Congratulations, you have just defined your first deep convolutional learning network! Let’s see how it looks visually:</p>
<figure class="mediaobject"><img alt="Screen Shot 2016-12-04 at 8.51.07 PM.png" height="333" src="../Images/B18331_03_04.png" width="880"/></figure>
<p class="packt_figref">Figure 3.4: Visualization of LeNet </p>
<p class="normal">Now we need some additional code for training the network, but this is very similar to what we <a id="_idIndexMarker273"/>described in <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>. This time we also show the code for printing the loss:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># data: shuffled and split between train and test sets</span>
(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()
<span class="hljs-comment"># reshape</span>
X_train = X_train.reshape((<span class="hljs-number">60000</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))
X_test = X_test.reshape((<span class="hljs-number">10000</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))
<span class="hljs-comment"># normalize</span>
X_train, X_test = X_train / <span class="hljs-number">255.0</span>, X_test / <span class="hljs-number">255.0</span>
<span class="hljs-comment"># cast</span>
X_train = X_train.astype(<span class="hljs-string">'float32'</span>)
X_test = X_test.astype(<span class="hljs-string">'float32'</span>)
<span class="hljs-comment"># convert class vectors to binary class matrices</span>
y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)
y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)
<span class="hljs-comment"># initialize the optimizer and model</span>
model = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)
model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">"categorical_crossentropy"</span>, optimizer=OPTIMIZER,
    metrics=[<span class="hljs-string">"accuracy"</span>])
model.summary()
<span class="hljs-comment"># use TensorBoard, princess Aurora!</span>
callbacks = [
  <span class="hljs-comment"># Write TensorBoard logs to './logs' directory</span>
  tf.keras.callbacks.TensorBoard(log_dir=<span class="hljs-string">'./logs'</span>)
]
<span class="hljs-comment"># fit </span>
history = model.fit(X_train, y_train, 
        batch_size=BATCH_SIZE, epochs=EPOCHS, 
        verbose=VERBOSE, validation_split=VALIDATION_SPLIT,
        callbacks=callbacks)
score = model.evaluate(X_test, y_test, verbose=VERBOSE)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nTest score:"</span>, score[<span class="hljs-number">0</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Test accuracy:'</span>, score[<span class="hljs-number">1</span>])
</code></pre>
<p class="normal">Now let’s run the code. As <a id="_idIndexMarker274"/>you can see in <em class="italic">Figure 3.5</em>, the time had a significant increase, and each iteration in our deep net now takes ~28 seconds against ~1-2 seconds for the network defined in <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>. However, the accuracy reached a new peak at 99.991% on training, 99.91% on validation, and 99.15% on test!</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="451" src="../Images/B18331_03_05.png" width="670"/></figure>
<p class="packt_figref">Figure 3.5: LeNet accuracy</p>
<p class="normal">Let’s see the execution <a id="_idIndexMarker275"/>of a full run for 20 epochs:</p>
<pre class="programlisting con"><code class="hljs-con">Model: "sequential_1"
_____________________________________________________________________
Layer (type)                    Output Shape              Param #    
=====================================================================
conv2d_2 (Conv2D)               (None, 24, 24, 20)        520        
                                                                     
max_pooling2d_2 (MaxPooling  2D) (None, 12, 12, 20)       0          
                                                                     
conv2d_3 (Conv2D)               (None, 8, 8, 50)          25050      
                                                                     
max_pooling2d_3 (MaxPooling  2D) (None, 4, 4, 50)         0          
                                                                     
flatten   (Flatten)             (None, 800)               0          
                                                                     
dense   (Dense)                 (None, 500)               400500     
                                                                     
dense_1 (Dense)                 (None, 10)                5010    
                                                                     
=====================================================================
Total params: 431,080
Trainable params: 431,080
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
[2019-04-04 14:18:28.546158: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profile Session started.
48000/48000 [==============================] - 28s 594us/sample - loss: 0.2035 - accuracy: 0.9398 - val_loss: 0.0739 - val_accuracy: 0.9783
Epoch 2/20
48000/48000 [==============================] - 26s 534us/sample - loss: 0.0520 - accuracy: 0.9839 - val_loss: 0.0435 - val_accuracy: 0.9868
Epoch 3/20
48000/48000 [==============================] - 27s 564us/sample - loss: 0.0343 - accuracy: 0.9893 - val_loss: 0.0365 - val_accuracy: 0.9895
Epoch 4/20
48000/48000 [==============================] - 27s 562us/sample - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.0452 - val_accuracy: 0.9868
Epoch 5/20
48000/48000 [==============================] - 27s 562us/sample - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.0428 - val_accuracy: 0.9873
Epoch 6/20
48000/48000 [==============================] - 28s 548us/sample - loss: 0.0585 - accuracy: 0.9820 - val_loss: 0.1038 - val_accuracy: 0.9685
Epoch 7/20
48000/48000 [==============================] - 26s 537us/sample - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0388 - val_accuracy: 0.9896
Epoch 8/20
48000/48000 [==============================] - 29s 589us/sample - loss: 0.0097 - accuracy: 0.9966 - val_loss: 0.0347 - val_accuracy: 0.9899
Epoch 9/20
48000/48000 [==============================] - 29s 607us/sample - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0515 - val_accuracy: 0.9859
Epoch 10/20
48000/48000 [==============================] - 27s 565us/sample - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0376 - val_accuracy: 0.9904
Epoch 11/20
48000/48000 [==============================] - 30s 627us/sample - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.0366 - val_accuracy: 0.9911
Epoch 12/20
48000/48000 [==============================] - 24s 505us/sample - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0389 - val_accuracy: 0.9910
Epoch 13/20
48000/48000 [==============================] - 28s 584us/sample - loss: 0.0057 - accuracy: 0.9978 - val_loss: 0.0531 - val_accuracy: 0.9890
Epoch 14/20
48000/48000 [==============================] - 28s 580us/sample - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.0409 - val_accuracy: 0.9911
Epoch 15/20
48000/48000 [==============================] - 26s 537us/sample - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0436 - val_accuracy: 0.9911
Epoch 16/20
48000/48000 [==============================] - 25s 513us/sample - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0480 - val_accuracy: 0.9890
Epoch 17/20
48000/48000 [==============================] - 24s 499us/sample - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0535 - val_accuracy: 0.9888
Epoch 18/20
48000/48000 [==============================] - 24s 505us/sample - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0349 - val_accuracy: 0.9926
Epoch 19/20
48000/48000 [==============================] - 29s 599us/sample - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0377 - val_accuracy: 0.9920
Epoch 20/20
48000/48000 [==============================] - 25s 524us/sample - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0477 - val_accuracy: 0.9917
10000/10000 [==============================] - 2s 248us/sample - loss: 0.0383 - accuracy: 0.9915
Test score: 0.03832608199457617
Test accuracy: 0.9915
</code></pre>
<p class="normal">Let’s plot the model<a id="_idIndexMarker276"/> accuracy and the model loss, and we understand that we can train in only 10 iterations to achieve a similar accuracy of 99.1%:</p>
<pre class="programlisting con"><code class="hljs-con">Train on 48000 samples, validate on 12000 samples
Epoch 1/10
[2019-04-04 15:57:17.848186: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profile Session started.
48000/48000 [==============================] - 26s 544us/sample - loss: 0.2134 - accuracy: 0.9361 - val_loss: 0.0688 - val_accuracy: 0.9783
Epoch 2/10
48000/48000 [==============================] - 30s 631us/sample - loss: 0.0550 - accuracy: 0.9831 - val_loss: 0.0533 - val_accuracy: 0.9843
Epoch 3/10
48000/48000 [==============================] - 30s 621us/sample - loss: 0.0353 - accuracy: 0.9884 - val_loss: 0.0410 - val_accuracy: 0.9874
Epoch 4/10
48000/48000 [==============================] - 37s 767us/sample - loss: 0.0276 - accuracy: 0.9910 - val_loss: 0.0381 - val_accuracy: 0.9887
Epoch 5/10
48000/48000 [==============================] - 24s 509us/sample - loss: 0.0200 - accuracy: 0.9932 - val_loss: 0.0406 - val_accuracy: 0.9881
Epoch 6/10
48000/48000 [==============================] - 31s 641us/sample - loss: 0.0161 - accuracy: 0.9950 - val_loss: 0.0423 - val_accuracy: 0.9881
Epoch 7/10
48000/48000 [==============================] - 29s 613us/sample - loss: 0.0129 - accuracy: 0.9955 - val_loss: 0.0396 - val_accuracy: 0.9894
Epoch 8/10
48000/48000 [==============================] - 27s 554us/sample - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.0454 - val_accuracy: 0.9871
Epoch 9/10
48000/48000 [==============================] - 24s 510us/sample - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0388 - val_accuracy: 0.9902
Epoch 10/10
48000/48000 [==============================] - 26s 542us/sample - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.0440 - val_accuracy: 0.99892
10000/10000 [==============================] - 2s 196us/sample - loss: 0.0327 - accuracy: 0.9910
Test score: 0.03265062951518773
Test accuracy: 0.991
</code></pre>
<p class="normal">Let us see some of the <a id="_idIndexMarker277"/>MNIST images just to understand how good the number 99.1% is! For instance, there are many ways in which humans write a 9, one of them being in <em class="italic">Figure 3.6</em>. The same goes for 3, 7, 4, and 5, and number 1 in this figure is so difficult to recognize that even a human would likely have trouble:</p>
<figure class="mediaobject"><img alt="Screen Shot 2016-12-04 at 8.19.34 PM.png" height="156" src="../Images/B18331_03_06.png" width="223"/></figure>
<p class="packt_figref">Figure 3.6: An example of MNIST handwritten characters</p>
<p class="normal">We can summarize all the progress made so far with our different models in the following graph. Our simple net started with an accuracy of 90.71%, meaning that about 9 handwritten characters<a id="_idIndexMarker278"/> out of 100 are not correctly recognized. Then, we gained 8% with the deep learning architecture, reaching an accuracy of 99.2%, which means that less than one handwritten character out of one hundred is incorrectly recognized, as shown in <em class="italic">Figure 3.7</em>:</p>
<figure class="mediaobject"><img alt="Chart, line chart, scatter chart  Description automatically generated" height="455" src="../Images/B18331_03_07.png" width="709"/></figure>
<p class="packt_figref">Figure 3.7: Accuracy for different models and optimizers</p>
<h2 class="heading-2" id="_idParaDest-78">Understanding the power of deep learning</h2>
<p class="normal">Another test we can run for a better understanding of the power of deep learning and ConvNets is to reduce the <a id="_idIndexMarker279"/>size of the training set and observe the resulting decay in performance. One way to do this is to split the training set of 50,000 examples into two different sets:</p>
<ul>
<li class="bulletList">The proper training set used for training our model will progressively reduce in size: 5,900, 3,000, 1,800, 600, and 300 examples.</li>
<li class="bulletList">The validation set used to estimate how well our model has been trained will consist of the remaining examples. Our test set is always fixed, and it consists of 10,000 examples.</li>
</ul>
<p class="normal">With this setup, we compare the previously defined deep learning ConvNet against the first example neural <a id="_idIndexMarker280"/>network defined in <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>. As we can see in the following graph, our deep network always outperforms the simple network when there is more data available. With 5,900 training examples, the deep learning net had an accuracy of 97.23% against an accuracy of 94% for the simple net. </p>
<p class="normal">In general, deep networks require more training data available to fully express their power, as shown in <em class="italic">Figure 3.8</em>:</p>
<figure class="mediaobject"> <img alt="Chart, line chart  Description automatically generated" height="468" src="../Images/B18331_03_08.png" width="754"/></figure>
<p class="packt_figref">Figure 3.8: Accuracy for different amounts of data</p>
<p class="normal">A list of state-of-the-art results (for example, the highest performance available) for MNIST is available online (see <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml"><span class="url">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml</span></a>). As of March 2019, the best result has an error rate of 0.21% [2].</p>
<h1 class="heading-1" id="_idParaDest-79">Recognizing CIFAR-10 images with deep learning</h1>
<p class="normal">The CIFAR-10 dataset <a id="_idIndexMarker281"/>contains 60,000 color images <a id="_idIndexMarker282"/>of 32 x 32 pixels in three channels, divided into 10 classes. Each class contains 6,000 images. The training set contains 50,000 images, while the test set provides 10,000 images. This image taken from the CIFAR repository (see <a href="https://www.cs.toronto.edu/~kriz/cifar.xhtml"><span class="url">https://www.cs.toronto.edu/~kriz/cifar.xhtml</span></a>) shows a few random examples from the 10 classes:</p>
<figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" height="452" src="../Images/B18331_03_09.png" width="585"/></figure>
<p class="packt_figref">Figure 3.9: An example of CIFAR-10 images</p>
<div class="note">
<p class="normal">The images in this section are from <em class="italic">Learning Multiple Layers of Features from Tiny Images</em>, Alex Krizhevsky, 2009: <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf"><span class="url">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</span></a>. They are part of the CIFAR-10 dataset (toronto.edu): <a href="https://www.cs.toronto.edu/~kriz/cifar.xhtml"><span class="url">https://www.cs.toronto.edu/~kriz/cifar.xhtml</span></a>.</p>
</div>
<p class="normal">The goal is to recognize previously unseen images and assign them to one of the ten classes. Let us define a suitable deep net.</p>
<p class="normal">First of all, we import a number of useful modules and define a few constants and load the dataset (the full code including the load operations is available online): </p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models, optimizers
<span class="hljs-comment"># CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels</span>
IMG_CHANNELS = <span class="hljs-number">3</span>
IMG_ROWS = <span class="hljs-number">32</span>
IMG_COLS = <span class="hljs-number">32</span>
<span class="hljs-comment">#constant</span>
BATCH_SIZE = <span class="hljs-number">128</span>
EPOCHS = <span class="hljs-number">20</span>
CLASSES = <span class="hljs-number">10</span>
VERBOSE = <span class="hljs-number">1</span>
VALIDATION_SPLIT = <span class="hljs-number">0.2</span>
OPTIM = tf.keras.optimizers.RMSprop()
</code></pre>
<p class="normal">Our net will learn 32 convolutional filters, each of which with a 3 x 3 size. The output dimension is the same as<a id="_idIndexMarker283"/> the input shape, so it will be 32 x 32 and the activation function used is a ReLU function, which is a simple way<a id="_idIndexMarker284"/> of introducing non-linearity. After that, we have a <code class="inlineCode">MaxPooling</code> operation with a pool size of 2 x 2 and dropout at 25%:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#define the convnet </span>
<span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">input_shape, classes</span>):
    model = models.Sequential() 
    model.add(layers.Convolution2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>,
                        input_shape=input_shape))
    model.add(layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
    model.add(layers.Dropout(<span class="hljs-number">0.25</span>)) 
</code></pre>
<p class="normal">The next stage in the deep pipeline is a dense network with 512 units and ReLU activation followed by dropout at 50% and by a softmax layer with 10 classes as output, one for each category:</p>
<pre class="programlisting code"><code class="hljs-code">    model.add(layers.Flatten())
    model.add(layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.Dropout(<span class="hljs-number">0.5</span>))
    model.add(layers.Dense(classes, activation=<span class="hljs-string">'softmax'</span>))
<span class="hljs-keyword">    return</span> model
</code></pre>
<p class="normal">After defining the network, we can train the model. In this case, we split the data and compute a validation set in addition to the training and testing sets. The training is used to build our models, the <a id="_idIndexMarker285"/>validation is used to select the best-performing approach, while the test set is used to check the performance<a id="_idIndexMarker286"/> of our best models on fresh unseen data:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># use TensorBoard, princess Aurora!</span>
callbacks = [
  <span class="hljs-comment"># Write TensorBoard logs to './logs' directory</span>
  tf.keras.callbacks.TensorBoard(log_dir=<span class="hljs-string">'./logs'</span>)
]
<span class="hljs-comment"># train</span>
model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'categorical_crossentropy'</span>, optimizer=OPTIM,
    metrics=[<span class="hljs-string">'</span><span class="hljs-string">accuracy'</span>])
 
model.fit(X_train, y_train, batch_size=BATCH_SIZE,
    epochs=EPOCHS, validation_split=VALIDATION_SPLIT, 
    verbose=VERBOSE, callbacks=callbacks) 
score = model.evaluate(X_test, y_test,
                     batch_size=BATCH_SIZE, verbose=VERBOSE)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nTest score:"</span>, score[<span class="hljs-number">0</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Test accuracy:'</span>, score[<span class="hljs-number">1</span>])
</code></pre>
<p class="normal">Let’s run the code. Our network reaches a test accuracy of 66.8% with 20 iterations. We also print the accuracy and loss plot and dump the network with <code class="inlineCode">model.summary()</code>:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 17/20
40000/40000 [==============================] - 112s 3ms/sample - loss: 0.6282 - accuracy: 0.7841 - val_loss: 1.0296 - val_accuracy: 0.6734
Epoch 18/20
40000/40000 [==============================] - 76s 2ms/sample - loss: 0.6140 - accuracy: 0.7879 - val_loss: 1.0789 - val_accuracy: 0.6489
Epoch 19/20
40000/40000 [==============================] - 74s 2ms/sample - loss: 0.5931 - accuracy: 0.7958 - val_loss: 1.0461 - val_accuracy: 0.6811
Epoch 20/20
40000/40000 [==============================] - 71s 2ms/sample - loss: 0.5724 - accuracy: 0.8042 - val_loss: 0.1.0527 - val_accuracy: 0.6773
10000/10000 [==============================] - 5s 472us/sample - loss: 1.0423 - accuracy: 0.6686
Test score: 1.0423416819572449
Test accuracy: 0.6686
</code></pre>
<p class="normal"><em class="italic">Figure 3.10</em> shows the accuracy and loss plot:</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="304" src="../Images/B18331_03_10.png" width="878"/></figure>
<p class="packt_figref">Figure 3.10: Accuracy and loss for the defined network</p>
<p class="normal">We have seen how<a id="_idIndexMarker287"/> to improve accuracy and how the loss<a id="_idIndexMarker288"/> changes for CIFAR-10 datasets. The next section is about improving the current results.</p>
<h2 class="heading-2" id="_idParaDest-80">Improving the CIFAR-10 performance with a deeper network</h2>
<p class="normal">One way to improve<a id="_idIndexMarker289"/> the performance is to define a deeper network with multiple convolutional operations. In the following example, we have a sequence of modules:</p>
<p class="center">1st module: (CONV+CONV+MaxPool+DropOut)</p>
<p class="center">2nd module: (CONV+CONV+MaxPool+DropOut)</p>
<p class="center">3rd module: (CONV+CONV+MaxPool+DropOut)</p>
<p class="normal">These are followed by a standard dense output layer. All the activation functions used are ReLU functions. There is a <a id="_idIndexMarker290"/>new layer that we also discussed in <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, <code class="inlineCode">BatchNormalization()</code>, used to introduce a form of regularization between modules:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>(): 
    model = models.Sequential()
    
    <span class="hljs-comment">#1st block</span>
    model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>, 
        input_shape=x_train.shape[<span class="hljs-number">1</span>:], activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))
    model.add(layers.Dropout(<span class="hljs-number">0.2</span>))
    <span class="hljs-comment">#2nd block</span>
    model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))
    model.add(layers.Dropout(<span class="hljs-number">0.3</span>))
    <span class="hljs-comment">#3d block </span>
    model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))
    model.add(layers.Dropout(<span class="hljs-number">0.4</span>))
    <span class="hljs-comment">#dense  </span>
    model.add(layers.Flatten())
    model.add(layers.Dense(NUM_CLASSES, activation=<span class="hljs-string">'softmax'</span>))
    <span class="hljs-keyword">return</span> model
    model.summary()
</code></pre>
<p class="normal">Congratulations! You have defined a deeper network. Let us run the code for 40 iterations reaching an <a id="_idIndexMarker291"/>accuracy of 82%! Let’s add the remaining part of the code for the sake of completeness. The first part is to load and normalize the data:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models, regularizers, optimizers
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
 
EPOCHS=<span class="hljs-number">50</span>
NUM_CLASSES = <span class="hljs-number">10</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span>():
    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()
    x_train = x_train.astype(<span class="hljs-string">'float32'</span>)
    x_test = x_test.astype(<span class="hljs-string">'float32'</span>)
 
    <span class="hljs-comment">#normalize </span>
    mean = np.mean(x_train,axis=(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))
    std = np.std(x_train,axis=(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))
    x_train = (x_train-mean)/(std+<span class="hljs-number">1e-7</span>)
    x_test = (x_test-mean)/(std+<span class="hljs-number">1e-7</span>)
 
    y_train =  tf.keras.utils.to_categorical(y_train,NUM_CLASSES)
    y_test =  tf.keras.utils.to_categorical(y_test,NUM_CLASSES)
    <span class="hljs-keyword">return</span> x_train, y_train, x_test, y_test
</code></pre>
<p class="normal">Then we need to have a part to train the network:</p>
<pre class="programlisting code"><code class="hljs-code">(x_train, y_train, x_test, y_test) = load_data()
model = build_model()
model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'categorical_crossentropy'</span>, 
            optimizer=<span class="hljs-string">'RMSprop'</span>, 
            metrics=[<span class="hljs-string">'accuracy'</span>])
<span class="hljs-comment">#train</span>
batch_size = <span class="hljs-number">64</span>
model.fit(x_train, y_train, batch_size=batch_size,
    epochs=EPOCHS, validation_data=(x_test,y_test)) 
score = model.evaluate(x_test, y_test,
                     batch_size=batch_size)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"</span><span class="hljs-string">\nTest score:"</span>, score[<span class="hljs-number">0</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Test accuracy:'</span>, score[<span class="hljs-number">1</span>])
</code></pre>
<p class="normal">So, we have an <a id="_idIndexMarker292"/>improvement of 15.14% with respect to the previous simpler deeper network.</p>
<h2 class="heading-2" id="_idParaDest-81">Improving the CIFAR-10 performance with data augmentation</h2>
<p class="normal">Another way to improve the <a id="_idIndexMarker293"/>performance is to generate more images for our training. The idea here is that we can take the standard CIFAR training set and augment this set with multiple types of transformation, including rotation, rescaling, horizontal or vertical flip, zooming, channel shift, and many more. Let’s see the code applied on the same network defined in the previous section:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.image <span class="hljs-keyword">import</span> ImageDataGenerator
<span class="hljs-comment">#image augmentation</span>
datagen = ImageDataGenerator(
    rotation_range=<span class="hljs-number">30</span>,
    width_shift_range=<span class="hljs-number">0.2</span>,
    height_shift_range=<span class="hljs-number">0.2</span>,
    horizontal_flip=<span class="hljs-literal">True</span>,
    )
datagen.fit(x_train)
</code></pre>
<p class="normal"><code class="inlineCode">rotation_range</code> is a value in degrees (0-180) for randomly rotating pictures; <code class="inlineCode">width_shift</code> and <code class="inlineCode">height_shift</code> are ranges for randomly translating pictures vertically or horizontally; <code class="inlineCode">zoom_range</code> is for randomly zooming pictures; <code class="inlineCode">horizontal_flip</code> is for randomly flipping half of the images horizontally; <code class="inlineCode">fill_mode</code> is the strategy used for filling in new pixels that can appear after a rotation or a shift.</p>
<p class="normal">After augmentation we have generated many more training images starting from the standard CIFAR-10 set, as shown in <em class="italic">Figure 3.11</em>:</p>
<figure class="mediaobject"><img alt="A screenshot of a video game  Description automatically generated with medium confidence" height="476" src="../Images/B18331_03_11.png" width="877"/></figure>
<p class="packt_figref">Figure.3.11: An example of image augmentation</p>
<p class="normal">Now we can apply this intuition directly for training. Using the same ConvNet defined before, we simply <a id="_idIndexMarker294"/>generate more augmented images, and then we train. For efficiency, the generator runs in parallel to the model. This allows an image augmentation on a CPU while training in parallel on a GPU. Here is the code:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#train</span>
batch_size = <span class="hljs-number">64</span>
model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                    epochs=EPOCHS,
                    verbose=<span class="hljs-number">1</span>,validation_data=(x_test,y_test))
<span class="hljs-comment">#save to disk</span>
model_json = model.to_json()
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'model.json'</span>, <span class="hljs-string">'</span><span class="hljs-string">w'</span>) <span class="hljs-keyword">as</span> json_file:
    json_file.write(model_json)
model.save_weights(<span class="hljs-string">'model.h5'</span>) 
<span class="hljs-comment">#test</span>
scores = model.evaluate(x_test, y_test, batch_size=<span class="hljs-number">128</span>, verbose=<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'\nTest result: %.3f loss: %.3f'</span> % (scores[<span class="hljs-number">1</span>]*<span class="hljs-number">100</span>,scores[<span class="hljs-number">0</span>])) 
</code></pre>
<p class="normal">Each iteration is now <a id="_idIndexMarker295"/>more expensive because we have more training data. Therefore, let’s run for 50 iterations only. We see that by doing this we reach an accuracy of 85.91%:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 46/50
50000/50000 [==============================] - 36s 722us/sample - loss: 0.2440 - accuracy: 0.9183 - val_loss: 0.4918 - val_accuracy: 0.8546
Epoch 47/50
50000/50000 [==============================] - 34s 685us/sample - loss: 0.2338 - accuracy: 0.9208 - val_loss: 0.4884 - val_accuracy: 0.8574
Epoch 48/50
50000/50000 [==============================] - 32s 643us/sample - loss: 0.2383 - accuracy: 0.9189 - val_loss: 0.5106 - val_accuracy: 0.8556
Epoch 49/50
50000/50000 [==============================] - 37s 734us/sample - loss: 0.2285 - accuracy: 0.9212 - val_loss: 0.5017 - val_accuracy: 0.8581
Epoch 49/50
50000/50000 [==============================] - 36s 712us/sample - loss: 0.2263 - accuracy: 0.9228 - val_loss: 0.4911 - val_accuracy: 0.8591
10000/10000 [==============================] - 2s 160us/sample - loss: 0.4911 - accuracy: 0.8591
Test score: 0.4911323667049408
Test accuracy: 0.8591
</code></pre>
<p class="normal">The results obtained during our experiments are summarized in the following figure:</p>
<figure class="mediaobject"><img alt="Chart" height="442" src="../Images/B18331_03_12.png" width="716"/></figure>
<p class="packt_figref">Figure 3.12: Accuracy on CIFAR-10 with different networks. On the x-axis, we have the increasing number of iterations</p>
<p class="normal">A list of state-of-the-art<a id="_idIndexMarker296"/> results for CIFAR-10 is available online (see <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml"><span class="url">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.xhtml</span></a>). As of April 2019, the best result has an accuracy of 96.53% [3].</p>
<h2 class="heading-2" id="_idParaDest-82">Predicting with CIFAR-10</h2>
<p class="normal">Let’s suppose that we want to <a id="_idIndexMarker297"/>use the deep learning model we just trained for CIFAR-10 for a bulk evaluation of images. Since we saved the model and the weights, we do not need to train each time:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> scipy.misc
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> model_from_json
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> SGD
<span class="hljs-comment">#load model</span>
model_architecture = <span class="hljs-string">'cifar10_architecture.json'</span>
model_weights = <span class="hljs-string">'cifar10_weights.h5'</span>
model = model_from_json(<span class="hljs-built_in">open</span>(model_architecture).read())
model.load_weights(model_weights)
<span class="hljs-comment">#load images</span>
img_names = [<span class="hljs-string">'cat-standing.jpg'</span>, <span class="hljs-string">'</span><span class="hljs-string">dog.jpg'</span>]
imgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), (<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),
                     (<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)).astype(<span class="hljs-string">'float32'</span>)
           <span class="hljs-keyword">for</span> img_name <span class="hljs-keyword">in</span> img_names]
imgs = np.array(imgs) / <span class="hljs-number">255</span>
<span class="hljs-comment"># train</span>
optim = SGD()
model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'categorical_crossentropy'</span>, optimizer=optim,
    metrics=[<span class="hljs-string">'accuracy'</span>])
<span class="hljs-comment"># predict </span>
predictions = model.predict_classes(imgs)
<span class="hljs-built_in">print</span>(predictions)
</code></pre>
<p class="normal">Note that we use SciPy’s <code class="inlineCode">imread</code> to load the images and then resize them to 32 × 32 pixels. The resulting<a id="_idIndexMarker298"/> image tensor has dimensions of (32, 32, 3). However, we want the color dimension to be first instead of last, so we take the transpose. After that, the list of image tensors is combined into a single tensor and normalized to be between 0 and 1.0.</p>
<p class="normal">Now let us get the prediction for a <img alt="cat-standing.jpg" height="44" src="../Images/B18331_03_Cat.png" width="53"/>and for a <img alt="dog.jpg" height="44" src="../Images/B18331_03_Dog.png" width="59"/>. We get categories 3 (cat) and 5 (dog) as output as expected. We successfully created a ConvNet to classify CIFAR-10 images. Next, we will look at VGG16: a breakthrough in deep learning.</p>
<h1 class="heading-1" id="_idParaDest-83">Very deep convolutional networks for large-scale image recognition</h1>
<p class="normal">In 2014, an interesting contribution to<a id="_idIndexMarker299"/> image recognition was presented in the paper <em class="italic">Very Deep Convolutional Networks for Large-Scale Image Recognition</em>, K. Simonyan and A. Zisserman [4]. The paper showed that a <em class="italic">significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers</em>. One model in the paper denoted as D or VGG16 had 16 deep layers. An implementation in Java Caffe (see <a href="http://caffe.berkeleyvision.org/"><span class="url">http://caffe.berkeleyvision.org/</span></a>) was <a id="_idIndexMarker300"/>used for training the model on the ImageNet ILSVRC-2012 (see <a href="http://image-net.org/challenges/LSVRC/2012/"><span class="url">http://image-net.org/challenges/LSVRC/2012/</span></a>) dataset, which includes images of 1,000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images). Each image is (224 x 224) on 3 channels. The model <a id="_idIndexMarker301"/>achieves 7.5% top-5 error (the error of the top 5 results) on ILSVRC-2012-val and 7.4% top-5 error on ILSVRC-2012-test.</p>
<p class="normal">According to the ImageNet site:</p>
<blockquote class="packt_quote">
<p class="quote">The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training. Test images will be presented with no initial annotation -- no segmentation or labels -- and algorithms will have to produce labelings specifying what objects are present in the images.</p>
</blockquote>
<p class="normal">The weights learned by the model implemented in Caffe have been directly converted (<a href="https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3"><span class="url">https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3</span></a>) in <code class="inlineCode">tf.Keras</code> and can be used by preloading them into the <code class="inlineCode">tf.Keras</code> model, which is implemented below, as described in the paper:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers, models
<span class="hljs-comment"># define a VGG16 network</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">VGG_16</span>(<span class="hljs-params">weights_path=</span><span class="hljs-literal">None</span>):
    model = models.Sequential()
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),input_shape=(<span class="hljs-number">224</span>,<span class="hljs-number">224</span>, <span class="hljs-number">3</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.ZeroPadding2D((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)))
    model.add(layers.Convolution2D(<span class="hljs-number">512</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.MaxPooling2D((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), strides=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)))
    model.add(layers.Flatten())
    <span class="hljs-comment">#top layer of the VGG net</span>
    model.add(layers.Dense(<span class="hljs-number">4096</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.Dropout(<span class="hljs-number">0.5</span>))
    model.add(layers.Dense(<span class="hljs-number">4096</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.Dropout(<span class="hljs-number">0.5</span>))
    model.add(layers.Dense(<span class="hljs-number">1000</span>, activation=<span class="hljs-string">'softmax'</span>))
    <span class="hljs-keyword">if</span> weights_path:
        model.load_weights(weights_path)
    <span class="hljs-keyword">return</span> model
</code></pre>
<p class="normal">We have implemented a VGG16 network. Note that we could also have used <code class="inlineCode">tf.keras.applications.vgg16</code>. to get the model and its weights directly. Here, I wanted to show how VGG16 works<a id="_idIndexMarker302"/> internally. Next, we are going to utilize it.</p>
<h2 class="heading-2" id="_idParaDest-84">Recognizing cats with a VGG16 network</h2>
<p class="normal">Now let us<a id="_idIndexMarker303"/> test the image of a <img alt="cat.jpg" height="32" src="../Images/B18331_03_Cat_2.png" width="44"/>.</p>
<p class="normal">Note that we are going to use predefined weights:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> cv2
im = cv2.resize(cv2.imread(<span class="hljs-string">'cat.jpg'</span>), (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)).astype(np.float32)
<span class="hljs-comment">#im = im.transpose((2,0,1))</span>
im = np.expand_dims(im, axis=<span class="hljs-number">0</span>)
<span class="hljs-comment"># Test pretrained model</span>
model = VGG_16(<span class="hljs-string">'/Users/antonio/.keras/models/vgg16_weights_tf_dim_ordering_tf_kernels.h5'</span>)
model.summary()
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'sgd'</span>, loss=<span class="hljs-string">'categorical_crossentropy'</span>)
out = model.predict(im)
<span class="hljs-built_in">print</span>(np.argmax(out))
</code></pre>
<p class="normal">When the code is executed, the class <code class="inlineCode">285</code> is returned, which corresponds (see <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"><span class="url">https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a</span></a>) to “Egyptian cat”:</p>
<pre class="programlisting con"><code class="hljs-con">Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
---------------------------------------------------------------
285
</code></pre>
<p class="normal">Impressive, isn’t it? Our VGG16 network can successfully recognize images of cats! An important step for deep learning. It is only seven years since the paper in [4], but that was a game-changing moment.</p>
<h2 class="heading-2" id="_idParaDest-85">Utilizing the tf.Keras built-in VGG16 net module</h2>
<p class="normal"><code class="inlineCode">tf.Keras</code> applications are <a id="_idIndexMarker304"/>pre-built and pretrained deep learning models. The weights are downloaded automatically when instantiating a model and stored at <code class="inlineCode">~/.keras/models/</code>. Using built-in code is very easy:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras.applications.vgg16 <span class="hljs-keyword">import</span> VGG16
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> cv2
<span class="hljs-comment"># pre built model with pre-trained weights on imagenet</span>
model = VGG16(weights=<span class="hljs-string">'imagenet'</span>, include_top=<span class="hljs-literal">True</span>)
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'sgd'</span>, loss=<span class="hljs-string">'categorical_crossentropy'</span>)
<span class="hljs-comment"># resize into VGG16 trained images' format</span>
im = cv2.resize(cv2.imread(<span class="hljs-string">'steam-locomotive.jpg'</span>), (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
im = np.expand_dims(im, axis=<span class="hljs-number">0</span>)
<span class="hljs-comment"># predict</span>
out = model.predict(im)
index = np.argmax(out)
<span class="hljs-built_in">print</span>(index)
plt.plot(out.ravel())
plt.show()
<span class="hljs-comment">#this should print 820 for steaming train</span>
</code></pre>
<p class="normal">Now, let us consider a train, <img alt="steam-locomotive.jpg" height="41" src="../Images/B18331_03_Train.png" width="39"/>. If we run the code, we get <code class="inlineCode">820</code> as a result, which is the ImageNet code for “steam locomotive.” Equally important, all the other classes have very weak support, as shown in <em class="italic">Figure 3.13</em>:</p>
<figure class="mediaobject"><img alt="Chart, histogram  Description automatically generated" height="292" src="../Images/B18331_03_13.png" width="422"/></figure>
<p class="packt_figref">Figure 3.13: A steam train is the most likely outcome</p>
<p class="normal">To conclude this section, note <a id="_idIndexMarker305"/>that VGG16 is only one of the modules that are pre-built in <code class="inlineCode">tf.Keras</code>. A full list of pretrained models is available online (see <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/applications</span></a>).</p>
<h2 class="heading-2" id="_idParaDest-86">Recycling pre-built deep learning models for extracting features</h2>
<p class="normal">One very simple idea is to <a id="_idIndexMarker306"/>use VGG16, and more generally DCNN, for feature extraction. This code implements the idea by extracting features from a specific layer. </p>
<p class="normal">Note that we need to switch to the functional API since the sequential model only accepts layers:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras.applications.vgg16 <span class="hljs-keyword">import</span> VGG16 
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> models
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing <span class="hljs-keyword">import</span> image
<span class="hljs-keyword">from</span> tensorflow.keras.applications.vgg16 <span class="hljs-keyword">import</span> preprocess_input
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> cv2
<span class="hljs-comment"># prebuild model with pre-trained weights on imagenet</span>
base_model = VGG16(weights=<span class="hljs-string">'imagenet'</span>, include_top=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span> (base_model)
<span class="hljs-keyword">for</span> i, layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(base_model.layers):
    <span class="hljs-built_in">print</span> (i, layer.name, layer.output_shape)
<span class="hljs-comment"># extract features from block4_pool block</span>
model = models.Model(inputs=base_model.<span class="hljs-built_in">input</span>, 
    outputs=base_model.get_layer(<span class="hljs-string">'block4_pool'</span>).output)
img_path = <span class="hljs-string">'cat.jpg'</span>
img = image.load_img(img_path, target_size=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=<span class="hljs-number">0</span>)
x = preprocess_input(x)
<span class="hljs-comment"># get the features from this block</span>
features = model.predict(x)
<span class="hljs-built_in">print</span>(features)
</code></pre>
<p class="normal">You might wonder why we want to extract the features from an intermediate layer in a DCNN. The reasoning is that as the network learns to classify images into categories, each layer learns to<a id="_idIndexMarker307"/> identify the features that are necessary to perform the final classification. Lower layers identify lower-order features such as color and edges, and higher layers compose these lower-order features into higher-order features such as shapes or objects. Hence, the intermediate layer has the capability to extract important features from an image, and these features are more likely to help in different kinds of classification.</p>
<p class="normal">This has multiple advantages. First, we can rely on publicly available large-scale training and transfer this learning to novel domains. Second, we can save time on expensive training. Third, we can provide reasonable solutions even when we don’t have a large number of training examples for our domain. We also get a good starting network shape for the task at hand, instead of guessing it.</p>
<p class="normal">With this, we will conclude the overview of VGG16 CNNs, the last deep learning model defined in this chapter.</p>
<h1 class="heading-1" id="_idParaDest-87">Deep Inception V3 for transfer learning</h1>
<p class="normal">Transfer learning is <a id="_idIndexMarker308"/>a very powerful deep learning technique that has applications in a number of different domains. The idea behind transfer learning is very simple and <a id="_idIndexMarker309"/>can be explained with an analogy. Suppose you want to learn a new language, say Spanish. Then it could be useful to start from what you already know in a different language, say English.</p>
<p class="normal">Following this line of thinking, computer vision researchers now commonly use pretrained CNNs to generate representations for novel tasks [1], where the dataset may not be large enough to train an entire CNN from scratch. Another common tactic is to take the pretrained ImageNet network and then fine-tune the entire network to the novel task. For instance, we can take a network trained to recognize 10 categories of music and fine-tune it to recognize 20 categories of movies.</p>
<p class="normal">Inception V3 is a <a id="_idIndexMarker310"/>very deep ConvNet developed by Google [2]. <code class="inlineCode">tf.Keras</code> implements<a id="_idIndexMarker311"/> the full network, as described in <em class="italic">Figure 3.14</em>, and it comes pretrained on ImageNet. The default input size for this model is 299x299 on three channels:</p>
<figure class="mediaobject"><img alt="" height="273" src="../Images/B18331_03_14.png" width="879"/></figure>
<p class="packt_figref">Figure 3.14: The Inception V3 deep learning model</p>
<p class="normal">This skeleton example is inspired by a scheme available online (see <a href="https://keras.io/applications/"><span class="url">https://keras.io/applications/</span></a>). Let’s suppose we have a training dataset <em class="italic">D</em> in a different domain from ImageNet. <em class="italic">D</em> has 1,024 features in input and 200 categories in output. Let’s look at a code fragment:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras.applications.inception_v3 <span class="hljs-keyword">import</span> InceptionV3
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing <span class="hljs-keyword">import</span> image
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers, models
<span class="hljs-comment"># create the base pre-trained model</span>
base_model = InceptionV3(weights=<span class="hljs-string">'imagenet'</span>, include_top=<span class="hljs-literal">False</span>)
</code></pre>
<p class="normal">We use a trained Inception V3 model: we do not include the fully connected layer – the dense layer with 1,024 inputs – because we want to fine-tune on <em class="italic">D</em>. The preceding code fragment will download the pretrained weights on our behalf: </p>
<pre class="programlisting con"><code class="hljs-con">Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5
87916544/87910968 [===========================] – 26s 0us/step
</code></pre>
<p class="normal">So, if you look at the last four layers (where <code class="inlineCode">include_top=True</code>), you see these shapes:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># layer.name, layer.input_shape, layer.output_shape</span>
(<span class="hljs-string">'mixed10'</span>, [(<span class="hljs-literal">None</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">320</span>), (<span class="hljs-literal">None</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">768</span>), (<span class="hljs-literal">None</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">768</span>), (<span class="hljs-literal">None</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">192</span>)], (<span class="hljs-literal">None</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2048</span>))
(<span class="hljs-string">'avg_pool'</span>, (<span class="hljs-literal">None</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2048</span>), (<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2048</span>))
(<span class="hljs-string">'flatten'</span>, (<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2048</span>), (<span class="hljs-literal">None</span>, <span class="hljs-number">2048</span>))
(<span class="hljs-string">'predictions'</span>, (<span class="hljs-literal">None</span>, <span class="hljs-number">2048</span>), (<span class="hljs-literal">None</span>, <span class="hljs-number">1000</span>))
</code></pre>
<p class="normal">When <code class="inlineCode">include_top=False</code>, you are removing the last three layers and exposing the <code class="inlineCode">mixed_10</code> layer. The <code class="inlineCode">GlobalAveragePooling2D</code> layer converts <code class="inlineCode">(None, 8, 8, 2048)</code> to <code class="inlineCode">(None, 2048)</code>, where <a id="_idIndexMarker312"/>each element in the <code class="inlineCode">(None, 2048)</code> tensor is the average<a id="_idIndexMarker313"/> value for each corresponding <code class="inlineCode">(8,8)</code> subtensor in the <code class="inlineCode">(None, 8, 8, 2048)</code> tensor. <code class="inlineCode">None</code> means an unspecified dimension, which is useful if you define a placeholder:</p>
<pre class="programlisting code"><code class="hljs-code">x = base_model.output
<span class="hljs-comment"># let's add a fully-connected layer as first layer</span>
x = layers.Dense(<span class="hljs-number">1024</span>, activation=<span class="hljs-string">'relu'</span>)(x)
<span class="hljs-comment"># and a logistic layer with 200 classes as last layer</span>
predictions = layers.Dense(<span class="hljs-number">200</span>, activation=<span class="hljs-string">'softmax'</span>)(x)
<span class="hljs-comment"># model to train</span>
model = models.Model(inputs=base_model.<span class="hljs-built_in">input</span>, outputs=predictions)
</code></pre>
<p class="normal">All the convolutional levels are pretrained, so we freeze them during the training of the full model:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># i.e. freeze all convolutional InceptionV3 layers</span>
<span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> base_model.layers:
    layer.trainable = <span class="hljs-literal">False</span>
</code></pre>
<p class="normal">The model is then compiled and trained for a few epochs so that the top layers are trained. For the sake of simplicity, here we are omitting the training code itself: </p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># compile the model (should be done *after* setting layers to non-trainable)</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'rmsprop'</span>, loss=<span class="hljs-string">'categorical_crossentropy'</span>)
<span class="hljs-comment"># train the model on the new data for a few epochs</span>
model.fit_generator(...)
</code></pre>
<p class="normal">Then, we freeze the top inception layers and fine-tune the other inception layers. In this example, we decide to freeze the first 172 layers (this is a tunable hyperparameter):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># we chose to train the top 2 inception blocks, i.e. we will freeze</span>
<span class="hljs-comment"># the first 172 layers and unfreeze the rest:</span>
<span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.layers[:<span class="hljs-number">172</span>]:
   layer.trainable = <span class="hljs-literal">False</span>
<span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.layers[<span class="hljs-number">172</span>:]:
   layer.trainable = <span class="hljs-literal">True</span>
</code></pre>
<p class="normal">The model is then<a id="_idIndexMarker314"/> recompiled for fine-tuning optimization:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># we need to recompile the model for these modifications to take effect</span>
<span class="hljs-comment"># we use SGD with a low learning rate</span>
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> SGD
model.<span class="hljs-built_in">compile</span>(optimizer=SGD(lr=<span class="hljs-number">0.0001</span>, momentum=<span class="hljs-number">0.9</span>), loss=<span class="hljs-string">'categorical_crossentropy'</span>)
<span class="hljs-comment"># we train our model again (this time fine-tuning the top 2 inception blocks</span>
<span class="hljs-comment"># alongside the top Dense layers</span>
model.fit_generator(...)
</code></pre>
<p class="normal">Now we have a new deep network that reuses a standard Inception V3 network, but it is trained on a new <a id="_idIndexMarker315"/>domain <em class="italic">D</em> via transfer learning. Of course, there are many fine-tuning parameters for achieving good accuracy. However, we are now re-using a very large pretrained network as a starting point via transfer learning. In doing so, we can save the need for training on our machines by reusing what is already available in <code class="inlineCode">tf.Keras</code>.</p>
<h1 class="heading-1" id="_idParaDest-88">Other CNN architectures</h1>
<p class="normal">In this section, we will discuss many other<a id="_idIndexMarker316"/> different CNN architectures, including AlexNet, residual networks, highwayNets, DenseNets, and Xception.</p>
<h2 class="heading-2" id="_idParaDest-89">AlexNet</h2>
<p class="normal">One of the first convolutional <a id="_idIndexMarker317"/>networks was AlexNet [4], which consisted of<a id="_idIndexMarker318"/> only eight layers; the first five were convolutional ones with max-pooling layers, and the last three were fully connected. AlexNet [4] is an article cited more than 35,000 times, which started the deep learning revolution (for computer vision). Then, networks<a id="_idIndexMarker319"/> started to become deeper and deeper. Recently, a <a id="_idIndexMarker320"/>new idea has been proposed.</p>
<h2 class="heading-2" id="_idParaDest-90">Residual networks</h2>
<p class="normal">Residual networks are based <a id="_idIndexMarker321"/>on the interesting idea of allowing earlier layers to be fed directly into deeper layers. These are the so-called skip <a id="_idIndexMarker322"/>connections (or fast-forward connections). The key idea is to minimize the risk of vanishing or exploding gradients for deep networks (see <em class="chapterRef">Chapter 8</em>, <em class="italic">Autoencoders</em>). </p>
<p class="normal">The building block of a ResNet is called a “residual <a id="_idIndexMarker323"/>block” or “identity block,” which <a id="_idIndexMarker324"/>includes both forward and fast-forward connections. In this example (<em class="italic">Figure 3.15</em>), the output of an earlier layer is added to the output of a later layer before being sent into a ReLU activation function:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="274" src="../Images/B18331_03_15.png" width="306"/></figure>
<p class="packt_figref">Figure 3.15: An example of image segmentation</p>
<h2 class="heading-2" id="_idParaDest-91">HighwayNets and DenseNets</h2>
<p class="normal">An additional weight matrix may be <a id="_idIndexMarker325"/>used to learn the skip weights<a id="_idIndexMarker326"/> and these models are frequently denoted as HighwayNets. Instead, models with several parallel skips are known as DenseNets [5]. It has been noticed that the human brain might have similar patterns to residual networks since the cortical layer VI <a id="_idIndexMarker327"/>neurons get input from layer I, skipping intermediary layers. In addition, residual networks can be faster to train than traditional CNNs since there are fewer layers to propagate through during each<a id="_idIndexMarker328"/> iteration (deeper layers get input sooner due to the skip connection). <em class="italic">Figure 3.16</em> shows an example of a DenseNet (based on <a href="http://arxiv.org/abs/1608.06993"><span class="url">http://arxiv.org/abs/1608.06993</span></a>):</p>
<figure class="mediaobject"><img alt="" height="463" src="../Images/B18331_03_16.png" width="883"/></figure>
<p class="packt_figref">Figure 3.16: An example of a DenseNet</p>
<h2 class="heading-2" id="_idParaDest-92">Xception</h2>
<p class="normal">Xception networks use two basic blocks: a depthwise convolution and a pointwise convolution. A depthwise <a id="_idIndexMarker329"/>convolution is the channel-wise n x n spatial <a id="_idIndexMarker330"/>convolution. Suppose an image has three channels, then we have three convolutions of n x n. A pointwise convolution is a 1 x 1 convolution. In Xception, an “extreme” version of an Inception module, we first use a 1 x 1 convolution to map cross-channel correlations, and then separately map the spatial correlations of every output channel as shown in <em class="italic">Figure 3.17</em> (from <a href="https://arxiv.org/pdf/1610.02357.pdf"><span class="url">https://arxiv.org/pdf/1610.02357.pdf</span></a>):</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="398" src="../Images/B18331_03_17.png" width="679"/></figure>
<p class="packt_figref">Figure 3.17: An example of an extreme form of an Inception module</p>
<p class="normal"><strong class="keyWord">Xception</strong> (<strong class="keyWord">eXtreme Inception</strong>) is a deep convolutional neural network architecture inspired by Inception, where<a id="_idIndexMarker331"/> Inception modules have been replaced with depthwise separable convolutions. Xception <a id="_idIndexMarker332"/>uses multiple skip-connections in a similar way to ResNet. The final architecture is rather complex as illustrated in <em class="italic">Figure 3.18</em> (from <a href="https://arxiv.org/pdf/1610.02357.pdf"><span class="url">https://arxiv.org/pdf/1610.02357.pdf</span></a>). Data first goes through the entry flow, then through the middle flow, which is repeated eight times, and finally through the exit flow:</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated with medium confidence" height="586" src="../Images/B18331_03_18.png" width="877"/></figure>
<p class="packt_figref">Figure 3.18: The full Xception architecture</p>
<p class="normal">Residual networks, HyperNets, DenseNets, Inception, and Xceptions are all available as pretrained nets<a id="_idIndexMarker333"/> in both <code class="inlineCode">tf.Keras.application</code> and <code class="inlineCode">tf.Hub</code>. The Keras website has a nice summary of the <a id="_idIndexMarker334"/>performance achieved on the ImageNet <a id="_idIndexMarker335"/>dataset and the depth of each network. The summary is available at <a href="https://keras.io/applications/"><span class="url">https://keras.io/applications/</span></a>:</p>
<figure class="mediaobject"> <img alt="Table  Description automatically generated" height="1775" src="../Images/B18331_03_19.png" width="2044"/></figure>
<p class="packt_figref">Figure 3.19: Different CNNs and accuracy on top-1 and top-5 results</p>
<p class="normal">The top-1 and <a id="_idIndexMarker336"/>top-5 accuracy refers to a model’s performance on the ImageNet validation dataset.</p>
<p class="normal">In this section, we have<a id="_idIndexMarker337"/> discussed many CNN architectures. The next section is about style transfer, a deep learning technique used for training neural networks to create art.</p>
<h1 class="heading-1" id="_idParaDest-93">Style transfer</h1>
<p class="normal">Style transfer is a funny neural network application that provides many insights into the power of neural networks. So what <a id="_idIndexMarker338"/>exactly is it? Imagine that you observe a painting done by a famous artist. In principle, you are observing two elements: the painting itself (say the face of a woman, or a landscape) and something more intrinsic, the “style” of the artist. What is the style? That is more difficult to define, but humans know that Picasso had his own style, Matisse had his own style, and each artist has his/her own style. Now, imagine taking a famous painting of Matisse, giving it to a neural network, and letting the neural network repaint it in Picasso’s style. Or, imagine taking your own photo, giving it to a neural network, and having your photo painted in Matisse’s or Picasso’s style, or in the style of any other artist that you like. That’s what style transfer does.</p>
<p class="normal">For instance, go to <a href="https://deepart.io/"><span class="url">https://deepart.io/</span></a> and see a cool demo as shown in the image below, where deepart has been applied by taking the “Van Gogh” style as observed in the Sunflowers painting (this is a public domain image: “Sonnenblumen. Arles, 1888 Öl auf Leinwand, 92,5 x 73 cm Vincent van Gogh” <a href="https://commons.wikimedia.org/wiki/Vincent_van_Gogh#/media/File:Vincent_Van_Gogh_0010.jpg"><span class="url">https://commons.wikimedia.org/wiki/Vincent_van_Gogh#/media/File:Vincent_Van_Gogh_0010.jpg</span></a>) and applying it to a picture of my daughter Aurora:</p>
<figure class="mediaobject"><img alt="A person smiling next to a painting  Description automatically generated with low confidence" height="241" src="../Images/B18331_03_20.png" width="749"/></figure>
<p class="packt_figref">Figure 3.20: An example of deepart</p>
<p class="normal">Now, how can we define more formally the process of style transfer? Well, style transfer is the task of producing an artificial image <em class="italic">x</em> that shares the content of a source content image <em class="italic">p</em> and the style of a source style image <em class="italic">a</em>. So, intuitively we need two distance functions: one distance function measures how different the content of two images is, <em class="italic">L</em><sub class="italic">content</sub>, while the other distance function measures how different the style of two images is, <em class="italic">L</em><sub class="italic">style</sub>. Then, the transfer style can be seen as an optimization problem where we try to minimize these two metrics. As in <em class="italic">A Neural Algorithm of Artistic Style</em> by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge (<a href="https://arxiv.org/abs/1508.06576"><span class="url">https://arxiv.org/abs/1508.06576</span></a>), we use a pretrained network to achieve style transfer. In particular, we can feed a VGG19 (or any suitable pretrained network) to extract features that represent images in an efficient way. Now <a id="_idIndexMarker339"/>we are going to define two functions used for training the network: the content distance and the style distance.</p>
<h2 class="heading-2" id="_idParaDest-94">Content distance</h2>
<p class="normal">Given two images, <em class="italic">p</em> content image<a id="_idIndexMarker340"/> and <em class="italic">x</em> input image, we define the content distance as the distance in the feature space defined by a layer <em class="italic">l</em> for a VGG19 network receiving the two images as an input. In other words, the two images are represented by the features extracted by a pretrained VGG19. These features project the images into a feature “content” space where the “content” distance can be conveniently computed as follows:</p>
<p class="center"><img alt="" height="113" src="../Images/B18331_03_001.png" style="height: 2.83em !important; vertical-align: 0.06em !important;" width="621"/></p>
<p class="normal">To generate nice images, we need to ensure that the content of the generated image is similar to (i.e. has a small distance from) that of the input image. The distance is therefore minimized with standard backpropagation. The code is simple:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#</span>
<span class="hljs-comment">#content distance</span>
<span class="hljs-comment">#</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">get_content_loss</span>(<span class="hljs-params">base_content, target</span>):
  <span class="hljs-keyword">return</span> tf.reduce_mean(tf.square(base_content - target))
</code></pre>
<h2 class="heading-2" id="_idParaDest-95">Style distance</h2>
<p class="normal">As discussed, the features in the higher layers of VGG19 are used as content representations. You can<a id="_idIndexMarker341"/> think about these features as filter responses. To represent the style we use in a gram matrix <em class="italic">G</em> (defined as the matrix <em class="italic">v</em><sup class="italic">T</sup><em class="italic">v</em> for a vector <em class="italic">v</em>), we consider <img alt="" height="54" src="../Images/B18331_03_002.png" style="height: 1.35em !important; vertical-align: -0.34em !important;" width="54"/> as the inner matrix for map <em class="italic">i</em> and map <em class="italic">j</em> at layer <em class="italic">l</em> of the VGG19. It is possible to show that the Gram matrix represents the correlation matrix between different filter responses.</p>
<p class="normal">The contribution of each layer to the total style loss is defined as: </p>
<p class="center"><img alt="" height="121" src="../Images/B18331_03_003.png" style="height: 3.02em !important; vertical-align: 0.03em !important;" width="467"/></p>
<p class="normal">where <img alt="" height="54" src="../Images/B18331_03_002.png" style="height: 1.35em !important; vertical-align: -0.34em !important;" width="54"/> is the Gram matrix for input image <em class="italic">x</em> and <img alt="" height="54" src="../Images/B18331_03_005.png" style="height: 1.35em !important; vertical-align: -0.34em !important;" width="54"/> is the gram matrix for the style image a, and <em class="italic">N</em><sub class="italic">l</sub> is the number of feature maps, each of size <img alt="" height="54" src="../Images/B18331_03_006.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="250"/>. The Gram matrix can project the images into a space where the style is taken into account. In addition, the feature<a id="_idIndexMarker342"/> correlations from multiple VGG19 layers are used because we want to consider multi-scale information and a more robust style representation. The total style loss across levels is the weighted sum:</p>
<table class="table-container" id="table005" style="border-style:none;">
<tbody>
<tr>
<td class="table-cell" style="border-style:none;">
<p class="normal"><img alt="" height="100" src="../Images/B18331_03_007.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="354"/></p>
</td>
<td class="table-cell" style="border-style:none; padding-left: 10em;">
<p class="normal"><img alt="" height="92" src="../Images/B18331_03_008.png" style="height: 2.30em !important; vertical-align: 0.07em !important;" width="183"/></p>
</td>
</tr>
</tbody>
</table>
<p class="normal">The key idea is therefore to perform gradient descent on the content image to make its style similar to the style image. The code is simple:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#style distance</span>
<span class="hljs-comment">#</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">gram_matrix</span>(<span class="hljs-params">input_tensor</span>):
  <span class="hljs-comment"># image channels first </span>
  channels = <span class="hljs-built_in">int</span>(input_tensor.shape[-<span class="hljs-number">1</span>])
  a = tf.reshape(input_tensor, [-<span class="hljs-number">1</span>, channels])
  n = tf.shape(a)[<span class="hljs-number">0</span>]
  gram = tf.matmul(a, a, transpose_a=<span class="hljs-literal">True</span>)
  <span class="hljs-keyword">return</span> gram / tf.cast(n, tf.float32)
 
<span class="hljs-keyword">def</span> <span class="hljs-title">get_style_loss</span>(<span class="hljs-params">base_style, gram_target</span>):
  <span class="hljs-comment"># height, width, num filters of each layer</span>
  height, width, channels = base_style.get_shape().as_list()
  gram_style = gram_matrix(base_style)
  
  <span class="hljs-keyword">return</span> tf.reduce_mean(tf.square(gram_style - gram_target))
</code></pre>
<p class="normal">In short, the concepts behind style transfer are simple: first, we use VGG19 as a feature extractor and then we define two suitable function distances, one for style and the other one for contents, which are appropriately minimized. If you want to try this out for yourself, then TensorFlow tutorials are available online. A tutorial is available at <a href="https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb"><span class="url">https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb</span></a>. If you are interested in a demo of this technique, you can go to the deepart.io free site where they do style transfer.</p>
<h1 class="heading-1" id="_idParaDest-96">Summary</h1>
<p class="normal">In this chapter, we have learned how to use deep learning ConvNets to recognize MNIST handwritten characters with high accuracy. We used the CIFAR-10 dataset to build a deep learning classifier with 10 categories, and the ImageNet dataset to build an accurate classifier with 1,000 categories. In addition, we investigated how to use large deep learning networks such as VGG16 and very deep networks such as Inception V3. We concluded with a discussion on transfer learning.</p>
<p class="normal">In the next chapter, we’ll see how to work with word embeddings and why these techniques are important for deep learning.</p>
<h1 class="heading-1" id="_idParaDest-97">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">LeCun, Y. and Bengio, Y. (1995). <em class="italic">Convolutional networks for images, speech, and time series</em>. The Handbook of Brain Theory Neural Networks, vol. 3361. </li>
<li class="numberedList">Wan. L, Zeiler M., Zhang S., Cun, Y. L., and Fergus R. (2014). <em class="italic">Regularization of neural networks using dropconnect</em>. <em class="italic">Proc. 30th Int. Conf. Mach. Learn</em>., pp. 1058–1066.</li>
<li class="numberedList">Graham B. (2014). <em class="italic">Fractional Max-Pooling</em>. arXiv Prepr. arXiv: 1412.6071.</li>
<li class="numberedList">Simonyan K. and Zisserman A. (Sep. 2014). <em class="italic">Very Deep Convolutional Networks for Large-Scale Image Recognition</em>. arXiv ePrints.</li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>