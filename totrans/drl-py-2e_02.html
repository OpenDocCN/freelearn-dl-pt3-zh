<html><head></head><body>
  <div id="_idContainer181">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-56" class="chapterTitle">A Guide to the Gym Toolkit</h1>
    <p class="normal">OpenAI is an <strong class="keyword">artificial intelligence</strong> (<strong class="keyword">AI</strong>) research organization that aims to build <strong class="keyword">artificial general intelligence</strong> (<strong class="keyword">AGI</strong>). OpenAI provides a famous toolkit called Gym for training a reinforcement learning agent.</p>
    <p class="normal">Let's suppose we need to train our agent to drive a car. We need an environment to train the agent. Can we train our agent in the real-world environment to drive a car? No, because we have learned that reinforcement learning (RL) is a trial-and-error learning process, so while we train our agent, it will make a lot of mistakes during learning. For example, let's suppose our agent hits another vehicle, and it receives a negative reward. It will then learn that hitting other vehicles is not a good action and will try not to perform this action again. But we cannot train the RL agent in the real-world environment by hitting other vehicles, right? That is why we use simulators and train the RL agent in the simulated environments.</p>
    <p class="normal">There are many toolkits that provide a simulated environment for training an RL agent. One such popular toolkit is Gym. Gym provides a variety of environments for training an RL agent ranging from classic control tasks to Atari game environments. We can train our RL agent to learn in these simulated environments using various RL algorithms. In this chapter, first, we will install Gym and then we will explore various Gym environments. We will also get hands-on with the concepts we have learned in the previous chapter by experimenting with the Gym environment.</p>
    <p class="normal">Throughout the book, we will use the Gym toolkit for building and evaluating reinforcement learning algorithms, so in this chapter, we will make ourselves familiar with the Gym toolkit.</p>
    <p class="normal">In this chapter, we will learn about the following topics:</p>
    <ul>
      <li class="bullet">Setting up our machine</li>
      <li class="bullet">Installing Anaconda and Gym</li>
      <li class="bullet">Understanding the Gym environment</li>
      <li class="bullet">Generating an episode in the Gym environment</li>
      <li class="bullet">Exploring more Gym environments</li>
      <li class="bullet">Cart-Pole balancing with the random agent</li>
      <li class="bullet">An agent playing the Tennis game</li>
    </ul>
    <h1 id="_idParaDest-57" class="title">Setting up our machine</h1>
    <p class="normal">In this section, we will <a id="_idIndexMarker143"/>learn how to install several dependencies that are required for running the code used throughout the book. First, we will learn how to install Anaconda and then we will explore how to install Gym. </p>
    <h2 id="_idParaDest-58" class="title">Installing Anaconda</h2>
    <p class="normal">Anaconda is an <a id="_idIndexMarker144"/>open-source distribution of Python. It is widely used for scientific computing and processing large volumes of data. It provides an <a id="_idIndexMarker145"/>excellent package management environment, and it supports Windows, Mac, and Linux operating systems. Anaconda comes with Python installed, along with popular packages used for scientific computing such as NumPy, SciPy, and so on.</p>
    <p class="normal">To download <a id="_idIndexMarker146"/>Anaconda, visit <a href="https://www.anaconda.com/download/"><span class="url">https://www.anaconda.com/download/</span></a>, where you will see an option for downloading Anaconda for different platforms. If you are using Windows or macOS, you can directly download the graphical installer according to your machine architecture and install Anaconda using the graphical installer.</p>
    <p class="normal">If you are using Linux, follow these steps:</p>
    <ol>
      <li class="numbered">Open the Terminal and type the following command to download Anaconda:
        <pre class="programlisting con"><code class="hljs-con">wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh
</code></pre>
      </li>
      <li class="numbered">After downloading, we can install Anaconda using the following command:
        <pre class="programlisting con"><code class="hljs-con">bash Anaconda3-5.0.1-Linux-x86_64.sh
</code></pre>
      </li>
    </ol>
    <p class="normal">After the successful installation of Anaconda, we need to create a virtual environment. What is the need for a virtual environment? Say we are working on project A, which uses NumPy version 1.14, and project B, which uses NumPy version 1.13. So, to work on project B we either downgrade NumPy or reinstall NumPy. In each project, we use different libraries with different versions that are not applicable to the other projects. Instead of downgrading or upgrading versions or reinstalling libraries every time for a new project, we use a virtual environment.</p>
    <p class="normal">The virtual environment is just an isolated environment for a particular project so that each project<a id="_idIndexMarker147"/> can have its own dependencies and will not affect other projects. We will create a virtual environment using the following command and name our environment <code class="Code-In-Text--PACKT-">universe</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">conda create --name universe python=3.6 anaconda
</code></pre>
    <p class="normal">Note that <a id="_idIndexMarker148"/>we use Python version 3.6. Once the virtual environment is created, we can activate it using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">source activate universe
</code></pre>
    <p class="normal">That's it! Now that we have learned how to install Anaconda and create a virtual environment, in the next section, we will learn how to install Gym.</p>
    <h2 id="_idParaDest-59" class="title">Installing the Gym toolkit</h2>
    <p class="normal">In this section, we will <a id="_idIndexMarker149"/>learn how to install the Gym toolkit. Before <a id="_idIndexMarker150"/>going ahead, first, let's activate our virtual environment, <code class="Code-In-Text--PACKT-">universe</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">source activate universe
</code></pre>
    <p class="normal">Now, install the following dependencies:</p>
    <pre class="programlisting con"><code class="hljs-con">sudo apt-get update
sudo apt-get install golang libcupti-dev libjpeg-turbo8-dev make tmux htop chromium-browser git cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig
conda install pip six libgcc swig
conda install opencv
</code></pre>
    <p class="normal">We can install Gym directly using <code class="Code-In-Text--PACKT-">pip</code>. Note that throughout the book, we will use Gym version 0.15.4. We can install Gym using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install gym==0.15.4
</code></pre>
    <p class="normal">We can also install Gym by cloning the Gym repository as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">cd ~
git clone https://github.com/openai/gym.git
cd gym
pip install -e '.[all]'
</code></pre>
    <h3 id="_idParaDest-60" class="title">Common error fixes</h3>
    <p class="normal">Just in case, if you <a id="_idIndexMarker151"/>get any of the following errors while installing Gym, the following commands will help:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Failed building wheel for pachi-py</strong> or <strong class="keyword">failed building wheel for pachi-py atari-py</strong>:
        <pre class="programlisting con"><code class="hljs-con">sudo apt-get update
sudo apt-get install xvfb libav-tools xorg-dev libsdl2-dev swig cmake
</code></pre>
      </li>
      <li class="bullet"><strong class="keyword">Failed building wheel for mujoco-py</strong>:
        <pre class="programlisting con"><code class="hljs-con">git clone https://github.com/openai/mujoco-py.git
cd mujoco-py
sudo apt-get update
sudo apt-get install libgl1-mesa-dev libgl1-mesa-glx libosmesa6-dev python3-pip python3-numpy python3-scipy
pip3 install -r requirements.txt
sudo python3 setup.py install
</code></pre>
      </li>
      <li class="bullet"><strong class="keyword">error: command 'gcc' failed with exit status 1</strong>:
        <pre class="programlisting con"><code class="hljs-con">sudo apt-get update
sudo apt-get install python-dev 
sudo apt-get install libevent-dev
</code></pre>
      </li>
    </ul>
    <p class="normal">Now that we have successfully installed Gym, in the next section, let's kickstart our hands-on reinforcement learning journey.</p>
    <h1 id="_idParaDest-61" class="title">Creating our first Gym environment</h1>
    <p class="normal">We have <a id="_idIndexMarker152"/>learned that Gym provides a variety of environments for training a reinforcement learning agent. To clearly understand how the Gym environment is designed, we will start with the basic Gym environment. After that, we will understand other complex Gym environments. </p>
    <p class="normal">Let's introduce one of the simplest environments called the Frozen Lake environment. <em class="italic">Figure 2.1</em> shows the Frozen Lake environment. As we can observe, in the Frozen Lake environment, the goal of the agent is to start from the initial state <strong class="keyword">S</strong> and reach the goal state <strong class="keyword">G</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.1: The Frozen Lake environment</p>
    <p class="normal">In the <a id="_idIndexMarker153"/>preceding environment, the following apply:</p>
    <ul>
      <li class="bullet"><strong class="keyword">S</strong> denotes the starting state</li>
      <li class="bullet"><strong class="keyword">F</strong> denotes the frozen state</li>
      <li class="bullet"><strong class="keyword">H</strong> denotes the hole state</li>
      <li class="bullet"><strong class="keyword">G</strong> denotes the goal state</li>
    </ul>
    <p class="normal">So, the agent has to start from state <strong class="keyword">S</strong> and reach the goal state <strong class="keyword">G</strong>. But one issue is that if the agent visits state <strong class="keyword">H</strong>, which is the hole state, then the agent will fall into the hole and die as shown in <em class="italic">Figure 2.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.2: The agent falls down a hole</p>
    <p class="normal">So, we need <a id="_idIndexMarker154"/>to make sure that the agent starts from <strong class="keyword">S</strong> and reaches <strong class="keyword">G</strong> without falling into the hole state <strong class="keyword">H</strong> as shown in <em class="italic">Figure 2.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.3: The agent reaches the goal state</p>
    <p class="normal">Each grid box in the preceding environment is called a state, thus we have 16 states (<strong class="keyword">S</strong> to <strong class="keyword">G</strong>) and we have 4 possible actions, which are <em class="italic">up</em>, <em class="italic">down</em>, <em class="italic">left</em>, and <em class="italic">right</em>. We learned that our goal is to reach the state <strong class="keyword">G</strong> from <strong class="keyword">S</strong> without visiting <strong class="keyword">H</strong>. So, we assign +1 reward for the goal state <strong class="keyword">G</strong> and 0 for all other states.</p>
    <p class="normal">Thus, we have learned how the Frozen Lake environment works. Now, to train our agent in the Frozen Lake environment, first, we need to create the environment by coding it from scratch in Python. But luckily we don't have to do that! Since Gym provides various environments, we can directly import the Gym toolkit and create a Frozen Lake environment.</p>
    <p class="normal">Now, we will learn how to create our Frozen Lake environment using Gym. Before running any code, make sure that you have activated our virtual environment <code class="Code-In-Text--PACKT-">universe</code>. First, let's import the Gym library:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
</code></pre>
    <p class="normal">Next, we can <a id="_idIndexMarker155"/>create a Gym environment using the <code class="Code-In-Text--PACKT-">make</code> function. The <code class="Code-In-Text--PACKT-">make</code> function requires the environment id as a parameter. In Gym, the id of the Frozen Lake environment is <code class="Code-In-Text--PACKT-">FrozenLake-v0</code>. So, we can create our Frozen Lake environment as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make("FrozenLake-v0")
</code></pre>
    <p class="normal">After creating the environment, we can see how our environment looks like using the <code class="Code-In-Text--PACKT-">render</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">env.render()
</code></pre>
    <p class="normal">The preceding code renders the following environment:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.4: Gym's Frozen Lake environment</p>
    <p class="normal">As we can observe, the Frozen Lake environment consists of 16 states (<strong class="keyword">S</strong> to <strong class="keyword">G</strong>) as we learned. The state <strong class="keyword">S</strong> is highlighted indicating that it is our current state, that is, the agent is in the state <strong class="keyword">S</strong>. So whenever we create an environment, an agent will always begin from the initial state, which in our case is state <strong class="keyword">S</strong>.</p>
    <p class="normal">That's it! Creating <a id="_idIndexMarker156"/>the environment using Gym is that simple. In the next section, we will understand more about the Gym environment by relating all the concepts we have learned in the previous chapter.</p>
    <h2 id="_idParaDest-62" class="title">Exploring the environment</h2>
    <p class="normal">In the <a id="_idIndexMarker157"/>previous chapter, we learned that the reinforcement learning environment can be modeled as a <strong class="keyword">Markov decision process</strong> (<strong class="keyword">MDP</strong>) and <a id="_idIndexMarker158"/>an MDP consists of the following:</p>
    <ul>
      <li class="bullet"><strong class="keyword">States</strong>: A set of states present in the environment.</li>
      <li class="bullet"><strong class="keyword">Actions</strong>: A set of actions that the agent can perform in each state.</li>
      <li class="bullet"><strong class="keyword">Transition probability</strong>: The transition probability is denoted by <img src="../Images/B15558_02_001.png" alt="" style="height: 1.2em;"/>. It implies the probability of moving from a state <em class="italic">s</em> to the state <img src="../Images/B15558_02_002.png" alt="" style="height: 1.2em;"/> while performing an action <em class="italic">a</em>.</li>
      <li class="bullet"><strong class="keyword">Reward function</strong>: The reward function is denoted by <img src="../Images/B15558_02_003.png" alt="" style="height: 1.2em;"/>. It implies the reward the agent obtains moving from a state <em class="italic">s</em> to the state <img src="../Images/B15558_02_004.png" alt="" style="height: 1.2em;"/> while performing an action <em class="italic">a</em>.</li>
    </ul>
    <p class="normal">Let's now understand how to obtain all the above information from the Frozen Lake environment we just created using Gym.</p>
    <h3 id="_idParaDest-63" class="title">States</h3>
    <p class="normal">A state <a id="_idIndexMarker159"/>space consists of all of our states. We can obtain the number of states in our environment by just typing <code class="Code-In-Text--PACKT-">env.observation_space</code> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.observation_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Discrete(<span class="hljs-number">16</span>)
</code></pre>
    <p class="normal">It implies that we have 16 discrete states in our state space starting from state <strong class="keyword">S</strong> to <strong class="keyword">G</strong>. Note that, in Gym, the states will be encoded as a number, so the state <strong class="keyword">S</strong> will be encoded <a id="_idIndexMarker160"/>as 0, state <strong class="keyword">F</strong> will be encoded as 1, and so on as <em class="italic">Figure 2.5</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.5: Sixteen discrete states</p>
    <h3 id="_idParaDest-64" class="title">Actions</h3>
    <p class="normal">We learned <a id="_idIndexMarker161"/>that the action space consists of all the possible actions in the environment. We can obtain the action space by using <code class="Code-In-Text--PACKT-">env.action_space</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.action_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Discrete(<span class="hljs-number">4</span>)
</code></pre>
    <p class="normal">It shows that we have <code class="Code-In-Text--PACKT-">4</code> discrete actions in our action space, which are <em class="italic">left</em>, <em class="italic">down</em>, <em class="italic">right</em>, and <em class="italic">up</em>. Note that, similar to states, actions also will be encoded into numbers as shown in <em class="italic">Table 2.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_06.png" alt=""/></figure>
    <p class="packt_figref">Table 2.1: Four discrete actions</p>
    <h3 id="_idParaDest-65" class="title">Transition probability and reward function</h3>
    <p class="normal">Now, let's look <a id="_idIndexMarker162"/>at how to obtain the <a id="_idIndexMarker163"/>transition probability and the reward function. We learned that in the stochastic environment, we cannot say that by performing some action <em class="italic">a</em>, the agent will always reach the next state <img src="../Images/B15558_02_004.png" alt="" style="height: 1.2em;"/> exactly because there will be some randomness associated with the stochastic environment, and by performing an action <em class="italic">a</em> in the state <em class="italic">s</em>, the agent reaches the next state <img src="../Images/B15558_02_004.png" alt="" style="height: 1.2em;"/> with some probability.</p>
    <p class="normal">Let's suppose we are in state 2 (<strong class="keyword">F</strong>). Now, if we perform action 1 (<em class="italic">down</em>) in state 2, we can reach state 6 as shown in <em class="italic">Figure 2.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/Image62187.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.6: The agent performing a down action from state 2</p>
    <p class="normal">Our Frozen Lake environment is a stochastic environment. When our environment is stochastic, we <a id="_idIndexMarker164"/>won't always <a id="_idIndexMarker165"/>reach state 6 by performing action 1 (<em class="italic">down</em>) in state 2; we also reach other states with some probability. So when we perform an action 1 (<em class="italic">down</em>) in state 2, we reach state 1 with probability 0.33333, we reach state 6 with probability 0.33333, and we reach state 3 with probability 0.33333 as shown in <em class="italic">Figure 2.7</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.7: Transition probability of the agent in state 2</p>
    <p class="normal">As we can see, in a stochastic environment we reach the next states with some probability. Now, let's learn how to obtain this transition probability using the Gym environment.</p>
    <p class="normal">We can obtain the transition probability and the reward function by just typing <code class="Code-In-Text--PACKT-">env.P[state][action]</code>. So, to obtain the transition probability of moving from state <strong class="keyword">S</strong> to the other states by performing the action <em class="italic">right</em>, we can type <code class="Code-In-Text--PACKT-">env.P[S][right]</code>. But we cannot just type state <strong class="keyword">S</strong> and action <em class="italic">right</em> directly since they are encoded as numbers. We learned that state <strong class="keyword">S</strong> is encoded as 0 and the action <em class="italic">right</em> is encoded as 2, so, to obtain the transition probability of state <strong class="keyword">S</strong> by performing the action <em class="italic">right</em>, we type <code class="Code-In-Text--PACKT-">env.P[0][2]</code> as the following shows:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.P[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>])
</code></pre>
    <p class="normal">The <a id="_idIndexMarker166"/>above code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">[(<span class="hljs-number">0.33333</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0.0</span>, <span class="hljs-literal">False</span>),
 (<span class="hljs-number">0.33333</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.0</span>, <span class="hljs-literal">False</span>),
 (<span class="hljs-number">0.33333</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-literal">False</span>)]
</code></pre>
    <p class="normal">What <a id="_idIndexMarker167"/>does this imply? Our output is in the form of <code class="Code-In-Text--PACKT-">[(transition probability, next state, reward, Is terminal state?)]</code>. It implies that if we perform an action 2 (<em class="italic">right</em>) in state 0 (<strong class="keyword">S</strong>) then:</p>
    <ul>
      <li class="bullet">We reach state 4 (<strong class="keyword">F</strong>) with probability 0.33333 and receive 0 reward. </li>
      <li class="bullet">We reach state 1 (<strong class="keyword">F</strong>) with probability 0.33333 and receive 0 reward.</li>
      <li class="bullet">We reach the same state 0 (<strong class="keyword">S</strong>) with probability 0.33333 and receive 0 reward.</li>
    </ul>
    <p class="normal"><em class="italic">Figure 2.8</em> shows the transition probability:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.8: Transition probability of the agent in state 0 </p>
    <p class="normal">Thus, when <a id="_idIndexMarker168"/>we type <code class="Code-In-Text--PACKT-">env.P[state][action]</code>, we get the result in the form of <code class="Code-In-Text--PACKT-">[(transition probability, next state, reward, Is terminal state?)]</code>. The last value is Boolean and tells us whether the <a id="_idIndexMarker169"/>next state is a terminal state. Since 4, 1, and 0 are not terminal states, it is given as false.</p>
    <p class="normal">The output of <code class="Code-In-Text--PACKT-">env.P[0][2]</code> is shown in <em class="italic">Table 2.2</em> for more clarity:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_10.png" alt=""/></figure>
    <p class="packt_figref">Table 2.2: Output of env.P[0][2]</p>
    <p class="normal">Let's understand this with one more example. Let's suppose we are in state 3 (<strong class="keyword">F</strong>) as <em class="italic">Figure 2.9</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.9: The agent in state 3</p>
    <p class="normal">Say we <a id="_idIndexMarker170"/>perform action 1 (<em class="italic">down</em>) in state 3 (<strong class="keyword">F</strong>). Then the transition probability of state 3 (<strong class="keyword">F</strong>) by performing action 1 (<em class="italic">down</em>) can be <a id="_idIndexMarker171"/>obtained as the following shows:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.P[<span class="hljs-number">3</span>][<span class="hljs-number">1</span>])
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">[(<span class="hljs-number">0.33333</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.0</span>, <span class="hljs-literal">False</span>),
 (<span class="hljs-number">0.33333</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0.0</span>, <span class="hljs-literal">True</span>),
 (<span class="hljs-number">0.33333</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0.0</span>, <span class="hljs-literal">False</span>)]
</code></pre>
    <p class="normal">As we learned, our output is in the form of <code class="Code-In-Text--PACKT-">[(transition probability, next state, reward, Is terminal state?)]</code>. It implies that if we perform action 1 (<em class="italic">down</em>) in state 3 (<strong class="keyword">F</strong>) then:</p>
    <ul>
      <li class="bullet">We reach state 2 (<strong class="keyword">F</strong>) with probability 0.33333 and receive 0 reward. </li>
      <li class="bullet">We reach state 7 (<strong class="keyword">H</strong>) with probability 0.33333 and receive 0 reward.</li>
      <li class="bullet">We reach the same state 3 (<strong class="keyword">F</strong>) with probability 0.33333 and receive 0 reward.</li>
    </ul>
    <p class="normal"><em class="italic">Figure 2.10</em> shows the transition probability:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.10: Transition probabilities of the agent in state 3</p>
    <p class="normal">The output of <code class="Code-In-Text--PACKT-">env.P[3][1]</code> is shown in <em class="italic">Table 2.3</em> for more clarity:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_13.png" alt=""/></figure>
    <p class="packt_figref">Table 2.3: Output of env.P[3][1]</p>
    <p class="normal">As we can <a id="_idIndexMarker172"/>observe, in the second <a id="_idIndexMarker173"/>row of our output, we have <code class="Code-In-Text--PACKT-">(0.33333, 7, 0.0, True)</code>, and the last value here is marked as <code class="Code-In-Text--PACKT-">True</code>. It implies that state 7 is a terminal state. That is, if we perform action 1 (<em class="italic">down</em>) in state 3 (<strong class="keyword">F</strong>) then we reach state 7 (<strong class="keyword">H</strong>) with 0.33333 probability, and since 7 (<strong class="keyword">H</strong>) is a hole, the agent dies if it reaches state 7 (<strong class="keyword">H</strong>). Thus 7(<strong class="keyword">H</strong>) is a terminal state and so it is marked as <code class="Code-In-Text--PACKT-">True</code>.</p>
    <p class="normal">Thus, we have learned how to obtain the state space, action space, transition probability, and the reward function using the Gym environment. In the next section, we will learn how to generate an episode.</p>
    <h2 id="_idParaDest-66" class="title">Generating an episode in the Gym environment</h2>
    <p class="normal">We <a id="_idIndexMarker174"/>learned that the agent-environment interaction <a id="_idIndexMarker175"/>starting from an initial state until the terminal state is called an episode. In this section, we will learn how to generate an episode in the Gym environment.</p>
    <p class="normal">Before we begin, we initialize the state by resetting our environment; resetting puts our agent back to the initial state. We can reset our environment using the <code class="Code-In-Text--PACKT-">reset()</code> function as shown as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">state = env.reset()
</code></pre>
    <h3 id="_idParaDest-67" class="title">Action selection</h3>
    <p class="normal">In order for <a id="_idIndexMarker176"/>the agent to interact with the environment, it has to perform some action in the environment. So, first, let's learn how to perform an action in the Gym environment. Let's suppose we are in state 3 (<strong class="keyword">F</strong>) as <em class="italic">Figure 2.11 </em>shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.11: The agent is in state 3 in the Frozen Lake environment</p>
    <p class="normal">Say we need to perform action 1 (<em class="italic">down</em>) and move to the new state 7 (<strong class="keyword">H</strong>). How can we do that? We can perform an action using the <code class="Code-In-Text--PACKT-">step</code> function. We just need to input our action as a parameter to the <code class="Code-In-Text--PACKT-">step</code> function. So, we can perform action 1 (<em class="italic">down</em>) in state 3 (<strong class="keyword">F</strong>) using the <code class="Code-In-Text--PACKT-">step</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">env.step(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Now, let's render our environment using the <code class="Code-In-Text--PACKT-">render</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">env.render()
</code></pre>
    <p class="normal">As shown in <em class="italic">Figure 2.12</em>, the agent performs action 1 (<em class="italic">down</em>) in state 3 (<strong class="keyword">F</strong>) and reaches the next state 7 (<strong class="keyword">H</strong>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.12: The agent in state 7 in the Frozen Lake environment</p>
    <p class="normal">Note that <a id="_idIndexMarker177"/>whenever we make an action using <code class="Code-In-Text--PACKT-">env.step()</code>, it outputs a tuple containing 4 values. So, when we take action 1 (<em class="italic">down</em>) in state 3 (<strong class="keyword">F</strong>) using <code class="Code-In-Text--PACKT-">env.step(1)</code>, it gives the output as:</p>
    <pre class="programlisting code"><code class="hljs-code">(<span class="hljs-number">7</span>, <span class="hljs-number">0.0</span>, <span class="hljs-literal">True</span>, {<span class="hljs-string">'prob'</span>: <span class="hljs-number">0.33333</span>})
</code></pre>
    <p class="normal">As you might have guessed, it implies that when we perform action 1 (<em class="italic">down</em>) in state 3 (<strong class="keyword">F</strong>):</p>
    <ul>
      <li class="bullet">We reach the next state 7 (<strong class="keyword">H</strong>).</li>
      <li class="bullet">The agent receives the reward <code class="Code-In-Text--PACKT-">0.0.</code></li>
      <li class="bullet">Since the next state 7 (<strong class="keyword">H</strong>) is a terminal state, it is marked as <code class="Code-In-Text--PACKT-">True</code>.</li>
      <li class="bullet">We reach the next state 7 (<strong class="keyword">H</strong>) with a probability of 0.33333.</li>
    </ul>
    <p class="normal">So, we can just store this information as:</p>
    <pre class="programlisting code"><code class="hljs-code">(next_state, reward, done, info) = env.step(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Thus:</p>
    <ul>
      <li class="bullet"><code class="Code-In-Text--PACKT-">next_state</code> represents the next state.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">reward</code> represents the obtained reward.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">done</code> implies whether our episode has ended. That is, if the next state is a terminal state, then our episode will end, so <code class="Code-In-Text--PACKT-">done</code> will be marked as <code class="Code-In-Text--PACKT-">True</code> else it will be marked as <code class="Code-In-Text--PACKT-">False</code>.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">info</code>—Apart from the transition probability, in some cases, we also obtain other information saved as info, which is used for debugging purposes.</li>
    </ul>
    <p class="normal">We can also sample action from our action space and perform a random action to explore our environment. We can sample an action using the <code class="Code-In-Text--PACKT-">sample</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">random_action = env.action_space.sample()
</code></pre>
    <p class="normal">After we <a id="_idIndexMarker178"/>have sampled an action from our action space, then we perform our sampled action using our step function:</p>
    <pre class="programlisting code"><code class="hljs-code">next_state, reward, done, info = env.step(random_action)
</code></pre>
    <p class="normal">Now that we have learned how to select actions in the environment, let's see how to generate an episode.</p>
    <h3 id="_idParaDest-68" class="title">Generating an episode</h3>
    <p class="normal">Now let's learn <a id="_idIndexMarker179"/>how to generate an episode. The episode is the agent environment interaction starting from the initial state to the terminal state. The agent interacts with the environment by performing some action in each state. An episode ends if the agent reaches the terminal state. So, in the Frozen Lake environment, the episode will end if the agent reaches the terminal state, which is either the hole state (<strong class="keyword">H</strong>) or goal state (<strong class="keyword">G</strong>).</p>
    <p class="normal">Let's understand <a id="_idIndexMarker180"/>how to generate an episode with the random policy. We learned that the random policy selects a random action in each state. So, we will generate an episode by taking random actions in each state. So for each time step in the episode, we take a random action in each state and our episode will end if the agent reaches the terminal state.</p>
    <p class="normal">First, let's set the number of time steps:</p>
    <pre class="programlisting code"><code class="hljs-code">num_timesteps = <span class="hljs-number">20</span>
</code></pre>
    <p class="normal">For each time step:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
</code></pre>
    <p class="normal">Randomly select an action by sampling from the action space:</p>
    <pre class="programlisting code"><code class="hljs-code">    random_action = env.action_space.sample()
</code></pre>
    <p class="normal">Perform the selected action:</p>
    <pre class="programlisting code"><code class="hljs-code">    next_state, reward, done, info = env.step(random_action)
</code></pre>
    <p class="normal">If the next state is the terminal state, then break. This implies that our episode ends:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> done:
        <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker181"/>complete snippet is provided for clarity. The following code denotes that on every time step, we select an action by randomly sampling <a id="_idIndexMarker182"/>from the action space, and our episode will end if the agent reaches the terminal state:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
env = gym.make(<span class="hljs-string">"FrozenLake-v0"</span>)
state = env.reset()
print(<span class="hljs-string">'Time Step 0 :'</span>)
env.render()
num_timesteps = <span class="hljs-number">20</span>
<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
  random_action = env.action_space.sample()
  new_state, reward, done, info = env.step(random_action)
  <span class="hljs-keyword">print</span> (<span class="hljs-string">'Time Step {} :'</span>.format(t+<span class="hljs-number">1</span>))
  env.render()
  <span class="hljs-keyword">if</span> done:
    <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">The preceding code will print something similar to <em class="italic">Figure 2.13</em>. Note that you might get a different result each time you run the preceding code since the agent is taking a random action in each time step.</p>
    <p class="normal">As we can observe from the following output, on each time step, the agent takes a random action in each state and our episode ends once the agent reaches the terminal state. As <em class="italic">Figure 2.13</em> shows, in <a id="_idIndexMarker183"/>time step 4, the <a id="_idIndexMarker184"/>agent reaches the terminal state <strong class="keyword">H,</strong> and so the episode ends:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.13: Actions taken by the agent in each time step</p>
    <p class="normal">Instead of generating one episode, we can also generate a series of episodes by taking some random action in each state:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
env = gym.make(<span class="hljs-string">"FrozenLake-v0"</span>)
<span class="code-highlight"><strong class="hljs-slc">num_episodes = </strong><strong class="hljs-number-slc">10</strong></span>
num_timesteps = <span class="hljs-number">20</span> 
<span class="code-highlight"><strong class="hljs-keyword-slc">for</strong><strong class="hljs-slc"> i </strong><strong class="hljs-keyword-slc">in</strong><strong class="hljs-slc"> range(num_episodes):</strong></span>
    
    state = env.reset()
    print(<span class="hljs-string">'Time Step 0 :'</span>)
    env.render()
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
        random_action = env.action_space.sample()
        
        new_state, reward, done, info = env.step(random_action)
        <span class="hljs-keyword">print</span> (<span class="hljs-string">'Time Step {} :'</span>.format(t+<span class="hljs-number">1</span>))
        env.render()
        <span class="hljs-keyword">if</span> done:
            <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">Thus, we can generate an episode by selecting a random action in each state by sampling from <a id="_idIndexMarker185"/>the action space. But wait! What is <a id="_idIndexMarker186"/>the use of this? Why do we even need to generate an episode?</p>
    <p class="normal">In the previous chapter, we learned that an agent can find the optimal policy (that is, the correct action in each state) by generating several episodes. But in the preceding example, we just took random actions in each state over all the episodes. How can the agent find the optimal policy? So, in the case of the Frozen Lake environment, how can the agent find the optimal policy that tells the agent to reach state <strong class="keyword">G</strong> from state <strong class="keyword">S</strong> without visiting the hole states <strong class="keyword">H</strong>?</p>
    <p class="normal">This is where we need a reinforcement learning algorithm. Reinforcement learning is all about finding the optimal policy, that is, the policy that tells us what action to perform in each state. We will learn how to find the optimal policy by generating a series of episodes using various reinforcement learning algorithms in the upcoming chapters. In this chapter, we will focus on getting acquainted with the Gym environment and various Gym functionalities as we will be using the Gym environment throughout the course of the book. </p>
    <p class="normal">So far we <a id="_idIndexMarker187"/>have understood how the Gym environment <a id="_idIndexMarker188"/>works using the basic Frozen Lake environment, but Gym has so many other functionalities and also several interesting environments. In the next section, we will learn about the other Gym environments along with exploring the functionalities of Gym.</p>
    <h1 id="_idParaDest-69" class="title">More Gym environments</h1>
    <p class="normal">In this section, we will <a id="_idIndexMarker189"/>explore several interesting Gym environments, along with exploring different functionalities of Gym. </p>
    <h2 id="_idParaDest-70" class="title">Classic control environments</h2>
    <p class="normal">Gym provides <a id="_idIndexMarker190"/>environments for several <a id="_idIndexMarker191"/>classic control tasks such as Cart-Pole balancing, swinging up an inverted pendulum, mountain car climbing, and so on. Let's understand how to create a Gym environment for a Cart-Pole balancing task. The Cart-Pole environment is shown below:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.14: Cart-Pole example</p>
    <p class="normal">Cart-Pole balancing is one of the classical control problems. As shown in <em class="italic">Figure 2.14</em>, the pole is attached to the cart and the goal of our agent is to balance the pole on the cart, that is, the goal of our agent is to keep the pole standing straight up on the cart as shown in <em class="italic">Figure 2.15</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.15: The goal is to keep the pole straight up</p>
    <p class="normal">So the agent <a id="_idIndexMarker192"/>tries to push the cart left and right to keep the pole standing straight on the cart. Thus our agent performs two actions, which are <a id="_idIndexMarker193"/>pushing the cart to the left and pushing the cart to the right, to keep the pole standing straight on the cart. You can also check out this very interesting video, <a href="https://youtu.be/qMlcsc43-lg"><span class="url">https://youtu.be/qMlcsc43-lg</span></a>, which shows how the RL agent balances the pole on the cart by moving the cart left and right.</p>
    <p class="normal">Now, let's learn how to create the Cart-Pole environment using Gym. The environment id of the Cart-Pole environment in Gym is <code class="Code-In-Text--PACKT-">CartPole-v0</code>, so we can just use our <code class="Code-In-Text--PACKT-">make</code> function to create the Cart-Pole environment as shown below:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"CartPole-v0"</span>)
</code></pre>
    <p class="normal">After creating the environment, we can view our environment using the <code class="Code-In-Text--PACKT-">render</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">env.render()
</code></pre>
    <p class="normal">We can also close the rendered environment using the <code class="Code-In-Text--PACKT-">close</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">env.close()
</code></pre>
    <h3 id="_idParaDest-71" class="title">State space</h3>
    <p class="normal">Now, let's look <a id="_idIndexMarker194"/>at the state space of our Cart-Pole environment. Wait! What are the states here? In the Frozen Lake environment, we had 16 discrete states from <strong class="keyword">S</strong> to <strong class="keyword">G</strong>. But how can we describe the states here? Can we describe the state by cart position? Yes! Note that the cart position is a continuous value. So, in this case, our state space will be continuous values, unlike the Frozen Lake environment where our state space had discrete values (<strong class="keyword">S</strong> to <strong class="keyword">G</strong>).</p>
    <p class="normal">But with just the cart position alone we cannot describe the state of the environment completely. So we include cart velocity, pole angle, and pole velocity at the tip. So we can describe our state space by an array of values as shown as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">array([cart position, cart velocity, pole angle, pole velocity at the tip])
</code></pre>
    <p class="normal">Note that all of these values are continuous, that is:</p>
    <ol>
      <li class="numbered" value="1">The value of the cart position ranges from <code class="Code-In-Text--PACKT-">-4.8</code> to <code class="Code-In-Text--PACKT-">4.8</code>.</li>
      <li class="numbered">The value of the cart velocity ranges from <code class="Code-In-Text--PACKT-">-Inf</code> to <code class="Code-In-Text--PACKT-">Inf</code> ( <img src="../Images/B15558_02_007.png" alt="" style="height: 0.93em;"/> to <img src="../Images/B15558_02_008.png" alt="" style="height: 0.93em;"/> ).</li>
      <li class="numbered">The value of the pole angle ranges from <code class="Code-In-Text--PACKT-">-0.418</code> radians to <code class="Code-In-Text--PACKT-">0.418 </code>radians. </li>
      <li class="numbered">The value of the pole velocity at the tip ranges from <code class="Code-In-Text--PACKT-">-Inf</code> to <code class="Code-In-Text--PACKT-">Inf</code>.</li>
    </ol>
    <p class="normal">Thus, our state space contains an array of continuous values. Let's learn how we can obtain this from Gym. In order to get the state space, we can just type <code class="Code-In-Text--PACKT-">env.observation_space</code> as shown as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.observation_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Box(<span class="hljs-number">4</span>,)
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">Box</code> implies that our state space consists of continuous values and not discrete values. That is, in the Frozen Lake environment, we obtained the state space as <code class="Code-In-Text--PACKT-">Discrete(16)</code>, which shows that we have 16 discrete states (<strong class="keyword">S</strong> to <strong class="keyword">G</strong>). But now we have our state space denoted as <code class="Code-In-Text--PACKT-">Box(4,)</code>, which implies that our state space is continuous and consists of an array of 4 values.</p>
    <p class="normal">For example, let's reset our environment and see how our initial state space will look like. We can reset the environment using the <code class="Code-In-Text--PACKT-">reset</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.reset())
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">array([ <span class="hljs-number">0.02002635</span>, <span class="hljs-number">-0.0228838</span> ,  <span class="hljs-number">0.01248453</span>,  <span class="hljs-number">0.04931007</span>])
</code></pre>
    <p class="normal">Note that <a id="_idIndexMarker195"/>here the state space is randomly initialized and so we will get different values every time we run the preceding code.</p>
    <p class="normal">The result of the preceding code implies that our initial state space consists of an array of 4 values that denote the cart position, cart velocity, pole angle, and pole velocity at the tip, respectively. That is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.16: Initial state space</p>
    <p class="normal">Okay, how can we obtain the maximum and minimum values of our state space? We can obtain the maximum values of our state space using <code class="Code-In-Text--PACKT-">env.observation_space.high</code> and the minimum values of our state space using <code class="Code-In-Text--PACKT-">env.observation_space.low</code>.</p>
    <p class="normal">For example, let's look at the maximum value of our state space:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.observation_space.high)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">[<span class="hljs-number">4.8000002e+00</span> <span class="hljs-number">3.4028235e+38</span> <span class="hljs-number">4.1887903e-01</span> <span class="hljs-number">3.4028235e+38</span>]
</code></pre>
    <p class="normal">It implies that:</p>
    <ol>
      <li class="numbered" value="1">The maximum value of the cart position is <code class="Code-In-Text--PACKT-">4.8</code>.</li>
      <li class="numbered">We learned that the maximum value of the cart velocity is <code class="Code-In-Text--PACKT-">+Inf</code>, and we know that infinity is not really a number, so it is represented using the largest positive real value <code class="Code-In-Text--PACKT-">3.4028235e+38</code>.</li>
      <li class="numbered">The maximum value of the pole angle is <code class="Code-In-Text--PACKT-">0.418</code> radians.</li>
      <li class="numbered">The maximum value of the pole velocity at the tip is <code class="Code-In-Text--PACKT-">+Inf</code>, so it is represented using the largest positive real value <code class="Code-In-Text--PACKT-">3.4028235e+38</code>.</li>
    </ol>
    <p class="normal">Similarly, we can <a id="_idIndexMarker196"/>obtain the minimum value of our state space as:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.observation_space.low)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">[<span class="hljs-number">-4.8000002e+00</span> <span class="hljs-number">-3.4028235e+38</span> <span class="hljs-number">-4.1887903e-01</span> <span class="hljs-number">-3.4028235e+38</span>]
</code></pre>
    <p class="normal">It states that:</p>
    <ol>
      <li class="numbered" value="1">The minimum value of the cart position is <code class="Code-In-Text--PACKT-">-4.8</code>.</li>
      <li class="numbered">We learned that the minimum value of the cart velocity is <code class="Code-In-Text--PACKT-">-Inf</code>, and we know that infinity is not really a number, so it is represented using the largest negative real value <code class="Code-In-Text--PACKT-">-3.4028235e+38</code>.</li>
      <li class="numbered">The minimum value of the pole angle is <code class="Code-In-Text--PACKT-">-0.418</code> radians.</li>
      <li class="numbered">The minimum value of the pole velocity at the tip is <code class="Code-In-Text--PACKT-">-Inf</code>, so it is represented using the largest negative real value <code class="Code-In-Text--PACKT-">-3.4028235e+38</code>.</li>
    </ol>
    <h3 id="_idParaDest-72" class="title">Action space</h3>
    <p class="normal">Now, let's look at the action space. We already learned that in the Cart-Pole environment <a id="_idIndexMarker197"/>we perform two actions, which are pushing the cart to the left and pushing the cart to the right, and thus the action space is discrete since we have only two discrete actions.</p>
    <p class="normal">In order to get the action space, we can just type <code class="Code-In-Text--PACKT-">env.action_space</code> as the following shows:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.action_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Discrete(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">As we can observe, <code class="Code-In-Text--PACKT-">Discrete(2)</code> implies that our action space is discrete, and we have two actions <a id="_idIndexMarker198"/>in our action space. Note that the actions will be encoded into numbers as shown in <em class="italic">Table 2.4</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_20.png" alt=""/></figure>
    <p class="packt_figref">Table 2.4: Two possible actions</p>
    <h3 id="_idParaDest-73" class="title">Cart-Pole balancing with random policy</h3>
    <p class="normal">Let's create an agent with the random policy, that is, we create the agent that selects a random <a id="_idIndexMarker199"/>action in the environment and tries to balance the pole. The agent receives a +1 reward every time the pole stands straight up on the cart. We will generate over 100 episodes, and we will see the return (sum of rewards) obtained over each episode. Let's learn this step by step.</p>
    <p class="normal">First, let's create our Cart-Pole environment:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
env = gym.make(<span class="hljs-string">'CartPole-v0'</span>)
</code></pre>
    <p class="normal">Set the number of episodes and number of time steps in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">num_episodes = <span class="hljs-number">100</span>
num_timesteps = <span class="hljs-number">50</span>
</code></pre>
    <p class="normal">For each episode:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_episodes):
</code></pre>
    <p class="normal">Set the return to <code class="Code-In-Text--PACKT-">0</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    Return = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">Initialize the state by resetting the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = env.reset()
</code></pre>
    <p class="normal">For each step in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
</code></pre>
    <p class="normal">Render the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        env.render()
</code></pre>
    <p class="normal">Randomly select an action by sampling from the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        random_action = env.action_space.sample()
</code></pre>
    <p class="normal">Perform the randomly selected action:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state, reward, done, info = env.step(random_action)
</code></pre>
    <p class="normal">Update the return:</p>
    <pre class="programlisting code"><code class="hljs-code">        Return = Return + reward
</code></pre>
    <p class="normal">If the next state is a terminal state then end the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> done:
            <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">For <a id="_idIndexMarker200"/>every 10 episodes, print the return (sum of rewards):</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> i%<span class="hljs-number">10</span>==<span class="hljs-number">0</span>:
        print(<span class="hljs-string">'Episode: {}, Return: {}'</span>.format(i, Return))
        
</code></pre>
    <p class="normal">Close the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">env.close()
</code></pre>
    <p class="normal">The preceding code will output the sum of rewards obtained over every 10 episodes:</p>
    <pre class="programlisting code"><code class="hljs-code">Episode: <span class="hljs-number">0</span>, Return: <span class="hljs-number">14.0</span>
Episode: <span class="hljs-number">10</span>, Return: <span class="hljs-number">31.0</span>
Episode: <span class="hljs-number">20</span>, Return: <span class="hljs-number">16.0</span>
Episode: <span class="hljs-number">30</span>, Return: <span class="hljs-number">9.0</span>
Episode: <span class="hljs-number">40</span>, Return: <span class="hljs-number">18.0</span>
Episode: <span class="hljs-number">50</span>, Return: <span class="hljs-number">13.0</span>
Episode: <span class="hljs-number">60</span>, Return: <span class="hljs-number">25.0</span>
Episode: <span class="hljs-number">70</span>, Return: <span class="hljs-number">21.0</span>
Episode: <span class="hljs-number">80</span>, Return: <span class="hljs-number">17.0</span>
Episode: <span class="hljs-number">90</span>, Return: <span class="hljs-number">14.0</span>
</code></pre>
    <p class="normal">Thus, we have learned about one of the interesting and classic control problems called Cart-Pole balancing and how to create the Cart-Pole balancing environment using Gym. Gym <a id="_idIndexMarker201"/>provides several other classic control environments as shown in <em class="italic">Figure 2.17</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_21.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.17: Classic control environments</p>
    <p class="normal">You can also do some experimentation by creating any of the above environments using Gym. We <a id="_idIndexMarker202"/>can check all the classic control environments offered by Gym here: <a href="https://gym.openai.com/envs/#classic_control"><span class="url">https://gym.openai.com/envs/#classic_control</span></a>.</p>
    <h2 id="_idParaDest-74" class="title">Atari game environments</h2>
    <p class="normal">Are you <a id="_idIndexMarker203"/>a fan of Atari games? If yes, then this <a id="_idIndexMarker204"/>section will interest you. Atari 2600 is a video game console from a game company called Atari. The Atari game console provides several popular games, which include Pong, Space Invaders, Ms. Pac-Man, Break Out, Centipede, and many more. Training our reinforcement learning agent to play Atari games is an interesting as well as challenging task. Often, most of the RL algorithms will be tested out on Atari game environments to evaluate the accuracy of the algorithm.</p>
    <p class="normal">In this <a id="_idIndexMarker205"/>section, we will learn how to create the Atari game environment using Gym. Gym provides about 59 Atari game environments including Pong, Space Invaders, Air Raid, Asteroids, Centipede, Ms. Pac-Man, and so on. Some of the <a id="_idIndexMarker206"/>Atari game environments provided by Gym are shown in <em class="italic">Figure 2.18</em> to keep you excited:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_22.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.18: Atari game environments</p>
    <p class="normal">In Gym, every Atari game environment has 12 different variants. Let's understand this with the Pong game environment. The Pong game environment will have 12 different variants as explained in the following sections.</p>
    <h2 id="_idParaDest-75" class="title">General environment</h2>
    <ul>
      <li class="bullet"><strong class="keyword">Pong-v0 and Pong-v4</strong>: We can create a Pong environment with the environment id as <a id="_idIndexMarker207"/>Pong-v0 or Pong-v4. Okay, what about the state of our environment? Since we are dealing with the game <a id="_idIndexMarker208"/>environment, we can just take the image of our game screen as our state. But we can't deal with the raw image directly so we will take the pixel values of our game screen as the state. We will learn more about this in the upcoming section.</li>
      <li class="bullet"><strong class="keyword">Pong-ram-v0 and Pong-ram-v4</strong>: This is similar to Pong-v0 and Pong-v4, respectively. However, here, the state of the environment is the RAM of the Atari <a id="_idIndexMarker209"/>machine, which is just the 128 bytes <a id="_idIndexMarker210"/>instead of the game screen's pixel values.</li>
    </ul>
    <h3 id="_idParaDest-76" class="title">Deterministic environment</h3>
    <ul>
      <li class="bullet"><strong class="keyword">PongDeterministic-v0 and PongDeterministic-v4</strong>: In this type, as the name suggests, the <a id="_idIndexMarker211"/>initial position of the game will be the same every time we initialize the environment, and the state of the environment is the pixel values of the game screen.</li>
      <li class="bullet"><strong class="keyword">Pong-ramDeterministic-v0 and Pong-ramDeterministic-v4</strong>: This is similar to PongDeterministic-v0 and PongDeterministic-v4, respectively, but here the state is the RAM of the Atari machine.</li>
    </ul>
    <h3 id="_idParaDest-77" class="title">No frame skipping</h3>
    <ul>
      <li class="bullet"><strong class="keyword">PongNoFrameskip-v0 and PongNoFrameskip-v4</strong>: In this type, no game frame is skipped; all <a id="_idIndexMarker212"/>game screens are visible to the agent and the state is the pixel value of the game screen.</li>
      <li class="bullet"><strong class="keyword">Pong-ramNoFrameskip-v0 and Pong-ramNoFrameskip-v4</strong>: This is similar to PongNoFrameskip-v0 and PongNoFrameskip-v4, but here the state is the RAM of the Atari machine.</li>
    </ul>
    <p class="normal">Thus in the Atari environment, the state of our environment will be either the game screen or the RAM of the Atari machine. Note that similar to the Pong game, all other Atari games have the id in the same fashion in the Gym environment. For example, suppose we want to create a deterministic Space Invaders environment; then we can just create it with the id <code class="Code-In-Text--PACKT-">SpaceInvadersDeterministic-v0</code>. Say we want to create a Space Invaders environment with no frame skipping; then we can create it with the id <code class="Code-In-Text--PACKT-">SpaceInvadersNoFrameskip-v0</code>.</p>
    <p class="normal">We can <a id="_idIndexMarker213"/>check out all the Atari game environments offered by Gym here: <a href="https://gym.openai.com/envs/#atari"><span class="url">https://gym.openai.com/envs/#atari</span></a>.</p>
    <h3 id="_idParaDest-78" class="title">State and action space</h3>
    <p class="normal">Now, let's <a id="_idIndexMarker214"/>explore the state space and action space of the Atari game environments in detail.</p>
    <h4 class="title">State space</h4>
    <p class="normal">In this section, let's understand the state space of the Atari games in the Gym environment. Let's learn this with the Pong game. We learned that in the Atari environment, the state of the environment will be either the game screen's pixel values or the RAM of the Atari machine. First, let's understand the state space where the state of the environment is the game screen's pixel values.</p>
    <p class="normal">Let's create a Pong environment with the <code class="Code-In-Text--PACKT-">make</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"Pong-v0"</span>)
</code></pre>
    <p class="normal">Here, the game <a id="_idIndexMarker215"/>screen is the state of our environment. So, we will just take the image of the game screen as the state. However, we can't deal with the raw images directly, so we will take the pixel values of the image (game screen) as our state. The dimension of the image pixel will be <code class="Code-In-Text--PACKT-">3</code> containing the image height, image width, and the number of the channel.</p>
    <p class="normal">Thus, the state of our environment will be an array containing the pixel values of the game screen:</p>
    <pre class="programlisting code"><code class="hljs-code"> [Image height, image width, number of the channel]
</code></pre>
    <p class="normal">Note that the pixel values range from 0 to 255. In order to get the state space, we can just type <code class="Code-In-Text--PACKT-">env.observation_space</code> as the following shows:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.observation_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Box(<span class="hljs-number">210</span>, <span class="hljs-number">160</span>, <span class="hljs-number">3</span>)
</code></pre>
    <p class="normal">This indicates that our state space is a 3D array with a shape of [<code class="Code-In-Text--PACKT-">210</code>,<code class="Code-In-Text--PACKT-">160</code>,<code class="Code-In-Text--PACKT-">3</code>]. As we've learned, <code class="Code-In-Text--PACKT-">210</code> denotes the height of the image, <code class="Code-In-Text--PACKT-">160</code> denotes the width of the image, and <code class="Code-In-Text--PACKT-">3</code> represents the number of channels.</p>
    <p class="normal">For example, we can reset our environment and see how the initial state space looks like. We can reset the environment using the reset function:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.reset())
</code></pre>
    <p class="normal">The preceding code will print an array representing the initial game screen's pixel value.</p>
    <p class="normal">Now, let's create a Pong environment where the state of our environment is the RAM of the Atari machine instead of the game screen's pixel value:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"Pong-ram-v0"</span>)
</code></pre>
    <p class="normal">Let's look at the state space:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.observation_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Box(<span class="hljs-number">128</span>,)
</code></pre>
    <p class="normal">This implies that our state space is a 1D array containing 128 values. We can reset our environment and see how the initial state space looks like:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.reset())
</code></pre>
    <p class="normal">Note that this <a id="_idIndexMarker216"/>applies to all Atari games in the Gym environment, for example, if we create a space invaders environment with the state of our environment as the game screen's pixel value, then our state space will be a 3D array with a shape of <code class="Code-In-Text--PACKT-">Box(210, 160, 3)</code>. However, if we create the Space Invaders environment with the state of our environment as the RAM of Atari machine, then our state space will be an array with a shape of <code class="Code-In-Text--PACKT-">Box(128,)</code>.</p>
    <h4 class="title">Action space</h4>
    <p class="normal">Let's now <a id="_idIndexMarker217"/>explore the action space. In general, the Atari game environment has 18 actions in the action space, and the actions are encoded from 0 to 17 as shown in <em class="italic">Table 2.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_23.png" alt=""/></figure>
    <p class="packt_figref">Table 2.5: Atari game environment actions</p>
    <p class="normal">Note that all the preceding 18 actions are not applicable to all the Atari game environments and the action space varies from game to game. For instance, some games use <a id="_idIndexMarker218"/>only the first six of the preceding actions as their action space, and some games use only the first nine of the preceding actions as their action space, while others use all of the preceding 18 actions. Let's understand this with an example using the Pong game:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"Pong-v0"</span>)
print(env.action_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Discrete(<span class="hljs-number">6</span>)
</code></pre>
    <p class="normal">The code shows that we have <code class="Code-In-Text--PACKT-">6</code> actions in the Pong action space, and the actions are encoded from <code class="Code-In-Text--PACKT-">0</code> to <code class="Code-In-Text--PACKT-">5</code>. So the possible actions in the Pong game are noop (no action), fire, up, right, left, and down.</p>
    <p class="normal">Let's now look at the action space of the Road Runner game. Just in case you have not come across this game before, the game screen looks like this:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_24.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.19: The Road Runner environment</p>
    <p class="normal">Let's see <a id="_idIndexMarker219"/>the action space of the Road Runner game:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"RoadRunner-v0"</span>)
print(env.action_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Discrete(<span class="hljs-number">18</span>)
</code></pre>
    <p class="normal">This shows us that the action space in the Road Runner game includes all 18 actions. </p>
    <h3 id="_idParaDest-79" class="title">An agent playing the Tennis game</h3>
    <p class="normal">In this section, let's explore how to create an agent to play the Tennis game. Let's create an agent <a id="_idIndexMarker220"/>with a random policy, meaning that the agent will select an action randomly from the action space and perform the randomly selected action.</p>
    <p class="normal">First, let's create our Tennis environment:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
env = gym.make(<span class="hljs-string">'Tennis-v0'</span>)
</code></pre>
    <p class="normal">Let's view the Tennis environment:</p>
    <pre class="programlisting code"><code class="hljs-code">env.render()
</code></pre>
    <p class="normal">The preceding code will display the following:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_25.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.20: The Tennis game environment</p>
    <p class="normal">Set the number of episodes and the number of time steps in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">num_episodes = <span class="hljs-number">100</span>
num_timesteps = <span class="hljs-number">50</span>
</code></pre>
    <p class="normal">For each episode:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_episodes):
</code></pre>
    <p class="normal">Set the return to <code class="Code-In-Text--PACKT-">0</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    Return = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">Initialize the state by resetting the environment: </p>
    <pre class="programlisting code"><code class="hljs-code">    state = env.reset()
</code></pre>
    <p class="normal">For each step in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
</code></pre>
    <p class="normal">Render the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        env.render()
</code></pre>
    <p class="normal">Randomly <a id="_idIndexMarker221"/>select an action by sampling from the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        random_action = env.action_space.sample()
</code></pre>
    <p class="normal">Perform the randomly selected action:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state, reward, done, info = env.step(random_action)
</code></pre>
    <p class="normal">Update the return:</p>
    <pre class="programlisting code"><code class="hljs-code">        Return = Return + reward
</code></pre>
    <p class="normal">If the next state is a terminal state, then end the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> done:
            <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">For every 10 episodes, print the return (sum of rewards):</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> i%<span class="hljs-number">10</span>==<span class="hljs-number">0</span>:
        print(<span class="hljs-string">'Episode: {}, Return: {}'</span>.format(i, Return))
        
</code></pre>
    <p class="normal">Close the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">env.close()
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker222"/>code will output the return (sum of rewards) obtained over every 10 episodes:</p>
    <pre class="programlisting code"><code class="hljs-code">Episode: <span class="hljs-number">0</span>, Return: <span class="hljs-number">-1.0</span>
Episode: <span class="hljs-number">10</span>, Return: <span class="hljs-number">-1.0</span>
Episode: <span class="hljs-number">20</span>, Return: <span class="hljs-number">0.0</span>
Episode: <span class="hljs-number">30</span>, Return: <span class="hljs-number">-1.0</span>
Episode: <span class="hljs-number">40</span>, Return: <span class="hljs-number">-1.0</span>
Episode: <span class="hljs-number">50</span>, Return: <span class="hljs-number">-1.0</span>
Episode: <span class="hljs-number">60</span>, Return: <span class="hljs-number">0.0</span>
Episode: <span class="hljs-number">70</span>, Return: <span class="hljs-number">0.0</span>
Episode: <span class="hljs-number">80</span>, Return: <span class="hljs-number">-1.0</span>
Episode: <span class="hljs-number">90</span>, Return: <span class="hljs-number">0.0</span>
</code></pre>
    <h3 id="_idParaDest-80" class="title">Recording the game</h3>
    <p class="normal">We have just <a id="_idIndexMarker223"/>learned how to create an agent that randomly selects an action from the action space and plays the Tennis game. Can we also record the game played by the agent and save it as a video? Yes! Gym provides a wrapper class, which we can use to save the agent's gameplay as video.</p>
    <p class="normal">To record the game, our system should support FFmpeg. FFmpeg is a framework used for processing media files. So before moving ahead, make sure that your system provides FFmpeg support.</p>
    <p class="normal">We can record our game using the <code class="Code-In-Text--PACKT-">Monitor</code> wrapper as the following code shows. It takes three parameters: the environment; the directory where we want to save our recordings; and the force option. If we set <code class="Code-In-Text--PACKT-">force = False</code>, it implies that we need to create a new directory every time we want to save new recordings, and when we set <code class="Code-In-Text--PACKT-">force = True</code>, old recordings in the directory will be cleared out and replaced by new recordings:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.wrappers.Monitor(env, <span class="hljs-string">'recording'</span>, force=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">We just need to add the preceding line of code after creating our environment. Let's take a simple example and see how the recordings work. Let's make our agent randomly play the Tennis game for a single episode and record the agent's gameplay as a video:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
env = gym.make(<span class="hljs-string">'Tennis-v0'</span>)
<span class="hljs-comment">#Record the game</span>
env = gym.wrappers.Monitor(env, <span class="hljs-string">'recording'</span>, force=<span class="hljs-literal">True</span>)
env.reset()
<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(<span class="hljs-number">5000</span>):
    env.render()
    action = env.action_space.sample() 
    next_state, reward, done, info = env.step(action)
    <span class="hljs-keyword">if</span> done:
        <span class="hljs-keyword">break</span>
env.close()
</code></pre>
    <p class="normal">Once the <a id="_idIndexMarker224"/>episode ends, we will see a new directory <a id="_idIndexMarker225"/>called <strong class="keyword">recording</strong> and we can find the video file in MP4 format in this directory, which has our agent's gameplay as shown in <em class="italic">Figure 2.21</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_26.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.21: The Tennis gameplay</p>
    <h2 id="_idParaDest-81" class="title">Other environments</h2>
    <p class="normal">Apart from <a id="_idIndexMarker226"/>the classic control and the Atari game environments we've discussed, Gym also provides several different categories of the environment. Let's find out more about them.</p>
    <h3 id="_idParaDest-82" class="title">Box2D</h3>
    <p class="normal">Box2D is the 2D simulator that is majorly used for training <a id="_idIndexMarker227"/>our agent to perform continuous control tasks, such as walking. For example, Gym provides a Box2D environment called <code class="Code-In-Text--PACKT-">BipedalWalker-v2</code>, which we can use to train our agent to walk. The <code class="Code-In-Text--PACKT-">BipedalWalker-v2</code> environment is shown in <em class="italic">Figure 2.22</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_27.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.22: The Bipedal Walker environment</p>
    <p class="normal">We can check out several other Box2D environments offered by Gym here: <a href="https://gym.openai.com/envs/#box2d"><span class="url">https://gym.openai.com/envs/#box2d</span></a>.</p>
    <h3 id="_idParaDest-83" class="title">MuJoCo</h3>
    <p class="normal"><strong class="keyword">Mujoco</strong> stands <a id="_idIndexMarker228"/>for <strong class="keyword">Multi-Joint dynamics with Contact</strong> and is <a id="_idIndexMarker229"/>one of the most popular simulators used for training our agent to perform continuous control tasks. For example, MuJoCo provides an interesting environment called <code class="Code-In-Text--PACKT-">HumanoidStandup-v2</code>, which we can use to train our agent to stand up. The <code class="Code-In-Text--PACKT-">HumanoidStandup-v2</code> environment is shown in <em class="italic">Figure 2.23</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_28.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.23: The Humanoid Stand Up environment</p>
    <p class="normal">We can <a id="_idIndexMarker230"/>check out several other Mujoco environments <a id="_idIndexMarker231"/>offered by Gym here: <a href="https://gym.openai.com/envs/#mujoco"><span class="url">https://gym.openai.com/envs/#mujoco</span></a>.</p>
    <h3 id="_idParaDest-84" class="title">Robotics</h3>
    <p class="normal">Gym provides several environments for performing <a id="_idIndexMarker232"/>goal-based tasks for the fetch and shadow hand robots. For example, Gym provides an environment called <code class="Code-In-Text--PACKT-">HandManipulateBlock-v0</code>, which we can use to train our agent to orient a box using a robotic hand. The <code class="Code-In-Text--PACKT-">HandManipulateBlock-v0</code> environment is shown in <em class="italic">Figure 2.24</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_02_29.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.24: The Hand Manipulate Block environment</p>
    <p class="normal">We can <a id="_idIndexMarker233"/>check out the several robotics environments <a id="_idIndexMarker234"/>offered by Gym here: <a href="https://gym.openai.com/envs/#robotics"><span class="url">https://gym.openai.com/envs/#robotics</span></a>.</p>
    <h3 id="_idParaDest-85" class="title">Toy text</h3>
    <p class="normal">Toy text is the simplest text-based environment. We <a id="_idIndexMarker235"/>already learned about one such environment <a id="_idIndexMarker236"/>at the beginning of this chapter, which is the Frozen Lake environment. We can check out other interesting toy text environments offered by Gym here: <a href="https://gym.openai.com/envs/#toy_text"><span class="url">https://gym.openai.com/envs/#toy_text</span></a>.</p>
    <h3 id="_idParaDest-86" class="title">Algorithms</h3>
    <p class="normal">Instead of <a id="_idIndexMarker237"/>using our RL agent to play games, can we make use of our agent to solve some interesting problems? Yes! The algorithmic environment provides several interesting problems like copying a given sequence, performing addition, and so on. We can make use of the RL agent to solve these problems by learning how to perform computation. For instance, Gym provides an environment called <code class="Code-In-Text--PACKT-">ReversedAddition-v0</code>, which we can use to train our agent to add <a id="_idIndexMarker238"/>multiple digit numbers.</p>
    <p class="normal">We can <a id="_idIndexMarker239"/>check the algorithmic environments offered by Gym here: <a href="https://gym.openai.com/envs/#algorithmic"><span class="url">https://gym.openai.com/envs/#algorithmic</span></a>.</p>
    <h1 id="_idParaDest-87" class="title">Environment synopsis</h1>
    <p class="normal">We have learned about several types of Gym environment. Wouldn't it be nice if we could have <a id="_idIndexMarker240"/>information about all the environments in a single place? Yes! The Gym wiki provides a description of all the environments with their environment id, state space, action space, and reward range in a table: <a href="https://github.com/openai/gym/wiki/Table-of-environments"><span class="url">https://github.com/openai/gym/wiki/Table-of-environments</span></a>.</p>
    <p class="normal">We can also check all the available environments in Gym using the <code class="Code-In-Text--PACKT-">registry.all()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> gym <span class="hljs-keyword">import</span> envs
print(envs.registry.all())
</code></pre>
    <p class="normal">The preceding code will print all the available environments in Gym.</p>
    <p class="normal">Thus, in this chapter, we have learned about the Gym toolkit and also several interesting environments offered by Gym. In the upcoming chapters, we will learn how to train our RL agent in a Gym environment to find the optimal policy.</p>
    <h1 id="_idParaDest-88" class="title">Summary</h1>
    <p class="normal">We started the chapter by understanding how to set up our machine by installing Anaconda and the Gym toolkit. We learned how to create a Gym environment using the <code class="Code-In-Text--PACKT-">gym.make()</code> function. Later, we also explored how to obtain the state space of the environment using <code class="Code-In-Text--PACKT-">env.observation_space</code> and the action space of the environment using <code class="Code-In-Text--PACKT-">env.action_space</code>. We then learned how to obtain the transition probability and reward function of the environment using <code class="Code-In-Text--PACKT-">env.P</code>. Following this, we also learned how to generate an episode using the Gym environment. We understood that in each step of the episode we select an action using the <code class="Code-In-Text--PACKT-">env.step()</code> function.</p>
    <p class="normal">We understood the classic control methods in the Gym environment. We learned about the continuous state space of the classic control environments and how they are stored in an array. We also learned how to balance a pole using a random agent. Later, we learned about interesting Atari game environments, and how Atari game environments are named in Gym, and then we explored their state space and action space. We also learned how to record the agent's gameplay using the wrapper class, and at the end of the chapter, we discovered other environments offered by Gym.</p>
    <p class="normal">In the next chapter, we will learn how to find the optimal policy using two interesting algorithms called value iteration and policy iteration.</p>
    <h1 id="_idParaDest-89" class="title">Questions</h1>
    <p class="normal">Let's evaluate our newly gained knowledge by answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">What is the use of a Gym toolkit?</li>
      <li class="numbered">How do we create an environment in Gym?</li>
      <li class="numbered">How do we obtain the action space of the Gym environment?</li>
      <li class="numbered">How do we visualize the Gym environment?</li>
      <li class="numbered">Name some classic control environments offered by Gym.</li>
      <li class="numbered">How do we generate an episode using the Gym environment?</li>
      <li class="numbered">What is the state space of Atari Gym environments?</li>
      <li class="numbered">How do we record the agent's gameplay?</li>
    </ol>
    <h1 id="_idParaDest-90" class="title">Further reading</h1>
    <p class="normal">Check out the following resources for more information:</p>
    <ul>
      <li class="bullet">To learn more about Gym, go to <a href="http://gym.openai.com/docs/"><span class="url">http://gym.openai.com/docs/</span></a>.</li>
      <li class="bullet">We can also check out the Gym repository to understand how Gym environments are coded: <a href="https://github.com/openai/gym"><span class="url">https://github.com/openai/gym</span></a>.</li>
    </ul>
  </div>
</body></html>