<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Q-Network</h1>
                </header>
            
            <article>
                
<p>So far, we've approached and developed reinforcement learning algorithms that learn about a value function, <em>V</em>, <span>for each state, </span>or an action-value function, <em>Q</em>, for each action-state pair. These methods involve storing and updating each value <span>separately</span> in a table (or an array). These approaches do not scale because, for a large number of states and actions, the table's dimensions increase exponentially and can easily exceed the available memory capacity.</p>
<p>In this chapter, we will introduce the use of function approximation in reinforcement learning algorithms to overcome this problem. In particular, we will focus on deep neural networks that are applied to Q-learning. In the first part of this chapter, we'll explain how to extend Q-learning with function approximation to store Q values, and we'll explore some major difficulties that we may face. In the second part, we will present a new algorithm called <strong>Deep Q-network</strong> (<strong>DQN</strong>), which using new ideas, offers an elegant solution to some challenges that are found in the vanilla version of Q-learning with neural networks. You'll see how this algorithm achieves surprising results on a wide variety of games that learn only from pixels. Moreover, you'll implement this algorithm and apply it to Pong, and see some of its strengths and vulnerabilities for yourself.</p>
<p>Since DQN was proposed, other researchers have proposed many variations that provide more stability and efficiency for the algorithm. We'll quickly look at and implement some of them so that we have a better understanding of the weaknesses of the basic version of DQN and so that we can provide you with some ideas so that you can improve it yourself.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Deep neural networks and Q-learning</li>
<li>DQN</li>
<li>DQN applied to Pong</li>
<li>DQN variations</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep neural networks and Q-learning</h1>
                </header>
            
            <article>
                
<p>The Q-learning algorithm, as we saw in <a href="6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml">Chapter 4</a>, <em>Q-Learning and SARSA Applications</em>, has many qualities that enable its application in many real-world contexts. A key ingredient of this algorithm is that it makes use of the Bellman equation for learning the Q-function. The Bellman equation, as used by the Q-learning algorithm, enables the updating of Q-values from subsequent state-action values. This makes the algorithm able to learn at every step, without waiting until the trajectory is completed. Also, every state or action-state pair has its own values stored in a lookup table that saves and retrieves the corresponding values. Being designed in this way, Q-learning converges to optimal values as long as all the state-action pairs are repeatedly sampled. Furthermore, the method uses two policies: a non-greedy behavior policy to gather experience from the environment (for example, <img class="fm-editor-equation" src="assets/2456b0f9-50be-4a72-8670-0095bae6292b.png" style="width:0.58em;height:0.83em;"/>-greedy) and a target greedy policy that follows the maximum Q-value.</p>
<p>Maintaining a tabular representation of values can be contraindicated and in some cases, harmful. That's because most problems have a very high number of states and actions. For example, images (including small ones) have more state than the atoms in the universe. You can easily guess that, in this situation, tables cannot be used. Besides the infinite memory that the storage of such a table requires, only a few states will be visited more than once, making learning about the Q-function or V-function extremely difficult. Thus, we may want to generalize across states. In this case, generalization means that we are <span>not only </span>interested in the precise value of a state, <em>V(s)</em>, but also in the values in similar and near states. If a state has never been visited, we could approximate it with the value of a state near it. Generally speaking, the concept of generalization is incredibly important in all machine learning, including reinforcement learning. </p>
<p>The concept of generalization is fundamental in circumstances where the agent doesn't have a complete view of the environment. In this case, the full state of the environment will be hidden by the agent that has to make decisions based solely on a restricted representation of the environment. This is known as <strong>observation</strong>. For example, think about a humanoid agent that deals with basic interactions in the real world. Obviously, it doesn't have a view of the complete state of the universe and of all the atoms. It only has a limited viewpoint, that is, observation, which is perceived by its sensors (such as video cameras). For this reason, the humanoid agent should generalize what's happening around it and behave accordingly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Function approximation </h1>
                </header>
            
            <article>
                
<p>Now that we have talked about the main constraints of tabular algorithms and expressed the need for generalization capabilities in RL algorithms, we have to deal with the tools that allow us to get rid of these tabular constraints and address the generalization problem.</p>
<p>We can now dismiss tables and represent value functions with a function approximator. Function approximation allows us to represent value functions in a constraint domain using only a fixed amount of memory. Resource allocation is only dependent on the function that's used to approximate the problem. The choice of function approximator is, as always, task-dependent. Examples of function approximation are linear functions, decision trees, nearest neighbor algorithms, artificial neural networks, and so on. As you may expect, artificial neural networks are preferred over all the others <span>– </span>it is not a coincidence that it is widespread across all kinds of RL algorithms. In particular, deep artificial neural networks, or for brevity, <strong>deep neural networks</strong> (<strong>DNNs</strong>), are used. Their popularity is due to their efficiency and ability to learn features by themselves, creating a hierarchical representation as the hidden layers of the network increase. Also, deep neural networks, and in particular, <strong>convolutional neural networks</strong> (<strong>CNNs</strong>), deal incredibly well with images, as demonstrated by recent breakthroughs, especially in supervised tasks. But despite the fact that almost all studies of deep neural networks have been done in supervised learning, their integration in an RL framework has produces very interesting results. However, as we'll see shortly, this is not easy. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning with neural networks</h1>
                </header>
            
            <article>
                
<p><span>In Q-learning, a deep neural network learns a set of weights to approximate the Q-value function. Thereby, the Q-value function is parametrized by </span><img class="fm-editor-equation" src="assets/47f90968-1930-465a-95df-834ba7ae1cde.png" style="width:0.50em;height:0.92em;"/><span> (the weights of the network) and written as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/98a40f0c-b8e7-4983-a2ba-d19839d56bf4.png" style="width:3.83em;height:1.33em;"/></p>
<p>To adapt Q-learning with deep neural networks (this combination takes the name of deep Q-learning), we have to come up with a loss function (or objective) to minimize. </p>
<p><span>As you may recall, the tabular Q-learning update is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e222333f-62b2-47e8-80ec-cd3e387e4699.png" style="width:24.50em;height:1.33em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/e4c0615d-ded4-4546-802e-410419febed6.png" style="width:0.92em;height:1.17em;"/> is the state at the next step. This update is done online on each sample that's collected by the behavior policy.</p>
<div class="packt_infobox">Compared to the previous chapters, to simplify the notation, here, we refer to <img class="fm-editor-equation" src="assets/00457ffe-7e2c-485e-adbf-7a8aff8e0bc5.png" style="width:1.83em;height:1.00em;"/> as the state and action in the present step, while <img class="fm-editor-equation" src="assets/1a6b4da8-b62e-4792-9d4d-94a718870eed.png" style="width:2.42em;height:1.33em;"/> is referred to as the state and action in the next step.</div>
<p>With the neural network, our objective is to optimize the weight, <img class="fm-editor-equation" src="assets/db3c4a2b-d406-4101-9489-dbdbefc7ac01.png" style="width:0.50em;height:0.92em;"/>, so that <img class="fm-editor-equation" src="assets/8b1968e5-7d3b-4b46-985f-cbb14efcb476.png" style="width:1.33em;height:1.08em;"/> resembles the optimal Q-value function. But since we don't have the optimal Q-function, we can only make small steps toward it by minimizing the Bellman error for one step, <img class="fm-editor-equation" src="assets/0a5374d6-404d-45e1-8c8e-1fd5288dbab8.png" style="width:13.75em;height:1.33em;"/>. This step is similar to what we've done in tabular Q-learning. However, in deep Q-learning, we don't update the single value, <img class="fm-editor-equation" src="assets/55fd7330-0019-4509-a791-cb1f3a7960a8.png" style="width:3.00em;height:1.17em;"/>. Instead, we take the gradient of the Q-function with respect to the parameters, <img class="fm-editor-equation" src="assets/a480d51f-1eb1-463f-9270-59e39cd03366.png" style="width:0.50em;height:0.92em;"/>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/231422ae-3ae5-47e8-b3a1-cf18a09cf8af.png" style="width:33.92em;height:1.58em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/bdd2ace6-17d7-4f48-a39e-ed1808930af8.png" style="width:5.33em;height:1.33em;"/> is the partial derivate of <img class="fm-editor-equation" src="assets/e941b29d-7599-4d23-81bc-a654b086904f.png" style="width:0.83em;height:1.00em;"/> with respect to <img class="fm-editor-equation" src="assets/9205cd8e-8704-4fbb-a34d-9063dcc45453.png" style="width:0.50em;height:0.92em;"/> . <img class="fm-editor-equation" src="assets/9f063db9-8f96-4021-9a6a-5a2adfd0af21.png" style="width:0.83em;height:0.75em;"/> is called the learning rate, which is the size of the step to take toward the gradient.</p>
<p>In reality, the smooth transition that we just saw from tabular Q-learning to deep Q-learning doesn't yield a good approximation. The first fix involves the use of the <strong>Mean Square Error</strong> (<strong>MSE</strong>) as a loss function (instead of the Bellman error). The second fix is to migrate from an online Q-iteration to a batch Q-iteration. This means that the parameters of the neural network are updated using multiple transitions at once (such as using a mini-batch of size greater than 1 in supervised settings). These changes produce the following loss function: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8b4a9659-3b2d-47fc-956b-91e4dfbe7805.png" style="width:26.08em;height:1.83em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/8e2d8ca5-6815-4a91-822f-23c145349a67.png" style="width:0.67em;height:1.17em;"/> isn't the true action-value function since we haven't used it. Instead, it is the Q-target value:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/89094609-0115-4184-84e1-040099cdd2b1.png" style="width:20.25em;height:1.67em;"/></p>
<p>Then, the network parameter, <img class="fm-editor-equation" src="assets/af8c754c-5f09-4058-901d-a6cbb18fc3c6.png" style="width:0.50em;height:0.92em;"/>, is updated by gradient descent on the MSE loss function, <img class="fm-editor-equation" src="assets/42695026-a675-49e2-b51e-d2b9526c6926.png" style="width:1.67em;height:1.00em;"/>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/38257569-a20d-4b7b-b4f6-23fac713e899.png" style="width:9.25em;height:1.50em;"/></p>
<p>It's very important to note that <img class="fm-editor-equation" src="assets/f31318a0-7dcd-43eb-8eac-270a9914848a.png" style="width:0.92em;height:0.92em;"/> is treated as a constant and that the gradient of the loss function isn't propagated further.</p>
<div class="packt_infobox">Since, in the previous chapter, we introduced MC algorithms, we want to highlight that these algorithms can also be adapted to work with neural networks. In this case, <img class="fm-editor-equation" src="assets/162cb052-26ad-42c8-b542-2c45e9dd0fe6.png" style="width:0.83em;height:0.75em;"/> will be the return, <img class="fm-editor-equation" src="assets/73088934-4648-4058-b4bd-266e60d814c0.png" style="width:0.75em;height:0.83em;"/>. Since the MC update isn't biased, it's asymptotically better than TD, but the latter has better results in practice.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep Q-learning instabilities</h1>
                </header>
            
            <article>
                
<p>With the loss function and the optimization technique we just presented, you should be able to develop a deep Q-learning algorithm. However, the reality is much more subtle. Indeed, if we try to implement it, it probably won't work. Why? Once we introduce neural networks, we can no longer guarantee improvement. Although tabular Q-learning has convergence capabilities, its neural network counterpart does not. </p>
<p>Sutton and Barto in <em>Reinforcement Learning: An Introduction</em>, introduced a problem called the deadly triad, which arises when the following three factors are combined:</p>
<ul>
<li>Function approximation</li>
<li>Bootstrapping (that is, the update used by other estimates)</li>
<li>Off-policy learning (Q-learning is an off-policy algorithm since its update is independent on the policy that's being used)</li>
</ul>
<p>But these are exactly the three main ingredients of the deep Q-learning algorithm. As the authors noted, we cannot get rid of bootstrapping without affecting the computational cost or data efficiency. Moreover, off-policy learning is important for creating more intelligent and powerful agents. And clearly, without deep neural networks, we'll lose an extremely important component. Therefore, it is very important to design algorithms that preserve these three components but at the same time mitigate the deadly triad problem.</p>
<p>Besides, from equations (5.2) and (5.3), the problem may seem similar to supervised regression, but it's not. In supervised learning, when performing SGD, the mini-batches are always sampled randomly from a dataset to make sure that they are <strong>independent and identically distributed</strong> (<strong>IID</strong>). In RL, it is the policy that gathers the experience. And because the states are sequential and strongly related to each other, the i.i.d assumption is immediately lost, causing severe instabilities when performing SGD.</p>
<p><span>Another cause of instability is due to the non-stationarity of the Q-learning process. From equation, (5.2) and (5.3), you can see that the same neural network that is updated is also the one that computes the target values,</span> <img style="font-size: 1em;width:0.50em;height:0.92em;" class="fm-editor-equation" src="assets/5867eec4-30f5-46f8-a86b-5809cb70bf16.png"/><span>. This is dangerous, considering that the target values will also be updated during training. It's like shooting at a moving circular target without taking into consideration its movement. These behaviors are only due to the generalization capabilities of the neural network; in fact, they are not a problem in a tabular case. </span></p>
<p>Deep Q-learning is poorly understood theoretically but, as we'll soon see, there is an algorithm that deploys a few tricks to increase the i.i.d of the data and alleviate the moving target problem. These tricks make the algorithm much more stable and flexible. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DQN</h1>
                </header>
            
            <article>
                
<p>DQN, which was introduced for the first time in the paper <em>Human-level control through deep reinforcement learning</em><q> </q>by Mnih and others from DeepMind, is the first scalable reinforcement learning algorithm that combines Q-learning with deep neural networks. To overcome stability issues, DQN adopts two novel techniques that turned out to be essential for the balance of the algorithm.</p>
<p>DQN has proven itself to be the first artificial agent capable of learning in a diverse array of challenging tasks. Furthermore, it has learned how to control many tasks using only high-dimensional row pixels as input and using an end-to-end RL approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The solution</h1>
                </header>
            
            <article>
                
<p>The key innovations brought by DQN involve a <strong>replay buffer</strong> to get over the data correlation drawback, and a separate <em>target network</em> to get over the non-stationarity problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Replay memory</h1>
                </header>
            
            <article>
                
<p>To use more IID data during SGD iterations, DQN introduced a replay memory (also called experienced replay) to collect and store the experience in a large buffer. This buffer ideally contains all the transitions that have taken place during the agent's lifetime. When doing SGD, a random mini-batch will be gathered from the experienced replay and used in the optimization procedure. Since the replay memory buffer holds varied experience, the mini-batch that's sampled from it will be diverse enough to provide independent samples. Another very important feature behind the use of an experience replay is that it enables the reusability of the data as the transitions will be sampled multiple times. This greatly increases the data efficiency of the algorithm. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The target network</h1>
                </header>
            
            <article>
                
<p>The moving target problem is due to continuously updating the network during training, which also modifies the target values. Nevertheless, the neural network has to update itself in order to provide the best possible state-action values. The solution that's employed in DQNs is to use two neural networks. One is called the <em>online network</em>, which is constantly updated, while the other is called the <em>target network</em>, which is updated only every <em>N</em> iterations (with <em>N</em> usually being between 1,000 and 10,000). The online network is used to interact with the environment while the target network is used to predict the target values. In this way, for <em>N</em> iterations, the target values that are produced by the target network remain fixed, preventing the propagation of instabilities and decreasing the risk of divergence. A potential disadvantage is that the target network is an old version of the online <span>network. Nonetheless, in practice, the advantages greatly outweigh the disadvantages and the stability of the algorithm will improve significantly.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The DQN algorithm</h1>
                </header>
            
            <article>
                
<p>The introduction of a replay buffer and of a separate target network in a deep Q-learning algorithm has been able to control Atari games (such as Space Invaders, Pong, and Breakout) from nothing but images, a reward, and a terminal signal. DQN learns completely end to end with a combination of CNN and fully connected neural networks.</p>
<p>DQN has been trained separately on 49 Atari games with the same algorithm, network architecture, and hyperparameters. It performed better than all the previous algorithms, achieving <span>a level comparable to or better than professional gamers</span> on many games. The Atari games are not easy to solve and many of them demand complex planning strategies. Indeed, a few of them (such as the well-known Montezuma's Revenge) required a level that even DQN hasn't been able to achieve.</p>
<p>A particularity of these games is that, as they provide only images to the agent, they are partially observable. They don't show the full state of the environment. In fact, a single image isn't enough to fully understand the current situation. For example, can you deduce <span>the direction of the ball</span> in the following image?</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1827 image-border" src="assets/35491dfb-ad59-49c7-aa1b-84480f56a173.png" style="width:13.75em;height:17.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.1. Rendering of pong</div>
<p><span>You can't, and neither can the agent. To overcome this situation, at each point in time, a sequence of the previous observations is considered. Usually the last two to five frames are used, and in most cases, they give a pretty accurate approximation of the actual overall state.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The loss function</h1>
                </header>
            
            <article>
                
<p>The deep Q-network is trained by minimizing the loss function (5.2) that we have already presented, but with the further employment of a separate Q-target network, <img class="fm-editor-equation" src="assets/3804d279-d7a4-4dab-b5d4-2f9287074dad.png" style="width:0.83em;height:1.25em;"/>, with a weight, <img class="fm-editor-equation" src="assets/cbd02dbe-17b9-4d28-a7ed-961ac768c913.png" style="width:0.75em;height:1.00em;"/>, putting everything together, the loss function becomes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e80dd0ac-24a7-43fd-8d6a-2b528a551450.png" style="width:31.83em;height:1.75em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/9420e431-c0af-40fd-8207-eec9aba0ffcb.png" style="width:0.50em;height:0.92em;"/> is the parameters of the online network.</p>
<p>The optimization of the differentiable loss function (5.4) is performed with our favorite iterative method, namely mini-batch gradient descent. That is, the learning update is applied to mini-batches that have been drawn uniformly from the experienced buffer. The derivative of the loss function is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/06c6fb1b-1b4d-4ff4-8f33-c1a63bfe59fd.png" style="width:40.00em;height:1.83em;"/></p>
<p><span>Unlike the problem framed in the case of deep Q-learning, in DQN, the learning process is more stable. Furthermore, because the data is more i.i.d. and the target is (somehow) fixed, it's very similar to a regression problem. But on the other hand, the targets still depend on the network weights.</span></p>
<div class="packt_infobox">If you optimize the loss function (5.4) at each step and only on a single sample, you would obtain the Q-learning algorithm with function approximation. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pseudocode</h1>
                </header>
            
            <article>
                
<p>Now that all the components of DQN have been explained, we can put all the pieces together and show you the pseudocode version of the algorithm to <span>clarify</span> any uncertainties (don't worry if it doesn't <span>–</span> in the next section, you'll implement it and everything will be clearer).</p>
<p>The DQN algorithm involves three main parts: </p>
<ul>
<li>Data collection and storage. The data is collected by following a behavior policy (for example, <img class="fm-editor-equation" src="assets/93596807-c9b3-4293-b605-b0981360a525.png" style="width:0.58em;height:0.83em;"/>-greedy).</li>
<li>Neural network optimization (performing SGD on mini-batches that have been sampled from the buffer).</li>
<li>Target update.</li>
</ul>
<p>The pseudocode of DQN is as follows:</p>
<pre>Initialize <img class="fm-editor-equation" src="assets/dcf84875-03f5-4c76-99e7-5e6efb84e148.png" style="width:0.75em;height:1.00em;"/> function with random weight <img class="fm-editor-equation" src="assets/caa635b4-6e8b-427b-a535-36046ba01156.png" style="width:0.50em;height:0.92em;"/><br/>Initialize <img class="fm-editor-equation" src="assets/e8df829a-72d3-48c1-89c5-083ef60d6c86.png" style="width:0.75em;height:1.17em;"/> function with random weight <img class="fm-editor-equation" src="assets/9ebd89b5-28fa-4278-886b-ceedc90a3244.png" style="width:2.58em;height:1.00em;"/><br/>Initialize empty replay memory <img class="fm-editor-equation" src="assets/5fe0b70d-6389-42f8-a639-c966137d9a0a.png" style="width:0.75em;height:0.75em;"/><br/><br/><strong>for</strong> <img class="fm-editor-equation" src="assets/e8d8d45d-1051-4dc4-8841-7894b7875887.png" style="width:7.17em;height:1.08em;"/> <strong>do<br/>    </strong>Initialize environment <img class="fm-editor-equation" src="assets/87fbb352-ec1f-42d6-8be9-62eface2ffbb.png" style="width:7.92em;height:1.33em;"/><br/>    <strong>for</strong> <img class="fm-editor-equation" src="assets/fa18c05e-906d-4c81-b291-3f427c3af0df.png" style="width:3.42em;height:0.83em;"/> <strong>do</strong><br/>        &gt; Collect observation from the env:<br/>        <img class="fm-editor-equation" src="assets/009795ec-391f-468c-8a4f-b0769e519d7d.png" style="width:8.42em;height:1.17em;"/><br/>        <img class="fm-editor-equation" src="assets/24bda192-a5f4-46bf-aa76-5bfcc0bfb021.png" style="width:7.00em;height:1.17em;"/><br/>        &gt; Store the transition in the replay buffer:<br/>        <img class="fm-editor-equation" src="assets/0e6ec8a0-c91c-4fe3-9813-9a87839fb73f.png" style="width:10.17em;height:1.25em;"/> <br/>        <img class="fm-editor-equation" src="assets/c28a0d44-ad2d-4f7d-a825-1311dcf3f040.png" style="width:9.75em;height:1.17em;"/><br/>        &gt; Update the model using (5.4):<br/>        Sample a random minibatch <img class="fm-editor-equation" src="assets/4a515179-7de2-4efd-ac00-0b9956ef9e8e.png" style="width:7.42em;height:1.50em;"/> from <img class="fm-editor-equation" src="assets/37e38229-200e-4f61-a93b-e2f90b98b292.png" style="width:0.75em;height:0.83em;"/><br/>        <img class="fm-editor-equation" src="assets/94c61509-7c5d-47b3-a120-9598ed5e9c13.png" style="width:22.58em;height:3.42em;"/> <br/>        Perform a step of GD on <img class="fm-editor-equation" src="assets/54dd4c80-10ee-46de-824d-c757c490d1b1.png" style="width:7.75em;height:1.25em;"/> on <img class="fm-editor-equation" src="assets/1eaecb5a-73cf-4c31-8f64-df061c836d3f.png" style="width:0.50em;height:0.92em;"/><br/>        &gt; Update target network:<br/>        Every C steps <img class="fm-editor-equation" src="assets/d99facbd-2714-46d5-bdda-363926366991.png" style="width:2.75em;height:1.00em;"/>  <img class="fm-editor-equation" src="assets/ded9f77f-3370-4d23-975b-f2a6cf5ca910.png" style="width:5.42em;height:1.33em;"/><br/>        <img class="fm-editor-equation" src="assets/962d9a59-a449-4dc5-be07-48489b8ce87d.png" style="width:3.58em;height:1.25em;"/><br/>    <strong>end for<br/></strong><br/><strong>end for</strong></pre>
<p>Here, <kbd>d</kbd> is a flag that's returned by the environment that signals whether the environment is in its final state. If <kbd>d=True</kbd>, that is, the episode has ended, the environment has to be reset. <br/>
<img class="fm-editor-equation" src="assets/d100d047-49cf-46bc-9477-439651e55000.png" style="width:0.67em;height:1.25em;"/> is a preprocessing step that changes the images to reduce their dimensionality (it converts the images into grayscale and resizes them into smaller images) and adds the last <kbd>n</kbd> frames to the current frame. Usually, <kbd>n</kbd> is a value between 2 and 4. The preprocessing part will be explained in more detail in the next section, where we'll implement DQN.</p>
<p>In DQN, the experienced replay, <img class="fm-editor-equation" src="assets/16e1586f-bf6d-42a9-9cb4-b16f41425950.png" style="width:0.75em;height:0.83em;"/>, is a dynamic buffer that stores a limited number of frames. In the paper, the buffer contains the last 1 million transitions and when it exceeds this dimension, it discards the older experiences. </p>
<p>All the other parts have already been described. If you are wondering why t<span>he target value, </span><img class="fm-editor-equation" src="assets/928f3116-b6f5-4883-a7b1-6bbf04bbfea4.png" style="width:0.75em;height:0.83em;"/>,<span> takes the</span> <img class="fm-editor-equation" src="assets/010c7f88-1ae7-4ef4-be70-385d773d785b.png" style="width:0.83em;height:0.92em;"/><span> if <img class="fm-editor-equation" src="assets/d963cc36-1c16-4e04-aa06-86afe819f31a.png" style="width:5.25em;height:1.17em;"/> value, it is because there won't be any other interactions with the environment after and so <img class="fm-editor-equation" src="assets/11b366a5-5a09-4f41-9fa0-9acef6d01b9a.png" style="width:0.92em;height:1.00em;"/> is its actual unbiased Q-value.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model architecture</h1>
                </header>
            
            <article>
                
<p>So far, we have talked about the algorithm itself, but we haven't explained the architecture of the DQN. Besides the new ideas that have been adopted to stabilize its training, the architecture of the DQN plays a crucial role in the final performance of the algorithm. In the <em>DQN</em> paper, a single model architecture is used in all of the Atari environments. It combines CNNs and FNNs. In particular, as observation images are given as input, it employs a CNN to learn about feature maps from those images. CNNs have been widely used with images for their translation <span>invariance </span>characteristics and for their property of sharing weights, which allows the network to learn with fewer weights compared to other deep neural network types. </p>
<p>The output of the model corresponds to the state-action values, with one for each action. Thus, to control an agent with five actions, the model will output a value for each of those five actions. Such a model architecture allows us to compute all the Q-values with only one forward pass.</p>
<p>There are three convolutional layers. Each layer includes a convolution operation with an increasing number of filters and a decreasing dimension, as well as a non-linear function. The last hidden layer is a fully connected layer, followed by a rectified activation function and a fully-connected linear layer with an output for each action. A simple representation of this architecture is shown in the following illustration:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1828 image-border" src="assets/01c3e6d6-1c32-4f3c-bef6-e4aa962214f0.png" style="width:29.17em;height:14.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.2. Illustration of a DNN architecture for DQN composed with a CNN and FNN</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DQN applied to Pong</h1>
                </header>
            
            <article>
                
<p>Equipped with all the technical knowledge about Q-learning, deep neural networks, and DQN, we can finally put it to work and start to warm up the GPU. In this section, we will apply DQN to an Atari environment, Pong. We have chosen Pong rather than all the other Atari environments because it's simpler to solve and thus requires less time, computational power, and memory. That being said, if you have <span>a decent GPU</span> available, you can apply the same exact configuration to almost all the other Atari games (some may require a little bit of fine-tuning). For the same reason, we adopted a lighter configuration compared to the original DQN paper, both in terms of the capacity of the function approximator (that is, fewer weights) and hyperparameters such as a smaller buffer size. This does not compromise the results rather on Pong but might degrade the performance of other games.</p>
<p>First, we will briefly introduce the Atari environment and the preprocessing pipeline before moving on to the DQN implementation. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Atari games </h1>
                </header>
            
            <article>
                
<p>Atari games became a standard testbed for deep RL algorithms since their introduction in the DQN paper. These were first provided in the <strong>Arcade Learning Environment</strong> (<strong>ALE</strong>) and subsequently wrapped by OpenAI Gym to provide a standard interface. ALE (and Gym) <span>includes 57 of the most popular Atari 2600 video games, such as Montezuma's Revenge, Pong, Breakout, and Space Invaders, as shown in the following illustration. These games have been widely used in RL research for their high-dimensional state space (210 x 160 pixels) and their task diversity between games:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1829 image-border" src="assets/4252c8ec-b509-42d2-93d8-d1cfc94e9e4b.png" style="width:162.50em;height:49.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.3 The Montezuma's Revenge, Pong, Breakout, <span>and Space Invaders environments</span></div>
<p>A very important note about Atari environments is that they are deterministic, meaning that, given a fixed set of actions, the results will be the same across multiple matches. From an algorithm perspective, this determinism holds true until all the history is used to choose an action from a stochastic policy. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing</h1>
                </header>
            
            <article>
                
<p>The frames in Atari are 210 x 160 pixels with RGB color, thus having an overall size of 210 x 160 x 3. If a history of 4 frames was used, the input would have a dimension of 210 x 160 x 12. Such dimensionality can be computationally demanding and it could be difficult to store a large number of frames in the experienced buffer. Therefore, a preprocessing step to reduce the dimensionality is necessary. In the original DQN implementation, the following preprocessing pipeline is used:</p>
<ul>
<li>RGB colors are converted into grayscale</li>
<li>The images are downsampled to 110 x 84 and then cropped to 84 x 84</li>
<li>The last three to four frames are concatenated to the current frame</li>
<li>The frames are normalized</li>
</ul>
<p>Furthermore, because the games are run at a high frame rate, a technique called frame-skipping is used to skip <img style="font-size: 1em;width:0.83em;height:1.42em;" class="fm-editor-equation" src="assets/c56514f6-eae8-4703-b818-0b357b964b3e.png"/> consecutive frames<span>. This technique allows the agent to store and train on fewer frames for each game without significantly degrading the performance of the algorithms. In practice, with the frame-skipping technique, the agent selects an action</span> every <img style="font-size: 1em;width:0.83em;height:1.42em;" class="fm-editor-equation" src="assets/6b679333-18df-4201-a115-199619733e58.png"/> frames<span> and repeats the action on the skipped frames. </span></p>
<p>In addition, in some environments, at the start of each game, the agent has to push the fire button in order to start the game. Also, because of the determinism of the environment, some no-ops are taken on the reset of the environment to start the agent in a random position.</p>
<p>Luckily for us, OpenAI released an implementation of the preprocessing pipeline that is compatible with the Gym interface. You can find it in this book's GitHub repository in the <kbd>atari_wrappers.py</kbd> file. Here, we will give just a brief explanation of the implementation:</p>
<ul>
<li><kbd>NoopResetEnv(n)</kbd><span>: Takes </span><kbd>n</kbd><span> no-ops on reset of the environment to provide a random starting position for the agent.</span></li>
<li><kbd>FireResetEnv()</kbd>: Fires on reset of the environment (required only in some games).</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<ul>
<li><kbd>MaxAndSkipEnv(skip)</kbd>: Skips <kbd>skip</kbd> frames while taking care of repeating the actions and summing the rewards.</li>
<li><kbd>WarpFrame()</kbd>: Resizes the frame to 84 x 84 and converts it into grayscale.</li>
<li><kbd>FrameStack(k)</kbd>: Stacks the last <kbd>k</kbd> frames.</li>
</ul>
<p>All of these functions are implemented as a wrapper. A wrapper is a way to easily transform an environment by adding a new layer on top of it. For example, to scale the frames on Pong, we would use the following code:</p>
<pre>env = gym.make('Pong-v0')<br/>env = ScaledFloatFrame(env)</pre>
<p> A wrapper has to inherit the <kbd>gym.Wrapper</kbd> class and override at least one of the following methods: <kbd>__init__(self, env)</kbd>, <kbd>step</kbd>, <kbd>reset</kbd>, <kbd>render</kbd>, <kbd>close</kbd>, or <kbd>seed</kbd>.</p>
<p>We won't show the implementation of all the wrappers listed here as they are outside of the scope of this book, but we will use <kbd><span>FireResetEnv </span></kbd>and <kbd>WrapFrame</kbd><span> as examples </span><span>to give you a general idea of their implementation. The complete code is available in this book's GitHub repository:</span></p>
<div>
<pre><span>class</span><span> </span><span>FireResetEnv</span><span>(</span><span>gym</span><span>.</span><span>Wrapper</span><span>):<br/></span><span>    def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>, </span><span>env</span><span>):<br/></span><span>        """</span><span>Take action on reset for environments that are fixed until firing.</span><span>"""<br/></span><span>        gym.Wrapper.</span><span>__init__</span><span>(</span><span>self</span><span>, env)<br/></span><span>        assert</span><span> env.unwrapped.</span><span>get_action_meanings</span><span>()[</span><span>1</span><span>] </span><span>==</span><span> </span><span>'</span><span>FIRE</span><span>'<br/></span><span>        assert</span><span> </span><span>len</span><span>(env.unwrapped.</span><span>get_action_meanings</span><span>()) </span><span>&gt;=</span><span> </span><span>3<br/><br/></span><span>    def</span><span> </span><span>reset</span><span>(</span><span>self</span><span>, </span><span>**</span><span>kwargs</span><span>):<br/></span><span>        self</span><span>.env.</span><span>reset</span><span>(</span><span>**</span><span>kwargs)<br/></span><span>        obs, _, done, _ </span><span>=</span><span> </span><span>self</span><span>.env.</span><span>step</span><span>(</span><span>1</span><span>)<br/></span><span>        if</span><span> done:<br/></span><span>            self</span><span>.env.</span><span>reset</span><span>(</span><span>**</span><span>kwargs)<br/></span><span>        obs, _, done, _ </span><span>=</span><span> </span><span>self</span><span>.env.</span><span>step</span><span>(</span><span>2</span><span>)<br/></span><span>        if</span><span> done:<br/></span><span>            self</span><span>.env.</span><span>reset</span><span>(</span><span>**</span><span>kwargs)<br/></span><span>        return</span><span> obs<br/><br/></span><span>    def</span><span> </span><span>step</span><span>(</span><span>self</span><span>, </span><span>ac</span><span>):<br/></span><span>        return</span><span> </span><span>self</span><span>.env.</span><span>step</span><span>(ac)</span></pre></div>
<p class="mce-root"/>
<p>First, <kbd>FireResetEnv</kbd> inherits the <kbd>Wrapper</kbd> class from Gym. Then, during the initialization, it checks the availability of the <kbd>fire</kbd> action by unwrapping the environment through <kbd>env.unwrapped</kbd>. The function overrides the <kbd>reset</kbd> function by calling <kbd>reset</kbd>, which was defined in the previous layer with <kbd>self.env.reset</kbd>, then takes a fire action by calling <kbd>self.env.step(1)</kbd> and an environment-dependent action, <kbd><span>self.env.step(2)</span></kbd>. </p>
<p><kbd>WrapFrame</kbd> has a similar definition:</p>
<div>
<div>
<pre><span>class</span><span> </span><span>WarpFrame</span><span>(</span><span>gym</span><span>.</span><span>ObservationWrapper</span><span>):<br/></span><span>    def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>, </span><span>env</span><span>):<br/></span><span>        """</span><span>Warp frames to 84x84 as done in the Nature paper and later work.</span><span>"""<br/></span><span>        gym.ObservationWrapper.</span><span>__init__</span><span>(</span><span>self</span><span>, env)<br/></span><span>        self</span><span>.width </span><span>=</span><span> </span><span>84<br/></span><span>        self</span><span>.height </span><span>=</span><span> </span><span>84<br/></span><span>        self</span><span>.observation_space </span><span>=</span><span> spaces.</span><span>Box</span><span>(</span><span>low</span><span>=</span><span>0</span><span>, </span><span>high</span><span>=</span><span>255</span><span>,<br/></span><span>                shape</span><span>=</span><span>(</span><span>self</span><span>.height, </span><span>self</span><span>.width, </span><span>1</span><span>), </span><span>dtype</span><span>=</span><span>np.uint8)<br/></span><span>    def</span><span> </span><span>observation</span><span>(</span><span>self</span><span>, </span><span>frame</span><span>):<br/></span><span>        frame </span><span>=</span><span> cv2.</span><span>cvtColor</span><span>(frame, cv2.</span><span>COLOR_RGB2GRAY</span><span>)<br/></span><span>        frame </span><span>=</span><span> cv2.</span><span>resize</span><span>(frame, (</span><span>self</span><span>.width, </span><span>self</span><span>.height), </span><span>interpolation</span><span>=</span><span>cv2.</span><span>INTER_AREA</span><span>)<br/></span><span>        return</span><span> frame[</span><span>:</span><span>, </span><span>:</span><span>, </span><span>None</span><span>]</span></pre></div>
</div>
<p>This time, <kbd>WarpFrame</kbd> inherits the properties from <kbd>gym.ObservationWrapper</kbd> and creates a <kbd>Box</kbd> space with values between 0 and 255 and with the shape 84 x 84. When <kbd>observation()</kbd> is called, it converts the RGB frames into grayscale and resizes the images to the chosen shape.</p>
<p class="mce-root">We can then create a function, <kbd>make_env</kbd>, to apply every wrapper to an environment:</p>
<div>
<pre><span>def</span><span> </span><span>make_env</span><span>(</span><span>env_name</span><span>, </span><span>fire</span><span>=</span><span>True</span><span>, </span><span>frames_num</span><span>=</span><span>2</span><span>, </span><span>noop_num</span><span>=</span><span>30</span><span>, </span><span>skip_frames</span><span>=</span><span>True</span><span>):<br/></span><span>    env </span><span>=</span><span> gym.</span><span>make</span><span>(env_name)<br/></span><span>    if</span><span> skip_frames:<br/></span><span>        env </span><span>=</span><span> </span><span>MaxAndSkipEnv</span><span>(env) </span><span># Return only every `skip`-th frame<br/></span><span>    if</span><span> fire:<br/></span><span>        env </span><span>=</span><span> </span><span>FireResetEnv</span><span>(env) </span><span># Fire at the beginning<br/></span><span>    env </span><span>=</span><span> </span><span>NoopResetEnv</span><span>(env, </span><span>noop_max</span><span>=</span><span>noop_num)<br/></span><span>    env </span><span>=</span><span> </span><span>WarpFrame</span><span>(env) </span><span># Reshape image<br/></span><span>    env </span><span>=</span><span> </span><span>FrameStack</span><span>(env, frames_num) </span><span># Stack last 4 frames<br/></span><span>    return</span><span> env</span></pre></div>
<p>The only preprocessing step that is missing is the scaling of the frame. We'll take care of scaling immediately before giving the observation frame as input to the neural network. This is because <kbd>FrameStack</kbd> uses a particular memory-efficient array called a lazy array, which is lost whenever scaling is applied as a wrapper.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DQN implementation</h1>
                </header>
            
            <article>
                
<p>Though DQN is a pretty simple algorithm, it requires particular attention when it comes to its implementation and design choices. This algorithm, like every other deep RL algorithm, is not easy to debug and tune. Therefore, throughout this book, we'll give you some techniques and suggestions for how to do this. </p>
<p><span>The DQN code contains four main components:</span></p>
<ul>
<li>DNNs</li>
<li>An experienced buffer</li>
<li>A computational graph</li>
<li>A training (and evaluation) loop</li>
</ul>
<p>The code, as usual, is written in Python and TensorFlow, and we'll use TensorBoard to visualize the training and the performance of the algorithm. </p>
<div class="packt_infobox">All the code is available in this book's GitHub repository. Make sure to check it out there. W<span>e don't provide the implementation of some simpler functions here to avoid weighing down the code.</span></div>
<p>Let's immediately jump into the implementation by importing the required libraries:</p>
<div>
<pre><span>import</span><span> numpy </span><span>as</span><span> np<br/></span><span>import</span><span> tensorflow </span><span>as</span><span> tf<br/></span><span>import</span><span> gym<br/></span><span>from</span><span> datetime </span><span>import</span><span> datetime<br/></span><span>from</span><span> collections </span><span>import</span><span> deque<br/></span><span>import</span><span> time<br/></span><span>import</span><span> sys<br/><br/>from atari_wrappers import make_env</span></pre></div>
<p><kbd>atari_wrappers</kbd> includes the <kbd>make_env</kbd> function we defined previously.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DNNs</h1>
                </header>
            
            <article>
                
<p>The DNN architecture is as follows (the components are built in sequential order):</p>
<ol>
<li>A convolution of 16 filters of <span>dimension </span>8 x 8 with 4 strides and rectifier nonlinearity.</li>
<li>A convolution of 32 filters of dimension 4 x 4 with 2 strides and <span>rectifier nonlinearity.</span></li>
<li>A convolution of 32 filters of dimension 3 x 3 with 1 strides and <span>rectifier nonlinearity.</span></li>
<li>A dense layer of 128 units and ReLU activation.</li>
<li>A dense layer with a number of units equal to the actions that are allowed in the environment and a linear activation.</li>
</ol>
<p class="mce-root">In <kbd>cnn</kbd>, we define the first three convolutional layers, while in <kbd>fnn</kbd>, we define the last two dense layers:</p>
<div>
<pre><span>def</span><span> </span><span>cnn</span><span>(</span><span>x</span><span>):<br/></span><span>    x </span><span>=</span><span> tf.layers.</span><span>conv2d</span><span>(x, </span><span>filters</span><span>=</span><span>16</span><span>, </span><span>kernel_size</span><span>=</span><span>8</span><span>, </span><span>strides</span><span>=</span><span>4</span><span>, </span><span>padding</span><span>=</span><span>'</span><span>valid</span><span>'</span><span>, </span><span>activation</span><span>=</span><span>'</span><span>relu</span><span>'</span><span>) </span><span><br/></span><span>    x </span><span>=</span><span> tf.layers.</span><span>conv2d</span><span>(x, </span><span>filters</span><span>=</span><span>32</span><span>, </span><span>kernel_size</span><span>=</span><span>4</span><span>, </span><span>strides</span><span>=</span><span>2</span><span>, </span><span>padding</span><span>=</span><span>'</span><span>valid</span><span>'</span><span>, </span><span>activation</span><span>=</span><span>'</span><span>relu</span><span>'</span><span>) </span><span><br/></span><span>    return</span><span> tf.layers.</span><span>conv2d</span><span>(x, </span><span>filters</span><span>=</span><span>32</span><span>, </span><span>kernel_size</span><span>=</span><span>3</span><span>, </span><span>strides</span><span>=</span><span>1</span><span>, </span><span>padding</span><span>=</span><span>'</span><span>valid</span><span>'</span><span>, </span><span>activation</span><span>=</span><span>'</span><span>relu</span><span>'</span><span>) </span><span><br/><br/></span><span>def</span><span> </span><span>fnn</span><span>(</span><span>x</span><span>, </span><span>hidden_layers</span><span>, </span><span>output_layer</span><span>, </span><span>activation</span><span>=</span><span>tf.nn.relu, </span><span>last_activation</span><span>=</span><span>None</span><span>):<br/></span><span>    for</span><span> l </span><span>in</span><span> hidden_layers:<br/></span><span>        x </span><span>=</span><span> tf.layers.</span><span>dense</span><span>(x, </span><span>units</span><span>=</span><span>l, </span><span>activation</span><span>=</span><span>activation)<br/></span><span>    return</span><span> tf.layers.</span><span>dense</span><span>(x, </span><span>units</span><span>=</span><span>output_layer, </span><span>activation</span><span>=</span><span>last_activation)<br/></span></pre></div>
<p>In the preceding code, <kbd>hidden_layers</kbd> is a list of integer values. In our implementation, this is <kbd>hidden_layers=[128]</kbd>. On the other hand, <kbd>output_layer</kbd> is the number of agent actions.</p>
<p>In <kbd>qnet</kbd>,the CNN and FNN layers are connected with a layer that flattens the 2D output of the CNN:</p>
<div>
<pre><span>def</span><span> </span><span>qnet</span><span>(</span><span>x</span><span>, </span><span>hidden_layers</span><span>, </span><span>output_size</span><span>, </span><span>fnn_activation</span><span>=</span><span>tf.nn.relu, </span><span>last_activation</span><span>=</span><span>None</span><span>):<br/></span><span>    x </span><span>=</span><span> </span><span>cnn</span><span>(x)<br/></span><span>    x </span><span>=</span><span> tf.layers.</span><span>flatten</span><span>(x)<br/></span><span>    return</span><span> </span><span>fnn</span><span>(x, hidden_layers, output_size, fnn_activation, last_activation)</span></pre></div>
<p>The deep neural network is now fully defined. All we need to do is connect it to the main computational graph.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The experienced buffer</h1>
                </header>
            
            <article>
                
<p>The experienced buffer is a class of the <span><kbd>ExperienceBuffer</kbd> type and stores a</span> queue of type <strong>FIFO</strong> (<strong>First In</strong>, <strong>First Out</strong>) for each of the following components: observation, reward, action, next observation, and done. FIFO means that once it reaches the maximum capacity specified by <kbd>maxlen</kbd>, it discards the elements starting from the oldest one. In our implementation, the capacity is <kbd>buffer_size</kbd>:</p>
<div>
<pre><span>class</span><span> </span><span>ExperienceBuffer</span><span>():<br/><br/></span><span>    def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>, </span><span>buffer_size</span><span>):<br/></span><span>        self</span><span>.obs_buf </span><span>=</span><span> </span><span>deque</span><span>(</span><span>maxlen</span><span>=</span><span>buffer_size)<br/></span><span>        self</span><span>.rew_buf </span><span>=</span><span> </span><span>deque</span><span>(</span><span>maxlen</span><span>=</span><span>buffer_size)<br/></span><span>        self</span><span>.act_buf </span><span>=</span><span> </span><span>deque</span><span>(</span><span>maxlen</span><span>=</span><span>buffer_size)<br/></span><span>        self</span><span>.obs2_buf </span><span>=</span><span> </span><span>deque</span><span>(</span><span>maxlen</span><span>=</span><span>buffer_size)<br/></span><span>        self</span><span>.done_buf </span><span>=</span><span> </span><span>deque</span><span>(</span><span>maxlen</span><span>=</span><span>buffer_size)<br/><br/></span><span>    def</span><span> </span><span>add</span><span>(</span><span>self</span><span>, </span><span>obs</span><span>, </span><span>rew</span><span>, </span><span>act</span><span>, </span><span>obs2</span><span>, </span><span>done</span><span>):<br/></span><span>        self</span><span>.obs_buf.</span><span>append</span><span>(obs)<br/></span><span>        self</span><span>.rew_buf.</span><span>append</span><span>(rew)<br/></span><span>        self</span><span>.act_buf.</span><span>append</span><span>(act)<br/></span><span>        self</span><span>.obs2_buf.</span><span>append</span><span>(obs2)<br/></span><span>        self</span><span>.done_buf.</span><span>append</span><span>(done)</span></pre></div>
<p>The <kbd>ExperienceBuffer</kbd> class also manages the sampling of mini-batches, which are used to train the neural network. These are uniformly sampled from the buffer and have a predefined <kbd>batch_size</kbd> size:</p>
<div>
<pre>    def<span> </span><span>sample_minibatch</span><span>(</span><span>self</span><span>, </span><span>batch_size</span><span>):<br/></span>        mb_indices <span>=</span><span> np.random.</span><span>randint</span><span>(</span><span>len</span><span>(</span><span>self</span><span>.obs_buf), </span><span>size</span><span>=</span><span>batch_size)<br/><br/></span>        mb_obs <span>=</span><span> </span><span>scale_frames</span><span>([</span><span>self</span><span>.obs_buf[i] </span><span>for</span><span> i </span><span>in</span><span> mb_indices])<br/></span>        mb_rew <span>=</span><span> [</span><span>self</span><span>.rew_buf[i] </span><span>for</span><span> i </span><span>in</span><span> mb_indices]<br/></span>        mb_act <span>=</span><span> [</span><span>self</span><span>.act_buf[i] </span><span>for</span><span> i </span><span>in</span><span> mb_indices]<br/></span>        mb_obs2 <span>=</span><span> </span><span>scale_frames</span><span>([</span><span>self</span><span>.obs2_buf[i] </span><span>for</span><span> i </span><span>in</span><span> mb_indices])<br/></span>        mb_done <span>=</span><span> [</span><span>self</span><span>.done_buf[i] </span><span>for</span><span> i </span><span>in</span><span> mb_indices]<br/><br/></span>        return<span> mb_obs, mb_rew, mb_act, mb_obs2, mb_done</span></pre></div>
<p class="mce-root">Lastly, we override the <kbd>_len</kbd> method to provide the length of the buffers. Note that because every buffer is the same size as the others, we only return the length of <kbd>self.obs_buf</kbd>:</p>
<div>
<div>
<pre><span>    def</span><span> </span><span>__len__</span><span>(</span><span>self</span><span>):<br/></span><span>        return</span><span> </span><span>len</span><span>(</span><span>self</span><span>.obs_buf)</span></pre></div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The computational graph and training loop</h1>
                </header>
            
            <article>
                
<p>The core of the algorithm, namely the computational graph and the training (and evaluation) loop, is implemented in the <kbd>DQN</kbd> function, which takes the name of the environment and all the other hyperparameters as arguments:</p>
<div>
<pre><span>def</span><span> </span><span>DQN</span><span>(</span><span>env_name</span><span>, </span><span>hidden_sizes</span><span>=</span><span>[</span><span>32</span><span>], </span><span>lr</span><span>=</span><span>1e-2</span><span>, </span><span>num_epochs</span><span>=</span><span>2000</span><span>, </span><span>buffer_size</span><span>=</span><span>100000</span><span>, </span><span>discount</span><span>=</span><span>0.99</span><span>, </span><span>update_target_net</span><span>=</span><span>1000</span><span>, </span><span>batch_size</span><span>=</span><span>64</span><span>, </span><span>update_freq</span><span>=</span><span>4</span><span>, </span><span>frames_num</span><span>=</span><span>2</span><span>, </span><span>min_buffer_size</span><span>=</span><span>5000</span><span>, </span><span>test_frequency</span><span>=</span><span>20</span><span>, </span><span>start_explor</span><span>=</span><span>1</span><span>, </span><span>end_explor</span><span>=</span><span>0.1</span><span>, </span><span>explor_steps</span><span>=</span><span>100000</span><span>):<br/><br/></span><span>    env </span><span>=</span><span> </span><span>make_env</span><span>(env_name, </span><span>frames_num</span><span>=</span><span>frames_num, </span><span>skip_frames</span><span>=</span><span>True</span><span>, </span><span>noop_num</span><span>=</span><span>20</span><span>)<br/></span><span>    env_test </span><span>=</span><span> </span><span>make_env</span><span>(env_name, </span><span>frames_num</span><span>=</span><span>frames_num, </span><span>skip_frames</span><span>=</span><span>True</span><span>, </span><span>noop_num</span><span>=</span><span>20</span><span>)<br/></span><span>    env_test </span><span>=</span><span> gym.wrappers.</span><span>Monitor</span><span>(env_test, </span><span>"</span><span>VIDEOS/TEST_VIDEOS</span><span>"</span><span>+</span><span>env_name</span><span>+</span><span>str</span><span>(</span><span>current_milli_time</span><span>()),</span><span>force</span><span>=</span><span>True</span><span>, </span><span>video_callable</span><span>=lambda</span><span> </span><span>x</span><span>: x</span><span>%</span><span>20</span><span>==</span><span>0</span><span>)<br/><br/></span><span>    obs_dim </span><span>=</span><span> env.observation_space.shape<br/></span><span>    act_dim </span><span>=</span><span> env.action_space.n </span></pre></div>
<p>In the first few lines of the preceding code, two environments are created: one for training and one for testing. Moreover, <kbd>gym.wrappers.Monitor</kbd> is a Gym wrapper that saves the games of an environment in video format, while <kbd>video_callable</kbd> is a function parameter that establishes how often the videos are saved, which in this case is every 20 episodes.</p>
<p>Then, we can reset the TensorFlow graph and create placeholders for the observations, the actions, and the target values. This is done with the following lines of code:</p>
<div>
<pre><span>    tf.</span><span>reset_default_graph</span><span>()<br/></span><span>    obs_ph </span><span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>, obs_dim[</span><span>0</span><span>], obs_dim[</span><span>1</span><span>], obs_dim[</span><span>2</span><span>]), </span><span>dtype</span><span>=</span><span>tf.float32, </span><span>name</span><span>=</span><span>'</span><span>obs</span><span>'</span><span>)<br/></span><span>    act_ph </span><span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>,), </span><span>dtype</span><span>=</span><span>tf.int32, </span><span>name</span><span>=</span><span>'</span><span>act</span><span>'</span><span>)<br/></span><span>    y_ph </span><span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>,), </span><span>dtype</span><span>=</span><span>tf.float32, </span><span>name</span><span>=</span><span>'</span><span>y</span><span>'</span><span>)</span></pre></div>
<p>Now, we can create a target and an online network by calling the <kbd>qnet</kbd> function that we defined previously. Because the target network has to update itself sometimes and take the parameters of the online network, we create an operation called <kbd>update_target_op</kbd>, which assigns every variable of the online network to the target network. This assignment is done by <span>the TensorFlow </span><kbd>assign</kbd><span> method.</span> <kbd>tf.group</kbd>, on the other hand, aggregates every element of the <kbd>update_target</kbd> list as a single operation. The implementation is as follows:</p>
<div>
<pre><span>    with</span><span> tf.</span><span>variable_scope</span><span>(</span><span>'</span><span>target_network</span><span>'</span><span>):<br/></span><span>        target_qv </span><span>=</span><span> </span><span>qnet</span><span>(obs_ph, hidden_sizes, act_dim)<br/></span><span>    target_vars </span><span>=</span><span> tf.</span><span>trainable_variables</span><span>()<br/><br/></span><span>    with</span><span> tf.</span><span>variable_scope</span><span>(</span><span>'</span><span>online_network</span><span>'</span><span>):<br/></span><span>        online_qv </span><span>=</span><span> </span><span>qnet</span><span>(obs_ph, hidden_sizes, act_dim)<br/></span><span>    train_vars </span><span>=</span><span> tf.</span><span>trainable_variables</span><span>()<br/><br/></span><span>    update_target </span><span>=</span><span> [train_vars[i].</span><span>assign</span><span>(train_vars[i</span><span>+</span><span>len</span><span>(target_vars)]) </span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(</span><span>len</span><span>(train_vars) </span><span>-</span><span> </span><span>len</span><span>(target_vars))]<br/></span><span>    update_target_op </span><span>=</span><span> tf.</span><span>group</span><span>(</span><span>*</span><span>update_target)</span></pre></div>
<p>Now that we have defined the placeholder that's created the deep neural network and defined the target update operation, all that remains is to define the loss function. The loss function is <img class="fm-editor-equation" src="assets/a831e7fc-45db-4469-a046-032ff17c8b3d.png" style="width:8.50em;height:1.42em;"/> (or, equivalently, (5.5)). It requires the target values, <img class="fm-editor-equation" src="assets/88f8b378-b9c3-4831-a529-36fd4cdca94e.png" style="width:1.08em;height:1.25em;"/>, <br/>
computed as they are in formula (5.6), which are passed through the <kbd>y_ph</kbd> placeholder and the Q-values of the online network, <img class="fm-editor-equation" src="assets/4c3d2453-13e9-4a90-a0f6-4f70c3870bce.png" style="width:5.08em;height:1.42em;"/>. A Q-value is dependent on the action, <img class="fm-editor-equation" src="assets/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png" style="width:1.00em;height:0.92em;"/>, but since the online network outputs a value for each action, we have to find a way to retrieve only the Q-value of <img class="fm-editor-equation" src="assets/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png" style="width:1.25em;height:1.08em;"/> while discarding the other action-values. This operation can be achieved <span>by</span> using a one-hot encoding of the action, <img class="fm-editor-equation" src="assets/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png" style="width:1.25em;height:1.08em;"/>, and then multiplying it by the output of the online network. For example, if there are five possible actions and <img class="fm-editor-equation" src="assets/2fd1422c-a331-4a0b-8e78-ea40cba7152e.png" style="width:2.75em;height:1.08em;"/>, then the one-hot encoding will be <img class="fm-editor-equation" src="assets/45cdd9fa-4046-428e-beb0-144c41ed74eb.png" style="width:4.92em;height:1.25em;"/>. Then, supposing that the network outputs <img class="fm-editor-equation" src="assets/5fa70caf-4f92-4cdc-9301-28ecd0a56d0e.png" style="width:7.75em;height:1.25em;"/>, the results of the multiplication with the one-hot encoding will be <img class="fm-editor-equation" src="assets/3e063c4c-06eb-47aa-9f2a-a80a98004442.png" style="width:5.67em;height:1.25em;"/>. After, the q-value is obtained by summing this vector. The result will be <img class="fm-editor-equation" src="assets/8578bb25-bfaf-444e-b319-116517194c6c.png" style="width:1.83em;height:1.25em;"/>. All of this is done in the following three lines of code:</p>
<div>
<pre><span>    act_onehot </span><span>=</span><span> tf.</span><span>one_hot</span><span>(act_ph, </span><span>depth</span><span>=</span><span>act_dim)<br/></span><span>    q_values </span><span>=</span><span> tf.</span><span>reduce_sum</span><span>(act_onehot </span><span>*</span><span> online_qv, </span><span>axis</span><span>=</span><span>1</span><span>)<br/></span><span>    v_loss </span><span>=</span><span> tf.</span><span>reduce_mean</span><span>((y_ph </span><span>-</span><span> q_values)</span><span>**</span><span>2</span><span>)</span></pre></div>
<p><span><span>T</span></span>o minimize the loss function we just defined, we will use Adam, a variant of SGD:</p>
<div>
<pre><span>    v_opt </span><span>=</span><span> tf.train.</span><span>AdamOptimizer</span><span>(lr).</span><span>minimize</span><span>(v_loss)</span></pre></div>
<p class="mce-root"/>
<p>This concludes the creation of the computation graph. Before going through the main DQN cycle, we have to prepare everything so that we can save the scalars and the histograms. By doing this, we will be able to visualize them later in TensorBoard:</p>
<div>
<pre><span>    now </span><span>=</span><span> datetime.</span><span>now</span><span>()<br/></span><span>    clock_time </span><span>=</span><span> </span><span>"</span><span>{}</span><span>_</span><span>{}</span><span>.</span><span>{}</span><span>.</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(now.day, now.hour, now.minute, </span><span>int</span><span>(now.second))<br/><br/></span><span>    mr_v </span><span>=</span><span> tf.</span><span>Variable</span><span>(</span><span>0.0</span><span>)<br/></span><span>    ml_v </span><span>=</span><span> tf.</span><span>Variable</span><span>(</span><span>0.0</span><span>)<br/><br/></span><span>    tf.summary.</span><span>scalar</span><span>(</span><span>'</span><span>v_loss</span><span>'</span><span>, v_loss)<br/></span><span>    tf.summary.</span><span>scalar</span><span>(</span><span>'</span><span>Q-value</span><span>'</span><span>, tf.</span><span>reduce_mean</span><span>(q_values))<br/></span><span>    tf.summary.</span><span>histogram</span><span>(</span><span>'</span><span>Q-values</span><span>'</span><span>, q_values)<br/><br/></span><span>    scalar_summary </span><span>=</span><span> tf.summary.</span><span>merge_all</span><span>()<br/></span><span>    reward_summary </span><span>=</span><span> tf.summary.</span><span>scalar</span><span>(</span><span>'</span><span>test_rew</span><span>'</span><span>, mr_v)<br/></span><span>    mean_loss_summary </span><span>=</span><span> tf.summary.</span><span>scalar</span><span>(</span><span>'</span><span>mean_loss</span><span>'</span><span>, ml_v)<br/><br/></span><span>    hyp_str </span><span>=</span><span> </span><span>"</span><span>-lr_</span><span>{}</span><span>-upTN_</span><span>{}</span><span>-upF_</span><span>{}</span><span>-frms_</span><span>{}</span><span>"</span><span>.</span><span>format</span><span>(lr, update_target_net, update_freq, frames_num)<br/></span><span>    file_writer </span><span>=</span><span> tf.summary.</span><span>FileWriter</span><span>(</span><span>'</span><span>log_dir/</span><span>'</span><span>+</span><span>env_name</span><span>+</span><span>'</span><span>/DQN_</span><span>'</span><span>+</span><span>clock_time</span><span>+</span><span>'</span><span>_</span><span>'</span><span>+</span><span>hyp_str, tf.</span><span>get_default_graph</span><span>())</span></pre></div>
<p>Everything is quite self-explanatory. The only things that you may question are the <kbd>mr_v</kbd> and <kbd>ml_v</kbd> variables. These are variables we want to track with TensorBoard. However, because they aren't defined internally by the computation graph, we have to declare them separately and assign them in <kbd>session.run</kbd> later. <kbd>FileWriter</kbd> is created with a unique name and associated with the default graph.</p>
<p>We can now define the <kbd>agent_op</kbd> function that computes the forward pass on a scaled observation. The observation has already passed through the preprocessing pipeline (built in the environment with the wrappers), but we left the scaling aside:</p>
<div>
<pre><span>    def</span><span> </span><span>agent_op</span><span>(</span><span>o</span><span>):<br/></span><span>        o </span><span>=</span><span> </span><span>scale_frames</span><span>(o)<br/></span><span>        return</span><span> sess.</span><span>run</span><span>(online_qv, </span><span>feed_dict</span><span>=</span><span>{obs_ph:[o]})<br/></span></pre></div>
<p>Then, the session is created, the variables are initialized, and the environment is reset:</p>
<div>
<pre><span>    sess </span><span>=</span><span> tf.</span><span>Session</span><span>()<br/></span><span>    sess.</span><span>run</span><span>(tf.</span><span>global_variables_initializer</span><span>())<br/><br/></span><span>    step_count </span><span>=</span><span> </span><span>0<br/></span><span>    last_update_loss </span><span>=</span><span> []<br/></span><span>    ep_time </span><span>=</span><span> </span><span>current_milli_time</span><span>()<br/></span><span>    batch_rew </span><span>=</span><span> []<br/><br/></span><span>    obs </span><span>=</span><span> env.</span><span>reset</span><span>()</span></pre></div>
<p>The next move involves instantiating the replay buffer, updating the target network so that it has the same parameters as the online network, and initializing the decay rate with <kbd>eps_decay</kbd>. The policy for the epsilon decay is the same as the one that was adopted in the DQN paper. A decay rate h<span>as been chosen</span> so that, when it's applied linearly to the <kbd>eps</kbd> variable, it reaches a terminal value, <kbd>end_explor</kbd>, in about <kbd>explor_steps</kbd> steps. For example, if you want to decrease from 1.0 to 0.1 in 1,000 steps, you have to decrement the variable by a value equal to <img class="fm-editor-equation" src="assets/951d4743-6522-4fda-a081-4e6b31ab9563.png" style="width:11.92em;height:1.42em;"/> on each step. All of this is accomplished in the following lines of code:</p>
<div>
<pre><span>    obs </span><span>=</span><span> env.</span><span>reset</span><span>()<br/><br/></span><span>    buffer </span><span>=</span><span> </span><span>ExperienceBuffer</span><span>(buffer_size)<br/><br/></span><span>    sess.</span><span>run</span><span>(update_target_op)<br/><br/></span><span>    eps </span><span>=</span><span> start_explor<br/></span><span>    eps_decay </span><span>=</span><span> (start_explor </span><span>-</span><span> end_explor) </span><span>/</span><span> explor_steps</span></pre></div>
<p>As you may recall, the training loop comprises two inner cycles: the first iterates across the epochs while the other iterates across each transition of the epoch. The first part of the innermost cycle is quite standard. It selects an action following an <img class="fm-editor-equation" src="assets/d28bbae0-9b7d-46ba-b063-afde075d2465.png" style="width:0.67em;height:0.92em;"/>-greedy behavior policy that uses the online network, takes a step in the environment, adds the new transition to the buffer, and finally, updates the variables:</p>
<div>
<pre><span>    for</span><span> ep </span><span>in</span><span> </span><span>range</span><span>(num_epochs):<br/></span><span>        g_rew </span><span>=</span><span> </span><span>0<br/></span><span>        done </span><span>=</span><span> </span><span>False<br/><br/></span><span>        while</span><span> </span><span>not</span><span> done:<br/></span><span>            act </span><span>=</span><span> </span><span>eps_greedy</span><span>(np.</span><span>squeeze</span><span>(</span><span>agent_op</span><span>(obs)), </span><span>eps</span><span>=</span><span>eps)<br/></span><span>            obs2, rew, done, _ </span><span>=</span><span> env.</span><span>step</span><span>(act)<br/></span><span>            buffer.</span><span>add</span><span>(obs, rew, act, obs2, done)<br/><br/></span><span>            obs </span><span>=</span><span> obs2<br/></span><span>            g_rew </span><span>+=</span><span> rew<br/></span><span>            step_count </span><span>+=</span><span> </span><span>1</span></pre></div>
<p>In the preceding code, <kbd>obs</kbd> takes the value of the next observation and the cumulative game reward is incremented.</p>
<p class="mce-root"/>
<p>Then, in the same cycle, <kbd>eps</kbd> is decayed and if some of the conditions are met, it trains the online network. These conditions make sure that the buffer has reached a minimal size and that the neural network is trained only once every <kbd>update_freq</kbd> steps. To train the online network, first, a minibatch is sampled from the buffer and the target values are calculated. Then, the session is run to minimize the loss function, <kbd>v_loss</kbd>, which feeds the dictionary with the target values, the actions, and the observations of the minibatch. While the session is running, it also returns <kbd>v_loss</kbd> and <kbd>scalar_summary</kbd> for statistics purposes. <kbd>scalar_summary</kbd> is then added to <kbd>file_writer</kbd> to be saved in the TensorBoard logging file. Finally, every <kbd>update_target_net</kbd> epochs, the target network is updated. A summary with the mean losses is also run and added to the <span>TensorBoard</span> logging file. All of this is done by the following snippet of code:</p>
<div>
<pre><span>            if</span><span> eps </span><span>&gt;</span><span> end_explor:<br/></span><span>                eps </span><span>-=</span><span> eps_decay<br/><br/></span><span>            if</span><span> </span><span>len</span><span>(buffer) </span><span>&gt;</span><span> min_buffer_size </span><span>and</span><span> (step_count </span><span>%</span><span> update_freq </span><span>==</span><span> </span><span>0</span><span>):<br/></span><span>                mb_obs, mb_rew, mb_act, mb_obs2, mb_done </span><span>=</span><span> buffer.</span><span>sample_minibatch</span><span>(batch_size)<br/></span><span>                mb_trg_qv </span><span>=</span><span> sess.</span><span>run</span><span>(target_qv, </span><span>feed_dict</span><span>=</span><span>{obs_ph:mb_obs2})<br/></span><span>                y_r </span><span>=</span><span> </span><span>q_target_values</span><span>(mb_rew, mb_done, mb_trg_qv, discount) # Compute the target values<br/></span><span>                train_summary, train_loss, _ </span><span>=</span><span> sess.</span><span>run</span><span>([scalar_summary, v_loss, v_opt], </span><span>feed_dict</span><span>=</span><span>{obs_ph:mb_obs, y_ph:y_r, act_ph: mb_act})<br/><br/></span><span>                file_writer.</span><span>add_summary</span><span>(train_summary, step_count)<br/></span><span>                last_update_loss.</span><span>append</span><span>(train_loss)<br/><br/></span><span>            if</span><span> (len(buffer) &gt; min_buffer_size) and (step_count </span><span>%</span><span> update_target_net) </span><span>==</span><span> </span><span>0</span><span>:<br/></span><span>                _, train_summary </span><span>=</span><span> sess.</span><span>run</span><span>([update_target_op, mean_loss_summary], </span><span>feed_dict</span><span>=</span><span>{ml_v:np.</span><span>mean</span><span>(last_update_loss)})<br/></span><span>                file_writer.</span><span>add_summary</span><span>(train_summary, step_count)<br/></span><span>                last_update_loss </span><span>=</span><span> []</span></pre></div>
<p>When an epoch terminates, the environment is reset, the total reward of the game is appended to <kbd>batch_rew</kbd>, and the latter is set to zero. Moreover, every <kbd>test_frequency</kbd> epochs, the agent is tested for 10 games, and the statistics are added to <kbd>file_writer</kbd>. At the end of the training, the environments and the writer are closed. The code is as follows:</p>
<div>
<pre><span>            if</span><span> done:<br/></span><span>                obs </span><span>=</span><span> env.</span><span>reset</span><span>()<br/></span><span>                batch_rew.</span><span>append</span><span>(g_rew)<br/></span><span>                g_rew </span><span>=</span><span> </span><span>0<br/></span><span>        if</span><span> ep </span><span>%</span><span> test_frequency </span><span>==</span><span> </span><span>0</span><span>:<br/></span><span>            test_rw </span><span>=</span><span> </span><span>test_agent</span><span>(env_test, agent_op, </span><span>num_games</span><span>=</span><span>10</span><span>)<br/></span><span>            test_summary </span><span>=</span><span> sess.</span><span>run</span><span>(reward_summary, </span><span>feed_dict</span><span>=</span><span>{mr_v: np.</span><span>mean</span><span>(test_rw)})<br/></span><span>            file_writer.</span><span>add_summary</span><span>(test_summary, step_count)<br/></span><span>            print</span><span>(</span><span>'</span><span>Ep:</span><span>%4d</span><span> Rew:</span><span>%4.2f</span><span>, Eps:</span><span>%2.2f</span><span> -- Step:</span><span>%5d</span><span> -- Test:</span><span>%4.2f</span><span> </span><span>%4.2f</span><span>'</span><span> </span><span>%</span><span> (ep,np.</span><span>mean</span><span>(batch_rew), eps, step_count, np.</span><span>mean</span><span>(test_rw), np.</span><span>std</span><span>(test_rw))<br/></span><span>            batch_rew</span><span> </span><span>=</span><span> []<br/></span>    file_writer.<span>close</span><span>()<br/>    </span>env.<span>close</span><span>()<br/>    env_test.close()</span></pre></div>
<p>That's it. We can now call the <kbd>DQN</kbd> function with the name of the Gym environment and all the hyperparameters:</p>
<div>
<pre><span>if __name__ == '__main__':<br/>    DQN</span><span>(</span><span>'</span><span>PongNoFrameskip-v4</span><span>'</span><span>, </span><span>hidden_sizes</span><span>=</span><span>[</span><span>128</span><span>], </span><span>lr</span><span>=</span><span>2e-4</span><span>, </span><span>buffer_size</span><span>=</span><span>100000</span><span>, </span><span>update_target_net</span><span>=</span><span>1000</span><span>, </span><span>batch_size</span><span>=</span><span>32</span><span>, </span><span>update_freq</span><span>=</span><span>2</span><span>, </span><span>frames_num</span><span>=</span><span>2</span><span>, </span><span>min_buffer_size</span><span>=</span><span>10000</span><span>)<br/></span></pre></div>
<p>There's one last note before reporting the results. The environment that's being used here isn't the default version of <kbd>Pong-v0</kbd> but a modified version of it. The reason for this is that in the regular version, each action is performed 2, 3, or 4 times where this number is sampled uniformly. But because we want to skip a fixed number of times, we opted for the version without the built-in skip feature, <kbd>NoFrameskip</kbd>, and added the custom <span><kbd>MaxAndSkipEnv</kbd> wrapper.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article>
                
<p>Evaluating the progress of an RL algorithm is very challenging. The most obvious way to do this is to keep track of its end goal; that is, monitoring the total reward that's accumulated during the epochs. This is a good metric. However, training the average reward can be very noisy due to changes in the weights. This leads to large changes in the distribution of the state that's being visited. </p>
<p>For these reasons, we evaluated the algorithm on 10 test games every 20 training epochs and kept track of the average of the total (non-discounted) reward that was accumulated throughout the games. Moreover, because of the determinism of the environment, we tested the agent on an <img class="fm-editor-equation" src="assets/3e83ab04-37b2-43ca-961d-e824a111b62b.png" style="width:0.67em;height:0.92em;"/>-greedy policy (with <img class="fm-editor-equation" src="assets/ddd88359-b1c5-411d-a2a1-3141eceef4d2.png" style="width:4.42em;height:1.08em;"/>) so that we have a more robust evaluation. The scalar summary is called <kbd>test_rew</kbd>. You can see it in TensorBoard if you access the directory where the logs have been saved, and execute the following command:</p>
<pre><strong>tensorboard --logdir</strong> .</pre>
<p>The plot, which should be similar to yours (if you run the DQN code), is shown in<span> the following diagram.</span> <span>The x axis represents the number of steps. </span><span>You can see that it reaches a steady score of </span><img style="font-size: 1em;width:2.25em;height:1.08em;" class="fm-editor-equation" src="assets/18045cf6-a26c-47a2-86c0-b0b4a12d89be.png"/><span> after a linear increase in the first 250,000 steps and a more significant growth in the next 300,000 steps:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1830 image-border" src="assets/ca751dbb-bf2d-4ab7-b33d-844493dd0249.png" style="width:98.33em;height:54.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.4. A plot of the mean total reward across 10 games. The x axis represents the number of steps</div>
<p>Pong is a relatively simple task to complete. In fact, our algorithm has been trained on around 1.1 million steps, whereas in the DQN paper, all the algorithms were trained on 200 million steps. </p>
<p><span>An alternative way to evaluate the algorithm involves the estimated action-values. Indeed, the estimated action-values are a valuable metric because they measure the belief of the quality of the state-action pair. Unfortunately, this option is not optimal as some algorithms tend to overestimate the Q-values, as we will soon learn. Despite this, </span>we tracked it during training<span>. The plot is visible in the following diagram and, as we expected, the Q-value increases throughout the training in a similar way to the plot in the preceding diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1831 image-border" src="assets/ef3a10c0-7a08-4540-8edc-131c01d9ec65.png" style="width:99.33em;height:45.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.5. A plot of the estimated training Q-values. The x axis represents the number of steps</div>
<p>Another important plot, shown in the following diagram, shows the loss function through time. It's not as useful as in supervised learning as the target values aren't the ground truth, but it can always provide a good insight into the quality of the model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1832 image-border" src="assets/d2981b99-48bc-461e-b12f-4d3e28b04cd3.png" style="width:97.92em;height:54.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.6. A plot of the loss function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DQN variations</h1>
                </header>
            
            <article>
                
<p>Following the amazing results of DQN, many researchers have studied it and come up with integrations and changes to improve its stability, efficiency, and performance. In this section, we will present three of these improved algorithms, explain the idea and solution behind them, and provide their implementation. The first is Double DQN or DDQN, which deals with the over-estimation problem we mentioned in the DQN algorithm. The second is Dueling DQN, which decouples the Q-value function in a state value function and an action-state advantage value function. The third is n-step DQN, an old idea taken from TD algorithms, which spaces the step length between one-step learning and MC learning. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Double DQN</h1>
                </header>
            
            <article>
                
<p>The over-estimation of the Q-values in Q-learning algorithms is a well-known problem. The cause of this is the max operator, which over-estimates the actual maximum estimated values. To comprehend this problem, let's assume that we have noisy estimates with a mean of 0 but a variance different from 0, as shown in the following illustration. Despite the fact that, asymptotically, the average value is 0, the max function will always return values greater than 0:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1833 image-border" src="assets/68325277-2d84-44bc-b051-22073024f483.png" style="width:28.25em;height:22.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.7. Six values sampled from a normal distribution with a mean of 0</div>
<p>In Q-learning, this over-estimation is not a real problem until the higher values are uniformly distributed. If, however, the over-estimation is not uniform and the error differs from states and actions, this over-estimation negatively affects the DQN algorithm, which degrades the resulting policy.</p>
<p>To address this problem, in the paper <a href="https://arxiv.org/abs/1509.06461"><em>Deep Reinforcement Learning with Double Q-learning</em></a>, the authors suggest using two different estimators (that is, two neural networks): one for the action selection and one for the Q-values estimation. But instead of using two different neural networks and increasing the complexity, the paper proposes the use of the online network to choose the best action with the max operation, and the use of the target network to compute its Q-values. With this solution, the target value, <img class="fm-editor-equation" src="assets/e442e4a2-47fe-4157-ba8b-6712adf8ce79.png" style="width:0.58em;height:1.00em;"/>, will change from being as follows for standard Q-learning:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/adee62f0-f82e-408b-8ad3-7712a033f964.png" style="width:33.25em;height:1.75em;"/></p>
<p>Now, it's as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7c3cadf2-80f3-4ec7-817f-a2e26452e0ba.png" style="width:19.75em;height:1.83em;"/> (5.7)</p>
<p>This uncoupled version significantly reduces<span> </span>over-estimation problems and improves the stability of the algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DDQN implementation</h1>
                </header>
            
            <article>
                
<p>From an implementation perspective, the only change to make in order to implement DDQN is in the training phase. You just need to replace the following lines of code in the DDQN implementation itself: </p>
<div>
<pre><span>mb_trg_qv </span><span>=</span><span> sess.</span><span>run</span><span>(target_qv, </span><span>feed_dict</span><span>=</span><span>{obs_ph:mb_obs2})<br/></span><span>y_r </span><span>=</span><span> </span><span>q_target_values</span><span>(mb_rew, mb_done, mb_trg_qv, discount)</span></pre></div>
<div>Replace this with the following code:</div>
<div>
<div>
<pre><span>mb_onl_qv, mb_trg_qv </span><span>=</span><span> sess.</span><span>run</span><span>([online_qv,target_qv], </span><span>feed_dict</span><span>=</span><span>{obs_ph:mb_obs2})<br/></span>y_r <span>=</span><span> </span><span>double_q_target_values</span><span>(mb_rew, mb_done, mb_trg_qv, mb_onl_qv, discount)</span></pre></div>
<p>Here, <kbd>double_q_target_values</kbd> is a function that computes (5.7) for each transition of the minibatch.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article>
                
<p>To see if DQN actually overestimates the Q-values in respect to DDQN, we reported the Q-value plot in the following diagram. We also included the results of DQN (the orange line) so that we have a direct comparison between the two algorithms:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1835 image-border" src="assets/159e4130-41ea-412a-81a0-053dc4d830f7.png" style="width:97.58em;height:54.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.8. A plot<span> of the estimated training Q-values. The DDQN values are plotted in blue and the DQN values are plotted in orange.</span> <span>The x axis represents the number of steps</span></div>
<p> </p>
<p><span>The performance of both DDQN (the blue line) and DQN (the orange line), which are represented by the average reward of the test games, is as follows:</span></p>
<div class="mce-root packt_infobox"><span>For all the color references mentioned in the chapter, please refer to the color images bundle at </span><a href="http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf">http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf</a><span>.</span></div>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1836 image-border" src="assets/e28c62d0-1fad-4ade-82ac-871bcd133565.png" style="width:97.83em;height:54.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.9. A plot<span> of the mean </span><span>test </span><span>rewards. The DDQN values are plotted in blue and the DQN values are plotted in orange</span>. <span>The x axis represents the number of steps</span></div>
<p>As we expected, the Q-values are always smaller in DDQN than in DQN, meaning that the latter was actually over-estimating the values. Nonetheless, the performance on the test games doesn't seem to be impacted, meaning that those over-estimations were probably not hurting the performance of the algorithm. However, be aware that we only tested the algorithm on Pong. The effectiveness of an algorithm shouldn't be evaluated in a single environment. In fact, in the paper, the authors apply it to all 57 ALE games and reported that DDQN not only yields more accurate value estimates but leads to much higher scores on several games. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dueling DQN</h1>
                </header>
            
            <article>
                
<p>In the paper <em>Dueling Network Architectures for Deep Reinforcement Learning</em> (<a href="https://arxiv.org/abs/1511.06581">https://arxiv.org/abs/1511.06581</a>), a novel neural network architecture with two separate estimators was proposed: one for the state value function and the other for the state-action advantage value function.</p>
<p>The advantage function is used everywhere in RL and is defined as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/64a74303-d3e7-43c5-8947-1545ae6515f9.png" style="width:10.67em;height:1.25em;"/></p>
<p>The advantage function tells us the improvement of an action, <img class="fm-editor-equation" src="assets/6f4dc9fe-7775-44c2-977d-dadd3a3638d7.png" style="width:0.67em;height:0.75em;"/>, compared to the average action in a given state, <img class="fm-editor-equation" src="assets/4f1e190b-1fd1-4e81-a14d-2b65d067daf1.png" style="width:0.67em;height:0.92em;"/>. Thus, if <img class="fm-editor-equation" src="assets/f25da7b2-1b41-4b79-8c00-c07aeb8e8e68.png" style="width:2.92em;height:1.17em;"/> is a positive value, this means that the action, <img class="fm-editor-equation" src="assets/f6f612ff-260f-44df-bc3d-9f6a1f847c4c.png" style="width:0.75em;height:0.83em;"/>, is better then the average action in the state, <img class="fm-editor-equation" src="assets/3046c2b4-7e2a-4d4c-bf7f-2045d93b9a47.png" style="width:0.67em;height:0.83em;"/>. On the contrary, if <img class="fm-editor-equation" src="assets/3ab03d6c-2d75-41c1-a225-da18a7ccf1eb.png" style="width:2.83em;height:1.17em;"/> is a negative value, this means that <img class="fm-editor-equation" src="assets/b6cfea02-e054-4d3c-bdcf-86bced05dc87.png" style="width:0.83em;height:0.92em;"/> is worse than the average action in the state, <img class="fm-editor-equation" src="assets/dd4b1b99-d383-470b-86a8-6e807cf54980.png" style="width:0.67em;height:0.83em;"/>.</p>
<p>Thus, estimating the value function and the advantage function separately, as done in the paper, allows us to rebuild the Q-function, like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/666038cd-facd-41ff-9d71-c791f381e9fd.png" style="width:21.92em;height:3.25em;"/> (5.8)</p>
<p>Here, the mean of the advantage has been added to increase the stability of the DQN.</p>
<p>The architecture of Dueling DQN consists of two heads (or streams): one for the value function and one for the advantage function, all while sharing a common convolutional module. The authors reported that this architecture can learn which states are or are not valuable, without having to learn the absolute value of each action in a state. They tested this new architecture on the Atari games and obtained considerable improvements regarding their overall performance. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dueling DQN implementation</h1>
                </header>
            
            <article>
                
<p>One of the benefits of this architecture and of formula (5.8) is that it doesn't impose any changes on the underlying RL algorithm. The only changes are in the construction of the Q-network. Thus, we can replace <kbd>qnet</kbd> with the <kbd>dueling_qnet</kbd> function, which can be implemented as follows:</p>
<div>
<pre><span>def</span><span> </span><span>dueling_qnet</span><span>(</span><span>x</span><span>, </span><span>hidden_layers</span><span>, </span><span>output_size</span><span>, </span><span>fnn_activation</span><span>=</span><span>tf.nn.relu, </span><span>last_activation</span><span>=</span><span>None</span><span>):<br/></span><span>    x </span><span>=</span><span> </span><span>cnn</span><span>(x)<br/></span><span>    x </span><span>=</span><span> tf.layers.</span><span>flatten</span><span>(x)<br/></span><span>    qf </span><span>=</span><span> </span><span>fnn</span><span>(x, hidden_layers, </span><span>1</span><span>, fnn_activation, last_activation)<br/></span><span>    aaqf </span><span>=</span><span> </span><span>fnn</span><span>(x, hidden_layers, output_size, fnn_activation, last_activation)<br/></span><span>    return</span><span> qf </span><span>+</span><span> aaqf </span><span>-</span><span> tf.</span><span>reduce_mean</span><span>(aaqf)</span></pre></div>
<p>Two forward neural networks are created: one with only one output (for the value function) and one with as many outputs as the actions of the agent (for the state-dependent action advantage function). The last line returns formula (5.8).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article>
                
<p>The results of the test rewards, as shown in the following diagram, are promising, proving a clear benefit in the use of a dueling architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1837 image-border" src="assets/ecc126ee-5f2f-4add-8071-4079f554f481.png" style="width:97.75em;height:54.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 5.10. A plot</span><span> of the </span><span>test</span><span> rewards. The dueling DQN values are plotted in red and the DQN values are plotted in orange</span><span>. </span><span>The x axis represents the number of steps</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">N-step DQN</h1>
                </header>
            
            <article>
                
<p>The idea behind n-step DQN is old and comes from the shift between temporal difference learning and Monte Carlo learning. These algorithms, which were introduced in <a href="6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml">Chapter 4</a>, <em>Q-Learning and SARSA Applications</em>, are at the opposite extremes of a common spectrum. TD learning learns from a single step, while MC learns from the complete trajectory. TD learning exhibits a minimal variance but a maximal bias, where as MC exhibits high variance but a minimal bias. The variance-bias problem can be balanced using an n-step return. An n-step return is a return computed after <kbd>n</kbd> steps. TD learning can be viewed as a 0-step return while MC can be viewed as a <img class="fm-editor-equation" src="assets/d58022ac-8a1b-4abc-9928-056dba5f837b.png" style="width:1.58em;height:0.92em;"/>-step return.</p>
<p>With the n-step return, we can update the target value, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a3b14f9a-3c6d-470a-a97b-c39b5b2c7083.png" style="width:27.17em;height:4.67em;"/> (5.9)</p>
<p>Here, <img class="fm-editor-equation" src="assets/8d681a90-0f01-4b60-9ad8-7e758e2fe930.png" style="width:1.00em;height:0.92em;"/> is the number of steps.</p>
<p>An n-step return is like looking ahead <kbd>n</kbd> steps, but in practice, as it's impossible to actually look into the future, it's done in the opposite way, that is, by computing the <img class="fm-editor-equation" src="assets/1735a426-eb5c-412e-9704-af27fbe339ca.png" style="width:0.50em;height:0.92em;"/> value of n-steps ago. This leads to values that are only available at time <img class="fm-editor-equation" src="assets/e0bc4154-db21-474c-ad06-9e3669b80c3c.png" style="width:2.33em;height:0.92em;"/>, delaying the learning process.</p>
<p>The main advantage of this approach is that the target values are less biased and this can lead to faster learning. An important problem that arises is that the target values that are calculated in this way are correct, but only when the learning is on-policy (<span>DQN is off-policy)</span>. This is because formula (5.9) assumes that the policy that the agent will follow for the next n-steps is the same policy that collected the experience. There are some ways to adjust for the off-policy case, but they are generally complicated to implement and the best general practice is just to keep a small <kbd>n</kbd> and ignore the problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation</h1>
                </header>
            
            <article>
                
<p>To implement n-step DQN, only a few changes in the buffer are <span>required</span>. When sampling from the buffer, the n-step reward, the n-step next state, and the n-step done flag <span>have to be returned</span>. We will not provide the implementation here as it is quite simple but you can look at it in the code provided in this book's GitHub repository. The code to support n-step return is in the <span><kbd>MultiStepExperienceBuffer</kbd> class.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article>
                
<p>For off-policy algorithms (such as DQN), n-step learning works well with small values of <kbd>n</kbd>. In DQN, it has been shown that the algorithm works well with values of <kbd>n</kbd> between 2 and 4, leading to improvements in a wide range of Atari games. </p>
<p>In the following graph, the results of our implementation <span>are visible.</span> We tested DQN with a three-step return. From the results, we can see that <span>it requires more time before taking off. Afterward, it has a steeper learning curve but with an overall similar learning curve compared to DQN:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1838 image-border" src="assets/4826853c-0cde-4d5d-9c63-6c4edb58eeda.png" style="width:98.00em;height:54.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5.11. A plot<span> of the mean </span><span>test </span><span>total reward. The three-step DQN values are plotted in violet and the DQN values are plotted in orange</span><span>. </span><span>The x axis represents the number of steps</span></div>
<div class="packt_figref"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we went further into RL algorithms and talked about how these can be combined with function approximators so that RL can be applied to a broader variety of problems. Specifically, we described how function approximation and deep neural networks can be used in Q-learning and the instabilities that derive from it. We demonstrated that, in practice, deep neural networks cannot be combined with Q-learning without any modifications. </p>
<p>The first algorithm that was able to use deep neural networks in combination with Q-learning was DQN. It integrates two key ingredients to stabilize learning and control complex tasks such as Atari 2600 games. The two ingredients are the replay buffer, which is used to store the old experience, and a separate target network, which is updated less frequently than the online network. The former is employed to exploit the off-policy quality of Q-learning so that it can learn from the experiences of different policies (in this case, old policies) and to sample more i.i.d mini-batches from a larger pool of data to perform stochastic gradient descent. The latter is introduced to stabilize the target values and reduce the non-stationarity problem.</p>
<p>After this formal introduction to DQN, we implemented it and tested it on Pong, an Atari game. Moreover, we showed more practical aspects of the algorithm, such as the preprocessing pipeline and the wrappers. Following the publication of DQN, many other variations have been introduced to improve the algorithm and overcome its instabilities. We took a look at them and implemented three variations, namely Double DQN, Dueling DQN, and n-step DQN. Despite the fact that, in this chapter, we applied these algorithms exclusively to Atari games, they can be employed in many real-world problems. </p>
<p>In the next chapter, we'll introduce a different category of deep RL algorithms called policy gradient algorithms. These are on-policy and, as we'll soon see, they have some very important and unique characteristics that widen their applicability to a larger set of problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the cause of the <span>deadly triad problem?</span></li>
<li>How does DQN overcome instabilities?</li>
<li>What's the moving target problem?</li>
<li>How is the moving target problem mitigated in DQN?</li>
<li>What's the optimization procedure that's used in DQN?</li>
<li>What's the definition of a state-action advantage value function?</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>For a comprehensive tutorial regarding OpenAI Gym wrappers, read the following article: <a href="https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/">https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/</a>.</li>
<li>For the original <em>Rainbow</em> paper, go to <a href="https://arxiv.org/abs/1710.02298">https://arxiv.org/abs/1710.02298</a>.</li>
</ul>


            </article>

            
        </section>
    </body></html>