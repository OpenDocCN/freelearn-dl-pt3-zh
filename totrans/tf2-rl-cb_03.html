<html><head></head><body>
		<div id="_idContainer069">
			<h1 id="_idParaDest-78"><em class="italic"><a id="_idTextAnchor090"/>Chapter 3</em>: Implementing Advanced RL Algorithms</h1>
			<p>This chapter provides short and crisp recipes to implement advanced <strong class="bold">Reinforcement Learning</strong> (<strong class="bold">RL</strong>) algorithms and agents from scratch using <strong class="bold">TensorFlow 2.x</strong>. It includes recipes to build <strong class="bold">Deep-Q-Networks</strong> (<strong class="bold">DQN</strong>), <strong class="bold">Double and Dueling Deep Q-Networks</strong> (<strong class="bold">DDQN</strong>, <strong class="bold">DDDQN</strong>), <strong class="bold">Deep Recurrent Q-Networks</strong> (<strong class="bold">DRQN</strong>), <strong class="bold">Asynchronous Advantage Actor-Critic</strong> (<strong class="bold">A3C</strong>), <strong class="bold">Proximal Policy Optimization</strong> (<strong class="bold">PPO</strong>), and <strong class="bold">Deep Deterministic Policy Gradients</strong> (<strong class="bold">DDPG</strong>).</p>
			<p>The following recipes are discussed in this chapter:</p>
			<ul>
				<li>Implementing the Deep Q-Learning algorithm, DQN, and Double-DQN agent</li>
				<li>Implementing the Dueling DQN agent</li>
				<li>Implementing the Dueling Double DQN algorithm and DDDQN agent</li>
				<li>Implementing the Deep Recurrent Q-Learning algorithm and DRQN agent</li>
				<li>Implementing the Asynchronous Advantage Actor-Critic algorithm and A3C agent</li>
				<li>Implementing the Proximal Policy Optimization algorithm and PPO agent</li>
				<li>Implementing the Deep Deterministic Policy Gradient algorithm and DDPG agent</li>
			</ul>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor091"/>Technical requirements</h1>
			<p>The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04 and should work with later versions of Ubuntu if Python 3.6+ is available. With Python 3.6+ installed along with the necessary Python packages as listed before the start of each of the recipes, the code should run fine on Windows and Mac OS X too. It is advised to create and use a Python virtual environment named <strong class="source-inline">tf2rl-cookbook</strong> to install the packages and run the code in this book. Miniconda or Anaconda installation for Python virtual environment management is recommended.</p>
			<p>The complete code for each recipe in each chapter is available here: <a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook</a>.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor092"/>Implementing the Deep Q-Learning algorithm, DQN, and Double-DQN agent</h1>
			<p>DQN agent <a id="_idIndexMarker220"/>uses a deep neural network to learn the Q-value function. DQN has shown itself to be a powerful algorithm for discrete action-space environments and problems and is considered to be a notable milestone in the history of deep reinforcement learning when DQN mastered Atari Games.</p>
			<p>The Double-DQN agent<a id="_idIndexMarker221"/> uses two identical deep neural networks that are updated differently and so hold different weights. The second neural network is a copy of the main neural network from some time in the past (typically from the last episode).</p>
			<p>By the end of this recipe, you will have implemented a complete DQN and Double-DQN agent from scratch using TensorFlow 2.x that is ready to be trained in any discrete action-space RL environment. </p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor093"/>Getting ready</h2>
			<p>To complete<a id="_idIndexMarker222"/> this recipe, you will first need to activate<a id="_idIndexMarker223"/> the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python virtual environment <a id="_idIndexMarker224"/>and <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started!</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from collections import deque</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import Dense, Input</p>
			<p>Now we can begin.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor094"/>How to do it…</h2>
			<p>The DQN agent<a id="_idIndexMarker225"/> comprises a few components, namely, the <strong class="bold">Replay Buffer</strong>, the <strong class="source-inline">DQN</strong> class, the <strong class="source-inline">Agent</strong> class, and the <strong class="source-inline">train</strong> method. Perform<a id="_idIndexMarker226"/> the following steps to implement each of these<a id="_idIndexMarker227"/> components from scratch to build a complete DQN agent using TensorFlow 2.x:</p>
			<ol>
				<li>First, let's create an <a id="_idIndexMarker228"/>argument parser to handle configuration inputs to the script:<p class="source-code">        parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch3-DQN")</p><p class="source-code">parser.add_argument("--env , default="CartPole-v0")</p><p class="source-code">parser.add_argument("--lr", type=float, default=0.005)</p><p class="source-code">parser.add_argument("--batch_size", type=int, default=256)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.95)</p><p class="source-code">parser.add_argument("--eps", type=float, default=1.0)</p><p class="source-code">parser.add_argument("--eps_decay", type=float, default=0.995)</p><p class="source-code">parser.add_argument("--eps_min", type=float, default=0.01)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>Let's now <a id="_idIndexMarker229"/>create a Tensorboard logger to<a id="_idIndexMarker230"/> log useful <a id="_idIndexMarker231"/>statistics during the agent's training:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, </p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Next, let's implement a <strong class="source-inline">ReplayBu<a id="_idTextAnchor095"/>ffer</strong> class:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self, capacity=10000):</p><p class="source-code">        self.buffer = deque(maxlen=capacity)</p><p class="source-code">    def store(self, state, action, reward, next_state,</p><p class="source-code">    done):</p><p class="source-code">        self.buffer.append([state, action, reward, </p><p class="source-code">        next_state, done])</p><p class="source-code">    def sample(self):</p><p class="source-code">        sample = random.sample(self.buffer, </p><p class="source-code">                               args.batch_size)</p><p class="source-code">        states, actions, rewards, next_states, done = \</p><p class="source-code">                            map(np.asarray, zip(*sample))</p><p class="source-code">        states = np.array(states).reshape(</p><p class="source-code">                                    args.batch_size, -1)</p><p class="source-code">        next_states = np.array(next_states).\</p><p class="source-code">                            reshape(args.batch_size, -1)</p><p class="source-code">        return states, actions, rewards, next_states,</p><p class="source-code">        done</p><p class="source-code">    def size(self):</p><p class="source-code">        return len(self.buffer)</p></li>
				<li>It's now<a id="_idIndexMarker232"/> time to implement the DQN class that <a id="_idIndexMarker233"/>defines the <a id="_idIndexMarker234"/>deep neural network in TensorFlow 2.x:<p class="source-code">class DQN:</p><p class="source-code">    def __init__(self, state_dim, aciton_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = aciton_dim</p><p class="source-code">        self.epsilon = args.eps</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        model = tf.keras.Sequential(</p><p class="source-code">            [</p><p class="source-code">                Input((self.state_dim,)),</p><p class="source-code">                Dense(32, activation="relu"),</p><p class="source-code">                Dense(16, activation="relu"),</p><p class="source-code">                Dense(self.action_dim),</p><p class="source-code">            ]</p><p class="source-code">        )</p><p class="source-code">        model.compile(loss="mse", </p><p class="source-code">                      optimizer=Adam(args.lr))</p><p class="source-code">        return model</p></li>
				<li>To get <a id="_idIndexMarker235"/>the prediction and action from the<a id="_idIndexMarker236"/> DQN, let's<a id="_idIndexMarker237"/> implement the <strong class="source-inline">predict</strong> and <strong class="source-inline">get_action</strong> methods:<p class="source-code">    def predict(self, state):</p><p class="source-code">        return self.model.predict(state)</p><p class="source-code">    def get_action(self, state):</p><p class="source-code">        state = np.reshape(state, [1, self.state_dim])</p><p class="source-code">        self.epsilon *= args.eps_decay</p><p class="source-code">        self.epsilon = max(self.epsilon, args.eps_min)</p><p class="source-code">        q_value = self.predict(state)[0]</p><p class="source-code">        if np.random.random() &lt; self.epsilon:</p><p class="source-code">            return random.randint(0, self.action_dim - 1)</p><p class="source-code">        return np.argmax(q_value)</p><p class="source-code">    def train(self, states, targets):</p><p class="source-code">        self.model.fit(states, targets, epochs=1)</p></li>
				<li>With the other components implemented, we can begin implementing our <strong class="source-inline">Agent</strong> class:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = \</p><p class="source-code">            self.env.observation_space.shape[0]</p><p class="source-code">        self.action_dim = self.env.action_space.n</p><p class="source-code">        self.model = DQN(self.state_dim, self.action_dim)</p><p class="source-code">        self.target_model = DQN(self.state_dim, </p><p class="source-code">                                self.action_dim)</p><p class="source-code">        self.update_target()</p><p class="source-code">        self.buffer = ReplayBuffer()</p><p class="source-code">    def update_target(self):</p><p class="source-code">        weights = self.model.model.get_weights()</p><p class="source-code">        self.target_model.model.set_weights(weights)</p></li>
				<li>The <a id="_idIndexMarker238"/>crux of the Deep Q-learning algorithm<a id="_idIndexMarker239"/> is the q-learning update and experience replay. Let's<a id="_idIndexMarker240"/> implement that next:<p class="source-code">    def replay_experience(self):</p><p class="source-code">        for _ in range(10):</p><p class="source-code">            states, actions, rewards, next_states, done=\</p><p class="source-code">                self.buffer.sample()</p><p class="source-code">            targets = self.target_model.predict(states)</p><p class="source-code">            next_q_values = self.target_model.\</p><p class="source-code">                         predict(next_states).max(axis=1)</p><p class="source-code">            targets[range(args.batch_size), actions] = (</p><p class="source-code">                rewards + (1 - done) * next_q_values * \</p><p class="source-code">                args.gamma</p><p class="source-code">            )</p><p class="source-code">            self.model.train(states, targets)</p></li>
				<li>The <a id="_idIndexMarker241"/>next crucial step is to implement the <strong class="source-inline">train</strong> function <a id="_idIndexMarker242"/>to train <a id="_idIndexMarker243"/>the agent:<p class="source-code">def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():  # Tensorboard logging</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                done, episode_reward = False, 0</p><p class="source-code">                observation = self.env.reset()</p><p class="source-code">                while not done:</p><p class="source-code">                    action = \</p><p class="source-code">                       self.model.get_action(observation)</p><p class="source-code">                    next_observation, reward, done, _ = \</p><p class="source-code">                       self.env.step(action)</p><p class="source-code">                    self.buffer.store(</p><p class="source-code">                        observation, action, reward * \</p><p class="source-code">                        0.01, next_observation, done</p><p class="source-code">                    )</p><p class="source-code">                    episode_reward += reward</p><p class="source-code">                    observation = next_observation</p><p class="source-code">                if self.buffer.size() &gt;= args.batch_size:</p><p class="source-code">                    self.replay_experience()</p><p class="source-code">                self.update_target()</p><p class="source-code">                print(f"Episode#{ep} Reward:{</p><p class="source-code">                                        episode_reward}")</p><p class="source-code">                tf.summary.scalar("episode_reward",</p><p class="source-code">                                 episode_reward, step=ep)</p><p class="source-code">                writer.flush()</p></li>
				<li>Finally, let's<a id="_idIndexMarker244"/> create the main function to start<a id="_idIndexMarker245"/> training the agent:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = gym.make("CartPole-v0")</p><p class="source-code">    agent = Agent(env)</p><p class="source-code">    agent.train(max_episodes=20000)</p></li>
				<li>To train the DQN<a id="_idIndexMarker246"/> agent in the default environment (<strong class="source-inline">CartPole-v0</strong>), execute the following command:<p class="source-code">python c<a id="_idTextAnchor096"/>h3-deep-rl-agents/1_dqn.py</p></li>
				<li>You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space environment using the command-line arguments:<p class="source-code">python ch3-deep-rl-agents/1_dqn.py –env "MountainCar-v0"</p></li>
				<li>Now, to implement the Double DQN agent, we must modify the <strong class="source-inline">replay_experience</strong> method to use Double Q-learning's update step, as shown here:<p class="source-code">    def replay_experience(self):</p><p class="source-code">        for _ in range(10):</p><p class="source-code">            states, actions, rewards, next_states, done=\</p><p class="source-code">                self.buffer.sample()</p><p class="source-code">            targets = self.target_model.predict(states)</p><p class="source-code">            next_q_values = \</p><p class="source-code">                self.target_model.predict(next_states)[</p><p class="source-code">                range(args.batch_size),</p><p class="source-code">                np.argmax(self.model.predict(</p><p class="source-code">                                   next_states), axis=1),</p><p class="source-code">            ]</p><p class="source-code">            targets[range(args.batch_size), actions] = (</p><p class="source-code">                rewards + (1 - done) * next_q_values * \</p><p class="source-code">                    args.gamma</p><p class="source-code">            )</p><p class="source-code">            self.model.train(states, targets)</p></li>
				<li>Finally, to <a id="_idIndexMarker247"/>train the Double DQN agent, save <a id="_idIndexMarker248"/>and run the script with the updated <strong class="source-inline">replay_experience</strong> method <a id="_idIndexMarker249"/>or use the script provided as part of the source code for this book:<p class="source-code">python ch3-deep-rl-agents/1_double_dqn.py</p></li>
			</ol>
			<p>Let's see how it works.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor097"/>How it works…</h2>
			<p>Updates to the weights in the DQN are performed as per the following Q learning equation:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/Formula_B15074_03_001.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_B15074_03_002.png" alt=""/> is the change in the parameters (weights) of the DQN, s is the current state, a is the current action, s' is the next state, w represents the weights of the DQN, <img src="image/Formula_B15074_03_003.png" alt=""/> is the discount factor, <img src="image/Formula_B15074_03_004.png" alt=""/> is the learning rate, and <img src="image/Formula_B15074_03_005.png" alt=""/> represents the Q-value for the given state (s) and action (a) predicted by the DQN with a weight <img src="image/Formula_B15074_03_006.png" alt=""/>.</p>
			<p>To understand the<a id="_idIndexMarker250"/> difference between the DQN agent and the <a id="_idIndexMarker251"/>Double-DQN agent, compare the <strong class="source-inline">replay_experience</strong> method in<a id="_idIndexMarker252"/> step 8 (DQN) and step 13 (Double DQN). You will notice that the key difference lies in calculating the <strong class="source-inline">next_q_values</strong>. The DQN agent uses the maximum of the predicted Q-values (which can be an overestimation), whereas the Double DQN agent uses the predicted Q-value using two distinct neural Q networks. This is done in Double DQN to avoid the problem of overestimating the Q-va<a id="_idTextAnchor098"/><a id="_idTextAnchor099"/><a id="_idTextAnchor100"/><a id="_idTextAnchor101"/><a id="_idTextAnchor102"/><a id="_idTextAnchor103"/><a id="_idTextAnchor104"/><a id="_idTextAnchor105"/><a id="_idTextAnchor106"/><a id="_idTextAnchor107"/><a id="_idTextAnchor108"/>lues by the DQN agent.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor109"/>Implementing the Dueling DQN agent</h1>
			<p>A <a id="_idIndexMarker253"/>Dueling DQN agent explicitly estimates two quantities through a modified network architecture: </p>
			<ul>
				<li>State values, V(<em class="italic">s</em>)</li>
				<li>Advantage values, A(<em class="italic">s</em>, <em class="italic">a</em>)</li>
			</ul>
			<p>The state value estimates the value of being in state s, and the advantage value represents the advantage of taking action <em class="italic">a</em> in state <em class="italic">s</em>. This key idea of explicitly and separately estimating the two quantities enables the Dueling DQN to perform better in comparison to DQN. This recipe will walk you through the steps to implement a Dueling DQN agent from scratch using TensorFlow 2.x.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor110"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python virtual environment and <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started!</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from collections import deque</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import Add, Dense, Input</p>
			<p class="source-code">from tensorflow.keras.optimizers import Adam</p>
			<p>Now we can begin.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor111"/>How to do it…</h2>
			<p>The <a id="_idIndexMarker254"/>Dueling DQN agent comprises a few components, namely, the <strong class="bold">Replay Buffer</strong>, the <strong class="source-inline">DuelingDQN</strong> class, the <strong class="source-inline">Agent</strong> class, and the <strong class="source-inline">train</strong> method. Perform the following steps to implement each of these components from scratch to build a complete Dueling DQN agent using TensorFlow 2.x:</p>
			<ol>
				<li value="1">As a first step, let's create an argument parser to handle command-line configuration inputs to the script:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch3-DuelingDQN")</p><p class="source-code">parser.add_argument("--env", default="CartPole-v0")</p><p class="source-code">parser.add_argument("--lr", type=float, default=0.005)</p><p class="source-code">parser.add_argument("--batch_size", type=int, default=64)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.95)</p><p class="source-code">parser.add_argument("--eps", type=float, default=1.0)</p><p class="source-code">parser.add_argument("--eps_decay", type=float, default=0.995)</p><p class="source-code">parser.add_argument("--eps_min", type=float, default=0.01)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>To log useful statistics during the agent's training, let's create a TensorBoard logger:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, </p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Next, let's<a id="_idIndexMarker255"/> implement a <strong class="source-inline">ReplayBuffer</strong> class:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self, capacity=10000):</p><p class="source-code">        self.buffer = deque(maxlen=capacity)</p><p class="source-code">    def store(self, state, action, reward, next_state,</p><p class="source-code">    done):</p><p class="source-code">        self.buffer.append([state, action, reward, </p><p class="source-code">                            next_state, done])</p><p class="source-code">    def sample(self):</p><p class="source-code">        sample = random.sample(self.buffer, </p><p class="source-code">                               args.batch_size)</p><p class="source-code">        states, actions, rewards, next_states, done = \</p><p class="source-code">                            map(np.asarray, zip(*sample))</p><p class="source-code">        states = np.array(states).reshape(</p><p class="source-code">                                     args.batch_size, -1)</p><p class="source-code">        next_states = np.array(next_states).reshape(</p><p class="source-code">                                     args.batch_size, -1)</p><p class="source-code">        return states, actions, rewards, next_states,</p><p class="source-code">        done</p><p class="source-code">    def size(self):</p><p class="source-code">        return len(self.buffer)</p></li>
				<li>It's time to<a id="_idIndexMarker256"/> implement the DuelingDQN class that defines the deep neural network in TensorFlow 2.x:<p class="source-code">class DuelingDQN:</p><p class="source-code">    def __init__(self, state_dim, aciton_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = aciton_dim</p><p class="source-code">        self.epsilon = args.eps</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        backbone = tf.keras.Sequential(</p><p class="source-code">            [</p><p class="source-code">                Input((self.state_dim,)),</p><p class="source-code">                Dense(32, activation="relu"),</p><p class="source-code">                Dense(16, activation="relu"),</p><p class="source-code">            ]</p><p class="source-code">        )</p><p class="source-code">        state_input = Input((self.state_dim,))</p><p class="source-code">        backbone_1 = Dense(32, activation="relu")\</p><p class="source-code">                          (state_input)</p><p class="source-code">        backbone_2 = Dense(16, activation="relu")\</p><p class="source-code">                          (backbone_1)</p><p class="source-code">        value_output = Dense(1)(backbone_2)</p><p class="source-code">        advantage_output = Dense(self.action_dim)\</p><p class="source-code">                                (backbone_2)</p><p class="source-code">        output = Add()([value_output, advantage_output])</p><p class="source-code">        model = tf.keras.Model(state_input, output)</p><p class="source-code">        model.compile(loss="mse", </p><p class="source-code">                      optimizer=Adam(args.lr))</p><p class="source-code">        return model</p></li>
				<li>To get the<a id="_idIndexMarker257"/> prediction and action from the Dueling DQN, let's implement the <strong class="source-inline">predict</strong>, <strong class="source-inline">get_action</strong>, and <strong class="source-inline">train</strong> methods:<p class="source-code">        def predict(self, state):</p><p class="source-code">        return self.model.predict(state)</p><p class="source-code">    def get_action(self, state):</p><p class="source-code">        state = np.reshape(state, [1, self.state_dim])</p><p class="source-code">        self.epsilon *= args.eps_decay</p><p class="source-code">        self.epsilon = max(self.epsilon, args.eps_min)</p><p class="source-code">        q_value = self.predict(state)[0]</p><p class="source-code">        if np.random.random() &lt; self.epsilon:</p><p class="source-code">            return random.randint(0, self.action_dim - 1)</p><p class="source-code">        return np.argmax(q_value)</p><p class="source-code">    def train(self, states, targets):</p><p class="source-code">        self.model.fit(states, targets, epochs=1)</p></li>
				<li>We can<a id="_idIndexMarker258"/> now begin implementing our <strong class="source-inline">Agent</strong> class:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = \</p><p class="source-code">            self.env.observation_space.shape[0]</p><p class="source-code">        self.action_dim = self.env.action_space.n</p><p class="source-code">        self.model = DuelingDQN(self.state_dim, </p><p class="source-code">                                self.action_dim)</p><p class="source-code">        self.target_model = DuelingDQN(self.state_dim,</p><p class="source-code">                                       self.action_dim)</p><p class="source-code">        self.update_target()</p><p class="source-code">        self.buffer = ReplayBuffer()</p><p class="source-code">    def update_target(self):</p><p class="source-code">        weights = self.model.model.get_weights()</p><p class="source-code">        self.target_model.model.set_weights(weights)</p></li>
				<li>The crux of <a id="_idIndexMarker259"/>the Dueling Deep Q-learning algorithm is the q-learning update and experience replay. Let's implement that next:<p class="source-code">    def replay_experience(self):</p><p class="source-code">        for _ in range(10):</p><p class="source-code">            states, actions, rewards, next_states, done=\</p><p class="source-code">                self.buffer.sample()</p><p class="source-code">            targets = self.target_model.predict(states)</p><p class="source-code">            next_q_values = self.target_model.\</p><p class="source-code">                         predict(next_states).max(axis=1)</p><p class="source-code">            targets[range(args.batch_size), actions] = (</p><p class="source-code">                rewards + (1 - done) * next_q_values * \</p><p class="source-code">                args.gamma</p><p class="source-code">            )</p><p class="source-code">            self.model.train(states, targets)</p></li>
				<li>The next <a id="_idIndexMarker260"/>crucial step is to implement the <strong class="source-inline">train</strong> function to train the agent:<p class="source-code">def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                done, episode_reward = False, 0</p><p class="source-code">                state = self.env.reset()</p><p class="source-code">                while not done:</p><p class="source-code">                    action = self.model.get_action(state)</p><p class="source-code">                    next_state, reward, done, _ = \</p><p class="source-code">                                    self.env.step(action)</p><p class="source-code">                    self.buffer.put(state, action, \</p><p class="source-code">                                    reward * 0.01, \</p><p class="source-code">                                    next_state, done)</p><p class="source-code">                    episode_reward += reward</p><p class="source-code">                    state = next_state</p><p class="source-code">                if self.buffer.size() &gt;= args.batch_size:</p><p class="source-code">                    self.replay_experience()</p><p class="source-code">                self.update_target()</p><p class="source-code">                print(f"Episode#{ep} \</p><p class="source-code">                      Reward:{episode_reward}")</p><p class="source-code">                tf.summary.scalar("episode_reward",\</p><p class="source-code">                                 episode_reward, step=ep)</p></li>
				<li>Finally, let's<a id="_idIndexMarker261"/> create the main function to start training the agent:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = gym.make("CartPole-v0")</p><p class="source-code">    agent = Agent(env)</p><p class="source-code">    agent.train(max_episodes=20000)</p></li>
				<li>To train the Dueling DQN agent in the default environment (<strong class="source-inline">CartPole-v0</strong>), execute the following command:<p class="source-code">python ch3-deep-rl-agents/2_dueling_dqn.py</p></li>
				<li>You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space environment using the command-line arguments:<p class="source-code">python ch3-deep-rl-agents/2_dueling_dqn.py –env "MountainCar-v0"</p></li>
			</ol>
			<p>Let's see how it works.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor112"/>How it works…</h2>
			<p>The Dueling-DQN<a id="_idIndexMarker262"/> agent differs from the DQN agent in terms of the neural network architecture. </p>
			<p>The differences are summarized in the following diagram:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B15074_03_001.jpg" alt="Figure 3.1 – DQN and Dueling-DQN compared "/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – DQN and Dueling-DQN compared</p>
			<p>The <a id="_idIndexMarker263"/>DQN (top half of the diagram) has a linear architecture and predicts a single quantity (Q(s, a)), whereas the Dueling-DQN has a bifurcation in the last layer and predicts multiple quantities.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor113"/>Implementing the Dueling Double DQN algorithm and DDDQN agent</h1>
			<p><strong class="bold">Dueling Double DQN</strong> (<strong class="bold">DDDQN</strong>) <a id="_idIndexMarker264"/>combines the benefits of both<a id="_idIndexMarker265"/> Double Q-learning and Dueling architecture. Double Q-learning corrects DQN from overestimating the action values. The Dueling architecture uses a modified architecture to separately learn the state value function (V) and the advantage function (A). This explicit separation allows the algorithm to learn faster, especially when there are many actions to choose from and when the actions are very similar to each other. The dueling architecture enables the agent to learn even when only one action in a state has been taken, as it can update and estimate the state value function, unlike the DQN agent, which cannot learn from actions that were not taken yet. By the end of this recipe, you will have a complete implementation of the DDDQN agent.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor114"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python virtual environment and <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import<a id="_idIndexMarker266"/> statements run without issues, you are ready<a id="_idIndexMarker267"/> to get started!</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from collections import deque</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import Add, Dense, Input</p>
			<p class="source-code">from tensorflow.keras.optimizers import Adam</p>
			<p>We are ready to begin!</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor115"/>How to do it…</h2>
			<p>The DDDQN agent combines the ideas in DQN, Double DQN and the Dueling DQN. Perform the following steps to implement each of these components from scratch to build a complete Dueling Double DQN agent using TensorFlow 2.x:</p>
			<ol>
				<li value="1">First, let's <a id="_idIndexMarker268"/>create an argument parser to handle <a id="_idIndexMarker269"/>configuration inputs to the script:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch3-DuelingDoubleDQN")</p><p class="source-code">parser.add_argument("--env", default="CartPole-v0")</p><p class="source-code">parser.add_argument("--lr", type=float, default=0.005)</p><p class="source-code">parser.add_argument("--batch_size", type=int, default=256)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.95)</p><p class="source-code">parser.add_argument("--eps", type=float, default=1.0)</p><p class="source-code">parser.add_argument("--eps_decay", type=float, default=0.995)</p><p class="source-code">parser.add_argument("--eps_min", type=float, default=0.01)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>Next, let's create a Tensorboard logger to log useful statistics during the agent's training process:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Now, let's implement a <strong class="source-inline">ReplayBuffer</strong>:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self, capacity=10000):</p><p class="source-code">        self.buffer = deque(maxlen=capacity)</p><p class="source-code">    def store(self, state, action, reward, next_state, done):</p><p class="source-code">        self.buffer.append([state, action, reward, \</p><p class="source-code">        next_state, done])</p><p class="source-code">    def sample(self):</p><p class="source-code">        sample = random.sample(self.buffer, \</p><p class="source-code">                               args.batch_size)</p><p class="source-code">        states, actions, rewards, next_states, done = \</p><p class="source-code">                            map(np.asarray, zip(*sample))</p><p class="source-code">        states = np.array(states).reshape(</p><p class="source-code">                                     args.batch_size, -1)</p><p class="source-code">        next_states = np.array(next_states).\</p><p class="source-code">                             reshape(args.batch_size, -1)</p><p class="source-code">        return states, actions, rewards, next_states, \</p><p class="source-code">        done</p><p class="source-code">    def size(self):</p><p class="source-code">        return len(self.buffer)</p></li>
				<li>It's now<a id="_idIndexMarker270"/> time to implement the Dueling DQN class that<a id="_idIndexMarker271"/> defines the neural network as per the dueling architecture to which we will add Double DQN updates in later steps:<p class="source-code">class DuelingDQN:</p><p class="source-code">    def __init__(self, state_dim, aciton_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = aciton_dim</p><p class="source-code">        self.epsilon = args.eps</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        state_input = Input((self.state_dim,))</p><p class="source-code">        fc1 = Dense(32, activation="relu")(state_input)</p><p class="source-code">        fc2 = Dense(16, activation="relu")(fc1)</p><p class="source-code">        value_output = Dense(1)(fc2)</p><p class="source-code">        advantage_output = Dense(self.action_dim)(fc2)</p><p class="source-code">        output = Add()([value_output, advantage_output])</p><p class="source-code">        model = tf.keras.Model(state_input, output)</p><p class="source-code">        model.compile(loss="mse", \</p><p class="source-code">                      optimizer=Adam(args.lr))</p><p class="source-code">        return model</p></li>
				<li>To get the<a id="_idIndexMarker272"/> prediction and action from the Dueling <a id="_idIndexMarker273"/>DQN, let's implement the <strong class="source-inline">predict</strong> and <strong class="source-inline">get_action</strong> methods:<p class="source-code">    def predict(self, state):</p><p class="source-code">        return self.model.predict(state)</p><p class="source-code">    def get_action(self, state):</p><p class="source-code">        state = np.reshape(state, [1, self.state_dim])</p><p class="source-code">        self.epsilon *= args.eps_decay</p><p class="source-code">        self.epsilon = max(self.epsilon, args.eps_min)</p><p class="source-code">        q_value = self.predict(state)[0]</p><p class="source-code">        if np.random.random() &lt; self.epsilon:</p><p class="source-code">            return random.randint(0, self.action_dim - 1)</p><p class="source-code">        return np.argmax(q_value)</p><p class="source-code">    def train(self, states, targets):</p><p class="source-code">        self.model.fit(states, targets, epochs=1)</p></li>
				<li>With the<a id="_idIndexMarker274"/> other components implemented, we<a id="_idIndexMarker275"/> can begin implementing our <strong class="source-inline">Agent</strong> class:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = \</p><p class="source-code">            self.env.observation_space.shape[0]</p><p class="source-code">        self.action_dim = self.env.action_space.n</p><p class="source-code">        self.model = DuelingDQN(self.state_dim, </p><p class="source-code">                                self.action_dim)</p><p class="source-code">        self.target_model = DuelingDQN(self.state_dim,</p><p class="source-code">                                       self.action_dim)</p><p class="source-code">        self.update_target()</p><p class="source-code">        self.buffer = ReplayBuffer()</p><p class="source-code">    def update_target(self):</p><p class="source-code">        weights = self.model.model.get_weights()</p><p class="source-code">        self.target_model.model.set_weights(weights)</p></li>
				<li>The main elements in the Dueling Double Deep Q-learning algorithm are the Q-learning <a id="_idIndexMarker276"/>update and experience replay. Let's<a id="_idIndexMarker277"/> implement that next:<p class="source-code">    def replay_experience(self):</p><p class="source-code">        for _ in range(10):</p><p class="source-code">            states, actions, rewards, next_states, done=\</p><p class="source-code">                                     self.buffer.sample()</p><p class="source-code">            targets = self.target_model.predict(states)</p><p class="source-code">            next_q_values = \</p><p class="source-code">                self.target_model.predict(next_states)[</p><p class="source-code">                range(args.batch_size),</p><p class="source-code">                np.argmax(self.model.predict(</p><p class="source-code">                                   next_states), axis=1),</p><p class="source-code">            ]</p><p class="source-code">            targets[range(args.batch_size), actions] = (</p><p class="source-code">                rewards + (1 - done) * next_q_values * \</p><p class="source-code">                args.gamma</p><p class="source-code">            )</p><p class="source-code">            self.model.train(states, targets)</p></li>
				<li>The next <a id="_idIndexMarker278"/>crucial step is to implement the <strong class="source-inline">train</strong> function <a id="_idIndexMarker279"/>to train the agent:<p class="source-code">def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                done, episode_reward = False, 0</p><p class="source-code">                observation = self.env.reset()</p><p class="source-code">                while not done:</p><p class="source-code">                    action = \</p><p class="source-code">                       self.model.get_action(observation)</p><p class="source-code">                    next_observation, reward, done, _ = \</p><p class="source-code">                        self.env.step(action)</p><p class="source-code">                    self.buffer.store(</p><p class="source-code">                        observation, action, reward * \</p><p class="source-code">                                                          0.01, next_observation, done</p><p class="source-code">                    )</p><p class="source-code">                    episode_reward += reward</p><p class="source-code">                    observation = next_observation</p><p class="source-code">                if self.buffer.size() &gt;= args.batch_size:</p><p class="source-code">                    self.replay_experience()</p><p class="source-code">                self.update_target()</p><p class="source-code">                print(f"Episode#{ep} \</p><p class="source-code">                      Reward:{episode_reward}")</p><p class="source-code">                tf.summary.scalar("episode_reward", </p><p class="source-code">                                   episode_reward, </p><p class="source-code">                                   step=ep)</p></li>
				<li>Finally, let's create the main function to start training the agent:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = gym.make("CartPole-v0")</p><p class="source-code">    agent = Agent(env)</p><p class="source-code">    agent.train(max_episodes=20000)</p></li>
				<li>To train <a id="_idIndexMarker280"/>the DQN agent in the default <a id="_idIndexMarker281"/>environment (<strong class="source-inline">CartPole-v0</strong>), execute the following command:<p class="source-code">python ch3-deep-rl-agents/3_dueling_double_dqn.py</p></li>
				<li>You can also train the Dueling Double DQN agent in any OpenAI Gym-compatible discrete action-space environment using the command-line arguments:<p class="source-code">python ch3-deep-rl-agents/3_dueling_double_dqn.py –env "MountainCar-v0"</p></li>
			</ol>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor116"/>How it works…</h2>
			<p>The Dueling Double DQN architecture combines the advancements introduced by the Double DQN and Dueling architectures together.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor117"/>Implementing the Deep Recurrent Q-Learning algorithm and DRQN agent</h1>
			<p>DRQN uses a recurrent neural network to learn the Q-value function. DRQN is more suited for<a id="_idIndexMarker282"/> reinforcement learning in <a id="_idIndexMarker283"/>environments with partial observability. The recurrent network layers in the DRQN allow the agent to learn by integrating information from a temporal sequence of observations. For example, DRQN agents can infer the velocity of moving objects in the environment without any changes to their inputs (for example, no frame stacking is required). By the end of this recipe, you will have a complete DRQN agent ready to be trained in an RL environment of your choice.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor118"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python virtual environment and <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started!</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import os</p>
			<p class="source-code">from tensorflow.keras.layers import Input, Dense, LSTM</p>
			<p class="source-code">from tensorflow.keras.optimizers import Adam</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from collections import deque</p>
			<p class="source-code">import random</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor119"/>How to do it…</h2>
			<p>The <a id="_idIndexMarker284"/>Dueling Double DQN agent combines the ideas in DQN, Double DQN, and the Dueling DQN. Perform the following steps to <a id="_idIndexMarker285"/>implement each of these components from scratch to build a complete DRQN agent using TensorFlow 2.x:</p>
			<ol>
				<li value="1">First, create an argument parser to handle configuration inputs to the script:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch3-DRQN")</p><p class="source-code">parser.add_argument("--env", default="CartPole-v0")</p><p class="source-code">parser.add_argument("--lr", type=float, default=0.005)</p><p class="source-code">parser.add_argument("--batch_size", type=int, default=64)</p><p class="source-code">parser.add_argument("--time_steps", type=int, default=4)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.95)</p><p class="source-code">parser.add_argument("--eps", type=float, default=1.0)</p><p class="source-code">parser.add_argument("--eps_decay", type=float, default=0.995)</p><p class="source-code">parser.add_argument("--eps_min", type=float, default=0.01)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>Let's log useful statistics during the agent's training using Tensorboard:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Next, let's<a id="_idIndexMarker286"/> implement<a id="_idIndexMarker287"/> a <strong class="source-inline">ReplayBuffer</strong>:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self, capacity=10000):</p><p class="source-code">        self.buffer = deque(maxlen=capacity)</p><p class="source-code">    def store(self, state, action, reward, next_state,\</p><p class="source-code">    done):</p><p class="source-code">        self.buffer.append([state, action, reward, \</p><p class="source-code">                            next_state, done])</p><p class="source-code">    def sample(self):</p><p class="source-code">        sample = random.sample(self.buffer, </p><p class="source-code">                               args.batch_size)</p><p class="source-code">        states, actions, rewards, next_states, done = \</p><p class="source-code">            map(np.asarray, zip(*sample))</p><p class="source-code">        states = np.array(states).reshape(</p><p class="source-code">                                     args.batch_size, -1)</p><p class="source-code">        next_states = np.array(next_states).reshape(</p><p class="source-code">                                     args.batch_size, -1)</p><p class="source-code">        return states, actions, rewards, next_states, \</p><p class="source-code">        done</p><p class="source-code">    def size(self):</p><p class="source-code">        return len(self.buffer)</p></li>
				<li>It's now <a id="_idIndexMarker288"/>time to implement the <a id="_idIndexMarker289"/>DRQN class that defines the deep neural network using TensorFlow 2.x:<p class="source-code">class DRQN:</p><p class="source-code">    def __init__(self, state_dim, action_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.epsilon = args.eps</p><p class="source-code">        self.opt = Adam(args.lr)</p><p class="source-code">        self.compute_loss = \</p><p class="source-code">            tf.keras.losses.MeanSquaredError()</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        return tf.keras.Sequential(</p><p class="source-code">            [</p><p class="source-code">                Input((args.time_steps, self.state_dim)),</p><p class="source-code">                LSTM(32, activation="tanh"),</p><p class="source-code">                Dense(16, activation="relu"),</p><p class="source-code">                Dense(self.action_dim),</p><p class="source-code">            ]</p><p class="source-code">        )</p></li>
				<li>To get the prediction and action from the DRQN, let's implement the <strong class="source-inline">predict</strong> and <strong class="source-inline">get_action</strong> methods:<p class="source-code">    def predict(self, state):</p><p class="source-code">        return self.model.predict(state)</p><p class="source-code">    def get_action(self, state):</p><p class="source-code">        state = np.reshape(state, [1, args.time_steps,</p><p class="source-code">                                   self.state_dim])</p><p class="source-code">        self.epsilon *= args.eps_decay</p><p class="source-code">        self.epsilon = max(self.epsilon, args.eps_min)</p><p class="source-code">        q_value = self.predict(state)[0]</p><p class="source-code">        if np.random.random() &lt; self.epsilon:</p><p class="source-code">            return random.randint(0, self.action_dim - 1)</p><p class="source-code">        return np.argmax(q_value)</p><p class="source-code">    def train(self, states, targets):</p><p class="source-code">        targets = tf.stop_gradient(targets)</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            logits = self.model(states, training=True)</p><p class="source-code">            assert targets.shape == logits.shape</p><p class="source-code">            loss = self.compute_loss(targets, logits)</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p></li>
				<li>With the <a id="_idIndexMarker290"/>other components<a id="_idIndexMarker291"/> implemented, we can begin implementing our <strong class="source-inline">Agent</strong> class:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = \</p><p class="source-code">            self.env.observation_space.shape[0]</p><p class="source-code">        self.action_dim = self.env.action_space.n</p><p class="source-code">        self.states = np.zeros([args.time_steps, </p><p class="source-code">                                self.state_dim])</p><p class="source-code">        self.model = DRQN(self.state_dim, </p><p class="source-code">                          self.action_dim)</p><p class="source-code">        self.target_model = DRQN(self.state_dim, </p><p class="source-code">                                 self.action_dim)</p><p class="source-code">        self.update_target()</p><p class="source-code">        self.buffer = ReplayBuffer()</p><p class="source-code">    def update_target(self):</p><p class="source-code">        weights = self.model.model.get_weights()</p><p class="source-code">        self.target_model.model.set_weights(weights)</p></li>
				<li>In addition<a id="_idIndexMarker292"/> to the <strong class="source-inline">train</strong> method <a id="_idIndexMarker293"/>in the DRQN class that we implemented in step 6, the crux of the deep recurrent Q-learning algorithm is the q-learning update and experience replay. Let's implement that next:<p class="source-code">    def replay_experience(self):</p><p class="source-code">        for _ in range(10):</p><p class="source-code">            states, actions, rewards, next_states, done=\</p><p class="source-code">                self.buffer.sample()</p><p class="source-code">            targets = self.target_model.predict(states)</p><p class="source-code">            next_q_values = self.target_model.\</p><p class="source-code">                         predict(next_states).max(axis=1)</p><p class="source-code">            targets[range(args.batch_size), actions] = (</p><p class="source-code">                rewards + (1 - done) * next_q_values * \</p><p class="source-code">                args.gamma</p><p class="source-code">            )</p><p class="source-code">            self.model.train(states, targets)</p></li>
				<li>Since the DRQN <a id="_idIndexMarker294"/>agent uses recurrent <a id="_idIndexMarker295"/>states, let's implement the <strong class="source-inline">update_states</strong> method to update the recurrent state of the agent:<p class="source-code">    def update_states(self, next_state):</p><p class="source-code">        self.states = np.roll(self.states, -1, axis=0)</p><p class="source-code">        self.states[-1] = next_state</p></li>
				<li>The next crucial step is to implement the <strong class="source-inline">train</strong> function to train the agent:<p class="source-code">def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                done, episode_reward = False, 0</p><p class="source-code">                self.states = np.zeros([args.time_steps, </p><p class="source-code">                                        self.state_dim])</p><p class="source-code">                self.update_states(self.env.reset())</p><p class="source-code">                while not done:</p><p class="source-code">                    action = self.model.get_action(</p><p class="source-code">                                             self.states)</p><p class="source-code">                    next_state, reward, done, _ = \</p><p class="source-code">                                    self.env.step(action)</p><p class="source-code">                    prev_states = self.states</p><p class="source-code">                    self.update_states(next_state)</p><p class="source-code">                    self.buffer.store(</p><p class="source-code">                        prev_states, action, reward * \</p><p class="source-code">                        0.01, self.states, done</p><p class="source-code">                    )</p><p class="source-code">                    episode_reward += reward</p><p class="source-code">                if self.buffer.size() &gt;= args.batch_size:</p><p class="source-code">                    self.replay_experience()</p><p class="source-code">                self.update_target()</p><p class="source-code">                print(f"Episode#{ep} \</p><p class="source-code">                      Reward:{episode_reward}")</p><p class="source-code">                tf.summary.scalar("episode_reward", episode_reward, step=ep)</p></li>
				<li>Finally, let's <a id="_idIndexMarker296"/>create the main training loop<a id="_idIndexMarker297"/> for the agent:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = gym.make("Pong-v0")</p><p class="source-code">    agent = Agent(env)</p><p class="source-code">    agent.train(max_episodes=20000)</p></li>
				<li>To train the DRQN agent in the default environment (<strong class="source-inline">CartPole-v0</strong>), execute the following command:<p class="source-code">python ch3-deep-rl-agents/4_drqn.py</p></li>
				<li>You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space environment using the command-line arguments:<p class="source-code">python ch3-deep-rl-agents/4_drqn.py –env "MountainCar-v0"</p></li>
			</ol>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor120"/>How it works…</h2>
			<p>The DRQN agent<a id="_idIndexMarker298"/> uses an LSTM layer, which <a id="_idIndexMarker299"/>adds a recurrent learning capability to the agent. The LSTM layer is added to the agent's network in step 5 of the recipe. The other steps in the recipe have similar components as the DQN agent.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor121"/>Implementing the Asynchronous Advantage Actor-Critic algorithm and A3C agent</h1>
			<p>The A3C algorithm<a id="_idIndexMarker300"/> builds upon the Actor-Critic<a id="_idIndexMarker301"/> class of algorithms by using a neural network to approximate the actor (and critic). The actor learns the policy function using a deep neural network, while the critic estimates the value function. The asynchronous nature of the algorithm allows the agent to learn from different parts of the state space, allowing parallel learning and faster convergence. Unlike DQN agents, which use an experience replay memory, the A3C agent uses multiple workers to gather more samples for learning. By the end of this recipe, you will have a complete script to train an A3C agent for any continuous action valued environment of your choice!</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor122"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python virtual environment and <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started!</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import os</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">from multiprocessing import cpu_count</p>
			<p class="source-code">from threading import Thread</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import Input, Dense, Lambda</p>
			<p>Now we can begin.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor123"/>How to do it…</h2>
			<p>We will<a id="_idIndexMarker302"/> implement an <strong class="bold">Asynchronous, Advantage Actor-Critic</strong> (<strong class="bold">A3C</strong>) algorithm by making use of Python's <a id="_idIndexMarker303"/>multiprocessing and multithreading capabilities. The following steps will help<a id="_idIndexMarker304"/> you to implement a complete A3C agent from scratch using TensorFlow 2.x:</p>
			<ol>
				<li value="1">First, let's create an argument parser to handle configuration inputs to the script:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch3-A3C")</p><p class="source-code">parser.add_argument("--env", default="MountainCarContinuous-v0")</p><p class="source-code">parser.add_argument("--actor-lr", type=float, default=0.001)</p><p class="source-code">parser.add_argument("--critic-lr", type=float, default=0.002)</p><p class="source-code">parser.add_argument("--update-interval", type=int, default=5)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.99)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>Let's now create a Tensorboard logger to log useful statistics during the agent's training:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">       datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>To<a id="_idIndexMarker305"/> have a count <a id="_idIndexMarker306"/>of the global episode number, let's define a global variable:<p class="source-code">GLOBAL_EPISODE_NUM = 0</p></li>
				<li>We can now focus on implementing the <strong class="source-inline">Actor</strong> class, which will contain a neural network-based policy to act in the environments:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    action_bound, std_bound):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.action_bound = action_bound</p><p class="source-code">        self.std_bound = std_bound</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = tf.keras.optimizers.Adam(</p><p class="source-code">                                           args.actor_lr)</p><p class="source-code">        self.entropy_beta = 0.01</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        state_input = Input((self.state_dim,))</p><p class="source-code">        dense_1 = Dense(32, activation="relu")\</p><p class="source-code">                        (state_input)</p><p class="source-code">        dense_2 = Dense(32, activation="relu")(dense_1)</p><p class="source-code">        out_mu = Dense(self.action_dim, \</p><p class="source-code">                       activation="tanh")(dense_2)</p><p class="source-code">        mu_output = Lambda(lambda x: x * \</p><p class="source-code">                           self.action_bound)(out_mu)</p><p class="source-code">        std_output = Dense(self.action_dim, </p><p class="source-code">                          activation="softplus")(dense_2)</p><p class="source-code">        return tf.keras.models.Model(state_input, </p><p class="source-code">                                 [mu_output, std_output])</p></li>
				<li>To <a id="_idIndexMarker307"/>get an action <a id="_idIndexMarker308"/>from the actor given a state, let's define the <strong class="source-inline">get_action</strong> method:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        state = np.reshape(state, [1, self.state_dim])</p><p class="source-code">        mu, std = self.model.predict(state)</p><p class="source-code">        mu, std = mu[0], std[0]</p><p class="source-code">        return np.random.normal(mu, std, </p><p class="source-code">                                size=self.action_dim)</p></li>
				<li>Next, to<a id="_idIndexMarker309"/> compute the<a id="_idIndexMarker310"/> loss, we need to calculate the log of the policy (probability) density function:<p class="source-code">    def log_pdf(self, mu, std, action):</p><p class="source-code">        std = tf.clip_by_value(std, self.std_bound[0],</p><p class="source-code">                               self.std_bound[1])</p><p class="source-code">        var = std ** 2</p><p class="source-code">        log_policy_pdf = -0.5 * (action - mu) ** 2 / var\</p><p class="source-code">                          - 0.5 * tf.math.log(</p><p class="source-code">            var * 2 * np.pi</p><p class="source-code">        )</p><p class="source-code">        return tf.reduce_sum(log_policy_pdf, 1,</p><p class="source-code">                             keepdims=True)</p></li>
				<li>Let's now use the <strong class="source-inline">log_pdf</strong> method to compute the actor loss:<p class="source-code">    def compute_loss(self, mu, std, actions, advantages):</p><p class="source-code">        log_policy_pdf = self.log_pdf(mu, std, actions)</p><p class="source-code">        loss_policy = log_policy_pdf * advantages</p><p class="source-code">        return tf.reduce_sum(-loss_policy)</p></li>
				<li>As the final step in the <strong class="source-inline">Actor</strong> class implementation, let's define the <strong class="source-inline">train</strong> method:<p class="source-code">    def train(self, states, actions, advantages):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            mu, std = self.model(states, training=True)</p><p class="source-code">            loss = self.compute_loss(mu, std, actions,</p><p class="source-code">                                     advantages)</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                         self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>With<a id="_idIndexMarker311"/> the <strong class="source-inline">Actor</strong> class<a id="_idIndexMarker312"/> defined, we can move on to define the <strong class="source-inline">Critic</strong> class:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = tf.keras.optimizers.Adam\</p><p class="source-code">                            (args.critic_lr)</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        return tf.keras.Sequential(</p><p class="source-code">            [</p><p class="source-code">                Input((self.state_dim,)),</p><p class="source-code">                Dense(32, activation="relu"),</p><p class="source-code">                Dense(32, activation="relu"),</p><p class="source-code">                Dense(16, activation="relu"),</p><p class="source-code">                Dense(1, activation="linear"),</p><p class="source-code">            ]</p><p class="source-code">        )</p></li>
				<li>Next, let's define the <strong class="source-inline">train</strong> method and a <strong class="source-inline">compute_loss</strong> method to train the critic:<p class="source-code">    def compute_loss(self, v_pred, td_targets):</p><p class="source-code">        mse = tf.keras.losses.MeanSquaredError()</p><p class="source-code">        return mse(td_targets, v_pred)</p><p class="source-code">    def train(self, states, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model(states, training=True)</p><p class="source-code">            assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, \</p><p class="source-code">                            tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, \</p><p class="source-code">                         self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>It is <a id="_idIndexMarker313"/>time to<a id="_idIndexMarker314"/> implement the <strong class="source-inline">A3CWorker</strong> class based on Python's Thread interface:<p class="source-code">class A3CWorker(Thread):</p><p class="source-code">    def __init__(self, env, global_actor, global_critic,</p><p class="source-code">    max_episodes):</p><p class="source-code">        Thread.__init__(self)</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = \</p><p class="source-code">            self.env.observation_space.shape[0]</p><p class="source-code">        self.action_dim = self.env.action_space.shape[0]</p><p class="source-code">        self.action_bound = self.env.action_space.high[0]</p><p class="source-code">        self.std_bound = [1e-2, 1.0]</p><p class="source-code">        self.max_episodes = max_episodes</p><p class="source-code">        self.global_actor = global_actor</p><p class="source-code">        self.global_critic = global_critic</p><p class="source-code">        self.actor = Actor(</p><p class="source-code">            self.state_dim, self.action_dim, </p><p class="source-code">            self.action_bound, self.std_bound</p><p class="source-code">        )</p><p class="source-code">        self.critic = Critic(self.state_dim)</p><p class="source-code">        self.actor.model.set_weights(</p><p class="source-code">            self.global_actor.model.get_weights())</p><p class="source-code">        self.critic.model.set_weights(</p><p class="source-code">            self.global_critic.model.get_weights())</p></li>
				<li>We <a id="_idIndexMarker315"/>will be using <strong class="bold">n-step Temporal Difference (TD)</strong> learning <a id="_idIndexMarker316"/>updates. Therefore, let's define<a id="_idIndexMarker317"/> a method to calculate the n-step TD target:<p class="source-code">    def n_step_td_target(self, rewards, next_v_value,</p><p class="source-code">    done):</p><p class="source-code">        td_targets = np.zeros_like(rewards)</p><p class="source-code">        cumulative = 0</p><p class="source-code">        if not done:</p><p class="source-code">            cumulative = next_v_value</p><p class="source-code">        for k in reversed(range(0, len(rewards))):</p><p class="source-code">            cumulative = args.gamma * cumulative + \</p><p class="source-code">                         rewards[k]</p><p class="source-code">            td_targets[k] = cumulative</p><p class="source-code">        return td_targets</p></li>
				<li>We will also need to calculate the advantage values. The advantage value in its simplest form is easy to implement:<p class="source-code">    def advantage(self, td_targets, baselines):</p><p class="source-code">        return td_targets - baselines</p></li>
				<li>We will<a id="_idIndexMarker318"/> split the<a id="_idIndexMarker319"/> implementation of the <strong class="source-inline">train</strong> method into the following two steps. First, let's implement the outer loop:<p class="source-code">    def train(self):</p><p class="source-code">        global GLOBAL_EPISODE_NUM</p><p class="source-code">        while self.max_episodes &gt;= GLOBAL_EPISODE_NUM:</p><p class="source-code">            state_batch = []</p><p class="source-code">            action_batch = []</p><p class="source-code">            reward_batch = []</p><p class="source-code">            episode_reward, done = 0, False</p><p class="source-code">            state = self.env.reset()</p><p class="source-code">            while not done:</p><p class="source-code">                # self.env.render()</p><p class="source-code">                action = self.actor.get_action(state)</p><p class="source-code">                action = np.clip(action, </p><p class="source-code">                                 -self.action_bound,</p><p class="source-code">                                 self.action_bound)</p><p class="source-code">                next_state, reward, done, _ = \</p><p class="source-code">                    self.env.step(action)</p><p class="source-code">                state = np.reshape(state, [1, </p><p class="source-code">                                        self.state_dim])</p><p class="source-code">                action = np.reshape(action, [1, 1])</p><p class="source-code">                next_state = np.reshape(next_state, </p><p class="source-code">                                     [1, self.state_dim])</p><p class="source-code">                reward = np.reshape(reward, [1, 1])</p><p class="source-code">                state_batch.append(state)</p><p class="source-code">                action_batch.append(action)</p><p class="source-code">                reward_batch.append(reward)</p></li>
				<li>In this <a id="_idIndexMarker320"/>step, we will <a id="_idIndexMarker321"/>complete the <strong class="source-inline">train</strong> method implementation:<p class="source-code">                if len(state_batch) &gt;= args.update_\</p><p class="source-code">                interval or done:</p><p class="source-code">                    states = np.array([state.squeeze() \</p><p class="source-code">                               for state in state_batch])</p><p class="source-code">                    actions = np.array([action.squeeze()\</p><p class="source-code">                             for action in action_batch])</p><p class="source-code">                    rewards = np.array([reward.squeeze()\</p><p class="source-code">                             for reward in reward_batch])</p><p class="source-code">                    next_v_value = self.critic.model.\</p><p class="source-code">                                      predict(next_state)</p><p class="source-code">                    td_targets = self.n_step_td_target(</p><p class="source-code">                        (rewards + 8) / 8, next_v_value,</p><p class="source-code">                         done</p><p class="source-code">                    )</p><p class="source-code">                    advantages = td_targets - \</p><p class="source-code">                        self.critic.model.predict(states)</p><p class="source-code">                    actor_loss = self.global_actor.train(</p><p class="source-code">                            states, actions, advantages)</p><p class="source-code">                    critic_loss = self.global_critic.\</p><p class="source-code">                            train(states, td_targets)</p><p class="source-code">                    self.actor.model.set_weights(self.\</p><p class="source-code">                        global_actor.model.get_weights())</p><p class="source-code">                    self.critic.model.set_weights(</p><p class="source-code">                        self.global_critic.model.\</p><p class="source-code">                        get_weights()</p><p class="source-code">                    )</p><p class="source-code">                    state_batch = []</p><p class="source-code">                    action_batch = []</p><p class="source-code">                    reward_batch = []</p><p class="source-code">                episode_reward += reward[0][0]</p><p class="source-code">                state = next_state[0]</p><p class="source-code">            print(f"Episode#{GLOBAL_EPISODE_NUM}\</p><p class="source-code">                  Reward:{episode_reward}")</p><p class="source-code">            tf.summary.scalar("episode_reward", </p><p class="source-code">                               episode_reward,</p><p class="source-code">                               step=GLOBAL_EPISODE_NUM)</p><p class="source-code">            GLOBAL_EPISODE_NUM += 1</p></li>
				<li>The run<a id="_idIndexMarker322"/> method <a id="_idIndexMarker323"/>for the <strong class="source-inline">A3CWorker</strong> thread will simply be the following:<p class="source-code">    def run(self):</p><p class="source-code">  <a id="_idTextAnchor124"/>      self.train()</p></li>
				<li>Next, let's implement the <strong class="source-inline">Agent</strong> class:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, env_name, </p><p class="source-code">                 num_workers=cpu_count()):</p><p class="source-code">        env = gym.make(env_name)</p><p class="source-code">        self.env_name = env_name</p><p class="source-code">        self.state_dim = env.observation_space.shape[0]</p><p class="source-code">        self.action_dim = env.action_space.shape[0]</p><p class="source-code">        self.action_bound = env.action_space.high[0]</p><p class="source-code">        self.std_bound = [1e-2, 1.0]</p><p class="source-code">        self.global_actor = Actor(</p><p class="source-code">            self.state_dim, self.action_dim, </p><p class="source-code">            self.action_bound, self.std_bound</p><p class="source-code">        )</p><p class="source-code">        self.global_critic = Critic(self.state_dim)</p><p class="source-code">        self.num_workers = num_workers</p></li>
				<li>The A3C<a id="_idIndexMarker324"/> agent makes<a id="_idIndexMarker325"/> use of several concurrent workers. In order to update each of the workers to update the A3C agent, the following code is necessary:<p class="source-code">def train(self, max_episodes=20000):</p><p class="source-code">        workers = []</p><p class="source-code">        for i in range(self.num_workers):</p><p class="source-code">            env = gym.make(self.env_name)</p><p class="source-code">            workers.append(</p><p class="source-code">                A3CWorker(env, self.global_actor, </p><p class="source-code">                        self.global_critic, max_episodes)</p><p class="source-code">            )</p><p class="source-code">        for worker in workers:</p><p class="source-code">            worker.start()</p><p class="source-code">        for worker in workers:</p><p class="source-code">            worker.join()</p></li>
				<li>With that, our A3C agent implementation is complete, and we are ready to define our main function:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "MountainCarContinuous-v0"</p><p class="source-code">    agent = Agent(env_name, args.num_workers)</p><p class="source-code">    agent.train(max_episodes=20000)</p></li>
			</ol>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor125"/>How it works…</h2>
			<p>In simple terms, the<a id="_idIndexMarker326"/> crux of the A3C <a id="_idIndexMarker327"/>algorithm can be summarized in the following sequence of steps for each iteration: </p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B15074_03_002.jpg" alt="Figure 3.2 – Updating steps in the A3C agent learning iteration "/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Updating steps in the A3C agent learning iteration</p>
			<p>The steps repeat again from top to bottom for the next iteration and so on until convergence.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor126"/>Implementing the Proximal Policy Optimization algorithm and PPO agent</h1>
			<p>The <strong class="bold">Proximal Policy Optimization</strong> (<strong class="bold">PPO</strong>) algorithm builds<a id="_idIndexMarker328"/> upon the work <a id="_idIndexMarker329"/>of <strong class="bold">Trust Region Policy Optimization</strong> (<strong class="bold">TRPO</strong>) to constrain the new policy to be within a trust region from the old policy. PPO simplifies the implementation of this core idea by using a clipped surrogate objective function that is easier to implement, yet quite powerful and efficient. It is one of the most widely used RL algorithms, especially for continuous control problems. By <a id="_idIndexMarker330"/>the end of this recipe, you will <a id="_idIndexMarker331"/>have built a PPO agent that you can train in your RL environment of choice.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor127"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python virtual environment and <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started!</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import os</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import Dense, Input, Lambda </p>
			<p>We are ready to get started.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor128"/>How to do it…</h2>
			<p>The following steps will help you to implement a complete PPO agent from scratch using TensorFlow 2.x:</p>
			<ol>
				<li value="1">First, let's<a id="_idIndexMarker332"/> create an argument <a id="_idIndexMarker333"/>parser to handle configuration inputs to the script:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch3-PPO")</p><p class="source-code">parser.add_argument("--env", default="Pendulum-v0")</p><p class="source-code">parser.add_argument("--update-freq", type=int, default=5)</p><p class="source-code">parser.add_argument("--epochs", type=int, default=3)</p><p class="source-code">parser.add_argument("--actor-lr", type=float, default=0.0005)</p><p class="source-code">parser.add_argument("--critic-lr", type=float, default=0.001)</p><p class="source-code">parser.add_argument("--clip-ratio", type=float, default=0.1)</p><p class="source-code">parser.add_argument("--gae-lambda", type=float, default=0.95)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.99)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>Next, let's create a Tensorboard logger to log useful statistics during the agent's training:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, </p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>We can now<a id="_idIndexMarker334"/> focus on implementing <a id="_idIndexMarker335"/>the <strong class="source-inline">Actor</strong> class, which will contain a neural network-based policy to act:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    action_bound, std_bound):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.action_bound = action_bound</p><p class="source-code">        self.std_bound = std_bound</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Adam(args.actor_lr)</p><p class="source-code">        </p><p class="source-code">    def nn_model(self):</p><p class="source-code">        state_input = Input((self.state_dim,))</p><p class="source-code">        dense_1 = Dense(32, activation="relu")\</p><p class="source-code">                       (state_input)</p><p class="source-code">        dense_2 = Dense(32, activation="relu")\</p><p class="source-code">                       (dense_1)</p><p class="source-code">        out_mu = Dense(self.action_dim,</p><p class="source-code">                       activation="tanh")(dense_2)</p><p class="source-code">        mu_output = Lambda(lambda x: x * \</p><p class="source-code">                           self.action_bound)(out_mu)</p><p class="source-code">        std_output = Dense(self.action_dim, </p><p class="source-code">                          activation="softplus")(dense_2)</p><p class="source-code">        return tf.keras.models.Model(state_input, </p><p class="source-code">                                 [mu_output, std_output])</p></li>
				<li>To get an <a id="_idIndexMarker336"/>action from the actor <a id="_idIndexMarker337"/>given a state, let's define the <strong class="source-inline">get_action</strong> method:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        state = np.reshape(state, [1, self.state_dim])</p><p class="source-code">        mu, std = self.model.predict(state)</p><p class="source-code">        action = np.random.normal(mu[0], std[0], </p><p class="source-code">                                  size=self.action_dim)</p><p class="source-code">        action = np.clip(action, -self.action_bound, </p><p class="source-code">                         self.action_bound)</p><p class="source-code">        log_policy = self.log_pdf(mu, std, action)</p><p class="source-code">        return log_policy, action</p></li>
				<li>Next, to compute the loss, we need to calculate the log of the policy (probability) density function:<p class="source-code">    def log_pdf(self, mu, std, action):</p><p class="source-code">        std = tf.clip_by_value(std, self.std_bound[0], </p><p class="source-code">                               self.std_bound[1])</p><p class="source-code">        var = std ** 2</p><p class="source-code">        log_policy_pdf = -0.5 * (action - mu) ** 2 / var\</p><p class="source-code">                          - 0.5 * tf.math.log(</p><p class="source-code">            var * 2 * np.pi</p><p class="source-code">        )</p><p class="source-code">        return tf.reduce_sum(log_policy_pdf, 1,</p><p class="source-code">                             keepdims=True)</p></li>
				<li>Let's now <a id="_idIndexMarker338"/>use the <strong class="source-inline">log_pdf</strong> method<a id="_idIndexMarker339"/> to compute the actor loss:<p class="source-code">    def compute_loss(self, log_old_policy, </p><p class="source-code">                     log_new_policy, actions, gaes):</p><p class="source-code">        ratio = tf.exp(log_new_policy - \</p><p class="source-code">                       tf.stop_gradient(log_old_policy))</p><p class="source-code">        gaes = tf.stop_gradient(gaes)</p><p class="source-code">        clipped_ratio = tf.clip_by_value(</p><p class="source-code">            ratio, 1.0 - args.clip_ratio, 1.0 + \</p><p class="source-code">            args.clip_ratio</p><p class="source-code">        )</p><p class="source-code">        surrogate = -tf.minimum(ratio * gaes, \</p><p class="source-code">                                clipped_ratio * gaes)</p><p class="source-code">        return tf.reduce_mean(surrogate)</p></li>
				<li>As the final step in the <strong class="source-inline">Actor</strong> class implementation, let's define the <strong class="source-inline">train</strong> method:<p class="source-code">    def train(self, log_old_policy, states, actions,</p><p class="source-code">    gaes):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            mu, std = self.model(states, training=True)</p><p class="source-code">            log_new_policy = self.log_pdf(mu, std,</p><p class="source-code">                                          actions)</p><p class="source-code">            loss = self.compute_loss(log_old_policy, </p><p class="source-code">                          log_new_policy, actions, gaes)</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>With <a id="_idIndexMarker340"/>the <strong class="source-inline">Actor</strong> class defined, we<a id="_idIndexMarker341"/> can move on to define the <strong class="source-inline">Critic</strong> class:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = tf.keras.optimizers.Adam(</p><p class="source-code">                                         args.critic_lr)</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        return tf.keras.Sequential(</p><p class="source-code">            [</p><p class="source-code">                Input((self.state_dim,)),</p><p class="source-code">                Dense(32, activation="relu"),</p><p class="source-code">                Dense(32, activation="relu"),</p><p class="source-code">                Dense(16, activation="relu"),</p><p class="source-code">                Dense(1, activation="linear"),</p><p class="source-code">            ]</p><p class="source-code">        )</p></li>
				<li>Next, let's define the <strong class="source-inline">train</strong> method and a <strong class="source-inline">compute_loss</strong> method to train the critic:<p class="source-code">    def compute_loss(self, v_pred, td_targets):</p><p class="source-code">        mse = tf.keras.losses.MeanSquaredError()</p><p class="source-code">        return mse(td_targets, v_pred)</p><p class="source-code">    def train(self, states, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model(states, training=True)</p><p class="source-code">            assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, </p><p class="source-code">                            tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                         self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>It is now<a id="_idIndexMarker342"/> time to implement the <a id="_idIndexMarker343"/>PPO <strong class="source-inline">Agent</strong> class:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = \</p><p class="source-code">            self.env.observation_space.shape[0]</p><p class="source-code">        self.action_dim = self.env.action_space.shape[0]</p><p class="source-code">        self.action_bound = self.env.action_space.high[0]</p><p class="source-code">        self.std_bound = [1e-2, 1.0]</p><p class="source-code">        self.actor_opt = \</p><p class="source-code">            tf.keras.optimizers.Adam(args.actor_lr)</p><p class="source-code">        self.critic_opt = \</p><p class="source-code">            tf.keras.optimizers.Adam(args.critic_lr)</p><p class="source-code">        self.actor = Actor(</p><p class="source-code">            self.state_dim, self.action_dim, </p><p class="source-code">            self.action_bound, self.std_bound</p><p class="source-code">        )</p><p class="source-code">        self.critic = Critic(self.state_dim)    </p></li>
				<li>We will be <a id="_idIndexMarker344"/>using the <strong class="bold">Generalized Advantage Estimates</strong> (<strong class="bold">GAE</strong>). Let's<a id="_idIndexMarker345"/> implement a method to calculate <a id="_idIndexMarker346"/>the GAE target values:<p class="source-code">    def gae_target(self, rewards, v_values, next_v_value,</p><p class="source-code">    done):</p><p class="source-code">        n_step_targets = np.zeros_like(rewards)</p><p class="source-code">        gae = np.zeros_like(rewards)</p><p class="source-code">        gae_cumulative = 0</p><p class="source-code">        forward_val = 0</p><p class="source-code">        if not done:</p><p class="source-code">            forward_val = next_v_value</p><p class="source-code">        for k in reversed(range(0, len(rewards))):</p><p class="source-code">            delta = rewards[k] + args.gamma * \</p><p class="source-code">                    forward_val - v_values[k]</p><p class="source-code">            gae_cumulative = args.gamma * \</p><p class="source-code">                args.gae_lambda * gae_cumulative + delta</p><p class="source-code">            gae[k] = gae_cumulative</p><p class="source-code">            forward_val = v_values[k]</p><p class="source-code">            n_step_targets[k] = gae[k] + v_values[k]</p><p class="source-code">        return gae, n_step_targets</p></li>
				<li>We will <a id="_idIndexMarker347"/>now split the implementation<a id="_idIndexMarker348"/> of the <strong class="source-inline">train</strong> method. First, let's implement the outer loop:<p class="source-code">    def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                state_batch = []</p><p class="source-code">                action_batch = []</p><p class="source-code">                reward_batch = []</p><p class="source-code">                old_policy_batch = []</p><p class="source-code">                episode_reward, done = 0, False</p><p class="source-code">                state = self.env.reset()</p></li>
				<li>In this step, we will start the inner loop (per episode) implementation and finish it in the next couple of steps:<p class="source-code">                while not done:</p><p class="source-code">                    # self.env.render()</p><p class="source-code">                    log_old_policy, action = \</p><p class="source-code">                        self.actor.get_action(state)</p><p class="source-code">                    next_state, reward, done, _ = \</p><p class="source-code">                                   self.env.step(action)</p><p class="source-code">                    state = np.reshape(state, [1, </p><p class="source-code">                                         self.state_dim])</p><p class="source-code">                    action = np.reshape(action, [1, 1])</p><p class="source-code">                    next_state = np.reshape(next_state, </p><p class="source-code">                                     [1, self.state_dim])</p><p class="source-code">                    reward = np.reshape(reward, [1, 1])</p><p class="source-code">                    log_old_policy = \</p><p class="source-code">                       np.reshape(log_old_policy, [1, 1])</p><p class="source-code">                    state_batch.append(state)</p><p class="source-code">                    action_batch.append(action)</p><p class="source-code">                    reward_batch.append((reward + 8) / 8)</p><p class="source-code">                    old_policy_batch.append(log_old_policy)</p></li>
				<li>In this step, we<a id="_idIndexMarker349"/> will use the value<a id="_idIndexMarker350"/> predictions made by the PPO algorithm to prepare for the policy update process:<p class="source-code">    if len(state_batch) &gt;= args.update_freq or done:</p><p class="source-code">                        states = np.array([state.\</p><p class="source-code">                                   squeeze() for state \</p><p class="source-code">                                   in state_batch])</p><p class="source-code">                        actions = np.array(</p><p class="source-code">                            [action.squeeze() for action\</p><p class="source-code">                             in action_batch]</p><p class="source-code">                        )</p><p class="source-code">                        rewards = np.array(</p><p class="source-code">                            [reward.squeeze() for reward\</p><p class="source-code">                             in reward_batch]</p><p class="source-code">                        )</p><p class="source-code">                        old_policies = np.array(</p><p class="source-code">                            [old_pi.squeeze() for old_pi\</p><p class="source-code">                             in old_policy_batch]</p><p class="source-code">                        )</p><p class="source-code">                        v_values = self.critic.model.\</p><p class="source-code">                                    predict(states)</p><p class="source-code">                        next_v_value =self.critic.model.\</p><p class="source-code">                                      predict(next_state)</p><p class="source-code">                        gaes, td_targets = \</p><p class="source-code">                             self.gae_target(</p><p class="source-code">                                 rewards, v_values, \</p><p class="source-code">                                 next_v_value, done</p><p class="source-code">                        )</p></li>
				<li>In this step, we <a id="_idIndexMarker351"/>will implement the PPO <a id="_idIndexMarker352"/>algorithm's policy update steps. These happen inside the inner loop whenever enough of an agent's trajectory information is available in the form of sampled experience batches:<p class="source-code">                        actor_losses, critic_losses=[],[]</p><p class="source-code">                        for epoch in range(args.epochs):</p><p class="source-code">                            actor_loss =self.actor.train(</p><p class="source-code">                                old_policies, states,\</p><p class="source-code">                                actions, gaes</p><p class="source-code">                            )</p><p class="source-code">                            actor_losses.append(</p><p class="source-code">                                             actor_loss)</p><p class="source-code">                            critic_loss = self.critic.\</p><p class="source-code">                                train(states, td_targets)</p><p class="source-code">                            critic_losses.append(</p><p class="source-code">                                             critic_loss)</p><p class="source-code">                        # Plot mean actor &amp; critic losses </p><p class="source-code">                        # on every update</p><p class="source-code">                        tf.summary.scalar("actor_loss", </p><p class="source-code">                          np.mean(actor_losses), step=ep)</p><p class="source-code">                        tf.summary.scalar(</p><p class="source-code">                            "critic_loss", </p><p class="source-code">                             np.mean(critic_losses), </p><p class="source-code">                             step=ep</p><p class="source-code">                        )</p></li>
				<li>As the final <a id="_idIndexMarker353"/>step of the <strong class="source-inline">train</strong> method, we<a id="_idIndexMarker354"/> will reset the intermediate variables and print a summary of the episode reward obtained by the agent:<p class="source-code">                        state_batch = []</p><p class="source-code">                        action_batch = []</p><p class="source-code">                        reward_batch = []</p><p class="source-code">                        old_policy_batch = []</p><p class="source-code">                    episode_reward += reward[0][0]</p><p class="source-code">                    state = next_state[0]</p><p class="source-code">                print(f"Episode#{ep} \</p><p class="source-code">                        Reward:{episode_reward}")</p><p class="source-code">                tf.summary.scalar("episode_reward", \</p><p class="source-code">                                   episode_reward, \</p><p class="source-code">                                   step=ep)</p></li>
				<li>With that, our PPO agent implementation is complete, and we are ready to define our main function to start training!<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "Pendulum-v0"</p><p class="source-code">    env = gym.make(env_name)</p><p class="source-code">    agent = Agent(env)</p><p class="source-code">    agent.train(max_episodes=20000)</p></li>
			</ol>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor129"/>How it works…</h2>
			<p>The PPO<a id="_idIndexMarker355"/> algorithm uses clipping to <a id="_idIndexMarker356"/>form a surrogate loss function, and uses multiple <a id="_idIndexMarker357"/>epochs of <strong class="bold">Stochastic Gradient Decent/Ascent</strong> (<strong class="bold">SGD</strong>) optimization per the policy update. The clipping introduced by PPO reduces the effective change that can be applied to the policy, thereby improving the stability of the policy while learning.</p>
			<p>The PPO agent uses<a id="_idIndexMarker358"/> actor(s) to collect<a id="_idIndexMarker359"/> samples from the environment using the latest policy parameters. The loop defined in step 15 of the recipe samples a mini-batch of experience and trains the network for n epochs (passed as the <strong class="source-inline">--epoch</strong> argument to the script) using the clipped surrogate objective function. The process is then repeated with new samples of experiences.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor130"/>Implementing the Deep Deterministic Policy Gradient algorithm and DDPG agent</h1>
			<p><strong class="bold">Deterministic Policy Gradient (DPG)</strong> is a type<a id="_idIndexMarker360"/> of Actor-Critic RL algorithm that uses two neural networks: one for estimating the action value function, and the other for estimating the optimal target policy. The <strong class="bold">Deep Deterministic Policy Gradient</strong> (<strong class="bold">DDPG</strong>) agent<a id="_idIndexMarker361"/> builds upon the idea of DPG and is quite efficient compared to vanilla Actor-Critic agents due<a id="_idIndexMarker362"/> to the use <a id="_idIndexMarker363"/>of deterministic action policies. By completing this recipe, you will have access to a powerful agent that can be trained efficiently in a variety of RL environments.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor131"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python virtual environment and <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started!</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from collections import deque</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import Dense, Input, Lambda, concatenate</p>
			<p>Now we can begin.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor132"/>How to do it…</h2>
			<p>The following <a id="_idIndexMarker364"/>steps will help you to <a id="_idIndexMarker365"/>implement a complete DDPG agent from scratch using TensorFlow 2.x:</p>
			<ol>
				<li value="1">Let's first create an argument parser to handle command-line configuration inputs to the script:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch3-DDPG")</p><p class="source-code">parser.add_argument("--env", default="Pendulum-v0")</p><p class="source-code">parser.add_argument("--actor_lr", type=float, default=0.0005)</p><p class="source-code">parser.add_argument("--critic_lr", type=float, default=0.001)</p><p class="source-code">parser.add_argument("--batch_size", type=int, default=64)</p><p class="source-code">parser.add_argument("--tau", type=float, default=0.05)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.99)</p><p class="source-code">parser.add_argument("--train_start", type=int, default=2000)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>Let's create a Tensorboard logger to log useful statistics during the agent's training:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Let's <a id="_idIndexMarker366"/>now implement an<a id="_idIndexMarker367"/> experience replay memory:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self, capacity=10000):</p><p class="source-code">        self.buffer = deque(maxlen=capacity)</p><p class="source-code">    def store(self, state, action, reward, next_state,</p><p class="source-code">              done):</p><p class="source-code">        self.buffer.append([state, action, reward, </p><p class="source-code">                            next_state, done])</p><p class="source-code">    def sample(self):</p><p class="source-code">        sample = random.sample(self.buffer, </p><p class="source-code">                               args.batch_size)</p><p class="source-code">        states, actions, rewards, next_states, done = \</p><p class="source-code">                            map(np.asarray, zip(*sample))</p><p class="source-code">        states = np.array(states).reshape(</p><p class="source-code">                                     args.batch_size, -1)</p><p class="source-code">        next_states = np.array(next_states).\</p><p class="source-code">                          reshape(args.batch_size, -1)</p><p class="source-code">        return states, actions, rewards, next_states, \</p><p class="source-code">        done</p><p class="source-code">    def size(self):</p><p class="source-code">        return len(self.buffer)</p></li>
				<li>We <a id="_idIndexMarker368"/>can now focus on<a id="_idIndexMarker369"/> implementing the <strong class="source-inline">Actor</strong> class, which will contain a neural network-based policy to act:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    action_bound):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.action_bound = action_bound</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = tf.keras.optimizers.Adam(args.actor_lr)</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        return tf.keras.Sequential(</p><p class="source-code">            [</p><p class="source-code">                Input((self.state_dim,)),</p><p class="source-code">                Dense(32, activation="relu"),</p><p class="source-code">                Dense(32, activation="relu"),</p><p class="source-code">                Dense(self.action_dim, </p><p class="source-code">                      activation="tanh"),</p><p class="source-code">                Lambda(lambda x: x * self.action_bound),</p><p class="source-code">            ]</p><p class="source-code">        )</p></li>
				<li>To get an action from the actor given a state, let's define the <strong class="source-inline">get_action</strong> method:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        state = np.reshape(state, [1, self.state_dim])</p><p class="source-code">        return self.model.predict(state)[0]</p></li>
				<li>Next, we'll<a id="_idIndexMarker370"/> implement a<a id="_idIndexMarker371"/> predict function to return the predictions made by the actor network:<p class="source-code">    def predict(self, state):</p><p class="source-code">        return self.model.predict(state)</p></li>
				<li>As the final step in the <strong class="source-inline">Actor</strong> class implementation, let's define the <strong class="source-inline">train</strong> method:<p class="source-code">    def train(self, states, q_grads):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            grads = tape.gradient(</p><p class="source-code">                self.model(states), </p><p class="source-code">                self.model.trainable_variables, -q_grads</p><p class="source-code">            )</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p></li>
				<li>With the <strong class="source-inline">Actor</strong> class defined, we can move on to define the <strong class="source-inline">Critic</strong> class:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim, action_dim):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.model = self.nn_model()</p><p class="source-code">        self.opt = \</p><p class="source-code">            tf.keras.optimizers.Adam(args.critic_lr)</p><p class="source-code">    def nn_model(self):</p><p class="source-code">        state_input = Input((self.state_dim,))</p><p class="source-code">        s1 = Dense(64, activation="relu")(state_input)</p><p class="source-code">        s2 = Dense(32, activation="relu")(s1)</p><p class="source-code">        action_input = Input((self.action_dim,))</p><p class="source-code">        a1 = Dense(32, activation="relu")(action_input)</p><p class="source-code">        c1 = concatenate([s2, a1], axis=-1)</p><p class="source-code">        c2 = Dense(16, activation="relu")(c1)</p><p class="source-code">        output = Dense(1, ac<a id="_idTextAnchor133"/>tivation="linear")(c2)</p><p class="source-code">        return tf.keras.Model([state_input, </p><p class="source-code">                               action_input], output)</p></li>
				<li>In <a id="_idIndexMarker372"/>this step, we will<a id="_idIndexMarker373"/> be implementing a method to calculate the gradients of the Q function:<p class="source-code">    def q_gradients(self, states, actions):</p><p class="source-code">        actions = tf.convert_to_tensor(actions)</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            tape.watch(actions)</p><p class="source-code">            q_values = self.model([states, actions])</p><p class="source-code">            q_values = tf.squeeze(q_values)</p><p class="source-code">        return tape.gradient(q_values, actions)</p></li>
				<li>As a <a id="_idIndexMarker374"/>convenience <a id="_idIndexMarker375"/>method, let's also define a <strong class="source-inline">predict</strong> function to return the critic network's prediction:<p class="source-code">    def predict(self, inputs):</p><p class="source-code">        return self.model.predict(inputs)</p></li>
				<li>Next, let's define the <strong class="source-inline">train</strong> method and a <strong class="source-inline">compute_loss</strong> method to train the critic:<p class="source-code">    def train(self, states, actions, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model([states, actions],</p><p class="source-code">                                  training=True)</p><p class="source-code">            assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, </p><p class="source-code">                            tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>It is <a id="_idIndexMarker376"/>now time to<a id="_idIndexMarker377"/> implement the DDPG <strong class="source-inline">Agent</strong> class:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = \</p><p class="source-code">            self.env.observation_space.shape[0]</p><p class="source-code">        self.action_dim = self.env.action_space.shape[0]</p><p class="source-code">        self.action_bound = self.env.action_space.high[0]</p><p class="source-code">        self.buffer = ReplayBuffer()</p><p class="source-code">        self.actor = Actor(self.state_dim, \</p><p class="source-code">                           self.action_dim, </p><p class="source-code">                           self.action_bound)</p><p class="source-code">        self.critic = Critic(self.state_dim, </p><p class="source-code">                             self.action_dim)</p><p class="source-code">        self.target_actor = Actor(self.state_dim, </p><p class="source-code">                                  self.action_dim, </p><p class="source-code">                                  self.action_bound)</p><p class="source-code">        self.target_critic = Critic(self.state_dim, </p><p class="source-code">                                   self.action_dim)</p><p class="source-code">        actor_weights = self.actor.model.get_weights()</p><p class="source-code">        critic_weights = self.critic.model.get_weights()</p><p class="source-code">        self.target_actor.model.set_weights(</p><p class="source-code">                                           actor_weights)</p><p class="source-code">        self.target_critic.model.set_weights(</p><p class="source-code">                                          critic_weights)</p></li>
				<li>Let's now <a id="_idIndexMarker378"/>implement<a id="_idIndexMarker379"/> the <strong class="source-inline">update_target</strong> method to update the <a id="_idIndexMarker380"/>actor and critic<a id="_idIndexMarker381"/> network's weights with that of the respective target networks:<p class="source-code">    def update_target(self):</p><p class="source-code">        actor_weights = self.actor.model.get_weights()</p><p class="source-code">        t_actor_weights = \</p><p class="source-code">            self.target_actor.model.get_weights()</p><p class="source-code">        critic_weights = self.critic.model.get_weights()</p><p class="source-code">        t_critic_weights = \</p><p class="source-code">            self.target_critic.model.get_weights()</p><p class="source-code">        for i in range(len(actor_weights)):</p><p class="source-code">            t_actor_weights[i] = (</p><p class="source-code">                args.tau * actor_weights[i] + \</p><p class="source-code">                (1 - args.tau) * t_actor_weights[i]</p><p class="source-code">            )</p><p class="source-code">        for i in range(len(critic_weights)):</p><p class="source-code">            t_critic_weights[i] = (</p><p class="source-code">                args.tau * critic_weights[i] + \</p><p class="source-code">                (1 - args.tau) * t_critic_weights[i]</p><p class="source-code">            )</p><p class="source-code">        self.target_actor.model.set_weights(</p><p class="source-code">                                         t_actor_weights)</p><p class="source-code">        self.target_critic.model.set_weights(</p><p class="source-code">                                        t_critic_weights)</p></li>
				<li>Next, let's <a id="_idIndexMarker382"/>implement a <a id="_idIndexMarker383"/>helper method to calculate the TD targets:<p class="source-code">    def get_td_target(self, rewards, q_values, dones):</p><p class="source-code">        targets = np.asarray(q_values)</p><p class="source-code">        for i in range(q_values.shape[0]):</p><p class="source-code">            if dones[i]:</p><p class="source-code">                targets[i] = rewards[i]</p><p class="source-code">            else:</p><p class="source-code">                targets[i] = args.gamma * q_values[i]</p><p class="source-code">        return targets</p></li>
				<li>The purpose of the Deterministic Policy Gradient algorithm is to add noise to the actions sampled from the deterministic policy. Let's use the <strong class="bold">Ornstein-Uhlenback</strong> (<strong class="bold">OU</strong>) process<a id="_idIndexMarker384"/> to generate noise:<p class="source-code">    def add_ou_noise(self, x, rho=0.15, mu=0, dt=1e-1,</p><p class="source-code">     sigma=0.2, dim=1):</p><p class="source-code">        return (</p><p class="source-code">            x + rho * (mu - x) * dt + sigma * \</p><p class="source-code">            np.sqrt(dt) * np.random.normal(size=dim)</p><p class="source-code">        )</p></li>
				<li>In <a id="_idIndexMarker385"/>this step, we will use<a id="_idIndexMarker386"/> experience replay to update the actor and critic:<p class="source-code">    def replay_experience(self):</p><p class="source-code">        for _ in range(10):</p><p class="source-code">            states, actions, rewards, next_states, \</p><p class="source-code">                dones = self.buffer.sample()</p><p class="source-code">            target_q_values = self.target_critic.predict(</p><p class="source-code">                [next_states, self.target_actor.\</p><p class="source-code">                 predict(next_states)]</p><p class="source-code">            )</p><p class="source-code">            td_targets = self.get_td_target(rewards,</p><p class="source-code">                                  target_q_values, dones)</p><p class="source-code">            self.critic.train(states, actions, </p><p class="source-code">                              td_targets)</p><p class="source-code">            s_actions = self.actor.predict(states)</p><p class="source-code">            s_grads = self.critic.q_gradients(states, </p><p class="source-code">                                              s_actions)</p><p class="source-code">            grads = np.array(s_grads).reshape((-1, </p><p class="source-code">                                        self.action_dim))</p><p class="source-code">            self.actor.train(states, grads)</p><p class="source-code">            self.update_target()</p></li>
				<li>With all the<a id="_idIndexMarker387"/> components we <a id="_idIndexMarker388"/>have implemented, we are now ready to put them together in the <strong class="source-inline">train</strong> method:<p class="source-code">    def train(self, max_episodes=1000):</p><p class="source-code">        with writer.as_default():</p><p class="source-code">            for ep in range(max_episodes):</p><p class="source-code">                episode_reward, done = 0, False</p><p class="source-code">                state = self.env.reset()</p><p class="source-code">                bg_noise = np.zeros(self.action_dim)</p><p class="source-code">                while not done:</p><p class="source-code">                    # self.env.render()</p><p class="source-code">                    action = self.actor.get_action(state)</p><p class="source-code">                    noise = self.add_ou_noise(bg_noise, \</p><p class="source-code">                                     dim=self.action_dim)</p><p class="source-code">                    action = np.clip(</p><p class="source-code">                        action + noise, -self.action_\</p><p class="source-code">                          bound, self.action_bound</p><p class="source-code">                    )</p><p class="source-code">                    next_state, reward, done, _ = \</p><p class="source-code">                                   self.env.step(action)</p><p class="source-code">                    self.buffer.store(state, action, \</p><p class="source-code">                      (reward + 8) / 8, next_state, done)</p><p class="source-code">                    bg_noise = noise</p><p class="source-code">                    episode_reward += reward</p><p class="source-code">                    state = next_state</p><p class="source-code">                if (</p><p class="source-code">                    self.buffer.size() &gt;= args.batch_size</p><p class="source-code">                    and self.buffer.size() &gt;= \</p><p class="source-code">                        args.train_start</p><p class="source-code">                ):</p><p class="source-code">                    self.replay_experience()</p><p class="source-code">                print(f"Episode#{ep} \</p><p class="source-code">                        Reward:{episode_reward}")</p><p class="source-code">                tf.summary.scalar("episode_reward", </p><p class="source-code">                                 episode_reward, step=ep)</p></li>
				<li>With that, our <a id="_idIndexMarker389"/>DDPG agent implementation is complete, and we are ready to define our<a id="_idIndexMarker390"/> main function to start training!<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "Pendulum-v0"</p><p class="source-code">    env = gym.make(env_name)</p><p class="source-code">    agent = Agent(env)</p><p class="source-code">    agent.train(max_episodes=20000)</p></li>
			</ol>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor134"/>How it works…</h2>
			<p>The DDPG agent estimates two quantities – the Q-value function and the optimal policy. DDPG combines the ideas introduced in DQN and DPG. DDPG uses a policy gradient update rule in addition to the ideas introduced in DQN, as can be seen in the update steps defined in step 14.</p>
		</div>
</body></html>