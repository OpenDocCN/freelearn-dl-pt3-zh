["```\n$> pip install Pillow tqdm\n```", "```\n    import glob\n    import os\n    import pathlib\n    import h5py\n    import numpy as np\n    import sklearn.utils as skutils\n    from sklearn.preprocessing import LabelEncoder\n    from tensorflow.keras.applications import imagenet_utils\n    from tensorflow.keras.applications.vgg16 import VGG16\n    from tensorflow.keras.preprocessing.image import *\n    from tqdm import tqdm\n    ```", "```\n    class FeatureExtractor(object):\n        def __init__(self,\n                     model,\n                     input_size,\n                     label_encoder,\n                     num_instances,\n                     feature_size,\n                     output_path,\n                     features_key='features',\n                     buffer_size=1000):\n    ```", "```\n            if os.path.exists(output_path):\n                error_msg = (f'{output_path} already \n                               exists. '\n                             f'Please delete it and try \n                              again.')\n                raise FileExistsError(error_msg)\n    ```", "```\n            self.model = model\n            self.input_size = input_size\n            self.le = label_encoder\n            self.feature_size = feature_size\n            self.buffer_size = buffer_size\n            self.buffer = {'features': [], 'labels': []}\n            self.current_index = 0\n    ```", "```\n            self.db = h5py.File(output_path, 'w')\n            self.features = self.db.create_dataset(features_      key,\n                                           (num_instances,\n                                             feature_size),\n                                             dtype='float')\n\n    self.labels = self.db.create_dataset('labels',\n                                      (num_instances,),\n                                      dtype='int')\n    ```", "```\n        def extract_features(self,\n                           image_paths,\n                           labels,\n                           batch_size=64,\n                           shuffle=True):\n            if shuffle:\n                image_paths, labels = \n                skutils.shuffle(image_paths,\n                               labels)\n            encoded_labels = self.le.fit_transform(labels)\n            self._store_class_labels(self.le.classes_)\n    ```", "```\n            for i in tqdm(range(0, len(image_paths), \n                                batch_size)):\n                batch_paths = image_paths[i: i + \n                                          batch_size]\n                batch_labels = encoded_labels[i:i + \n                                             batch_size]\n                batch_images = []\n                for image_path in batch_paths:\n                    image = load_img(image_path,\n\n                            target_size=self.input_size)\n                    image = img_to_array(image)\n                    image = np.expand_dims(image, axis=0)\n                    image = \n                   imagenet_utils.preprocess_input(image)\n                    batch_images.append(image)\n                batch_images = np.vstack(batch_images)\n                feats = self.model.predict(batch_images,\n                                    batch_size=batch_size)\n                new_shape = (feats.shape[0], \n                            self.feature_size)\n                feats = feats.reshape(new_shape)\n                self._add(feats, batch_labels)\n            self._close()\n    ```", "```\n        def _add(self, rows, labels):\n            self.buffer['features'].extend(rows)\n            self.buffer['labels'].extend(labels)\n            if len(self.buffer['features']) >= \n                                   self.buffer_size:\n                self._flush()\n    ```", "```\n        def _flush(self):\n            next_index = (self.current_index +\n                          len(self.buffer['features']))\n            buffer_slice = slice(self.current_index, \n                                next_index)\n            self.features[buffer_slice] = \n                           self.buffer['features']\n            self.labels[buffer_slice] = self.buffer['labels']\n            self.current_index = next_index\n            self.buffer = {'features': [], 'labels': []}\n    ```", "```\n        def _store_class_labels(self, class_labels):\n            data_type = h5py.special_dtype(vlen=str)\n            shape = (len(class_labels),)\n            label_ds = self.db.create_dataset('label_names',\n                          shape,\n                        dtype=data_type)\n            label_ds[:] = class_labels\n    ```", "```\n        def _close(self):\n            if len(self.buffer['features']) > 0:\n                self._flush()\n            self.db.close()\n    ```", "```\n    files_pattern = (pathlib.Path.home() / '.keras' / \n                    'datasets' /'car_ims' / '*.jpg')\n    files_pattern = str(files_pattern)\n    input_paths = [*glob.glob(files_pattern)]\n    ```", "```\n    output_path = (pathlib.Path.home() / '.keras' / \n                  'datasets' /\n                   'car_ims_rotated')\n    if not os.path.exists(str(output_path)):\n        os.mkdir(str(output_path))\n    ```", "```\n    labels = []\n    output_paths = []\n    for index in tqdm(range(len(input_paths))):\n        image_path = input_paths[index]\n        image = load_img(image_path)\n        rotation_angle = np.random.choice([0, 90, 180, 270])\n        rotated_image = image.rotate(rotation_angle)\n        rotated_image_path = str(output_path / \n                              f'{index}.jpg')\n        rotated_image.save(rotated_image_path, 'JPEG')\n        output_paths.append(rotated_image_path)\n        labels.append(rotation_angle)\n        image.close()\n        rotated_image.close()\n    ```", "```\n    features_path = str(output_path / 'features.hdf5')\n    model = VGG16(weights='imagenet', include_top=False)\n    fe = FeatureExtractor(model=model,\n                          input_size=(224, 224, 3),\n                          label_encoder=LabelEncoder(),\n                          num_instances=len(input_paths),\n                          feature_size=512 * 7 * 7,\n                          output_path=features_path)\n    ```", "```\n    fe.extract_features(image_paths=output_paths, \n                        labels=labels)\n    ```", "```\n    import pathlib\n    import h5py\n    from sklearn.linear_model import LogisticRegressionCV\n    from sklearn.metrics import classification_report\n    ```", "```\n    dataset_path = str(pathlib.Path.home()/'.keras'/'datasets'/'car_ims_rotated'/'features.hdf5')\n    db = h5py.File(dataset_path, 'r')\n    ```", "```\n    SUBSET_INDEX = int(db['labels'].shape[0] * 0.5)\n    features = db['features'][:SUBSET_INDEX]\n    labels = db['labels'][:SUBSET_INDEX]\n    ```", "```\n    TRAIN_PROPORTION = 0.8\n    SPLIT_INDEX = int(len(labels) * TRAIN_PROPORTION)\n    X_train, y_train = (features[:SPLIT_INDEX],\n                        labels[:SPLIT_INDEX])\n    X_test, y_test = (features[SPLIT_INDEX:],\n                      labels[SPLIT_INDEX:])\n    ```", "```\n    model = LogisticRegressionCV(n_jobs=-1)\n    model.fit(X_train, y_train)\n    ```", "```\n    predictions = model.predict(X_test)\n    report = classification_report(y_test, predictions,\n                           target_names=db['label_names'])\n    print(report)\n    ```", "```\n                  precision    recall  f1-score   support\n               0       1.00      1.00      1.00       404\n              90       0.98      0.99      0.99       373\n             180       0.99      1.00      1.00       409\n             270       1.00      0.98      0.99       433\n        accuracy                           0.99      1619\n       macro avg       0.99      0.99      0.99      1619\n    weighted avg       0.99      0.99      0.99      1619\n    ```", "```\n    db.close()                    \n    ```", "```\n$> pip install Pillow tqdm\n```", "```\n    import json\n    import os\n    import pathlib\n    from glob import glob\n    import h5py\n    from sklearn.ensemble import *\n    from sklearn.linear_model import *\n    from sklearn.metrics import accuracy_score\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.svm import LinearSVC\n    from sklearn.tree import *\n    from tensorflow.keras.applications import *\n    from tqdm import tqdm\n    from ch3.recipe1.feature_extractor import FeatureExtractor\n    ```", "```\n    INPUT_SIZE = (224, 224, 3)\n    ```", "```\n    def get_pretrained_networks():\n        return [\n            (VGG16(input_shape=INPUT_SIZE,\n                   weights='imagenet',\n                   include_top=False),\n             7 * 7 * 512),\n            (VGG19(input_shape=INPUT_SIZE,\n                   weights='imagenet',\n                   include_top=False),\n             7 * 7 * 512),\n            (Xception(input_shape=INPUT_SIZE,\n                      weights='imagenet',\n                      include_top=False),\n             7 * 7 * 2048),\n            (ResNet152V2(input_shape=INPUT_SIZE,\n                         weights='imagenet',\n                         include_top=False),\n             7 * 7 * 2048),\n            (InceptionResNetV2(input_shape=INPUT_SIZE,\n                               weights='imagenet',\n                               include_top=False),\n             5 * 5 * 1536)\n        ]\n    ```", "```\n    def get_classifiers():\n        models = {}\n        models['LogisticRegression'] = \n                                LogisticRegression()\n        models['SGDClf'] = SGDClassifier()\n        models['PAClf'] = PassiveAggressiveClassifier()\n        models['DecisionTreeClf'] = \n                             DecisionTreeClassifier()\n        models['ExtraTreeClf'] = ExtraTreeClassifier()\n        n_trees = 100\n        models[f'AdaBoostClf-{n_trees}'] = \\\n            AdaBoostClassifier(n_estimators=n_trees)\n        models[f'BaggingClf-{n_trees}'] = \\\n            BaggingClassifier(n_estimators=n_trees)\n        models[f'RandomForestClf-{n_trees}'] = \\\n            RandomForestClassifier(n_estimators=n_trees)\n        models[f'ExtraTreesClf-{n_trees}'] = \\\n            ExtraTreesClassifier(n_estimators=n_trees)\n        models[f'GradientBoostingClf-{n_trees}'] = \\\n            GradientBoostingClassifier(n_estimators=n_trees)\n        number_of_neighbors = range(3, 25)\n        for n in number_of_neighbors:\n            models[f'KNeighborsClf-{n}'] = \\\n                KNeighborsClassifier(n_neighbors=n)\n        reg = [1e-3, 1e-2, 1, 10]\n        for r in reg:\n            models[f'LinearSVC-{r}'] = LinearSVC(C=r)\n            models[f'RidgeClf-{r}'] = \n                  RidgeClassifier(alpha=r)\n        print(f'Defined {len(models)} models.')\n        return models\n    ```", "```\n    dataset_path = (pathlib.Path.home() / '.keras' / \n                   'datasets' 'flowers17')\n    files_pattern = (dataset_path / 'images' / '*' / '*.jpg')\n    images_path = [*glob(str(files_pattern))]\n    ```", "```\n    labels = []\n    for index in tqdm(range(len(images_path))):\n        image_path = images_path[index]\n        label = image_path.split(os.path.sep)[-2]\n        labels.append(label)\n    ```", "```\n    final_report = {}\n    best_model = None\n    best_accuracy = -1\n    best_features = None\n    ```", "```\n    for model, feature_size in get_pretrained_networks():\n        output_path = dataset_path / f'{model.name}_features.hdf5'\n        output_path = str(output_path)\n        fe = FeatureExtractor(model=model,\n                              input_size=INPUT_SIZE,\n                              label_encoder=LabelEncoder(),\n                              num_instances=len(images_path),\n                              feature_size=feature_size,\n                              output_path=output_path)\n        fe.extract_features(image_paths=images_path,\n                            labels=labels)\n    ```", "```\n        db = h5py.File(output_path, 'r')\n        TRAIN_PROPORTION = 0.8\n        SPLIT_INDEX = int(len(labels) * TRAIN_PROPORTION)\n        X_train, y_train = (db['features'][:SPLIT_INDEX],\n                            db['labels'][:SPLIT_INDEX])\n        X_test, y_test = (db['features'][SPLIT_INDEX:],\n                          db['labels'][SPLIT_INDEX:])\n        classifiers_report = {\n            'extractor': model.name\n        }\n        print(f'Spot-checking with features from \n              {model.name}')\n    ```", "```\n        for clf_name, clf in get_classifiers().items():\n            try:\n                clf.fit(X_train, y_train)\n            except Exception as e:\n                print(f'\\t{clf_name}: {e}')\n                continue\n            predictions = clf.predict(X_test)\n            accuracy = accuracy_score(y_test, predictions)\n            print(f'\\t{clf_name}: {accuracy}')\n            classifiers_report[clf_name] = accuracy\n    ```", "```\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_model = clf_name\n                best_features = model.name\n    ```", "```\n        final_report[output_path] = classifiers_report\n        db.close()\n    ```", "```\n    final_report['best_model'] = best_model\n    final_report['best_accuracy'] = best_accuracy\n    final_report['best_features'] = best_features\n    with open('final_report.json', 'w') as f:\n        json.dump(final_report, f)\n    ```", "```\n$> pip install creme==0.5.1\n```", "```\n    import pathlib\n    import h5py\n    from creme import stream\n    from creme.linear_model import LogisticRegression\n    from creme.metrics import Accuracy\n    from creme.multiclass import OneVsRestClassifier\n    from creme.preprocessing import StandardScaler\n    ```", "```\n    def write_dataset(output_path, feats, labels, \n                      batch_size):\n        feature_size = feats.shape[1]\n        csv_columns = ['class'] + [f'feature_{i}'\n                                   for i in range(feature_        size)]\n    ```", "```\n        dataset_size = labels.shape[0]\n        with open(output_path, 'w') as f:\n            f.write(f'{“,”.join(csv_columns)}\\n')\n    ```", "```\n            for batch_number, index in \\\n                    enumerate(range(0, dataset_size, \n                              batch_size)):\n                print(f'Processing batch {batch_number + \n                                          1} of '\n                      f'{int(dataset_size / \n                      float(batch_size))}')\n                batch_feats = feats[index: index + \n                                     batch_size]\n                batch_labels = labels[index: index + \n                                      batch_size]\n    ```", "```\n                for label, vector in \\\n                        zip(batch_labels, batch_feats):\n                    vector = ','.join([str(v) for v in \n                                       vector])\n                    f.write(f'{label},{vector}\\n')\n    ```", "```\n    dataset_path = str(pathlib.Path.home()/'.keras'/'datasets'/'car_ims_rotated'/'features.hdf5')\n    db = h5py.File(dataset_path, 'r')\n    ```", "```\n    TRAIN_PROPORTION = 0.8\n    SPLIT_INDEX = int(db['labels'].shape[0] * \n                      TRAIN_PROPORTION) \n    ```", "```\n    BATCH_SIZE = 256\n    write_dataset('train.csv',\n                  db['features'][:SPLIT_INDEX],\n                  db['labels'][:SPLIT_INDEX],\n                  BATCH_SIZE)\n    write_dataset('test.csv',\n                  db['features'][SPLIT_INDEX:],\n                  db['labels'][SPLIT_INDEX:],\n                  BATCH_SIZE)\n    ```", "```\n    FEATURE_SIZE = db['features'].shape[1]\n    types = {f'feature_{i}': float for i in range(FEATURE_SIZE)}\n    types['class'] = int\n    ```", "```\n    model = StandardScaler()\n    model |= OneVsRestClassifier(LogisticRegression())\n    ```", "```\n    metric = Accuracy()\n    dataset = stream.iter_csv('train.csv',\n                              target_name='class',\n                              converters=types)\n    ```", "```\n    print('Training started...')\n    for i, (X, y) in enumerate(dataset):\n        predictions = model.predict_one(X)\n        model = model.fit_one(X, y)\n        metric = metric.update(y, predictions)\n        if i % 100 == 0:\n            print(f'Update {i} - {metric}')\n    print(f'Final - {metric}')\n    ```", "```\n    metric = Accuracy()\n    test_dataset = stream.iter_csv('test.csv',\n                                   target_name='class',\n                                   converters=types)\n    ```", "```\n    print('Testing model...')\n    for i, (X, y) in enumerate(test_dataset):\n        predictions = model.predict_one(X)\n        metric = metric.update(y, predictions)\n        if i % 1000 == 0:\n            print(f'(TEST) Update {i} - {metric}')\n    print(f'(TEST) Final - {metric}')\n    ```", "```\n$> pip install Pillow\n```", "```\n    import os\n    import pathlib\n    from glob import glob\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Model\n    from tensorflow.keras.applications import VGG16\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.optimizers import *\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    SEED = 999\n    ```", "```\n    def build_network(base_model, classes):\n        x = Flatten()(base_model.output)\n        x = Dense(units=256)(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.5)(x)\n        x = Dense(units=classes)(x)\n        output = Softmax()(x)\n        return output\n    ```", "```\n    def load_images_and_labels(image_paths,\n                               target_size=(256, 256)):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, \n                             target_size=target_size)\n            image = img_to_array(image)\n            label = image_path.split(os.path.sep)[-2]\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    dataset_path = (pathlib.Path.home() / '.keras' / \n                   'datasets' /'flowers17')\n    files_pattern = (dataset_path / 'images' / '*' / '*.jpg')\n    image_paths = [*glob(str(files_pattern))]\n    CLASSES = {p.split(os.path.sep)[-2] for p in \n               image_paths}\n    ```", "```\n    X, y = load_images_and_labels(image_paths)\n    X = X.astype('float') / 255.0\n    y = LabelBinarizer().fit_transform(y)\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y,\n                                         test_size=0.2,\n\n                                      random_state=SEED)\n    ```", "```\n    base_model = VGG16(weights='imagenet',\n                       include_top=False,\n                       input_tensor=Input(shape=(256, 256, \n                                                  3)))\n    ```", "```\n    for layer in base_model.layers:\n        layer.trainable = False\n    ```", "```\n    model = build_network(base_model, len(CLASSES))\n    model = Model(base_model.input, model)\n    ```", "```\n    BATCH_SIZE = 64\n    augmenter = ImageDataGenerator(rotation_range=30,\n                                   horizontal_flip=True,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   fill_mode='nearest')\n    train_generator = augmenter.flow(X_train, y_train, \n                                     BATCH_SIZE)\n    ```", "```\n    WARMING_EPOCHS = 20\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=RMSprop(lr=1e-3),\n                  metrics=['accuracy'])\n    model.fit(train_generator,\n              steps_per_epoch=len(X_train) // BATCH_SIZE,\n              validation_data=(X_test, y_test),\n              epochs=WARMING_EPOCHS)\n    result = model.evaluate(X_test, y_test)\n    print(f'Test accuracy: {result[1]}')\n    ```", "```\n    for layer in base_model.layers[15:]:\n        layer.trainable = True\n    EPOCHS = 50\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=SGD(lr=1e-3),\n                  metrics=['accuracy'])\n    model.fit(train_generator,\n              steps_per_epoch=len(X_train) // BATCH_SIZE,\n              validation_data=(X_test, y_test),\n              epochs=EPOCHS)\n    result = model.evaluate(X_test, y_test)\n    print(f'Test accuracy: {result[1]}')\n    ```", "```\n$> pip install tensorflow-hub Pillow\n```", "```\n    import os\n    import pathlib\n    from glob import glob\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.optimizers import RMSprop\n    from tensorflow.keras.preprocessing.image import *\n    from tensorflow_hub import KerasLayer\n    ```", "```\n    SEED = 999\n    ```", "```\n    def build_network(base_model, classes):\n        return Sequential([\n            base_model,\n            Dense(classes),\n            Softmax()\n        ])\n    ```", "```\n    def load_images_and_labels(image_paths,\n                               target_size=(256, 256)):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, \n                             target_size=target_size)\n            image = img_to_array(image)\n            label = image_path.split(os.path.sep)[-2]\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    dataset_path = (pathlib.Path.home() / '.keras' / \n                     'datasets' /'flowers17')\n    files_pattern = (dataset_path / 'images' / '*' / '*.jpg')\n    image_paths = [*glob(str(files_pattern))]\n    CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}\n    ```", "```\n    X, y = load_images_and_labels(image_paths)\n    X = X.astype('float') / 255.0\n    y = LabelBinarizer().fit_transform(y)\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y,\n                                         test_size=0.2,\n                                       random_state=SEED)\n    ```", "```\n    model_url = ('https://tfhub.dev/google/imagenet/'\n                 'resnet_v1_152/feature_vector/4')\n    base_model = KerasLayer(model_url, input_shape=(256, \n                                                  256, 3))\n    ```", "```\n    base_model.trainable = False\n    ```", "```\n    model = build_network(base_model, len(CLASSES))\n    ```", "```\n    BATCH_SIZE = 32\n    augmenter = ImageDataGenerator(rotation_range=30,\n                                   horizontal_flip=True,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   fill_mode='nearest')\n    train_generator = augmenter.flow(X_train, y_train, \n                                     BATCH_SIZE)\n    ```", "```\n    EPOCHS = 20\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=RMSprop(lr=1e-3),\n                  metrics=['accuracy'])\n    model.fit(train_generator,\n              steps_per_epoch=len(X_train) // BATCH_SIZE,\n              validation_data=(X_test, y_test),\n              epochs=EPOCHS)\n    result = model.evaluate(X_test, y_test)\n    print(f'Test accuracy: {result[1]}')\n    ```"]