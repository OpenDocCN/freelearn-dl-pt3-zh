["```\nregressor = Sequential()\nregressor.add(LSTM(units= 50, activation = 'relu', \\\n                   return_sequences = True, \\\n                   input_shape = (X_train.shape[1], 5)))\nregressor.add(Dropout(0.2))\nregressor.add(LSTM(units= 60, activation = 'relu', \\\n                   return_sequences = True))\nregressor.add(Dropout(0.3))\nregressor.add(LSTM(units= 80, activation = 'relu', \\\n              return_sequences = True))\nregressor.add(Dropout(0.4))\nregressor.add(LSTM(units= 120, activation = 'relu'))\nregressor.add(Dropout(0.5))\nregressor.add(Dense(units = 1))\n```", "```\ndef clean_text(txt):\n    txt = \"\".join(v for v in txt \\\n                  if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \ncorpus = [clean_text(x) for x in all_headlines]\ncorpus[:10]\n```", "```\nfrom keras.preprocessing.text import Tokenizer\n```", "```\ntokenizer = Tokenizer()\ndef get_seq_of_tokens(corpus):\n    tokenizer.fit_on_texts(corpus)\n    all_words = len(tokenizer.word_index) + 1\n\n    input_seq = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_seq.append(n_gram_sequence)\n    return input_seq, all_words\nyour_sequences, all_words = get_seq_of_tokens(corpus)\nyour_sequences[:10]\n```", "```\ndef generate_padded_sequences(input_seq):\n    max_sequence_len = max([len(x) for x in input_seq])\n    input_seq = np.array(pad_sequences\\\n                         (input_seq, maxlen=max_sequence_len, \\\n                          padding='pre'))\n\n    predictors, label = input_seq[:,:-1],input_seq[:,-1]\n    label = keras.utils.to_categorical(label, num_classes=all_words)\n    return predictors, label, max_sequence_len\npredictors, label, max_sequence_len = generate_padded_sequences\\\n                                      (your_sequences)\n```", "```\n    from keras.preprocessing.sequence import pad_sequences\n    from keras.models import Sequential\n    from keras.layers import Embedding, LSTM, Dense, Dropout\n    import tensorflow.keras.utils as ku \n    from keras.preprocessing.text import Tokenizer\n    import pandas as pd\n    import numpy as np\n    from keras.callbacks import EarlyStopping\n    import string, os \n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n    ```", "```\n    Using TensorFlow backend.\n    ```", "```\n    your_dir = 'content/'\n    your_headlines = []\n    for filename in os.listdir(your_dir):\n        if 'Articles' in filename:\n            article_df = pd.read_csv(your_dir + filename)\n            your_headlines.extend(list(article_df.headline.values))\n            break\n    your_headlines = [h for h in your_headlines if h != \"Unknown\"]\n    len(our_headlines)\n    ```", "```\n    831\n    ```", "```\n    def clean_text(txt):\n        txt = \"\".join(v for v in txt \\\n                      if v not in string.punctuation).lower()\n        txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n        return txt \n    corpus = [clean_text(x) for x in all_headlines]\n    corpus[60:80]\n    ```", "```\n    tokenizer = Tokenizer()\n    def get_seq_of_tokens(corpus):\n        tokenizer.fit_on_texts(corpus)\n        all_words = len(tokenizer.word_index) + 1\n\n        input_seq = []\n        for line in corpus:\n            token_list = tokenizer.texts_to_sequences([line])[0]\n            for i in range(1, len(token_list)):\n                n_gram_sequence = token_list[:i+1]\n                input_seq.append(n_gram_sequence)\n        return input_seq, all_words\n    your_sequences, all_words = get_seq_of_tokens(corpus)\n    your_sequences[:20]\n    ```", "```\n    def generate_padded_sequences(input_seq):\n        max_sequence_len = max([len(x) for x in input_seq])\n        input_seq = np.array(pad_sequences\\\n                             (input_seq, maxlen=max_sequence_len, \\\n                              padding='pre'))\n        predictors, label = input_seq[:,:-1],input_seq[:,-1]\n        label = ku.to_categorical(label, num_classes=all_words)\n        return predictors, label, max_sequence_len\n    predictors, label, \\\n    max_sequence_len = generate_padded_sequences(inp_seq)\n    ```", "```\n    def create_model(max_sequence_len, all_words):\n        input_len = max_sequence_len - 1\n        model = Sequential()\n\n        model.add(Embedding(all_words, 10, input_length=input_len))\n\n        model.add(LSTM(100))\n        model.add(Dropout(0.1))\n\n        model.add(Dense(all_words, activation='softmax'))\n        model.compile(loss='categorical_crossentropy', \\\n                      optimizer='adam')\n\n        return model\n    model = create_model(max_sequence_len, all_words)\n    model.summary()\n    ```", "```\n    model.fit(predictors, label, epochs=200, verbose=5)\n    ```", "```\n    def generate_text(seed_text, next_words, model, max_sequence_len):\n        for _ in range(next_words):\n            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n            token_list = pad_sequences([token_list], \\\n                                        maxlen = max_sequence_len-1, \\\n                                        padding='pre')\n            predicted = model.predict\\\n                        (token_list, verbose=0)\n            output_word = \"\"\n            for word,index in tokenizer.word_index.items():\n                if index == predicted.any():\n                    output_word = word\n                    break\n            seed_text += \" \"+output_word\n        return seed_text.title()\n    ```", "```\n    print (generate_text(\"10 ways\", 11, model, max_sequence_len))\n    print (generate_text(\"europe looks to\", 8, model, \\\n                         max_sequence_len))\n    print (generate_text(\"best way\", 10, model, max_sequence_len))\n    print (generate_text(\"homeless in\", 10, model, max_sequence_len))\n    print (generate_text(\"unexpected results\", 10, model,\\\n                         max_sequence_len))\n    print (generate_text(\"critics warn\", 10, model, \\\n                         max_sequence_len))\n    ```", "```\nmodel.add(Dense(n_outputs, activation='linear'))\n```", "```\ndef define_your_gen(latent_dim, n_outputs=2):\n    model = Sequential()\n    model.add(Dense(5, activation='relu', \\\n                    kernel_initializer='he_uniform', \\\n                    input_dim=latent_dim))\n    model.add(Dense(n_outputs, activation='linear'))\n    return model\n```", "```\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```\nmodel.compile(loss='binary_crossentropy', \\\n              optimizer='adam', metrics=['accuracy'])\n```", "```\ndef define_disc(n_inputs=2):\n    model = Sequential()\n    model.add(Dense(25, activation='relu', \\\n                    kernel_initializer='he_uniform', \\\n                    input_dim=n_inputs))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', \\\n                  optimizer='adam', metrics=['accuracy'])\n    return model\n```", "```\ndef define_your_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    return model\n```", "```\ndef generate_real(n):\n    X1 = rand(n) - 0.5\n    X2 = X1 * X1\n    X1 = X1.reshape(n, 1)\n    X2 = X2.reshape(n, 1)\n    X = hstack((X1, X2))\n    y = ones((n, 1))\n    return X, y\n```", "```\ndef gen_latent_points(latent_dim, n):\n    x_input = randn(latent_dim * n)\n    x_input = x_input.reshape(n, latent_dim)\n    return x_input\n```", "```\ndef gen_fake(generator, latent_dim, n):\n    x_input = gen_latent_points(latent_dim, n)\n    X = generator.predict(x_input)\n    y = zeros((n, 1))\n    return X, y\n```", "```\ndef performance_summary(epoch, generator, \\\n                        discriminator, latent_dim, n=100):\n    x_real, y_real = generate_real(n)\n    _, acc_real = discriminator.evaluate\\\n                  (x_real, y_real, verbose=0)\n    x_fake, y_fake = gen_fake\\\n                     (generator, latent_dim, n)\n    _, acc_fake = discriminator.evaluate\\\n                  (x_fake, y_fake, verbose=0)\n    print(epoch, acc_real, acc_fake)\n    plt.scatter(x_real[:, 0], x_real[:, 1], color='green')\n    plt.scatter(x_fake[:, 0], x_fake[:, 1], color='red')\n    plt.show()\n```", "```\ndef train(g_model, d_model, your_gan_model, \\\n          latent_dim, n_epochs=1000, n_batch=128, n_eval=100):\n    half_batch = int(n_batch / 2)\n    for i in range(n_epochs):\n        x_real, y_real = generate_real(half_batch)\n        x_fake, y_fake = gen_fake\\\n                         (g_model, latent_dim, half_batch)\n        d_model.train_on_batch(x_real, y_real)\n        d_model.train_on_batch(x_fake, y_fake)\n        x_gan = gen_latent_points(latent_dim, n_batch)\n        y_gan = ones((n_batch, 1))\n        your_gan_model.train_on_batch(x_gan, y_gan)\n        if (i+1) % n_eval == 0:\n            performance_summary(i, g_model, d_model, latent_dim)\n```", "```\nlatent_dim = 5\ngenerator = define_gen(latent_dim)\ndiscriminator = define_discrim()\nyour_gan_model = define_your_gan(generator, discriminator)\ntrain(generator, discriminator, your_gan_model, latent_dim)\n```", "```\n    from keras.models import Sequential\n    from numpy import hstack, zeros, ones\n    from numpy.random import rand, randn\n    from keras.layers import Dense\n    import matplotlib.pyplot as plt \n    ```", "```\n    def define_gen(latent_dim, n_outputs=2):\n        model = Sequential()\n        model.add(Dense(15, activation='relu', \\\n                  kernel_initializer='he_uniform', \\\n                  input_dim=latent_dim))\n        model.add(Dense(n_outputs, activation='linear'))\n        return model\n    ```", "```\n    def define_disc(n_inputs=2):\n        model = Sequential()\n        model.add(Dense(25, activation='relu', \\\n                        kernel_initializer='he_uniform', \\\n                        input_dim=n_inputs))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy', \\\n                      optimizer='adam', metrics=['accuracy'])\n        return model\n    ```", "```\n    def define_your_gan(generator, discriminator):\n        discriminator.trainable = False\n        model = Sequential()\n        model.add(generator)\n        model.add(discriminator)\n        model.compile(loss='binary_crossentropy', optimizer='adam')\n        return model\n    ```", "```\n    def generate_real(n):\n        X1 = rand(n) - 0.5\n        X2 = X1 * X1\n        X1 = X1.reshape(n, 1)\n        X2 = X2.reshape(n, 1)\n        X = hstack((X1, X2))\n        y = ones((n, 1))\n        return X, y\n    ```", "```\n    def gen_latent_points(latent_dim, n):\n        x_input = randn(latent_dim * n)\n        x_input = x_input.reshape(n, latent_dim)\n        return x_input\n    ```", "```\n    def gen_fake(generator, latent_dim, n):\n        x_input = gen_latent_points(latent_dim, n)\n        X = generator.predict(x_input)\n        y = zeros((n, 1))\n        return X, y\n    ```", "```\n    def performance_summary(epoch, generator, \\\n                            discriminator, latent_dim, n=100):\n        x_real, y_real = generate_real(n)\n        _, acc_real = discriminator.evaluate\\\n                      (x_real, y_real, verbose=0)\n        x_fake, y_fake = gen_fake\\\n                         (generator, latent_dim, n)\n        _, acc_fake = discriminator.evaluate\\\n                      (x_fake, y_fake, verbose=0)\n        print(epoch, acc_real, acc_fake)\n        plt.scatter(x_real[:, 0], x_real[:, 1], color='green')\n        plt.scatter(x_fake[:, 0], x_fake[:, 1], color='red')\n        plt.show()\n    ```", "```\n    def train(g_model, d_model, your_gan_model, \\\n              latent_dim, n_epochs=1000, \\\n              n_batch=128, n_eval=100):\n        half_batch = int(n_batch / 2)\n        for i in range(n_epochs):\n            x_real, y_real = generate_real(half_batch)\n            x_fake, y_fake = gen_fake\\\n                             (g_model, latent_dim, half_batch)\n            d_model.train_on_batch(x_real, y_real)\n            d_model.train_on_batch(x_fake, y_fake)\n            x_gan = gen_latent_points(latent_dim, n_batch)\n            y_gan = ones((n_batch, 1))\n            your_gan_model.train_on_batch(x_gan, y_gan)\n            if (i+1) % n_eval == 0:\n                performance_summary(i, g_model, d_model, latent_dim)\n    ```", "```\n    latent_dim = 5\n    generator = define_gen(latent_dim)\n    discriminator = define_disc()\n    your_gan_model = define_your_gan(generator, discriminator)\n    train(generator, discriminator, your_gan_model, latent_dim)\n    ```", "```\ngen_res = 3\ngen_square = 32 * gen_res\nimg_chan = 3\nimg_rows = 5\nimg_cols = 5\nimg_margin = 16\nseed_vector = 200\ndata_path = '/content/drive/MyDrive/Datasets\\\n            '/apple-or-tomato/training_set/'\nepochs = 1000\nnum_batch = 32\nnum_buffer = 1000\n```", "```\ndef create_generator(seed_size, channels):\n    model = Sequential()\n```", "```\nmodel.add(Reshape((4,4,256)))\n```", "```\nmodel.add(UpSampling2D())\n```", "```\nmodel.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n```", "```\nmodel.add(BatchNormalization(momentum=0.8))\n```", "```\nmodel.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\nmodel.add(Activation(\"tanh\"))\n```", "```\ndef create_generator(seed_size, channels):\n    model = Sequential()\n    model.add(Dense(4*4*256,activation=\"relu\", \\\n                    input_dim=seed_size))\n    model.add(Reshape((4,4,256)))\n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    model.add(UpSampling2D())\n    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    if gen_res>1:\n      model.add(UpSampling2D(size=(gen_res,gen_res)))\n      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n      model.add(BatchNormalization(momentum=0.8))\n      model.add(Activation(\"relu\"))\n    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n    model.add(Activation(\"tanh\"))\n    return model\n```", "```\ndef create_discriminator(image_shape):\n    model = Sequential()\n```", "```\nmodel.add(Conv2D(32, kernel_size=3, \\\n                 strides=2, input_shape=image_shape, \\\n                 padding=\"same\"))\nmodel.add(LeakyReLU(alpha=0.2))\n```", "```\nmodel.add(Flatten())\n```", "```\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```\ndef create_discriminator(image_shape):\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=3, strides=2, \\\n                     input_shape=image_shape, \n                     padding=\"same\"))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(64, kernel_size=3, strides=2, \\\n                     padding=\"same\"))\n    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, kernel_size=3, strides=2, \\\n                     padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(256, kernel_size=3, strides=1, \\\n                     padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(512, kernel_size=3, \\\n                     strides=1, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n```", "```\ncross_entropy = tf.keras.losses.BinaryCrossentropy()\ndef discrim_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), \\\n                              real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), \\\n                              fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\ndef gen_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), \\\n                         fake_output)\n```", "```\ngen_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\ndisc_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n```", "```\n@tf.function\ndef train_step(images):\n    seed = tf.random.normal([num_batch, seed_vector])\n    with tf.GradientTape() as gen_tape, \\\n              tf.GradientTape() as disc_tape:\n    gen_imgs = generator(seed, training=True)\n    real_output = discriminator(images, training=True)\n    fake_output = discriminator(gen_imgs, training=True)\n    g_loss = gen_loss(fake_output)\n    d_loss = discrim_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(\\\n        g_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(\\\n        d_loss, discriminator.trainable_variables)\n    gen_optimizer.apply_gradients(zip(\n        gradients_of_generator, generator.trainable_variables))\n    disc_optimizer.apply_gradients(zip(\n        gradients_of_discriminator, \n        discriminator.trainable_variables))\n    return g_loss,d_loss\n```", "```\ndef train(dataset, epochs):\n    fixed_seed = np.random.normal\\\n                (0, 1, (img_rows * img_cols, seed_vector))\n    start = time.time()\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        g_loss_list = []\n        d_loss_list = []\n        for image_batch in dataset:\n            t = train_step(image_batch)\n            g_loss_list.append(t[0])\n            d_loss_list.append(t[1])\n        generator_loss = sum(g_loss_list) / len(g_loss_list)\n        discriminator_loss = sum(d_loss_list) / len(d_loss_list)\n        epoch_elapsed = time.time()-epoch_start\n        print (f'Epoch {epoch+1}, gen loss={generator_loss}', \\\n               f'disc loss={discriminator_loss},'\\\n               f' {time_string(epoch_elapsed)}')\n        save_images(epoch,fixed_seed)\n    elapsed = time.time()-start\n    print (f'Training time: {time_string(elapsed)}')\n```", "```\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive', force_remount=True)\n        COLAB = True\n        print(\"Note: using Google CoLab\")\n        %tensorflow_version 2.x\n    except:\n        print(\"Note: not using Google CoLab\")\n        COLAB = False\n    ```", "```\n    Mounted at /content/drive\n    Note: using Google Colab\n    ```", "```\n    import tensorflow as tf\n    from tensorflow.keras.layers\n    import Input, Reshape, Dropout, Dense \n    from tensorflow.keras.layers\n    import Flatten, BatchNormalization\n    from tensorflow.keras.layers\n    import Activation, ZeroPadding2D\n    from tensorflow.keras.layers import LeakyReLU\n    from tensorflow.keras.layers import UpSampling2D, Conv2D\n    from tensorflow.keras.models\n    import Sequential, Model, load_model\n    from tensorflow.keras.optimizers import Adam\n    import zipfile\n    import numpy as np\n    from PIL import Image\n    from tqdm import tqdm\n    import os \n    import time\n    import matplotlib.pyplot as plt\n    from skimage.io import imread\n    ```", "```\n    def time_string(sec_elapsed):\n        hour = int(sec_elapsed / (60 * 60))\n        minute = int((sec_elapsed % (60 * 60)) / 60)\n        second = sec_elapsed % 60\n        return \"{}:{:>02}:{:>05.2f}\".format(hour, minute, second)\n    ```", "```\n    gen_res = 3 \n    gen_square = 32 * gen_res\n    img_chan = 3\n    img_rows = 5\n    img_cols = 5\n    img_margin = 16\n    seed_vector = 200\n    data_path = '/content/drive/MyDrive/Datasets'\\\n                '/apple-or-tomato/training_set/'\n    epochs = 5000\n    num_batch = 32\n    num_buffer = 60000\n    print(f\"Will generate a resolution of {gen_res}.\")\n    print(f\"Will generate {gen_square}px square images.\")\n    print(f\"Will generate {img_chan} image channels.\")\n    print(f\"Will generate {img_rows} preview rows.\")\n    print(f\"Will generate {img_cols} preview columns.\")\n    print(f\"Our preview margin equals {img_margin}.\")\n    print(f\"Our data path is: {data_path}.\")\n    print(f\"Our number of epochs are: {epochs}.\")\n    print(f\"Will generate a batch size of {num_batch}.\")\n    print(f\"Will generate a buffer size of {num_buffer}.\")\n    ```", "```\n    training_binary_path = os.path.join(data_path,\\\n            f'training_data_{gen_square}_{gen_square}.npy')\n    print(f\"Looking for file: {training_binary_path}\")\n    if not os.path.isfile(training_binary_path):\n        start = time.time()\n        print(\"Loading training images...\")\n        train_data = []\n        images_path = os.path.join(data_path,'tomato')\n        for filename in tqdm(os.listdir(images_path)):\n            path = os.path.join(images_path,filename)\n            images = Image.open(path).resize((gen_square,\n                gen_square),Image.ANTIALIAS)\n            train_data.append(np.asarray(images))\n        train_data = np.reshape(train_data,(-1,gen_square,\n                gen_square,img_chan))\n        train_data = train_data.astype(np.float32)\n        train_data = train_data / 127.5 - 1.\n        print(\"Saving training images...\")\n        np.save(training_binary_path,train_data)\n        elapsed = time.time()-start\n        print (f'Image preprocessing time: {time_string(elapsed)}')\n    else:\n        print(\"Loading the training data...\")\n        train_data = np.load(training_binary_path)\n    ```", "```\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_data) \\\n                      .shuffle(num_buffer).batch(num_batch)\n    ```", "```\n    def create_generator(seed_size, channels):\n        model = Sequential()\n        model.add(Dense(4*4*256,activation=\"relu\", \\\n                        input_dim=seed_size))\n        model.add(Reshape((4,4,256)))\n        model.add(UpSampling2D())\n        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        model.add(UpSampling2D())\n        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n\n        model.add(UpSampling2D())\n        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        if gen_res>1:\n            model.add(UpSampling2D(size=(gen_res,gen_res)))\n            model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n            model.add(BatchNormalization(momentum=0.8))\n            model.add(Activation(\"relu\"))\n        model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n        model.add(Activation(\"tanh\"))\n        return model\n    ```", "```\n    def create_discriminator(image_shape):\n        model = Sequential()\n        model.add(Conv2D(32, kernel_size=3, strides=2, \\\n                         input_shape=image_shape, \n                         padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, kernel_size=3, \\\n                         strides=2, padding=\"same\"))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size=3, strides=2, \\\n                         padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(256, kernel_size=3, strides=1, \\\n                         padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(512, kernel_size=3, strides=1, \\\n                         padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n        model.add(Dense(1, activation='sigmoid'))\n        return model\n    ```", "```\n    def save_images(cnt,noise):\n        img_array = np.full(( \n          img_margin + (img_rows * (gen_square+img_margin)), \n          img_margin + (img_cols * (gen_square+img_margin)), 3), \n          255, dtype=np.uint8)\n        gen_imgs = generator.predict(noise)\n        gen_imgs = 0.5 * gen_imgs + 0.5\n        img_count = 0\n        for row in range(img_rows):\n        for col in range(img_cols):\n            r = row * (gen_square+16) + img_margin\n            c = col * (gen_square+16) + img_margin\n            img_array[r:r+gen_square,c:c+gen_square] \\\n                = gen_imgs[img_count] * 255\n            img_count += 1\n        output_path = os.path.join(data_path,'output')\n        if not os.path.exists(output_path):\n        os.makedirs(output_path)\n        filename = os.path.join(output_path,f\"train-{cnt}.png\")\n        im = Image.fromarray(img_array)\n        im.save(filename)\n    ```", "```\n    generator = create_generator(seed_vector, img_chan)\n    noise = tf.random.normal([1, seed_vector])\n    gen_img = generator(noise, training=False)\n    plt.imshow(gen_img[0, :, :, 0])\n    ```", "```\n    img_shape = (gen_square,gen_square,img_chan)\n    discriminator = create_discriminator(img_shape)\n    decision = discriminator(gen_img)\n    print (decision)\n    ```", "```\n    tf.Tensor([[0.4994658]], shape=(1,1), dtype=float32)\n    ```", "```\n    cross_entropy = tf.keras.losses.BinaryCrossentropy()\n    def discrim_loss(real_output, fake_output):\n        real_loss = cross_entropy(tf.ones_like(real_output), \\\n                                  real_output)\n        fake_loss = cross_entropy(tf.zeros_like(fake_output), \\\n                                  fake_output)\n        total_loss = real_loss + fake_loss\n        return total_loss\n    def gen_loss(fake_output):\n        return cross_entropy(tf.ones_like(fake_output), \\\n                             fake_output)\n    ```", "```\n    gen_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    disc_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    ```", "```\n    @tf.function\n    def train_step(images):\n        seed = tf.random.normal([num_batch, seed_vector])\n        with tf.GradientTape() as gen_tape, \\\n            tf.GradientTape() as disc_tape:\n        gen_imgs = generator(seed, training=True)\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(gen_imgs, training=True)\n        g_loss = gen_loss(fake_output)\n        d_loss = discrim_loss(real_output, fake_output)\n\n        gradients_of_generator = gen_tape.gradient(\\\n            g_loss, generator.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(\\\n            d_loss, discriminator.trainable_variables)\n        gen_optimizer.apply_gradients(zip(\n            gradients_of_generator, generator.trainable_variables))\n        disc_optimizer.apply_gradients(zip(\n            gradients_of_discriminator, \n            discriminator.trainable_variables))\n        return g_loss,d_loss\n    ```", "```\n    def train(dataset, epochs):\n        fixed_seed = np.random.normal(0, 1, (img_rows * img_cols, \n                                            seed_vector))\n        start = time.time()\n        for epoch in range(epochs):\n        epoch_start = time.time()\n        g_loss_list = []\n        d_loss_list = []\n        for image_batch in dataset:\n            t = train_step(image_batch)\n            g_loss_list.append(t[0])\n            d_loss_list.append(t[1])\n        generator_loss = sum(g_loss_list) / len(g_loss_list)\n        discriminator_loss = sum(d_loss_list) / len(d_loss_list)\n        epoch_elapsed = time.time()-epoch_start\n        print (f'Epoch {epoch+1}, gen loss={generator_loss}', \\\n               f'disc loss={discriminator_loss},'\\\n               f' {time_string(epoch_elapsed)}')\n        save_images(epoch,fixed_seed)\n        elapsed = time.time()-start\n        print (f'Training time: {time_string(elapsed)}')\n    ```", "```\n    train(train_dataset, epochs)\n    ```", "```\n    a = imread('/content/drive/MyDrive/Datasets'\\\n               '/apple-or-tomato/training_set/output/train-0.png')\n    plt.imshow(a)\n    ```", "```\na = imread('/content/drive/MyDrive/Datasets'\\\n           '/apple-or-tomato/training_set/output/train-100.png')\nplt.imshow(a)\n```", "```\na = imread('/content/drive/MyDrive/Datasets'\\\n           '/apple-or-tomato/training_set/output/train-500.png')\nplt.imshow(a)\n```", "```\na = imread('/content/drive/MyDrive/Datasets'\\\n           '/apple-or-tomato/training_set/output/train-999.png')\nplt.imshow(a)\n```"]