<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Evaluating Algorithms</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As we have seen in the previous chapters, several AI solutions are available to achieve certain cybersecurity goals, so it is important to learn how to evaluate the effectiveness of various alternative solutions, using appropriate analysis metrics. </span><span class="koboSpan" id="kobo.2.2">At the same time, it is important to prevent phenomena such as overfitting, which can compromise the reliability of forecasts when switching from training data to test data.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will learn about the following topics:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.4.1">Feature engineering best practices in dealing with raw data</span></li>
<li><span class="koboSpan" id="kobo.5.1">How to evaluate a detector's performance using the ROC curve</span></li>
<li><span class="koboSpan" id="kobo.6.1">How to appropriately split sample data into training and test sets</span></li>
<li><span class="koboSpan" id="kobo.7.1">How to manage algorithms' overfitting and bias–variance trade-offs with cross validation</span></li>
</ul>
<p><span class="koboSpan" id="kobo.8.1">Now, let's begin our discussion of we need feature engineering by examining the very nature of raw data.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Best practices of feature engineering </span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the previous chapters, we looked at different </span><strong><span class="koboSpan" id="kobo.3.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">AI</span></strong><span class="koboSpan" id="kobo.6.1">) algorithms, analyzing their application to the different scenarios and their use cases in a cybersecurity context. </span><span class="koboSpan" id="kobo.6.2">Now, the time has come to learn how to evaluate these algorithms, starting from the assumption that algorithms are the foundation of data-driven learning models.</span></p>
<p><span class="koboSpan" id="kobo.7.1">We will therefore have to deal with the very nature of the data, which is the basis of the algorithm learning process, which aims to make generalizations in the form of predictions based on the samples received as input in the training phase.</span></p>
<p><span class="koboSpan" id="kobo.8.1">The choice of algorithm will therefore fall on the one that is best for generalizing beyond the training data, thereby obtaining the best predictions when facing new data. </span><span class="koboSpan" id="kobo.8.2">In fact, it is relatively simple to identify an algorithm that fits the training data; the problem becomes more complicated when the algorithm must correctly make predictions on data that has never been seen before. </span><span class="koboSpan" id="kobo.8.3">In fact, we will see that the tendency to optimize the accuracy of the algorithm's predictions on training data gives rise to the phenomenon known as </span><strong><span class="koboSpan" id="kobo.9.1">overfitting</span></strong><span class="koboSpan" id="kobo.10.1">, where predictions become worse when dealing with new test data.</span></p>
<p><span class="koboSpan" id="kobo.11.1">It therefore becomes important to understand how to correctly perform the algorithm training, from the selection of the training dataset up to the correct tuning of the learning parameters characterizing the chosen algorithm.</span></p>
<p><span class="koboSpan" id="kobo.12.1">There are several methods available for performing algorithm training, such as using the same training dataset (for example, by dividing the training dataset into two separate subsets, one for training and one for testing) and choosing a suitable percentage of the original training dataset to be assigned to the two distinct subsets.</span></p>
<p><span class="koboSpan" id="kobo.13.1">Another strategy is based on cross validation, which, as we will see, consists of randomly dividing the training dataset into a certain number of subsets on which to train the algorithm and calculate the average of the results obtained in order to verify the accuracy of predictions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Better algorithms or more data?</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">While it is true that, in order to make correct predictions (which, in turn, are nothing more than generalizations starting from sample data), the data alone is not enough; you need to combine the data with algorithms (which, in turn, are nothing more than data representations). </span><span class="koboSpan" id="kobo.2.2">In practice, </span><span><span class="koboSpan" id="kobo.3.1">however,</span></span><span class="koboSpan" id="kobo.4.1"> we are often faced with a dilemma when improving our predictions: should we design a better algorithm, or do we just need to collect more data? </span><span class="koboSpan" id="kobo.4.2">The answer to this question has not always been the same over time, since, when research in the field of AI began, the emphasis was on the quality of the algorithms, since the availability of data was dictated by the cost of storage.</span></p>
<p><span class="koboSpan" id="kobo.5.1">With the reduction in costs associated with storage, in recent years, we have witnessed an unprecedented explosion in the availability of data, which has given rise to new analytical techniques based on big data, and the emphasis has consequently shifted to the availability of data. </span><span class="koboSpan" id="kobo.5.2">However, as the amount of data available increases, the amount of time required to analyze it increases accordingly, so, in choosing between the quality of the algorithms and the amount of training data, we must face a trade-off.</span></p>
<p><span class="koboSpan" id="kobo.6.1">In general, practical experience shows us that even a dumb algorithm powered by large amounts of data is able to produce better predictions than a clever algorithm fed with less data.</span></p>
<p><span class="koboSpan" id="kobo.7.1">However, the very nature of the data is often the element that makes the difference.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The very nature of raw data</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The emphasis given to the relevance of data often resonates in the motto that states let the data speak for itself. </span><span class="koboSpan" id="kobo.2.2">In reality, data is almost never able to speak for itself, and when it does, it usually deceives us. </span><span class="koboSpan" id="kobo.2.3">The raw data is nothing more than fragments of information that behave like pieces of a puzzle where we do not (yet) </span><span><span class="koboSpan" id="kobo.3.1">know </span></span><span class="koboSpan" id="kobo.4.1">the bigger picture.</span></p>
<p><span class="koboSpan" id="kobo.5.1">To make sense of the raw data, we therefore need models that help us to distinguish the necessary pieces (the signal) from the useless pieces (the noise), in addition to identifying the missing pieces to complete our puzzle.</span></p>
<p><span class="koboSpan" id="kobo.6.1">The models, in the case of AI, take the form of mathematical relations between features, through which we are able to show the different aspects and the different functions that the data represents, based on the purpose we intend to achieve with our analysis. </span><span class="koboSpan" id="kobo.6.2">In order for the raw data to be inserted in to our mathematical models, it must first be treated </span><span><span class="koboSpan" id="kobo.7.1">appropriately</span></span><span class="koboSpan" id="kobo.8.1">, thereby becoming the feature of our models. </span><span class="koboSpan" id="kobo.8.2">A feature, in fact, is nothing but the numerical representation of the raw data.</span></p>
<p><span class="koboSpan" id="kobo.9.1">For example, raw data often does not occur in numerical form. </span><span class="koboSpan" id="kobo.9.2">However, the representation of data in numerical form is a necessary prerequisite, in order to get processed by algorithms. </span><span class="koboSpan" id="kobo.9.3">Therefore, we must convert the raw data into numerical form before feeding it to our algorithms.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Feature engineering to the rescue</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Therefore, in implementing our predictive models, we must not simply limit ourselves to specifying the choice of the algorithm(s), but we must also define the features required to power them. </span><span class="koboSpan" id="kobo.2.2">As such, the correct definition of features is an essential step, both for the achievement of our goals and for the efficiency of the implementation of our predictive model.</span></p>
<p><span class="koboSpan" id="kobo.3.1">As we have said, the features constitute the numerical representations of the raw data. </span><span class="koboSpan" id="kobo.3.2">There are obviously different ways to convert raw data into numerical form, and these are distinguished according to the varying nature of the raw data, in addition to the types of algorithms of choice. </span><span class="koboSpan" id="kobo.3.3">Different algorithms, in fact, require different features in order to work.</span></p>
<p><span class="koboSpan" id="kobo.4.1">The number of features is also equally important for the predictive performance of our model. </span><span class="koboSpan" id="kobo.4.2">Choosing the quality and quantity of features therefore constitutes a preliminary process, known as feature engineering.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Dealing with raw data</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A first screening is conducted on the basis of the nature of the numerical values that are to be associated with our models. </span><span class="koboSpan" id="kobo.2.2">We should ask ourselves whether the number values we require are only positive or negative, or just Boolean values, whether we can limit ourselves to certain orders of magnitude, whether we can determine in advance the maximum value and the minimum value that the features can assume, and so on.</span></p>
<p><span class="koboSpan" id="kobo.3.1">We can also artificially create complex features, starting from simple features, in order to increase the explanatory, as well as the predictive, capacity of our models.</span></p>
<p><span class="koboSpan" id="kobo.4.1">Here are some of the most common transformations applicable to transforming raw data into model features:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">Data binarization</span></li>
<li><span class="koboSpan" id="kobo.6.1">Data binning</span></li>
<li><span class="koboSpan" id="kobo.7.1">Logarithmic data transformation</span></li>
</ul>
<p><span class="koboSpan" id="kobo.8.1">We will now examine each of these transformations in detail.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Data binarization</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the most basic forms of transformation, based on raw data counting, is binarization, which consists of assigning the value </span><kbd><span class="koboSpan" id="kobo.3.1">1</span></kbd><span class="koboSpan" id="kobo.4.1"> to all the counts greater than </span><kbd><span class="koboSpan" id="kobo.5.1">0</span></kbd><span class="koboSpan" id="kobo.6.1">, and assigning the value </span><kbd><span class="koboSpan" id="kobo.7.1">0</span></kbd><span class="koboSpan" id="kobo.8.1"> in the remaining cases. To understand the usefulness of binarization, we only need to consider the development of a predictive model whose goal is to predict user preferences based on video visualizations. </span><span class="koboSpan" id="kobo.8.2">We could therefore decide to assess the preferences of the individual users simply by counting their respective visualizations of videos; however, the problem is that the order of magnitude of the visualizations varies according to the habits of the individual users.</span></p>
<p><span class="koboSpan" id="kobo.9.1">Therefore, the absolute value of the visualizations—that is, the raw count—does not constitute a reliable measure of the greater or lesser preference accorded to each video. </span><span class="koboSpan" id="kobo.9.2">In fact, some users have the habit of repeatedly visualizing the same video, without paying particular attention to it, while other users prefer to focus their attention, thereby reducing the number of visualizations.</span></p>
<p><span class="koboSpan" id="kobo.10.1">Moreover, the different orders of magnitude associated with video visualizations by each user, varying from tens to hundreds, or even thousands of views, based on a user's habits, makes some statistical measurements, such as the arithmetic average, less representative of individual preferences.</span></p>
<p><span class="koboSpan" id="kobo.11.1">Instead of using the raw count of visualizations, we can binarize the counts, associating the value </span><kbd><span class="koboSpan" id="kobo.12.1">1</span></kbd><span class="koboSpan" id="kobo.13.1"> with all the videos that obtained a number of visualizations greater than </span><kbd><span class="koboSpan" id="kobo.14.1">0</span></kbd><span class="koboSpan" id="kobo.15.1"> and the value </span><kbd><span class="koboSpan" id="kobo.16.1">0</span></kbd><span class="koboSpan" id="kobo.17.1"> otherwise. </span><span class="koboSpan" id="kobo.17.2">Obtaining the results in this way is more efficient and robust measure of individual preferences.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Data binning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Managing the different orders of magnitude of the counts is a problem that occurs in different situations, and there are many algorithms that behave badly when faced with data that exhibits a wide ranges of values, such as clustering algorithms that measure similarity on the basis of Euclidean distance.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In a similar way to binarization, it is possible to reduce the dimensional scale by grouping the raw data counts into containers called </span><strong><span class="koboSpan" id="kobo.4.1">bins</span></strong><span class="koboSpan" id="kobo.5.1">, with fixed amplitude (fixed-with binning), sorted in ascending order, thereby scaling their absolute values </span><span><span class="koboSpan" id="kobo.6.1">linearly or exponentially</span></span><span class="koboSpan" id="kobo.7.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Logarithmic data transformation</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Similarly, it is possible to reduce the magnitude of raw data counts by replacing their absolute values with logarithms.</span></p>
<p><span class="koboSpan" id="kobo.3.1">A peculiar feature of the logarithmic transformation is precisely that of reducing the relevance of greater values, and, at the same time, of amplifying smaller values, thereby achieving greater uniformity of value distribution.</span></p>
<p><span class="koboSpan" id="kobo.4.1">In addition to logarithms, it is possible to use other power functions, which allow the stabilization of the variance of a data distribution (such as the </span><strong><span class="koboSpan" id="kobo.5.1">Box–Cox transformation</span></strong><span class="koboSpan" id="kobo.6.1">).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Data normalization</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Also known as feature normalization or feature scaling, data normalization improves the performance of algorithms that can be influenced by the scale of input values.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The following are the most common examples of feature normalization.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Min–max scaling</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">With the min–max scaling transformation, we let the data fall within a limited range of values: </span><kbd><span class="koboSpan" id="kobo.3.1">0</span></kbd><span class="koboSpan" id="kobo.4.1"> and </span><kbd><span class="koboSpan" id="kobo.5.1">1</span></kbd><span class="koboSpan" id="kobo.6.1">.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The transformation of the data involves replacing the original values </span><span class="koboSpan" id="kobo.8.1"><img class="fm-editor-equation" src="assets/b50a1285-4c07-480b-9570-0cfe78242d22.png" style="width:1.08em;height:0.83em;"/></span><span class="koboSpan" id="kobo.9.1"> with the values calculated with the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.10.1"><img class="fm-editor-equation" src="assets/0f345394-1d4d-40bd-a45e-4812c056ff5e.png" style="width:38.83em;height:1.67em;"/></span></p>
<p><span class="koboSpan" id="kobo.11.1">Here, </span><span class="koboSpan" id="kobo.12.1"><img class="fm-editor-equation" src="assets/b040dc4a-5be0-4b03-b363-0cd1a90eb0f8.png" style="width:3.17em;height:1.08em;"/></span><span class="koboSpan" id="kobo.13.1"> represents the minimum value of the entire distribution, and </span><span class="koboSpan" id="kobo.14.1"><img class="fm-editor-equation" src="assets/61bfae4a-ae32-4882-8498-a06ebf8fe84f.png" style="width:3.25em;height:1.08em;"/></span><span class="koboSpan" id="kobo.15.1"> the maximum value.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Variance scaling </span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Another very common data normalization method involves subtracting the mean of the distribution from each single </span><span class="koboSpan" id="kobo.3.1"><img class="fm-editor-equation" src="assets/627215c2-d54f-4636-840d-349182d572f7.png" style="width:1.25em;height:1.00em;"/></span> <span><span class="koboSpan" id="kobo.4.1">value</span></span><span class="koboSpan" id="kobo.5.1">, and then dividing the result obtained by the variance of the distribution.</span></p>
<p><span class="koboSpan" id="kobo.6.1">Following normalization (also known as </span><strong><span class="koboSpan" id="kobo.7.1">standardization</span></strong><span class="koboSpan" id="kobo.8.1">), the distribution of the recalculated data shows a mean equal to </span><kbd><span class="koboSpan" id="kobo.9.1">0</span></kbd><span class="koboSpan" id="kobo.10.1"> and a variance equal to </span><kbd><span class="koboSpan" id="kobo.11.1">1</span></kbd><span class="koboSpan" id="kobo.12.1">.</span></p>
<p><span class="koboSpan" id="kobo.13.1">The formula for variance scaling is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.14.1"><img class="fm-editor-equation" src="assets/d352f705-2937-4bb7-b122-f20007038cbe.png" style="width:24.75em;height:1.67em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to manage categorical variables</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Raw data can be represented by categorical variables that take non-numeric values.</span></p>
<p><span class="koboSpan" id="kobo.3.1">A typical example of a categorical variable is nationality. </span><span class="koboSpan" id="kobo.3.2">In order to mathematically manage categorical variables, we need to use some form </span><span><span class="koboSpan" id="kobo.4.1">of category </span></span><span class="koboSpan" id="kobo.5.1">transformation in numerical values, also called encoding.</span></p>
<p><span class="koboSpan" id="kobo.6.1">The following are the most common methods of categorical encoding.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Ordinal encoding</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">An intuitive approach to encoding could be to assign a single progressive value to the individual categories:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img src="assets/8f0f4e40-c79a-4776-b3da-ce4424d745db.png" style="width:23.33em;height:8.33em;"/></span></p>
<p><span class="koboSpan" id="kobo.4.1">The advantage and disadvantage of this encoding method is that the transformed values may be numerically ordered, even when this numerical ordering has no real meaning.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">One-hot encoding</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">With the one-hot encoding method, a set of bits is assigned to each variable, with each bit representing a distinct category.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The set of bits enables us to distinguish the variables that cannot belong to more than one category, resulting in only one bit data set:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.4.1"><img class="aligncenter size-full wp-image-568 image-border" src="assets/05b97bc3-bed1-4916-a6bc-322d838ca09a.png" style="width:43.92em;height:7.83em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Dummy encoding</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The one-hot encoding method actually wastes a bit (that, in fact, is not strictly necessary), which can be eliminated using the dummy encoding method:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img class="aligncenter size-full wp-image-569 image-border" src="assets/715eddd8-8a7f-4a82-949a-97112dabc963.png" style="width:32.67em;height:8.25em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Feature engineering examples with sklearn</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Now let's look at some examples of feature engineering implementation using the NumPy library and the preprocessing package of the </span><kbd><span class="koboSpan" id="kobo.3.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.4.1"> library.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Min–max scaler</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following code, we see an example of feature engineering using the </span><kbd><span class="koboSpan" id="kobo.3.1">MinMaxScaler</span></kbd><span class="koboSpan" id="kobo.4.1"> class of </span><kbd><span class="koboSpan" id="kobo.5.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.6.1">, aimed at scaling features to lie between a given range of values (minimum and maximum), such as </span><kbd><span class="koboSpan" id="kobo.7.1">0</span></kbd><span class="koboSpan" id="kobo.8.1"> and </span><kbd><span class="koboSpan" id="kobo.9.1">1</span></kbd><span class="koboSpan" id="kobo.10.1">:</span><br/></p>
<pre class="mce-root"><span class="koboSpan" id="kobo.11.1">from sklearn import preprocessing</span><br/><span class="koboSpan" id="kobo.12.1"> import numpy as np</span><br/><span class="koboSpan" id="kobo.13.1"> raw_data = np.array([</span><br/><span class="koboSpan" id="kobo.14.1"> [ 2., -3., 4.],</span><br/><span class="koboSpan" id="kobo.15.1"> [ 5., 0., 1.],</span><br/><span class="koboSpan" id="kobo.16.1"> [ 4., 0., -2.]])</span><br/><span class="koboSpan" id="kobo.17.1"> min_max_scaler = preprocessing.MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.18.1"> scaled_data = min_max_scaler.fit_transform(raw_data)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Standard scaler</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The following example shows the </span><kbd><span class="koboSpan" id="kobo.3.1">StandardScaler</span></kbd><span class="koboSpan" id="kobo.4.1"> class of </span><kbd><span class="koboSpan" id="kobo.5.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.6.1"> in action, used to compute the mean and standard deviation on a training set by leveraging the </span><kbd><span class="koboSpan" id="kobo.7.1">transform()</span></kbd><span class="koboSpan" id="kobo.8.1"> method:</span></p>
<pre><span class="koboSpan" id="kobo.9.1">from sklearn import preprocessing</span><br/><span class="koboSpan" id="kobo.10.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.11.1">raw_data = np.array([</span><br/><span class="koboSpan" id="kobo.12.1">[ 2., -3., 4.],</span><br/><span class="koboSpan" id="kobo.13.1">[ 5., 0., 1.],</span><br/><span class="koboSpan" id="kobo.14.1">[ 4., 0., -2.]])</span><br/><span class="koboSpan" id="kobo.15.1">std_scaler = preprocessing.StandardScaler().fit(raw_data)</span><br/><span class="koboSpan" id="kobo.16.1">std_scaler.transform(raw_data)</span><br/><span class="koboSpan" id="kobo.17.1">test_data = [[-3., 1., 2.]]</span><br/><span class="koboSpan" id="kobo.18.1">std_scaler.transform(test_data)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Power transformation</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we see the </span><kbd><span class="koboSpan" id="kobo.3.1">PowerTransformer</span></kbd><span class="koboSpan" id="kobo.4.1"> class of </span><span><kbd><span class="koboSpan" id="kobo.5.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.6.1"> </span></span><span class="koboSpan" id="kobo.7.1">in action, applying the zero-mean, unit-variance normalization to the transformed output using a Box–Cox transformation:</span></p>
<pre><span class="koboSpan" id="kobo.8.1">from sklearn import preprocessing</span><br/><span class="koboSpan" id="kobo.9.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.10.1">pt = preprocessing.PowerTransformer(method='box-cox', standardize=False) </span><br/><span class="koboSpan" id="kobo.11.1">X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))</span><br/><span class="koboSpan" id="kobo.12.1">pt.fit_transform(X_lognormal)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Ordinal encoding with sklearn</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we see how to encode categorical features into integers using the </span><kbd><span class="koboSpan" id="kobo.3.1">OrdinalEncoder</span></kbd><span class="koboSpan" id="kobo.4.1"> class of </span><span><kbd><span class="koboSpan" id="kobo.5.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.6.1"> </span></span><span class="koboSpan" id="kobo.7.1">and its </span><kbd><span class="koboSpan" id="kobo.8.1">transform()</span></kbd><span class="koboSpan" id="kobo.9.1"> method:</span></p>
<pre><span class="koboSpan" id="kobo.10.1">from sklearn import preprocessing</span><br/><span class="koboSpan" id="kobo.11.1">ord_enc = preprocessing.OrdinalEncoder()</span><br/><span class="koboSpan" id="kobo.12.1">cat_data = [['Developer', 'Remote Working', 'Windows'], ['Sysadmin', 'Onsite Working', 'Linux']]</span><br/><span class="koboSpan" id="kobo.13.1">ord_enc.fit(cat_data)</span><br/><span class="koboSpan" id="kobo.14.1">ord_enc.transform([['Developer', 'Onsite Working', 'Linux']])</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">One-hot encoding with sklearn</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The following example shows how to transform categorical features into binary representation, making use of the </span><kbd><span class="koboSpan" id="kobo.3.1">OneHotEncoder</span></kbd><span class="koboSpan" id="kobo.4.1"> class of </span><kbd><span><span class="koboSpan" id="kobo.5.1">scikit-learn</span></span></kbd><span class="koboSpan" id="kobo.6.1">:</span></p>
<pre><span class="koboSpan" id="kobo.7.1">from sklearn import preprocessing</span><br/><span class="koboSpan" id="kobo.8.1">one_hot_enc = preprocessing.OneHotEncoder()</span><br/><span class="koboSpan" id="kobo.9.1">cat_data = [['Developer', 'Remote Working', 'Windows'], ['Sysadmin', 'Onsite Working', 'Linux']]</span><br/><span class="koboSpan" id="kobo.10.1">one_hot_enc.fit(cat_data)</span><br/><span class="koboSpan" id="kobo.11.1">one_hot_enc.transform([['Developer', 'Onsite Working', 'Linux']])</span></pre>
<p><span class="koboSpan" id="kobo.12.1">After having described feature engineering best practices, we can move on to evaluating the performance of our models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Evaluating a detector's performance with ROC</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have previously encountered the ROC curve and AUC measure (</span><a href="a6eab48a-f031-44c9-ae4a-0cfd5db2e05e.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 5</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Network Anomaly Detection with AI</span></em><em><span class="koboSpan" id="kobo.6.1">,</span></em><span class="koboSpan" id="kobo.7.1"> and </span><a href="98ce7db1-f53d-47ca-b6ca-ec0e5f882566.xhtml"><span class="koboSpan" id="kobo.8.1">Chapter 7</span></a><span class="koboSpan" id="kobo.9.1">, </span><em><span class="koboSpan" id="kobo.10.1">Fraud Prevention with Cloud AI Solutions</span></em><span class="koboSpan" id="kobo.11.1">) to evaluate and compare the performance of different classifiers.</span></p>
<p><span class="koboSpan" id="kobo.12.1">Now let's explore the topic in a more systematic way, introducing the confusion matrix associated with all the possible results returned by a fraud-detection classifier, comparing the predicted values with the real values:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.13.1"><img class="aligncenter size-full wp-image-570 image-border" src="assets/da832600-40a3-4ae9-9307-cd648dae232d.png" style="width:34.67em;height:7.67em;"/></span></p>
<p><span class="koboSpan" id="kobo.14.1">We can then calculate the following values (listed with their interpretation) based on the previous confusion matrix:</span></p>
<ul>
<li class="mce-root"><span><strong><span class="koboSpan" id="kobo.15.1">Sensitivity</span></strong><span class="koboSpan" id="kobo.16.1"> = </span><strong><span class="koboSpan" id="kobo.17.1">Recall</span></strong><span class="koboSpan" id="kobo.18.1"> = </span><strong><span class="koboSpan" id="kobo.19.1">Hit rate</span></strong><span class="koboSpan" id="kobo.20.1"> =</span></span> <em><strong><span class="koboSpan" id="kobo.21.1">TP/(TP + FP)</span></strong></em><span><span class="koboSpan" id="kobo.22.1">: This value measures the rate of correctly labeled fraudsters and represents the </span><strong><span class="koboSpan" id="kobo.23.1">true positive rate</span></strong><span class="koboSpan" id="kobo.24.1"> (</span><strong><span class="koboSpan" id="kobo.25.1">TPR</span></strong><span class="koboSpan" id="kobo.26.1">)</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.27.1">False Positive Rate</span></strong> <em><strong><span class="koboSpan" id="kobo.28.1">(FPR) = FP/(FP + TN)</span></strong></em><span><span class="koboSpan" id="kobo.29.1">: FPR is also calculated as 1 – Specificity</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.30.1">Classification accuracy</span></strong><span class="koboSpan" id="kobo.31.1"> = </span><em><strong><span class="koboSpan" id="kobo.32.1">(TP + TN)/(TP + FP + FN + TN)</span></strong></em><span><span class="koboSpan" id="kobo.33.1">: This value represents the percentage of correctly classified observations</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.34.1">Classification error</span></strong><span class="koboSpan" id="kobo.35.1"> = </span><em><strong><span class="koboSpan" id="kobo.36.1">(FP + FN)/(TP + FP + FN + TN)</span></strong></em><span><span class="koboSpan" id="kobo.37.1">: This value represents the misclassification rate</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.38.1">Specificity</span></strong><span class="koboSpan" id="kobo.39.1"> = </span><em><strong><span class="koboSpan" id="kobo.40.1">TN/(FP + TN)</span></strong></em><span class="koboSpan" id="kobo.41.1">:</span><span><span class="koboSpan" id="kobo.42.1"> This value measures the rate of correctly labeled nonfraudsters</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.43.1">Precision</span></strong><span class="koboSpan" id="kobo.44.1"> = </span><em><strong><span class="koboSpan" id="kobo.45.1">TP/(TP + FP)</span></strong></em><span><span class="koboSpan" id="kobo.46.1">: This value measures how many of the predicted fraudsters are actually fraudsters</span></span></li>
<li class="mce-root"><strong><em><span class="koboSpan" id="kobo.47.1">F</span></em></strong><span class="koboSpan" id="kobo.48.1"> - </span><strong><em><span class="koboSpan" id="kobo.49.1">measure</span></em></strong><span class="koboSpan" id="kobo.50.1"> = </span><strong><em><span class="koboSpan" id="kobo.51.1">2</span></em></strong><span class="koboSpan" id="kobo.52.1"> x </span><strong><span class="koboSpan" id="kobo.53.1">(</span><em><span class="koboSpan" id="kobo.54.1">Precision</span></em></strong><span class="koboSpan" id="kobo.55.1"> x </span><strong><em><span class="koboSpan" id="kobo.56.1">Recall</span></em><span class="koboSpan" id="kobo.57.1">)/(</span><em><span class="koboSpan" id="kobo.58.1">Precision</span></em></strong><span class="koboSpan" id="kobo.59.1"> + </span><strong><em><span class="koboSpan" id="kobo.60.1">Recall</span></em><span class="koboSpan" id="kobo.61.1">)</span></strong><span class="koboSpan" id="kobo.62.1">:</span><span><span class="koboSpan" id="kobo.63.1"> This value represents the weighted harmonic mean of the precision and recall. </span><span class="koboSpan" id="kobo.63.2">The </span><em><span class="koboSpan" id="kobo.64.1">F</span></em><span class="koboSpan" id="kobo.65.1">-measure varies from 0 (worst score) to 1 (best value)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.66.1">We are now able to analyze in detail the ROC curve and its associated AUC measure.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">ROC curve and AUC measure</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Among the techniques most commonly used to compare the performance of different classifiers, we have the </span><strong><span class="koboSpan" id="kobo.3.1">receiving operating characteristic</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">ROC</span></strong><span class="koboSpan" id="kobo.6.1">) curve, which describes the relationship between the TPR, or sensitivity, and the FPR, or 1 – Specificity, associated with each classifier:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img class="aligncenter size-full wp-image-571 image-border" src="assets/8318045e-fdbd-4639-8182-2f3d23ffe968.png" style="width:32.00em;height:24.58em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><em><span class="koboSpan" id="kobo.8.1">(Image Credits: http://scikit-learn.org/)</span></em></div>
<p><span class="koboSpan" id="kobo.9.1">How can the performances of the different classifiers be compared?</span></p>
<p><span class="koboSpan" id="kobo.10.1">Let's start by taking into consideration the characteristics that the best possible classifier should have. </span><span class="koboSpan" id="kobo.10.2">Its curve would correspond to the pair of values, ​​</span><em><strong><span class="koboSpan" id="kobo.11.1">x</span></strong></em><span class="koboSpan" id="kobo.12.1"> = </span><em><strong><span class="koboSpan" id="kobo.13.1">0</span></strong></em><span class="koboSpan" id="kobo.14.1"> and </span><em><strong><span class="koboSpan" id="kobo.15.1">y</span></strong></em><span class="koboSpan" id="kobo.16.1"> = </span><em><strong><span class="koboSpan" id="kobo.17.1">1</span></strong></em><span class="koboSpan" id="kobo.18.1">, in the ROC space. In other words, the best possible classifier is the one that correctly identifies all cases of fraud without generating any false positives, meaning that </span><em><strong><span class="koboSpan" id="kobo.19.1">FPR </span></strong></em><span class="koboSpan" id="kobo.20.1">= </span><em><strong><span class="koboSpan" id="kobo.21.1">0</span></strong></em><span class="koboSpan" id="kobo.22.1"> and </span><em><strong><span class="koboSpan" id="kobo.23.1">TPR</span></strong></em><span class="koboSpan" id="kobo.24.1"> = </span><em><strong><span class="koboSpan" id="kobo.25.1">1</span></strong></em><span class="koboSpan" id="kobo.26.1"> are ideal.</span></p>
<p><span class="koboSpan" id="kobo.27.1">Similarly, the performance of a random classifier, which makes predictions at random, would fall on the diagonal described by the pair of coordinates [</span><em><strong><span class="koboSpan" id="kobo.28.1">x</span></strong></em><span class="koboSpan" id="kobo.29.1"> = </span><em><strong><span class="koboSpan" id="kobo.30.1">0</span></strong></em><span class="koboSpan" id="kobo.31.1">, </span><em><strong><span class="koboSpan" id="kobo.32.1">y</span></strong></em><span class="koboSpan" id="kobo.33.1"> = </span><em><strong><span class="koboSpan" id="kobo.34.1">0</span></strong></em><span class="koboSpan" id="kobo.35.1">] and </span><span><span class="koboSpan" id="kobo.36.1">[</span></span><em><strong><span class="koboSpan" id="kobo.37.1">x</span></strong></em><span><span class="koboSpan" id="kobo.38.1"> = </span></span><em><strong><span class="koboSpan" id="kobo.39.1">1</span></strong></em><span><span class="koboSpan" id="kobo.40.1">, </span></span><em><strong><span class="koboSpan" id="kobo.41.1">y</span></strong></em><span><span class="koboSpan" id="kobo.42.1"> = </span></span><em><strong><span class="koboSpan" id="kobo.43.1">1</span></strong></em><span><span class="koboSpan" id="kobo.44.1">]</span></span><span class="koboSpan" id="kobo.45.1">.</span></p>
<p><span class="koboSpan" id="kobo.46.1">The comparison between the performances of different classifiers would therefore involve verifying how much their curves deviate from the </span><em><span class="koboSpan" id="kobo.47.1">L</span></em><span class="koboSpan" id="kobo.48.1">-curve (corresponding to the best classifier).</span></p>
<p><span class="koboSpan" id="kobo.49.1">To achieve a more precise measure of performance, we can calculate the </span><strong><span class="koboSpan" id="kobo.50.1">area under the ROC curve </span></strong><span class="koboSpan" id="kobo.51.1">(</span><strong><span class="koboSpan" id="kobo.52.1">AUC</span></strong><span class="koboSpan" id="kobo.53.1">) </span><span><span class="koboSpan" id="kobo.54.1">metric</span></span><span class="koboSpan" id="kobo.55.1"> associated with the individual classifiers. </span><span class="koboSpan" id="kobo.55.2">The values ​​that the AUC metric can assume fall in the range between 0 and 1.</span></p>
<p><span class="koboSpan" id="kobo.56.1">The best classifier's AUC metric is equal to 1, corresponding to the maximum value of AUC. </span><span class="koboSpan" id="kobo.56.2">The AUC metric can also be interpreted as a measure of the probability.</span></p>
<p><span class="koboSpan" id="kobo.57.1">In fact, a random classifier, whose curve corresponds to the diagonal in the ROC space, would have an AUC value of 0.5. </span><span class="koboSpan" id="kobo.57.2">Consequently, the performance of any other classifier should fall between a minimum AUC value of 0.5 and a maximum value of 1. </span><span class="koboSpan" id="kobo.57.3">Values ​​of </span><strong><em><span class="koboSpan" id="kobo.58.1">AUC</span></em></strong><span class="koboSpan" id="kobo.59.1"> &lt; </span><em><strong><span class="koboSpan" id="kobo.60.1">0.5</span></strong></em><span class="koboSpan" id="kobo.61.1"> indicate that the chosen classifier behaves worse than the random classifier.</span></p>
<p><span class="koboSpan" id="kobo.62.1">To correctly evaluate the quality of the estimated probabilities of the individual classifiers, we can use the </span><strong><span class="koboSpan" id="kobo.63.1">Brier score</span></strong><span class="koboSpan" id="kobo.64.1"> (</span><strong><span class="koboSpan" id="kobo.65.1">BS</span></strong><span class="koboSpan" id="kobo.66.1">), which measures the average of the differences between the estimated probabilities and the actual values.</span></p>
<p><span class="koboSpan" id="kobo.67.1">Here is the BS formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.68.1"><img class="fm-editor-equation" src="assets/e252f1d0-4e6a-421d-8e08-7fd7071da4d2.png" style="width:11.92em;height:2.17em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.69.1">Here, </span><em><span class="koboSpan" id="kobo.70.1">P</span></em></span><em><sub><span class="koboSpan" id="kobo.71.1">i</span></sub></em> <span><span class="koboSpan" id="kobo.72.1">is the estimated probability for observation </span><em><span class="koboSpan" id="kobo.73.1">i</span></em><span class="koboSpan" id="kobo.74.1">, and </span><span class="koboSpan" id="kobo.75.1"><img class="fm-editor-equation" src="assets/5d83a679-ee27-42df-b126-418635c4971a.png" style="width:1.42em;height:1.17em;"/></span></span> <span><span class="koboSpan" id="kobo.76.1">is a binary estimator (that assumes values of 0 or 1) for the actual value, </span><em><span class="koboSpan" id="kobo.77.1">i</span></em><span class="koboSpan" id="kobo.78.1">. </span></span><span class="koboSpan" id="kobo.79.1">Also, the value of </span><em><span class="koboSpan" id="kobo.80.1">BS</span></em><span class="koboSpan" id="kobo.81.1"> falls in the interval between 0 and 1, but, unlike the AUC, smaller values of </span><em><span class="koboSpan" id="kobo.82.1">BS</span></em><span class="koboSpan" id="kobo.83.1"> (that is, </span><em><span class="koboSpan" id="kobo.84.1">BS</span></em><span class="koboSpan" id="kobo.85.1"> values closer to 0) correspond to more accurate probability estimates.</span></p>
<p><span class="koboSpan" id="kobo.86.1">The following are some examples of calculations of the ROC curves and of the metrics associated with them, using the </span><kbd><span class="koboSpan" id="kobo.87.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.88.1"> library.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Examples of ROC metrics</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following code, we can see an example of a calculation of ROC metrics, using scikit-learn's methods, such as </span><kbd><span class="koboSpan" id="kobo.3.1">precision_recall_curve()</span></kbd><span class="koboSpan" id="kobo.4.1">, </span><kbd><span class="koboSpan" id="kobo.5.1">average_precision_score()</span></kbd><span class="koboSpan" id="kobo.6.1">, </span><kbd><span class="koboSpan" id="kobo.7.1">recall_score()</span></kbd><span class="koboSpan" id="kobo.8.1">, and </span><kbd><span class="koboSpan" id="kobo.9.1">f1_score()</span></kbd><span class="koboSpan" id="kobo.10.1">:</span></p>
<pre><span class="koboSpan" id="kobo.11.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.12.1">from sklearn import metrics</span><br/><span class="koboSpan" id="kobo.13.1">from sklearn.metrics import precision_recall_curve</span><br/><span class="koboSpan" id="kobo.14.1">from sklearn.metrics import average_precision_score</span><br/><span class="koboSpan" id="kobo.15.1">y_true = np.array([0, 1, 1, 1])</span><br/><span class="koboSpan" id="kobo.16.1">y_pred = np.array([0.2, 0.7, 0.65, 0.9])</span><br/><span class="koboSpan" id="kobo.17.1">prec, rec, thres = precision_recall_curve(y_true, y_pred)</span><br/><span class="koboSpan" id="kobo.18.1">average_precision_score(y_true, y_pred)</span><br/><span class="koboSpan" id="kobo.19.1">metrics.precision_score(y_true, y_pred)</span><br/><span class="koboSpan" id="kobo.20.1">metrics.recall_score(y_true, y_pred)</span><br/><span class="koboSpan" id="kobo.21.1">metrics.f1_score(y_true, y_pred)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">ROC curve example</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The following code shows how to calculate an ROC curve using the </span><kbd><span class="koboSpan" id="kobo.3.1">roc_curve()</span></kbd><span class="koboSpan" id="kobo.4.1"> method of </span><kbd><span class="koboSpan" id="kobo.5.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.6.1">:</span></p>
<pre><span class="koboSpan" id="kobo.7.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.8.1">from sklearn.metrics import roc_curve</span><br/><span class="koboSpan" id="kobo.9.1">y_true = np.array([0, 1, 1, 1])</span><br/><span class="koboSpan" id="kobo.10.1">y_pred = np.array([0.2, 0.7, 0.65, 0.9])</span><br/><span class="koboSpan" id="kobo.11.1">FPR, TPR, THR = roc_curve(y_true, y_pred)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">AUC score example</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example code, we can see how to calculate the AUC curve using the </span><kbd><span class="koboSpan" id="kobo.3.1">roc_auc_score()</span></kbd><span class="koboSpan" id="kobo.4.1"> method of </span><kbd><span class="koboSpan" id="kobo.5.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.6.1">:</span></p>
<pre><span class="koboSpan" id="kobo.7.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.8.1">from sklearn.metrics import roc_auc_score</span><br/><span class="koboSpan" id="kobo.9.1">y_true = np.array([0, 1, 1, 1])</span><br/><span class="koboSpan" id="kobo.10.1">y_pred = np.array([0.2, 0.7, 0.65, 0.9])</span><br/><span class="koboSpan" id="kobo.11.1">roc_auc_score(y_true, y_pred)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Brier score example</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we evaluate the quality of estimated probabilities using the </span><kbd><span class="koboSpan" id="kobo.3.1">brier_score_loss()</span></kbd><span class="koboSpan" id="kobo.4.1"> method of </span><kbd><span class="koboSpan" id="kobo.5.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.6.1">:</span></p>
<pre><span class="koboSpan" id="kobo.7.1"> import numpy as np</span><br/><span class="koboSpan" id="kobo.8.1"> from sklearn.metrics import brier_score_loss</span><br/><span class="koboSpan" id="kobo.9.1"> y_true = np.array([0, 1, 1, 1])</span><br/><span class="koboSpan" id="kobo.10.1"> y_cats = np.array(["fraud", "legit", "legit", "legit"])</span><br/><span class="koboSpan" id="kobo.11.1"> y_prob = np.array([0.2, 0.7, 0.9, 0.3])</span><br/><span class="koboSpan" id="kobo.12.1"> y_pred = np.array([1, 1, 1, 0])</span><br/><span class="koboSpan" id="kobo.13.1"> brier_score_loss(y_true, y_prob)</span><br/><span class="koboSpan" id="kobo.14.1"> brier_score_loss(y_cats, y_prob, pos_label="legit")</span><br/><span class="koboSpan" id="kobo.15.1"> brier_score_loss(y_true, y_prob &gt; 0.5)</span></pre>
<p><span class="koboSpan" id="kobo.16.1">Now, let's continue the evaluation of model performances by introducing the effects deriving from the splitting of the sample dataset into training and testing subsets.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to split data into training and test sets</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the most commonly used methods to evaluate the learning effectiveness of our models is to test the predictions made by the algorithms on data it has never seen before. </span><span class="koboSpan" id="kobo.2.2">However, it is not always possible to feed fresh data into our models. </span><span class="koboSpan" id="kobo.2.3">One alternative involves subdividing the data at our disposal into training and testing subsets, varying the percentages of d</span><span><span class="koboSpan" id="kobo.3.1">ata to be assigned to each subset. </span><span class="koboSpan" id="kobo.3.2">The percentages usually chosen vary between 70% and 80% for the training subset, with the remai</span></span><span><span class="koboSpan" id="kobo.4.1">ning 20–30% assigned to the testing subset.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">The subdivision of the original sample dataset into two subsets for training and testing can be easily performed using the </span><kbd><span class="koboSpan" id="kobo.6.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.7.1"> library, as we have done several times in our examples:</span></p>
<pre><span class="koboSpan" id="kobo.8.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.9.1"># Create training and testing subsets</span><br/><span class="koboSpan" id="kobo.10.1">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</span></pre>
<p><span class="koboSpan" id="kobo.11.1">By invoking the </span><kbd><span class="koboSpan" id="kobo.12.1">train_test_split()</span></kbd> <span><span class="koboSpan" id="kobo.13.1">method </span></span><span class="koboSpan" id="kobo.14.1">of the </span><kbd><span class="koboSpan" id="kobo.15.1">sklearn.model_selection</span></kbd> <span><span class="koboSpan" id="kobo.16.1">package </span></span><span class="koboSpan" id="kobo.17.1">and setting the </span><kbd><span class="koboSpan" id="kobo.18.1">test_size = 0.2</span></kbd> <span><span class="koboSpan" id="kobo.19.1">parameter, </span></span><span class="koboSpan" id="kobo.20.1">we are splitting the original sample dataset into training and testing subsets, reserving a percentage equal to 20% of the original dataset for the testing dataset and assigning the remaining 80% to the training dataset.</span></p>
<p><span class="koboSpan" id="kobo.21.1">This technique, however simple, may nonetheless have important effects on the learning effectiveness of our algorithms.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Algorithm generalization error</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As we have seen, the purpose of the algorithms is to learn to make correct predictions by generalizing from the training samples. </span><span class="koboSpan" id="kobo.2.2">All the algorithms, as a result of this learning process, manifest a generalization error that can be expressed as the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img class="fm-editor-equation" src="assets/0d7a09e9-ab03-4acf-a353-122d729b597b.png" style="width:25.33em;height:1.17em;"/></span></p>
<p><span class="koboSpan" id="kobo.4.1">By </span><em><span class="koboSpan" id="kobo.5.1">Bias</span></em><span class="koboSpan" id="kobo.6.1">, we mean the systematic error made by the algorithm in carrying out its predictions, and by </span><em><span class="koboSpan" id="kobo.7.1">Variance</span></em><span class="koboSpan" id="kobo.8.1">, we mean the sensitivity of the algorithm to the variations affecting the analyzed data. </span><span class="koboSpan" id="kobo.8.2">Finally, </span><em><span class="koboSpan" id="kobo.9.1">Noise</span></em><span class="koboSpan" id="kobo.10.1"> is an irreducible component that characterizes the data being analyzed.</span></p>
<p><span class="koboSpan" id="kobo.11.1">The following diagram shows different estimators that are characterized by their ability to adapt to data. </span><span class="koboSpan" id="kobo.11.2">Starting from the simplest estimator and moving up to the most complex estimator, we can see how the components of </span><em><span class="koboSpan" id="kobo.12.1">Bias</span></em><span class="koboSpan" id="kobo.13.1"> and </span><em><span class="koboSpan" id="kobo.14.1">Variance</span></em><span class="koboSpan" id="kobo.15.1"> vary. </span><span class="koboSpan" id="kobo.15.2">A lower complexity of the estimator usually corresponds to a higher </span><em><span class="koboSpan" id="kobo.16.1">Bias</span></em><span class="koboSpan" id="kobo.17.1"> (systematic error) and a reduced variance—that is, sensitivity to data change.</span></p>
<p><span class="koboSpan" id="kobo.18.1">Conversely, as the complexity of the model increases, the </span><em><span class="koboSpan" id="kobo.19.1">Bias</span></em><span class="koboSpan" id="kobo.20.1"> is reduced but the </span><em><span class="koboSpan" id="kobo.21.1">Variance</span></em><span class="koboSpan" id="kobo.22.1"> is increased, so that more complex models tend to over-adapt (overfit) their predictions to training data, thereby producing inferior predictions when switching from training data to testing data:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.23.1"><img class="aligncenter size-full wp-image-572 image-border" src="assets/6e2393d9-95d0-4e22-936e-27a7d913053d.png" style="width:50.25em;height:20.75em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><em><span class="koboSpan" id="kobo.24.1">(Image Credits: http://scikit-learn.org/)</span></em></div>
<p><span class="koboSpan" id="kobo.25.1">A method to reduce the </span><em><span class="koboSpan" id="kobo.26.1">Variance</span></em><span class="koboSpan" id="kobo.27.1"> associated with the complexity of the algorithm involves increasing the amount of data constituting the training dataset; however, it is not always easy to distinguish which component (the </span><em><span class="koboSpan" id="kobo.28.1">Bias</span></em><span class="koboSpan" id="kobo.29.1"> or the </span><em><span class="koboSpan" id="kobo.30.1">Variance</span></em><span class="koboSpan" id="kobo.31.1">) assumes greater importance in the determination of the generalization error. </span><span class="koboSpan" id="kobo.31.2">Therefore, we must use appropriate tools to distinguish the role played by the various components in the generalization error determination.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Algorithm learning curves</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A useful tool for identifying a component between the </span><em><span class="koboSpan" id="kobo.3.1">Bias</span></em><span class="koboSpan" id="kobo.4.1"> and the </span><em><span class="koboSpan" id="kobo.5.1">Variance</span></em><span class="koboSpan" id="kobo.6.1"> is important in determining the generalization error of an algorithm. </span><span class="koboSpan" id="kobo.6.2">This is the learning curve, through which the predictive performance of the algorithm is compared with the amount of training data. </span><span class="koboSpan" id="kobo.6.3">This way, it is possible to evaluate how the training score and the testing score of an algorithm vary as the training dataset changes:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img class="aligncenter size-full wp-image-573 image-border" src="assets/c82113c4-7be1-4048-af6a-e548e938369c.png" style="width:19.67em;height:10.75em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><em><span class="koboSpan" id="kobo.8.1">(Image Credits: Wikipedia https://commons.wikimedia.org/wiki/File:Variance-bias.svg)</span></em></div>
<p><span class="koboSpan" id="kobo.9.1">If the training score and the testing score tend to converge when the training dataset grows (as shown in the preceding diagram), in order to improve our predictions, we will have to increase the complexity of our algorithm, thereby reducing the </span><em><span class="koboSpan" id="kobo.10.1">Bias</span></em><span class="koboSpan" id="kobo.11.1"> component.</span></p>
<p><span class="koboSpan" id="kobo.12.1">If instead the training score is constantly higher than the testing score, an increase in the training dataset would improve the predictive performance of our algorithm, thereby reducing the </span><em><span class="koboSpan" id="kobo.13.1">Variance</span></em><span class="koboSpan" id="kobo.14.1"> component. </span><span class="koboSpan" id="kobo.14.2">Finally, in the case where the training score and the testing score do not converge, our model is characterized by high variance, and we will therefore have to act both on the complexity of the algorithm and on the size of the training dataset.</span></p>
<p><span class="koboSpan" id="kobo.15.1">In the following example, we can see how to use the </span><kbd><span class="koboSpan" id="kobo.16.1">learning_curve()</span></kbd><span class="koboSpan" id="kobo.17.1"> method of the </span><kbd><span class="koboSpan" id="kobo.18.1">sklearn.model_selection</span></kbd> <span><span class="koboSpan" id="kobo.19.1">package </span></span><span class="koboSpan" id="kobo.20.1">to obtain the values necessary to design a learning curve combined with a </span><strong><span class="koboSpan" id="kobo.21.1">support vector classifier</span></strong><span class="koboSpan" id="kobo.22.1"> (</span><strong><span class="koboSpan" id="kobo.23.1">SVC</span></strong><span class="koboSpan" id="kobo.24.1">), based on different training dataset sizes:</span></p>
<pre><span class="koboSpan" id="kobo.25.1">from sklearn.model_selection import learning_curve</span><br/><span class="koboSpan" id="kobo.26.1">from sklearn.svm import SVC</span><br/><span class="koboSpan" id="kobo.27.1">_sizes = [ 60, 80, 100]</span><br/><span class="koboSpan" id="kobo.28.1">train_sizes, train_scores, valid_scores = learning_curve(SVC(), X, y, train_sizes=_sizes)</span></pre>
<p><span class="koboSpan" id="kobo.29.1">In conclusion, we can say that our choice of the size of the training dataset affects on the learning effectiveness of our algorithms. </span><span class="koboSpan" id="kobo.29.2">In principle, by reducing the percentage assigned to the training dataset, we are increasing the </span><em><span class="koboSpan" id="kobo.30.1">Bias</span></em><span class="koboSpan" id="kobo.31.1"> error component. </span><span class="koboSpan" id="kobo.31.2">If instead we increase the size of the training dataset while maintaining the original sample dataset's size constant, we risk over-adapting the algorithm to the training data, which results in inferior predictions when we feed our algorithm new data, not to mention the fact that some highly informative samples could be excluded from the training dataset, due to the simple effect of the case, in relation to the specific splitting strategy we choose.</span></p>
<p><span class="koboSpan" id="kobo.32.1">Furthermore, if the training dataset is characterized by high dimensionality, the similarities between the testing and training data might only be apparent, thus making the learning process more difficult.</span></p>
<p><span class="koboSpan" id="kobo.33.1">Therefore, the simple strategy of splitting the sample dataset according to fixed percentages is not always the best solution, especially when it comes to evaluating and fine-tuning the performance of algorithms.</span></p>
<p><span class="koboSpan" id="kobo.34.1">An alternative solution is to use cross validation.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Using cross validation for algorithms</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The type of cross validation most commonly used is known as k-folds cross validation, and it involves randomly dividing the sample dataset into a number of folds, </span><em><span class="koboSpan" id="kobo.3.1">k</span></em><span class="koboSpan" id="kobo.4.1"> corresponding to equal portions (if possible) of data.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The learning process is performed in an iterative way, based on the different compositions of folds, used both as a training dataset and as a testing dataset. </span><span class="koboSpan" id="kobo.5.2">In this way, each fold is used in turn as a training dataset or as a testing dataset:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img class="aligncenter size-full wp-image-574 image-border" src="assets/75f8f430-1f26-4878-a2c8-caea432fc8f3.png" style="width:29.42em;height:20.42em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><em><span class="koboSpan" id="kobo.7.1">(Image Credits: http://scikit-learn.org/)</span></em></div>
<p><span class="koboSpan" id="kobo.8.1">In practice, the different folds (randomly generated) alternate in the role of training and testing datasets, and the iterative process ends when all the k-folds have been used both as training and testing datasets.</span></p>
<p><span class="koboSpan" id="kobo.9.1">Since, at each iteration, the generalization error generated is different (as the algorithm is trained and tested with different datasets), we can calculate the average of these errors as a representative measure of the cross validation strategy.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">K-folds cross validation pros and cons</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">K-folds cross validation has several advantages, including the following:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.3.1">It enables all the available data to be used, both for training and testing goals</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.4.1">The specific composition of the individual folds is irrelevant since each fold is used at most once for training, and once for testing</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.5.1">We can increase the number of folds by increasing the </span><em><span class="koboSpan" id="kobo.6.1">k</span></em><span class="koboSpan" id="kobo.7.1"> value, thereby increasing the size of the training dataset to reduce the </span><em><span class="koboSpan" id="kobo.8.1">Bias</span></em><span class="koboSpan" id="kobo.9.1"> component of the generalization error</span></li>
</ul>
<p><span class="koboSpan" id="kobo.10.1">In terms of disadvantages, we must stress that k-folds cross validation considers the order that characterizes our original sample dataset to be </span><span><span class="koboSpan" id="kobo.11.1">irrelevant</span></span><span class="koboSpan" id="kobo.12.1">. </span><span class="koboSpan" id="kobo.12.2">In case the order of the data constitutes relevant information (as in the case of time series datasets), we will have to use a different strategy that takes into account the original sequence—perhaps by splitting the data in accordance with the oldest data—to be used as a training dataset, thereby reserving the most recent data for testing purposes.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">K-folds cross validation example</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, we will use the k-folds cross validation implemented by the </span><kbd><span class="koboSpan" id="kobo.3.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.4.1"> package, </span><kbd><span class="koboSpan" id="kobo.5.1">sklearn.model_selection</span></kbd><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">For simplicity, we will assign the value </span><kbd><span class="koboSpan" id="kobo.7.1">2</span></kbd><span class="koboSpan" id="kobo.8.1"> to the variable </span><kbd><span class="koboSpan" id="kobo.9.1">k</span></kbd><span class="koboSpan" id="kobo.10.1">, thereby obtaining a 2-folds cross validation.</span></p>
<p><span class="koboSpan" id="kobo.11.1">The sample dataset consists of just four samples. </span><span class="koboSpan" id="kobo.11.2">Therefore, each fold will contain two arrays to be used in turn, one for training and the other for testing. </span><span class="koboSpan" id="kobo.11.3">Finally, note how it is possible to associate the different folds with training and testing data, using the syntax provided by </span><kbd><span class="koboSpan" id="kobo.12.1">numpy</span></kbd><span class="koboSpan" id="kobo.13.1"> indexing:</span></p>
<pre><span class="koboSpan" id="kobo.14.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.15.1">from sklearn.model_selection import KFold</span><br/><span class="koboSpan" id="kobo.16.1">X = np.array([[1., 0.], [2., 1.], [-2., -1.], [3., 2.]])</span><br/><span class="koboSpan" id="kobo.17.1">y = np.array([0, 1, 0, 1])</span><br/><span class="koboSpan" id="kobo.18.1">k_folds = KFold(n_splits=2)</span><br/><span class="koboSpan" id="kobo.19.1">for train, test in k_folds.split(X):</span><br/><span class="koboSpan" id="kobo.20.1">print("%s %s" % (train, test))</span><br/><span class="koboSpan" id="kobo.21.1">[2 0] [3 1]</span><br/><span class="koboSpan" id="kobo.22.1">[3 1] [2 0]</span><br/><span class="koboSpan" id="kobo.23.1">X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, we have looked at the different techniques commonly adopted to evaluate the predictive performances of different algorithms. </span><span class="koboSpan" id="kobo.2.2">We looked at how to transform raw data into features, following feature engineering best practices, thereby allowing algorithms to use data that does not have a numeric form, such as categorical variables. </span><span class="koboSpan" id="kobo.2.3">We then focused on the techniques needed to </span><span><span class="koboSpan" id="kobo.3.1">correctly </span></span><span class="koboSpan" id="kobo.4.1">evaluate the various components (such as bias and variance) that constitute the generalization error associated with the algorithms, and finally, we learned how to perform the cross validation of the algorithms to improve the training process.</span></p>
<p><span class="koboSpan" id="kobo.5.1">In the next chapter, we will learn how to assess your AI arsenal.</span></p>


            </article>

            
        </section>
    </body></html>