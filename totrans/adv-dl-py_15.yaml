- en: Deep Learning for Autonomous Vehicles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's think about how **autonomous vehicles** (**AVs**) will affect our lives.
    For one thing, instead of focusing our attention on driving, we'll be able to
    do something else during our trip. Catering to the needs of such travelers could
    probably spawn a whole industry in itself. But that's just an added bonus. If
    we can be more productive or just relax during our travels, it is likely that
    we'll start traveling more, not to mention the benefits for people with limited
    ability to drive themselves. Making such an essential and basic commodity as transportation more
    accessible has the potential to transform our lives. And that's just the effect
    on us as individuals—AVs can have profound effects on the economy too, starting
    from delivery services to just-in-time manufacturing. In short, making AVs work
    is a very high-stakes game. No wonder, then, that in recent years the research
    in this area has moved from the academic world to the real economy. Companies
    from Waymo, Uber, and NVIDIA to virtually all major vehicle manufacturers are
    rushing to develop AVs.
  prefs: []
  type: TYPE_NORMAL
- en: However, we are not there just yet. One of the reasons for this is that self-driving
    is a complex task, composed of multiple subproblems, each a major task in its
    own right. To navigate successfully, the vehicle's program needs an accurate 3D
    model of the environment. The way to construct such a model is to combine the
    signals coming from multiple sensors. Once we have the model, we still need to
    solve the actual driving task. Think about the many unexpected and unique situations
    a driver has to overcome without crashing. But even if we create a driving policy,
    it needs to be accurate almost 100% of the time. Say that our AV will successfully
    stop at 99 out of 100 red traffic lights. 99% accuracy is a great success for
    any other **machine learning **(**ML**) task; not so for autonomous driving, where
    even a single mistake can lead to a crash.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll explore the applications of deep learning in AVs. We'll
    look at how to use deep networks to help the vehicle make sense of its surrounding
    environment. We'll also see how to use them in actually controlling the vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to AVs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of an AV system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to 3D data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imitation driving policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driving policy with ChauffeurNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to AVs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start this section with a brief history of AV research (which started
    surprisingly long ago). We'll also try to define the different levels of AV automation
    according to the **Society of Automotive Engineers** (**SAE**).
  prefs: []
  type: TYPE_NORMAL
- en: Brief history of AV research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first serious attempts to implement self-driving cars began in the 1980s
    in Europe and the USA. Since the mid 2000s, progress has rapidly accelerated. The
    first major effort in the area was the Eureka Prometheus Project ([https://en.wikipedia.org/wiki/Eureka_Prometheus_Project](https://en.wikipedia.org/wiki/Eureka_Prometheus_Project)),
    which lasted from 1987 to 1995\. It culminated in 1995, when an autonomous Mercedes-Benz S-Class
    took a 1,600 km trip from Munich to Copenhagen and back using computer vision.
    At some points, the car achieved speeds of up to 175 km/h on the German Autobahn
    (fun fact: some sections of the Autobahn don''t have speed restrictions). The
    car was able to overtake other cars on its own. The average distance between human
    interventions was 9 km, and at one point it drove 158 km without interventions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1989, Dean Pomerleau from Carnegie Mellon University published *ALVINN:
    An Autonomous Land Vehicle in a Neural Network* ([https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf](https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf)),
    a pioneering paper on the use of neural networks for AVs. This work is especially
    interesting, as it applied many of the topics we''ve discussed in this book in
    AVs 30 years ago. Let''s look at the most important properties of ALVINN:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses a simple neural network to decide the steering angle of a vehicle (it
    doesn't control the acceleration and the brakes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network is fully connected with one input layer, one hidden layer, and one
    output layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The input consists of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 30 × 32 single-color image (they used the blue channel from an RGB image)
    from a forward-facing camera mounted on the vehicle.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An 8 × 32 image from a laser range finder. This is simply a grid, where each
    cell contains the distance to the nearest obstacle covered by that cell in the
    field of view.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One scalar input, which indicates the road intensity—that is, whether the road
    is lighter or darker than the nonroad in the image from the camera. This values
    comes recursively from the network output.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A single fully connected hidden layer with 29 neurons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fully connected output layer with 46 neurons. The curvature of the road is
    represented by 45 of those neurons in a way that resembles one-hot encoding—that
    is, if the middle neuron has the highest activation, then the road is straight.
    Conversely, the left and right neurons represent increasing road curvature. The
    final output unit indicates the road intensity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The network was trained for 40 epochs on a dataset of 1,200 images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/36614e50-69fd-4749-9fc3-5914fdb767fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The network architecture of ALVINN. Source: The ALVINN paper'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s take a look at the more recent timeline of (mostly) commercial
    AV progress:'
  prefs: []
  type: TYPE_NORMAL
- en: The DARPA Grand Challenge ([https://en.wikipedia.org/wiki/DARPA_Grand_Challenge](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge))
    was organized in 2004, 2005, and 2007\. In the first year, the participating teams'
    AVs had to navigate a 240 km route in the Mojave Desert. The best-performing AV
    managed just 11.78 km of that route, before getting hung up on a rock. In 2005,
    the teams had to overcome a 212 km off-road course in California and Nevada. This
    time, five vehicles managed to drive the whole route. The 2007 challenge was to
    navigate a mock urban environment, built in an air force base. The total route
    length was 89 km and the participants had to obey the traffic rules. Six vehicles
    finished the whole course.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2009, Google started developing self-driving technology. This effort led
    to the creation of Alphabet's (Google's parent company) subsidiary Waymo ([https://waymo.com/](https://waymo.com/)).
    In December 2018, they launched the first commercial on-demand ride-hailing service
    with AVs in Phoenix, Arizona. In October 2019, Waymo announced the start of the
    first truly driverless cars as part of their robotaxi service (previously, a safety
    driver had always been present).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mobileye ([https://www.mobileye.com/](https://www.mobileye.com/)) uses deep
    neural networks to provide driver-assistance systems (for example, lane-keeping
    assistance). The company has developed a series of **system-on-chip** (**SOC**)
    devices, specifically optimized to run neural networks with low energy consumption,
    required for automotive use. Its products are used by many of the major vehicle
    manufacturers. In 2017, Mobileye was acquired by Intel for $15.3 billion. Since
    then, BMW, Intel, Fiat-Chrysler, SAIC, Volkswagen, NIO, and the automotive supplier
    Delphi (now Aptiv) have cooperated on the joint development of self-driving technology.
    In the first three quarters of 2019, the total sales of Mobileye were $822 million,
    compared to $358 million in all four quarters of 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2016, General Motors acquired Cruise Automation ([https://getcruise.com/](https://getcruise.com/)),
    a developer of self-driving technology, for more than $500 million (the exact
    figure is unknown). Since then, Cruise Automation has tested and demonstrated
    multiple AV prototypes, driving in San Francisco. In October 2018, it was announced
    that Honda will also participate in the venture by investing $750 million in return
    for a 5.7% stake. In May 2019, Cruise secured $1.15 billion additional investment
    from a group of new and existing investors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2017, Ford Motor Co. acquired majority ownership of the self-driving startup
    Argo AI. In 2019, Volkswagen announced that it will invest $2.6 billion in Argo
    AI as part of a larger deal with Ford. Volkswagen would contribute $1 billion
    in funding and its $1.6 billion Autonomous Intelligence Driving subsidiary with
    more than 150 employees, based in Munich.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levels of automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about AVs, we usually imagine fully driverless vehicles. But in
    reality, we have cars that require a driver, but still provide some automated
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SAE has developed a scale of six levels of automation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Level 0**: The driver handles the steering, acceleration, and braking of
    the vehicle. The features at this level can only provide warnings and immediate
    assistance to the driver''s actions. Examples of features of this level include
    the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lane-departure warning simply warns the driver when the vehicle has crossed
    one of the lane markings.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A blind-spot warning warns the driver when another vehicle is located in the
    blind spot area of the car (the area immediately left or right of the rear end
    of the vehicle).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 1**: Features that provide either steering or acceleration/braking
    assistance to the driver. The most popular of these features in vehicles today
    are the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lane-keeping assist** (**LKA**): The vehicle can detect the lane markings
    and use the steering to keep itself centered in the lane.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive cruise control** (**ACC**): The vehicle can detect other vehicles
    and use brakes and acceleration to maintain a preset speed or reduce it, depending
    on the circumstances.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic emergency braking** (**AEB**): The vehicle can stop automatically
    if it detects an obstacle and the driver doesn''t react.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 2**: Features that provide both steering and brake/acceleration assistance
    to the driver. One such feature is a combination of LKA and adaptive cruise control.
    At this level, the car can return control to the driver without advance warning at
    any moment. Therefore, he or she has to maintain a constant focus on the road
    situation. For example, if the lane markings suddenly disappear, the LKA system
    can prompt the driver to take control of the steering immediately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 3**: This is the first level where we can talk about real autonomy.
    It is similar to level 2 in the sense that the car can drive itself under certain
    limited conditions and can prompt the driver to take control; however, this is
    guaranteed to happen in advance with sufficient time to allow an inattentive person
    to familiarize themselves with the road conditions. For example, say that the
    car drives itself on the highway, but the cloud-connected navigation obtains information
    about construction works on the road ahead. The driver will be prompted to take
    control well in advance of reaching the construction area.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 4**: Vehicles at level 4 are fully autonomous in a wider range of situations,
    compared to level 3\. For example, a locally geofenced (that is, limited to a
    certain region) taxi service could be at level 4\. There is no requirement for
    the driver to take control. Instead, if the vehicle goes outside this region,
    it should be able to safely abort the trip.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 5**: Full autonomy under all circumstances. The steering wheel is optional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All commercially available vehicles today have features at level 2 at most (even
    Tesla's Autopilot). The only exception (according to the manufacturer) is the
    2018 Audi A8, which has a level 3 feature called AI Traffic Jam Pilot. The system takes
    charge of driving at speeds up to 60 km/h on multilane roads with a physical barrier
    between the two directions of traffic. The driver can be prompted to take control
    with 10 seconds of advance warning. This feature was demonstrated during the launch
    of the vehicle, but as of the writing of this chapter, Audi cites regulatory limitations
    and doesn't include it in all markets. I have no information on where (or if)
    this feature is available.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at the components that make up an AV system.
  prefs: []
  type: TYPE_NORMAL
- en: Components of an AV system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll outline two types of AV system from a software architecture perspective.
    The first type uses sequential architecture with multiple components, as illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/748d39a7-f202-48aa-862e-71ac49331605.png)'
  prefs: []
  type: TYPE_IMG
- en: The components of an AV system
  prefs: []
  type: TYPE_NORMAL
- en: 'The system resembles the reinforcement-learning framework we briefly discussed
    in [Chapter 10](f641c4c2-60f2-41cb-a437-a961851dcc7f.xhtml), *Meta Learning*.
    We have a feedback loop where the environment (either the physical world or a
    simulation) provides the agent (vehicle) with its current state. In turn, the
    agent decides on its new trajectory, the environment reacts to it, and so on. Let''s
    start with the environment-perception subsystem, which has the following modules
    (we''ll discuss them in more detail in the following sections):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensors:** Physical devices, such as cameras and radars.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Localization:** Determines the exact position of the vehicle (with centimeter
    accuracy) within a high-definition map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving object detection and tracking:** Detects and tracks other traffic
    participants, such as vehicles and pedestrians.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of the perception system combines the data from its various modules
    to produce a **middle-level** virtual representation of the surrounding environment.
    This representation is usually a top-down (birds-eye) 2D view of the environment,
    referred to as the **occupancy map**. The following screenshot shows an example
    occupancy map of the ChauffeurNet system, which we''ll discuss later in the chapter.
    It includes road surfaces (white and yellow lines), traffic lights (red lines),
    and other vehicles (white rectangles). The image is best viewed in color:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c78674a-7d9a-4252-82a9-44391a061de9.png)'
  prefs: []
  type: TYPE_IMG
- en: Occupancy map of ChauffeurNet. Source: https://arxiv.org/abs/1812.03079
  prefs: []
  type: TYPE_NORMAL
- en: The occupancy map serves as input for the **path-planning** module, which uses
    it to determine the future trajectory of the vehicle. The **control** module takes
    the desired trajectory and translates it to low-level control inputs to the vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: The middle-level representation approach has several advantages. Firstly, it
    is well-suited for the functions of the path-planning and control modules. Also, instead
    of using the sensor data to create the top-down image, we can produce it with
    a simulator. In this way, it will be easier to collect training data, as we won't
    have to drive a real car. Even more important is that we'll be able to simulate
    situations that rarely occur in the real world. For example, our AV has to avoid
    crashes at any cost, yet real-world training data will have very few, if any,
    crashes. If we only use real sensor data, one of the most important driving situations
    will be severely underrepresented.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second type of AV system uses a single end-to-end component, which takes
    the raw sensor data as input and produces driving policy in the form of steering
    controls, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9696a032-249e-4692-adeb-fbc984c28b16.png)'
  prefs: []
  type: TYPE_IMG
- en: End-to-end AV system
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we already mentioned an end-to-end system when we discussed ALVINN
    (in the *Brief history of AV research *section). Next, we'll focus on the different
    modules of the sequential system. We'll cover the end-to-end system in more detail
    later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Environment perception
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For any automation feature to work, the vehicle needs a good perception of its
    surrounding environment. The environment-perception system has to identify the
    exact position, distance, and direction of moving objects, such as pedestrians,
    cyclists, and other vehicles. Additionally, it has to create a precise mapping
    of the road surface and the exact position of the vehicle on that surface and
    in the environment as a whole. Let's discuss the hardware and software components
    that help the AV create this virtual model of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Sensing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key to building a good environment model is the vehicle sensors. The following
    is a list of the most important sensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Camera**: Its images are used to detect the road surface, lane markings,
    pedestrians, cyclists, other vehicles, and so on. An important camera property
    in the automotive context (besides the resolution) is the field of view. It measures
    how much of the observable world the camera sees at any given moment. For example,
    with a 180^o field of view, it can see everything in front of it and nothing behind.
    With a 360^o field of view, it can see everything in front of it and everything
    behind the vehicle (full observation). The following different types of camera
    systems exist:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mono camera**: Uses a single forward-facing camera, usually mounted on the
    top of the windshield. Most automation features rely on this type of camera to
    work. A typical field of view for the mono camera is 125^o.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stereo camera**: A system of two forward-facing cameras, slightly removed
    from each other. The distance between the cameras allows them to capture the same
    picture from a slightly different angle and combine them into a 3D image (in the
    same way we use our eyes). A stereo system can measure the distance to some of
    the objects in the image, while a mono camera relies only on heuristics to do
    this.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**360^o surrounding ****view of the environment**: some vehicles have a system
    of four cameras (front, back, left, and right).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Night vision camera**: a system, where the vehicle includes a special type
    of headlight, which emits light in the infrared spectrum in addition to its regular
    function. The light is recorded from infrared cameras, which can display an enhanced
    image to the driver and detect obstacles during the night.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Radar**: A system that uses a transmitter to emit electromagnetic waves (in
    the radio or microwave spectrum) in different directions. When the waves reach
    an object, they are usually reflected, some of them in the direction of the radar
    itself. The radar can detect them with a special receiver antenna. Since we know
    that radio waves travel at the speed of light, we can calculate the distance to
    the reflected object by measuring how much time has passed between emitting and
    receiving the signal. We can also calculate the speed of an object (for example,
    another vehicle) by measuring the difference between the frequencies of the outgoing
    and incoming waves (Doppler effect). The "image" of the radar is noisier, narrower,
    and with lower resolution, compared to a camera image. For example, a long-range
    radar can detect objects at a distance of 160 m, but in a narrow 12^o field of
    view. The radar can detect other vehicles and pedestrians, but it won''t be able
    to detect the road surface or lane markings. It is usually used for ACC and AEB,
    while the LKA system uses a camera. Most vehicles have one or two front-facing
    radars and, on rare occasions, a rear-facing radar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lidar** (**light detection and ranging**): This sensor issomewhat similar to
    radar, but instead of radio waves, it emits laser beams in the near-infrared spectrum.
    Because of this, one emitted pulse can accurately measure the distance to a single
    point. Lidar emits multiple signals very fast in a pattern, which creates a 3D
    point cloud of the environment (the sensor can rotate very fast). The following
    is a diagram of how a vehicle would see the world with a lidar:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/60018263-4727-4132-9f86-8a6e09b8b17a.png)'
  prefs: []
  type: TYPE_IMG
- en: A diagram of how a vehicle sees the world through lidar
  prefs: []
  type: TYPE_NORMAL
- en: '**Sonar** (**sound navigation ranging**): This sensor emits pulses of ultrasonic
    waves and maps the environment by listening to the echos of the waves, reflected
    by the surrounding objects. Sonar is inexpensive compared to radar, but has a
    limited effective range of detection. Because of this, they are usually used in
    parking assistance features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data from multiple sensors can be merged into a single environment model
    with a process called **sensor fusion**. Sensor fusion is usually implemented using
    Kalman filters ([https://en.wikipedia.org/wiki/Kalman_filter](https://en.wikipedia.org/wiki/Kalman_filter)).
  prefs: []
  type: TYPE_NORMAL
- en: Localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Localization** is the process of determining the exact position of the vehicle on the
    map. Why is this important? Companies such as HERE ([https://www.here.com/](https://www.here.com/))
    specialize in creating extremely accurate road maps, where the entire area of
    the road surface is known to within a few centimeters. These maps could also include
    information about static objects of interest, such as lane markings, traffic signs,
    traffic lights, speed restrictions, zebra crossings, speed bumps, and so on. Therefore,
    if we know the exact position of the vehicle on the road, it won''t be hard to
    calculate the optimal trajectory.'
  prefs: []
  type: TYPE_NORMAL
- en: One obvious solution is to use GPS; however, GPS can be accurate to within 1-2
    meters under perfect conditions. In areas with high-rise buildings or mountains,
    the accuracy can suffer because the GPS receiver won't be able to get a signal
    from a sufficient number of satellites. One way to solve this problem is with **simultaneous
    localization and mapping** (**SLAM**) algorithms. These algorithms are beyond
    the scope of this book, but I encourage you to do your own research on the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Moving object detection and tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have an idea of what sensors the vehicle uses, and we have briefly mentioned
    the importance of knowing its exact location on the map. With this knowledge,
    the vehicle could theoretically navigate to its destination by simply following
    a breadcrumb trail of fine-grained points. However, the task of autonomous driving
    isn't that simple, because the environment is dynamic, as it includes moving objects
    such as vehicles, pedestrians, cyclists, and so on. An autonomous vehicle must
    constantly know the positions of the moving objects and track them as it plans
    its trajectory. This is one area where we can apply deep-learning algorithms to
    the raw sensor data. First, we'll do this for the camera. In [Chapter 5](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml),* Object
    Detection and Image Segmentation*, we discussed how to use **convolutional networks**
    (**CNNs**) in two advanced vision tasks—object detection and semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, object detection creates a bounding box around different classes of
    objects detected in the image. Semantic segmentation assigns a class label to
    every pixel of the image. We can use segmentation to detect the exact shape of
    the road surface and the lane markings on the camera image. We can use object
    detection to classify and localize the moving objects of interest in the environment;
    however, we have already covered these topics in [Chapter 5](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml), *Object
    Detection and Image Segmentation. *In this chapter, we'll focus on the lidar sensor
    and we'll discuss how to apply CNNs over the 3D point cloud this sensor produces.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've outlined the perception subsystem components, in the next section,
    we'll introduce the path planning subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: Path planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Path planning (or driving policy) is the process of calculating the vehicle trajectory
    and speed. Although we might have an accurate map and exact location of the vehicle,
    we still need to keep in mind the dynamics of the environment. The car is surrounded
    by other moving vehicles, pedestrians, traffic lights, and so on. What happens
    if the vehicle in front stops suddenly? Or if it's moving too slow? Our AV has
    to make the decision to overtake and then execute the maneuver. This is an area
    where ML and DL in particular can be especially useful, and we'll discuss two
    ways to implement these in this chapter. More specifically, we'll discuss using
    an imitation driving policy in an end-to-end learning system, as well as a driving
    policy algorithm called ChauffeurNet, which was developed by Waymo.
  prefs: []
  type: TYPE_NORMAL
- en: One obstacle in AV research is that building an AV and obtaining the necessary
    permits to test it is very expensive and time-consuming. Thankfully, we can still
    train our algorithms with the help of AV simulators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most popular simulators are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft AirSim, built on the Unreal Engine ([https://github.com/Microsoft/AirSim/](https://github.com/Microsoft/AirSim/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CARLA, built on the Unreal Engine ([https://github.com/carla-simulator/carla](https://github.com/carla-simulator/carla))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Udacity's Self-Driving Car Simulator, built with Unity ([https://github.com/udacity/self-driving-car-sim](https://github.com/udacity/self-driving-car-sim))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI Gym's `CarRacing-v0` environment (we'll see an example of this in the
    *Imitation driving policy *section)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our description of the components of an AV system. Next, we'll
    discuss how to process 3D spatial data.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to 3D data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The lidar produces a point cloud—a set of data points in a three-dimensional
    space. Remember that the lidar emits laser beams. A beam reflecting off of a surface
    and returning to the receiver generates a single data point of the point cloud.
    If we assume that the lidar device is the center of the coordinate system and
    each laser beam is a vector, then a point is defined by the vector''s direction
    and magnitude. Therefore, the point cloud is an **unordered set** of vectors.
    Alternatively, we can define the points by their Cartesian coordinates in ![](img/625cb830-2d9a-49f5-a000-0aab3caee3fd.png) space,
    as illustrated in the left side of the following diagram. In this case, the point
    cloud is a set of vectors ![](img/c75c2878-da97-4a02-aa93-f0662c483bd1.png), where
    each vector ![](img/579fc922-e0cf-4c6a-b7f4-5c2390bd2284.png) contains the three
    coordinates of the point. For the sake of clarity, each point is represented as
    a cube:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8461de45-e32b-4ef9-be6a-9245e28daec5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: Points (represented as cubes) in the 3D space; Right: A 3D grid of voxels'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's focus on the input data format for neural networks, and specifically
    CNNs. A 2D color image is represented as a tensor with three slices (one for each
    channel) and each slice is a matrix (2D grid) composed of pixels. The CNN uses
    2D convolutions (see[ Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*). Intuitively, we might think that we can
    use a similar 3D grid of **voxels** (a voxel is a 3D pixel) for 3D point clouds,
    as illustrated in the right image of the preceding diagram. Assuming the point
    cloud points have no color, we can represent the grid as a 3D tensor and use it
    as input to a CNN with 3D convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we take a closer look at this 3D grid, we can see that it is sparse.
    For example, in the preceding diagram, we have a point cloud with 8 points, but
    the grid contains 4 x 4 x 4 = 64 cells. In this simple case, we increase the memory
    footprint of the data eightfold, but in the real world the conditions, could be
    even worse. In this section, we''ll introduce PointNet (see *PointNet: Deep Learning
    on Point Sets for 3D* *Classification and Segmentation*, [https://arxiv.org/abs/1612.00593](https://arxiv.org/abs/1612.00593)),
    which provides a solution to this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PointNet takes as input the set of point cloud vectors **p***[i]*, rather than
    their 3D grid representation. To understand its architecture, we''ll start with
    the properties of the set of point cloud vectors that led to the network design
    (the following bullets contain quotes from the original paper):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unordered**: Unlike pixel arrays in images or voxel arrays in 3D grids, a
    point cloud is a set of points without a specific order. Therefore, a network
    that consumes *N* 3D point sets needs to be invariant to *N*! permutations of
    the input set in data-feeding order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interaction among points**: Similar to the pixels of an image, the distance
    between 3D points can indicate the level of relation among them—that is, it''s
    more likely that nearby points are part of the same object, compared to distant
    ones. Therefore, the model needs to be able to capture local structures from nearby
    points and the combinatorial interactions among local structures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invariance under transformations**: As a geometric object, the learned representation
    of the point set should be invariant to certain transformations. For example,
    rotating and translating points all together should not modify the global point
    cloud category, nor the segmentation of the points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we know these prerequisites, let''s see how PointNet addresses them.
    We''ll start with the network architecture and then we''ll discuss its components in
    more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed73d15b-694e-481b-9f54-a357d3b2435b.png)'
  prefs: []
  type: TYPE_IMG
- en: PointNet architecture. Source: https://arxiv.org/abs/1612.00593
  prefs: []
  type: TYPE_NORMAL
- en: PointNet is a **multilayer perceptron** (**MLP**). This is a feed-forward network
    that consists only of fully connected layers (and max pooling, but more on that
    later). As we mentioned, the set of input point cloud vectors **p***[i]* is represented
    as an *n* × 3 tensor. It is important to note that the network (up until the max
    pooling layer) is **shared** among all points of the set. That is, although the
    input size is *n* × 3, we can think of PointNet as applying the same network *n*
    times over *n* input vectors of size 1 × 3\. In other words, the network weights
    are shared among all points of the point cloud. This sequential arrangement also
    allows for an arbitrary number of input points.
  prefs: []
  type: TYPE_NORMAL
- en: The input passes through the input transform (we'll look at this in more detail
    later), which outputs another *n* × 3 tensor, where each of the *n* points is
    defined by three components (similar to the input tensor). This tensor is fed
    to an upsampling fully connected layer, which encodes each point to a 64-dimensional
    vector for *n* × 64 output. The network continues with another transformation,
    similar to the input transform. The result is then gradually upsampled with 64,
    then 128, and finally 1,024 fully connected layers to produce the final *n* ×
    1024 output. This tensor serves as input to a max pooling layer, which takes the
    maximum element of the same location among all *n* points and produces a 1,024-dimensional
    output vector. This vector is an aggregated representation of the whole set of
    points.
  prefs: []
  type: TYPE_NORMAL
- en: But why use max pooling in the first place? Remember that max pooling is a symmetric
    operation—that is, it will produce the same output regardless of the order of
    the inputs. At the same time, the set of points is unordered as well. Using max
    pooling ensures that the network will produce the same result regardless of the
    order of the points. The authors of the paper chose max pooling over other symmetric
    functions, such as average pooling and sum, because max pooling demonstrated the
    highest accuracy in the benchmark datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the max pooling, the network splits into two networks, depending on the
    type of task (see the preceding diagram):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: The 1024D aggregate vector serves as input to several fully
    connected layers, which end with *k*-way softmax, where *k* is the number of classes.
    This is a standard classification pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Segmentation**: This assigns a class to each point of the set. An extension
    of the classification net, this task requires a combination of local and global
    knowledge. As the diagram illustrates, we concatenate each of the *n* 64D intermediate
    point representations with the global 1024D vector for a combined *n* × 1088 tensor.
    Like the initial segment of the network, this path is also shared among all points.
    The vector of each point is downsampled to 128D with a series (1088 to 512, then
    to 256, and finally, to 128) fully connected layers. The final fully connected
    layer has *m* units (one for each class) and softmax activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we have explicitly addressed the unordered nature of the input data
    with the max pooling operation, but we still have to address the invariance and
    interaction among points. This is where the input and feature transforms will
    help. Let''s start with the input transform (in the preceding diagram, this is
    T-net). T-net is an MLP, which is similar to the full PointNet (it is referred
    to as a mini-PointNet), as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb033604-0864-4697-93ed-c98a43d30f3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Input (and feature) transform T-nets
  prefs: []
  type: TYPE_NORMAL
- en: The input transform T-net takes as input the *n* × 3 set of points (the same
    input as the full network). Like the full PointNet, T-net is shared among all
    points. First, the input is upsampled to *n* × 1024 with 64-, then 128-, and finally,
    1024-unit fully connected layers. The upsampled output is fed to a max pooling
    operation, which outputs 1 × 1024 vector. Then, the vector is downsampled to 1
    × 256 using two 512- and 256-unit fully connected layers. The 1 × 256 vector is
    multiplied by a 256 × 9 matrix of global (shared) learnable weights. The result
    is reshaped as a 3 × 3 matrix, which is multiplied by the original input point **p***[i]* over
    all points to produce the final *n* × 3 output tensor. The intermediate 3 × 3
    matrix acts as a type of learnable affine transformation matrix over the set of
    points. In this way, the points are normalized into a familiar perspective with
    respect to the network—that is, the network becomes invariant under transformations.
    The second T-net (feature transform) is almost identical to the first, with the
    exception that the input tensor is *n* × 64, which results in a 64 × 64 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the global max pooling layer ensures that the network is not influenced
    by the order of the data, it has another disadvantage, because it creates a single
    representation of the whole input set of points; however, these points might belong
    to different objects (for example, vehicles and pedestrians). In situations like
    this, the global aggregation could be problematic. To solve this, the authors
    of PointNet introduced PointNet++ (see *PointNet++: Deep Hierarchical Feature
    Learning on Point Sets in a Metric Space *at [https://arxiv.org/abs/1706.02413](https://arxiv.org/abs/1706.02413)),
    which is a hierarchical neural network that applies PointNet recursively on a
    nested partitioning of the input point set.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at 3D data processing in the context of the AV environment-perception
    system. In the next section, we'll shift our attention to the path-planning system
    with an imitation driving policy.
  prefs: []
  type: TYPE_NORMAL
- en: Imitation driving policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *Components of an AV system* section, we outlined several modules that
    were necessary for a self-driving system. In this section, we''ll look at how
    to implement one of them—the driving policy—with the help of DL. One way to do
    this is with RL, where the car is the agent and the environment is, well, the
    environment. Another popular approach is **imitation learning**, where the model
    (network) learns to imitate the actions of an expert (human). Let''s look at the
    properties of imitation learning in the AV scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll use a type of imitation learning, known as **behavioral cloning**. This
    simply means that we'll train our network in a supervised way. Alternatively,
    we could use imitation learning in a reinforcement learning (RL) scenario, which
    is known as inverse RL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the network is the driving policy, represented by the desired
    steering angle and/or acceleration or braking. For example, we can have one regression
    output neuron for the steering angle and one neuron for acceleration or braking
    (as we cannot have both at the same time).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The network input can be either of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw sensor data for end-to-end systems—for example, an image from the forward-facing
    camera. AV systems, where a single model uses raw sensor inputs and outputs a
    driving policy, are referred to as **end-to-end**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Middle-level environment representation for sequential composite systems.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll create the training dataset with the help of an expert. We''ll let the
    expert drive the vehicle manually, either in the real world or in a simulator.
    At each step of the journey, we''ll record the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current state of the environment. This could be the raw sensor data or the
    top-down view representation. We'll use the current state as input for the model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The actions of the expert in the current state of the environment (steering
    angle and braking/acceleration). This will be the target data for the network.
    During training, we'll simply minimize the error between the network predictions
    and the driver actions using the familiar gradient descent. In this way, we'll
    teach the network to imitate the driver.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The behavioral cloning scenario is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ceac0735-09ab-4cc5-b4f8-b8d8793e6663.png)'
  prefs: []
  type: TYPE_IMG
- en: Behavioral cloning scenario
  prefs: []
  type: TYPE_NORMAL
- en: As we have already mentioned, ALVINN (from the *Brief history of AV research *section)
    is a behavioral cloning end-to-end system. More recently, the paper *End to End
    Learning for Self-Driving Cars* ([https://arxiv.org/abs/1604.07316](https://arxiv.org/abs/1604.07316))
    introduced a similar system, which uses a CNN with five convolutional layers instead
    of a fully connected network. In their experiment, the images of a forward-facing
    camera on the vehicle are fed as input to the CNN. The output of the CNN is a
    single scalar value, which represents the desired steering angle of the car. The
    network doesn't control acceleration and braking. To build the training dataset,
    the authors of the paper collected about 72 hours of real-world driving videos.
    During the evaluation, the car was able to drive itself 98% of the time in a suburban
    area (excluding making lane changes and turns from one road to another). Additionally,
    it managed to drive without intervention for 16 km on a multilane divided highway. In
    the following section, we'll implement something fun—a behavioral cloning example
    with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral cloning with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll implement a behavioral cloning example with PyTorch 1.3.1. To
    help us with this task, we'll use OpenAI Gym ([https://gym.openai.com/](https://gym.openai.com/)),
    which is an open source toolkit for the development and comparison of reinforcement
    learning algorithms. It allows us to teach **agents** to undertake various tasks,
    such as walking or playing games such as Pong, Pinball, some other Atari games,
    and even Doom.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can install it with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we''ll use the `CarRacing-v0` OpenAI Gym environment, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/741b8c00-00c9-4f8a-834c-cbca078fd4f7.png)'
  prefs: []
  type: TYPE_IMG
- en: In the CarRacing-v0 environment, the agent is a racing car; a birds-eye view
    is used the whole time
  prefs: []
  type: TYPE_NORMAL
- en: This example contains multiple Python files. In this section, we'll mention
    the most important parts. The full source code is at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning).
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is for the red racing car (referred to as the agent) to drive around
    the track as quickly as it can without sliding off of the road surface. We can
    control the car using four actions: accelerate, brake, turn left, and turn right.
    The input for each action is continuous—for example, we can specify full throttle
    with the value 1.0 and half throttle with the value 0.5 (the same goes for the
    other controls).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of simplicity, we''ll assume that we can only specify two discrete
    action values: 0 for no action and 1 for full action. Since, originally, this
    was an RL environment, the agent will receive an award at each step as it progresses
    along the track; however, we''ll not use it, since the agent will learn directly
    from our actions. We''ll perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a training dataset by driving the car around the track ourselves (we'll
    control it with the keyboard arrows). In other words, we'll be the expert that
    the agent tries to imitate. At every step of the episode, we'll record the current
    game frame (state) and the currently pressed keys, and we'll store them in a file.
    The full code for this step is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/keyboard_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/keyboard_agent.py).
    All you have to do is run the file and the game will start. As you play, the episodes
    will be recorded (once every five episodes) in the `imitation_learning/data/data.gzip` file.
    If you want to start over, you can simply delete it. You can exit the game by
    pressing *Escape* and pause the game using the *Spacebar*. You can also start
    a new episode by pressing *Enter*. In this case, the current episode will be discarded
    and its sequence will not be stored. We suggest that you play at least 20 episodes
    for a sufficient size of the training dataset. It would be good to use the brake
    more often because otherwise, the dataset will become too imbalanced. In normal
    play, acceleration is used much more frequently than the brake or the steering.
    Alternatively, if you don't want to play, the GitHub repository already includes
    an existing data file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent is represented by a CNN. We'll train it in a supervised manner using
    the dataset we just generated. The input will be a single game frame and the output
    will be a combination of steering direction and brake/acceleration. The target
    (labels) will be the action recorded for the human operator. If you want to omit
    this step, the repository already has a trained PyTorch network located at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning/data/model.pt](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter11/imitation_learning/data/model.pt).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the CNN agent play by using the network output to determine the next action
    to send to the environment. You can do this by simply running the [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py)[ file.
    If you haven't performed any of the previous two steps, this file will use the
    existing agent. ](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that introduction, let's continue by preparing the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll look at how to generate a training dataset and load it
    as an instance of PyTorch's `torch.utils.data.DataLoader` class. We'll highlight
    the most relevant parts of the code, but the full source code is located at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/train.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/train.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create the training dataset in several steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `read_data` function reads `imitation_learning/data/data.gzip` in two `numpy` arrays:
    one for the game frames and the other for the keyboard combinations associated
    with them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The environment accepts actions, composed of a three-element array, where the
    following are true:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first element has a value in the range `[-1, 1]` and represents the steering
    angle (`-1` for right, `1` for left).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The second element is in the `[0, 1]` range and represents the throttle.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The third element is in the `[0, 1] `range and represents the brake power.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll use the seven most common key combinations: `[0, 0, 0]` for no action
    (the car is coasting), `[0, 1, 0]` for acceleration, `[0, 0, 1]` for brake, `[-1,
    0, 0]` for left, `[-1, 0, 1]` for a combination of left and brake, `[1, 0, 0]` for
    right, and` [1, 0, 1]` for the right and brake combination. We have deliberately
    prevented the simultaneous use of acceleration and left or right, as the car becomes
    very unstable. The rest of the combinations are implausible. The `read_data` phrase
    will convert these arrays to a single class label from `0` to `6`. In this way,
    we'll simply solve a classification problem with seven classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `read_data` function will also balance the dataset. As we mentioned, acceleration
    is the most common key combination, while some of the others, such as brake, are
    the rarest. Therefore, we'll remove some of the acceleration samples and we'll
    multiply some of the braking (and left/right + brake). However, the author did this in
    a heuristic way by trying multiple combinations of deletion/multiplication ratios
    and selected the ones that work best. If you record your own dataset, your driving
    style may differ, and you may want to modify these ratios.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have the `numpy` arrays of the training samples, we'll use the `create_datasets`
    function to convert them to `torch.utils.data.DataLoader` instances. These classes
    simply allow us to extract the data in mini batches and apply data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, let''s implement the `data_transform` list of transformations, which
    modify the image before feeding it to the network. The full implementation is
    available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/util.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/util.py).
    We''ll convert the image to grayscale, normalize the color values in the `[0,
    1]` range, and crop the bottom part of the frame (the black rectangle, which shows the
    rewards and other information). The implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s shift our attention back to the `create_datasets` function. We''ll
    start with the declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll implement the `TensorDatasetTransforms` helper class to be able
    to apply the `data_transform` transformations over the input image. The implementation
    is as follows (please bear in mind the indentation, as this code is still part
    of the `create_datasets` function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll read the previously generated dataset in full:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll create the training and validation data loaders (`train_loader`
    and `val_loader`). Finally, we''ll return them as the result of the `create_datasets`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's focus on the agent NN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the agent neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The agent is represented by a CNN with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: A single-input 84 × 84 slice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three convolutional layers with striding for downsampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELU activations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two fully connected layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seven output neurons (one for each neuron).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization and dropout, applied after each layer (even the convolutional)
    to prevent overfitting. Overfitting in this task is particularly exaggerated because
    we cannot use any meaningful data augmentation techniques. For example, say that
    we randomly flipped the image horizontally. In this case, we would have to also
    alter the label to reverse the steering value. Therefore, we'll rely on regularization
    as much as we can.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code block shows the network implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Having implemented the training dataset and the agent, we can proceed with the
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll implement the training itself with the help of the `train` function, which
    takes the network and the `cuda` device as parameters. We''ll use cross-entropy
    loss and the Adam optimizer (the usual combination for classification tasks).
    The function simply iterates `EPOCHS` times and calls the `train_epoch` and `test` functions
    for each epoch. The following is the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll implement the `train_epoch` for a single epoch training. This
    function iterates over all mini batches and performs forward and backward passes
    for each one. The following is the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `train_epoch` and `test` functions are similar to the ones we implemented
    for the transfer learning code example in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),* Understanding
    Convolutional Networks*. To avoid repetition, we won't implement the `test` function
    here, although it's available in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: We'll run the training for around 100 epochs, but you can shorten this to 20
    or 30 epochs for rapid experiments. One epoch usually takes less than a minute
    using the default training set. Now that we are familiar with the training, let's
    see how to use the agent NN to drive the race car in our simulated environment.
  prefs: []
  type: TYPE_NORMAL
- en: Letting the agent drive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start by implementing the `nn_agent_drive` function, which allows the
    agent to play the game (defined in [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/nn_agent.py)).
    The function will start the `env` environment with an initial state (game frame).
    We'll use it as an input to the network. Then, we'll convert the softmax network
    output from one-hot encoding to an array-based action and we'll send it to the
    environment to make the next step. We'll repeat these steps until the episode
    ends. The `nn_agent_drive` function also allows the user to exit by pressing *Escape*.
    Note that we still use the same `data_transform` transformations as we did for
    the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll implement the initialization part, which binds the *Esc* key
    and initializes the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll implement the main loop, where the agent (vehicle) takes an `action`,
    the environment returns the new `state`, and so on. This dynamic is reflected
    in the infinite `while` loop (please mind the indentation, as this code is still
    part of `nn_agent_play`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We now have all the ingredients to run the program, which we will do in the
    following section.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we can run the whole thing. The full code for this is available at
    [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/main.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/blob/master/Chapter11/imitation_learning/main.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet builds and restores (if available) the network, runs
    the training, and evaluates the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Although we cannot show the agent in action here, you can easily see it in action
    by following the instructions in this section. Still, we can say that it learns
    well and is able to make full laps of the racing track on a regular basis (but
    not always). Interestingly, the network's driving style strongly resembles the
    style of the operator who generated the dataset. The example also goes to show
    that we shouldn't underestimate supervised learning. We were able to create a
    decently performing agent with a small dataset, and in a relatively short training
    time.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude our imitation learning example. Next, we'll discuss a
    much more sophisticated driving policy algorithm called ChauffeurNet.
  prefs: []
  type: TYPE_NORMAL
- en: Driving policy with ChauffeurNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll discuss a recent paper called *ChauffeurNet: Learning
    to Drive by Imitating the Best and Synthesizing the Worst* ([https://arxiv.org/abs/1812.03079](https://arxiv.org/abs/1812.03079)).
    It was released in December 2018 by Waymo, one of the leaders in the field of
    AV. Let''s look at some of the properties of the ChaffeurNet model:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a combination of two interconnected networks. The first is a CNN called
    FeatureNet, which extracts features from the environment. These features are fed
    as inputs to a second, recurrent network called AgentRNN, which determines the
    driving policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses imitation supervised learning in a similar way to the algorithms we
    described in the *Imitation driving policy* section. The training set is generated
    based on records of real-world driving episodes. ChauffeurNet can handle complex
    driving situations, such as lane changes, traffic lights, traffic signs, changing
    from one street to another, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper is published by Waymo on arxiv.org and is used here for referential
    purposes only. Waymo and arxiv.org are not affiliated, and do not endorse this
    book, or the authors.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start our discussion about ChauffeurNet with the input and output data
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The end-to-end approach feeds raw sensor data (for example, camera images) to
    the ML algorithm (NN), which in turn produces the driving policy (steering angle
    and acceleration). In contrast, ChauffeurNet uses the middle-level input and output
    that we introduced in the *Components of an AV system *section. Let's look at
    the input to the ML algorithm first. This is a series of top-down (birds-eye)
    view 400 × 400 images, similar to the images of the `CarRacing-v0` environment,
    but much more complex. One moment of time *t* is represented by multiple images,
    where each one contains different elements of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see an example of a ChauffeurNet input/output combination in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b1c1d0b-b3f6-43aa-b788-9ca9cc18fec2.png)'
  prefs: []
  type: TYPE_IMG
- en: ChauffeurNet inputs. Source: https://arxiv.org/abs/1812.03079
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the input elements ((a) through (g)) in alphabetical order:'
  prefs: []
  type: TYPE_NORMAL
- en: (a) is a precise representation of the road map. It is an RGB image, which uses
    different colors to represent various road features, such as lanes, cross-walks,
    traffic signs, and curbs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (b) is a temporal sequence of grayscale images of the traffic lights. Unlike
    the features of (a), the traffic lights are dynamic—that is, they can be green,
    red, or yellow at different times. In order to properly convey their dynamics,
    the algorithm uses a series of images, displaying the state of the traffic lights
    for each lane at each of the past *T[scene]* seconds up to the current moment.
    The color of the lines in each image represents the state of each traffic light,
    where the brightest color is red, intermediate is for yellow, and the darkest
    is green or unknown.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (c) is a grayscale image with the known speed limit for each lane. Different
    color intensities represent different speed limits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (d) is the intended route between the start and the destination. Think of it
    as the directions generated by Google Maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (e) is a grayscale image that represents the current location of the agent (displayed
    as a white box).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (f) is a temporal sequence of grayscale images that represents the dynamic elements
    of the environment (displayed as boxes). These could be other vehicles, pedestrians,
    or cyclists. As these objects change locations over time, the algorithm conveys
    their trajectories with a series of snapshot images, representing their positions
    over the last *T[scene]* seconds. This works in the same way as the traffic lights
    (b).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (g) is a single grayscale image for the agent trajectory of the past *T[pose]* seconds
    until the current moment. The agent locations are displayed as a series of points
    on the image. Note that we display them in a single image, and not with a temporal
    sequence like the other dynamic elements. The agent at moment *t* is represented
    in the same top-down environment with the properties ![](img/84bc5eb3-b984-40f9-83a0-017cbc17a302.png),
    where ![](img/ef972f65-5d7b-415e-9661-845d5e4be8c8.png)is the coordinates, ![](img/c760b272-4b6d-4b96-bfd5-ea1372ae9872.png) is
    the orientation (or heading), and ![](img/2fa074d1-6bdd-498c-9926-88bc260d5ed9.png) is
    the speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(h) is the algorithm middle-level output: the agent''s future trajectory, represented
    as a series of points. These points carry the same meaning as the past trajectory
    (g). The future location output at time *t+1* is generated by using the past trajectory
    (g) up to the current moment *t.* We''ll denote ChauffeurNet as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d515d56e-ac45-42e9-9921-252b0148adbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *I* is all the preceding input images, **p***[t]* is the agent position
    at time *t*, and *δt* is a 0.2 s time delta. The value of *δt* is arbitrary, chosen
    by the authors of the paper. Once we have *t+δt*, we can add it to the past trajectory
    (g) and we can use it to generate the next location at step *t+2δt* in a recurrent
    manner. The newly generated trajectory is fed to the control module of the vehicle,
    which tries its best to execute it via the vehicle controls (steering, acceleration,
    and brakes).
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in the *Components of an AV system *section, this middle-level
    input representation allows us to use different sources of training data with
    ease. It can be generated from real-world driving with a fusion of the vehicle
    sensor inputs (such as cameras and lidar) and mapping data (such as streets, traffic
    lights, traffic signs, and so on). But we can also generate images of the same
    format with a simulated environment. The same applies to the middle-level output,
    where the control module can be attached to various types of physical vehicles
    or to a simulated vehicle. Using a simulation makes it possible to learn from
    situations that occur rarely in the real world, such as emergency braking or even
    crashes. To help the agent learn about such situations, the authors of the paper
    explicitly synthesized multiple rare scenarios using simulations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are familiar with the data representation, let's shift our focus
    to the model's core components.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the ChauffeurNet model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f9e71b0-3b08-4565-8e1c-ff12d9fe8d86.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) ChauffeurNet architecture and (b) the memory updates over the iterations Source: https://arxiv.org/abs/1812.03079
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have FeatureNet (in the preceding diagram, this is marked by (a)).
    This is a CNN with residual connections, whose inputs are the top-down images
    we looked at in the *Input and output representations *section. The output of FeatureNet
    is a feature vector *F* which represents the synthesized network''s understanding
    of the current environment. This vector serves as one of the inputs to the recurrent
    network AgentRNN, which predicts successive points in the driving trajectory iteratively.
    Let''s say that we want to predict the next point of the agent''s trajectory at
    step *k*. In this case, AgentRNN has the following outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**p***[k]* is the predicted next point of the driving trajectory at that step.
    As we can see from the diagram, the output of AgentRNN is actually a heatmap with
    the same dimensions as the input images. It represents a probability distribution *P[k](x,
    y)* over the spatial coordinates, which indicates the probability of the next
    waypoint over each cell (pixel) of the heatmap. We use the `arg-max` operation
    to obtain the coarse pose prediction **p***[k]* from this heatmap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*B[k]* is the predicted bounding box of the agent at step *k.* Like the waypoint
    output, *B[k]* is a heatmap, but, here, each cell uses sigmoid activation and
    represents the probability that the agent occupies that particular pixel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also two additional outputs that are not displayed in the diagram: *θ[k]* for
    the heading (or orientation) of the agent and *s[k]* for the desired speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChauffeurNet also includes an additive memory, denoted by *M* (in the preceding diagram,
    this is marked by (b)). *M* is the single-channel input image (g) that we defined in
    the *Input and output representations *section. It represents the waypoint predictions (**p***[k, ]***p***[k-1], ....,* **p***[0]*)
    of the past steps *k*. The current waypoint **p***[k]* is added to the memory
    at each step, as displayed in the preceding diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outputs **p***[k]* and *B[k]* are fed back recursively as inputs to AgentRNN
    for the next step *k+1*. The formula for the AgentRNN output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/252b8e39-5d59-4993-93b1-094d00c4ccb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s check how ChauffeurNet integrates within the sequential AV pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3876c6c-0069-4f16-b7dd-c5977df1b337.png)'
  prefs: []
  type: TYPE_IMG
- en: ChauffeurNet within the full end-to-end driving pipeline. Source: https://arxiv.org/abs/1812.03079
  prefs: []
  type: TYPE_NORMAL
- en: 'The system resembles the feedback loop that we introduced in the *Components
    of an AV system *section. Let''s look at its components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Renderer**: Receives input from both the environment and the dynamic
    router. Its role is to transform these signals into the top-down input images
    we defined in the *Input and output representations* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic Router**: Provides the intended route, which is dynamically updated,
    based on whether the agent was able to reach the previous target coordinates.
    Think of it as a navigation system, where you input a destination and it provides
    you with a route to the target. You start navigating this route and, if you stray
    from it, the system will calculate a new route dynamically based on your current
    location and your destination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural Net**: The ChauffeurNet module, which outputs the desired future trajectory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controls Optimization**: Receives the future trajectory and translates it
    into low-level control signals that drive the vehicle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChauffeurNet is a rather complex system, so let's now look at how to train it.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ChauffeurNet was trained with 30 million expert driving examples using imitation
    supervised learning. The model inputs are the top-down images we defined in the
    *Input and output representations *section, as illustrated in the following flattened
    (aggregated) input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6203fbf-aed4-4f4b-aaa7-f16469147f55.png)'
  prefs: []
  type: TYPE_IMG
- en: The image is best viewed in color. Source: https://arxiv.org/abs/1812.03079
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the components of the ChauffeurNet training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/890829fc-0939-4861-8b1e-b450fd4da108.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ChauffeurNet training components: (a) the model itself, (b) the additional
    networks, and (c) the losses. Source: https://arxiv.org/abs/1812.03079'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are already familiar with the ChauffeurNet model itself (marked as (a) in
    the preceding image). Let''s focus on the two additional networks involved in
    the process (marked as (b) in the preceding image):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Road Mask** N**et**: Outputs a segmentation mask with the exact area of the
    road surface over the current input images. To better understand this, the following
    image illustrates a target road mask (left) and the network''s predicted road
    mask (right):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/63622968-9603-4650-9a12-80bc4b73e053.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: https://arxiv.org/abs/1812.03079
  prefs: []
  type: TYPE_NORMAL
- en: '**PerceptionRNN**: Outputs a segmentation mask with the predicted future locations
    of every other dynamic object in the environment (vehicles, cyclists, pedestrians,
    and so on). The output of PerceptionRNN is illustrated in the following diagram,
    which shows the predicted location of other vehicles (the light rectangles):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/eabe2207-e95e-4023-8315-8dd16e9750b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: https://arxiv.org/abs/1812.03079
  prefs: []
  type: TYPE_NORMAL
- en: These networks don't participate in the final vehicle control and are used only
    during training. The goal behind their use is that the FeatureNet network will
    learn better representations if it receives feedback from the tree tasks (AgentRNN,
    Road Mask Net, and PerceptionRNN), compared to simply getting feedback from AgentRNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s focus on the various loss functions (the bottom section (c) of
    the ChauffeurNet schema). We''ll start with the imitation losses, which reflect
    how the model prediction of the future agent position differs from the human expert
    ground truth. The following list shows the AgentRNN outputs with their corresponding
    loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A probability distribution *P[k](x, y)* over the spatial coordinates of the
    predicted waypoint **p***[k]*. We''ll train this component with the following
    loss:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/06ac107b-6888-48bf-add1-08c315700ae4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/eb3acd58-a819-4e47-b23a-d09f0d696616.png) is the cross-entropy
    loss, ![](img/a8bff431-0e74-4985-bef3-9b94eb2e25e1.png) is the predicted distribution,
    and ![](img/172dac83-1613-41ae-a687-beb7a8e1faf9.png) is the ground truth distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'A heatmap of the agent bounding box *B[k]*. We can train it with the following
    loss (applied along the cells of the heatmap):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6e47c556-c586-4a2a-8e19-b305c2ecee06.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *W* and *H* are the input image dimensions, ![](img/8ec84f29-ae73-4729-b881-976620f96840.png) is
    the predicted heatmap, and ![](img/ee23c93f-9e74-40a8-aadb-8a18d59e2f70.png) is
    the ground truth heatmap.
  prefs: []
  type: TYPE_NORMAL
- en: 'The heading  (orientation) of the agent *θ[k]* with the following loss:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6e7cd74c-8631-4413-8ed4-b7ce69925a00.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *θ[k]* is the predicted orientation and ![](img/1d0f268f-c485-4cca-a0c4-2961c61c9c6a.png) is
    the ground truth orientation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the paper also introduce past motion dropout. We can best explain
    this by citing the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model is provided the past motion history as one of the
    inputs (image (g) of the schema in section *Input and output representations*).
    Since the past motion history during training is from an expert demonstration,
    the net can learn to "cheat" by just extrapolating from the past rather than finding
    the underlying causes of the behavior. During closed-loop inference, this breaks
    down because the past history is from the net’s own past predictions. For example,
    such a trained net may learn to only stop for a stop sign if it sees a deceleration
    in the past history, and will therefore never stop for a stop sign during closed-loop
    inference. To address this, we introduce a dropout on the past pose history, where
    for 50% of the examples, we keep only the current position *(u[0], v[0])* of the
    agent in the past agent poses channel of the input data. This forces the net to
    look at other cues in the environment to explain the future motion profile in
    the training example.
  prefs: []
  type: TYPE_NORMAL
- en: 'They also observed that the imitation learning approach works well when the
    driving situation does not differ significantly from the expert driving training
    data. However, the agent has to be prepared for many driving situations that are
    not part of the training, such as collisions. If the agent only relies on the
    training data, it will have to learn about collisions implicitly, which is not
    easy. To solve this problem, the paper proposes explicit loss functions for the
    most important situations. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Waypoint loss**: The error between the ground truth and the predicted agent
    future position *p[k]*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed loss**: The error between the ground truth and the predicted agent
    future speed *s[k]*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Heading loss**: The error between the ground truth and the predicted agent
    future direction *θ[k]*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent-box loss**: The error between the ground truth and the predicted agent
    bounding box *B[k]*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Geometry loss**: Force the agent to explicitly follow the target trajectory,
    independent of the speed profile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-road loss**: Force the agent to navigate only over the road surface area
    and avoid the nonroad areas of the environment. This loss will increase if the
    predicted bounding box of the agent overlaps with the nonroad area of the image,
    predicted by the road mask network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collision loss**: Explicitly force the agent to avoid collisions. This loss
    will increase if the agent''s predicted bounding box overlaps with the bounding
    boxes of any of the other dynamic objects of the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChauffeurNet performed well in various real-world driving situations. You can
    see some of the results at [https://medium.com/waymo/learning-to-drive-beyond-pure-imitation-465499f8bcb2](https://medium.com/waymo/learning-to-drive-beyond-pure-imitation-465499f8bcb2).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the applications of deep learning in AVs. We started
    with a brief historical overview of AV research and we discussed the different
    levels of autonomy. Then we described the components of the AV system and identified
    when it's appropriate to use DL techniques. Next, we looked at 3D-data processing
    and PointNet. Then we introduced the topic of implementing driving policies using
    behavioral cloning, and we implemented an imitation learning example with PyTorch.
    Finally, we looked at Waymo's ChauffeurNet system.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes our book. I hope you enjoyed the read!
  prefs: []
  type: TYPE_NORMAL
