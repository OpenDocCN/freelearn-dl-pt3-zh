- en: Credit Card Fraud Detection using Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The digital world is growing rapidly. We are used to performing many of our
    daily tasks online, such as booking cabs, shopping on e-commerce websites, and
    even recharging our phones. For the majority of these tasks, we are used to paying
    with credit cards. However, it is a known fact that a credit card can be compromised,
    which could result in a fraudulent transaction. The Nilson report estimates that
    for every $100 spent, seven cents are stolen. It estimates the total credit card
    fraud market to be around $30 billion.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting whether a transaction is fraudulent or not is a very impactful data
    science problem. Every bank that issues credit cards invests in technology to
    detect fraud and take the appropriate actions immediately. There are lot of standard
    supervised learning techniques such as logistic regression, from random forest
    to classifying fraud.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will take a closer look at an unsupervised approach to
    detecting credit card fraud using auto-encoders by exploring the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding auto-encoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining and training a fraud detection model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing a fraud detection model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding auto-encoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Auto-encoders are a type of artificial neural network whose job is to learn
    a low-dimensional representation of input data using unsupervised learning. They
    are quite popular when it comes to dimensionality reduction of input and in generative
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, an auto-encoder learns to compress data into a low-dimensional
    representation and then reconstructs that representation into something that matches
    the original data. This way, the low-dimensional representation ignores the noise,
    which is not helpful in reconstructing the original data.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, they are also useful in generating models, particularly
    images. For example, if we feed the representation of *dog* and *flying*, it may
    attempt to generate an image of a *flying cat*, which it has not seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Structurally, auto-encoders consist of two parts, the encoder and the decoder.
    The encoder generates the low-dimensional representation of inputs, and the decoder
    helps regenerate the input from the representation. Generally, the encoder and
    the decoder are feedforward neural networks with one or multiple hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the configuration of a typical auto-encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cf79baa-d1b7-46d2-8b33-5bf772eeb6bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To define the encoder (![](img/3edd31b1-5963-4738-b6fa-6c48d1fed1d8.png)) and
    the decoder (![](img/50571af6-621f-44c8-bb24-8f8c7aca02ce.png)), an auto-encoder
    can be mathematically represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7192ce52-f955-4587-8bdb-6bcb988d11ae.png)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned in the equation, the parameters of the encoder and the decoder
    are optimized in a way that minimizes a special kind of error, which is known
    as **reconstruction error.** Reconstruction error is the error between the reconstructed
    input and the original input.
  prefs: []
  type: TYPE_NORMAL
- en: Building a fraud detection model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this project, we are going to use the credit card dataset from Kaggle ([https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)),
    Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating
    Probability with Undersampling for Unbalanced Classification. In Symposium on
    Computational Intelligence and Data Mining (CIDM), IEEE, 2015\. It consists of
    credit card transaction data from two days, from European cardholders. The dataset
    is highly imbalanced and contains approximately 284,000 pieces of transaction
    data with 492 instances of fraud (0.172% of the total).
  prefs: []
  type: TYPE_NORMAL
- en: There are 31 numerical columns in the dataset. Two of them are time and amount.
    **Time** denotes the amount of time elapsed (in seconds) between each transaction
    and the first transaction in the dataset. **Amount** is the total amount regarding
    the transaction. For our model, we will eliminate the time column as it doesn't
    help with the accuracy of the model. The rest of the features (V1, V2 ... V28)
    are obtained from Principal Component Analysis ([https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/)) of
    original features for confidential reasons. **Class** is the target variable,
    which indicates whether the transaction was fraudulent or not.
  prefs: []
  type: TYPE_NORMAL
- en: To pre-process the data, there is not much that needs to be done. This is mainly
    because a lot of the data is already cleaned up.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, in a classical machine learning model such as logistic regression,
    we feed the data points of both negative and positive classes into the algorithm.
    However, since we are using auto-encoders, we will model it differently.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, our training set will consist of only non-fraudulent transaction
    data. The idea is that whenever we pass a fraudulent transaction through our trained
    model, it should detect it as an anomaly. We are framing this problem as anomaly
    detection rather than classification.
  prefs: []
  type: TYPE_NORMAL
- en: The model will consist of two fully connected encoder layers with 14 and seven
    neurons, respectively. There will be two decoder layers with seven and 29 neurons,
    respectively. Additionally, we will use L1 regularization during training.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, to define the model, we will use Keras with Tensorflow at the backend
    for training auto-encoders.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is a technique in machine learning that's used to reduce overfitting.
    Overfitting happens when the model learns a signal as well as noise in the training
    data and can't generalize well to unseen dataset. While there are many ways to
    avoid overfitting such as cross validation, sampling, and so on, regularization
    specifically adds a penalty to weights of the model so that we don't learn an
    overly complex model. L1 regularization adds an L1 norm penalty on all the weights
    of the model. This way, any weight that doesn't contribute to the accuracy is
    shrunk to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Defining and training a fraud detection model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps for defining and training the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform `''Amount''` by removing the mean and scaling it to the unit''s variance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that we use the `StandardScaler` utility from scikit-learn for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'To model our dataset, split it into train and test data, with train consisting
    of only non-fraudulent transaction and test consisting of both fraudulent and
    non-fraudulent transactions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the model by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is defined, train the model using Keras:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following parameters to find the output of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: EPOCHS = 100.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: BATCH_SIZE = 32.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: OPTIMIZER = 'Adam'.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LOSS = Mean squared error between reconstructed and original input.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: EVAL_METRIC = 'Accuracy'. This is the usual binary classification accuracy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Store a `TensorBoard` file to visualize the graph or other variables on it.
    Also, store the best-performing model through the checkpoints provided by Keras. Generate
    the loss curves by epoch for the training and testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram illustrates the loss curves that are generated when the
    model is trained for 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/996d8452-9c97-496b-a9cf-c5e3ff1ecbe8.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that for the training set, the loss or reconstruction error decreases
    at the start of the training and saturates toward the end. This saturation implies
    that the model has finished learning the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Save the model with the lowest loss in the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: Testing a fraud detection model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the training process is complete, break down the reconstruction error
    in the testing set by fraudulent and non-fraudulent (normal) transactions. Generate
    the reconstruction error by different classes of transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagrams illustrate the reconstruction error distribution of
    fraudulent and normal transactions in the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ca17733-d635-496e-ba21-ec6d1717ded3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next diagram is for fraud transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9facdbf-16d4-451d-a201-2015bc4ffe6f.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the reconstruction error for normal transactions is very close
    to zero for most of the transactions. However, the reconstruction error with fraudulent
    transactions has a wide distribution, with the majority still being close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that a threshold on the reconstruction error can serve as a classification
    threshold on normal versus fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For evaluating the model, we will use the metrics Precision and Recall which
    were defined in [Chapter 4](5de5c0e0-5fcc-4352-af84-4b813138ea90.xhtml), *Digit
    Classification Using TensorFlow Lite*, of the book. Firstly, let''s look at the
    precision and recall at various thresholds of reconstruction error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The reconstruction error threshold for precision and recall are shown in the
    following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f72f802-1e2c-47bc-a9dd-8aaa6acf9b9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The diagram represents the error threshold for recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb99eb0a-81d4-4ee5-803e-4e5566976672.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, recall decreases when there is an increase in reconstruction
    error, and vice versa for precision. There are a few dips due to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There is one other thing that we need to keep in mind. As mentioned previously, there
    is always a trade-off between high precision and high recall in machine learning.
    We need to choose any one for our particular model.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, businesses prefer a model with high precision or high recall. For
    fraud detection, we would like to have a model with high recall. This is essential
    as we can classify the majority of fraudulent transactions as fraud. One method
    to counter the loss of precision is to do a manual verification of transactions
    classified as fraud to determine whether they are actually fraudulent. This will
    help in ensuring a good experience for the end user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to generate a confusion matrix with `min_recall` = 80%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix that''s obtained from the preceding code is shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bd4f84c-ac36-4477-abb8-f39711b06812.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that out of 120 fraudulent transactions, 97 of them have been
    classified correctly. However, we have also classified 1,082 normal transaction
    as being fraudulent, which will have to go through a manual verification process
    to ensure a good experience for the end user.
  prefs: []
  type: TYPE_NORMAL
- en: As a note of caution, we should not assume that auto-encoders are helpful in
    all binary classification tasks and can achieve better performance than state-of-the-art
    classification models. The idea behind this project was to illustrate a different
    approach of using auto-encoders to perform classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this chapter, we have used the same validation and test set for
    illustrative purposes. Ideally, once we have defined the threshold on the reconstruction
    error, we should test the model on some unseen dataset to evaluate its performance
    in a better manner.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Credit card fraud are ubiquitous in nature. Every company in today's world is
    employing machine learning to combat payment fraud on their platform. In this
    chapter, we looked at the problem of classifying fraud using the credit card dataset
    from Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned about auto-encoders as a dimensionality reduction technique. We
    understood that the auto-encoder architecture consists of two components: an encoder
    and a decoder. We model the parameters of a fully connected network using reconstruction
    loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Thereafter, we looked at the fraud classification problem through the lens of
    an anomaly detection problem. We trained the auto-encoder model using normal transactions.
    We then looked at the reconstruction error of the auto-encoder for both normal
    and fraudulent transactions, and observed that the reconstruction error has a
    wide distribution for fraudulent transactions. We then defined a threshold on
    reconstruction to classify the model and generated the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the concept of Bayesian neural networks,
    which combines the concepts of deep learning and Bayesian learning to model uncertainty
    in the prediction of deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an auto-encoder?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are different components of an auto-encoder?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the reconstruction loss?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the precision and recall?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
