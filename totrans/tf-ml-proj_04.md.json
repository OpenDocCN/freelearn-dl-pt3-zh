["```\nx_train = x_train.reshape(x_train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 1)\nx_test = x_test.reshape(x_test.shape[0], IMAGE_SIZE, IMAGE_SIZE, 1)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nNext, we normalize the image pixels by 255 as follows:\nx_train /= 255\nx_test /= 255\nAnd finally, we convert the class labels to one hot for training as follows:\ny_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\ny_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\n\n```", "```\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\nactivation='relu',\ninput_shape=INPUT_SHAPE))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(NUM_CLASSES))model.add(Activation('softmax', name = 'softmax_tensor'))\n```", "```\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n\noptimizer=keras.optimizers.Adadelta(),\n\nmetrics=['accuracy'])\n```", "```\ntensorboard = TensorBoard(log_dir=MODEL_DIR)\n```", "```\nself.model.fit(self.x_train, self.y_train,\nbatch_size=BATCH_SIZE,\n\nepochs=EPOCHS,\n\nverbose=1,\n\nvalidation_data=(self.x_test, self.y_test),\n\ncallbacks = [self.tensorboard])\n\nscore = self.model.evaluate(self.x_test, self.y_test, verbose=0)\n```", "```\ntensorboard --logdir <model_folder>\n```", "```\nfrom TensorFlow.python.framework.graph_util import convert_variables_to_constants\n\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n\ngraph = session.graph\n\nwith graph.as_default():\n\nfreeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n\noutput_names = output_names or []\n\noutput_names += [v.op.name for v in tf.global_variables()]\n\ninput_graph_def = graph.as_graph_def()\n\nif clear_devices:\n\nfor node in input_graph_def.node:\n\nnode.device = \"\"\n\nfrozen_graph = convert_variables_to_constants(session, input_graph_def,\n\noutput_names, freeze_var_names)\n\nreturn frozen_graph\n\n```", "```\n\ndef pb_to_tensorboard(input_graph_dir,graph_type =\"freeze\"):\n\n  file_name = \"\"\n\n  if graph_type == \"freeze\":\n\n      file_name = FREEZE_FILE_NAME\n\n  elif graph_type == \"optimize\":\n\n      file_name = OPTIMIZE_FILE_NAME\n\n  with tf.Session() as sess:\n\n      model_filename = input_graph_dir + \"/\" + file_name\n\n      with gfile.FastGFile(model_filename, 'rb') as f:\n\n           graph_def = tf.GraphDef()\n\n           graph_def.ParseFromString(f.read())\n\n  train_writer = tf.summary.FileWriter(input_graph_dir)\n\n  train_writer.add_graph(sess.graph)\n\n```", "```\ndef optimize_graph(input_dir, output_dir):\ninput_graph = os.path.join(input_dir, FREEZE_FILE_NAME)\noutput_graph = os.path.join(output_dir, OPTIMIZE_FILE_NAME)\ninput_graph_def = tf.GraphDef()\nwith tf.gfile.FastGFile(input_graph, \"rb\") as f:\ninput_graph_def.ParseFromString(f.read())\noutput_graph_def = strip(input_graph_def, u'dropout_1', u'conv2d_2/bias', u'dense_1/kernel', u'training')\noutput_graph_def = strip(output_graph_def, u'dropout_3', u'max_pooling2d_2/MaxPool', u'flatten_2/Shape',\nu'training')\noutput_graph_def = strip(output_graph_def, u'dropout_4', u'dense_3/Relu', u'dense_4/kernel', u'training')\noutput_graph_def = strip(output_graph_def, u'Adadelta_1', u'softmax_tensor_1/Softmax',\nu'training/Adadelta/Variable', u'training')\noutput_graph_def = strip(output_graph_def, u'training', u'softmax_tensor_1/Softmax',\nu'_', u'training')\nwith tf.gfile.GFile(output_graph, \"wb\") as f:\nf.write(output_graph_def.SerializeToString())\n```", "```\ntoco \\\n--input_file=<model_folder>/logs/optimized/MNIST_optimized.pb\\\n--input_format=TensorFlow_GRAPHDEF \\\n--output_format=TFLITE \\\n--inference_type=FLOAT \\\n--input_type=FLOAT \\\n--input_arrays=conv2d_1_input \\\n--output_arrays=softmax_tensor_1/Softmax \\\n--input_shapes=1,28,28,1 \\\n--output_file=<model_folder>//mnist.tflite\n```"]