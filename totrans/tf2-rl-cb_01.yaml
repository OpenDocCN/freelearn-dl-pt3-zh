- en: '*Chapter 1*: Developing Building Blocks for Deep Reinforcement Learning Using
    Tensorflow 2.x'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides a practical and concrete description of the fundamentals
    of **Deep Reinforcement Learning** (**Deep RL**) filled with recipes for implementing
    the building blocks using the latest major version of **TensorFlow 2.x**. It includes
    recipes for getting started with RL environments, **OpenAI Gym**, developing neural
    network-based agents, and evolutionary neural agents for addressing applications
    with both discrete and continuous value spaces for Deep RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes are discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Building an environment and reward mechanism for training RL agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing neural network-based RL policies for discrete action spaces and
    decision-making problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing neural network-based RL policies for continuous action spaces and
    continuous-control problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with OpenAI Gym for RL training environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural evolutionary agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code in the book has been extensively tested on Ubuntu 18.04 and Ubuntu
    20.04 and should work with later versions of Ubuntu as long as Python 3.6+ is
    available. With Python 3.6 installed along with the necessary Python packages
    as listed before the start of each of the recipes, the code should run fine on
    Windows and macOS X too. It is advised to create and use a Python virtual environment
    named `tf2rl-cookbook` to install the packages and run the code in this book.
    Miniconda or Anaconda installation for Python virtual environment management is
    recommended.The complete code for each recipe in this chapter will be available
    here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Building an environment and reward mechanism for training RL agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe will walk you through the steps to build a **Gridworld** learning
    environment to train RL agents. Gridworld is a simple environment where the world
    is represented as a grid. Each location on the grid can be referred to as a cell.
    The goal of an agent in this environment is to find its way to the goal state
    in a grid like the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – A screenshot of the Gridworld environment ](img/B15074_01_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – A screenshot of the Gridworld environment
  prefs: []
  type: TYPE_NORMAL
- en: The agent's location is represented by the blue cell in the grid, while the
    goal and a mine/bomb/obstacle's location is represented in the grid using green
    and red cells, respectively. The agent (blue cell) needs to find its way through
    the grid to reach the goal (green cell) without running over the mine/bomb (red
    cell).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/Conda virtual environment and `pip install numpy gym`. If the following
    import statements run without issues, you are ready to get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we can begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train RL agents, we need a learning environment that is akin to the datasets
    used in supervised learning. The learning environment is a simulator that provides
    the observation for the RL agent, supports a set of actions that the RL agent
    can perform by executing the actions, and returns the resultant/new observation
    as a result of the agent taking the action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to implement a Gridworld learning environment that
    represents a simple 2D map with colored cells representing the location of the
    agent, goal, mine/bomb/obstacle, wall, and empty space on a grid:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by first defining the mapping between different cell states and
    their color codes to be used in the Gridworld environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, generate a color map using RGB intensity values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now define the action mapping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s then create a `GridworldEnv` class with an `__init__` function to define
    necessary class variables, including the observation and action space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will implement `__init__()` in the following steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, let''s define the layout of the Gridworld environment using the
    grid cell state mapping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding layout, `0` corresponds to the empty cells, `1` corresponds
    to walls, `2` corresponds to the agent's starting location, `3` corresponds to
    the location of the mine/bomb/obstacle, and `4` corresponds to the goal location
    based on the mapping we defined in step 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we are ready to define the observation space for the Gridworld RL environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the action space and the mapping between the actions and the
    movement of the agent in the grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now wrap up the `__init__` function by initializing the agent''s start
    and goal states using the `get_state()` method (which we will implement in the
    next step):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to implement the `get_state()` method that returns the start and
    goal state for the Gridworld environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will be implementing the `step(action)` method to execute
    the action and retrieve the next state/observation, the associated reward, and
    whether the episode ended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s specify the rewards and finally, return `grid_state`, `reward`,
    `done`, and `info`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Up next is the `reset()` method, which resets the Gridworld environment when
    an episode completes (or if a request to reset the environment is made):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To visualize the state of the Gridworld environment in a human-friendly manner,
    let's implement a render function that will convert the `grid_layout` that we
    defined in step 5 to an image and display it. With that, the Gridworld environment
    implementation will be complete!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To test whether the environment is working as expected, let''s add a `__main__`
    function that gets executed if the environment script is run directly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All set! The Gridworld environment is ready and we can quickly test it by running
    the script (`python envs/gridworld.py`). An output such as the following will
    be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following rendering of the Gridworld environment will also be displayed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.2 – The Gridworld ](img/B15074_01_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – The Gridworld
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see how it works!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `grid_layout` defined in step 5 in the *How to do it…* section represents
    the state of the learning environment. The Gridworld environment defines the observation
    space, action spaces, and the rewarding mechanism to implement a `env.render()`
    method converts the environment's internal grid representation to an image and
    displays it for visual understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural network-based RL policies for discrete action spaces and
    decision-making problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many environments (both simulated and real) for RL requires the RL agent to
    choose an action from a list of actions or, in other words, take discrete actions.
    While simple linear functions can be used to represent policies for such agents,
    they are often not scalable to complex problems. A non-linear function approximator
    such as a (deep) neural network can approximate arbitrary functions, even those
    required to solve complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: The neural network-based policy network is a crucial building block for advanced
    RL and **Deep RL** and will be applicable to general, discrete decision-making
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this recipe, you will have an agent with a neural network-based
    policy implemented in **TensorFlow 2.x** that can take actions in the **Gridworld**
    environment and (with little or no modifications) in any discrete-action space
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activate the `tf2rl-cookbook` Python virtual environment and run the following
    to install and import the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will look at policy distribution types that can be used by agents in environments
    with discrete action spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by creating a binary policy distribution in TensorFlow 2.x using
    the `tensorflow_probability` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code should print something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The values of the action that you get will differ from what is shown here because
    they will be sampled from the Bernoulli distribution, which is not a deterministic
    process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s quickly visualize the binary policy distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will generate a distribution plot as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.3 – A distribution plot of the binary policy ](img/B15074_01_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 1.3 – A distribution plot of the binary policy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we will be implementing a discrete policy distribution. A categorical
    distribution over a single discrete variable with *k* finite categories is referred
    to as a **multinoulli** distribution. The generalization of the multinoulli distribution
    to multiple trials is the multinomial distribution that we will be using to represent
    discrete policy distributions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code should print something along the lines of the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we visualize the discrete probability distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will generate a distribution plot, like the one shown here
    for `discrete_policy`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.4 – A distribution plot of the discrete policy ](img/B15074_01_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 1.4 – A distribution plot of the discrete policy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, calculate the entropy of a discrete policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, implement a discrete policy class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we implement a helper method to evaluate the agent in a given environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now implement a neural network Brain class using TensorFlow 2.x:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now implement a simple agent class that uses a `DiscretePolicy` object
    to act in discrete environments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now test the agent in `GridworldEnv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This shows how to implement the policy. We will see how this works in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the central components of an RL agent is the policy function that maps
    between observations and actions. Formally, a policy is a distribution over actions
    that prescribes the probabilities of choosing an action given an observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In environments where the agent can take at most two different actions, for
    example, in a binary action space, we can represent the policy using a **Bernoulli
    distribution**, where the probability of taking action 0 is given by ![](img/Formula_01_001.png),
    and the probability of taking action 1 is given by ![](img/Formula_01_002.png),
    which gives rise to the following probability distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_01_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A discrete probability distribution can be used to represent an RL agent's policy
    when the agent can take one of *k* possible actions in an environment.
  prefs: []
  type: TYPE_NORMAL
- en: In a general sense, such distributions can be used to describe the possible
    results of a random variable that can take one of *k* possible categories and
    is therefore also called a **categorical distribution**. This is a generalization
    of the Bernoulli distribution to k-way events and is therefore a multinoulli distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural network-based RL policies for continuous action spaces and
    continuous-control problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning has been used to achieve the state of the art in many
    control problems, not only in games as varied as Atari, Go, Chess, Shogi, and
    StarCraft, but also in real-world deployments, such as HVAC control systems.
  prefs: []
  type: TYPE_NORMAL
- en: In environments where the action space is continuous, meaning that the actions
    are real-valued, a real-valued, continuous policy distribution is necessary. A
    continuous probability distribution can be used to represent an RL agent's policy
    when the action space of the environment contains real numbers. In a general sense,
    such distributions can be used to describe the possible results of a random variable
    when the random variable can take any (real) value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the recipe is complete, you will have a complete script to control a car
    in two dimensions to drive up a hill using the `MountainCarContinuous` environment
    with a continuous action space. A screenshot from the `MountainCarContinuous`
    environment is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – A screenshot of the MountainCarContinuous environment ](img/B15074_01_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – A screenshot of the MountainCarContinuous environment
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activate the `tf2rl-cookbook` Conda Python environment and run the following
    command to install and import the necessary Python packages for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will begin by creating continuous policy distributions using `tensorflow_probability`
    library and build upon the necessary action sampling methods to generate action
    for a given continuous space of an RL environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a continuous policy distribution in TensorFlow 2.x using the `tensorflow_probability`
    library. We will use a Gaussian/normal distribution to create a policy distribution
    over continuous values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we visualize a continuous policy distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will generate a distribution plot of the continuous policy,
    like the plot shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.6 – A distribution plot of the continuous policy ](img/B15074_01_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 1.6 – A distribution plot of the continuous policy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now implement a continuous policy distribution using a Gaussian/normal
    distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code should print something similar to what is shown in the following
    code block:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The values of the action that you get will differ from what is shown here because
    they will be sampled from the Gaussian distribution, which is not a deterministic
    process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now move one step further and implement a multi-dimensional continuous
    policy. A **multivariate Gaussian distribution** can be used to represent multi-dimensional
    continuous policies. Such polices are useful for agents when acting in environments
    with action spaces that are multi-dimensional, as well as continuous and real-valued:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code should print something similar to what follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before moving on, let''s visualize the multi-dimensional continuous policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will generate a joint distribution plot similar to the plot
    shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Joint distribution plot of a multi-dimensional continuous policy
    ](img/B15074_01_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 1.7 – Joint distribution plot of a multi-dimensional continuous policy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we are ready to implement the continuous policy class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a next step, let''s implement a multi-dimensional continuous policy class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now implement a function to evaluate an agent in an environment with
    a continuous action space to assess episodic performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to test the agent in a continuous action environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement a simple agent class that utilizes the `ContinuousPolicy`
    object to act in continuous action space environments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a final step, we will test the performance of the agent in a continuous
    action space environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding script will call the `MountainCarContinuous` environment, render
    it to the screen, and show how the agent is performing in this continuous action
    space environment:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.8 – A screenshot of the agent in the MountainCarContinuous-v0 environment
    ](img/B15074_01_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – A screenshot of the agent in the MountainCarContinuous-v0 environment
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's explore how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We implemented a continuous-valued policy for RL agents using a **Gaussian
    distribution**. Gaussian distribution, which is also known as **normal distribution**,
    is the most widely used distribution for real numbers. It is represented using
    two parameters, µ and σ. We generated continuous-valued actions from such a policy
    by sampling from the distribution, based on the probability density that is given
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_01_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The **multivariate normal distribution** extends the normal distribution to
    multiple variables. We used this distribution to generate multi-dimensional continuous
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: Working with OpenAI Gym for RL training environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe provides a quick run-through for getting up and running with OpenAI
    Gym environments. The Gym environment and the interface provide a platform for
    training RL agents and is the most widely used and accepted RL environment interface.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be needing the full installation of OpenAI Gym to be able to use the
    available environments. Please follow the Gym installation steps listed at [https://github.com/openai/gym#id5](https://github.com/openai/gym#id5).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a minimum, you should execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by picking an environment and exploring the Gym interface. You may
    already be familiar with the basic function calls to create a Gym environment
    from the previous recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your steps should be formatted like so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first explore the list of environments in Gym:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This script will print the names of all the environments available through your
    Gym installation, sorted alphabetically. You can run this script using the following
    command to see the names of the environments that are installed and available
    in your system. You should see a long list of environments listed. The first few
    are shown in the following screenshot for your reference:![Figure 1.9 – List of
    environments available using the openai-gym package ](img/B15074_01_009.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 1.9 – List of environments available using the openai-gym package
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's now see how we can run one of the Gym environments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following script will let you explore any of the available Gym environments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can save the preceding script to `run_gym_env.py` and run the script like
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 1.10 – Sample output of run_gym_env.py with Alien-v4 1000 as the arguments
    ](img/B15074_01_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Sample output of run_gym_env.py with Alien-v4 1000 as the arguments
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You can change `Alien-v4` to any of the available Gym environments listed in
    the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A summary of how the Gym environments work is presented in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1.1 – Summary of the Gym environment interface ](img/Table_1.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1.1 – Summary of the Gym environment interface
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can find more information on OpenAI Gym here: [http://gym.openai.com/](http://gym.openai.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe will guide you through the steps to build a complete agent and
    the agent-environment interaction loop, which is the main building block for any
    RL application. When you complete the recipe, you will have an executable script
    where a simple agent tries to act in a Gridworld environment. A glimpse of what
    the agent you build will likely be doing is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Screenshot of output from the neural_agent.py script ](img/B15074_01_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – Screenshot of output from the neural_agent.py script
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s get started by activating the `tf2rl-cookbook` Conda Python environment
    and running the following code to install and import the necessary Python modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: import tensorflow as tf
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start by implementing a Brain class powered by a neural network implemented
    using TensorFlow 2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first initialize a neural brain model using TensorFlow 2.x and the Keras
    functional API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we implement the Brain class''s `call(…)` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to implement the Brain class''s `process()` method to conveniently
    perform predictions on a batch of inputs/observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now implement the init function of the agent class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let''s define a simple policy function for the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, let''s implement a convenient `get_action` method for the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now create a placeholder function for `learn()` that will be implemented
    as part of RL algorithm implementation in future recipes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This completes our basic agent implementation with the necessary ingredients!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now evaluate the agent in a given environment for one episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s implement the main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the script as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the Gridworld environment GUI pop up. This will show you what
    the agent is doing in the environment, and it will look like the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.12 – A screenshot of the neural agent acting in the Gridworld environment
    ](img/B15074_01_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – A screenshot of the neural agent acting in the Gridworld environment
  prefs: []
  type: TYPE_NORMAL
- en: This provides a simple, yet complete, recipe to build an agent and the agent-environment
    interaction loop. All that is left is to add the RL algorithm of your choice to
    the `learn()` method and the agent will start acting intelligently!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe puts together the necessary ingredients to build a complete agent-environment
    system. The `Brain` class implements the neural network that serves as the processing
    unit of the agent, and the agent class utilizes the `Brain` class and a simple
    policy that chooses an action based on the output of the brain after processing
    the observations received from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: We implemented the `Brain` class as a subclass of the `keras.Model` class, which
    allows us to define a custom neural network-based model for the agent's brain.
    The `__init__` method initializes the `Brain` model and defines the necessary
    layers using the `Brain` model, we are creating two `__init__` method, the `call(…)`
    method is also a mandatory method that needs to be implemented by child classes
    inheriting from the `keras.Model` class. The `call(…)` method first converts the
    inputs to a TensorFlow 2.x tensor and then flattens the inputs to be of the shape
    `1 x total_number_of_elements` in the input tensor. For example, if the input
    data has a shape of 8 x 8 (8 rows and 8 columns), the data is first converted
    to a tensor and the shape is flattened to 1 x 8 * 8 = 1 x 64\. The flattened inputs
    are then processed by the dense1 layer, which contains 32 neurons and a ReLU activation
    function. Finally, the logits layer processes the output from the previous layer
    and produces n number of outputs corresponding to the action dimension (n).
  prefs: []
  type: TYPE_NORMAL
- en: The `predict_on_batch(…)` method performs predictions on the batch of inputs
    given as the argument. This function (unlike the `predict()` function of **Keras**)
    assumes that the inputs (observations) provided as the argument are exactly one
    batch of inputs and thus feeds the batch to the network without any further splitting
    of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then implemented the `Agent` class and, in the agent initialization function,
    we created an object instance of the Brain class by defining the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Here, `input_shape` is the shape of the input that is expected to be processed
    by the brain, and `action_dim` is the shape of the output expected from the brain.
    The agent's policy is defined to be a custom `DiscretePolicy` from the previous
    recipe to initialize the agent's policy as well.
  prefs: []
  type: TYPE_NORMAL
- en: The agent's policy function, `policy_mlp`, flattens the input observations and
    sends it for processing by the agent's brain to receive the `action_logits`, which
    are the unnormalized probabilities for the actions. The final action to be taken
    is obtained by using TensorFlow 2.x's `categorical` method from the random module,
    which samples a valid action from the given `action_logits` (unnormalized probabilities).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If all of the observations supplied to the `predict_on_batch` function cannot
    be accommodated in the given amount of GPU memory or RAM (CPU), the operation
    can cause a GPU **Out Of Memory** (**OOM**) error.
  prefs: []
  type: TYPE_NORMAL
- en: The main function that gets launched – if the `neural_agent.py` script is run
    directly – creates an instance of the Gridworld-v0 environment, initializes an
    agent using the action and observation space of this environment, and starts evaluating
    the agent for 10 episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural evolutionary agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evolutionary methods are based on black-box optimization and are also known
    as gradient-free methods since no gradient computation is involved. This recipe
    will walk you through the steps for implementing a simple, approximate cross-entropy-based
    neural evolutionary agent using **TensorFlow 2.x**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activate the `tf2rl-cookbook` Python environment and import the following packages
    necessary to run this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: With the packages installed, we are ready to begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s put together all that we have learned in this chapter to build a neural
    agent that improves its policy to navigate the Gridworld environment using an
    evolutionary process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the basic neural agent and the Brain class from `neural_agent.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s implement a method to roll out the agent in a given environment
    for one episode and return `obs_batch`, `actions_batch`, and `episode_reward`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now test the trajectory rollout method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it''s time for us to verify that the experience data generated using the
    rollouts is coherent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now roll out multiple complete trajectories to collect experience data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then visualize the reward distribution from a sample of experience data.
    Let''s also plot a red vertical line at the 50th percentile of the episode reward
    values in the collected experience data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code will generate a plot like the one shown in the following
    diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.13 – Histogram plot of the episode reward values ](img/B15074_01_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 1.13 – Histogram plot of the episode reward values
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now create a container for storing trajectories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now it''s time to choose elite experiences for the evolution process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now test the elite experience gathering routine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now look at implementing a helper method to convert discrete action
    indices to one-hot encoded vectors or probability distribution over actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s now time to test the action distribution generation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s create and compile the neural network brain with TensorFlow 2.x
    using the Keras functional API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now test the brain training loop as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should produce the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The numbers may vary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next big step is to implement an agent class that can be initialized with
    a brain to act in an environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement a helper function to evaluate the agent in a given
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now test the agent evaluation loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a next step, let''s define the parameters for the training loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now create the `environment`, `brain`, and `agent` objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate a plot like the one shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The episode rewards will vary and the plots may look different.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Plot of the mean reward (solid, red) and reward threshold for
    elites (dotted, green) ](img/B15074_01_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Plot of the mean reward (solid, red) and reward threshold for
    elites (dotted, green)
  prefs: []
  type: TYPE_NORMAL
- en: The solid line in the plot is the mean reward obtained by the neural evolutionary
    agent, and the dotted line shows the reward threshold used for determining the
    elites.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On every iteration, the evolutionary process rolls out or collects a bunch of
    trajectories to build up the experience data using the current set of neural weights
    in the agent's brain. An elite selection process is then employed that picks the
    top *k* percentile (elitism criterion) trajectories/experiences based on the episode
    reward obtained in that trajectory. This shortlisted experience data is then used
    to update the agent's brain model. The process repeats for a preset number of
    iterations allowing the agent's brain model to improve and collect more rewards.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information, I suggest reading *The CMA Evolution Strategy: A Tutorial*:
    [https://arxiv.org/pdf/1604.00772.pdf](https://arxiv.org/pdf/1604.00772.pdf).'
  prefs: []
  type: TYPE_NORMAL
