["```\nsudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev \n```", "```\nsudo pip install mpi4py \n```", "```\npip install stable-baselines[mpi] \n```", "```\nimport gym\nfrom stable_baselines import DQN \n```", "```\nenv = gym.make('MountainCar-v0') \n```", "```\nagent = DQN('MlpPolicy', env, learning_rate=1e-3) \n```", "```\nagent.learn(total_timesteps=25000) \n```", "```\nfrom stable_baselines.common.evaluation import evaluate_policy \n```", "```\nmean_reward, n_steps = evaluate_policy(agent, agent.get_env(), n_eval_episodes=10) \n```", "```\nagent.save(\"DQN_mountain_car_agent\") \n```", "```\nagent = DQN.load(\"DQN_mountain_car_agent\") \n```", "```\nstate = env.reset() \n```", "```\nfor t in range(5000): \n```", "```\n action, _ = agent.predict(state) \n```", "```\n next_state, reward, done, info = env.step(action) \n```", "```\n state = next_state \n```", "```\n env.render() \n```", "```\n#import the libraries\nimport gym\nfrom stable_baselines import DQN\nfrom stable_baselines.common.evaluation import evaluate_policy\n#create the gym environment\nenv = gym.make('MountainCar-v0')\n#instantiate the agent\nagent = DQN('MlpPolicy', env, learning_rate=1e-3)\n#train the agent\nagent.learn(total_timesteps=25000)\n#evaluate the agent\nmean_reward, n_steps = evaluate_policy(agent, agent.get_env(), n_eval_episodes=10)\n#save the trained agent\nagent.save(\"DQN_mountain_car_agent\")\n#view the trained agent\nstate = env.reset()\nfor t in range(5000):\n    action, _ = agent.predict(state)\n    next_state, reward, done, info = env.step(action)\n    state = next_state\n    env.render() \n```", "```\nfrom stable_baselines.common.vec_env import SubprocVecEnv\nfrom stable_baselines.common import set_global_seeds \n```", "```\ndef make_env(env_name, rank, seed=0):\n    def _init():\n        env = gym.make(env_name)\n        env.seed(seed + rank)\n        return env\n    set_global_seeds(seed)\n    return _init \n```", "```\nenv_name = 'Pendulum-v0'\nnum_process = 2\nenv = SubprocVecEnv([make_env(env_name, i) for i in range(num_process)]) \n```", "```\nfrom stable_baselines.common.vec_env import DummyVecEnv \n```", "```\nenv_name = 'Pendulum-v0'\nenv = DummyVecEnv([lambda: gym.make(env_name)]) \n```", "```\nenv = CustomEnv() \n```", "```\nagent = DQN('MlpPolicy', env, learning_rate=1e-3)\nagent.learn(total_timesteps=25000) \n```", "```\nfrom stable_baselines import DQN \n```", "```\nfrom stable_baselines.deepq.policies import CnnPolicy \n```", "```\nfrom stable_baselines.common.atari_wrappers import make_atari \n```", "```\nenv = make_atari('IceHockeyNoFrameskip-v4') \n```", "```\nagent = DQN(CnnPolicy, env, verbose=1) \n```", "```\nagent.learn(total_timesteps=25000) \n```", "```\nstate = env.reset()\nwhile True:\n    action, _ = agent.predict(state)\n    next_state, reward, done, info = env.step(action)\n    state = next_state\n    env.render() \n```", "```\nkwargs = {\"double_q\": True, \"prioritized_replay\": True, \"policy_kwargs\": dict(dueling=True)} \n```", "```\nagent = DQN(CnnPolicy, env, verbose=1, **kwargs) \n```", "```\nagent.learn(total_timesteps=25000) \n```", "```\nimport gym\nfrom stable_baselines.common.policies import MlpPolicy\nfrom stable_baselines.common.vec_env import DummyVecEnv\nfrom stable_baselines.common.evaluation import evaluate_policy\nfrom stable_baselines import A2C \n```", "```\nenv = gym.make('LunarLander-v2') \n```", "```\nenv = DummyVecEnv([lambda: env]) \n```", "```\nagent = A2C(MlpPolicy, env, ent_coef=0.1, verbose=0) \n```", "```\nagent.learn(total_timesteps=25000) \n```", "```\nmean_reward, n_steps = evaluate_policy(agent, agent.get_env(), n_eval_episodes=10) \n```", "```\nstate = env.reset()\nwhile True:\n    action, _states = agent.predict(state)\n    next_state, reward, done, info = env.step(action)\n    state = next_state\n    env.render() \n```", "```\nfrom stable_baselines.common.policies import FeedForwardPolicy \nnet_arch=[dict(pi=[128, 128, 128], vf=[128, 128, 128])], which specifies our network architecture. pi represents the architecture of the policy network and vf represents the architecture of value network: \n```", "```\nclass CustomPolicy(FeedForwardPolicy):\n    def __init__(self, *args, **kargs):\n        super(CustomPolicy, self).__init__(*args, **kargs,\n                                           net_arch=[dict(pi=[128, 128, 128], vf=[128, 128, 128])], feature_extraction=\"mlp\") \n```", "```\nagent = A2C(CustomPolicy, 'LunarLander-v2', verbose=1) \n```", "```\nagent.learn(total_timesteps=25000) \n```", "```\nimport gym\nimport numpy as np\nfrom stable_baselines.ddpg.policies import MlpPolicy\nfrom stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec\nfrom stable_baselines import DDPG \n```", "```\nenv = gym.make('Pendulum-v0') \n```", "```\nn_actions = env.action_space.shape[-1] \n```", "```\naction_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions)) \n```", "```\nagent = DDPG(MlpPolicy, env, verbose=1, param_noise=None, action_noise=action_noise) \n```", "```\nagent.learn(total_timesteps=25000) \n```", "```\nagent = DDPG(MlpPolicy, env, verbose=1, param_noise=None, action_noise=action_noise, tensorboard_log=\"logs\") \n```", "```\nagent.learn(total_timesteps=25000) \n```", "```\ntensorboard --logdir logs \n```", "```\nchmod +x getid_linux \n```", "```\n./getid_linux \n```", "```\nnano ~/.bashrc \n```", "```\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/.mujoco/mujoco200/bin \n```", "```\nsource ~/.bashrc \n```", "```\ngit clone https://github.com/openai/mujoco-py.git \n```", "```\ncd mujoco-py \n```", "```\nsudo apt-get update \n```", "```\nsudo apt-get install libgl1-mesa-dev libgl1-mesa-glx libosmesa6-dev python3-pip python3-numpy python3-scipy \n```", "```\npip3 install -r requirements.txt\nsudo python3 setup.py install \n```", "```\nimport gym\nenv = gym.make('Humanoid-v2')\nenv.reset()\nfor t in range(1000):\n  env.render()\n  env.step(env.action_space.sample())\nenv.close() \n```", "```\npython mujoco_test.py \n```", "```\nimport gym\nfrom stable_baselines.common.policies import MlpPolicy\nfrom stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines import TRPO\nfrom stable_baselines.common.vec_env import VecVideoRecorder \n```", "```\nenv = DummyVecEnv([lambda: gym.make(\"Humanoid-v2\")]) \n```", "```\nenv = VecNormalize(env, norm_obs=True, norm_reward=False,\n                   clip_obs=10.) \n```", "```\nagent = TRPO(MlpPolicy, env) \n```", "```\nagent.learn(total_timesteps=250000) \n```", "```\nstate = env.reset()\nwhile True:\n    action, _ = agent.predict(state)\n    next_state, reward, done, info = env.step(action)\n    state = next_state\n    env.render() \n```", "```\npython trpo.py \n```", "```\nsudo add-apt-repository ppa:mc3man/trusty-media\nsudo apt-get update\nsudo apt-get dist-upgrade\nsudo apt-get install ffmpeg \n```", "```\nfrom stable_baselines.common.vec_env import VecVideoRecorder \n```", "```\ndef record_video(env_name, agent, video_length=500, prefix='', video_folder='videos/'): \n```", "```\n env = DummyVecEnv([lambda: gym.make(env_name)]) \n```", "```\n env = VecVideoRecorder(env, video_folder=video_folder,\n        record_video_trigger=lambda step: step == 0, video_length=video_length, name_prefix=prefix) \n```", "```\n state = env.reset()\n    for t in range(video_length):\n        action, _ = agent.predict(state)\n        next_state, reward, done, info = env.step(action)\n        state = next_state\n    env.close() \n```", "```\nrecord_video('Humanoid-v2', agent, video_length=500, prefix='Humanoid_walk_TRPO') \n```", "```\nimport gym\nfrom stable_baselines.common.policies import MlpPolicy\nfrom stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines import PPO2 \n```", "```\nenv = DummyVecEnv([lambda: gym.make(\"HalfCheetah-v2\")]) \n```", "```\nenv = VecNormalize(env,norm_obs=True) \n```", "```\nagent = PPO2(MlpPolicy, env) \n```", "```\nagent.learn(total_timesteps=250000) \n```", "```\nstate = env.reset()\nwhile True:\n    action, _ = agent.predict(state)\n    next_state, reward, done, info = env.step(action)\n    state = next_state\n    env.render() \n```", "```\npython ppo.py \n```", "```\nimport imageio\nimport numpy as np \n```", "```\nimages = [] \n```", "```\nstate = agent.env.reset() \n```", "```\nimg = agent.env.render(mode='rgb_array') \n```", "```\nfor i in range(500):\n    images.append(img)\n    action, _ = agent.predict(state)\n    next_state, reward, done ,info = agent.env.step(action)\n    state = next_state\n    img = agent.env.render(mode='rgb_array') \n```", "```\nimageio.mimsave('HalfCheetah.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29) \n```", "```\nimport gym\nfrom stable_baselines import GAIL, TD3\nfrom stable_baselines.gail import ExpertDataset, generate_expert_traj \n```", "```\nagent = TD3('MlpPolicy', 'MountainCarContinuous-v0', verbose=1) \n```", "```\ngenerate_expert_traj(agent, 'expert_traj', n_timesteps=100, n_episodes=20) \n```", "```\ndataset = ExpertDataset(expert_path='expert_traj.npz', traj_limitation=10, verbose=1) \n```", "```\nagent = GAIL('MlpPolicy', 'MountainCarContinuous-v0', dataset, verbose=1) \n```", "```\nagent.learn(total_timesteps=25000) \n```"]