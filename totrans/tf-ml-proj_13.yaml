- en: Generating Book Scripts Using LSTMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language generation** (**NLG**), which is a sub-field of artificial
    intelligence, is a natural language processing task of generating human-readable
    text from various data inputs. It is an active area of research that has achieved
    great popularity in recent times.'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to generate natural language through machines can have wide variety
    of applications, including text autocomplete feature in phones, generating the
    summary of a document, and even generating new scripts for comedies. Google's
    Smart Reply also uses a technology that runs on similar lines to give reply suggestions
    when you're writing an email.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at an NLG task of generating a book script from
    another Packt book that goes by the name of *Mastering PostgreSQL 10*. We took
    almost 100 pages of this book and removed any figures, tables, and SQL code. The
    data is fairly large and has enough words for a neural network to learn the nuances
    of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will learn how to generate book scripts using reinforcement neural networks
    by going through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to recurrent neural networks and LSTMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description of the book script dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling using LSTMs and generating a new book script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNNs**) have become extremely popular for
    any task that involves sequential data. The core idea behind RNNs is to exploit
    the sequential information present in the data. Under usual circumstances, every
    neural network assumes that all of the inputs are independent of each other. However,
    if we are trying to predict the next word in a sequence or the next point in a
    time series, it is imperative to use information based on the words used prior or
    on the historical points in the time series.'
  prefs: []
  type: TYPE_NORMAL
- en: One way to perceive the concept of RNNs is that they have a memory that stores
    information about historical data in a sequence. In theory, RNNs can remember
    history for arbitrarily long sequences, however, in practice, they do a bad job
    in tasks where the historical information needs to be retained for more than a
    few steps back.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical structure of a RNN is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8d2ba9e-f120-4469-96d3-28b8893e4eed.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, *Xt* is the sequence value at different time steps.
    RNNs are called **recurrent** as they apply the exact same operation on every
    element of the sequence, with the output being dependent on the preceding steps.
    The connection between cells can be clearly observed. These connections help to
    transfer information from the previous step to the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned previously, RNNs are not great at capturing long-term dependencies.
    There are different variants of RNNs. A few of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Long Short-Term Memory** (**LSTMs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gated recurrent units** (**GRU**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peephole LSTMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LSTMs are better at capturing long-term dependencies in comparison to vanilla
    RNNs. LSTMs have become very popular regarding tasks such as word/sentence prediction,
    image caption generation, and even forecasting time series data that requires
    long-term dependencies. The following are some of the advantages of using LSTMs:'
  prefs: []
  type: TYPE_NORMAL
- en: Great at modeling tasks involving long-term dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight sharing between different time steps greatly reduces the number of parameters
    in the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suffers less from the vanishing and exploding gradient problem faced by traditional
    RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some of the disadvantages of using LSTMs:'
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are data-hungry. They usually require a lot of training data to produce
    any meaningful results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slower to train than traditional neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are computationally more efficient RNN variants such as GRU that achieve
    a similar performance as LSTMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discussion of other types of RNNs is outside the scope of this chapter. If
    you are interested, you can refer to the sequential modeling chapter in the *Deep
    Learning* Book ([https://www.deeplearningbook.org/contents/rnn.html](https://www.deeplearningbook.org/contents/rnn.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, the dataset used in this project is from a popular
    Packt book that goes by the name of *Mastering PostgreSQL 10*, and was written
    by Hans-Jürgen Schönig ([https://www.cybertec-postgresql.com](https://www.cybertec-postgresql.com)). We
    considered text from the first 100 pages of the book, excluding any figures, tables,
    and SQL code. The cleaned dataset is stored, alongside the code, in a text file.
    The dataset contains almost 44,000 words, which is just enough to train the model.
    The following are a few lines from the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"PostgreSQL Overview*'
  prefs: []
  type: TYPE_NORMAL
- en: '*PostgreSQL is one of the world''s most advanced open source database systems,
    and it has many features that are widely used by developers and system administrators
    alike. Starting with PostgreSQL 10, many new features have been added to PostgreSQL,
    which contribute greatly to the success of this exceptional open source product.
    In this book, many of these cool features will be covered and discussed in great
    detail.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In this chapter, you will be introduced to PostgreSQL and the cool new features
    available in PostgreSQL 10.0 and beyond. All relevant new functionalities will
    be covered in detail. Given the sheer number of changes made to the code and given
    the size of the PostgreSQL project, this list of features is of course by far
    not complete, so I tried to focus on the most important aspects that are relevant
    to most people.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The features outlined in this chapter will be split into the following categories
    Database administration*'
  prefs: []
  type: TYPE_NORMAL
- en: '*SQL and developer related Backup, recovery, and replication Performance related
    topics*'
  prefs: []
  type: TYPE_NORMAL
- en: '*What is new in PostgreSQL 10.0.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*PostgreSQL 10.0 was released in late 2017 and is the first version that follows
    the new numbering scheme introduced by the PostgreSQL community. From now on,
    the way major releases are done will change and therefore, the next major version
    after PostgreSQL*'
  prefs: []
  type: TYPE_NORMAL
- en: '*10.0 will not be 10.1 but PostgreSQL 11\. Versions 10.1 and 10.2 are merely
    service releases and will only contain bug fixes."*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For pre-processing the data so that we prepare it for a LSTM model, go through
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenize punctuation**: While pre-processing, we consider the splitting criteria
    as words using spaces. However, in this scenario, the neural network will have
    a hard time distinguishing words such as Hello and Hello!. Due to this limitation,
    it is required that you tokenize the punctuations in the dataset. For example,
    `!` will be mapped to `_Sym_Exclamation_`. In the code, we implement a function
    named `define_tokens`. This is used to create a dictionary. Here, each piece of
    punctuation is considered a key, and its respective token is a value. In this
    example, we will create tokens for the following symbols:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Period ( . )
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Comma ( , )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Quotation mark (" )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Semicolon ( ; )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exclamation mark ( ! )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Question mark ( ? )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Left parenthesis ( ( )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Right parenthesis ( ) )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dash ( -- )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Return ( \n )
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid using a token that will probably be a word in the dataset. For example,
    `?` is replaced by `_Sym_Question_`, which is not a word in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lower and split**: We must convert all of the uppercase letters in the text
    to lowercase so that the neural network will learn that the two words "Hello"
    and "hello" are actually the same. As the basic unit of input to neural networks
    will be words, the very next step would be to split the sentences in the text
    into words.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Map creation**: Neural networks do not accept text as an input, and so we
    need to map these words to indexes/IDs. To do this, we must create two dictionaries,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Vocab_to_int`: Mapping of each word in the text to its unique ID'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Int_to_vocab`: Inverse dictionary which maps IDs to their corresponding words'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before training the model using the pre-processed data, let''s understand the
    model definition for this problem. In the code, we define a model class in the `model.py` file.
    The class contains four major components, and are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: We define the TensorFlow placeholders in the model for both input
    (X) and target (Y).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network definition**: There are four components of the network for this model.
    They are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Initializing the LSTM cell**: To do this, we begin by stacking two layers
    of LSTMs together. We then set the size of the LSTM to be a `RNN_SIZE` parameter,
    which is as defined in the code. RNN is then initialized with a zero state.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word embeddings**: We encode the words in the text using word embeddings
    rather than one-hot encoding. This is done, mainly, to reduce the dimension of
    the training set, which can help neural networks learn faster. We generate embeddings
    from a uniform distribution for each word in the vocabulary and use TensorFlow''s
    `embedding_lookup` function to get embedding sequences for the input data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building LSTMs**: To obtain the final state of LSTMs, we use TensorFlow''s
    `tf.nn.dynamic_rnn` function with the initial cell and the embeddings of the input
    data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability generation**: After obtaining the final state and output from
    LSTM, we pass it through a fully connected layer to generate logits for predictions.
    We convert those logits into probability estimates by using the `softmax` function.
    The code is as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequence loss**: We must define the loss, which in this case is the Sequence
    loss. This is nothing but a weighted cross entropy for a sequence of logits. We
    equally weight the observations across batch and time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer**: We will use the Adam optimizer, along with its default parameters.
    We will also clip the gradients to keep it within the range of -1 to 1\. Gradient
    clipping is a common phenomenon in recurrent neural networks. When gradients are
    backpropagated in time, they can vanish if they are constantly multiplied by numbers
    less than 1, or can explode due to being multiplied by numbers greater than 1\.
    Gradient clipping will help to resolve both of these problems by restricting the
    gradient to be between -1 and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before understanding the implementation of the training loop, let's take a closer
    look at how we can generate batches of data.
  prefs: []
  type: TYPE_NORMAL
- en: It is common knowledge that batches are used in neural networks to speed up
    the training of the model and to consume less memory. Batches are samples of the
    original dataset that are used for a forward and backward pass to the network.
    The forward pass refers to the process of multiplying inputs with weights of different
    layers in the network and obtaining the final output. The backward pass, on the
    other hand, refers to the process of updating the weights in the neural network
    based on the loss obtained from the outputs of the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this model, since we are predicting the next set of words given a set of
    previous words to generate the TV script, the targets are basically the next few
    (depending on sequence length) words in the original training dataset. Let''s
    consider an example where the training dataset contains the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The quick brown fox jumps over the lazy dog*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the sequence length (the number of words to process together) used is 4,
    then the following are true:'
  prefs: []
  type: TYPE_NORMAL
- en: X is the sequence of every four words, for example, [*The quick brown fox*,
    *quick brown fox jumps* …..] .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y is the sequence of every four words, skipping the first word, for example,
    [*quick brown fox jumps*, *brown fox jumps over* …].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining and training a text-generating model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Begin by loading the saved text data for pre-processing with the help of the
    `load_data` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement `define_tokens`, as defined in the *Pre-processing the data* section
    of this chapter. This will help us create a dictionary of the key words and their
    corresponding tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The dictionary that we've created will be used to replace the punctuation marks
    in the dataset with their respective tokens and delimiters (space in this case)
    around them. For example, `Hello!` will be replaced with `Hello _Sym_Exclamation_`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there is a space between `Hello` and the token. This will help the
    LSTM model treat each punctuation marks as its own word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Map the words to indexes/IDs with the help of the `Vocab_to_int` and `int_to_vocab` dictionaries.
    We are doing this since neural networks do not accept text as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Combine all of the preceding steps to create a function that will pre-process
    the data that''s available for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will then generate integer text for the mapping dictionaries and dump the
    pre-processed data and relevant dictionaries in a `pickle` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define our model, we will create a model class in the `model.py` file. We
    will begin by defining the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We must define variable type to be integers since the words in the dataset have
    been transformed to integers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the network of our model by defining the LSTM cell, word embeddings, building
    LSTMs, and probability generation. To define the LSTM cell, stack two LSTM layers
    and set the size of the LSTM to be a `RNN_SIZE` parameter. Assign RNN the value
    0:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To reduce the dimension of the training set and increase the speed of the neural
    network, generate and look up embeddings using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `tf.nn.dynamic_rnn` function to find the final state of the LSTMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the logits obtained from the final state of the LSTMs to a probability
    estimate by using the `softmax` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a weighted cross entropy or sequence loss for a sequence of logits,
    which further helps fine-tune our network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement the Adam optimizer with the default parameters, and clip the gradients
    to keep it within the range of `-1` to `1` to avoid diminishing the gradient when
    it is backpropagated in time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the sequence length using the `generate_batch_data` function. This helps
    generate batches that are necessary for the neural network training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input for this function will be the text data that is encoded as integers,
    batch size, and sequence length.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output will be a numpy array with the shape [# batches, 2, batch size,
    sequence length]. Each batch contains two parts, defined as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: X with shape [batch size, sequence length]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Y with shape [batch size, sequence length]:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model using the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Num Epochs = 500
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Rate = 0.001
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch Size = 128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RNN Size = 128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sequence Length= 32:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Since the dataset wasn't very large, the code was executed on the CPU itself. We
    will save the output graph, since it will come in useful for generating book scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Generating book scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the model has been trained, we can have some fun with it. In this
    section, we will see how we can use the model to generate book scripts. Use the
    following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Script Length = 200 words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting word = `postgresql`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Follow these steps to generate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the graph of the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract four tensors, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Input/input:0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Network/initial_state:0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Network/final_state:0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Network/probs:0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extract the four tensors using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the starting word and obtain an initial state from the graph, which
    will be used later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Given a starting word and an initial state, proceed to iterate over a for loop
    to generate the next word for the script. In each iteration of the for loop, generate
    the probabilities from the model using the previously generated sequence as input
    and select the word with a maximum probability using the `select_next_word` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a loop to generate the next word in the sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Join all of the words in the sentences using a space delimiter and replace
    the punctuation tokens with the actual symbols. The obtained script is then saved
    in a text file for future reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a sample of the text that was generated from the execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, the model learns to use a full stop after a sentence, leaves
    a blank line between paragraphs, and follows basic grammar. The model has learned
    all of this by itself, without us having to provide any guidance/rules. Despite
    the fact that the script is far from perfect, it's amazing how a machine is able
    to generate real-sounding sentences of a book. We can further fine-tune the hyperparameters
    of the model to generate more meaningful text.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about how LSTMs can be used to generate book scripts.
  prefs: []
  type: TYPE_NORMAL
- en: We began by looking at the basics of RNNs and its popular variant, commonly
    known as LSTMs. We learned that RNNs are hugely successful in predicting datasets
    that involve sequences such as time series, next word prediction in natural language
    processing tasks, and so on. We also looked at the advantages and disadvantages
    of using LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter then helped us understand how to pre-process text data and prepare
    it so that we can feed it into LSTMs. We also looked at the model's structure
    for training. Next, we looked at how to train the neural networks by creating
    batches of data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we understood how to generate book script using the TensorFlow model
    we trained. Although the script that was generated doesn't make complete sense,
    it was amazing to observe the neural network generate a book's sentences. We then
    saved the generated book script in a text file for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall play Pac-Man using deep reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Can you try to use from a different book to see how well the model is able to
    generate new text?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens to the generated text if you double the batch size and decrease
    the learning rate?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you train the model without gradient clipping and see if the result improves?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
