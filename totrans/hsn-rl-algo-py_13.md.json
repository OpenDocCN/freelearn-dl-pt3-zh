["```\n$ sudo apt-get install git python3-dev python3-numpy libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libsdl1.2-dev libportmidi-dev libswscale-dev libavformat-dev libavcodec-dev libfreetype6-dev\n$ sudo pip install pygame\n```", "```\n$ brew install sdl sdl_ttf sdl_image sdl_mixer portmidi \n$ pip install -c https://conda.binstar.org/quasiben pygame\n```", "```\ngit clone https://github.com/ntasfi/PyGame-Learning-Environment\n```", "```\ncd PyGame-Learning-Environment\n```", "```\nsudo pip install -e .\n```", "```\nfrom ple.games.flappybird import FlappyBird\nfrom ple import PLE\n```", "```\ngame = FlappyBird()\np = PLE(game, fps=30, display_screen=False)\n```", "```\np.init()\n```", "```\nfrom ple.games.flappybird import FlappyBird\nfrom ple import PLE\n\ngame = FlappyBird()\np = PLE(game, fps=30, display_screen=False)\np.init()\n\nreward = 0\n\nfor _ in range(5):\n    reward += p.act(get_action(p.getGameState()))\n\n    if p.game_over():\n        p.reset_game()\n```", "```\nInitialize \nInitialize  ( is the expert policy)\n\nfor i :\n    > Populate dataset  with . States are given by  (sometimes the expert could take the control over it) and actions are given by the expert \n\n    > Train a classifier  on the aggregate dataset \n```", "```\ndef expert():\n    graph = tf.get_default_graph()\n    sess_expert = tf.Session(graph=graph)\n\n    saver = tf.train.import_meta_graph('expert/model.ckpt.meta')\n    saver.restore(sess_expert,tf.train.latest_checkpoint('expert/'))\n\n    p_argmax = graph.get_tensor_by_name('actor_nn/max_act:0') \n    obs_ph = graph.get_tensor_by_name('obs:0') \n```", "```\n    def expert_policy(state):\n        act = sess_expert.run(p_argmax, feed_dict={obs_ph:[state]})\n        return np.squeeze(act)\n\n    return expert_policy\n```", "```\n    obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32, name='obs')\n    act_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='act')\n\n    p_logits = mlp(obs_ph, hidden_sizes, act_dim, tf.nn.relu, last_activation=None)\n    act_max = tf.math.argmax(p_logits, axis=1)\n    act_onehot = tf.one_hot(act_ph, depth=act_dim)\n\n    p_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=act_onehot, logits=p_logits))\n    p_opt = tf.train.AdamOptimizer(p_lr).minimize(p_loss)\n```", "```\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    def learner_policy(state):\n        action = sess.run(act_max, feed_dict={obs_ph:[state]})\n        return np.squeeze(action)\n```", "```\n    X = []\n    y = []\n\n    env = FlappyBird()\n    env = PLE(env, fps=30, display_screen=False)\n    env.init() \n```", "```\n    for it in range(dagger_iterations):\n        sess.run(tf.global_variables_initializer())\n        env.reset_game()\n        no_op(env)\n\n        game_rew = 0\n        rewards = []\n```", "```\n        for _ in range(step_iterations):\n            state = flappy_game_state(env)\n\n            if np.random.rand() < (1 - it/5):\n                action = expert_policy(state)\n            else:\n                action = learner_policy(state)\n\n            action = 119 if action == 1 else None\n\n            rew = env.act(action)\n            rew += env.act(action)\n\n            X.append(state)\n            y.append(expert_policy(state)) \n            game_rew += rew\n\n            if env.game_over():\n                env.reset_game()\n                np_op(env)\n\n                rewards.append(game_rew)\n                game_rew = 0\n```", "```\n        n_batches = int(np.floor(len(X)/batch_size))\n\n        shuffle = np.arange(len(X))\n        np.random.shuffle(shuffle)\n        shuffled_X = np.array(X)[shuffle]\n        shuffled_y = np.array(y)[shuffle]\n\n        ep_loss = []\n            for _ in range(train_epochs):\n\n                for b in range(n_batches):\n                    p_start = b*batch_size\n                    tr_loss, _ = sess.run([p_loss, p_opt], feed_dict=\n                            obs_ph:shuffled_X[p_start:p_start+batch_size], \n                            act_ph:shuffled_y[p_start:p_start+batch_size]})\n\n                    ep_loss.append(tr_loss)\n        print('Ep:', it, np.mean(ep_loss), 'Test:', np.mean(test_agent(learner_policy)))\n```"]