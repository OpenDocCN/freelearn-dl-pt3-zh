["```\nimport numpy as np\nimport tensorflow as tf\n\nfrom collections import namedtuple\n```", "```\n#reading the Anna Karenina novel text file\nwith open('Anna_Karenina.txt', 'r') as f:\n    textlines=f.read()\n\n#Building the vocan and encoding the characters as integers\nlanguage_vocab = set(textlines)\nvocab_to_integer = {char: j for j, char in enumerate(language_vocab)}\ninteger_to_vocab = dict(enumerate(language_vocab))\nencoded_vocab = np.array([vocab_to_integer[char] for char in textlines], dtype=np.int32)\n```", "```\ntextlines[:200]\nOutput:\n\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on\"\n```", "```\nencoded_vocab[:200]\nOutput:\narray([70, 34, 54, 29, 24, 19, 76, 45, 2, 79, 79, 79, 69, 54, 29, 29, 49,\n       45, 66, 54, 39, 15, 44, 15, 19, 12, 45, 54, 76, 19, 45, 54, 44, 44,\n      45, 54, 44, 15, 27, 19, 58, 45, 19, 30, 19, 76, 49, 45, 59, 56, 34,\n       54, 29, 29, 49, 45, 66, 54, 39, 15, 44, 49, 45, 15, 12, 45, 59, 56,\n       34, 54, 29, 29, 49, 45, 15, 56, 45, 15, 24, 12, 45, 11, 35, 56, 79,\n       35, 54, 49, 53, 79, 79, 36, 30, 19, 76, 49, 24, 34, 15, 56, 16, 45,\n       35, 54, 12, 45, 15, 56, 45, 31, 11, 56, 66, 59, 12, 15, 11, 56, 45,\n       15, 56, 45, 24, 34, 19, 45, 1, 82, 44, 11, 56, 12, 27, 49, 12, 37,\n       45, 34, 11, 59, 12, 19, 53, 45, 21, 34, 19, 45, 35, 15, 66, 19, 45,\n       34, 54, 64, 79, 64, 15, 12, 31, 11, 30, 19, 76, 19, 64, 45, 24, 34,\n       54, 24, 45, 24, 34, 19, 45, 34, 59, 12, 82, 54, 56, 64, 45, 35, 54,\n       12, 45, 31, 54, 76, 76, 49, 15, 56, 16, 45, 11, 56], dtype=int32)\n```", "```\nlen(language_vocab)\nOutput:\n83\n```", "```\ndef generate_character_batches(data, num_seq, num_steps):\n    '''Create a function that returns batches of size\n       num_seq x num_steps from data.\n    '''\n    # Get the number of characters per batch and number of batches\n    num_char_per_batch = num_seq * num_steps\n    num_batches = len(data)//num_char_per_batch\n\n    # Keep only enough characters to make full batches\n    data = data[:num_batches * num_char_per_batch]\n\n    # Reshape the array into n_seqs rows\n    data = data.reshape((num_seq, -1))\n\n    for i in range(0, data.shape[1], num_steps):\n        # The input variables\n        input_x = data[:, i:i+num_steps]\n\n        # The output variables which are shifted by one\n        output_y = np.zeros_like(input_x)\n\n        output_y[:, :-1], output_y[:, -1] = input_x[:, 1:], input_x[:, 0]\n        yield input_x, output_y\n```", "```\ngenerated_batches = generate_character_batches(encoded_vocab, 15, 50)\ninput_x, output_y = next(generated_batches)\nprint('input\\n', input_x[:10, :10])\nprint('\\ntarget\\n', output_y[:10, :10])\nOutput:\n\ninput\n [[70 34 54 29 24 19 76 45 2 79]\n [45 19 44 15 16 15 82 44 19 45]\n [11 45 44 15 16 34 24 38 34 19]\n [45 34 54 64 45 82 19 19 56 45]\n [45 11 56 19 45 27 56 19 35 79]\n [49 19 54 76 12 45 44 54 12 24]\n [45 41 19 45 16 11 45 15 56 24]\n [11 35 45 24 11 45 39 54 27 19]\n [82 19 66 11 76 19 45 81 19 56]\n [12 54 16 19 45 44 15 27 19 45]]\n\ntarget\n [[34 54 29 24 19 76 45 2 79 79]\n [19 44 15 16 15 82 44 19 45 16]\n [45 44 15 16 34 24 38 34 19 54]\n [34 54 64 45 82 19 19 56 45 82]\n [11 56 19 45 27 56 19 35 79 35]\n [19 54 76 12 45 44 54 12 24 45]\n [41 19 45 16 11 45 15 56 24 11]\n [35 45 24 11 45 39 54 27 19 33]\n [19 66 11 76 19 45 81 19 56 24]\n [54 16 19 45 44 15 27 19 45 24]]\n```", "```\ndef build_model_inputs(batch_size, num_steps):\n\n    # Declare placeholders for the input and output variables\n    inputs_x = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n    targets_y = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n\n    # define the keep_probability for the dropout layer\n    keep_probability = tf.placeholder(tf.float32, name='keep_prob')\n\n    return inputs_x, targets_y, keep_probability\n```", "```\nlstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n```", "```\ntf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_probability)\n```", "```\ntf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n```", "```\ninitial_state = cell.zero_state(batch_size, tf.float32)\n```", "```\ndef build_lstm_cell(size, num_layers, batch_size, keep_probability):\n\n    ### Building the LSTM Cell using the tensorflow function\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(size)\n\n    # Adding dropout to the layer to prevent overfitting\n    drop_layer = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_probability)\n\n    # Add muliple cells together and stack them up to oprovide a level of more understanding\n    stakced_cell = tf.contrib.rnn.MultiRNNCell([drop_layer] * num_layers)\n    initial_cell_state = lstm_cell.zero_state(batch_size, tf.float32)\n\n    return lstm_cell, initial_cell_state\n```", "```\ndef build_model_output(output, input_size, output_size):\n\n    # Reshaping output of the model to become a bunch of rows, where each row correspond for each step in the seq\n    sequence_output = tf.concat(output, axis=1)\n    reshaped_output = tf.reshape(sequence_output, [-1, input_size])\n\n    # Connect the RNN outputs to a softmax layer\n    with tf.variable_scope('softmax'):\n        softmax_w = tf.Variable(tf.truncated_normal((input_size, output_size), stddev=0.1))\n        softmax_b = tf.Variable(tf.zeros(output_size))\n\n    # the output is a set of rows of LSTM cell outputs, so the logits will be a set\n    # of rows of logit outputs, one for each step and sequence\n    logits = tf.matmul(reshaped_output, softmax_w) + softmax_b\n\n    # Use softmax to get the probabilities for predicted characters\n    model_out = tf.nn.softmax(logits, name='predictions')\n\n    return model_out, logits\n```", "```\ndef model_loss(logits, targets, lstm_size, num_classes):\n\n    # convert the targets to one-hot encoded and reshape them to match the logits, one row per batch_size per step\n    output_y_one_hot = tf.one_hot(targets, num_classes)\n    output_y_reshaped = tf.reshape(output_y_one_hot, logits.get_shape())\n\n    #Use the cross entropy loss\n    model_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=output_y_reshaped)\n    model_loss = tf.reduce_mean(model_loss)\n    return model_loss\n```", "```\ndef build_model_optimizer(model_loss, learning_rate, grad_clip):\n\n    # define optimizer for training, using gradient clipping to avoid the exploding of the gradients\n    trainable_variables = tf.trainable_variables()\n    gradients, _ = tf.clip_by_global_norm(tf.gradients(model_loss, trainable_variables), grad_clip)\n\n    #Use Adam Optimizer\n    train_operation = tf.train.AdamOptimizer(learning_rate)\n    model_optimizer = train_operation.apply_gradients(zip(gradients, trainable_variables))\n\n    return model_optimizer\n```", "```\nclass CharLSTM:\n\n    def __init__(self, num_classes, batch_size=64, num_steps=50, \n                       lstm_size=128, num_layers=2, learning_rate=0.001, \n                       grad_clip=5, sampling=False):\n\n        # When we're using this network for generating text by sampling, we'll be providing the network with\n        # one character at a time, so providing an option for it.\n        if sampling == True:\n            batch_size, num_steps = 1, 1\n        else:\n            batch_size, num_steps = batch_size, num_steps\n\n        tf.reset_default_graph()\n\n        # Build the model inputs placeholders of the input and target variables\n        self.inputs, self.targets, self.keep_prob = build_model_inputs(batch_size, num_steps)\n\n        # Building the LSTM cell\n        lstm_cell, self.initial_state = build_lstm_cell(lstm_size, num_layers, batch_size, self.keep_prob)\n\n        ### Run the data through the LSTM layers\n        # one_hot encode the input\n        input_x_one_hot = tf.one_hot(self.inputs, num_classes)\n\n        # Runing each sequence step through the LSTM architecture and finally collecting the outputs\n        outputs, state = tf.nn.dynamic_rnn(lstm_cell, input_x_one_hot, initial_state=self.initial_state)\n        self.final_state = state\n\n        # Get softmax predictions and logits\n        self.prediction, self.logits = build_model_output(outputs, lstm_size, num_classes)\n\n        # Loss and optimizer (with gradient clipping)\n        self.loss = model_loss(self.logits, self.targets, lstm_size, num_classes)\n        self.optimizer = build_model_optimizer(self.loss, learning_rate, grad_clip)\n```", "```\n\nbatch_size = 100        # Sequences per batch\nnum_steps = 100         # Number of sequence steps per batch\nlstm_size = 512         # Size of hidden layers in LSTMs\nnum_layers = 2          # Number of LSTM layers\nlearning_rate = 0.001   # Learning rate\nkeep_probability = 0.5  # Dropout keep probability\n```", "```\nepochs = 5\n\n# Save a checkpoint N iterations\nsave_every_n = 100\n\nLSTM_model = CharLSTM(len(language_vocab), batch_size=batch_size, num_steps=num_steps,\n                lstm_size=lstm_size, num_layers=num_layers, \n                learning_rate=learning_rate)\n\nsaver = tf.train.Saver(max_to_keep=100)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # Use the line below to load a checkpoint and resume training\n    #saver.restore(sess, 'checkpoints/______.ckpt')\n    counter = 0\n    for e in range(epochs):\n        # Train network\n        new_state = sess.run(LSTM_model.initial_state)\n        loss = 0\n        for x, y in generate_character_batches(encoded_vocab, batch_size, num_steps):\n            counter += 1\n            start = time.time()\n            feed = {LSTM_model.inputs: x,\n                    LSTM_model.targets: y,\n                    LSTM_model.keep_prob: keep_probability,\n                    LSTM_model.initial_state: new_state}\n            batch_loss, new_state, _ = sess.run([LSTM_model.loss, \n                                                 LSTM_model.final_state, \n                                                 LSTM_model.optimizer], \n                                                 feed_dict=feed)\n\n            end = time.time()\n            print('Epoch number: {}/{}... '.format(e+1, epochs),\n                  'Step: {}... '.format(counter),\n                  'loss: {:.4f}... '.format(batch_loss),\n                  '{:.3f} sec/batch'.format((end-start)))\n\n            if (counter % save_every_n == 0):\n                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n\n    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n```", "```\n.\n.\n.\nEpoch number: 5/5...  Step: 978...  loss: 1.7151...  0.050 sec/batch\nEpoch number: 5/5...  Step: 979...  loss: 1.7428...  0.051 sec/batch\nEpoch number: 5/5...  Step: 980...  loss: 1.7151...  0.050 sec/batch\nEpoch number: 5/5...  Step: 981...  loss: 1.7236...  0.050 sec/batch\nEpoch number: 5/5...  Step: 982...  loss: 1.7314...  0.051 sec/batch\nEpoch number: 5/5...  Step: 983...  loss: 1.7369...  0.051 sec/batch\nEpoch number: 5/5...  Step: 984...  loss: 1.7075...  0.065 sec/batch\nEpoch number: 5/5...  Step: 985...  loss: 1.7304...  0.051 sec/batch\nEpoch number: 5/5...  Step: 986...  loss: 1.7128...  0.049 sec/batch\nEpoch number: 5/5...  Step: 987...  loss: 1.7107...  0.051 sec/batch\nEpoch number: 5/5...  Step: 988...  loss: 1.7351...  0.051 sec/batch\nEpoch number: 5/5...  Step: 989...  loss: 1.7260...  0.049 sec/batch\nEpoch number: 5/5...  Step: 990...  loss: 1.7144...  0.051 sec/batch\n```", "```\ntf.train.get_checkpoint_state('checkpoints')\n```", "```\nOutput:\nmodel_checkpoint_path: \"checkpoints/i990_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i100_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i300_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i500_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i700_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i900_l512.ckpt\"\nall_model_checkpoint_paths: \"checkpoints/i990_l512.ckpt\"\n```", "```\ndef choose_top_n_characters(preds, vocab_size, top_n_chars=4):\n    p = np.squeeze(preds)\n    p[np.argsort(p)[:-top_n_chars]] = 0\n    p = p / np.sum(p)\n    c = np.random.choice(vocab_size, 1, p=p)[0]\n    return c\n```", "```\ndef sample_from_LSTM_output(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n    samples = [char for char in prime]\n    LSTM_model = CharLSTM(len(language_vocab), lstm_size=lstm_size, sampling=True)\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        saver.restore(sess, checkpoint)\n        new_state = sess.run(LSTM_model.initial_state)\n        for char in prime:\n            x = np.zeros((1, 1))\n            x[0,0] = vocab_to_integer[char]\n            feed = {LSTM_model.inputs: x,\n                    LSTM_model.keep_prob: 1.,\n                    LSTM_model.initial_state: new_state}\n            preds, new_state = sess.run([LSTM_model.prediction, LSTM_model.final_state], \n                                         feed_dict=feed)\n\n        c = choose_top_n_characters(preds, len(language_vocab))\n        samples.append(integer_to_vocab[c])\n\n        for i in range(n_samples):\n            x[0,0] = c\n            feed = {LSTM_model.inputs: x,\n                    LSTM_model.keep_prob: 1.,\n                    LSTM_model.initial_state: new_state}\n            preds, new_state = sess.run([LSTM_model.prediction, LSTM_model.final_state], \n                                         feed_dict=feed)\n\n            c = choose_top_n_characters(preds, len(language_vocab))\n            samples.append(integer_to_vocab[c])\n\n    return ''.join(samples)\n```", "```\ntf.train.latest_checkpoint('checkpoints')\n```", "```\nOutput:\n'checkpoints/i990_l512.ckpt'\n```", "```\ncheckpoint = tf.train.latest_checkpoint('checkpoints')\nsampled_text = sample_from_LSTM_output(checkpoint, 1000, lstm_size, len(language_vocab), prime=\"Far\")\nprint(sampled_text)\n```", "```\nOutput:\nINFO:tensorflow:Restoring parameters from checkpoints/i990_l512.ckpt\n```", "```\nFarcial the\nconfiring to the mone of the correm and thinds. She\nshe saw the\nstreads of herself hand only astended of the carres to her his some of the princess of which he came him of\nall that his white the dreasing of\nthisking the princess and with she was she had\nbettee a still and he was happined, with the pood on the mush to the peaters and seet it.\n\n\"The possess a streatich, the may were notine at his mate a misted\nand the\nman of the mother at the same of the seem her\nfelt. He had not here.\n\n\"I conest only be alw you thinking that the partion\nof their said.\"\n\n\"A much then you make all her\nsomether. Hower their centing\nabout\nthis, and I won't give it in\nhimself.\nI had not come at any see it will that there she chile no one that him.\n\n\"The distiction with you all.... It was\na mone of the mind were starding to the simple to a mone. It to be to ser in the place,\" said Vronsky.\n\"And a plais in\nhis face, has alled in the consess on at they to gan in the sint\nat as that\nhe would not be and t\n```"]