["```\nimport tf.keras.losses.binary_crossentropy as bce\ndef discriminator_loss(pred_fake, pred_real):\n    real_loss = bce(tf.ones_like(pred_real), pred_real)\n    fake_loss = bce(tf.zeros_like(pred_fake), pred_fake)\n    d_loss = 0.5 *(real_loss + fake_loss)\n    return d_loss\n```", "```\n    def generator_loss(pred_fake):\n        g_loss = bce(tf.ones_like(pred_fake), pred_fake)\n        return g_loss\n```", "```\ng_loss = bce(tf.ones_like(pred_fake), pred_fake) \n# generator\nfake_loss = bce(tf.zeros_like(pred_fake), pred_fake) \n# generator\n```", "```\ndef train_step(g_input, real_input):\n    with tf.GradientTape() as g_tape,\\\n         tf.GradientTape() as d_tape:\n        # Forward pass\n        fake_input = G(g_input)\n        pred_fake = D(fake_input)\n        pred_real = D(real_input)   \n        # Calculate losses\n        d_loss = discriminator_loss(pred_fake, pred_real)\n        g_loss = generator_loss(pred_fake)\n```", "```\n        gradient_g = g_tape.gradient(g_loss,\\ \t\t\t\t\t\tG.trainable_variables)\n        gradient_d = d_tape.gradient(d_loss,\\ \t\t\t\t\t\tD.trainable_variables)\n```", "```\n        G_optimizer.apply_gradients(zip(gradient_g, \t\t\t\t\tself.G.trainable_variables))\n        D_optimizer.apply_gradients(zip(gradient_d, \t\t\t\t\tself.D.trainable_variables))\n```", "```\ndef Generator(self, z_dim): \n        model = tf.keras.Sequential(name='Generator') \n        model.add(layers.Input(shape=[z_dim])) \n        model.add(layers.Dense(7*7*512))        \n        model.add(layers.BatchNormalization(momentum=0.9)) \n        model.add(layers.LeakyReLU())\n        model.add(layers.Reshape((7,7,512))) \n        model.add(layers.UpSampling2D((2,2), \t\t\t\t    interpolation=\"bilinear\"))\n        model.add(layers.Conv2D(256, 3, padding='same')) \n        model.add(layers.BatchNormalization(momentum=0.9)) \n        model.add(layers.LeakyReLU())         \n        model.add(layers.UpSampling2D((2,2), \t\t\t\t    interpolation=\"bilinear\"))        \n        model.add(layers.Conv2D(128, 3, padding='same')) \n        model.add(layers.LeakyReLU())\n        model.add(layers.Conv2D(image_shape[-1], 3,  \t\t\tpadding='same', activation='tanh')) \n    return model     \n```", "```\ndef Discriminator(self, input_shape): \n    model = tf.keras.Sequential(name='Discriminator') \n    model.add(layers.Input(shape=input_shape)) \n    model.add(layers.Conv2D(32, 3, strides=(2,2),  \t\t\t\t\t padding='same'))\n    model.add(layers.BatchNormalization(momentum=0.9))\n    model.add(layers.ReLU()) \n    model.add(layers.Conv2D(64, 3, strides=(2,2), \t\t\t\t\t padding='same')) \n    model.add(layers.BatchNormalization(momentum=0.9)) \n    model.add(layers.ReLU())\n    model.add(layers.Flatten()) \n    model.add(layers.Dense(1, activation='sigmoid')) \n    return model\n```", "```\n    def wasserstein_loss(self, y_true, y_pred):\n        w_loss = -tf.reduce_mean(y_true*y_pred)\n        return w_loss\n```", "```\nclass WeightsClip(tf.keras.constraints.Constraint):\n    def __init__(self, min_value=-0.01, max_value=0.01):\n        self.min_value = min_value\n        self.max_value = max_value\n    def __call__(self, w):\n        return tf.clip_by_value(w, self.min, \t\t\t\t\t\tself.max_value)\n```", "```\nmodel = tf.keras.Sequential(name='critics')        \nmodel.add(Conv2D(16, 3, strides=2, padding='same', \n                  kernel_constraint=WeightsClip(),\n                  bias_constraint=WeightsClip()))\nmodel.add(BatchNormalization(\n \t\t\t beta_constraint=WeightsClip(),\n \t\t gamma_constraint=WeightsClip()))\n```", "```\nfor layer in critic.layers:\n    weights = layer.get_weights() \n    weights = [tf.clip_by_value(w, -0.01, 0.01) for  \t\t\tw in weights]\n    layer.set_weights(weights)\n```", "```\nfor _ in range(self.n_critic):\n    real_images = next(data_generator)\n    critic_loss = self.train_critic(real_images, \t\t\t\t\t\t    batch_size)\n```", "```\nself.critic = self.build_critic()\nself.critic.trainable = False\nself.generator = self.build_generator()\ncritic_output = self.critic(self.generator.output)\nself.model = Model(self.generator.input, critic_output)\nself.model.compile(loss = self.wasserstein_loss,  \t\t\t  optimizer = RMSprop(3e-4))\nself.critic.trainable = True\n```", "```\ng_loss = self.model.train_on_batch(g_input,  \t\t\t\t\t\t  real_labels)\n```", "```\nepsilon = tf.random.uniform((batch_size,1,1,1))\ninterpolates = epsilon*real_images + \\ \t\t\t\t\t(1-epsilon)*fake_images\n```", "```\nwith tf.GradientTape() as gradient_tape:\n \tgradient_tape.watch(interpolates) \n\tcritic_interpolates = self.critic(interpolates)\n \tgradient_d = gradient_tape.gradient( \t\t\t\t\t\t critic_interpolates,  \t\t\t\t\t\t [interpolates])\n```", "```\ngrad_loss = tf.square(grad)\ngrad_loss = tf.reduce_sum(grad_loss, \t\t\t\t    axis=np.arange(1, \t\t\t\t\t\tlen(grad)loss.shape)))\ngraid_loss = tf.sqrt(grad_loss)\n```", "```\ngrad_loss = tf.reduce_mean(tf.square(grad_loss - 1))\n```", "```\ntotal_loss = loss_real + loss_fake + LAMBDA * grad_loss\ngradients = total_tape.gradient(total_loss, \t\t\t\t\t\tself.critic.variables)\nself.optimizer_critic.apply_gradients(zip(gradients, \t\t\t\t\t\tself.critic.variables))\n```", "```\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.LayerNormalization())\n```"]