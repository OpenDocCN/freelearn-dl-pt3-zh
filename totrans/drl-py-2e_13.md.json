["```\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gym \n```", "```\nenv = gym.make('Pendulum-v0').unwrapped \n```", "```\nstate_shape = env.observation_space.shape[0] \n```", "```\naction_shape = env.action_space.shape[0] \n```", "```\naction_bound = [env.action_space.low, env.action_space.high] \n```", "```\nepsilon = 0.2 \n```", "```\nclass PPO(object): \n```", "```\n def __init__(self): \n```", "```\n self.sess = tf.Session() \n```", "```\n self.state_ph = tf.placeholder(tf.float32, [None, state_shape], 'state') \n```", "```\n with tf.variable_scope('value'):\n            layer1 = tf.layers.dense(self.state_ph, 100, tf.nn.relu)\n            self.v = tf.layers.dense(layer1, 1) \n```", "```\n self.Q = tf.placeholder(tf.float32, [None, 1], 'discounted_r') \n```", "```\n self.advantage = self.Q - self.v \n```", "```\n self.value_loss = tf.reduce_mean(tf.square(self.advantage)) \n```", "```\n self.train_value_nw = tf.train.AdamOptimizer(0.002).minimize(self.value_loss) \n```", "```\n pi, pi_params = self.build_policy_network('pi', trainable=True) \n```", "```\n oldpi, oldpi_params = self.build_policy_network('oldpi', trainable=False) \n```", "```\n with tf.variable_scope('sample_action'):\n            self.sample_op = tf.squeeze(pi.sample(1), axis=0) \n```", "```\n with tf.variable_scope('update_oldpi'):\n            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)] \n```", "```\n self.action_ph = tf.placeholder(tf.float32, [None, action_shape], 'action') \n```", "```\n self.advantage_ph = tf.placeholder(tf.float32, [None, 1], 'advantage') \n```", "```\n with tf.variable_scope('loss'):\n            with tf.variable_scope('surrogate'): \n```", "```\n ratio = pi.prob(self.action_ph) / oldpi.prob(self.action_ph) \n```", "```\n objective = ratio * self.advantage_ph \n```", "```\n L = tf.reduce_mean(tf.minimum(objective, tf.clip_by_value(ratio, 1.-epsilon, 1.+ epsilon)*self.advantage_ph)) \n```", "```\n self.policy_loss = -L \n```", "```\n with tf.variable_scope('train_policy'):\n            self.train_policy_nw = tf.train.AdamOptimizer(0.001).minimize(self.policy_loss) \n```", "```\n self.sess.run(tf.global_variables_initializer()) \n```", "```\n def train(self, state, action, reward): \n```", "```\n self.sess.run(self.update_oldpi_op) \n```", "```\n adv = self.sess.run(self.advantage, {self.state_ph: state, self.Q: reward}) \n```", "```\n [self.sess.run(self.train_policy_nw, {self.state_ph: state, self.action_ph: action, self.advantage_ph: adv}) for _ in range(10)] \n```", "```\n [self.sess.run(self.train_value_nw, {self.state_ph: state, self.Q: reward}) for _ in range(10)] \n```", "```\n def build_policy_network(self, name, trainable):\n        with tf.variable_scope(name): \n```", "```\n layer = tf.layers.dense(self.state_ph, 100, tf.nn.relu, trainable=trainable) \n```", "```\n mu = 2 * tf.layers.dense(layer, action_shape, tf.nn.tanh, trainable=trainable) \n```", "```\n sigma = tf.layers.dense(layer, action_shape, tf.nn.softplus, trainable=trainable) \n```", "```\n norm_dist = tf.distributions.Normal(loc=mu, scale=sigma) \n```", "```\n params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n        return norm_dist, params \n```", "```\n def select_action(self, state):\n        state = state[np.newaxis, :] \n```", "```\n action = self.sess.run(self.sample_op, {self.state_ph: state})[0] \n```", "```\n action = np.clip(action, action_bound[0], action_bound[1])\n        return action \n```", "```\n def get_state_value(self, state):\n        if state.ndim < 2: state = state[np.newaxis, :]\n        return self.sess.run(self.v, {self.state_ph: state})[0, 0] \n```", "```\nppo = PPO() \n```", "```\nnum_episodes = 1000 \n```", "```\nnum_timesteps = 200 \n```", "```\ngamma = 0.9 \n```", "```\nbatch_size = 32 \n```", "```\nfor i in range(num_episodes): \n```", "```\n state = env.reset() \n```", "```\n episode_states, episode_actions, episode_rewards = [], [], [] \n```", "```\n Return = 0 \n```", "```\n for t in range(num_timesteps): \n```", "```\n env.render() \n```", "```\n action = ppo.select_action(state) \n```", "```\n next_state, reward, done, _ = env.step(action) \n```", "```\n episode_states.append(state)\n        episode_actions.append(action)\n        episode_rewards.append((reward+8)/8) \n```", "```\n state = next_state \n```", "```\n Return += reward \n```", "```\n if (t+1) % batch_size == 0 or t == num_timesteps-1: \n```", "```\n v_s_ = ppo.get_state_value(next_state) \n```", "```\n discounted_r = []\n            for reward in episode_rewards[::-1]:\n                v_s_ = reward + gamma * v_s_\n                discounted_r.append(v_s_)\n            discounted_r.reverse() \n```", "```\n es, ea, er = np.vstack(episode_states), np.vstack(episode_actions), np.array(discounted_r)[:, np.newaxis] \n```", "```\n ppo.train(es, ea, er) \n```", "```\n episode_states, episode_actions, episode_rewards = [], [], [] \n```", "```\n if i %10 ==0:\n         print(\"Episode:{}, Return: {}\".format(i,Return)) \n```"]