- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probabilistic TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Uncertainty is a fact of life; whether you are doing a classification task
    or a regression task, it is important to know how confident your model is in its
    prediction. Till now, we have covered the traditional deep learning models, and
    while they are great at many tasks, they are not able to handle uncertainty. Instead,
    they are deterministic in nature. In this chapter, you will learn how to leverage
    TensorFlow Probability to build models that can handle uncertainty, specifically
    probabilistic deep learning models and Bayesian networks. The chapter will include:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributions, events, and shapes in TensorFlow Probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian networks using TensorFlow Probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand uncertainty in machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model aleatory and epistemic uncertainty using TensorFlow Probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp12](https://packt.link/dltfchp12)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with first understanding TensorFlow Probability.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow Probability** (**TFP**), a part of the TensorFlow ecosystem, is
    a library that provides tools for developing probabilistic models. It can be used
    to perform probabilistic reasoning and statistical analysis. It is built over
    TensorFlow and provides the same computational advantage.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.1* shows the major components constituting TensorFlow Probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Different components of TensorFlow Probability'
  prefs: []
  type: TYPE_NORMAL
- en: At the root, we have all numerical operations supported by TensorFlow, specifically
    the `LinearOperator` class (part of `tf.linalg`) – it contains all the methods
    that can be performed on a matrix, without the need to actually materialize the
    matrix. This provides computationally efficient matrix-free computations. TFP
    includes a large collection of probability distributions and their related statistical
    computations. It also has `tfp.bijectors`, which offers a wide range of transformed
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Bijectors encapsulate the change of variables for probability density. That
    is, when one transforms one variable from space A to space B, we need a way to
    map the probability distributions of the variables as well. Bijectors provide
    us with all the tools needed to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow Probability also provides `JointDistribution`, which allows the
    user to draw a joint sample and compute a joint log-density (log probability density
    function). The standard TFP distributions work on tensors, but `JointDistribution`
    works on the structure of tensors. `tfp.layers` provides neural network layers
    that can be used to extend the standard TensorFlow layers and add uncertainty
    to them. And finally, it provides a wide range of tools for probabilistic inference.
    In this chapter, we will go through some of these functions and classes; let us
    first start with installation. To install TFP in your working environment, just
    run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us have some fun with TFP. To use TFP, we will need to import it. Additionally,
    we are going to do some plots. So, we import some additional modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we explore the different classes of distributions available in `tfp.distributions`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that a rich range of distributions is available in TFP. Let us
    now try one of the distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that as `N` increases, the plot follows a nice normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **N=100** | ![Chart, histogram  Description automatically generated](img/B18331_12_02_1.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **N=1000** | ![Chart, histogram  Description automatically generated](img/B18331_12_02_2.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **N=10000** | ![A picture containing histogram  Description automatically
    generated](img/B18331_12_02_3.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 12.2: Normal distribution from randomly generated samples of sizes 100,
    1,000, and 10,000\. The distribution has a mean of zero and a standard deviation
    of one'
  prefs: []
  type: TYPE_NORMAL
- en: Let us now explore the different distributions available with TFP.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Every distribution in TFP has a shape, batch, and event size associated with
    it. The shape is the sample size; it represents independent and identically distributed
    draws or observations. Consider the normal distribution that we defined in the
    previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This defines a single normal distribution, with mean zero and standard deviation
    one. When we use the `sample` function, we do a random draw from this distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the details regarding `batch_shape` and `event_shape` if you print the
    object `normal`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us try and define a second `normal` object, but this time, `loc` and `scale`
    are lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Did you notice the change in `batch_shape`? Now, if we draw a single sample
    from it, we will draw from two normal distributions, one with a mean of zero and
    standard deviation of one, and the other with a mean of zero and standard deviation
    of three. Thus, the batch shape determines the number of observations from the
    same distribution family. The two normal distributions are independent; thus,
    it is a batch of distributions of the same family.
  prefs: []
  type: TYPE_NORMAL
- en: You can have batches of the same type of distribution family, like in the preceding
    example of having two normal distributions. You cannot create a batch of, say,
    a normal and a Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we need a single normal distribution that is dependent on two variables,
    each with a different mean? This is made possible using `MultivariateNormalDiag`,
    and this influences the event shape – it is the atomic shape of a single draw
    or observation from this distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can see that in the above output the `event_shape` has changed.
  prefs: []
  type: TYPE_NORMAL
- en: Using TFP distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have defined a distribution, you can do a lot more. TFP provides a
    good range of functions to perform various operations. We have already used the
    `Normal` distribution and `sample` method. The section above also demonstrated
    how we can use TFP for creating univariate, multivariate, or independent distribution/s.
    TFP provides many important methods to interact with the created distributions.
    Some of the important ones include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sample(n)`: It samples `n` observations from the distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prob(value)`: It provides probability (discrete) or probability density (continuous)
    for the value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_prob(values)`: Provides log probability or log-likelihood for the values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean()`: It gives the mean of the distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stddev()`: It provides the standard deviation of the distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coin Flip Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us now use some of the features of TFP to describe data by looking at an
    example: the standard coin-flipping example we are familiar with from our school
    days. We know that if we flip a coin, there are only two possibilities – we can
    have either a head or a tail. Such a distribution, where we have only two discrete
    values, is called a **Bernoulli** distribution. So let us consider different scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A fair coin with a `0.5` probability of heads and `0.5` probability of tails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us create the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now get some samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us visualize the samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Shape  Description automatically generated](img/B18331_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Distribution of heads and tails from 2,000 observations'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that we have both heads and tails in equal numbers; after all,
    it is a fair coin. The probability of heads and tails as `0.5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Scenario 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A biased coin with a 0.8 probability of heads and 0.2 probability of tails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, since the coin is biased, with the probability of heads being `0.8`, the
    distribution would be created using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now get some samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us visualize the samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Shape  Description automatically generated with medium confidence](img/B18331_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Distribution of heads and tails from 2,000 coin flips of a biased
    coin'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that now heads are much larger in number than tails. Thus, the probability
    of tails is no longer `0.5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You will probably get a number close to `0.2`.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Two coins with one biased toward heads with a `0.8` probability, and the other
    biased toward heads with a `0.6` probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have two independent coins. Since the coins are biased, with the probabilities
    of heads being `0.8` and `0.6` respectively, we create a distribution using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now get some samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us visualize the samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphical user interface  Description automatically generated](img/B18331_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Distribution of heads and tails from 2,000 flips for two independent
    coins'
  prefs: []
  type: TYPE_NORMAL
- en: The bar in blue corresponds to Coin 1, and the bar in orange corresponds to
    Coin 2\. The brown part of the graphs is the area where the results of the two
    coins overlap. You can see that for Coin 1, the number of heads is much larger
    as compared to Coin 2, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Normal distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the Bernoulli distribution where the data can have only two possible
    discrete values: heads and tails, good and bad, spam and ham, and so on. However,
    a large amount of data in our daily lives is continuous in range, with the normal
    distribution being very common. So let us also explore different normal distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the probability density function of a normal distribution can
    be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_12_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B18331_08_023.png) is the mean of the distribution, and ![](img/B18331_07_010.png)
    is the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: In TFP, the parameter `loc` represents the mean and the parameter `scale` represents
    the standard deviation. Now, to illustrate the use of how we can use distribution,
    let us consider that we want to represent the weather data of a location for a
    particular season, say summer in Delhi, India.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate normal
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can think that weather depends only on temperature. So, by having a sample
    of temperature in the summer months over many years, we can get a good representation
    of data. That is, we can have a univariate normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, based on weather data, the average high temperature in the month of June
    in Delhi is 35 degrees Celsius, with a standard deviation of 4 degrees Celsius.
    So, we can create a normal distribution using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Get some observation samples from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And let us now visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Chart, histogram  Description automatically generated](img/B18331_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Probability density function for the temperature of Delhi in the
    month of June'
  prefs: []
  type: TYPE_NORMAL
- en: It would be good to verify if the mean and standard deviation of our sample
    data is close to the values we described.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the distribution, we can find the mean and standard deviation using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'And from the sampled data, we can verify using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the sampled data is following the same mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'All is good so far. I show my distribution to a friend working in meteorology,
    and he says that using only temperature is not sufficient; the humidity is also
    important. So now, each weather point depends on two parameters – the temperature
    of the day and the humidity of the day. This type of data distribution can be
    obtained using the `MultivariateNormalDiag` distribution class, as defined in
    TFP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 12.7*, shows the multivariate normal distribution of two variables,
    temperature and humidity, generated using TFP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Multivariate normal distribution with the x-axis representing
    temperature and the y-axis humidity'
  prefs: []
  type: TYPE_NORMAL
- en: Using the different distributions and bijectors available in TFP, we can generate
    synthetic data that follows the same joint distribution as real data to train
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bayesian Networks** (**BNs**) make use of the concepts from graph theory,
    probability, and statistics to encapsulate complex causal relationships. Here,
    we build a **Directed Acyclic Graph** (**DAG**), where nodes, called factors (random
    variables), are connected by the arrows representing cause-effect relationships.
    Each node represents a variable with an associated probability (also called a
    **Conditional Probability Table** (**CPT**)). The links tell us about the dependence
    of one node over another. Though they were first proposed by Pearl in 1988, they
    have regained attention in recent years. The main cause of this renowned interest
    in BNs is that standard deep learning models are not able to represent the cause-effect
    relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Their strength lies in the fact that they can be used to model uncertainties
    combined with expert knowledge and data. They have been employed in diverse fields
    for their power to do probabilistic and causal reasoning. At the heart of the
    Bayesian network is Bayes’ rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_12_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes’ rule is used to determine the joint probability of an event given certain
    conditions. The simplest way to understand the BN is that the BN can determine
    the causal relationship between the hypothesis and evidence. There is some unknown
    hypothesis H, about which we want to assess the uncertainty and make some decisions.
    We start with some prior belief about hypothesis H, and then based on evidence
    E, we update our belief about H.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us try to understand it by example. We consider a very standard example:
    a garden with grass and a sprinkler. Now, using common sense, we know that if
    the sprinkler is on, the grass is wet. Let us now reverse the logic: what if you
    come back home and find that the grass is wet, what is the probability that the
    sprinkler is on, and what is the probability that it actually rained? Interesting,
    right? Let us add further evidence – you find that the sky is cloudy. Now, what
    do you think is the reason for the grass being wet?'
  prefs: []
  type: TYPE_NORMAL
- en: This sort of reasoning based on evidence is encompassed by BNs in the form of
    DAGs, also called causal graphs – because they provide an insight into the cause-effect
    relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'To model the problem, we make use of the `JointDistributionCoroutine` distribution
    class. This distribution allows both the sampling of data and computation of the
    joint probability from a single model specification. Let us make some assumptions
    to build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: The probability that it is cloudy is `0.2`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability that it is cloudy and it rains is `0.8`, and the probability
    that it is not cloudy but it rains is `0.1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability that it is cloudy and the sprinkler is on is `0.1`, and the
    probability that it is not cloudy and the sprinkler is on is `0.5`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, for the grass, we have four possibilities:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Sprinkler** | **Rain** | **Grass Wet** |'
  prefs: []
  type: TYPE_TB
- en: '| F | F | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| F | T | 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| T | F | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| T | T | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12.1: The conditional probability table for the Sprinkler-Rain-Grass
    scenario'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.8* shows the corresponding BN DAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18331_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: Bayesian Network for our toy problem'
  prefs: []
  type: TYPE_NORMAL
- en: 'This information can be represented by the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The above model will function like a data generator. The `Root` function is
    used to tell the node in the graph without any parent. We define a few utility
    functions, `broadcast` and `stack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'To do inferences, we make use of the `MarginalizableJointDistributionCoroutine`
    class, as this allows us to compute marginalized probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now, based on our observations, we can obtain the probability of other factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We observe that the grass is wet (the observation corresponding to this is
    1 – if the grass was dry, we would set it to 0), we have no idea about the state
    of the clouds or the state of the sprinkler (the observation corresponding to
    an unknown state is set to “marginalize”), and we want to know the probability
    of rain (the observation corresponding to the probability we want to find is set
    to “tabulate”). Converting this into observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we get the probability of rain using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The result is `array([0.27761015, 0.72238994], dtype=float32)`, that is, there
    is a 0.722 probability that it rained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 2:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We observe that the grass is wet, we have no idea about the state of the clouds
    or rain, and we want to know the probability of whether the sprinkler is on. Converting
    this into observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This results in probabilities `array([0.61783344, 0.38216656], dtype=float32)`,
    that is, there is a `0.382` probability that the sprinkler is on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 3:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'What if we observe that there is no rain, and the sprinkler is off? What do
    you think is the state of the grass? Logic says the grass should not be wet. Let
    us confirm this from the model by sending it the observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This results in the probabilities `array([1., 0], dtype=float32)`, that is,
    there is a 100% probability that the grass is dry, just the way we expected.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, once we know the state of the parents, we do not need to know
    the state of the parent’s parents – that is, the BN follows the local Markov property.
    In the example that we covered here, we started with the structure, and we had
    the conditional probabilities available to us. We demonstrate how we can do inference
    based on the model, and how despite the same model and CPDs, the evidence changes
    the **posterior probabilities**.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian networks, the structure (the nodes and how they are interconnected)
    and the parameters (the conditional probabilities of each node) are learned from
    the data. They are referred to as structured learning and parameter learning respectively.
    Covering the algorithms for structured learning and parameter learning are beyond
    the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Handling uncertainty in predictions using TensorFlow Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we talked about the uncertainties in prediction
    by deep learning models and how the existing deep learning architectures are not
    able to account for those uncertainties. In this chapter, we will use the layers
    provided by TFP to model uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Before adding the TFP layers, let us first understand the uncertainties a bit.
    There are two classes of uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Aleatory uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This exists because of the random nature of the natural processes. It is inherent
    uncertainty, present due to the probabilistic variability. For example, when tossing
    a coin, there will always be a certain degree of uncertainty in predicting whether
    the next toss will be heads or tails. There is no way to remove this uncertainty.
    In essence, every time you repeat the experiment, the results will have certain
    variations.
  prefs: []
  type: TYPE_NORMAL
- en: Epistemic uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This uncertainty comes from a lack of knowledge. There can be various reasons
    for this lack of knowledge, for example, an inadequate understanding of the underlying
    processes, an incomplete knowledge of the phenomena, and so on. This type of uncertainty
    can be reduced by understanding the reason, for example, to get more data, we
    conduct more experiments.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of these uncertainties increases risk. We require a way to quantify
    these uncertainties and, hence, quantify the risk.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a synthetic dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to modify the standard deep neural networks
    to quantify uncertainties. Let us start with creating a synthetic dataset. To
    create the dataset, we consider that output prediction y depends on input x linearly,
    as given by the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_12_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B18331_12_006.png) follows a normal distribution with mean zero
    and standard deviation 1 around x. The function below will generate this synthetic
    data for us. Do observe that to generate this data, we made use of the `Uniform`
    distribution and `Normal` distributions available as part of TFP distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`y_true` is the value without including the normal distributed noise ![](img/B18331_10_003.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we use it to create a training dataset and a validation dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us 2,000 datapoints for training and 500 datapoints for validation.
    *Figure 12.9* shows the plots of the two datasets, with ground truth (the value
    of *y* in the absence of any noise) in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Plot of the synthetic dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Building a regression model using TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can build a simple Keras model to perform the task of regression on the
    synthetic dataset created in the preceding section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us see how good the fitted model works on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Ground truth and fitted regression line'
  prefs: []
  type: TYPE_NORMAL
- en: It was a simple problem, and we can see that the fitted regression line almost
    overlaps the ground truth. However, there is no way to tell the uncertainty of
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic neural networks for aleatory uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What if instead of linear regression, we build a model that can fit the distribution?
    In our synthetic dataset, the source of aleatory uncertainty is the noise, and
    we know that our noise follows a normal distribution, which is characterized by
    two parameters: the mean and standard deviation. So, we can modify our model to
    predict the mean and standard deviation distributions instead of actual *y* values.
    We can accomplish this using either the `IndependentNormal` TFP layer or the `DistributionLambda`
    TFP layer. The following code defines the modified model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to make one more change. Earlier, we predicted the *y* value;
    therefore, the mean square error loss was a good choice. Now, we are predicting
    the distribution; therefore, a better choice is the negative log-likelihood as
    the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now train this new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Since now our model returns a distribution, we require the statistics mean
    and standard deviation for the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the predicted mean now corresponds to the fitted line in the first
    case. Let us now see the plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The following curve shows the fitted line, along with the aleatory uncertainty:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Line chart  Description automatically generated](img/B18331_12_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Modelling aleatory uncertainty using TFP layers'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that our model shows less uncertainty near the origin, but as we
    move further away, the uncertainty increases.
  prefs: []
  type: TYPE_NORMAL
- en: Accounting for the epistemic uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In conventional neural networks, each weight is represented by a single number,
    and it is updated such that the loss of the model with respect to its weight is
    minimized. We assume that weights so learned are the optimum weights. But are
    they? To answer this question, we replace each weight with a distribution, and
    instead of learning a single value, we will now make our model learn a set of
    parameters for each weight distribution. This is accomplished by replacing the
    Keras `Dense` layer with the `DenseVariational` layer. The `DenseVariational`
    layer uses a variational posterior over the weights to represent the uncertainty
    in their values. It tries to regularize the posterior to be close to the prior
    distribution. Hence, to use the `DenseVariational` layer, we will need to define
    two functions, one prior generating function and another posterior generating
    function. We use the posterior and prior functions defined at [https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression](https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model now has two layers, a `DenseVariational` layer followed by a `DistributionLambda`
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, as we are looking for distributions, the loss function that we use is
    the negative log-likelihood function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We continue with the same synthetic data that we created earlier and train
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the model has been trained, we make the prediction, and to understand
    the concept of uncertainty, we make multiple predictions for the same input ranges.
    We can see the difference in variance in the result in the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Chart, scatter chart  Description automatically generated](img/B18331_12_12_1.png)
    | ![Chart, scatter chart  Description automatically generated](img/B18331_12_12_2.png)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 12.12: Epistemic uncertainty'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.12* shows two graphs, one when only 200 training data points were
    used to build the model, and the second when 2,000 data points were used to train
    the model. We can see that when there is more data, the variance and, hence, the
    epistemic uncertainty reduces. Here, *overall mean* refers to the mean of all
    the predictions (100 in number), and in the case of *ensemble mean*, we considered
    only the first 15 predictions. All machine learning models suffer from some level
    of uncertainty in predicting outcomes. Getting an estimate or quantifiable range
    of uncertainty in the prediction will help AI users build more confidence in their
    AI predictions and will boost overall AI adoption.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced TensorFlow Probability, the library built over TensorFlow
    to perform probabilistic reasoning and statistical analysis. The chapter started
    with the need for probabilistic reasoning – the uncertainties both due to the
    inherent nature of data and due to a lack of knowledge. We demonstrated how to
    use TensorFlow Probability distributions to generate different data distributions.
    We learned how to build a Bayesian network and perform inference. Then, we built
    Bayesian neural networks using TFP layers to take into account aleatory uncertainty.
    Finally, we learned how to account for epistemic uncertainty with the help of
    the `DenseVariational` TFP layer.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about TensorFlow AutoML frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D.,
    Patton, B., Alemi, A., Hoffman, M., and Saurous, R. A. (2017). *TensorFlow distributions*.
    arXiv preprint arXiv:1711.10604.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Piponi, D., Moore, D., and Dillon, J. V. (2020). *Joint distributions for TensorFlow
    probability*. arXiv preprint arXiv:2001.11819.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fox, C. R. and Ülkümen, G. (2011). *Distinguishing Two Dimensions of Uncertainty*,
    in Essays in Judgment and Decision Making, Brun, W., Kirkebøen, G. and Montgomery,
    H., eds. Oslo: Universitetsforlaget.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hüllermeier, E. and Waegeman, W. (2021). *Aleatoric and epistemic uncertainty
    in machine learning: An introduction to concepts and methods*. Machine Learning
    110, no. 3: 457–506.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1831217224278819687.png)'
  prefs: []
  type: TYPE_IMG
