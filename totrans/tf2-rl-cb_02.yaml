- en: '*Chapter 2*: Implementing Value-Based, Policy-Based, and Actor-Critic Deep
    RL Algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides a practical approach to building value-based, policy-based,
    and actor-critic algorithm-based **reinforcement learning** (**RL**) agents. It
    includes recipes for implementing value iteration-based learning agents and breaks
    down the implementation details of several foundational algorithms in RL into
    simple steps. The policy gradient-based agent and the actor-critic agent make
    use of the latest major version of **TensorFlow 2.x** to define the neural network
    policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Building stochastic environments for training RL agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building value-based (RL) agent algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing temporal difference learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building Monte Carlo prediction and control algorithms for RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the SARSA algorithm and an RL agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Q-learning agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing policy gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing actor-critic algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in this book has been tested extensively on Ubuntu 18.04 and Ubuntu
    20.04, and should work with later versions of Ubuntu if Python 3.6+ is available.
    With Python 3.6 installed, along with the necessary Python packages listed at
    the beginning of each recipe, the code should run fine on Windows and Mac OS X
    too. It is advised that you create and use a Python virtual environment named
    `tf2rl-cookbook` to install the packages and run the code in this book. Installing
    Miniconda or Anaconda for Python virtual environment management is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code for each recipe in each chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Building stochastic environments for training RL agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train RL agents for the real world, we need learning environments that are
    stochastic, since real-world problems are stochastic in nature. This recipe will
    walk you through the steps for building a **Maze** learning environment to train
    RL agents. The Maze is a simple, stochastic environment where the world is represented
    as a grid. Each location on the grid can be referred to as a cell. The goal of
    an agent in this environment is to find its way to the goal state. Consider the
    maze shown in the following diagram, where the black cells represent walls:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – The Maze environment ](img/B15074_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – The Maze environment
  prefs: []
  type: TYPE_NORMAL
- en: The agent's location is initialized to be at the top-left cell in the Maze.
    The agent needs to find its way around the grid to reach the goal located at the
    top-right cell in the Maze, collecting a maximum number of coins along the way
    while avoiding walls. The location of the goal, coins, walls, and the agent's
    starting location can be modified in the environment's code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four-dimensional discrete actions that are supported in this environment
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0*: Move up'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1*: Move down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*2*: Move left'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*3*: Move right'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward is based on the number of coins that are collected by the agent before
    they reach the goal state. Because the environment is stochastic, the action that's
    taken by the environment has a slight (0.1) probability of "slipping" wherein
    the actual action that's executed will be altered stochastically. The slip action
    will be the clockwise directional action (LEFT -> UP, UP -> RIGHT, and so on).
    For example, with `slip_probability=0.2`, there is a 0.2 probability that a RIGHT
    action may result in DOWN.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The learning environment is a simulator that provides observations for the RL
    agent, supports a set of actions that the RL agent can perform by executing the
    actions, and returns the resultant/new observation as a result of the agent taking
    the action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to implement a stochastic Maze learning environment that
    represents a simple 2D map with cells representing the location of the agent,
    their goal, walls, coins, and empty space:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by defining the MazeEnv class and a map of the Maze environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, place the obstacles/walls on the environment map in the appropriate places:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the slip mapping action in clockwise order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define a lookup table in the form of a dictionary to map indices
    to cells in the Maze environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s define the reverse lookup to find a cell, when given an index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, we have finished initializing the environment!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s define a method that will handle the coins and their statuses in
    the Maze, where 0 means that the coin wasn''t collected by the agent and 1 means
    that the coin was collected by the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define a quick method that will do the inverse operation of finding
    the number status/value of a coin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define a setter function to set the state of the environment.
    This is useful for algorithms such as value iteration, where each and every state
    needs to be visited in the environment for it to calculate values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it is time to implement the `step` method. We''ll begin by implementing
    the `step` method and applying the `slip` action based on `slip_probability`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Continuing with our implementation of the `step` function, we''ll update the
    state of the maze based on the action that''s taken:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will determine whether the agent has reached the goal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll handle cases when the action results in hitting an obstacle/wall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last case you need to handle is seeing whether the action leads to collecting
    a coin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To visualize the state of the Gridworld in a human-friendly manner, let''s
    implement a render function that will print out a text version of the current
    state of the Maze environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To test whether the environment is working as expected, let''s add a `__main__`
    function that gets executed if the environment script is run directly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we''re all set! The Maze environment is ready and we can quickly
    test it by running the script (`python envs/maze.py`). An output similar to the
    following will be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Textual representation of the Maze environment highlighting
    and underlining the agent''s current state ](img/B15074_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Textual representation of the Maze environment highlighting and
    underlining the agent's current state
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our `map`, as defined in *step 1* in the *How to do it…* section, represents
    the state of the learning environment. The Maze environment defines the observation
    space, the action space, and the rewarding mechanism for implementing a `env.render()`
    method converts the environment's internal grid representation into a simple text/string
    grid and prints it for easy visual understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Building value-based reinforcement learning agent algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Value-based reinforcement learning works by learning the state-value function
    or the action-value function in a given environment. This recipe will show you
    how to create and update the value function for the Maze environment to obtain
    an optimal policy. Learning value functions, especially in model-free RL problems
    where a model of the environment is not available, can prove to be quite effective,
    especially for RL problems with low-dimensional state space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon completing this recipe, you will have an algorithm that can generate the
    following optimal action sequence based on value functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Optimal action sequence generated by a value-based RL algorithm
    with state values represented through a jet color map ](img/B15074_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Optimal action sequence generated by a value-based RL algorithm
    with state values represented through a jet color map
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install numpy gym`. If the following import statement
    runs without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's implement a value function learning algorithm based on value iteration.
    We will use the Maze environment to implement and analyze the value iteration
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to implement this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the Maze learning environment from `envs.maze`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of `MazeEnv` and print the observation space and action
    space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the state dimension in order to initialize `state-values`, `state-action
    values`, and our policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to implement a function that can calculate the state/action
    value when given a state in the environment and an action. We will begin by declaring
    the `calculate_values` function; we''ll complete the implementation in the following
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the next step, we will generate `slip_action`, which is a stochastic action
    based on the stochasticity of the learning environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When calculating the values of a given state-action pair, it is important to
    be able to set the state of the environment before executing an action to observe
    the reward/result. The Maze environment provides a convenient `set_state` method
    for setting the current state of the environment. Let''s make use of it and step
    through the environment with the desired (input) action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need a list of transitions in the environment to be able to calculate the
    rewards, as per the Bellman equations. Let''s create a `transitions` list and
    append the newly obtained environment transition information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s obtain another transition using the state and the action, this time
    without stochasticity. We can do this by not using `slip_action` and setting `slip=False`
    while stepping through the Maze environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There is only one more step needed to complete the `calculate_values` function,
    which is to calculate the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can start implementing the state/action value learning. We will begin
    by defining the `max_iteration` hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement the `state-value` function learning loop using value iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the `state-value` function learning loop implemented, let''s
    move on and implement the `action-value` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the `action-value` function computed, we are only one step away from obtaining
    the optimal policy. Let's go get it!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can print the Q values (the `state-action` values) and the policy using
    the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a final step, let''s visualize the value function''s learning and policy
    updates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will generate the following diagrams, which show the progress
    of the value function while it''s learning and the policy updates:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Progression (from left to right and from top to bottom) of the  learned
    value function and the policy ](img/B15074_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Progression (from left to right and from top to bottom) of the
    learned value function and the policy
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Maze environment contains a start cell, a goal cell, and a few cells containing
    coins, walls, and open spaces. There are 112 distinct states in the Maze environment
    due to the varying nature of the cells with coins. For illustration purposes,
    when an agent collects one of the coins, the environment is in a completely different
    state compared to the state when the agent collects a different coin. This is
    because the location of the coin also matters.
  prefs: []
  type: TYPE_NORMAL
- en: '`q_values` (state-action values) is a big matrix of size 112 x 4, so it will
    print a long list of values. We will not show these here. The other two print
    statements in *step 14* should produce an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Textual representation of the optimal action sequence ](img/B15074_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Textual representation of the optimal action sequence
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration-based value function learning follows Bellman equations, and
    the optimal policy is obtained from the Q-value function by simply choosing the
    action with the highest Q/action-value.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2.4*, the value function is represented using a jet color map, while
    the policy is represented using the green-arrows. Initially, the values for the
    states are almost even. As the learning progress, states with coins get more value
    than states without coins, and the state that leads to the goal gets a very high
    value that's only slightly less than the goal state itself. The black cells in
    the maze represents the walls. The arrows represent the directional action that
    the policy is prescribing from the given cell in the maze. As the learning converges,
    as shown in the bottom-right diagram, the policy is optimal, leading the agent
    to the goal after it's collected every coin.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The color versions of the diagrams in this book are available to download. You
    can find the link to these diagrams in the *Preface* of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing temporal difference learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe will walk you through how to implement the **temporal difference**
    (**TD**) learning algorithm. TD algorithms allow us to incrementally learn from
    incomplete episodes of agent experiences, which means they can be used for problems
    that require online learning capabilities. TD algorithms are useful in model-free
    RL settings as they do not depend on a model of the MDP transitions or rewards.
    To visually understand the learning progression of the TD algorithm, this recipe
    will also show you how to implement the GridworldV2 learning environment, which
    looks as follows when rendered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – The GridworldV2 learning environment 2D rendering with  state
    values and grid cell coordinates ](img/B15074_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – The GridworldV2 learning environment 2D rendering with state values
    and grid cell coordinates
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install numpy gym`. If the following import statements
    run without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe will contain two components that we will put together at the end.
    The first component is the GridworldV2 implementation, while the second component
    is the TD learning algorithm''s implementation. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by implementing GridworldV2 and then by defining the `GridworldV2Eng`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, you will continue implementing the `__init__` method and define
    the necessary values that define the size of the Gridworld, the goal location,
    the wall location, and the location of the bomb, among other things:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can move on to the definition of the `reset()` method, which will be
    called at the start of every episode, including the first one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement a `get_next_state` method so that we can conveniently obtain
    the next state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we are ready to implement the main `step` method of the `GridworldV2`
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can move on and implement the temporal difference learning algorithm.
    Let''s begin by initializing the state values of the grid using a 2D `numpy` array
    and then set the value of the goal location and the bomb state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s define the discount factor, `gamma`, the learning rate, `alpha`,
    and initialize `done` to `False`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now define the main outer loop so that it runs `max_episodes` times,
    resetting the state of the environment to its initial state at the start of every
    episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it''s time to implement the inner loop with the temporal difference learning
    update one-liner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the learning has converged, we want to be able to visualize the state
    values for each state in the GridwordV2 environment. To do that, we can make use
    of the `visualize_grid_state_values` function from `value_function_utils`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to run the `temporal_difference_learning` function from our
    main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will take a few seconds to run temporal difference learning
    for `max_episodes`. It will then produce a diagram similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Rendering of the GridworldV2 environment, with the grid cell
    coordinates and state values colored according to the scale shown on the right  ](img/B15074_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Rendering of the GridworldV2 environment, with the grid cell coordinates
    and state values colored according to the scale shown on the right
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on our environment''s implementation, you may have noticed that `goal_state`
    is located at `(0, 3)` and that `bomb_state` is located at `(1, 3)`. This is based
    on the coordinates, colors, and values of the grid cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Rendering of the GridWorldV2 environment with initial state
    values ](img/B15074_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Rendering of the GridWorldV2 environment with initial state values
  prefs: []
  type: TYPE_NORMAL
- en: 'The state is linearized and is represented using a single integer indicating
    each of the 12 distinct states in the GridWorldV2 environment. The following diagram
    shows a linearized rendering of the grid states to give you a better understanding
    of the state encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Linearized representation of the states ](img/B15074_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Linearized representation of the states
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to implement temporal difference learning, let's move
    on to building Monte Carlo algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Building Monte Carlo prediction and control algorithms for RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe provides the ingredients for building a **Monte Carlo** prediction
    and control algorithm so that you can build your RL agents. Similar to the temporal
    difference learning algorithm, Monte Carlo learning methods can be used to learn
    both the state and the action value functions. Monte Carlo methods have zero bias
    since they learn from complete episodes with real experience, without approximate
    predictions. These methods are suitable for applications that require good convergence
    properties. The following diagram illustrates the value that''s learned by the
    Monte Carlo method for the GridworldV2 environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Monte Carlo prediction of state values (left) and state-action
    values (right) ](img/B15074_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Monte Carlo prediction of state values (left) and state-action
    values (right)
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statement runs without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start by implementing the `monte_carlo_prediction` algorithm and visualizing
    the learned value function for each state in the `GridworldV2` environment. After
    that, we will implement an `monte_carlo_control` algorithm to construct an agent
    that will act in an RL environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the import statements and import the necessary Python modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to define the `monte_carlo_prediction` function and initialize
    the necessary objects, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s implement the outer loop. Outer loops are commonplace in all RL
    agent training code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next up is the inner loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have all the information we need to compute the state values of the
    states in the grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it''s time to run our Monte Carlo predictor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code should produce a diagram showing the rendering for the GridworldV2
    environment, along with state values:![Figure 2.11 – Rendering of GridworldV2
    with state values learned using the  Monte Carlo prediction algorithm ](img/B15074_02_011.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.11 – Rendering of GridworldV2 with state values learned using the Monte
    Carlo prediction algorithm
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s implement a function for the epsilon-greedy policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s move on to the implementation of the **Monte Carlo Control** algorithm
    for reinforcement learning. We will start by defining the function, along with
    the initial values for the state-action values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s continue with the implementation of the Monte Carlo Control function
    by initializing the returns for all the possible state and action pairs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the next step, let''s define the outer loop for each episode and then the
    inner loop for each step in an episode. By doing this, we can collect trajectories
    of experience until the end of an episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have a full trajectory for an episode in the inner loop, we can
    implement our Monte Carlo Control update to update the state-action values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the outer loop completes, we can visualize the state-action values using
    the `visualize_grid_action_values` helper function from the `value_function_utils`
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s run our `monte_carlo_control` function to learn the `state-action`
    values in the GridworldV2 environment and display the learned values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will produce a rendering similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Rendering of the GridworldV2 environment with four action values
    per grid state shown using rectangles ](img/B15074_02_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Rendering of the GridworldV2 environment with four action values
    per grid state shown using rectangles
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this recipe!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monte Carlo methods for episodic tasks learn directly from experience from
    full sample returns obtained in an episode. The Monte Carlo prediction algorithm
    for estimating the value function based on first visit averaging is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Monte Carlo prediction algorithm ](img/B15074_02_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Monte Carlo prediction algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Once a series of trajectories have been collected by the agent, we can use the
    transition information in the Monte Carlo Control algorithm to learn the state-action
    value function. This can be used by an agent so that they can act in a given RL
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Monte Carlo Control algorithm is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Monte-Carlo Control algorithm ](img/B15074_02_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Monte-Carlo Control algorithm
  prefs: []
  type: TYPE_NORMAL
- en: The results of the learned state-action value function are shown in *Figure
    2.12*, where each triangle in a grid cell shows the state-action value of taking
    that directional action in that grid state. The base of the triangle lies in the
    direction of the action. For example, the triangle in the top-left corner of *Figure
    2.12* that has a value of 0.44 is the state-action value of taking the LEFT action
    in that grid state.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the SARSA algorithm and an RL agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will show you how to implement the **State-Action-Reward-State-Action**
    (**SARSA**) algorithm, as well as how to develop and train an agent using the
    SARSA algorithm so that it can act in a reinforcement learning environment. The
    SARSA algorithm can be applied to model-free control problems and allows us to
    optimize the value function of an unknown MDP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon completing this recipe, you will have a working RL agent that, when acting
    in the GridworldV2 environment, will generate the following state-action value
    function using the SARSA algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Rendering of the GridworldV2 environment – each triangle represents
    the action value of taking that directional action in that grid state ](img/B15074_02_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Rendering of the GridworldV2 environment – each triangle represents
    the action value of taking that directional action in that grid state
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's implement the SARSA learning update as a function and make use of an epsilon-greedy
    exploration policy. With these two pieces combined, we will have a complete agent
    to act in a given RL environment. In this recipe, we will train and test the agent
    in the GridworldV2 environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our implementation step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define a function for implementing the SARSA algorithm and initialize
    the state-action values with zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now update the values for the goal state and the bomb state based on
    the environment''s configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the discount factor, `gamma`, and the learning rate hyperparameter,
    `alpha`. Also, let''s create a convenient alias for `grid_action_values` by calling
    it `q`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s begin to implement the outer loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it''s time to implement the inner loop with the SARSA learning update
    step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the final step in the `sarsa` function, let''s visualize the state-action
    value function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will implement the epsilon-greedy policy that the agent will use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must implement the main function and run the SARSA algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When executed, a rendering of the GridworldV2 environment with the state-action
    values will appear, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Output of the SARSA algorithm in the GridworldV2 environment
    ](img/B15074_02_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Output of the SARSA algorithm in the GridworldV2 environment
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SARSA is an on-policy temporal difference learning-based control algorithm.
    This recipe made uses of the SARSA algorithm to estimate the optimal state-action
    values. The SARSA algorithm can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – SARSA algorithm ](img/B15074_02_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – SARSA algorithm
  prefs: []
  type: TYPE_NORMAL
- en: As you may be able to tell, this is very similar to the Q-learning algorithm.
    The similarities will become clear when we look at the next recipe in this chapter,
    *Building a Q-learning agent*.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Q-learning agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will show you how to build a **Q-learning** agent. Q-learning can
    be applied to model-free RL problems. It supports off-policy learning and therefore
    provides a practical solution to problems where available experiences were/are
    collected using some other policy or by some other agent (even humans).
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon completing this recipe, you will have a working RL agent that, when acting
    in the GridworldV2 environment, will generate the following state-action value
    function using the SARSA algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18 – State-action values obtained using the Q-learning algorithm
    ](img/B15074_02_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – State-action values obtained using the Q-learning algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's implement the Q-learning algorithm as a function, as well as an epsilon-greedy
    policy to build our Q-learning agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define a function for implementing the Q-learning algorithm and
    initialize the state-action values with zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now update the values for the goal state and the bomb state based on
    the environment''s configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the discount factor, `gamma`, and the learning rate hyperparameter,
    `alpha`. Also, let''s create a convenient alias for `grid_action_values` by calling
    it `q`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s begin to implement the outer loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the next step, let''s implement the inner loop with the Q-learning update.
    We will also decay the epsilon used in the epsilon-greedy policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the final step in the `q_learning` function, let''s visualize the state-action
    value function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement the epsilon-greedy policy that the agent will use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will implement the main function and run the SARSA algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When executed, a rendering of the GridworldV2 environment with the state-action
    values will appear, as shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Rendering of the GridworldV2 environment with the action values
    obtained using the Q-learning algorithm ](img/B15074_02_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Rendering of the GridworldV2 environment with the action values
    obtained using the Q-learning algorithm
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Q-learning algorithm involves the Q value update, which can be summarized
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_02_002.png) is the value of the Q function for the current
    state, s, and action, a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_02_003.png) is used for choosing the maximum value from the
    possible next steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_02_004.png) is the current position of the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_02_005.png) is the current action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_02_006.png) is the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_02_007.png) is the reward that is received in the current position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_02_008.png) is the gamma (reward decay, discount factor).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_02_009.png) is the next state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_02_010.png) is the actions that are available in the next state,
    ![](img/Formula_02_011.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you may now be able to tell, the difference between Q-learning and SARSA
    is only in how the action-value/Q-value of the next state and action pair is calculated.
    In Q-learning, we use ![](img/Formula_02_012.png), the maximum value of the Q-function,
    whereas in the SARSA algorithm, we take the Q-value of the action that was chosen
    in the next state. This may sound subtle, but because the Q-learning algorithm
    infers the value by taking the max over all actions and doesn't just infer based
    on the current behavior policy, it can directly learn the optimal policy. On the
    other hand, the SARSA algorithm learns a near-optimal policy based on the behavior
    policy's exploration parameter (for example, the ε parameter in the ε-greedy policy).
    The SARSA algorithm has a better convergence property than the Q-learning algorithm,
    so it is more suited for cases where learning happens online and or on a real-world
    system, or even if there are real resources (time and/or money) being spent compared
    to training in a simulation or simulated worlds. Q-learning is more suited for
    training an "optimal" agent in simulation or when the resources (like time/money)
    are not too costly.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing policy gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Policy gradient algorithms** are fundamental to reinforcement learning and
    serve as the basis for several advanced RL algorithms. These algorithms directly
    optimize for the best policy, which can lead to faster learning compared to value-based
    algorithms. Policy gradient algorithms are effective for problems/applications
    with high-dimensional or continuous action spaces. This recipe will show you how
    to implement policy gradient algorithms using TensorFlow 2.0\. Upon completing
    this recipe, you will be able to train an RL agent in any compatible OpenAI Gym
    environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are three main parts to this recipe. The first one is applying the policy
    function, which is going to be represented using a neural network implemented
    in TensorFlow 2.x. The second part is applying the Agent class' implementation,
    while the final part will be to apply a trainer function, which is used to train
    the policy gradient-based agent in a given RL environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start implementing the parts one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to define the `PolicyNet` class. We will define the model
    so that it has three fully connected or **dense** neural network layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement the `call` function, which will be called to process
    inputs to the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also define a `process` function that we can call with a batch of observations
    to be processed by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the policy network defined, we can implement the `Agent` class, which
    utilizes the policy network, and an optimizer for training the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define a policy helper function that takes an observation as input,
    has it processed by the policy network, and returns the action as the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define another helper function to get the action from the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it''s time to define the learning updates for the policy gradient algorithm.
    Let''s initialize the `learn` function with an empty list for discounted rewards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the right place to calculate the discounted rewards while using the
    episodic rewards as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s implement the crucial step of calculating the policy gradient and
    update the parameters of the neural network policy using an optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement the loss function that we referred to in the previous step
    to calculate the policy parameter updates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the Agent class fully implemented, we can move on to implementing the
    agent training function. Let''s start with the function''s definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s begin with the outer loop implementation of the agent training
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s continue to implement the inner loop to finalize the `train` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we need to implement the main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will launch the training process for the agent in the `render=True`)
    and display what the agent is doing in the environment with respect to driving
    the car uphill. Once the agent has been trained for a sufficient number of episodes,
    you will see the agent driving the car all the way up hill, as shown in the following
    diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.20 – Policy gradient agent completing the MountainCar task ](img/B15074_02_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 – Policy gradient agent completing the MountainCar task
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this recipe!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We used TensorFlow 2.x''s `MountainCar` RL environment. The policy gradient
    algorithm is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – Policy gradient algorithm ](img/B15074_02_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.21 – Policy gradient algorithm
  prefs: []
  type: TYPE_NORMAL
- en: As you train the policy gradient-based agent, you will observe that while the
    agent can learn to drive the car up the mountain, this can take a long time or
    they may get stuck in local minima. This basic version of the policy gradient
    has some limitations. The policy gradient is an on-policy algorithm that can only
    use experiences/trajectories or episode transitions from the same policy that
    is being optimized. The basic version of the policy gradient algorithm does not
    provide a guarantee for monotonic improvements in performance as it can get stuck
    in local minima.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing actor-critic RL algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Actor-critic algorithms** allow us to combine value-based and policy-based
    reinforcement learning – an all-in-one agent. While policy gradient methods directly
    search and optimize the policy in the policy space, leading to smoother learning
    curves and improvement guarantees, they tend to get stuck at the local maxima
    (for a long-term reward optimization objective). Value-based methods do not get
    stuck at local optimum values, but they lack convergence guarantees, and algorithms
    such as Q-learning tend to have high variance and are not very sample-efficient.
    Actor-critic methods combine the good qualities of both value-based and policy
    gradient-based algorithms. Actor-critic methods are also more sample-efficient.
    This recipe will make it easy for you to implement an actor-critic-based RL agent
    using TensorFlow 2.x. Upon completing this recipe, you will be able to train the
    actor-critic agent in any OpenAI Gym-compatible reinforcement learning environment.
    As an example, we will train the agent in the CartPole-V0 environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment and run `pip install -r requirements.txt`. If the following
    import statements run without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are three main parts to this recipe. The first is creating the actor-critic
    model, which is going to be represented using a neural network implemented in
    TensorFlow 2.x. The second part is creating the Agent class' implementation, while
    the final part is going to be about creating a trainer function that will train
    the policy gradient-based agent in a given RL environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start implementing the parts one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin with our implementation of the `ActorCritic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final thing we need to do in the `ActorCritic` class is implement the `call`
    function, which performs a forward pass through the neural network model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the `ActorCritic` class defined, we can move on and implement the `Agent`
    class and initialize an `ActorCritic` model, along with an optimizer to update
    the parameters of the actor-critic model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must implement the agent''s `get_action` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s implement a function that will calculate the actor loss based on
    the actor-critic algorithm. This will drive the parameters of the actor-critic
    network and allow the agent to improve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to implement the learning function of the actor-critic agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define the training function for training the agent in a given
    RL environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final step is to implement the main function, which will call the trainer
    to train the agent for the specified number of episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the agent has been sufficiently trained, you will see that the agent is
    able to balance the pole on the cart pretty well, as shown in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.22 – Actor-critic agent solving the CartPole task ](img/B15074_02_022.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 2.22 – Actor-critic agent solving the CartPole task
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we defined a neural network-based actor-critic model using TensorFlow
    2.x's Keras API. In the neural network model, we defined two fully connected or
    dense neural network layers to extract features from the input. This produced
    two outputs corresponding to the output for an actor and an output for the critic.
    The critic's output is a single float value, whereas the actor's output represents
    the logits for each of the allowed actions in a given RL environment.
  prefs: []
  type: TYPE_NORMAL
