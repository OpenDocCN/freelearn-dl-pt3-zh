- en: Appendix 1 – Reinforcement Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's have a look at all the reinforcement learning algorithms we have learned
    about in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps involved in a typical reinforcement learning algorithm are given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the agent interacts with the environment by performing an action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent performs an action and moves from one state to another.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the agent will receive a reward based on the action it performed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the reward, the agent will understand whether the action is good or bad.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the action was good, that is, if the agent received a positive reward, then
    the agent will prefer performing that action, else the agent will try performing
    other actions that can result in a positive reward. So reinforcement learning
    is basically a trial-and-error learning process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Value Iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm of value iteration is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the optimal value function by taking maximum over the Q function, that
    is, ![](img/B15558_18_001.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the optimal policy from the computed optimal value function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Policy Iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm of policy iteration is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a random policy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value function using the given policy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract a new policy using the value function obtained from *step 2*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the extracted policy is the same as the policy used in *step 2* then stop,
    else send the extracted new policy to *step 2* and repeat *steps 2* to *4*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First-Visit MC Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm of first-visit MC prediction is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*) be the sum of the return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_18_002.png) is given as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using the policy ![](img/B15558_04_054.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all rewards obtained in the episode in a list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the state *s*[t] is occurring for the first time in the episode:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the return of the state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state *s*[t] as total_return(*s*[t]) = total_return(*s*[t])
    + *R*(*s*[t])
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_056.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every-Visit MC Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm of every-visit MC prediction is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*) be the sum of the return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_03_084.png) is given as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using the policy ![](img/B15558_14_001.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return of the state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state *s*[t] as total_return(*s*[t]) = total_return(*s*[t])
    + *R*(*s*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_056.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MC Prediction – the Q Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for MC prediction of the Q function is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero. The policy ![](img/B15558_03_084.png)
    is given as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using policy ![](img/B15558_03_084.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return for the state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state-action pair, total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + *R*(*s*[t], *a*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q function (Q value) by just taking the average, that is:![](img/B15558_04_067.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MC Control Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for the MC control method is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_03_008.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using policy ![](img/B15558_04_032.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the return of a state-action pair,*R*(*s*[t], *a*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + *R*(*s*[t], *a*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q value by just taking the average, that is, ![](img/B15558_18_013.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the new updated policy from ![](img/B15558_03_139.png) using the Q function:![](img/B15558_18_015.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On-Policy MC Control – Exploring starts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for on-policy MC control by exploring the starts method is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_04_099.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the initial state *s*[0] and initial action *a*[0] randomly such that
    all state-action pairs have a probability greater than 0
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode from the selected initial state *s*[0] and action *a*[0]
    using policy ![](img/B15558_04_099.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + *R*(*s*[t], *a*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q value by just taking the average, that is,![](img/B15558_18_013.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the updated policy from ![](img/B15558_04_099.png) using the Q function:![](img/B15558_18_020.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On-Policy MC Control – Epsilon-Greedy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for on-policy MC control with the epsilon-greedy policy is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_03_084.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using policy ![](img/B15558_03_008.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:]).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + *R*(*s*[t], *a*[t]).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q value by just taking the average, that is,![](img/B15558_18_013.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the updated policy ![](img/B15558_03_084.png) using the Q function.
    Let ![](img/B15558_14_185.png). The policy ![](img/B15558_03_084.png) selects
    the best action ![](img/B15558_18_027.png) with probability ![](img/B15558_04_123.png),
    and a random action with probability ![](img/B15558_13_276.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Off-Policy MC Control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for the off-policy MC control method is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the Q function *Q*(*s*, *a*) with random values and set the behavior
    policy *b* to be epsilon-greedy, set the target policy ![](img/B15558_03_140.png)
    to be greedy policy and initialize the cumulative weights as *C*(*s*, *a*) = 0
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using the behavior policy *b*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize return *R* to 0 and weight *W* to 1
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode, *t* = *T* – 1, *T* – 2, . . . , 0:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return as *R* = *R* + *r*[t+1]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the cumulative weights to *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t]) +*W*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Q value to ![](img/B15558_18_031.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target policy ![](img/B15558_18_032.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If ![](img/B15558_18_033.png) then break
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weight to ![](img/B15558_18_034.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the target policy ![](img/B15558_03_139.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TD Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for the TD prediction method is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the value function *V*(*s*) with random values. A policy ![](img/B15558_03_084.png)
    is given.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the state *s*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform an action *a* in the state *s* according to given policy ![](img/B15558_03_055.png),
    get the reward *r*, and move to the next state ![](img/B15558_12_264.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value of the state to ![](img/B15558_18_039.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update ![](img/B15558_18_040.png) (this step implies we are changing the next
    state ![](img/B15558_18_041.png) to the current state *s*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *s* is not a terminal state, repeat *steps 1* to *4*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: On-Policy TD Control – SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for on-policy TD control – SARSA is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the Q function *Q*(*s*, *a*) with random values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the state *s*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract a policy from *Q*(*s*, *a*) and select an action *a* to perform in the
    state *s*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the action *a*, move to the new state ![](img/B15558_18_041.png), and
    observe the reward *r*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In the state ![](img/B15558_18_043.png), select the action ![](img/B15558_18_044.png)
    using the epsilon-greedy policy
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Q value to ![](img/B15558_18_045.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update ![](img/B15558_18_046.png) and ![](img/B15558_18_047.png) (update the
    next state ![](img/B15558_03_021.png)-action ![](img/B15558_18_049.png) pair to the
    current state *s*-action *a* pair)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *s* is not the terminal state, repeat *steps 1* to *5*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Off-Policy TD Control – Q Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for off-policy TD control – Q learning is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a Q function *Q*(*s*, *a*) with random values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the state *s*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract a policy from *Q*(*s*, *a*) and select an action *a* to perform in the
    state *s*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the action *a*, move to the new state ![](img/B15558_18_050.png), and
    observe the reward *r*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Q value to ![](img/B15558_18_051.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update ![](img/B15558_18_052.png) (update the next state ![](img/B15558_18_053.png)
    to the current state *s*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *s* is not a terminal state, repeat *steps 1* to *5*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep Q Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for deep Q learning is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_10_037.png) with random
    values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_09_061.png) by copying
    the main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, perform *step 5*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the state *s* and select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select random action *a*, and with probability
    1-epsilon, select the action as ![](img/B15558_09_072.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action and move to the next state ![](img/B15558_14_036.png)
    and obtain the reward *r*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the transition information in the replay buffer ![](img/B15558_09_075.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_259.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value, that is, ![](img/B15558_18_062.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss, ![](img/B15558_09_035.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients of the loss and update the main network parameter ![](img/B15558_12_330.png)
    using gradient descent, ![](img/B15558_18_065.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Freeze the target network parameter ![](img/B15558_18_066.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_054.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Double DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for double DQN is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_09_054.png) with random
    values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_09_059.png) by copying
    the main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_12_259.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *step 5*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the state *s* and select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select random action *a* with probability 1-epsilon,
    and select the action as ![](img/B15558_18_072.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action and move to the next state ![](img/B15558_18_073.png)
    and obtain the reward *r*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the transition information in the replay buffer ![](img/B15558_18_074.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_14_098.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value, that is, ![](img/B15558_18_076.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss, ![](img/B15558_09_035.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the gradients of the loss and update the main network parameter ![](img/B15558_09_087.png)
    using gradient descent: ![](img/B15558_18_079.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Freeze the target network parameter ![](img/B15558_09_061.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_10_066.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: REINFORCE Policy Gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for REINFORCE policy gradient is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the network parameter ![](img/B15558_15_152.png) with random values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate some *N* number of trajectories ![](img/B15558_10_058.png) following
    the policy ![](img/B15558_13_124.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return of the trajectory ![](img/B15558_18_085.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients ![](img/B15558_18_086.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the network parameter, ![](img/B15558_11_005.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *5* for several iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Policy Gradient with Reward-To-Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for policy gradient with reward-to-go is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the network parameter ![](img/B15558_09_098.png) with random values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate some *N* number of trajectories ![](img/B15558_10_058.png) following
    the policy ![](img/B15558_10_111.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return (reward-to-go) *R*[t]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients ![](img/B15558_10_128.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the network parameter: ![](img/B15558_18_092.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *5* for several iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: REINFORCE with Baseline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for REINFORCE with baseline is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the policy network parameter ![](img/B15558_09_098.png) and value
    network parameter ![](img/B15558_13_234.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate some *N* number of trajectories ![](img/B15558_10_058.png) following
    the policy ![](img/B15558_13_124.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return (reward-to-go) *R*[t]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the policy gradient, ![](img/B15558_10_163.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy network parameter ![](img/B15558_09_098.png) using gradient
    ascent, ![](img/B15558_18_099.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the mean squared error of the value network, ![](img/B15558_18_100.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value network parameter ![](img/B15558_12_213.png) using gradient
    descent, ![](img/B15558_11_013.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *7* for several iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantage Actor Critic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for the advantage actor critic method is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the actor network parameter ![](img/B15558_10_037.png) and critic
    network parameter ![](img/B15558_10_148.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *step 3*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for, *t* = 0, . . ., *T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an action using the policy ![](img/B15558_11_017.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the action *a*[t] in the state *s*[t], and observe the reward *r* and move
    to the next state ![](img/B15558_18_106.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the policy gradients: ![](img/B15558_11_009.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the actor network parameter ![](img/B15558_09_054.png) using gradient
    ascent: ![](img/B15558_11_005.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the loss of the critic network: ![](img/B15558_18_110.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute gradients ![](img/B15558_10_093.png) and update the critic network
    parameter ![](img/B15558_12_371.png) using gradient descent: ![](img/B15558_11_013.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Asynchronous Advantage Actor-Critic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps involved in **Advantage Actor-Critic** (**A3C**) are given below:'
  prefs: []
  type: TYPE_NORMAL
- en: The worker agent interacts with its own copies of the environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each worker follows a different policy and collects the experience
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the worker agents compute the loss of the actor and critic networks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the loss, they calculates the gradients of the loss and sends
    those gradients to the global agent asynchronously
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The global agent updates its parameter with the gradients received from the
    worker agents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, the updated parameter from the global agent will be sent to the worker
    agents periodically
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep Deterministic Policy Gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for **Deep Deterministic Policy Gradient** (**DDPG**) is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main critic network parameter ![](img/B15558_09_118.png) and
    main actor network parameter ![](img/B15558_13_234.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target critic network parameter ![](img/B15558_14_246.png) by
    just copying the main critic network parameter ![](img/B15558_09_123.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target actor network parameter ![](img/B15558_18_117.png) by
    just copying the main actor network parameter ![](img/B15558_13_234.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *steps 6* to *7*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize an Ornstein-Uhlenbeck random process ![](img/B15558_18_120.png) for
    action space exploration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select action *a* based on the policy ![](img/B15558_18_121.png) and exploration
    noise, that is, ![](img/B15558_12_061.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_14_105.png)
    and get the reward *r*, and store this transition information in the replay buffer
    ![](img/B15558_12_259.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_09_075.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value of the critic, that is, ![](img/B15558_12_095.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network ![](img/B15558_12_047.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the loss ![](img/B15558_10_028.png) and update the critic
    network parameter using gradient descent, ![](img/B15558_18_129.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the actor network ![](img/B15558_10_093.png) and update
    the actor network parameter using gradient ascent, ![](img/B15558_18_131.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic and target actor network parameters, ![](img/B15558_18_132.png)
    and ![](img/B15558_18_133.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Twin Delayed DDPG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for **Twin Delayed DDPG** (**TD3**) is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize two main critic networks parameters, ![](img/B15558_12_211.png) and
    ![](img/B15558_12_217.png), and the main actor network parameter ![](img/B15558_12_213.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize two target critic networks parameters, ![](img/B15558_12_214.png)
    and ![](img/B15558_12_320.png), by copying the main critic network parameters
    ![](img/B15558_18_139.png) and ![](img/B15558_12_217.png), respectively
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target actor network parameter ![](img/B15558_18_141.png) by
    copying the main actor network parameter ![](img/B15558_12_213.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_18_143.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *step 6*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select action *a* based on the policy ![](img/B15558_18_121.png) and with exploration
    noise ![](img/B15558_18_145.png), that is, ![](img/B15558_12_224.png) where, ![](img/B15558_18_147.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_18_043.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_124.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_374.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action ![](img/B15558_18_151.png) for computing the target value
    ![](img/B15558_18_152.png) where ![](img/B15558_12_269.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value of the critic, that is, ![](img/B15558_12_230.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network ![](img/B15558_12_227.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients of the loss ![](img/B15558_12_234.png) and minimize the
    loss using gradient descent, ![](img/B15558_12_235.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod *d* =0, then:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png) and
    update the actor network parameter using gradient ascent, ![](img/B15558_18_131.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic networks parameter and target actor network parameter
    as ![](img/B15558_18_132.png) and ![](img/B15558_18_161.png), respectively
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Soft Actor-Critic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for **Soft Actor-Critic** (**SAC**) is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main value network parameter ![](img/B15558_12_302.png), the
    Q network parameters ![](img/B15558_12_216.png) and ![](img/B15558_12_206.png),
    and the actor network parameter ![](img/B15558_14_245.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target value network ![](img/B15558_12_364.png) by just copying
    the main value network parameter ![](img/B15558_12_302.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *step 5*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each step in the episode, that is, for *t* = 0, . . ., *T* – 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select action *a* based on the policy ![](img/B15558_18_169.png), that is, ![](img/B15558_18_170.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_09_126.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_124.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute target state value ![](img/B15558_12_380.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of value network ![](img/B15558_12_323.png) and update the
    parameter using gradient descent, ![](img/B15558_18_175.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target Q value ![](img/B15558_18_176.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the Q networks ![](img/B15558_12_383.png) and update the
    parameter using gradient descent, ![](img/B15558_12_386.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute gradients of the actor objective function,![](img/B15558_18_179.png)
    and update the parameter using gradient ascent, ![](img/B15558_18_180.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target value network parameter, ![](img/B15558_18_181.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Trust Region Policy Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for **Trust Region Policy Optimization** (**TRPO**) is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the policy network parameter ![](img/B15558_09_098.png) and value
    network parameter ![](img/B15558_18_183.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate *N* number of trajectories ![](img/B15558_10_058.png) following the
    policy ![](img/B15558_10_111.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return (reward-to-go) *R*[t]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the advantage value *A*[t]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the policy gradients ![](img/B15558_13_232.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute ![](img/B15558_13_238.png) using the conjugate gradient method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy network parameter ![](img/B15558_09_098.png) using the update
    rule ![](img/B15558_13_240.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the mean squared error of the value network, ![](img/B15558_18_100.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value network parameter ![](img/B15558_13_289.png) using gradient
    descent, ![](img/B15558_10_150.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *9* for several iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PPO-Clipped
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for the PPO-clipped method is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the policy network parameter ![](img/B15558_09_098.png) and value
    network parameter ![](img/B15558_10_152.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect some *N* number of trajectories ![](img/B15558_10_058.png) following
    the policy ![](img/B15558_10_036.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return (reward-to-go) *R*[t]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_09_043.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy network parameter ![](img/B15558_09_098.png) using gradient
    ascent, ![](img/B15558_13_316.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the mean squared error of the value network, ![](img/B15558_10_166.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the value network ![](img/B15558_10_093.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value network parameter ![](img/B15558_13_289.png) using gradient
    descent, ![](img/B15558_10_150.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *8* for several iterations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PPO-Penalty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for the PPO-penalty method is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the policy network parameter ![](img/B15558_09_098.png) and value
    network parameter ![](img/B15558_14_249.png) and initialize the penalty coefficient
    ![](img/B15558_13_309.png) and the target KL divergence ![](img/B15558_18_207.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For iterations ![](img/B15558_18_208.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect some *N* number of trajectories following the policy ![](img/B15558_13_124.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return (reward-to-go) *R*[t]
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute ![](img/B15558_18_210.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_18_211.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy network parameter ![](img/B15558_18_212.png) using gradient
    ascent, ![](img/B15558_13_286.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *d* is greater than or equal to ![](img/B15558_18_214.png), then we set ![](img/B15558_18_215.png);
    if *d* is lesser than or equal to ![](img/B15558_18_216.png), then we set, ![](img/B15558_18_217.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the mean squared error of the value network: ![](img/B15558_10_166.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients of the value network ![](img/B15558_10_093.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value network parameter ![](img/B15558_13_234.png) using gradient
    descent, ![](img/B15558_10_150.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Categorical DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for a categorical DQN is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_09_098.png) with random
    values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_12_025.png) by copying
    the main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_15_027.png), the number of atoms,
    and also ![](img/B15558_18_226.png) and ![](img/B15558_18_227.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, perform *step 5*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the state *s* and support values to the main categorical DQN parameterized
    by ![](img/B15558_09_098.png), and get the probability value for each support
    value. Then compute the Q value as ![](img/B15558_14_103.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the Q value, select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select a random action *a* and with probability
    1-epsilon, select an action as ![](img/B15558_18_230.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action and move to the next state ![](img/B15558_12_376.png)
    and obtain the reward *r.*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the transition information in the replay buffer ![](img/B15558_09_088.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a transition from the replay buffer ![](img/B15558_09_124.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the next state ![](img/B15558_12_376.png) and support values to the target
    categorical DQN parameterized by ![](img/B15558_12_025.png) and get the probability
    value for each support. Then compute the value as ![](img/B15558_18_236.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the Q value, we select the best action in the state ![](img/B15558_12_376.png)
    as the one that has the maximum Q value ![](img/B15558_14_112.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the array *m* with zero values with its shape as the number of support.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *j* in the range of the number of support values:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the target support value: ![](img/B15558_18_239.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the value of *b*: ![](img/B15558_18_240.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the lower bound and upper bound: ![](img/B15558_14_047.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_116.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_117.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the cross entropy loss ![](img/B15558_14_131.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the loss using gradient descent and update the parameter of the main
    network
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Freeze the target network parameter ![](img/B15558_18_066.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Distributed Distributional DDPG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Distributed Distributional Deep Deterministic Policy Gradient** (**D4PG**)
    algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the critic network parameter ![](img/B15558_09_054.png) and the actor
    network parameter ![](img/B15558_18_248.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target critic network parameter ![](img/B15558_18_066.png) and
    the target actor network parameter ![](img/B15558_18_250.png) by copying from
    ![](img/B15558_09_098.png) and ![](img/B15558_12_283.png), respectively
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch *L* number of actors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *step 6*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_09_088.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value distribution of the critic, that is, ![](img/B15558_18_255.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network and calculate the gradient as ![](img/B15558_14_229.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After computing gradients, update the critic network parameter using gradient
    descent: ![](img/B15558_12_052.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the actor network ![](img/B15558_11_014.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the actor network parameter by gradient ascent: ![](img/B15558_12_068.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod ![](img/B15558_18_260.png) then:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic and target actor network parameters using soft replacement
    as ![](img/B15558_18_261.png) and ![](img/B15558_12_077.png), respectively
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If *t* mod ![](img/B15558_14_261.png) then:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Replicate the network weights to the actors
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We perform the following steps in the actor network:'
  prefs: []
  type: TYPE_NORMAL
- en: Select action *a* based on the policy ![](img/B15558_14_262.png) and exploration
    noise, that is, ![](img/B15558_18_265.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_016.png)
    and get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_12_259.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 1* and *2* until the learner finishes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DAgger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for DAgger is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize an empty dataset ![](img/B15558_18_268.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a policy ![](img/B15558_18_269.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For iterations *i* = 1 to *N*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a policy ![](img/B15558_18_270.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a trajectory using the policy ![](img/B15558_15_086.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dataset ![](img/B15558_18_272.png) by collecting states visited by
    the policy ![](img/B15558_18_273.png) and the actions of those states provided
    by the expert ![](img/B15558_15_053.png). Thus, ![](img/B15558_15_090.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate the dataset as ![](img/B15558_18_276.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a classifier on the updated dataset ![](img/B15558_12_259.png) and extract
    a new policy ![](img/B15558_18_278.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep Q learning from demonstrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for **Deep Q Learning from Demonstrations** (**DQfD**) is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_18_280.png) by copying
    the main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_09_124.png) with the expert demonstrations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *d*, the number of time steps we want to delay updating the target network
    parameter
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pre-training phase**: For steps *t* = 1, 2, . . ., *T*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a minibatch of experience from the replay buffer ![](img/B15558_12_088.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss *J*(*Q*)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameter of the network using gradient descent
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod *d* = 0:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target network parameter ![](img/B15558_18_284.png) by copying the
    main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Training phase**: For steps *t* =1, 2, . . ., *T*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an action
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action and move to the next state, observe the reward,
    and store this transition information in the replay buffer ![](img/B15558_09_088.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a minibatch of experience from the replay buffer ![](img/B15558_12_259.png)
    with prioritization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss *J*(*Q*)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameter of the network using gradient descent
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod *d* = 0:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target network parameter ![](img/B15558_18_066.png) by copying the
    main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: MaxEnt Inverse Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for maximum entropy inverse reinforcement learning is given as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the parameter ![](img/B15558_09_098.png) and gather the expert demonstrations
    ![](img/B15558_15_027.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *N* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the reward function ![](img/B15558_18_292.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the policy using the value iteration with the reward function obtained
    in the previous step
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the state visitation frequency ![](img/B15558_18_293.png) using the
    policy obtained in the previous step
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient with respect to ![](img/B15558_09_098.png), that is, ![](img/B15558_15_176.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value of ![](img/B15558_09_106.png) as ![](img/B15558_18_297.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: MAML in Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm for MAML in the reinforcement learning setting is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Say we have a model *f* parameterized by a parameter ![](img/B15558_09_118.png)
    and we have a distribution over tasks *p*(*T*). First, we randomly initialize
    the model parameter ![](img/B15558_09_054.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a batch of tasks *T*[i] from a distribution of tasks, that is, *T*[i]
    *~ p(T).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each task *T*[i]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sample *k* trajectories using ![](img/B15558_17_022.png) and prepare the training
    dataset: ![](img/B15558_17_128.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model ![](img/B15558_18_302.png) on the training dataset ![](img/B15558_17_089.png)
    and compute the loss
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the loss using gradient descent and get the optimal parameter ![](img/B15558_17_056.png)
    as ![](img/B15558_18_305.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sample *k* trajectories using ![](img/B15558_17_041.png) and prepare the test
    dataset: ![](img/B15558_17_115.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we minimize the loss on the test dataset ![](img/B15558_18_308.png). Parameterize
    the model *f* with the optimal parameter ![](img/B15558_18_309.png) calculated
    in the previous step and compute the loss ![](img/B15558_17_137.png). Calculate
    the gradients of the loss and update our randomly initialized parameter ![](img/B15558_09_098.png)
    using our test (meta-training) dataset: ![](img/B15558_17_139.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *4* for several iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
