["```\nimport numpy as np\nimport pandas as pd\ntry:\n    from sklearn.impute import IterativeImputer\nexcept:\n    from sklearn.experimental import enable_iterative_imputer\n    from sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import Pipeline \n```", "```\nexample = pd.DataFrame([[1, 2, 3, np.nan], [1, 3, np.nan, 4], [1, 2, 2, 2]], columns = ['a', 'b', 'c', 'd']) \n```", "```\ndef assemble_numeric_pipeline(variance_threshold=0.0, \n                              imputer='mean', \n                              multivariate_imputer=False, \n                              add_indicator=True,\n                              quantile_transformer='normal',\n                              scaler=True):\n    numeric_pipeline = []\n    if variance_threshold is not None:\n        if isinstance(variance_threshold, float):\n            numeric_pipeline.append(('var_filter', \n                                    VarianceThreshold(threshold=variance_threshold)))\n        else:\n            numeric_pipeline.append(('var_filter',\n                                     VarianceThreshold()))\n    if imputer is not None:\n        if multivariate_imputer is True:\n            numeric_pipeline.append(('imputer', \n                                     IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=100, n_jobs=-2), \n                                                      initial_strategy=imputer,\n                                                      add_indicator=add_indicator)))\n        else:\n            numeric_pipeline.append(('imputer', \n                                     SimpleImputer(strategy=imputer, \n                                                   add_indicator=add_indicator)\n                                    )\n                                   )\n    if quantile_transformer is not None:\n        numeric_pipeline.append(('transformer',\n                                 QuantileTransformer(n_quantiles=100, \n                                                     output_distribution=quantile_transformer, \n                                                     random_state=42)\n                                )\n                               )\n    if scaler is not None:\n        numeric_pipeline.append(('scaler', \n                                 StandardScaler()\n                                )\n                               )\n    return Pipeline(steps=numeric_pipeline) \n```", "```\nnumeric_pipeline = assemble_numeric_pipeline(variance_threshold=0.0, \n                              imputer='mean', \n                              multivariate_imputer=False, \n                              add_indicator=True,\n                              quantile_transformer='normal',\n                              scaler=True) \n```", "```\nnumeric_pipeline.fit(example)\nnp.round(numeric_pipeline.transform(example), 3) \n```", "```\narray([[-0.707,  1.225, -0\\.   , -0.707,  1.414],\n       [ 1.414, -0\\.   ,  1.225,  1.414, -0.707],\n       [-0.707, -1.225, -1.225, -0.707, -0.707]]) \n```", "```\ndef derive_numeric_columns(df, pipeline):\n    columns = df.columns\n    if 'var_filter' in pipeline.named_steps:\n        threshold = pipeline.named_steps.var_filter.threshold\n        columns = columns[pipeline.named_steps.var_filter.variances_>threshold]\n    if 'imputer' in pipeline.named_steps:\n        missing_cols = pipeline.named_steps.imputer.indicator_.features_\n        if len(missing_cols) > 0:\n            columns = columns.append(columns[missing_cols] + '_missing')\n    return columns \n```", "```\nderive_numeric_columns(example, numeric_pipeline) \n```", "```\nIndex(['b', 'c', 'd', 'c_missing', 'd_missing'], dtype='object') \n```", "```\nfrom sklearn.base import BaseEstimator, TransformerMixin \n```", "```\nexample = pd.DataFrame({'date_1': ['04/12/2018', '05/12/2019',  \n                                   '07/12/2020'],\n                        'date_2': ['12/5/2018', '15/5/2015', \n                                   '18/5/2016'],\n                        'date_3': ['25/8/2019', '28/8/2018', \n                                   '29/8/2017']}) \n```", "```\nclass DateProcessor(BaseEstimator, TransformerMixin):\n    def __init__(self, date_format='%d/%m/%Y', hours_secs=False):\n        self.format = date_format\n        self.columns = None\n        self.time_transformations = [\n            ('day_sin', lambda x: np.sin(2*np.pi*x.dt.day/31)),\n            ('day_cos', lambda x: np.cos(2*np.pi*x.dt.day/31)),\n            ('dayofweek_sin', \n                      lambda x: np.sin(2*np.pi*x.dt.dayofweek/6)),\n            ('dayofweek_cos', \n                      lambda x: np.cos(2*np.pi*x.dt.dayofweek/6)),\n            ('month_sin', \n                      lambda x: np.sin(2*np.pi*x.dt.month/12)),\n            ('month_cos', \n                      lambda x: np.cos(2*np.pi*x.dt.month/12)),\n            ('year', \n                      lambda x: (x.dt.year - x.dt.year.min()                          ) / (x.dt.year.max() - x.dt.year.min()))\n        ]\n        if hours_secs:\n            self.time_transformations = [\n                ('hour_sin', \n                      lambda x: np.sin(2*np.pi*x.dt.hour/23)),\n                ('hour_cos', \n                      lambda x: np.cos(2*np.pi*x.dt.hour/23)),\n                ('minute_sin', \n                      lambda x: np.sin(2*np.pi*x.dt.minute/59)),\n                ('minute_cos', \n                      lambda x: np.cos(2*np.pi*x.dt.minute/59))\n            ] + self.time_transformations\n\n    def fit(self, X, y=None, **fit_params):\n        self.columns = self.transform(X.iloc[0:1,:]).columns\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        transformed = list()\n        for col in X.columns:\n            time_column = pd.to_datetime(X[col],\n                                   format=self.format)\n            for label, func in self.time_transformations:\n                transformed.append(func(time_column))\n                transformed[-1].name += '_' + label\n        transformed = pd.concat(transformed, axis=1)\n        return transformed\n\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X) \n```", "```\nDateProcessor().fit_transform(example) \n```", "```\nimport matplotlib.pyplot as plt\nsin_time = np.array([[t, np.sin(2*np.pi*t/23)] for t in range(0, 24)])\ncos_time = np.array([[t, np.cos(2*np.pi*t/23)] for t in range(0, 24)])\nplt.plot(sin_time[:,0], sin_time[:,1], label='sin hour')\nplt.plot(cos_time[:,0], cos_time[:,1], label='cos hour')\nplt.axhline(y=0.0, linestyle='--', color='lightgray')\nplt.legend()\nplt.show() \n```", "```\nax = plt.subplot()\nax.set_aspect('equal')\nax.set_xlabel('sin hour')\nax.set_ylabel('cos hour')\nplt.scatter(sin_time[:,1], cos_time[:,1])\nplt.show() \n```", "```\nfrom sklearn.preprocessing import OneHotEncoder\nexample = pd.DataFrame([['car', 1234], ['house', 6543], \n                  ['tree', 3456]], columns=['object', 'code']) \n```", "```\nclass ToString(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None, **fit_params):\n        return self\n    def transform(self, X, y=None, **fit_params):\n        return X.astype(str)\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X)\n\ncategorical_pipeline = Pipeline(steps=[\n         ('string_converter', ToString()),\n         ('imputer', SimpleImputer(strategy='constant', \n                                   fill_value='missing')),\n         ('onehot', OneHotEncoder(handle_unknown='ignore'))]) \n```", "```\ncategorical_pipeline.fit_transform(example).todense() \n```", "```\ndef derive_ohe_columns(df, pipeline):\n    return [str(col) + '_' + str(lvl) \n         for col, lvls in zip(df.columns,      \n         pipeline.named_steps.onehot.categories_) for lvl in lvls] \n```", "```\nderive_ohe_columns(example, categorical_pipeline) \n```", "```\n['object_car',\n 'object_house',\n 'object_tree',\n 'code_1234',\n 'code_3456',\n 'code_6543'] \n```", "```\nfrom sklearn.preprocessing import OrdinalEncoder \n```", "```\nexample = pd.DataFrame([['first', 'very much'], \n                        ['second', 'very little'], \n                        ['third', 'average']],\n                       columns = ['rank', 'importance']) \n```", "```\noe = OrdinalEncoder(categories=[['first', 'second', 'third'],  \n                     ['very much', 'average', 'very little']])\ncategorical_pipeline = Pipeline(steps=[\n            ('string_converter', ToString()),\n            ('imputer', SimpleImputer(strategy='constant', \n                                      fill_value='missing')),\n            ('onehot', OneHotEncoder(handle_unknown='ignore'))]) \n```", "```\nnp.hstack((oe.fit_transform(example), categorical_pipeline.fit_transform(example).todense())) \n```", "```\nmatrix([[0., 0., 1., 0., 0., 0., 0., 1.],\n        [1., 2., 0., 1., 0., 0., 1., 0.],\n        [2., 1., 0., 0., 1., 1., 0., 0.]]) \n```", "```\nexample.columns.tolist() + derive_ohe_columns(example, categorical_pipeline) \n```", "```\n['rank',\n 'importance',\n 'rank_first',\n 'rank_second',\n 'rank_third',\n 'importance_average',\n 'importance_very little',\n 'importance_very much'] \n```", "```\nfrom sklearn.preprocessing import LabelEncoder \n```", "```\nimport string\nimport random\ndef random_id(length=8):\n    voc = string.ascii_lowercase + string.digits\n    return ''.join(random.choice(voc) for i in range(length))\nexample = pd.DataFrame({'high_cat_1': [random_id(length=2) \n                                       for i in range(500)], \n                        'high_cat_2': [random_id(length=3) \n                                       for i in range(500)], \n                        'high_cat_3': [random_id(length=4) \n                                       for i in range(500)]}) \n```", "```\nclass LEncoder(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        self.encoders = dict()\n        self.dictionary_size = list()\n        self.unk = -1\n\n    def fit(self, X, y=None, **fit_params):\n        for col in range(X.shape[1]):\n            le = LabelEncoder()\n            le.fit(X.iloc[:, col].fillna('_nan'))\n            le_dict = dict(zip(le.classes_, \n                               le.transform(le.classes_)))\n\n            if '_nan' not in le_dict:\n                max_value = max(le_dict.values())\n                le_dict['_nan'] = max_value\n\n            max_value = max(le_dict.values())\n            le_dict['_unk'] = max_value\n\n            self.unk = max_value\n            self.dictionary_size.append(len(le_dict))\n            col_name = X.columns[col]\n            self.encoders[col_name] = le_dict\n\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        output = list()\n        for col in range(X.shape[1]):\n            col_name = X.columns[col]\n            le_dict = self.encoders[col_name]\n            emb = X.iloc[:, col].fillna('_nan').apply(lambda x: \n                           le_dict.get(x, le_dict['_unk'])).values\n            output.append(pd.Series(emb, \n                                name=col_name).astype(np.int32))\n        return output\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X) \n```", "```\nle = LEncoder()\nle.fit_transform(example) \n```", "```\nle.dictionary_size \n```", "```\n[412, 497, 502] \n```", "```\nfrom sklearn.pipeline import FeatureUnion \n```", "```\nexample = pd.concat([\npd.DataFrame([[1, 2, 3, np.nan], [1, 3, np.nan, 4],[1, 2, 2, 2]], \n             columns = ['a', 'b', 'c', 'd']),\npd.DataFrame({'date_1': ['04/12/2018', '05/12/2019','07/12/2020'],\n              'date_2': ['12/5/2018', '15/5/2015', '18/5/2016'],\n              'date_3': ['25/8/2019', '28/8/2018', '29/8/2017']}),\npd.DataFrame([['first', 'very much'], ['second', 'very little'],   \n              ['third', 'average']], \n             columns = ['rank', 'importance']),\npd.DataFrame([['car', 1234], ['house', 6543], ['tree', 3456]], \n             columns=['object', 'code']),\npd.DataFrame({'high_cat_1': [random_id(length=2) \n                             for i in range(3)], \n              'high_cat_2': [random_id(length=3) \n                             for i in range(3)], \n              'high_cat_3': [random_id(length=4) \n                             for i in range(3)]})\n], axis=1) \n```", "```\nclass TabularTransformer(BaseEstimator, TransformerMixin):\n\n    def instantiate(self, param):\n        if isinstance(param, str):\n            return [param]\n        elif isinstance(param, list):\n            return param\n        else:\n            return None\n\n    def __init__(self, numeric=None, dates=None, \n                 ordinal=None, cat=None, highcat=None,\n                 variance_threshold=0.0, missing_imputer='mean',  \n                 use_multivariate_imputer=False,\n                 add_missing_indicator=True, \n                 quantile_transformer='normal', scaler=True,\n                 ordinal_categories='auto', \n                 date_format='%d/%m/%Y', hours_secs=False):\n\n        self.numeric = self.instantiate(numeric)\n        self.dates = self.instantiate(dates)\n        self.ordinal = self.instantiate(ordinal)\n        self.cat  = self.instantiate(cat)\n        self.highcat = self.instantiate(highcat)\n        self.columns = None\n        self.vocabulary = None \n```", "```\n self.numeric_process = assemble_numeric_pipeline(\n                    variance_threshold=variance_threshold, \n                    imputer=missing_imputer, \n                    multivariate_imputer=use_multivariate_imputer, \n                    add_indicator=add_missing_indicator,\n                    quantile_transformer=quantile_transformer,\n                    scaler=scaler) \n```", "```\n self.dates_process = DateProcessor(\n                   date_format=date_format, hours_secs=hours_secs) \n```", "```\n self.ordinal_process = FeatureUnion(\n                [('ordinal', \n                 OrdinalEncoder(categories=ordinal_categories)),\n                 ('categorial',   \n                 Pipeline(steps=[('string_converter', ToString()),\n                 ('imputer', \n                 SimpleImputer(strategy='constant', \n                               fill_value='missing')),\n                 ('onehot', \n                 OneHotEncoder(handle_unknown='ignore'))]))]) \n```", "```\n self.cat_process = Pipeline(steps=[\n              ('string_converter', ToString()),\n              ('imputer', SimpleImputer(strategy='constant', \n                                        fill_value='missing')),\n              ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n        self.highcat_process = LEncoder() \n```", "```\n def fit(self, X, y=None, **fit_params):\n        self.columns = list()\n        if self.numeric:\n            self.numeric_process.fit(X[self.numeric])\n            self.columns += derive_numeric_columns(\n                               X[self.numeric], \n                               self.numeric_process).to_list()\n        if self.dates:\n            self.dates_process.fit(X[self.dates])\n            self.columns += self.dates_process.columns.to_list()\n        if self.ordinal:\n            self.ordinal_process.fit(X[self.ordinal])\n            self.columns += self.ordinal + derive_ohe_columns(\n                      X[self.ordinal], \n                      self.ordinal_process.transformer_list[1][1])\n        if self.cat:\n            self.cat_process.fit(X[self.cat])\n            self.columns += derive_ohe_columns(X[self.cat], \n                                               self.cat_process)\n        if self.highcat:\n            self.highcat_process.fit(X[self.highcat])\n            self.vocabulary = dict(zip(self.highcat,  \n                            self.highcat_process.dictionary_size))\n            self.columns = [self.columns, self.highcat]\n        return self \n```", "```\n def transform(self, X, y=None, **fit_params):\n        flat_matrix = list()\n        if self.numeric:\n            flat_matrix.append(\n                   self.numeric_process.transform(X[self.numeric])\n                              .astype(np.float32))\n        if self.dates:\n            flat_matrix.append(\n                   self.dates_process.transform(X[self.dates])\n                              .values\n                              .astype(np.float32))\n        if self.ordinal:\n            flat_matrix.append(\n                   self.ordinal_process.transform(X[self.ordinal])\n                              .todense()\n                              .astype(np.float32))\n        if self.cat:\n            flat_matrix.append(\n                   self.cat_process.transform(X[self.cat])\n                              .todense()\n                              .astype(np.float32))\n        if self.highcat:\n            cat_vectors = self.highcat_process.transform(\n                                                  X[self.highcat])\n            if len(flat_matrix) > 0:\n                return [np.hstack(flat_matrix)] + cat_vectors\n            else:\n                return cat_vectors\n        else:\n            return np.hstack(flat_matrix) \n```", "```\n def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X) \n```", "```\nnumeric_vars = ['a', 'b', 'c', 'd']\ndate_vars = ['date_1', 'date_2', 'date_3']\nordinal_vars = ['rank', 'importance']\ncat_vars = ['object', 'code']\nhighcat_vars = ['high_cat_1', 'high_cat_2', 'high_cat_3']\ntt = TabularTransformer(numeric=numeric_vars, dates=date_vars, \n                        ordinal=ordinal_vars, cat=cat_vars, \n                        highcat=highcat_vars) \n```", "```\ninput_list = tt.fit_transform(example) \n```", "```\nprint([(item.shape, item.dtype) for item in input_list]) \n```", "```\n[((3, 40), dtype('float32')), ((3,), dtype('int32')), ((3,), dtype('int32')), ((3,), dtype('int32'))] \n```", "```\n{'high_cat_1': 5, 'high_cat_2': 5, 'high_cat_3': 5} \n```", "```\nfrom tensorflow.keras.utils import Sequence \n```", "```\nclass DataGenerator(Sequence):\n    def __init__(self, X, y,\n                 tabular_transformer=None,\n                 batch_size=32, \n                 shuffle=False,\n                 dict_output=False\n                 ):\n\n        self.X = X\n        self.y = y\n        self.tbt = tabular_transformer\n        self.tabular_transformer = tabular_transformer\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.dict_output = dict_output\n        self.indexes = self._build_index()\n        self.on_epoch_end()\n        self.item = 0\n\n    def _build_index(self):\n        return np.arange(len(self.y))\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __len__(self):\n        return int(len(self.indexes) / self.batch_size) + 1\n\n    def __iter__(self):\n        for i in range(self.__len__()):\n            self.item = i\n            yield self.__getitem__(index=i)\n\n        self.item = 0\n\n    def __next__(self):\n        return self.__getitem__(index=self.item)\n\n    def __call__(self):\n        return self.__iter__()\n\n    def __data_generation(self, selection):\n        if self.tbt is not None:\n            if self.dict_output:\n                dct = {'input_'+str(j) : arr for j, \n                        arr in enumerate(\n                  self.tbt.transform(self.X.iloc[selection, :]))}\n                return dct, self.y[selection]\n            else:\n                return self.tbt.transform(\n                     self.X.iloc[selection, :]), self.y[selection]\n        else:\n            return self.X.iloc[selection, :], self.y[selection]\n\n    def __getitem__(self, index):\n        indexes = self.indexes[\n                  index*self.batch_size:(index+1)*self.batch_size]\n        samples, labels = self.__data_generation(indexes)\n        return samples, labels, [None] \n```", "```\nfrom tensorflow import keras as keras\nimport numpy as np\nimport matplotlib.pyplot as plt \n```", "```\ndef gelu(x):\n    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * \n                        (x + 0.044715 * tf.pow(x, 3))))\nkeras.utils.get_custom_objects().update(\n                         {'gelu': keras.layers.Activation(gelu)})\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\nkeras.utils.get_custom_objects().update(\n                         {'mish': keras.layers.Activation(mish)}) \n```", "```\ngelu_vals = list()\nmish_vals = list()\nabscissa = np.arange(-4, 1, 0.1)\nfor val in abscissa:\n    gelu_vals.append(gelu(tf.cast(val, tf.float32)).numpy())\n    mish_vals.append(mish(tf.cast(val, tf.float32)).numpy())\n\nplt.plot(abscissa, gelu_vals, label='gelu')\nplt.plot(abscissa, mish_vals, label='mish')\nplt.axvline(x=0.0, linestyle='--', color='darkgray')\nplt.axhline(y=0.0, linestyle='--', color='darkgray')\nplt.legend()\nplt.show() \n```", "```\nimport tensorflow as tf\nimport tensorflow.keras as keras \n```", "```\ntf.compat.v1.disable_eager_execution() \n```", "```\nfrom catboost.datasets import amazon\nX, Xt = amazon()\ny = X[\"ACTION\"].apply(lambda x: 1 if x == 1 else 0).values\nX.drop([\"ACTION\"], axis=1, inplace=True) \n```", "```\ndef dnn(categorical_variables, categorical_counts,\n        feature_selection_dropout=0.2, categorical_dropout=0.1,\n        first_dense = 256, second_dense = 256, \n        dense_dropout = 0.2, \n        activation_type=gelu):\n\n    categorical_inputs = []\n    categorical_embeddings = []\n\n    for category in categorical_variables:\n        categorical_inputs.append(keras.layers.Input(\n                 shape=[1], name=category))\n        category_counts = categorical_counts[category]\n        categorical_embeddings.append(\n            keras.layers.Embedding(category_counts+1, \n                      int(np.log1p(category_counts)+1), \n                      name = category +  \n                              \"_embed\")(categorical_inputs[-1]))\n\n    def flatten_dropout(x, categorical_dropout):\n        return keras.layers.Flatten()(\n            keras.layers.SpatialDropout1D(categorical_dropout)(x))\n\n    categorical_logits = [flatten_dropout(cat_emb, \n                                          categorical_dropout) \n                          for cat_emb in categorical_embeddings]\n    categorical_concat = keras.layers.Concatenate(\n                  name = \"categorical_concat\")(categorical_logits)\n    x = keras.layers.Dense(first_dense,  \n                   activation=activation_type)(categorical_concat)\n    x = keras.layers.Dropout(dense_dropout)(x)  \n    x = keras.layers.Dense(second_dense, \n                   activation=activation_type)(x)\n    x = keras.layers.Dropout(dense_dropout)(x)\n    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(categorical_inputs, output)\n\n    return model \n```", "```\nfrom sklearn.metrics import average_precision_score, roc_auc_score\ndef mAP(y_true, y_pred):\n    return tf.py_function(average_precision_score, \n                          (y_true, y_pred), tf.double)\ndef auc(y_true, y_pred):\n    try:\n        return tf.py_function(roc_auc_score, \n                              (y_true, y_pred), tf.double)\n    except: \n        return 0.5\ndef compile_model(model, loss, metrics, optimizer):\n    model.compile(loss=loss, metrics=metrics, optimizer=optimizer)\n    return model\ndef plot_keras_history(history, measures):\n    \"\"\"\n    history: Keras training history\n    measures = list of names of measures\n    \"\"\"\n    rows = len(measures) // 2 + len(measures) % 2\n    fig, panels = plt.subplots(rows, 2, figsize=(15, 5))\n    plt.subplots_adjust(top = 0.99, bottom=0.01, \n                        hspace=0.4, wspace=0.2)\n    try:\n        panels = [item for sublist in panels for item in sublist]\n    except:\n        pass\n    for k, measure in enumerate(measures):\n        panel = panels[k]\n        panel.set_title(measure + ' history')\n        panel.plot(history.epoch, history.history[measure], \n                   label=\"Train \"+measure)\n        panel.plot(history.epoch, history.history[\"val_\"+measure], \n                   label=\"Validation \"+measure)\n        panel.set(xlabel='epochs', ylabel=measure)\n        panel.legend()\n\n    plt.show(fig) \n```", "```\nfrom sklearn.model_selection import StratifiedKFold\nSEED = 0\nFOLDS = 3\nBATCH_SIZE = 512\nskf = StratifiedKFold(n_splits=FOLDS, \n                      shuffle=True, \n                      random_state=SEED)\nroc_auc = list()\naverage_precision = list()\ncategorical_variables = X.columns.to_list()\nfor fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n\n    tt = TabularTransformer(highcat = categorical_variables)\n    tt.fit(X.iloc[train_idx])   \n    categorical_levels = tt.vocabulary\n\n    model = dnn(categorical_variables,\n                categorical_levels, \n                feature_selection_dropout=0.1,\n                categorical_dropout=0.1,\n                first_dense=64,\n                second_dense=64,\n                dense_dropout=0.1,\n                activation_type=mish)\n\n    model = compile_model(model, \n                          keras.losses.binary_crossentropy, \n                          [auc, mAP], \n                          tf.keras.optimizers.Adam(learning_rate=0.0001))\n\n    train_batch = DataGenerator(X.iloc[train_idx], \n                                y[train_idx],\n                                tabular_transformer=tt,\n                                batch_size=BATCH_SIZE,\n                                shuffle=True)\n\n    val_X, val_y = tt.transform(X.iloc[test_idx]), y[test_idx]\n\n    history = model.fit(train_batch,\n                        validation_data=(val_X, val_y),\n                        epochs=30,\n                        class_weight=[1.0, \n                                   (np.sum(y==0) / np.sum(y==1))],\n                        verbose=2)\n\n    print(\"\\nFOLD %i\" % fold)\n    plot_keras_history(history, measures=['auc', 'loss'])\n\n    preds = model.predict(val_X, verbose=0, \n                          batch_size=1024).flatten()\n    roc_auc.append(roc_auc_score(y_true=val_y, y_score=preds))\n    average_precision.append(average_precision_score(\n                                 y_true=val_y, y_score=preds))\n\nprint(f\"mean cv roc auc {np.mean(roc_auc):0.3f}\")\nprint(f\"mean cv ap {np.mean(average_precision):0.3f}\") \n```"]