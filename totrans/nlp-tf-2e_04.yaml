- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced Word Vector Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Chapter 3*, *Word2vec – Learning Word Embeddings*, we introduced you to
    Word2vec, the basics of learning word embeddings, and the two common Word2vec
    algorithms: skip-gram and CBOW. In this chapter, we will discuss several other
    word vector algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: GloVe – Global Vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELMo – Embeddings from Language Models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document classification with ELMo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, you will learn a word embedding learning technique known as **Global
    Vectors** (**GloVe**) and the specific advantages that GloVe has over skip-gram
    and CBOW.
  prefs: []
  type: TYPE_NORMAL
- en: You will also look at a recent approach for representing language called **Embeddings
    from Language Models** (**ELMo**). ELMo has an edge over other algorithms as it
    is able to disambiguate words, as well as capture semantics. Specifically, ELMo
    generates “contextualized” word representations, by using a given word along with
    its surrounding words, as opposed to treating word representations independently,
    as in skip-gram or CBOW.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will solve an exciting use-case of document classification using
    our newly founded ELMo vectors.
  prefs: []
  type: TYPE_NORMAL
- en: GloVe – Global Vectors representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main limitations of skip-gram and CBOW algorithms is that they can
    only capture local contextual information, as they only look at a fixed-length
    window around a word. There’s an important part of the puzzle missing here as
    these algorithms do not look at global statistics (by global statistics we mean
    a way for us to see all the occurrences of words in the context of another word
    in a text corpus).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we have already studied a structure that could contain this information
    in *Chapter 3*, *Word2vec – Learning Word Embeddings*: the co-occurrence matrix.
    Let’s refresh our memory on the co-occurrence matrix, as GloVe uses the statistics
    captured in the co-occurrence matrix to compute vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Co-occurrence matrices encode the context information of words, but they require
    maintaining a V × V matrix, where V is the size of the vocabulary. To understand
    the co-occurrence matrix, let’s take two example sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Jerry and Mary are friends*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jerry buys flowers for Mary*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we assume a context window of size 1, on each side of a chosen word, the
    co-occurrence matrix will look like the following (we only show the upper triangle
    of the matrix, as the matrix is symmetric):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Jerry** | **and** | **Mary** | **are** | **friends** | **buys** | **flowers**
    | **for** |'
  prefs: []
  type: TYPE_TB
- en: '| **Jerry** | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **and** |  | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Mary** |  |  | 0 | 1 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **are** |  |  |  | 0 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **friends** |  |  |  |  | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **buys** |  |  |  |  |  | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **flowers** |  |  |  |  |  |  | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **for** |  |  |  |  |  |  |  | 0 |'
  prefs: []
  type: TYPE_TB
- en: We can see that this matrix shows us how a word in a corpus is related to any
    other word, hence it contains global statistics about the corpus. That said, what
    are some of the advantages of having a co-occurrence matrix, as opposed to seeing
    just the local context?
  prefs: []
  type: TYPE_NORMAL
- en: It provides you with additional information about the characteristics of the
    words. For example, if you consider the sentence “the cat sat on the mat,” it
    is difficult to say if “the” is a special word that appears in the context of
    words such as “cat” or “mat.” However, if you have a large-enough corpus and a
    co-occurrence matrix, it’s very easy to see that “the” is a frequently occurring
    stop word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The co-occurrence matrix recognizes the repeating usages of contexts or phrases,
    whereas in the local context this information is ignored. For example, in a large
    enough corpus, “New York” will be a clear winner, showing that the two words appear
    in the same context many times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to keep in mind that Word2vec algorithms use various techniques
    to approximately inject some word co-occurrence patterns, while learning word
    vectors. For example, the sub-sampling technique we used in the previous chapter
    (i.e. sampling lower-frequency words more) helps to detect and avoid stop words.
    But they introduce additional hyperparameters and are not as informative as the
    co-occurrence matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Using global statistics to come up with word representations is not a new concept.
    An algorithm known as **Latent Semantic Analysis** (**LSA**) has been using global
    statistics in its approach.
  prefs: []
  type: TYPE_NORMAL
- en: LSA is used as a document analysis technique that maps words in the documents
    to something known as a **concept**, a common pattern of words that appears in
    a document. Global matrix factorization-based methods efficiently exploit the
    global statistics of a corpus (for example, co-occurrence of words in a global
    scope), but have been shown to perform poorly at word analogy tasks. On the other
    hand, context window-based methods have been shown to perform well at word analogy
    tasks, but do not utilize global statistics of the corpus, leaving space for improvement.
    GloVe attempts to get the best of both worlds—an approach that efficiently leverages
    global corpus statistics while optimizing the learning model in a context window-based
    manner similar to skip-gram or CBOW.
  prefs: []
  type: TYPE_NORMAL
- en: 'GloVe, a new technique for learning word embeddings was introduced in the paper
    “GloVe: Global Vectors for Word Representation” by Pennington et al. ([https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)).
    GloVe attempts to bridge the gap of missing global co-occurrence information in
    Word2vec algorithms. The main contribution of GloVe is a new cost function (or
    an objective function) that uses the valuable statistics available in the co-occurrence
    matrix. Let’s first understand the motivation behind the GloVe method.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GloVe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before looking at the implementation details of GloVe, let’s take time to understand
    the concepts governing the computations in GloVe. To do so, let’s consider an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider word *i*=Ice and *j*=Steam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define an arbitrary probe word *k*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define ![](img/B14070_04_001.png) to be the probability of words *i* and *k*
    occurring close to each other, and ![](img/B14070_04_002.png) to be the words
    *j* and *k* occurring together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at how the ![](img/B14070_04_003.png) entity behaves with different
    values for *k*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For *k* = “Solid” , it is highly likely to appear with *i*, thus, ![](img/B14070_04_001.png)
    will be high. However, *k* would not often appear along with *j* causing a low
    ![](img/B14070_04_002.png). Therefore, we get the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, for *k* = “gas”, it is unlikely to appear in the close proximity of *i*
    and therefore will have a low ![](img/B14070_04_001.png); however, since *k* highly
    correlates with *j*, the value of ![](img/B14070_04_002.png) will be high. This
    leads to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, for words such as *k* = “water”, which has a strong relationship with
    both *i* and *j*, or *k* = “Fashion”, which *i* and *j* both have minimal relevance
    to, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you assume we have learned sensible word embeddings for these words, these
    relationships can be visualized in a vectors space to understand why the ratio
    ![](img/B14070_04_003.png) behaves this way (see *Figure 4\. 1*). In the figure
    below, the solid arrow shows the distance between the words (*i, j*), whereas
    the dashed lines express the distance between the words, (*i, k*) and (*j, k*).
    These distances can then be associated with the probability values we discussed.
    For example, when *i* = “ice” and *k* = “solid”, we expect their vectors to have
    a shorter distance between them (i.e. more frequently co-occurring). Therefore,
    we can associate distance between (*i, k*) as the inverse of ![](img/B14070_04_001.png)
    (i.e. ![](img/B14070_04_013.png)) due to the definition of ![](img/B14070_04_001.png).
    This diagram shows how these distances vary as the probe word *k* changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: How the entities P_ik and P_jk behave as the probe word changes
    in proximity to the words i and j'
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be seen that the ![](img/B14070_04_003.png) entity, which is calculated
    by measuring the frequency of two words appearing close to each other, behaves
    in different ways as the relationship between the three words changes. As a result,
    it becomes a good candidate for learning word vectors. Therefore, a good starting
    point for defining the loss function will be as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *F* is some function and *w* and ![](img/B14070_04_017.png) are two different
    embedding spaces we’ll be using. In other words, the words ![](img/B14070_04_018.png)
    and ![](img/B14070_04_019.png) are looked up from one embedding space, whereas
    the probe word ![](img/B14070_04_020.png) is looked up from another. From this
    point, the original paper goes through the derivation meticulously to reach the
    following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_021.png)'
  prefs: []
  type: TYPE_IMG
- en: We will not go through the derivation here, as that’s out of scope for this
    book. Rather we will use the derived loss function and implement the algorithm
    with TensorFlow. If you need a less mathematically dense explanation of how we
    can derive this cost function, please refer to the author-written article at [https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010).
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![](img/B14070_04_022.png) is defined as ![](img/B14070_04_023.png), if
    ![](img/B14070_04_024.png), else 1, where ![](img/B14070_04_025.png) is the frequency
    with which the word *j* appeared in the context of the word *i*. ![](img/B14070_04_034.png)
    is a hyperparameter we set. Remember that we defined two embedding spaces ![](img/B14070_04_026.png)
    and ![](img/B14070_04_017.png) in our loss function. ![](img/B14070_04_028.png)
    and ![](img/B14070_04_029.png) represent the word embedding and the bias embedding
    for the word *i* obtained from embedding space ![](img/B14070_04_026.png), respectively.
    And, ![](img/B14070_04_031.png) and ![](img/B14070_04_032.png) represent the word
    embedding and bias embedding for word *j* obtained from embedding space ![](img/B14070_04_017.png),
    respectively. Both these embeddings behave similarly except for the randomization
    at the initialization. At the evaluation phase, these two embeddings are added
    together, leading to improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing GloVe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will discuss the steps for implementing GloVe. The full
    code is available in the `ch4_glove.ipynb` exercise file located in the `ch4`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll define the hyperparameters as we did in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The hyperparameters you define here are the same hyperparameters we defined
    in the previous chapter. We have a batch size, embedding size, window size, the
    number of epochs, and, finally, a set of held-out validation word IDs that we
    will print the most similar words to.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then define the model. First, we will import a few things we will need
    down the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is going to have two input layers: `word_i` and `word_j`. They represent
    a batch of context words and a batch of target words (or a batch of positive skip-grams):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note how the shape is defined. The shape is defined as an empty tuple. This
    means the final shape of `word_i` and `word_j` would be `[None]`, meaning it will
    take a vector of an arbitrary number of elements as the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to define the embedding layers. There will be four embedding
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`embeddings_i` – The context embedding layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embeddings_j` – The target embedding layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b_i` – The context embedding bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b_j` – The target embedding bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code defines these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to compute the output. The output of this model will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, that’s a portion of our final loss function. We have all the
    right ingredients to compute this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: First we will use the `tensorflow.keras.layers.Dot` layer to compute the dot
    product batch-wise between the context embedding lookup (`embeddings_i`) and the
    target embedding lookup (`embeddings_j`). For example, the two inputs to the `Dot`
    layer will be of size `[batch size, embedding size]`. After the dot product, the
    output `ij_dot` will be `[batch size, 1]`, where `ij_dot[k]` will be the dot product
    between `embeddings_i[k, :]` and `embeddings_j[k, :]`. Then we simply add `b_i`
    and `b_j` (which has shape `[None, 1]`) element-wise to `ij_dot`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the model is defined as taking `word_i` and `word_j` as inputs and
    outputting `pred`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, we are going to do something quite important.
  prefs: []
  type: TYPE_NORMAL
- en: We have to devise a way to compute the complex loss function defined above,
    using various components/functionality available in a model. First let’s revisit
    the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_021.png)'
  prefs: []
  type: TYPE_IMG
- en: where,
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_023.png), if ![](img/B14070_04_024.png), else 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it looks complex, we can use already existing loss functions and other
    functionality to implement the GloVe loss. You can abstract this loss function
    into three components as shown in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: The breakdown of the GloVe loss function showing how predictions,
    targets, and weights interact with each other to compute the final loss'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, if sample weights are denoted by ![](img/B14070_04_039.png), predictions
    are denoted by ![](img/B14070_04_040.png), and true targets are denoted by ![](img/B14070_04_041.png),
    then we can write the loss as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is simply a weighted mean squared loss. Therefore, we will use `"mse"`
    as the loss for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We will later see how we can feed in sample weights to the model to complete
    the loss function. So far, we have defined different components of the GloVe algorithm
    and compiled the model. Next, we are going to have a look at how data can be generated
    to train the GloVe model.
  prefs: []
  type: TYPE_NORMAL
- en: Generating data for GloVe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset we will be using is the same as the dataset from the previous chapter.
    To recap, we will be using the BBC news articles dataset available at [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).
    It contains 2225 news articles belonging to 5 topics, business, entertainment,
    politics, sport, and tech, which were published on the BBC website between 2004
    and 2005.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now generate the data. We will be encapsulating the data generation in
    a function called `glove_data_generator()`. As the first step, let us write a
    function signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The function takes several arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`List[List[int]]`) – a list of a list of word IDs. This is the
    output generated by tokenizer’s `texts_to_sequences()` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` (`int`) – Window size for the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) – Batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`) – Vocabulary size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cooccurrence_matrix` (`scipy.sparse.lil_matrix`) – A sparse matrix containing
    co-occurrences of words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x_max` (`int`) – Hyperparameter used by GloVe to compute sample weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha` (`float`) – Hyperparameter used by GloVe to compute sample weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` – The random seed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It also has several outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: A batch of (target, context) word ID tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The corresponding ![](img/B14070_04_043.png) values for the (target, context)
    tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample weights (i.e. ![](img/B14070_04_044.png)) values for the (target, context)
    tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First we will shuffle the order of news articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create the sampling table, so that we can use sub-sampling to
    avoid over-sampling common words (e.g. stop words):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, for every sequence (i.e. list of word IDs) representing an article,
    we generate positive skip-grams. Note how we are keeping `negative_samples=0.0`
    as, unlike skip-gram or CBOW algorithms, GloVe does not rely on negative candidates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we first break down the skip-gram tuples into two lists, one containing
    targets and the other containing context words, and convert them to NumPy arrays
    subsequently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We then index the positions given by the (target, context) word pairs, from
    the co-occurrence matrix to retrieve the corresponding ![](img/B14070_04_025.png)
    values, where (*i,j*) represents a (target, context) pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we compute a corresponding ![](img/B14070_04_043.png) (denoted by `log_x_ij`)
    and ![](img/B14070_04_044.png) (denoted by `sample_weights`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If a code is not chosen, a random seed is set. Afterward, all of `context`,
    `targets`, `log_x_ij`, and `sample_weights` are shuffled while maintaining the
    correspondence of elements between the arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we iterate through batches of the data we created above. Each batch
    will consist of
  prefs: []
  type: TYPE_NORMAL
- en: A batch of (target, context) word ID tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The corresponding ![](img/B14070_04_043.png) values for the (target, context)
    tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample weights (i.e. ![](img/B14070_04_044.png)) values for the (target, context)
    tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in that order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data is ready to be pumped in, let’s discuss the final piece of
    the puzzle: training the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating GloVe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training the model is effortless, as we have all the components to train the
    model. As the first step, we will reuse the `ValidationCallback` we created in
    *Chapter 3*, *Word2vec – Learning Word Embeddings*. To recap, `ValidationCallback`
    is a Keras callback. Keras callbacks give you a way to execute some important
    operation(s) at the end of every training iteration, epoch, prediction step, etc.
    Here we are using the callback to perform a validation step at the end of every
    epoch. Our callback would take a list of word IDs intended as the validation words
    (held out in `valid_term_ids`), the model containing the embedding matrix, and
    a tokenizer to decode word IDs. Then it will compute the most similar top-k words
    for every word in the validation word set and print that as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get a sensible-looking output once the model has finished training.
    Here are some of the cherry-picked results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that words like “months,” “weeks,” and “years” are grouped together.
    Numbers like “5bn,” “8bn,” and “2bn” are grouped together as well. “Deutsche”
    is surrounded by “Austria’s” and “Austria.” Finally, we will save the embeddings
    to the disk. We will combine weights and the bias of each context and target vector
    space to a single array, where the last column of the array will represent the
    bias and save it to the disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We will save embeddings as pandas DataFrames. First we get all the words sorted
    by their IDs. We subtract 1 to discount the reserved word ID 0 as we’ll add that
    manually, in the following line. Note that, word ID 0 will not show up in `tokenizer.index_word`.
    Next we get the required layers by name (namely, `context_embedding`, `target_embedding`,
    `context_embedding_bias` and `target_embedding_bias`). Once we have the layers
    we can use the `get_weights()` function to retrieve weights.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at GloVe, another word embedding learning technique.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of GloVe over the Word2vec techniques discussed in *Chapter
    3*, *Word2vec – Learning Word Embeddings*, is that it pays attention to both global
    and local statistics of the corpus to learn embeddings. As GloVe is able to capture
    the global information about words, it tends to give better performance, especially
    when the corpus size increases. Another advantage is that, unlike in Word2vec
    techniques, GloVe does not approximate the cost function (for example, Word2vec
    using negative sampling), but calculates the true cost. This leads to better and
    easier optimization of the loss.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to look at one more word vector algorithm
    known as **Embeddings from Language Models** (**ELMo**).
  prefs: []
  type: TYPE_NORMAL
- en: ELMo – Taking ambiguities out of word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve looked at word embedding algorithms that can give only a unique
    representation of the words in the vocabulary. However, they will give a constant
    representation for a given word, no matter how many times you query. Why would
    this be a problem? Consider the following two phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I went to the bank to deposit some money*'
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '*I walked along the river bank*'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the word “bank” is used in two totally different contexts. If you use
    a vanilla word vector algorithm (e.g. skip-gram), you can only have one representation
    for the word “bank”, and it is probably going to be muddled between the concept
    of a financial institution and the concept of walkable edges along a river, depending
    on the references to this word found in the corpus it’s trained on. Therefore,
    it is more sensible to provide embeddings for a word while preserving and leveraging
    the context around it. This is exactly what ELMo is striving for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, ELMo takes in a sequence, as opposed to a single token, and provides
    contextualized representations for each token in the sequence. *Figure 4.3* depicts
    various components encompassing the model. The first thing to understand is that
    ELMo is a complicated beast! There are lots of neural network models orchestrating
    in ELMo to produce the output. Particularly, the model uses:'
  prefs: []
  type: TYPE_NORMAL
- en: A character embedding layer (an embedding vector for each character).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **convolutional neural network** (**CNN**) – a CNN consists of many convolutional
    layers followed by an optional fully connected classification layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A convolution layer takes in a sequence of inputs (e.g. sequence of characters
    in a word) and moves a window of weights over the input to generate a latent representation.
    We will discuss CNNs in detail in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Two bi-directional LSTM layers – an LSTM is a type of model that is used to
    process time-series data. Given a sequence of inputs (e.g. sequence of word vectors),
    an LSTM goes from one input to the other, on the time dimension, and produces
    an output at each position. Unlike fully connected networks, LSTMs have memory,
    meaning the output at the current position will be affected by what the LSTM has
    seen in the past. We will discuss LSTMs in detail in the coming chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specifics of these different components are outside the scope of this chapter.
    They will be discussed in detail in the coming chapters. Therefore, do not worry
    if you do not understand the exact mechanisms of the sub-components shown here
    (*Figure 4.3*).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Different components of the ELMo model. Token embeddings are generated
    using a type of neural network known as a CNN. These token embeddings are fed
    to an LSTM model (that can process time-series data). The output of the first
    LSTM model is fed to a second LSTM model to generate a latent contextualized representation
    for each token'
  prefs: []
  type: TYPE_NORMAL
- en: We can download a pretrained ELMo model from TensorFlow Hub ([https://tfhub.dev](https://tfhub.dev)).
    TF Hub is a repository for various pretrained models.
  prefs: []
  type: TYPE_NORMAL
- en: It hosts models for tasks such as image classification, text classification,
    text generation, etc. You can go to the site and browse various available models.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading ELMo from TensorFlow Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ELMo model we will be using is found at [https://tfhub.dev/google/elmo/3](https://tfhub.dev/google/elmo/3).
    It has been trained on a very large corpus of text to solve a task known as language
    modeling. In language modeling, we try to predict the next word given the previous
    sequence of tokens. We will learn more about language modeling in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before downloading the model, let’s set the following environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`TF_FORCE_GPU_ALLOW_GROWTH` allows TensorFlow to allocate GPU memory on-demand
    as opposed to allocating all GPU memory at once. `TFHUB_CACHE_DIR` sets the directory
    where the models will be downloaded. We will first import TensorFlow Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, as usual, we will clear any running TensorFlow sessions by running the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will download the ELMo model. You can employ two ways to download
    pretrained models from TF Hub and use them in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hub.load(<url>, **kwargs)` – Recommended way for downloading and using TensorFlow
    2-compatible models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hub.KerasLayer(<url>, **kwargs)` – This is a workaround for using TensorFlow
    1-based models in TensorFlow 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unfortunately, ELMo has not been ported to TensorFlow 2 yet. Therefore, we
    will use the `hub.KerasLayer()` as the workaround to load ELMo in TensorFlow 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are providing two arguments, `signature` and `signature_outputs_as_dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`signature` (`str`) – Can be `default` or `tokens`. The default signature accepts
    a list of strings, where each string will be converted to a list of tokens internally.
    The tokens signature takes in inputs as dictionary having two keys. Namely, `tokens`
    (a list of list of tokens. Each list of tokens is a single phrase/sentence and
    includes padding tokens to bring them to a fixed length) and “`sequence_len`"
    (the length of each list of tokens, to determine the padding length).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`signature_outputs_as_dict` (`bool`) – When set to `true`, it will return all
    the outputs defined in the provided signature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have understood the components of ELMo and downloaded it from TensorFlow
    Hub, let’s see how we can process input data for ELMo.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing inputs for ELMo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we will define a function that will convert a given list of strings to
    the format ELMo expects the inputs to be in. Remember that we set the signature
    of ELMo to be `tokens`. An example input to the signature `"tokens"` would look
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a moment to process what the input comprises. First it has the key
    `tokens`, which has a list of tokens. Each list of tokens can be thought of as
    a sentence. Note how padding is added to the end of the short sentence to match
    the length. This is important as, otherwise, the model will throw an error as
    it can’t convert arbitrary-length sequences to a tensor. Next we have `sequence_len`,
    which is a list of integers. Each integer specifies the true length of each sequence.
    Note how the second element says 3, to match the actual tokens present in the
    second sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a list of strings, we can write a function to do this transformation
    for us. That’s what the `format_text_for_elmo()` function will do for us. Let’s
    sink our teeth into the specifics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We first create two lists, `token_inputs` and `token_lengths`, to contain individual
    tokens and their respective lengths. Next we go through each string in `texts`,
    and get the individual tokens using the `tf.keras.preprocessing.text.text_to_word_sequence()`
    function. While doing so, we will calculate the maximum token length we have observed
    so far. After iterating through the sequences, we check if the maximum length
    inferred from the inputs is different to `max_len` (if specified). If so, we will
    use `max_len_inferred` as the maximum length. This is important, because if you
    do otherwise, you may unnecessarily lengthen the inputs by defining a large value
    for `max_len`. Not only that, the model will raise an error like the one below
    if you do so.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Once the proper maximum length is found, we will go through the sequences and
  prefs: []
  type: TYPE_NORMAL
- en: If it is longer than `max_len`, truncate the sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is shorter than `max_len`, add tokens until it reaches `max_len`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we will convert them to `tf.Tensor` objects using the `tf.constant`
    construct. For example, you can call this function with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We will now see how ELMo can be used to generate embeddings for the prepared
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Generating embeddings with ELMo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the input is prepared, generating embeddings is quite easy. First we will
    transform the inputs to the stipulated format of the ELMo layer. Here we are using
    some example titles from the BBC dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, simply pass the `elmo_inputs` to the `elmo_layer` as the input and get
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now print the results and their shapes with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the model returns 6 different outputs. Let’s go through them
    one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence_len` – The same input we provided containing the lengths of the sequences
    in the input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word_emb` – The token embeddings obtained via the CNN layer in the ELMo model.
    We got a vector of size 512 for all sequence positions (i.e. 6) and for all rows
    in the batch (i.e. 5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lstm_output1` – The contextualized representations of tokens obtained via
    the first LSTM layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lstm_output2` – The contextualized representations of tokens obtained via
    the second LSTM layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default` – The mean embedding vector obtained by averaging all of the `lstm_output1`
    and `lstm_output2` embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`elmo` – The weighted sum of all of `word_emb`, `lstm_output1`, and `lstm_output2`,
    where weights are a set of task-specific trainable parameters that will be jointly
    trained during the task-specific training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we are interested in here is the `default` output. That would give us a
    very good representation of what’s contained in the document.
  prefs: []
  type: TYPE_NORMAL
- en: '**Other word embedding techniques**'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the word embedding techniques we discussed here, there are a few
    notable widely used word embedding techniques. We will discuss a few of those
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '**FastText**'
  prefs: []
  type: TYPE_NORMAL
- en: FastText ([https://fasttext.cc/](https://fasttext.cc/)), introduced in the paper
    “Enriching Word Vectors with Subword Information” by Bojanowski et al. ([https://arxiv.org/pdf/1607.04606.pdf](https://arxiv.org/pdf/1607.04606.pdf)),
    introduces a technique where word embeddings are computed by considering the sub-components
    of a word. Specifically, they compute the word embedding as a summation of embeddings
    of *n*-grams of the word for several values of *n*. In the paper, they use *3*
    <= *n* <=*6*. For example, for the word “banana,” the tri-grams (*n*=3) would
    be `['ban', 'ana', 'nan', 'ana']`. This leads to robust embeddings that can withstand
    common problems of text, such as spelling mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Swivel embeddings**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Swivel embeddings, introduced by the paper “*Swivel: Improving Embeddings by
    Noticing What’s Missing*” by Shazeer et al. ([https://arxiv.org/pdf/1602.02215.pdf](https://arxiv.org/pdf/1602.02215.pdf)),
    tries to blend GloVe and skip-grams with negative sampling. One of the critical
    limitations of GloVe is that it only uses information about positive contexts.
    Therefore, the method is not penalized for trying to create similar vectors of
    words that have not been observed together. But the negative sampling used in
    skip-grams directly tackles this problem. The biggest innovation of Swivel is
    a loss function that incorporates unobserved word pairs. As an added benefit,
    it can also be trained in a distributed environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer models**'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are a type of model that has reimagined the way we think about
    NLP problems. The Transformer model was initially introduced in the paper “*Attention
    is all you need*” by Vaswani ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).
    This model has many different embeddings within it and, like ELMo, can generate
    an embedding per token by processing a sequence of text. We will talk about Transformer
    models in detail in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed all the bells and whistles required to confidently use the
    ELMo model. Next we will classify documents using ELMo, in which ELMo will generate
    document embeddings as inputs to a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Document classification with ELMo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Word2vec gives a very elegant way of learning numerical representations
    of words, learning word representations alone is not convincing enough to realize
    the power of word vectors in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are used as the feature representation of words for many tasks,
    such as image caption generation and machine translation. However, these tasks
    involve combining different learning models such as **Convolutional Neural Networks**
    (**CNNs**) and **Long Short-Term Memory** (**LSTM**) models or two LSTM models
    (the CNN and LSTM models will be discussed in more detail in later chapters).
    To understand a real-world usage of word embeddings let’s stick to a simpler task—document
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Document classification is one of the most popular tasks in NLP. Document classification
    is extremely useful for anyone who is handling massive collections of data such
    as those for news websites, publishers, and universities. Therefore, it is interesting
    to see how learning word vectors can be adapted to a real-world task such as document
    classification by means of embedding entire documents instead of words.
  prefs: []
  type: TYPE_NORMAL
- en: This exercise is available in the `Ch04-Advance-Word-Vectors` folder (`ch4_document_classification.ipynb`).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this task, we will use an already-organized set of text files. These are
    news articles from the BBC. Every document in this collection belongs to one of
    the following categories: *Business*, *Entertainment*, *Politics*, *Sports*, or
    *Technology*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a couple of brief snippets from the actual data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Business*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Japan narrowly escapes recession*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Japan’s economy teetered on the brink of a technical recession in the three
    months to September, figures show.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Revised figures indicated growth of just 0.1% - and a similar-sized contraction
    in the previous quarter. On an annual basis, the data suggests annual growth of
    just 0.2%,...*'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will download the data and load the data into memory. We will use
    the same `download_data()` function to download the data. Then we will slightly
    modify the `read_data()` function to not only return a list of articles, where
    each article is a string, but also to return a list of filenames, where each filename
    corresponds to the file the article was stored in. The filenames will subsequently
    help us to create the labels for our classification model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We will then create and fit a tokenizer on the data, as we have done before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As the next step, we will create labels. Since we are training a classification
    model, we need both inputs and labels. Our inputs will be document embeddings
    (we will see how to compute them soon), and the targets will be a label ID between
    0 and 4\. Each class we mentioned above (e.g. business, tech, etc.) will be assigned
    to a separate category. Since the filename includes the category as a folder,
    we can leverage the filename to generate a label ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the pandas library to create the labels. First we will convert
    the list of filenames to a pandas Series object using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'An example entry in this series could look like `data/bbc/tech/127.txt`. Next,
    we will split each item on the “/” character, which will return a list `[''data'',
    ''bbc'', ''tech'', ''127.txt'']`. We will also set `expand=True`. `expand=True`
    will transform our Series object to a DataFrame by turning each item in the list
    of tokens into a separate column of a DataFrame. In other words, our `pd.Series`
    object will become an `[N, 4]`-sized `pd.DataFrame` with one token in each column,
    where `N` is the number of files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the resulting data, we only care about the third column, which has the category
    of a given article (e.g. `tech`). Therefore, we will discard the rest of the data
    and only keep that column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will map the string label to an integer ID using the pandas `map()`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'What we did here can be written as just one line by chaining the sequence of
    commands to a single line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we move on to the next important step, i.e. splitting the data into
    train/test subsets. When training a supervised model, we generally need three
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: A training set – This is the dataset the model will be trained on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A validation set – This will be used during the training to monitor model performance
    (e.g. signs of overfitting).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A testing set – This will be not exposed to the model at any time during the
    model training. It will only be used after the model training to evaluate the
    model on unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this exercise, we will only use the training set and the testing set. This
    will help us to keep our conversation more focused on embeddings and keep the
    discussion about the downstream classification model simple. Here we will use
    67% of the data as training data and use 33% of data as testing data. Data will
    be split randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a training dataset to train the model and a test dataset to test
    it on unseen data. We will now see how we can generate document embeddings from
    token or word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Generating document embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first remind ourselves how we stored embeddings for skip-gram, CBOW, and
    GloVe algorithms. *Figure 4.4* depicts how these look in a `pd.DataFrame` object.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: A snapshot of the context embeddings of the skip-gram algorithm
    we saved to the disk. You can see below it says that it has 128 columns (i.e.
    the embedding size)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ELMo embeddings are an exception to this. Since ELMo generates contextualized
    representations for all tokens in a sequence, we have stored the mean embedding
    vectors resulting from averaging all the generated vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: A snapshot of ELMo vectors. ELMo vectors have 1024 elements'
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the document embeddings from skip-gram, CBOW, and GloVe embeddings,
    let us write the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `generate_document_embeddings()` function takes the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`texts` – A list of strings, where each string represents an article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filenames` – A list of filenames corresponding to the articles in `texts`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` – A tokenizer that can process `texts`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embeddings` – The embeddings as a `pd.DataFrame`, where each row represents
    a word vector, indexed by the corresponding token'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function first preprocesses the texts by converting the strings to sequences,
    and then back to a list of strings. This helps us to use the built-in preprocessing
    functionalities of the tokenizer to clean the text. Next, each preprocessed string
    is split by the space character to return a list of tokens. Then we index all
    the positions in the embeddings matrix that corresponds to all the tokens in the
    text. Finally, the mean vector is computed for the document by computing the mean
    of all the chosen embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we can load the embeddings from different algorithms (skip-gram,
    CBOW, and GloVe), and compute the document embeddings. Here we will only show
    the process for the skip-gram algorithm. But you can easily extend it to the other
    algorithms, as they have similar inputs and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now we will see how we can leverage the generated document embedding to train
    a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying documents with document embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be training a simple multi-class (or a multinomial) logistic regression
    classifier on this data. The logistic regression model will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: This diagram depicts the multinomial logistic regression model.
    The model takes in an embedding vector and outputs a probability distribution
    over different available classes'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a very simple model with a single layer, where the input is the embedding
    vector (e.g. a 128-element-long vector), and the output is a 5-node softmax layer
    that will output the likelihood of the input belonging to each category, as a
    probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be training several models, as opposed to a single run. This will give
    us a more consistent result on the performance of the model. To implement the
    model, we’ll be using a popular general-purpose machine learning library called
    scikit-learn ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)).
    In each run, a multi-class logistic regression classifier is created with the
    `sklearn.linear_model.LogisticRegression` object. Additionally, in each run:'
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained on the training inputs and targets
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model predicts the class (a value from 0 to 4) for each test input, where
    the class of an input is the one that has the maximum probability from all classes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model computes the test accuracy using the predicted classes and true classes
    of the test set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'By setting `multi_class=''multinomial''`, we are making sure it’s a multi-class
    logistic regression model (or a softmax classifier). This will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'When you follow the procedure for all the skip-gram, CBOW, GloVe, and ELMo
    algorithms, you will see a result similar to the following. This is a box plot
    diagram. However, as performance is quite similar between trials, you won’t see
    much variation present in the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Box plot interpreting performance on document classification for
    different models. We can see that ELMo is a clear-cut winner, where GloVe performs
    the worst'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that skip-gram achieves around 86% accuracy, followed closely by
    CBOW, which achieves on-par performance. Surprisingly GloVe achieves performance
    far below the skip-gram and CBOW, around 66% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: This could be pointing to a limitation of the GloVe loss function. Unlike, skip-gram
    and CBOW, which are considered both positive (observed) and negative (unobserved)
    target and context pairs, GloVe only focuses on observed pairs.
  prefs: []
  type: TYPE_NORMAL
- en: This could be hurting GloVe’s ability to generate effective representations
    of words. Finally, ELMo achieves the best, which is around 98% accuracy. But it
    is important to keep in mind that ELMo has been trained on a much larger dataset
    than the BBC dataset, thus it is not fair to compare ELMo with other models just
    on this number.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you learned how we can extend word embeddings turned to document
    embeddings and how these can be used in a downstream classifier model to classify
    documents. First, you learned about word embeddings using a selected algorithm
    (e.g. skip-gram, CBOW, and GloVe). Then we created document embeddings by averaging
    the word embeddings of all the words found in that document. This was the case
    for the skip-gram, CBOW, and GloVe algorithms. In the case of the ELMo algorithm,
    we were able to infer document embeddings straight from the model. Later we used
    these document embeddings to classify some BBC news articles that fall into these
    categories: entertainment, tech, politics, business, and sports.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed GloVe—another word embedding learning technique.
    GloVe takes the current Word2vec algorithms a step further by incorporating global
    statistics into the optimization, thus increasing the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned about a much more advanced algorithm known as ELMo (which stands
    for Embeddings from Language Models). ELMo provides contextualized representations
    of words by looking at a word within a sentence or a phrase, not by itself.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed a real-world application of using word embeddings—document
    classification. We showed that word embeddings are very powerful and allow us
    to classify related documents with a simple multi-class logistic regression model
    reasonably well. ELMo performed the best out of skip-gram, CBOW, and GloVe, due
    to the vast amount of data it has been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move on to discussing a different family of deep
    networks that are more powerful in exploiting spatial information present in data,
    known as **Convolutional Neural Networks** (**CNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: Precisely, we will see how CNNs can be used to exploit the spatial structure
    of sentences to classify them into different classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)'
  prefs: []
  type: TYPE_IMG
