- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The answers to the assessment questions found at the end of each chapter are
    shared in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Which of the following tasks does not belong to computer vision: a web search
    of images similar to a query, a 3D scene reconstruction from image sequences,
    or the animation of a video character?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The *latter*, which instead belongs to the domain of **computer graphics.**
    Note, however, that increasingly, computer vision algorithms are helping artists
    to generate or animate content more efficiently (such as the *motion capture*
    methods, for instance, which record actors performing some actions and transfer
    the motions to virtual characters).
  prefs: []
  type: TYPE_NORMAL
- en: '**Which** **activ****ation** **function did the original perceptrons use?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `step` function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Suppose we want to train a method to detect whether a handwritten digit is
    a *4*. How should we adapt the network implemented in the chapter for this task?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the chapter, we trained a classification network to identify pictures of
    digits from `0` to `9`. Therefore, the network had to predict the proper class
    among 10, hence, an output vector of 10 values (one for each class score/probability).
  prefs: []
  type: TYPE_NORMAL
- en: In this question, we define a different classification task. We want the network
    to identify whether an image contains a *4* or *not a 4*. This is a **binary classification**,
    and the network should, therefore, be edited to *output only two values*.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is Keras compared to TensorFlow? What is its purpose?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keras was designed as a wrapper around other deep learning libraries to make
    development easier. TensorFlow is now fully integrated with Keras through `tf.keras`.
    It is best practice to use this module to create models in TensorFlow 2.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why does TensorFlow use graphs? How can they be created manually?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TensorFlow relies on graphs to ensure model performance and portability. In
    TensorFlow 2, the best way to create graphs manually is to employ the `tf.function`
    decorator.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the difference between eager execution mode and lazy execution mode?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In lazy execution mode, no computation is performed until the user specifically
    asks for a result. In eager execution mode, every operation is run when it is
    defined. While the former can be faster thanks to graph optimizations, the latter
    is easier to use and easier to debug. In TensorFlow 2, lazy execution mode has
    been deprecated in favor of eager execution mode.
  prefs: []
  type: TYPE_NORMAL
- en: '**How do you log information in TensorBoard, and how do you display it? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To log information in TensorBoard, you can use the `tf.keras.callbacks.TensorBoard`
    callback and pass it to the `.fit` method when training a model. To log information
    manually, you can use the `tf.summary` module. To display information, launch
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, `model_logs` is the directory where TensorBoard logs are stored. This
    command will output a URL. Navigate to this URL to monitor training.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the main differences between TensorFlow 1 and 2?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TensorFlow 2 focuses on simplicity by removing graph management from the hands
    of the user. It also uses eager execution by default, making models easier to
    debug. Nevertheless, it still maintains its performance thanks to AutoGraph and
    `tf.function`. It also integrates deeply with Keras, making model creation easier
    than ever.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Why does the output of a convolutional layer have a smaller width and height
    than the input, unless it is padded?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The spatial dimensions of the output of a convolutional layer represent the
    number of valid positions the kernels could take when sliding over the input tensors,
    vertically and horizontally. Since kernels span over *k* × *k* pixels (if square),
    the number of positions they can take over the input image without being partially
    out of it can only be equal to (if *k* = 1), or less than, the image dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: This is expressed by the equations presented in the chapter, to compute the
    output dimensions based on the layer's hyper parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**What would be the output of a max-pooling layer with a receptive field of
    (2, 2) and a stride of 2 on the input matrix in Figure 3-6?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b377c7f8-43ec-4690-b38b-7be67231f8a7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**How could LeNet-5 be implemented using the Keras Functional API in a non-object-oriented
    manner ?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**How does L1/L2 regularization affect the networks?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**L1 regularization** forces the layers to which it is applied to bring toward
    zero the values of the parameters linked to less important features; that is,
    to ignore less meaningful features (such as features tied to dataset noise).'
  prefs: []
  type: TYPE_NORMAL
- en: '**L2 regularization** compels the layers to keep their variables low, and,
    hence, more homogeneously distributed. It prevents the network from developing
    a small set of parameters with large values that overly influence its predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Which TensorFlow Hub module can be used to instantiate an Inception classifier
    for ImageNet?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model at [https://tfhub.dev/google/tf2-preview/inception_v3/classification/2](https://tfhub.dev/google/tf2-preview/inception_v3/classification/2)
    can be directly used to classify ImageNet-like images, as this classification
    model was pretrained over this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**How can the first three residual macro-blocks of a ResNet-50 model from Keras
    Applications be frozen?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**When is transfer learning discouraged?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transfer learning may not be beneficial when the *domains* are too dissimilar
    and the target data has a structure that is completely different to the source
    data structure. As mentioned in the chapter, while CNNs can be applied to images,
    text, and audio files, transferring weights trained for one modality to another
    is not encouraged.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is the difference between a bounding box, an anchor box, and a ground
    truth box?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A **bounding box** is the smallest rectangle enclosing an object. An **anchor
    box** is a bounding box with a specific size. For each position in the image grid,
    there are usually several anchor boxes with different aspect ratios—square, vertical
    rectangle, and horizontal rectangle. By refining the size and the position of
    the anchor box, the object detection model generates predictions. A **ground truth
    box** is a bounding box corresponding to a specific object in the training set.
    If a model is trained perfectly, it generates predictions that are very close
    to ground truth boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the role of the feature extractor?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A feature extractor is a CNN that converts an image into a feature volume. The
    feature volume is usually smaller in dimension than the input image and contains
    meaningful features that can be passed to the remainder of the network in order
    to generate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Which of the following models should you choose: YOLO or Faster R-CNN?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If speed is the priority, you should pick YOLO as it is the fastest architecture.
    If accuracy is paramount, you should choose Faster R-CNN as it generates the best
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**When are anchor boxes used?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before anchor boxes, box prediction dimensions were generated using the output
    of the network. As object sizes vary (a person usually fits in a vertical rectangle,
    while a car fits in a horizontal rectangle), anchor boxes were introduced. Using
    this technique, each anchor box is able to specialize for one object ratio, leading
    to more precise predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is the partic****ularity of autoencoders?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Autoencoders are encoders-decoders whose **inputs and targets are the same**.
    Their goal is to properly encode and then decode images without impacting their
    quality, despite their *bottleneck* (that is, their latent space of lower dimensionality).
  prefs: []
  type: TYPE_NORMAL
- en: '**Which classification architecture are fully convolutional networks (FCNs)
    based on?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**FCNs** use **VGG-16** as the feature extractor.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How can a semantic segmentation model be trained so that it does not ignore
    small classes?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Per-class weighing** can be applied to the cross-entropy loss, thereby penalizing
    more heavy pixels from smaller classes that are misclassified. Losses that are
    not affected by the classes'' proportions can also be used instead, such as **Dice**.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Given an `a = [1, 2, 3]` tensor and a `b = [4, 5, 6]` tensor, how can a `tf.data`
    pipeline that would output each value separately, from `1` to `6`, be built?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**According to the documentation of `tf.data.Options`, how can you ensure that
    a dataset always returns samples in the same order, run after run?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `.experimental_deterministic` attribute of `tf.data.Options` should be set
    to `True` before being passed to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Which domain adaptation methods that we introduced can be used when no target
    annotations are available for training?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unsupervised domain adaptation methods should be considered, such as *Learning
    Transferable Features with Deep Adaptation Networks*, by Mingsheng Long et al.
    (from Tsinghua University, China), or **Domain-Adversarial Neural Networks** (**DANN**),
    by Yaroslav Ganin et al. (from Skoltech).
  prefs: []
  type: TYPE_NORMAL
- en: '**What role does the discriminator play in GANs?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It plays against the generator, trying to distinguish fake images from real
    images. The discriminator can be considered as a **trainable loss function** to
    guide the generator—the generator tries to minimize how *correct* the discriminator
    is, with both networks becoming better and better at their task as the training
    proceeds.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What are the main advantages of LSTMs over the simple RNN architecture?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LSTMs suffer less from gradient vanishing and are more capable of storing long-term
    relationships in recurrent data. While they require more computing power, this
    usually leads to better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**How is a CNN used when it is applied before the LSTM?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CNN acts as a feature extractor and reduces the dimensionality of the input
    data. By applying a pretrained CNN, we extract meaningful features from the input
    images. The LSTM trains faster since those features have a much smaller dimensionality
    than the input image.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is vanishing gradient and why does it occur? Why is it a problem?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When backpropagating the error in RNNs, we need to go back through the time
    steps as well. If there are many time steps, the information slowly fades away
    due to the way in which the gradient is computed. It is a problem since it makes
    it harder for the network to learn how to generate good predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are some of the workarounds for the vanishing gradient problem?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One workaround is to use truncated backpropagation, which is a technique described
    in the chapter. Another option is to use LSTMs instead of simple RNNs, as they
    suffer less from gradient vanishing.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**When measuring a model''s inference speed, should you measure with single
    or multiple images?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiple images should be used to avoid measure bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is a model with `float32` weights larger or smaller than one with `float16`
    weights?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Float16` weights use about half the space of `float32` weights. On compatible
    devices, they can also be faster.'
  prefs: []
  type: TYPE_NORMAL
- en: '**On iOS devices, should you use Core ML or TensorFlow Lite? What about Android
    devices?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On iOS devices, we recommend using Core ML where possible as it is available
    natively and is tightly integrated with the hardware. On Android devices, TensorFlow
    Lite should be used as there is no alternative.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the benefits and limitations of running a model in the browser?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It does not require any installation on the user side and does not require computing
    power on the server side, making the application almost infinitely scalable.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the most important requirement for embedded devices running deep
    learning algorithms?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On top of computing power, the most important requirement is power consumption,
    since most embedded devices run on batteries.
  prefs: []
  type: TYPE_NORMAL
