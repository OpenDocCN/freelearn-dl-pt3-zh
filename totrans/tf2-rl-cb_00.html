<html><head></head><body>
		<div id="_idContainer004">
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>Preface</h1>
			<p>Deep reinforcement learning enables the building of intelligent agents, products, and services that can go beyond computer vision or perception to perform actions. TensorFlow 2.x is the latest major release of the most popular deep learning framework that is used to develop and train <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>).</p>
			<p>The book begins with an introduction to the fundamentals of deep reinforcement learning and the latest major version of TensorFlow 2.x. You'll then cover OpenAI Gym, model-based RL, and model-free RL, and learn how to develop basic agents. Moving on, you will discover how to implement advanced deep reinforcement learning algorithms such as actor-critic, deep deterministic policy gradients, deep-Q networks, proximal policy optimization, deep recurrent Q-networks, and the soft actor-critic algorithm to train your RL agents. You'll also explore reinforcement learning in the real world by building cryptocurrency trading agents, stock/share trading agents, and intelligent agents for automating task completion. Lastly, you will find out how to deploy deep reinforcement learning agents to the cloud and build cross-platform apps for the web, mobile, and other platforms using TensorFlow 2.x.</p>
			<p>By the end of this cookbook, you will have gained a solid understanding of deep reinforcement learning algorithms with the help of easy-to-follow and concise implementations from scratch using TensorFlow 2.x.</p>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>Who this book is for</h1>
			<p>The book is for machine learning application developers, AI and applied AI researchers, data scientists, deep learning practitioners, and students with a basic understanding of the reinforcement learning concepts who want to build, train, and deploy their own reinforcement learning systems from scratch using TensorFlow 2.x.</p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>What this book covers</h1>
			<p><a href="B15074_01_Final_AM.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Developing Building Blocks for Deep Reinforcement Learning Using TensorFlow 2.x</em>, provides recipes for getting started with RL environments, deep neural network-based RL agents, evolutionary neural agents, and other building blocks for both discrete and continuous action-space RL applications.</p>
			<p><a href="B15074_02_Final_AM.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a>, <em class="italic">Implementing Value-Based Policy Gradients and Actor-Critic Deep RL Algorithms</em>, includes recipes for implementing value iteration-based learning agents and breaks down the implementation of several foundational algorithms in RL, such as Monte-Carlo control, SARSA and Q-learning, actor-critic, and policy gradient algorithms into simple steps.</p>
			<p><a href="B15074_03_ePub_AM.xhtml#_idTextAnchor090"><em class="italic">Chapter 3</em></a>, <em class="italic">Implementing Advanced RL Algorithms</em>, provides concise recipes to implement complete agent training systems using Deep Q-Network (DQN), Double and Dueling Deep Q-Network (DDQN, DDDQN), Deep Recurrent Q-Network (DRQN), Asynchronous Advantage Actor-Critic (A3C), Proximal Policy Optimization (PPO), and Deep Deterministic Policy Gradient (DDPG) RL algorithms.</p>
			<p><a href="B15074_04_ePub_AM.xhtml#_idTextAnchor135"><em class="italic">Chapter 4</em></a>, <em class="italic">RL in the Real World </em><em class="italic">–</em><em class="italic"> Building Cryptocurrency Trading Agents</em>, shows how to implement and train a soft actor-critic agent in custom RL environments for  bitcoin and ether trading using real market data from trading exchanges such as Gemini, containing both tabular and visual (image) state/observation and discrete and continuous action spaces. </p>
			<p><a href="B15074_05_ePub_AM.xhtml#_idTextAnchor153"><em class="italic">Chapter 5</em></a>, <em class="italic">RL in the Real World </em><em class="italic">–</em><em class="italic"> Building Stock/Share Trading Agents</em>, covers how to train advanced RL agents to trade for profit in the stock market using visual price charts and/or tabular ticket data and more in custom RL environments powered by real stock market exchange data.</p>
			<p><a href="B15074_06_ePub_AM.xhtml#_idTextAnchor167"><em class="italic">Chapter 6</em></a>, <em class="italic">RL in the Real World </em><em class="italic">–</em><em class="italic"> Building Intelligent Agents to Complete Your To-Dos</em>, provides recipes to build, train, and test vision-based RL agents for completing tasks on the web to help you automate tasks such as clicking on pop-up/confirmation dialogs on web pages, logging into various websites, finding and booking the cheapest flight tickets for your travel, decluttering your email inbox, and like/share/retweeting posts on social media sites to engage with your followers.</p>
			<p><a href="B15074_07_ePub_AM.xhtml#_idTextAnchor193"><em class="italic">Chapter 7</em></a>, <em class="italic">Deploying Deep RL Agents to the Cloud</em>, contains recipes to equip you with tools and details to get ahead of the curve and build cloud-based Simulation-as-a-Service and Agent/Bot-as-a-Service programs using deep RL. Learn how to train RL agents using remote simulators running on the cloud, package runtime components of RL agents, and deploy deep RL agents to the cloud by deploying your own trading bot-as-a-service.</p>
			<p><a href="B15074_08_ePub_AM.xhtml#_idTextAnchor221"><em class="italic">Chapter 8</em></a>, <em class="italic">Distributed Training for the Accelerated Development of Deep RL Agents</em>, contains recipes to speed up deep RL agent development using the distributed training of deep neural network models by leveraging TensorFlow 2.x's capabilities. Learn how to utilize multiple CPUs and GPUs both on a single machine as well as on a cluster of machines to scale up/out your deep RL agent training and also learn how to leverage Ray, Tune, and RLLib for large-scale accelerated training.</p>
			<p><a href="B15074_09_ePub_AM.xhtml#_idTextAnchor244"><em class="italic">Chapter 9</em></a>, <em class="italic">Deploying Deep RL Agents on Multiple Platforms</em>, provides customizable templates that you can utilize for building and deploying your own deep RL applications for your use cases. Learn how to export RL agent models for serving/deployment in various production-ready formats, such as TensorFlow Lite, TensorFlow.js, and ONNX, and learn how to leverage NVIDIA Triton or build your own solution to launch production-ready, RL-based AI services. You will also deploy an RL agent in a mobile and web app and learn how to deploy RL bots in your Node.js applications.</p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>To get the most out of this book</h1>
			<p>The code in this book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04 and should work with later versions of Ubuntu if Python 3.6+ is available. With Python 3.6+ installed along with the necessary Python packages, as listed at the start of each of the recipes, the code should run fine on Windows and macOS X too. </p>
			<div>
				<div id="_idContainer003" class="IMG---Figure">
					<img src="image/B15074_Table_1.jpg" alt=""/>
				</div>
			</div>
			<p>It is advised to create and use a Python virtual environment named tfrl-cookbook to install the packages and run the code in this book. A Miniconda or Anaconda installation for Python virtual environment management is recommended.</p>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code via the GitHub repository (link available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.</strong></p>
			<p>It is highly recommended to star and fork the GitHub repository to receive updates and improvements to the code recipes.We urge you to share what you build and also engage with other readers and the community at<em class="italic"> </em><a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook/discussions">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook/discussions</a>.</p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Download the example code files</h1>
			<p>You can download the example code files for this book from your account at <a href="http://www.packt.com">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="http://www.packtpub.com/support">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
			<p>You can download the code files by following these steps:</p>
			<ol>
				<li>Log in or register at <a href="http://www.packt.com">www.packt.com</a>.</li>
				<li>Select the <strong class="bold">Support</strong> tab.</li>
				<li>Click on <strong class="bold">Code Downloads</strong>.</li>
				<li>Enter the name of the book in the <strong class="bold">Search</strong> box and follow the onscreen instructions.</li>
			</ol>
			<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
			<ul>
				<li>WinRAR/7-Zip for Windows</li>
				<li>Zipeg/iZip/UnRarX for Mac</li>
				<li>7-Zip/PeaZip for Linux</li>
			</ul>
			<p>The code bundle for the book is also hosted on GitHub at <a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook/">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook/</a>. In case there's an update to the code, it will be updated on the existing GitHub repository.</p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Download the color images</h1>
			<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://static.packt-cdn.com/downloads/9781838982546_ColorImages.pdf">https://static.packt-cdn.com/downloads/9781838982546_ColorImages.pdf</a>.</p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Conventions used</h1>
			<p>There are a number of text conventions used throughout this book.</p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words used in the recipes. Here is an example: "We will start with the implementation of the <strong class="source-inline">save</strong> method in the <strong class="source-inline">Actor</strong> class to export the Actor model to TensorFlow's <strong class="source-inline">SavedModel</strong> format."</p>
			<p>A block of code is set as follows:</p>
			<p class="source-code">def save(self, model_dir: str, version: int = 1):</p>
			<p class="source-code">    actor_model_save_dir = os.path.join(model_dir, "actor", str(version), "model.savedmodel")</p>
			<p class="source-code">    self.model.save(actor_model_save_dir, save_format="tf")</p>
			<p class="source-code">    print(f"Actor model saved at:{actor_model_save_dir}") </p>
			<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
			<p class="source-code">if args.agent != "SAC":</p>
			<p class="source-code">    print(f"Unsupported Agent: {args.agent}. Using SAC Agent")</p>
			<p class="source-code">    args.agent = "SAC"</p>
			<p class="source-code">    # Create an instance of the Soft Actor-Critic Agent</p>
			<p class="source-code">    agent = SAC(env.observation_space.shape, env.action_space) </p>
			<p>Any command-line input or output is written as follows:</p>
			<p class="source-code">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python 3_training_rl_agents_using_remote_sims.py </p>
			<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "Click on the <strong class="bold">Open an Existing Project</strong> option and you will see a popup asking you to choose the directory on your filesystem. Navigate to the <a href="B15074_09_ePub_AM.xhtml#_idTextAnchor244"><em class="italic">Chapter 9</em></a> recipes and choose <strong class="bold">9.2_rl_android_app</strong>."</p>
			<p class="callout-heading">Tips or important notes	</p>
			<p class="callout">Appear like this.</p>
			<h1 id="_idParaDest-14"><a id="_idTextAnchor013"/>Get in touch</h1>
			<p>Feedback from our readers is always welcome.</p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, mention the book title in the subject of your message and email us at <strong class="source-inline">customercare@packtpub.com</strong>.</p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <strong class="source-inline">copyright@packt.com</strong> with a link to the material.</p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a>.</p>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Reviews</h1>
			<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
			<p>For more information about Packt, please visit <a href="http://packt.com">packt.com</a>.</p>
		</div>
</body></html>