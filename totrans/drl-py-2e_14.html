<html><head></head><body>
  <div id="_idContainer2568">
    <h1 class="chapterNumber">14</h1>
    <h1 id="_idParaDest-376" class="chapterTitle">Distributional Reinforcement Learning</h1>
    <p class="normal">In this chapter, we will learn about distributional reinforcement learning. We will begin the chapter by understanding what exactly distributional reinforcement learning is and why it is useful. Next, we will learn about one of the most popular distributional reinforcement learning algorithms called <strong class="keyword">categorical DQN</strong>. We will understand what a categorical DQN is and how it differs from the DQN we learned in <em class="chapterRef">Chapter 9</em>, <em class="italic">Deep Q Networks and Its Variants</em>, and then we will explore the categorical DQN algorithm in detail. </p>
    <p class="normal">Following this, we will learn another interesting algorithm called <strong class="keyword">Quantile Regression DQN</strong> (<strong class="keyword">QR-DQN</strong>). We will understand what a QR-DQN is and how it differs from a categorical DQN, and then we will explore the QR-DQN algorithm in detail. </p>
    <p class="normal">At the end of the chapter, we will learn about the policy gradient algorithm called the <strong class="keyword">Distributed Distributional Deep Deterministic Policy Gradient</strong> (<strong class="keyword">D4PG</strong>). We will learn what the D4PG is and how it differs from the DDPG we covered in <em class="chapterRef">Chapter 12</em>,<em class="italic"> Learning DDPG, TD3, and SAC</em>, in detail</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bullet">Why distributional reinforcement learning?</li>
      <li class="bullet">Categorical DQN</li>
      <li class="bullet">Quantile regression DQN</li>
      <li class="bullet">Distributed distributional deep deterministic policy gradient</li>
    </ul>
    <p class="normal">Let's begin the chapter by understanding what distributional reinforcement learning is and why we need it.</p>
    <h1 id="_idParaDest-377" class="title">Why distributional reinforcement learning?</h1>
    <p class="normal">Say we <a id="_idIndexMarker1294"/>are in state <em class="italic">s</em> and we have two possible actions to perform in this state. Let the actions be <em class="italic">up</em> and <em class="italic">down</em>. How do we decide which action to perform in the state? We compute Q values for all actions in the state and select the action that has the maximum Q value. So, we compute <em class="italic">Q</em>(<em class="italic">s</em>, up) and <em class="italic">Q</em>(<em class="italic">s</em>, down) and select the action that has the maximum Q value.</p>
    <p class="normal">We learned that the Q value is the expected return an agent would obtain when starting from state <em class="italic">s</em> and performing an action <em class="italic">a</em> following the policy <img src="../Images/B15558_14_001.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_002.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">But there is a small problem in computing the Q value in this manner because the Q value is just an expectation of the return, and the expectation does not include the intrinsic randomness. Let's understand exactly what this means with an example. </p>
    <p class="normal">Let's suppose we want to drive from work to home and we have two routes <strong class="keyword">A</strong> and <strong class="keyword">B</strong>. Now, we have to decide which route is better, that is, which route helps us to reach home in the minimum amount of time. To find out which route is better, we can calculate the Q values and select the route that has the maximum Q value, that is, the route that gives us the maximum expected return.</p>
    <p class="normal">Say the Q value of choosing route <em class="italic">A</em> is <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">A</em>) = 31, and the Q value of choosing route <em class="italic">B</em> is <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">B</em>) = 28. Since the Q value (the expected return of route <strong class="keyword">A</strong>) is higher, we can choose route <strong class="keyword">A</strong> to travel home. But are we missing something here? Instead of viewing the Q value as an expectation over a return, can we directly look into the distribution of return and make a better decision?</p>
    <p class="normal">Yes!</p>
    <p class="normal">But first, let's take a look at the distribution of route <strong class="keyword">A</strong> and route <strong class="keyword">B</strong> and understand which route is best. The following plot shows the distribution of route <strong class="keyword">A</strong>. It tells us with 70% probability we reach home in 10 minutes, and with 30% probability we reach home in 80 minutes. That is, if we choose route <strong class="keyword">A</strong> we usually reach home in 10 minutes but when there is heavy traffic we reach home in 80 minutes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.1: Distribution of route A</p>
    <p class="normal"><em class="italic">Figure 14.2</em> shows <a id="_idIndexMarker1295"/>the distribution of route <strong class="keyword">B</strong>. It tells us that with 80% probability we reach home in 20 minutes and with 20% probability we reach home in 60 minutes.</p>
    <p class="normal">That is, if we choose route <strong class="keyword">B</strong> we usually reach home in 20 minutes but when there is heavy traffic we reach home in 60 minutes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.2: Distribution of route B</p>
    <p class="normal">After <a id="_idIndexMarker1296"/>looking at these two distributions, it makes more sense to choose route <strong class="keyword">B</strong> instead of choosing route <strong class="keyword">A</strong>. With route <strong class="keyword">B</strong>, even in the worst case, that is, even when there is heavy traffic, we can reach home in 60 minutes. But with route <strong class="keyword">A</strong>, when there is heavy traffic, we reach home in 80 minutes. So, it is a wise decision to choose route <strong class="keyword">B</strong> rather than <strong class="keyword">A</strong>.</p>
    <p class="normal">Similarly, if we can observe the distribution of return of route <strong class="keyword">A</strong> and route <strong class="keyword">B</strong>, we can understand more information and we will miss out on these details when we take actions just based on the maximum expected return, that is, the maximum Q value. So, instead of using the expected return to select an action, we use the distribution of return and then select optimal action based on the distribution.</p>
    <p class="normal">This is the basic idea and motivation behind distributional reinforcement learning. In the next <a id="_idIndexMarker1297"/>section, we will learn one of the most popular distributional reinforcement learning algorithms, called categorical DQN, which is also known as the C51 algorithm.</p>
    <h1 id="_idParaDest-378" class="title">Categorical DQN</h1>
    <p class="normal">In the last <a id="_idIndexMarker1298"/>section, we learned why it is more beneficial to choose an action based on the distribution of return than to choose an action based on the Q value, which is just the expected return. In this section, we will understand how to compute the distribution of return using an algorithm called categorical DQN.</p>
    <p class="normal">The distribution <a id="_idIndexMarker1299"/>of return is often called the value distribution or return distribution. Let <em class="italic">Z</em> be <a id="_idIndexMarker1300"/>the random variable and <em class="italic">Z</em>(<em class="italic">s</em>, <em class="italic">a</em>) denote the value distribution of a state <em class="italic">s</em> and an action <em class="italic">a</em>. We know that the Q function is represented by <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) and it gives the value of a state-action pair. Similarly, now we have <em class="italic">Z</em>(<em class="italic">s</em>, <em class="italic">a</em>) and it gives the value distribution (return distribution) of the state-action pair.</p>
    <p class="normal">Okay, how can we compute <em class="italic">Z</em>(<em class="italic">s</em>, <em class="italic">a</em>)? First, let's recollect how we compute <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>).</p>
    <p class="normal">In DQN, we learned that we use a neural network to approximate the Q function, <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a),</em> Since we use a neural network to approximate the Q function, we can represent the Q function by <img src="../Images/B15558_12_331.png" alt="" style="height: 1.11em;"/>, where <img src="../Images/B15558_14_004.png" alt="" style="height: 1.11em;"/> is the parameter of the network. Given a state as an input to the network, it outputs the Q values of all the actions that can be performed in that state, and then we select the action that has the maximum Q value.</p>
    <p class="normal">Similarly, in categorical DQN, we use a neural network to approximate the value of <em class="italic">Z</em>(<em class="italic">s</em>, <em class="italic">a</em>). We can represent this by <img src="../Images/B15558_14_005.png" alt="" style="height: 1.11em;"/>, where <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/> is the parameter of the network. Given a state as an input to the network, it outputs the value distribution (return distribution) of all the actions that can be performed in that state as an output and then we select an action based on this value distribution.</p>
    <p class="normal">Let's understand the difference between the DQN and categorical DQN with an example. Suppose we are in the state <em class="italic">s</em> and say our action space has two actions <em class="italic">a</em> and <em class="italic">b</em>. Now, as shown in <em class="italic">Figure 14.3</em>, given the state <em class="italic">s</em> as an input to the DQN, it returns the Q value of all the actions, then we select the action that has the maximum Q value, whereas in the categorical DQN, given the state <em class="italic">s</em> as an input, it returns the value distribution of all the actions, then we select the action based on this value distribution:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.3: DQN vs categorical DQN</p>
    <p class="normal">Okay, how <a id="_idIndexMarker1301"/>can we train the network? In DQN, we learned that we train the network by minimizing the loss between the target Q value and the Q value predicted by the network. We learned that the target Q value is obtained by the Bellman optimality equation. Thus, we minimize the loss between the target value (the optimal Bellman Q value) and the predicted value (the Q value predicted by the network) and train the network.</p>
    <p class="normal">Similarly, in categorical DQN, we train the network by minimizing the loss between the target value distribution and the value distribution predicted by the network. Okay, how can we obtain the target value distribution? In DQN, we obtained the target Q value using the Bellman equation; similarly in categorical DQN, we can obtain the target value distribution using the distributional Bellman equation. What's the distributional Bellman equation? First, let's recall the Bellman equation before learning about the distributional Bellman equation.</p>
    <p class="normal">We learned that the Bellman equation for the Q function <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_007.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Similarly, the Bellman equation for the value distribution <em class="italic">Z</em>(<em class="italic">s</em>, <em class="italic">a</em>) is given as: </p>
    <figure class="mediaobject"><img src="../Images/B15558_14_008.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">This <a id="_idIndexMarker1302"/>equation is called the distributional Bellman equation. Thus, in categorical DQN, we train the network by minimizing the loss between the target value distribution, which is given by the distributional Bellman equation, and the value distribution predicted by the network.</p>
    <p class="normal">Okay, what <a id="_idIndexMarker1303"/>loss function should we use? In DQN, we use the <strong class="keyword">mean squared error</strong> (<strong class="keyword">MSE</strong>) as our loss function. Unlike a DQN, we cannot use the MSE as the <a id="_idIndexMarker1304"/>loss function in the categorical DQN because in categorical DQN, we predict the probability distribution and not the Q value. Since we are dealing with the distribution we use the cross entropy loss as our loss function. Thus, in categorical DQN, we train the network by minimizing the cross entropy loss between the target value distribution and the value distribution predicted by the network.</p>
    <p class="normal">In a nutshell, a categorical DQN is similar to DQN, except that in a categorical DQN, we predict the value distribution whereas in a DQN we predict the Q value. Thus, given a state as an input, a categorical DQN returns the value distribution of each action in that state. We train the network by minimizing the cross entropy loss between the target value distribution, which is given by the distributional Bellman equation, and the value distribution predicted by the network.</p>
    <p class="normal">Now that we have understood what a categorical DQN is and how it differs from a DQN, in the next section we will learn how exactly the categorical DQN predicts the value distribution.</p>
    <h2 id="_idParaDest-379" class="title">Predicting the value distribution</h2>
    <p class="normal"><em class="italic">Figure 14.4</em> shows <a id="_idIndexMarker1305"/>a simple value distribution:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.4: Value distribution</p>
    <p class="normal">The horizontal axis values are called support or atoms and the vertical axis values are the probability. We denote the support by <em class="italic">Z</em> and the probability by <em class="italic">P</em>. In order to predict the value distribution, along with the state, our network takes the support of the distribution as input and it returns the probability of each value in the support. </p>
    <p class="normal">So, now, we <a id="_idIndexMarker1306"/>will see how to compute the support of the distribution. To compute support, first, we need to decide the number of values of the support <em class="italic">N</em>, the minimum value of the support <img src="../Images/B15558_14_009.png" alt="" style="height: 1.11em;"/>, and the maximum value of the support <img src="../Images/B15558_14_010.png" alt="" style="height: 1.11em;"/>. Given a number of support <em class="italic">N</em>, we divide them into <em class="italic">N</em> equal parts from <img src="../Images/B15558_14_009.png" alt="" style="height: 1.11em;"/> to <img src="../Images/B15558_14_012.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Let's understand this with an example. Say the number of support <em class="italic">N</em> = 5, the minimum value of support <img src="../Images/B15558_14_013.png" alt="" style="height: 1.11em;"/>, and the maximum value of the support <img src="../Images/B15558_14_014.png" alt="" style="height: 1.11em;"/>. Now, how can we find the values of the support? In order to find the values of the support, first, we will compute the step size called <img src="../Images/B15558_14_015.png" alt="" style="height: 1.11em;"/>. The value of <img src="../Images/B15558_14_016.png" alt="" style="height: 1.11em;"/> can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_017.png" alt="" style="height: 2.22em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_14_018.png" alt="" style="height: 2.22em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_14_019.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">Now, to <a id="_idIndexMarker1307"/>compute the values of support, we start with the minimum value of support <img src="../Images/B15558_14_009.png" alt="" style="height: 1.11em;"/> and add <img src="../Images/B15558_14_021.png" alt="" style="height: 1.11em;"/> to every value until we reach the number of support <em class="italic">N</em>. In our example, we start with <img src="../Images/B15558_14_022.png" alt="" style="height: 1.11em;"/>, which is 2, and we add <img src="../Images/B15558_14_023.png" alt="" style="height: 1.11em;"/> to every value until we reach the number of support <em class="italic">N</em>. Thus, the support values become:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_024.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Thus, we can write the value of support as <img src="../Images/B15558_14_025.png" alt="" style="height: 1.11em;"/>. The following Python snippet gives us more clarity on how to obtain the support values:</p>
    <pre class="programlisting code"><code class="hljs-code">def get_support(N, V_min, V_max):
    dz = (V_max – V_min) / (N<span class="hljs-number">-1</span>)
    <span class="hljs-keyword">return</span> [V_min + i * dz <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(N)]
</code></pre>
    <p class="normal">Okay, we have learned how to compute the support of the distribution, now how does the neural network take this support as input and return the probabilities? </p>
    <p class="normal">In order to predict the value distribution, along with the state, we also need to give the support of the distribution as input and then the network returns the probabilities of our value distribution as output. Let's understand this with an example. Say we are in a state <em class="italic">s</em> and we have two actions to perform in this state, and let the actions be <em class="italic">up</em> and <em class="italic">down</em>. Say our calculated support values are <em class="italic">z</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">z</em><sub class="Subscript--PACKT-">2</sub>, and <em class="italic">z</em><sub class="Subscript--PACKT-">3</sub>.</p>
    <p class="normal">As <em class="italic">Figure 14.5</em> shows, along with giving the state <em class="italic">s</em> as input to the network, we also give the support <a id="_idIndexMarker1308"/>of our distribution <em class="italic">z</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">z</em><sub class="Subscript--PACKT-">2</sub>, and <em class="italic">z</em><sub class="Subscript--PACKT-">3</sub>. Then our network returns the probabilities <em class="italic">p</em><sub class="" style="font-style: italic;">i</sub>(<em class="italic">s</em>, <em class="italic">a</em>) of the given support for the distribution of action <em class="italic">up</em> and distribution of action <em class="italic">down</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.5: A categorical DQN</p>
    <p class="normal">The authors of the categorical DQN paper (see the <em class="italic">Further reading</em> section for more details) suggest that it will be efficient to set the number of support <em class="italic">N</em> as 51, and so the categorical DQN is also known as the C51 algorithm. Thus, we have learned how categorical DQN predicts the value distribution. In the next section, we will learn how to select <a id="_idIndexMarker1309"/>the action based on this predicted value distribution.</p>
    <h2 id="_idParaDest-380" class="title">Selecting an action based on the value distribution</h2>
    <p class="normal">We have <a id="_idIndexMarker1310"/>learned that a categorical DQN returns the value distribution of each action in the given state. But how can we select the best action based on the value distribution predicted by the network? </p>
    <p class="normal">We generally select an action based on the Q value, that is, we usually select the action that has the maximum Q value. But now we don't have a Q value; instead, we have a value distribution. How can we select an action based on the value distribution? </p>
    <p class="normal">First, we will extract the Q value from the value distribution and then we select the action as the one that has the maximum Q value. Okay, how can we extract the Q value? We can compute the Q value by just taking the expectation of the value distribution. The expectation of the distribution is given as the sum of support <em class="italic">z</em><sub class="" style="font-style: italic;">i</sub> multiplied by their corresponding probability <em class="italic">p</em><sub class="" style="font-style: italic;">i</sub>. So the expectation of the value distribution <em class="italic">Z</em> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_026.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Where <em class="italic">z</em><sub class="" style="font-style: italic;">i</sub> is the support and <em class="italic">p</em><sub class="" style="font-style: italic;">i</sub> is the probability.</p>
    <p class="normal">Thus, the Q value of the value distribution can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_027.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">After computing the Q value, we select the best action as the one that has the maximum Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_028.png" alt="" style="height: 1.4em;"/></figure>
    <p class="normal">Let's understand how this works exactly. Suppose we are in the state <em class="italic">s</em> and say we have two actions in the state. Let the actions be <em class="italic">up</em> and <em class="italic">down</em>. First, we need to compute support. Let the number of support <em class="italic">N</em> = 3, the minimum value of the support <img src="../Images/B15558_14_013.png" alt="" style="height: 1.11em;"/>, and the maximum value of the support <img src="../Images/B15558_14_030.png" alt="" style="height: 1.11em;"/>. Then, our computed support values will be [2,3,4].</p>
    <p class="normal">Now, along with the state <em class="italic">s</em>, we feed the support, then the categorical DQN returns the probabilities <em class="italic">p</em><sub class="" style="font-style: italic;">i</sub>(<em class="italic">s</em>, <em class="italic">a</em>) of the given support for the value distribution of action <em class="italic">up</em> and distribution of action <em class="italic">down</em> as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.6: Categorical DQN</p>
    <p class="normal">Now, how <a id="_idIndexMarker1311"/>can we select the best action, based on these two value distributions? First, we will extract the Q value from the value distributions and then we select the action that has the maximum Q value.</p>
    <p class="normal">We learned that the Q value can be extracted from the value distribution as the sum of support multiplied by their probabilities:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_027.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">So, we can compute the Q value of action <em class="italic">up</em> in state <em class="italic">s</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_032.png" alt="" style="height: 6.29em;"/></figure>
    <p class="normal">Now, we can compute the Q value of action <em class="italic">down</em> in state <em class="italic">s</em> as:</p>
   <figure class="mediaobject"><img src="../Images/B15558_14_033.png" alt="" style="height: 6.29em;"/></figure>
<p class="normal">Now, we select the action that has the maximum Q value. Since the action <em class="italic">up</em> has the high Q value, we select the action <em class="italic">up</em> as the best action. </p>
    <p class="normal">Wait! What <a id="_idIndexMarker1312"/>makes categorical DQN special then? Because just like DQN, we are selecting the action based on the Q value at the end. One important point we have to note is that, in DQN, we compute the Q value based on the expectation of the return directly, but in categorical DQN, first, we learn the return distribution and then we compute the Q value based on the expectation of the return distribution, which captures the intrinsic randomness.</p>
    <p class="normal">We have learned that the categorical DQN outputs the value distribution of all the actions in the given state and then we extract the Q value from the value distribution and select the action that has the maximum Q value as the best action. But the question is how exactly does our categorical DQN learn? How do we train the categorical DQN to predict the accurate value distribution? Let's discuss this in the next section.</p>
    <h2 id="_idParaDest-381" class="title">Training the categorical DQN</h2>
    <p class="normal">We train <a id="_idIndexMarker1313"/>the categorical DQN by minimizing the cross entropy loss between the target value distribution and the predicted value distribution. How can we compute the target distribution? We can compute the target distribution using the distributional Bellman equation given as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_034.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_14_035.png" alt="" style="height: 1.2em;"/> represents the immediate reward <em class="italic">r</em>, which we obtain while performing an action <em class="italic">a</em> in the state <em class="italic">s</em> and moving to the next state <img src="../Images/B15558_14_036.png" alt="" style="height: 1.2em;"/>, so we can just denote <img src="../Images/B15558_14_035.png" alt="" style="height: 1.2em;"/> by <em class="italic">r</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_038.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Remember <a id="_idIndexMarker1314"/>in DQN we computed the target value using the target network parameterized by <img src="../Images/B15558_14_039.png" alt="" style="height: 1.2em;"/>? Similarly, here, we use the target categorical DQN parameterized by <img src="../Images/B15558_14_040.png" alt="" style="height: 1.2em;"/> to compute the target distribution.</p>
    <p class="normal">After computing the target distribution, we train the network by minimizing the cross entropy loss between the target value distribution and the predicted value distribution. One important point we need to note here is that we can apply the cross entropy loss between any two distributions only when their supports are equal; when their supports are not equal we cannot apply the cross entropy loss.</p>
    <p class="normal">For instance, <em class="italic">Figure 14.7</em> shows the support of both the target and predicted distribution is the same, (1,2,3,4). Thus, in this case, we can apply the cross entropy loss: </p>
    <figure class="mediaobject"><img src="../Images/B15558_14_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.7: Target and predicted distribution</p>
    <p class="normal">In <em class="italic">Figure 14.8</em>, we can see that the target distribution support (1,3,4,5) and the predicted distribution support (1,2,3,4) are different, so in this case, we cannot apply the cross entropy loss.</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.8: Target and predicted distribution</p>
    <p class="normal">So, when the <a id="_idIndexMarker1315"/>support of the target and prediction distribution is different, we perform a special step called the projection step using which we can make the support of the target and prediction distribution equal. Once we make the support of the target and prediction distribution equal then we can apply the cross entropy loss.</p>
    <p class="normal">In the next section, we will learn how exactly the projection works and how it makes the support of the target and prediction distribution equal.</p>
    <h3 id="_idParaDest-382" class="title">Projection step</h3>
    <p class="normal">Let's <a id="_idIndexMarker1316"/>understand how exactly the projection step works with an example. Suppose the input support is <em class="italic">z</em> = [1, 2].</p>
    <p class="normal">Let the probability of predicted distribution be <em class="italic">p</em> = [0.5, 0.5]. <em class="italic">Figure 14.9</em> shows the predicted distribution:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.9: Predicted distribution</p>
    <p class="normal">Let the probability of target distribution be <em class="italic">p</em> = [0.3, 0.7]. Let the reward <em class="italic">r</em> = 0.1 and the discount factor <img src="../Images/B15558_14_041.png" alt="" style="height: 1.11em;"/>. The target distribution support value is computed as<img src="../Images/B15558_14_042.png" alt="" style="height: 1.02em;"/>, so, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_043.png" alt="" style="height: 2.13em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_14_044.png" alt="" style="height: 2.13em;"/></figure>
    <p class="normal">Thus, the target distribution becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.10: Target distribution</p>
    <p class="normal">As we can observe from the preceding plots, the supports of the predicted distribution and target <a id="_idIndexMarker1317"/>distribution are different. The predicted distribution has the support [1, 2] while the target distribution has the support [1, 1.9], so in this case, we cannot apply the cross entropy loss.</p>
    <p class="normal">Now, using the projection step we can convert the support of our target distribution to be the same support as the predicted distribution. Once the supports of the predicted and target distribution are the same then we can apply the cross entropy loss.</p>
    <p class="normal">Okay, what's that projection step exactly? How can we apply it and convert the support of the target distribution to match the support of the predicted distribution?</p>
    <p class="normal">Let's understand this with the same example. As the following shows, we have the target distribution support [1, 1.9] and we need to make it equal to the predicted distribution support [1, 2], how can we do that?</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.11: Target distribution</p>
    <p class="normal">So, what <a id="_idIndexMarker1318"/>we can do is that we can distribute the probability 0.7 from the support 1.9 to the support 1 and 2:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.12: Target distribution</p>
    <p class="normal">Okay, but how can we distribute the probabilities from the support 1.9 to the support 1 and 2? Should it be an equal distribution? Of course not. Since 2 is closer to 1.9, we distribute more probability to 2 and less to 1. </p>
    <p class="normal">As shown in <em class="italic">Figure 14.13</em>, from 0.7, we will distribute 0.63 to support 2 and 0.07 to support 1.</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.13: Target distribution</p>
    <p class="normal">Thus, now <a id="_idIndexMarker1319"/>our target distribution will look like:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.14: Target distribution</p>
    <p class="normal">From <em class="italic">Figure 14.14</em>, we can see that support of the target distribution is changed from [1, 1.9] to [1, 2] and now it matches the support of the predicted distribution. This step is called the projection step.</p>
    <p class="normal">What we learned is just a simple example, consider a case where our target and predicted distribution support varies very much. In this case, we cannot manually determine the amount of probability we have to distribute across the supports to make them equal. So, we introduce a set of steps to perform the projection, as the following shows. After performing these steps, our target distribution support will match our predicted distribution by distributing the probabilities across the support.</p>
    <p class="normal">First, we initialize an array <em class="italic">m</em> with its shape as the number of support with zero values. The <em class="italic">m</em> denotes the distributed probability of the target distribution after the projection step.</p>
    <p class="normal">For <em class="italic">j</em> in range <a id="_idIndexMarker1320"/>of the number of support:</p>
    <ol>
      <li class="numbered">Compute the target support value: <img src="../Images/B15558_14_045.png" alt="" style="height: 1.76em;"/></li>
      <li class="numbered">Compute the value of <em class="italic">b</em>: <img src="../Images/B15558_14_046.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Compute the lower bound and the upper bound: <img src="../Images/B15558_14_047.png" alt="" style="height: 1.29em;"/></li>
      <li class="numbered">Distribute the probability on the lower bound: <img src="../Images/B15558_14_048.png" alt="" style="height: 1.29em;"/></li>
      <li class="numbered">Distribute the probability on the upper bound: <img src="../Images/B15558_14_049.png" alt="" style="height: 1.29em;"/></li>
    </ol>
    <p class="normal">Understanding how exactly these projection steps work is a little tricky! So, let's understand this by considering the same example we used earlier. Let z = [1, 2], <em class="italic">N</em> = 2, <img src="../Images/B15558_14_050.png" alt="" style="height: 1.11em;"/>, and <img src="../Images/B15558_14_051.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Let the probability of predicted distribution be <em class="italic">p</em> = [0.5, 0.5]. <em class="italic">Figure 14.15</em> shows the predicted distribution:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.15: Predicted distribution</p>
    <p class="normal">Let the <a id="_idIndexMarker1321"/>probability of target distribution be <em class="italic">p</em> = [0.3, 0.7]. Let the reward <em class="italic">r</em> = 0.1 and the discount factor <img src="../Images/B15558_14_041.png" alt="" style="height: 1.11em;"/>, and we know <img src="../Images/B15558_14_053.png" alt="" style="height: 1.02em;"/>, thus, the target distribution becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.16: Target distribution</p>
    <p class="normal">From <em class="italic">Figure 14.16</em>, we can infer that support in target distribution is different from the predicted distribution. Now, we will learn how to perform the projection using the preceding steps.</p>
    <p class="normal">First, we initialize an array <em class="italic">m</em> with its shape as the number of support with zero values. Thus, <em class="italic">m</em> = [0, 0].</p>
    <p class="normal"><strong class="keyword">Iteration, j=0</strong>:</p>
    <ol>
      <li class="numbered" value="1">Compute the target support value:<figure class="mediaobject"><img src="../Images/B15558_14_054.png" alt="" style="height: 3.07em;"/></figure>
      </li>
      <li class="numbered">Compute the value of <em class="italic">b</em>:<figure class="mediaobject"><img src="../Images/B15558_14_055.png" alt="" style="height: 3.42em;"/></figure>
      </li>
      <li class="numbered">Compute the lower and upper bound:<figure class="mediaobject"><img src="../Images/B15558_14_056.png" alt="" style="height: 1.11em;"/></figure>
      </li>
      <li class="numbered">Distribute the probability on the lower bound:<figure class="mediaobject"><img src="../Images/B15558_14_057.png" alt="" style="height: 1.11em;"/></figure>
        <figure class="mediaobject"><img src="../Images/B15558_14_058.png" alt="" style="height: 3.33em;"/></figure>
      </li>
      <li class="numbered">Distribute <a id="_idIndexMarker1322"/>the probability on the upper bound:<figure class="mediaobject"><img src="../Images/B15558_14_059.png" alt="" style="height: 1.11em;"/></figure>
        <figure class="mediaobject"><img src="../Images/B15558_14_060.png" alt="" style="height: 3.33em;"/></figure>
      </li>
    </ol>
    <p class="normal">After the 1st iteration, the value of <em class="italic">m</em> becomes [0, 0].</p>
    <p class="normal"><strong class="keyword">Iteration, j=1</strong>:</p>
    <ol>
      <li class="numbered" value="1">Compute the target support value:<figure class="mediaobject"><img src="../Images/B15558_14_061.png" alt="" style="height: 3.07em;"/></figure>
      </li>
      <li class="numbered">Compute the value of <em class="italic">b</em>:<figure class="mediaobject"><img src="../Images/B15558_14_062.png" alt="" style="height: 3.42em;"/></figure>
      </li>
      <li class="numbered">Compute <a id="_idIndexMarker1323"/>the lower and upper bound of <em class="italic">b</em>:<figure class="mediaobject"><img src="../Images/B15558_14_063.png" alt="" style="height: 1.11em;"/></figure>
      </li>
      <li class="numbered">Distribute the probability on the lower bound:<figure class="mediaobject"><img src="../Images/B15558_14_064.png" alt="" style="height: 1.4em;"/></figure>
        <figure class="mediaobject"><img src="../Images/B15558_14_065.png" alt="" style="height: 4.53em;"/></figure>
      </li>
      <li class="numbered">Distribute the probability on the upper bound:<figure class="mediaobject"><img src="../Images/B15558_14_066.png" alt="" style="height: 1.29em;"/></figure>
        <figure class="mediaobject"><img src="../Images/B15558_14_067.png" alt="" style="height: 3.33em;"/></figure>
      </li>
    </ol>
    <p class="normal">After <a id="_idIndexMarker1324"/>the second iteration, the value of <em class="italic">m</em> becomes [0.07, 0,63]. The number of iterations = the length of our support. Since the length of our support is 2, we will stop here and thus the value of <em class="italic">m</em> becomes our new distributed probability for the modified support, as <em class="italic">Figure 14.17</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.17: Target distribution</p>
    <p class="normal">The following snippet will give us more clarity on how exactly the projection step works:</p>
    <pre class="programlisting code"><code class="hljs-code">m = np.zeros(num_support)
<span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(num_support):
    Tz = min(v_max,max(v_min,r+gamma * z[j]))
    bj = (Tz - v_min) / delta_z
    l,u = math.floor(bj),math.ceil(bj)
    pj = p[j] 
    m[int(l)] += pj * (u - bj)
    m[int(u)] += pj * (bj - l)
</code></pre>
    <p class="normal">Now that we have understood how to compute the target value distribution and how we can make the support of target value distribution equal to the support of predicted value distribution <a id="_idIndexMarker1325"/>using the projection step, we will learn how to compute the cross entropy loss. Cross entropy loss is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_068.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Where <em class="italic">y</em> is the actual value and <img src="../Images/B15558_14_069.png" alt="" style="height: 1.11em;"/> is the predicted value. Thus, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_070.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Where <em class="italic">m</em> is the target probabilities from the target value distribution and <em class="italic">p</em>(<em class="italic">s</em>, <em class="italic">a</em>) is the predicted probabilities from the predicted value distribution. We train our network by minimizing the cross entropy loss.</p>
    <p class="normal">Thus, using a categorical DQN, we select the action based on the distribution of the return (value distribution). In the next section, we will put all these concepts together and see how a categorical DQN works. </p>
    <h2 id="_idParaDest-383" class="title">Putting it all together</h2>
    <p class="normal">First, we <a id="_idIndexMarker1326"/>initialize the main network parameter <img src="../Images/B15558_09_106.png" alt="" style="height: 1.11em;"/> with random values, and we initialize the target network parameter <img src="../Images/B15558_14_039.png" alt="" style="height: 1.2em;"/> by just copying the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>. We also initialize the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, for each step in the episode, we feed the state of the environment and support values to the main categorical DQN parameterized by <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>. The main network takes the support and state of the environment as input and returns the probability value for each support. Then the Q value of the value distribution can be computed as the sum of support <a id="_idIndexMarker1327"/>multiplied by their probabilities:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_075.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">After computing the Q value of all the actions in the state, we select the best action in the state <em class="italic">s</em> as the one that has the maximum Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_028.png" alt="" style="height: 1.4em;"/></figure>
    <p class="normal">However, instead of selecting the action that has the maximum Q value all the time, we select the action using the epsilon-greedy policy. With the epsilon-greedy policy, we select a random action with probability epsilon and with the probability 1-epsilon, we select the best action that has the maximum Q value. We perform the selected action, move to the next state, obtain the reward, and store this transition information <img src="../Images/B15558_14_077.png" alt="" style="height: 1.2em;"/> in the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, we sample a transition <img src="../Images/B15558_14_077.png" alt="" style="height: 1.2em;"/> from the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/> and feed the next state <img src="../Images/B15558_12_376.png" alt="" style="height: 1.2em;"/> and support values to the target categorical DQN parameterized by <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/>. The target network takes the support and next state <img src="../Images/B15558_12_376.png" alt="" style="height: 1.2em;"/> as input and returns the probability value for each support. </p>
    <p class="normal">Then the Q value can be computed as the sum of support multiplied by their probabilities:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_082.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">After <a id="_idIndexMarker1328"/>computing the Q value of all next state-action pairs, we select the best action in the state <img src="../Images/B15558_14_083.png" alt="" style="height: 1.2em;"/> as the one that has the maximum Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_084.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Now, we perform the projection step. The <em class="italic">m</em> denotes the distributed probability of the target distribution after the projection step.</p>
    <p class="normal">For <em class="italic">j</em> in range of the number of support:</p>
    <ol>
      <li class="numbered" value="1">Compute the target support value: <img src="../Images/B15558_14_085.png" alt="" style="height: 1.76em;"/></li>
      <li class="numbered">Compute the value of <em class="italic">b</em>: <img src="../Images/B15558_14_046.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Compute the lower bound and the upper bound: <img src="../Images/B15558_14_087.png" alt="" style="height: 1.29em;"/></li>
      <li class="numbered">Distribute the probability on the lower bound: <img src="../Images/B15558_14_088.png" alt="" style="height: 1.49em;"/></li>
      <li class="numbered">Distribute the probability on the upper bound: <img src="../Images/B15558_14_089.png" alt="" style="height: 1.49em;"/></li>
    </ol>
    <p class="normal">After performing the projection step, compute the cross entropy loss:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_090.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Where <em class="italic">m</em> is the target probabilities from the target value distribution and <em class="italic">p</em>(<em class="italic">s</em>, <em class="italic">a</em>) is the predicted probabilities from the predicted value distribution. We train our network by minimizing the cross entropy loss.</p>
    <p class="normal">We don't <a id="_idIndexMarker1329"/>update the target network parameter <img src="../Images/B15558_14_040.png" alt="" style="height: 1.2em;"/> in every time step. We freeze the target network parameter <img src="../Images/B15558_14_040.png" alt="" style="height: 1.2em;"/> for several time steps, and then we copy the main network parameter <img src="../Images/B15558_10_037.png" alt="" style="height: 1.11em;"/> to the target network parameter <img src="../Images/B15558_14_039.png" alt="" style="height: 1.2em;"/>. We keep repeating the preceding steps for several episodes to approximate the optimal value distribution. To give us a more detailed understanding, the categorical DQN algorithm is given in the next section.</p>
    <h2 id="_idParaDest-384" class="title">Algorithm – categorical DQN</h2>
    <p class="normal">The <a id="_idIndexMarker1330"/>categorical DQN algorithm is given in the following steps:</p>
    <ol>
      <li class="numbered" value="1">Initialize the main network parameter <img src="../Images/B15558_10_095.png" alt="" style="height: 1.11em;"/> with random values</li>
      <li class="numbered">Initialize the target network parameter <img src="../Images/B15558_14_039.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_10_037.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_14_098.png" alt="" style="height: 1.11em;"/>, the number of support (atoms), and also <img src="../Images/B15558_14_022.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_14_100.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes perform <em class="italic">step 5</em></li>
      <li class="numbered">For each step in the episode, that is, for <img src="../Images/B15558_14_101.png" alt="" style="height: 1.11em;"/>:<ol>
          <li class="numbered-l2">Feed the state <em class="italic">s</em> and support values to the main categorical DQN parameterized by <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> and get the probability value for each support. Then compute the Q value as <img src="../Images/B15558_14_103.png" alt="" style="height: 2.4em;"/></li>
          <li class="numbered-l2">After computing the Q value, select an action using the epsilon-greedy policy, that is, with the probability epsilon, select random action <em class="italic">a</em> and with probability 1-epsilon, select the action as <img src="../Images/B15558_14_028.png" alt="" style="height: 1.4em;"/></li>
          <li class="numbered-l2">Perform the selected action and move to the next state <img src="../Images/B15558_14_105.png" alt="" style="height: 1.2em;"/> and obtain the reward <em class="italic">r</em></li>
          <li class="numbered-l2">Store the transition information in the replay buffer <img src="../Images/B15558_14_098.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a transition from the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Feed the next state <img src="../Images/B15558_12_376.png" alt="" style="height: 1.2em;"/> and support values to the target categorical DQN parameterized by <img src="../Images/B15558_14_040.png" alt="" style="height: 1.2em;"/> and get the probability value for each support. Then compute the value as <img src="../Images/B15558_14_110.png" alt="" style="height: 2.4em;"/></li>
          <li class="numbered-l2">After computing the Q value, we select the best action in the state <img src="../Images/B15558_14_105.png" alt="" style="height: 1.2em;"/> as the one that has the maximum Q value <img src="../Images/B15558_14_112.png" alt="" style="height: 1.4em;"/></li>
          <li class="numbered-l2">Initialize <a id="_idIndexMarker1331"/>the array <em class="italic">m</em> with zero values with its shape as the number of support</li>
          <li class="numbered-l2">For <em class="italic">j</em> in range of the number of support: 
        <ol>
          <li class="numbered-l2" value="1">Compute the target support value: <img src="../Images/B15558_14_113.png" alt="" style="height: 1.76em;"/></li>
          <li class="numbered-l2">Compute the value of b: <img src="../Images/B15558_14_114.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the lower bound and upper bound: <img src="../Images/B15558_14_047.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Distribute the probability on the lower bound: <img src="../Images/B15558_14_116.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">Distribute the probability on the upper bound: <img src="../Images/B15558_14_117.png" alt="" style="height: 1.49em;"/></li>
        </ol></li>
            <li class="numbered-l2"> Compute the cross entropy loss: <img src="../Images/B15558_14_118.png" alt="" style="height: 2.69em;"/></li>
            <li class="numbered-l2"> Minimize the loss using gradient descent and update the parameter of the main network</li>
            <li class="numbered-l2"> Freeze the target network parameter <img src="../Images/B15558_14_040.png" alt="" style="height: 1.2em;"/> for several time steps and then update it by just copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></li>
          
        </ol>
      </li>
    </ol>
    <p class="normal">Now that <a id="_idIndexMarker1332"/>we have learned the categorical DQN algorithm, to understand how a categorical DQN works, we will implement it in the next section.</p>
    <h2 id="_idParaDest-385" class="title">Playing Atari games using a categorical DQN</h2>
    <p class="normal">Let's <a id="_idIndexMarker1333"/>implement the categorical DQN <a id="_idIndexMarker1334"/>algorithm to play Atari games. The code used in this section is adapted from an open-source categorical DQN implementation, <a href="https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo"><span class="url">https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo</span></a>, provided by Prince Wen. </p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque
<span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> tensorflow.compat.v1 <span class="hljs-keyword">as</span> tf
tf.disable_v2_behavior()
<span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">from</span> tensorflow.python.framework <span class="hljs-keyword">import</span> ops
</code></pre>
    <h3 id="_idParaDest-386" class="title">Defining the variables</h3>
    <p class="normal">Now, let's <a id="_idIndexMarker1335"/>define some of the important variables.</p>
    <p class="normal">Initialize the <img src="../Images/B15558_14_022.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_14_012.png" alt="" style="height: 1.11em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">v_min = <span class="hljs-number">0</span>
v_max = <span class="hljs-number">1000</span>
</code></pre>
    <p class="normal">Initialize the number of atoms (supports):</p>
    <pre class="programlisting code"><code class="hljs-code">atoms = <span class="hljs-number">51</span>
</code></pre>
    <p class="normal">Set the discount factor, <img src="../Images/B15558_03_190.png" alt="" style="height: 0.93em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">gamma = <span class="hljs-number">0.99</span>
</code></pre>
    <p class="normal">Set the batch size:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">64</span>
</code></pre>
    <p class="normal">Set the <a id="_idIndexMarker1336"/>time step at which we want to update the target network:</p>
    <pre class="programlisting code"><code class="hljs-code">update_target_net = <span class="hljs-number">50</span>
</code></pre>
    <p class="normal">Set the epsilon value that is used in the epsilon-greedy policy:</p>
    <pre class="programlisting code"><code class="hljs-code">epsilon = <span class="hljs-number">0.5</span>
</code></pre>
    <h3 id="_idParaDest-387" class="title">Defining the replay buffer</h3>
    <p class="normal">First, let's <a id="_idIndexMarker1337"/>define the buffer length:</p>
    <pre class="programlisting code"><code class="hljs-code">buffer_length = <span class="hljs-number">20000</span>
</code></pre>
    <p class="normal">Define the replay buffer as a deque structure:</p>
    <pre class="programlisting code"><code class="hljs-code">replay_buffer = deque(maxlen=buffer_length)
</code></pre>
    <p class="normal">We define a function called <code class="Code-In-Text--PACKT-">sample_transitions</code> that returns the randomly sampled minibatch of transitions from the replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">sample_transitions</span><span class="hljs-function">(</span><span class="hljs-params">batch_size</span><span class="hljs-function">):</span>
    batch = np.random.permutation(len(replay_buffer))[:batch_size]
    trans = np.array(replay_buffer)[batch]
    <span class="hljs-keyword">return</span> trans
</code></pre>
    <h3 id="_idParaDest-388" class="title">Defining the categorical DQN class</h3>
    <p class="normal">Let's define <a id="_idIndexMarker1338"/>a class called <code class="Code-In-Text--PACKT-">Categorical_DQN</code> where we will implement the categorical DQN algorithm. Instead of looking into the whole code at once, we will look into only the important parts. The complete code used in this section is available in the GitHub repo of the book.</p>
    <p class="normal">For a clear understanding, let's take a look into the code line by line:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Categorical_DQN</span><span class="hljs-class">():</span>
</code></pre>
    <h4 class="title">Defining the init method</h4>
    <p class="normal">First, let's <a id="_idIndexMarker1339"/>define the init method:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self,env</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Start the TensorFlow session:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess = tf.InteractiveSession()
</code></pre>
    <p class="normal">Initialize the <img src="../Images/B15558_14_022.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_14_012.png" alt="" style="height: 1.11em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.v_max = v_max
        self.v_min = v_min
</code></pre>
    <p class="normal">Initialize the number of atoms:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.atoms = atoms 
</code></pre>
    <p class="normal">Initialize the epsilon value:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.epsilon = epsilon
</code></pre>
    <p class="normal">Get the state shape of the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.state_shape = env.observation_space.shape
</code></pre>
    <p class="normal">Get the action shape of the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.action_shape = env.action_space.n
</code></pre>
    <p class="normal">Initialize the time step:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.time_step = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">Initialize the target state shape:</p>
    <pre class="programlisting code"><code class="hljs-code">        target_state_shape = [<span class="hljs-number">1</span>]
        target_state_shape.extend(self.state_shape)
</code></pre>
    <p class="normal">Define the placeholder for the state:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.state_ph = tf.placeholder(tf.float32,target_state_shape)
</code></pre>
    <p class="normal">Define the placeholder for the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.action_ph = tf.placeholder(tf.int32,[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])
</code></pre>
    <p class="normal">Define <a id="_idIndexMarker1340"/>the placeholder for the <em class="italic">m</em> value (the distributed probability of the target distribution):</p>
    <pre class="programlisting code"><code class="hljs-code">        self.m_ph = tf.placeholder(tf.float32,[self.atoms])
</code></pre>
    <p class="normal">Compute the value of <img src="../Images/B15558_14_015.png" alt="" style="height: 1.11em;"/> as <img src="../Images/B15558_14_127.png" alt="" style="height: 1.11em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.delta_z = (self.v_max - self.v_min) / (self.atoms - <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Compute the support values as <img src="../Images/B15558_14_025.png" alt="" style="height: 1.11em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.z = [self.v_min + i * self.delta_z <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.atoms)]
</code></pre>
    <p class="normal">Build the categorical DQN:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.build_categorical_DQN()
</code></pre>
    <p class="normal">Initialize all the TensorFlow variables:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess.run(tf.global_variables_initializer())
</code></pre>
    <h4 class="title">Building the categorical DQN</h4>
    <p class="normal">Let's <a id="_idIndexMarker1341"/>define a function called <code class="Code-In-Text--PACKT-">build_network</code> for building a deep network. Since we are dealing with Atari games, we use the convolutional neural network:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_network</span><span class="hljs-function">(</span><span class="hljs-params">self, state, action, name, units_1, units_2, weights, bias</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Define the first convolutional layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'conv1'</span>):
            conv1 = conv(state, [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">6</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], weights, bias)
</code></pre>
    <p class="normal">Define the second convolutional layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'conv2'</span>):
            conv2 = conv(conv1, [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">12</span>], [<span class="hljs-number">12</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], weights, bias)
</code></pre>
    <p class="normal">Flatten <a id="_idIndexMarker1342"/>the feature maps obtained as a result of the second convolutional layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'flatten'</span>):
            flatten = tf.layers.flatten(conv2)
</code></pre>
    <p class="normal">Define the first dense layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'dense1'</span>):
            dense1 = dense(flatten, units_1, [units_1], weights, bias)
</code></pre>
    <p class="normal">Define the second dense layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'dense2'</span>):
            dense2 = dense(dense1, units_2, [units_2], weights, bias)
</code></pre>
    <p class="normal">Concatenate the second dense layer with the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'concat'</span>):
            concatenated = tf.concat([dense2, tf.cast(action, tf.float32)], <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Define the third layer and apply the softmax function to the result of the third layer and obtain the probabilities for each of the atoms:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'dense3'</span>):
            dense3 = dense(concatenated, self.atoms, [self.atoms], weights, bias) 
        <span class="hljs-keyword">return</span> tf.nn.softmax(dense3)
</code></pre>
    <p class="normal">Now, let's define a function called <code class="Code-In-Text--PACKT-">build_categorical_DQN </code>for building the main and target categorical DQNs:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_categorical_DQN</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Define the main categorical DQN and obtain the probabilities:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'main_net'</span>):
            name = [<span class="hljs-string">'main_net_params'</span>,tf.GraphKeys.GLOBAL_VARIABLES]
            weights = tf.random_uniform_initializer(<span class="hljs-number">-0.1</span>,<span class="hljs-number">0.1</span>)
            bias = tf.constant_initializer(<span class="hljs-number">0.1</span>)
            self.main_p = self.build_network(self.state_ph,self.action_ph,name,<span class="hljs-number">24</span>,<span class="hljs-number">24</span>,weights,bias)
</code></pre>
    <p class="normal">Define <a id="_idIndexMarker1343"/>the target categorical DQN and obtain the probabilities:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'target_net'</span>):
            name = [<span class="hljs-string">'target_net_params'</span>,tf.GraphKeys.GLOBAL_VARIABLES]
            weights = tf.random_uniform_initializer(<span class="hljs-number">-0.1</span>,<span class="hljs-number">0.1</span>)
            bias = tf.constant_initializer(<span class="hljs-number">0.1</span>)
            self.target_p = self.build_network(self.state_ph,self.action_ph,name,<span class="hljs-number">24</span>,<span class="hljs-number">24</span>,weights,bias)
</code></pre>
    <p class="normal">Compute the main Q value with the probabilities obtained from the main categorical DQN as <img src="../Images/B15558_14_129.png" alt="" style="height: 2.69em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.main_Q = tf.reduce_sum(self.main_p * self.z)
</code></pre>
    <p class="normal">Similarly, compute the target Q value with probabilities obtained from the target categorical DQN as <img src="../Images/B15558_14_130.png" alt="" style="height: 2.69em;"/>:</p>
    <pre class="programlisting gen"><code class="hljs">        self.target_Q = tf.reduce_sum(self.target_p * self.z)
</code></pre>
    <p class="normal">Define the cross entropy loss as <img src="../Images/B15558_14_131.png" alt="" style="height: 2.69em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.cross_entropy_loss = -tf.reduce_sum(self.m_ph * tf.log(self.main_p))
</code></pre>
    <p class="normal">Define the optimizer and minimize the cross entropy loss using the Adam optimizer:</p>
    <pre class="programlisting code"><code class="hljs-code">       self.optimizer = tf.train.AdamOptimizer(<span class="hljs-number">0.01</span>).minimize(self.cross_entropy_loss)
</code></pre>
    <p class="normal">Get the main network parameters:</p>
    <pre class="programlisting code"><code class="hljs-code">        main_net_params = tf.get_collection(<span class="hljs-string">"main_net_params"</span>)
</code></pre>
    <p class="normal">Get the <a id="_idIndexMarker1344"/>target network parameters:</p>
    <pre class="programlisting code"><code class="hljs-code">        target_net_params = tf.get_collection(<span class="hljs-string">'target_net_params'</span>)
</code></pre>
    <p class="normal">Define the <code class="Code-In-Text--PACKT-">update_target_net</code> operation for updating the target network parameters by copying the parameters of the main network:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.update_target_net = [tf.assign(t, e) <span class="hljs-keyword">for</span> t, e <span class="hljs-keyword">in</span> zip(target_net_params, main_net_params)]
</code></pre>
    <h4 class="title">Defining the train function</h4>
    <p class="normal">Let's define <a id="_idIndexMarker1345"/>a function called <code class="Code-In-Text--PACKT-">train</code> to train the network:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">train</span><span class="hljs-function">(</span><span class="hljs-params">self,s,r,action,s_,gamma</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Increment the time step:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.time_step += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Get the target Q values:</p>
    <pre class="programlisting code"><code class="hljs-code">        list_q_ = [self.sess.run(self.target_Q,feed_dict={self.state_ph:[s_],self.action_ph:[[a]]}) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> range(self.action_shape)]
</code></pre>
    <p class="normal">Select the next state action <img src="../Images/B15558_14_132.png" alt="" style="height: 1.2em;"/> as the one that has the maximum Q value:</p>
    <pre class="programlisting code"><code class="hljs-code">        a_ = tf.argmax(list_q_).eval()
</code></pre>
    <p class="normal">Initialize an array <em class="italic">m</em> with its shape as the number of support with zero values. The <em class="italic">m</em> denotes the distributed probability of the target distribution after the projection step:</p>
    <pre class="programlisting code"><code class="hljs-code">        m = np.zeros(self.atoms)
</code></pre>
    <p class="normal">Get the probability for each atom using the target categorical DQN:</p>
    <pre class="programlisting code"><code class="hljs-code">        p = self.sess.run(self.target_p,feed_dict = {self.state_ph:[s_],self.action_ph:[[a_]]})[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">Perform the projection step:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(self.atoms):
            Tz = min(self.v_max,max(self.v_min,r+gamma * self.z[j]))
            bj = (Tz - self.v_min) / self.delta_z 
            l,u = math.floor(bj),math.ceil(bj) 
            pj = p[j]
            m[int(l)] += pj * (u - bj)
            m[int(u)] += pj * (bj - l)
</code></pre>
    <p class="normal">Train <a id="_idIndexMarker1346"/>the network by minimizing the loss:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess.run(self.optimizer,feed_dict={self.state_ph:[s] , self.action_ph:[action], self.m_ph: m })
</code></pre>
    <p class="normal">Update the target network parameters by copying the main network parameters:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> self.time_step % update_target_net == <span class="hljs-number">0</span>:
            self.sess.run(self.update_target_net)
</code></pre>
    <h4 class="title">Selecting the action</h4>
    <p class="normal">Let's define <a id="_idIndexMarker1347"/>a function called <code class="Code-In-Text--PACKT-">select_action</code> for selecting the action:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">select_action</span><span class="hljs-function">(</span><span class="hljs-params">self,s</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">We generate a random number, and if the number is less than epsilon we select the random action, else we select the action that has the maximum Q value:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> random.random() &lt;= self.epsilon:
            <span class="hljs-keyword">return</span> random.randint(<span class="hljs-number">0</span>, self.action_shape - <span class="hljs-number">1</span>)
        <span class="hljs-keyword">else</span>: 
            <span class="hljs-keyword">return</span> np.argmax([self.sess.run(self.main_Q,feed_dict={self.state_ph:[s],self.action_ph:[[a]]}) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> range(self.action_shape)])
</code></pre>
    <h4 class="title">Training the network</h4>
    <p class="normal">Now, let's <a id="_idIndexMarker1348"/>start training the network. First, create the Atari game environment using <code class="Code-In-Text--PACKT-">gym</code>. Let's create a Tennis game environment:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"Tennis-v0"</span>)
</code></pre>
    <p class="normal">Create an object to our <code class="Code-In-Text--PACKT-">Categorical_DQN</code> class:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = Categorical_DQN(env)
</code></pre>
    <p class="normal">Set the number of episodes:</p>
    <pre class="programlisting code"><code class="hljs-code">num_episodes = <span class="hljs-number">800</span>
</code></pre>
    <p class="normal">For each episode:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_episodes):
</code></pre>
    <p class="normal">Set <code class="Code-In-Text--PACKT-">done</code> to <code class="Code-In-Text--PACKT-">False</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    done = <span class="hljs-literal">False</span>
</code></pre>
    <p class="normal">Initialize the return:</p>
    <pre class="programlisting code"><code class="hljs-code">    Return = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">Initialize the state by resetting the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = env.reset()
</code></pre>
    <p class="normal">While the episode is not over:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
</code></pre>
    <p class="normal">Render the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        env.render()
</code></pre>
    <p class="normal">Select an action:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = agent.select_action(state)
</code></pre>
    <p class="normal">Perform the selected action:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state, reward, done, info = env.step(action)
</code></pre>
    <p class="normal">Update the return:</p>
    <pre class="programlisting code"><code class="hljs-code">        Return = Return + reward
</code></pre>
    <p class="normal">Store the <a id="_idIndexMarker1349"/>transition information in the replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code">        replay_buffer.append([state, reward, [action], next_state])
</code></pre>
    <p class="normal">If the length of the replay buffer is greater than or equal to the buffer size then start training the network by sampling transitions from the replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> len(replay_buffer) &gt;= batch_size:
            trans = sample_transitions(batch_size)
            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> trans:
                agent.train(item[<span class="hljs-number">0</span>],item[<span class="hljs-number">1</span>], item[<span class="hljs-number">2</span>], item[<span class="hljs-number">3</span>],gamma)
</code></pre>
    <p class="normal">Update the state to the next state:</p>
    <pre class="programlisting code"><code class="hljs-code">        state = next_state
</code></pre>
    <p class="normal">Print the return obtained in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    print(<span class="hljs-string">"Episode:{}, Return: {}"</span>.format(i,Return))
</code></pre>
    <p class="normal">Now that we have learned how a categorical DQN works and how to implement it, in the next section, we will learn about another interesting algorithm.</p>
    <h1 id="_idParaDest-389" class="title">Quantile Regression DQN</h1>
    <p class="normal">In this <a id="_idIndexMarker1350"/>section, we will look into another interesting distributional RL algorithm called QR-DQN. It is a distributional DQN algorithm similar to the categorical DQN; however, it has several features that make it more advantageous than a categorical DQN. </p>
    <h2 id="_idParaDest-390" class="title">Math essentials </h2>
    <p class="normal">Before <a id="_idIndexMarker1351"/>going ahead, let's recap two important concepts that we use in QR-DQN:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Quantile</strong></li>
      <li class="bullet"><strong class="keyword">Inverse cumulative distribution function</strong> (<strong class="keyword">Inverse CDF</strong>)</li>
    </ul>
    <h3 id="_idParaDest-391" class="title">Quantile </h3>
    <p class="normal">When we <a id="_idIndexMarker1352"/>divide our distribution into equal areas of <a id="_idIndexMarker1353"/>probability, they are called quantiles. For instance, as <em class="italic">Figure 14.18</em> shows, we have divided our distribution into two equal areas of probabilities and we have two quantiles with 50% probability each:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.18: 2-quantile plot</p>
    <h3 id="_idParaDest-392" class="title">Inverse CDF (quantile function)</h3>
    <p class="normal">To <a id="_idIndexMarker1354"/>understand an <strong class="keyword">inverse cumulative distribution function</strong> (<strong class="keyword">inverse CDF</strong>), first, let's <a id="_idIndexMarker1355"/>learn what a <strong class="keyword">cumulative distribution function</strong> (<strong class="keyword">CDF</strong>) is.</p>
    <p class="normal">Consider <a id="_idIndexMarker1356"/>a random variable <em class="italic">X</em>, and <em class="italic">P</em>(<em class="italic">X</em>) denotes the probability distribution of <em class="italic">X</em>. Then the cumulative distribution function is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_133.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">It basically implies that <em class="italic">F</em>(<em class="italic">x</em>) can be obtained by adding up all the probabilities that are less than or equal to <em class="italic">x</em>.</p>
    <p class="normal">Let's look at the following CDF:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.19: CDF</p>
    <p class="normal">In the <a id="_idIndexMarker1357"/>preceding plot, <img src="../Images/B15558_14_134.png" alt="" style="height: 0.84em;"/> represents the cumulative <a id="_idIndexMarker1358"/>probability, that is, <img src="../Images/B15558_14_135.png" alt="" style="height: 1.11em;"/>. Say <em class="italic">i</em> =1, then <img src="../Images/B15558_14_136.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">The CDF takes <em class="italic">x</em> as an input and returns the cumulative probability <img src="../Images/B15558_14_137.png" alt="" style="height: 0.84em;"/>. Hence, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_138.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Say <em class="italic">x</em> = 2, then we get <img src="../Images/B15558_14_139.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, we will look at the inverse CDF. Inverse CDF, as the name suggests, is the inverse of the CDF. That is, in CDF, given the support <em class="italic">x</em>, we obtain the cumulative probability <img src="../Images/B15558_14_140.png" alt="" style="height: 0.84em;"/>, whereas <a id="_idIndexMarker1359"/>in inverse CDF, given the cumulative <a id="_idIndexMarker1360"/>probability <img src="../Images/B15558_10_026.png" alt="" style="height: 0.84em;"/>, we obtain the support <em class="italic">x</em>. Inverse CDF can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_142.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">The following plot shows the inverse CDF:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_20.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.20: Inverse CDF</p>
    <p class="normal">As shown in <em class="italic">Figure 14.20</em>, given the cumulative probability <img src="../Images/B15558_14_143.png" alt="" style="height: 0.84em;"/>, we obtain the support <em class="italic">x</em>.</p>
    <p class="normal">Say <img src="../Images/B15558_14_144.png" alt="" style="height: 1.11em;"/>, then we get <em class="italic">x</em> = 2.</p>
    <p class="normal">We have learned that the quantiles are equally divided probabilities. As <em class="italic">Figure 14.20 </em>shows, we have three quantiles <em class="italic">q</em><sub class="Subscript--PACKT-">1</sub> to <em class="italic">q</em><sub class="Subscript--PACKT-">3</sub> with equally divided probabilities and the quantile values are [0.3,0.6,1.0], which are just our cumulative probabilities. Hence, we can say <a id="_idIndexMarker1361"/>that the inverse CDF (quantile function) helps us to obtain the value of support given the equally divided probabilities. Note that <a id="_idIndexMarker1362"/>in inverse CDF, the support should always be increasing as it is based on the cumulative probability.</p>
    <p class="normal">Now that we have learned what the quantile function is, we will gain an understanding of how we can make use of the quantile function in the distributional RL setting using an algorithm called QR-DQN.</p>
    <h2 id="_idParaDest-393" class="title">Understanding QR-DQN</h2>
    <p class="normal">In categorical DQN (C51), we learned that in order to predict the value distribution, the network <a id="_idIndexMarker1363"/>takes the support of the distribution as input and returns the probabilities. </p>
    <p class="normal">To compute the support, we also need to decide the number of support <em class="italic">N</em>, the minimum value of support <img src="../Images/B15558_14_022.png" alt="" style="height: 1.11em;"/>, and the maximum value of support <img src="../Images/B15558_14_146.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">If you recollect in C51, our support values are equally spaced at fixed locations <img src="../Images/B15558_14_147.png" alt="" style="height: 1.11em;"/> and we feed this equally spaced support as input and obtained the non-uniform probabilities <img src="../Images/B15558_14_148.png" alt="" style="height: 1.11em;"/>. As <em class="italic">Figure 14.21</em> shows, in C51, we feed the equally spaced support <img src="../Images/B15558_14_149.png" alt="" style="height: 1.11em;"/> as input to the network along with the state(s) and obtain the non-uniform probabilities <img src="../Images/B15558_14_150.png" alt="" style="height: 1.11em;"/> as output:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_21.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.21: Categorical DQN</p>
    <p class="normal">QR-DQN can be viewed just as the opposite of C51. In QR-DQN, to estimate the value distribution, we feed the uniform probabilities <img src="../Images/B15558_14_151.png" alt="" style="height: 1.11em;"/> and the network outputs the supports at variable locations <img src="../Images/B15558_14_152.png" alt="" style="height: 1.11em;"/>. As shown in the following figure, we feed the <a id="_idIndexMarker1364"/>uniform probabilities <img src="../Images/B15558_14_153.png" alt="" style="height: 1.11em;"/> as input to the network along with the state(s) and obtain the support <img src="../Images/B15558_14_154.png" alt="" style="height: 1.11em;"/> placed at variable locations as output:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_22.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.22: QR-DQN</p>
    <p class="normal">Thus, from the two preceding figures we can observe that, in a categorical DQN, along with the state, we feed the fixed support at equally spaced intervals as input to the network and it returns the non-uniform probabilities, whereas in a QR-DQN, along with the state, we feed the fixed uniform probabilities as input to the network and it returns the support at variable locations (unequally spaced support).</p>
    <p class="normal">Okay, but what's the use of this? How does a QR-DQN work exactly? Let's explore this in detail.</p>
    <p class="normal">We understood that a QR-DQN takes the uniform probabilities as input and returns the support values for estimating the value distribution. Can we make use of the quantile function to estimate the value distribution? Yes! We learned that the quantile function helps us to obtain the values of support given the equally divided probabilities. Thus, in QR-DQN, we <a id="_idIndexMarker1365"/>estimate the value distribution by estimating the quantile function. </p>
    <p class="normal">The quantile function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_155.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where <em class="italic">z</em> is the support and <img src="../Images/B15558_14_143.png" alt="" style="height: 0.84em;"/> is the equally divided cumulative probability. Thus, we can obtain the support <em class="italic">z</em> given <img src="../Images/B15558_14_157.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">Let <em class="italic">N</em> be the number of quantiles, then the probability can be obtained as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_158.png" alt="" style="height: 2.13em;"/></figure>
    <p class="normal">For example, if <em class="italic">N</em> = 4, then <em class="italic">p</em> = [0.25, 0.25. 0.25, 0.25]. If <em class="italic">N</em> = 5, then p = [0.20, 0.20, 0.20, 0.20, 0.20].</p>
    <p class="normal">Once we decide the number of quantiles <em class="italic">N</em>, the cumulative probabilities <img src="../Images/B15558_14_143.png" alt="" style="height: 0.84em;"/> (quantile values) can be obtained as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_160.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">For example, if <em class="italic">N</em> = 4, then <img src="../Images/B15558_14_161.png" alt="" style="height: 1.11em;"/>. If <em class="italic">N</em> = 5, then<img src="../Images/B15558_14_162.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">We just feed this equally divided cumulative probability <img src="../Images/B15558_14_163.png" alt="" style="height: 0.84em;"/> (quantile values) as input to the QR-DQN and it returns the support value. That is, we have learned that the QR-DQN estimates the value distribution as the quantile function, so we just feed the <img src="../Images/B15558_14_164.png" alt="" style="height: 0.84em;"/> and obtain the support values <em class="italic">z</em> of the value distribution.</p>
    <p class="normal">Let's <a id="_idIndexMarker1366"/>understand this with a simple example. Say we are in a state <em class="italic">s</em> and we have two possible actions <em class="italic">up</em> and <em class="italic">down</em> to perform in the state. As shown in the following figure, along with giving the state <em class="italic">s</em> as input to the network, we also feed the quantile value <img src="../Images/B15558_14_165.png" alt="" style="height: 0.84em;"/>, which is just the equally divided cumulative probability. Then our network returns the support for the distribution of action <em class="italic">up</em> and the distribution of action <em class="italic">down</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_23.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.23: QR-DQN</p>
    <p class="normal">If you recollect, in C51, we computed the probability <em class="italic">p</em>(<em class="italic">s</em>, <em class="italic">a</em>) for the given state and action, whereas here in QR-DQN, we compute the support <em class="italic">z</em>(<em class="italic">s</em>, <em class="italic">a</em>) for the given state and action. </p>
    <div class="note">
      <p class="Information-Box--PACKT-">Note that we use capital <em class="italic">Z</em>(<em class="italic">s</em>, <em class="italic">a</em>) to represent the value distribution and small <em class="italic">z</em>(<em class="italic">s</em>, <em class="italic">a</em>) to represent the support of the distribution.</p>
    </div>
    <p class="normal">Similarly, we can also compute the target value distribution using the quantile function. Then we train our network by minimizing the distance between the predicted quantile and the target quantile distribution.</p>
    <p class="normal">Still, the <a id="_idIndexMarker1367"/>fundamental question is why are we doing this? How it is more beneficial than C51? There are several advantages of quantile regression DQN over categorical DQN. In quantile regression DQN:</p>
    <ul>
      <li class="bullet">We don't have to choose the number of supports and the bounds of support, which is <img src="../Images/B15558_14_009.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_14_167.png" alt="" style="height: 1.11em;"/>.</li>
      <li class="bullet">There are no limitations on the bounds of support, thus the range of returns can vary across states.</li>
      <li class="bullet">We can also get rid of the projection step that we performed in the C51 to match the supports of the target and predicted distribution.</li>
    </ul>
    <p class="normal">One more important advantage of a QR-DQN is that it minimizes the p-Wasserstein distance between the predicted and target distribution. But why is this important? Minimizing the Wasserstein distance between the target and predicted distribution helps us in attaining convergence better than minimizing the cross entropy. </p>
    <p class="normal">Okay, what exactly is the p-Wasserstein distance? The p-Wasserstein distance, <em class="italic">W</em><sub class="" style="font-style: italic;">p</sub>, is characterized as the <em class="italic">L</em><sup class="" style="font-style: italic;">p</sup> metric on inverse CDF. Say we have two distributions <em class="italic">U</em> and <em class="italic">V</em>, then the p-Wasserstein metric between these two distributions is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_168.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_14_169.png" alt="" style="height: 1.4em;"/> and <img src="../Images/B15558_14_170.png" alt="" style="height: 1.4em;"/> denote the inverse CDF of the distributions <em class="italic">U</em> and <em class="italic">V</em> respectively. Thus, minimizing the distance between two inverse CDFs implies that we minimize the Wasserstein distance.</p>
    <p class="normal">We learned that in QR-DQN, we train the network by minimizing the distance between the predicted and target distribution, and both of them are quantile functions (inverse CDF). Thus, minimizing the distance between the predicted and target distribution (inverse CDFs) implies that we minimize the Wasserstein distance. </p>
    <p class="normal">The authors <a id="_idIndexMarker1368"/>of the QR-DQN paper (see the <em class="italic">Further reading</em> section for more details) also highlighted that instead of computing the support for the quantile values <img src="../Images/B15558_14_157.png" alt="" style="height: 0.84em;"/>, they suggest using the quantile midpoint values <img src="../Images/B15558_14_172.png" alt="" style="height: 1.11em;"/>. The quantile midpoint can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_173.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">That is, the value of the support <em class="italic">z</em> can be obtained using quantile midpoint values as <img src="../Images/B15558_14_174.png" alt="" style="height: 1.2em;"/> instead of obtaining support using the quantile values as <img src="../Images/B15558_14_175.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">But why the quantile midpoint? The quantile midpoint acts as a unique minimizer, that is, the Wasserstein distance between two inverse CDFs will be less when we use quantile midpoint values <img src="../Images/B15558_14_176.png" alt="" style="height: 1.11em;"/> instead of quantile values <img src="../Images/B15558_14_157.png" alt="" style="height: 0.84em;"/>. Since we are trying to minimize the Wasserstein distance between the target and predicted distribution, we can use quantile midpoints <img src="../Images/B15558_14_178.png" alt="" style="height: 1.11em;"/> so that the distance between them will be less. For instance, as <em class="italic">Figure 14.24</em> shows, the Wasserstein distance is less when we use the quantile midpoint values <img src="../Images/B15558_14_179.png" alt="" style="height: 1.11em;"/> instead of quantile values <img src="../Images/B15558_14_164.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_24.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.24: Using quantile midpoint values instead of quantile values</p>
    <p class="normal">Source (<a href="https://arxiv.org/pdf/1710.10044.pdf"><span class="url">https://arxiv.org/pdf/1710.10044.pdf</span></a>)</p>
    <p class="normal">In a <a id="_idIndexMarker1369"/>nutshell, in QR-DQNs, we compute the value distribution as a quantile function. So, we just feed the cumulative probabilities that are equally divided probabilities into the network and obtain the support values of the distribution and we train the network by minimizing the Wasserstein distance between the target and predicted distribution.</p>
    <h3 id="_idParaDest-394" class="title">Action selection</h3>
    <p class="normal">Action selection in QR-DQN is just the same as in C51. First, we extract Q value from the predicted <a id="_idIndexMarker1370"/>value distribution and then we select the action as the one that has the maximum Q value. We can extract the Q value by just taking the expectation of the value distribution. The expectation of distribution is given as a sum of support multiplied by their corresponding probability.</p>
    <p class="normal">In C51, we computed the Q value as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_027.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Where <em class="italic">p</em><sub class="" style="font-style: italic;">i</sub>(<em class="italic">s</em>, <em class="italic">a</em>) is the probability given by the network for state <em class="italic">s</em> and action <em class="italic">a</em> and <em class="italic">z</em><sub class="" style="font-style: italic;">i</sub> is the support.</p>
    <p class="normal">Whereas in a QR-DQN, our network outputs the support instead of the probability. So, the Q value in the QR-DQN can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_182.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Where <em class="italic">z</em><sub class="" style="font-style: italic;">i</sub>(<em class="italic">s</em>, <em class="italic">a</em>) is the support given by the network for state <em class="italic">s</em> and action <em class="italic">a</em> and <em class="italic">p</em><sub class="" style="font-style: italic;">i</sub> is the probability.</p>
    <p class="normal">After <a id="_idIndexMarker1371"/>computing the Q value, we select the action that has the maximum Q value. For instance, let's say, we have a state <em class="italic">s</em> and two actions in the state, let them be <em class="italic">up</em> and <em class="italic">down</em>.<em class="italic"> </em>The Q value for action <em class="italic">up</em> in the state <em class="italic">s</em> is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_183.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">The Q value for action <em class="italic">down</em> in the state <em class="italic">s</em> is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_184.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">After computing the Q value, we select the optimal action as the one that has the maximum Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_185.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">Now that <a id="_idIndexMarker1372"/>we have learned how to select actions in QR-DQN, in the next section, we will look into the loss function of QR-DQN. </p>
    <h3 id="_idParaDest-395" class="title">Loss function</h3>
    <p class="normal">In C51, we used cross entropy loss as our loss function because our network predicts the <a id="_idIndexMarker1373"/>probability of the value distribution. So we used the cross entropy loss to minimize the probabilities between the target and predicted distribution. But in QR-DQN, we predict the support of the distribution instead of the probabilities. That is, in QR-DQN, we feed the probabilities as input and predict the support as output. So, how can we define the loss function for a QR-DQN?</p>
    <p class="normal">We can use the quantile regression loss to minimize the distance between the target support and the predicted support. But first, let's understand how to calculate the target support value.</p>
    <p class="normal">Before going ahead, let's recall how we compute the target value in a DQN. In DQN, we use the Bellman equation and compute the target value as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_186.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">In the preceding equation, we select action <img src="../Images/B15558_14_187.png" alt="" style="height: 1.2em;"/> by taking the maximum Q value over all possible next state-action pairs.</p>
    <p class="normal">Similarly, in QR-DQN, to compute the target value, we can use the distributional Bellman equation. The distributional Bellman equation can be given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_188.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">So, the target support <em class="italic">z</em><sub class="" style="font-style: italic;">j</sub> can be computed as: </p>
    <figure class="mediaobject"><img src="../Images/B15558_14_189.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">To compute support <em class="italic">z</em><sub class="" style="font-style: italic;">j</sub> for the state <img src="../Images/B15558_14_036.png" alt="" style="height: 1.2em;"/>, we also need to select some action <img src="../Images/B15558_14_132.png" alt="" style="height: 1.2em;"/>. How can we select an action? We just compute the return distribution of all next state-action pairs using the target network and select the action <img src="../Images/B15558_14_192.png" alt="" style="height: 1.11em;"/> that has the maximum Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_193.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Now that we have learned how to compute the target support value, let's see how to compute the quantile regression loss. The advantage of using the quantile regression loss is that it adds a penalty to the overestimation and underestimation error. Let's understand this with an example.</p>
    <p class="normal">Let's say <a id="_idIndexMarker1374"/>the target support value is [1, 5, 10, 15, 20] and the predicted support value is [100, 5, 10, 15, 20]. As we can see, our predicted support has a very high value in the initial quantile and then it is decreasing. In the inverse CDF section, we learned that support should always be increasing as it is based on the cumulative probability. But if you look at the predicted values the support starts from 100 and then decreases. </p>
    <p class="normal">Let's consider another case. Suppose the target support value is [1, 5, 10, 15, 20] and the predicted support value is [1, 5, 10, 15, 4]. As we can see, our predicted support value is increasing from the initial quantile and then it is decreasing to 4 in the final quantile. But this should not happen. Since we are using inverse CDF, our support values should always be increasing.</p>
    <p class="normal">Thus, we need to make sure that our support should be increasing and not decreasing. So, if the initial quantile values are overestimated with high values and if the later quantile values are underestimated with low values, we can penalize them. That is, we multiply the overestimated value by <img src="../Images/B15558_14_163.png" alt="" style="height: 0.84em;"/> and the underestimated value by <img src="../Images/B15558_14_195.png" alt="" style="height: 1.11em;"/>. Okay, how can we determine if the value is overestimated or underestimated?</p>
    <p class="normal">First, we compute the difference between the target and the predicted value. Let <em class="italic">u</em> be the difference between the target support value and the predicted support value. Then, if the <a id="_idIndexMarker1375"/>value of <em class="italic">u </em>is less than 0, we multiply <em class="italic">u</em> by <img src="../Images/B15558_14_196.png" alt="" style="height: 1.11em;"/>, else we multiply <em class="italic">u</em> by <img src="../Images/B15558_10_071.png" alt="" style="height: 0.84em;"/>. This is known as <strong class="keyword">quantile regression loss</strong>.</p>
    <p class="normal">But the problem with quantile regression loss is that it will not be smooth at 0 and it makes the gradient stay constant. So, instead of using quantile regression loss, we use a new modified version of loss called quantile Huber loss.</p>
    <p class="normal">To understand how exactly quantile Huber loss works, first, let's look into the Huber loss. Let's denote the difference between our actual and predicted values as <em class="italic">u</em>. Then the Huber loss <img src="../Images/B15558_14_198.png" alt="" style="height: 1.11em;"/> can be given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_199.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">Let <img src="../Images/B15558_14_200.png" alt="" style="height: 1.11em;"/>, then, when the absolute value, <img src="../Images/B15558_14_201.png" alt="" style="height: 1.11em;"/>, is less than or equal to <img src="../Images/B15558_14_202.png" alt="" style="height: 0.93em;"/>, the Huber loss is <a id="_idIndexMarker1376"/>given as the quadratic loss, <img src="../Images/B15558_14_203.png" alt="" style="height: 1.2em;"/>, else it is a linear loss, <img src="../Images/B15558_14_204.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">The following Python snippet helps us to understand Huber loss better:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">huber_loss</span><span class="hljs-function">(</span><span class="hljs-params">target,predicted, kappa=</span><span class="hljs-number">1</span><span class="hljs-function">):</span>
    <span class="hljs-comment">#compute u as difference between target and predicted value</span>
    u = target – predicted
    <span class="hljs-comment">#absolute value of u</span>
    abs_u = abs(u)
    <span class="hljs-comment">#compute quadratic loss</span>
    quad_loss = <span class="hljs-number">0.5</span> * (abs_u ** <span class="hljs-number">2</span>) 
    <span class="hljs-comment">#compute linear loss</span>
    linear_loss = kappa * (abs_u - <span class="hljs-number">0.5</span> * kappa)
    <span class="hljs-comment">#true where the absolute value is less than or equal to kappa</span>
    flag = abs_u &lt;= kappa
    <span class="hljs-comment">#Loss is the quadratic loss where the absolute value is less than kappa </span>
    <span class="hljs-comment">#else it is linear loss</span>
    loss = (flag) * quad_loss + (~flag) * linear_loss
    
    <span class="hljs-keyword">return</span> loss
</code></pre>
    <p class="normal">Now that we have understood what the Huber loss <img src="../Images/B15558_14_198.png" alt="" style="height: 1.11em;"/> is, let's look into the quantile Huber loss. In the quantile Huber loss, when the value of <em class="italic">u</em> (the difference between target <a id="_idIndexMarker1377"/>and predicted support) is less than 0, then we multiply the Huber loss <img src="../Images/B15558_14_198.png" alt="" style="height: 1.11em;"/> by <img src="../Images/B15558_14_207.png" alt="" style="height: 1.11em;"/>, and when the value of <em class="italic">u</em> is greater than or equal to 0, we multiply the Huber loss <img src="../Images/B15558_14_198.png" alt="" style="height: 1.11em;"/> by <img src="../Images/B15558_14_157.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">Now that we have understood how a QR-DQN works, in the next section, we will look into another interesting algorithm called D4PG.</p>
    <h1 id="_idParaDest-396" class="title">Distributed Distributional DDPG</h1>
    <p class="normal"><strong class="keyword">D4PG</strong>, which stands for <strong class="keyword">D</strong>istributed <strong class="keyword">D</strong>istributional <strong class="keyword">D</strong>eep <strong class="keyword">D</strong>eterministic <strong class="keyword">P</strong>olicy <strong class="keyword">G</strong>radient, is one <a id="_idIndexMarker1378"/>of the most <a id="_idIndexMarker1379"/>interesting policy gradient algorithms. We can make a guess about how D4PG works just by its name. As the name suggests, D4PG is basically a combination of <strong class="keyword">deep deterministic policy gradient</strong> (<strong class="keyword">DDPG</strong>) and distributional reinforcement learning, and it works in a distributed fashion. Confused? Let's go deeper and understand how D4PG works in detail.</p>
    <p class="normal">To understand how D4PG works, it is highly recommended to revise the DDPG algorithm we covered in <em class="chapterRef">Chapter 12</em>,<em class="italic"> Learning DDPG, TD3, and SAC</em>. We learned that DDPG is an actor critic method where the actor tries to learn the policy while the critic tries to evaluate the policy produced by the actor using the Q function. The critic uses the deep Q network for estimating the Q function and the actor uses the policy network for computing the policy. Thus, the actor performs an action while the critic gives feedback to the action performed by the actor and, based on the critic feedback, the actor network will be updated.</p>
    <p class="normal">D4PG works <a id="_idIndexMarker1380"/>just like DDPG but in the critic network, instead of using a DQN for estimating the Q function, we can use our distributional DQN to estimate the value distribution. That is, in the previous sections, we have learned several distributional DQN algorithms, such as C51 and QR-DQN. So, in the critic network, instead of using a regular DQN, we can use any distributional DQN algorithm, say C51.</p>
    <p class="normal">Apart from this, D4PG also proposes several changes to the DDPG architecture. So, we will get into the details and learn how exactly D4PG differs from DDPG. Before going ahead, let's be clear with the notation:</p>
    <ul>
      <li class="bullet">The policy network parameter is represented by <img src="../Images/B15558_13_234.png" alt="" style="height: 1.11em;"/> and the target policy network parameter is represented by <img src="../Images/B15558_12_210.png" alt="" style="height: 1.2em;"/>.</li>
      <li class="bullet">The critic network parameter is represented by <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> and the target critic network parameter is represented by <img src="../Images/B15558_14_039.png" alt="" style="height: 1.2em;"/>.</li>
      <li class="bullet">Since we are talking about a deterministic policy, let's represent it by <img src="../Images/B15558_14_214.png" alt="" style="height: 0.84em;"/>, and our policy is parameterized by the policy network, so we can denote the policy by <img src="../Images/B15558_14_215.png" alt="" style="height: 1.02em;"/>.</li>
    </ul>
    <p class="normal">Now, we will understand how exactly the critic and actor network in D4PG works. </p>
    <h2 id="_idParaDest-397" class="title">Critic network</h2>
    <p class="normal">In DDPG, we learned that we use the critic network to estimate the Q function. Thus, given a <a id="_idIndexMarker1381"/>state and action, the critic <a id="_idIndexMarker1382"/>network estimates the Q function as <img src="../Images/B15558_14_216.png" alt="" style="height: 1.11em;"/>. To train the critic network we minimize the MSE between the target Q value given by the Bellman optimality equation and the Q value predicted by the network.</p>
    <p class="normal">The target value in DDPG is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_217.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Once we compute the target value, we compute the loss as the MSE between the target value and the predicted value as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_047.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <em class="italic">K</em> denotes the number of transitions randomly sampled from the replay buffer. After computing the loss, we compute the gradients <img src="../Images/B15558_14_219.png" alt="" style="height: 1.11em;"/> and update the critic network parameter using gradient descent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_052.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Now, let's talk about the critic in D4PG. As we learned in D4PG, we use the distributional DQN to estimate the Q value. Thus, given a state and action, the critic network estimates the value distribution as <img src="../Images/B15558_14_221.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">To train the critic network, we minimize the distance between the target value distribution given by the distributional Bellman equation and the value distribution predicted by the network.</p>
    <p class="normal">The target value distribution in D4PG is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_222.png" alt="" style="height: 1.4em;"/></figure>
    <p class="normal">As you <a id="_idIndexMarker1383"/>can observe, equation (2) is similar to (1) except that we just replaced <em class="italic">Q</em> with <em class="italic">Z</em>, indicating that we are computing the target value distribution. D4PG <a id="_idIndexMarker1384"/>proposes one more change to the target value computation (2). Instead of using the one-step return <em class="italic">r</em>, we use the <strong class="keyword">N-step return</strong>, and it can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_223.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Where <em class="italic">N</em> is the length of the transition, which we sample from the replay buffer.</p>
    <p class="normal">After computing the target value distribution, we can compute the distance between the target value distribution and the predicted value distribution as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_224.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <em class="italic">d</em> denotes any distance measure for measuring the distance between two distributions. Say we are using C51, then <em class="italic">d</em> denotes the cross entropy and <em class="italic">K</em> denotes the number of transitions sampled from the replay buffer. After computing the loss, we calculate the gradients and update the critic network parameter. The gradients can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_225.png" alt="" style="height: 3.07em;"/></figure>
    <p class="normal">D4PG proposes a small change to our gradient updates. In D4PG, we use a <strong class="keyword">prioritized experience replay. </strong>Let's say we have an experience replay buffer of size <em class="italic">R</em>. Each transition in the replay buffer will have a non-uniform probability <em class="italic">p</em><sub class="" style="font-style: italic;">i</sub>. The non-uniform probability helps us to give more importance to one transition than the other. Say we have a sample <em class="italic">i</em>, then its probability can be given as <img src="../Images/B15558_14_226.png" alt="" style="height: 2.4em;"/> or <img src="../Images/B15558_14_227.png" alt="" style="height: 1.29em;"/>. While updating the critic <a id="_idIndexMarker1385"/>network, we weight the updates using <img src="../Images/B15558_14_228.png" alt="" style="height: 1.2em;"/>, which gives importance to the updates.</p>
    <p class="normal">Thus <a id="_idIndexMarker1386"/>our gradient computation becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_229.png" alt="" style="height: 3.07em;"/></figure>
    <p class="normal">After computing the gradient, we can update the critic network parameter using gradient descent as <img src="../Images/B15558_12_052.png" alt="" style="height: 1.11em;"/>. Now that we have understood how the critic network works in D4PG, let's look into the actor network in the next section.</p>
    <h2 id="_idParaDest-398" class="title">Actor network</h2>
    <p class="normal">First, let's <a id="_idIndexMarker1387"/>quickly recap <a id="_idIndexMarker1388"/>how the actor network in DDPG works. In DDPG, we learned that the actor network takes the state as input and returns the action:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_231.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Note that we are using the deterministic policy in the continuous action space, and to explore new actions we just add some noise <img src="../Images/B15558_14_232.png" alt="" style="height: 1.11em;"/> to the action produced by the actor network since the action is a continuous value.</p>
    <p class="normal">So, our modified action can be represented as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_233.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Thus, the <a id="_idIndexMarker1389"/>objective function of the actor is to generate an action that maximizes the Q value produced by the citric network:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_234.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_14_235.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">We learned <a id="_idIndexMarker1390"/>that to maximize the objective, we compute the gradients of our objective function <img src="../Images/B15558_11_014.png" alt="" style="height: 1.29em;"/> and update the actor network parameter by performing gradient ascent.</p>
    <p class="normal">Now let's come to D4PG. In D4PG we perform the same steps with a little difference. Note that here we are not using the Q function in the critic. Instead, we are computing the value distribution and thus our objective function becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_14_237.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where the action, <img src="../Images/B15558_14_235.png" alt="" style="height: 1.29em;"/> and just like we saw in DDPG, to maximize the objective, first, we compute the gradients of our objective function <img src="../Images/B15558_14_238.png" alt="" style="height: 1.29em;"/>. After computing the gradients we update the actor network parameter by performing gradient ascent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_068.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">We learned that D4PG is a <strong class="keyword">distributed</strong> algorithm, meaning that instead of using one actor, we use <em class="italic">L</em> <strong class="keyword">number of actors</strong>, each of which acts parallel and is independent of the environment, collects experience, and stores the experience in the replay buffer. Then we <a id="_idIndexMarker1391"/>update the network <a id="_idIndexMarker1392"/>parameter to the actors periodically.</p>
    <p class="normal">Thus, to summarize, D4PG is similar to DDPG except for the following:</p>
    <ol>
      <li class="numbered" value="1">We use the distributional DQN in the critic network instead of using the regular DQN to estimate the Q values.</li>
      <li class="numbered">We calculate <em class="italic">N</em>-step returns in the target instead of calculating the one-step return.</li>
      <li class="numbered">We use a prioritized experience replay and add importance to the gradient update in the critic network.</li>
      <li class="numbered">Instead of using one actor, we use <strong class="keyword">L</strong> independent actors, each of which acts in parallel, collects experience, and stores the experience in the replay buffer.</li>
    </ol>
    <p class="normal">Now that we have understood how D4PG works, putting together all the concepts we have learned, let's look into the algorithm of D4PG in the next section. </p>
    <h2 id="_idParaDest-399" class="title">Algorithm – D4PG</h2>
    <p class="normal">Let <img src="../Images/B15558_14_240.png" alt="" style="height: 1.11em;"/> denote the time steps at which we want to update the target critic and actor network <a id="_idIndexMarker1393"/>parameters. We set <img src="../Images/B15558_14_241.png" alt="" style="height: 1.2em;"/>, which states that we update the target critic network and target actor network parameter for every 2 steps of the episode. Similarly, let <img src="../Images/B15558_14_242.png" alt="" style="height: 1.02em;"/><strong class="keyword"> </strong>denote the<strong class="keyword"> </strong>time steps at which we want to replicate the network weights to the <strong class="keyword">L</strong> actors. We set <img src="../Images/B15558_14_243.png" alt="" style="height: 1.11em;"/>, which states that we replicate the network weights to the actors on every 2 steps of the episode.</p>
    <p class="normal">The algorithm of D4PG is given as follows:</p>
    <ol>
      <li class="numbered" value="1">Initialize the critic network parameter <img src="../Images/B15558_09_056.png" alt="" style="height: 1.11em;"/> and actor network parameter <img src="../Images/B15558_14_245.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize target critic network parameter <img src="../Images/B15558_14_246.png" alt="" style="height: 1.2em;"/> and target actor network parameter <img src="../Images/B15558_14_247.png" alt="" style="height: 1.4em;"/> by copying from <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_14_249.png" alt="" style="height: 1.11em;"/> respectively</li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_14_098.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Launch the <em class="italic">L</em> number of actors</li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat <em class="italic">step 6</em></li>
      <li class="numbered">For each step in the episode, that is, for <img src="../Images/B15558_14_101.png" alt="" style="height: 1.11em;"/>:<ol>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_12_266.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the target value distribution of the critic, that is,<figure class="mediaobject"><img src="../Images/B15558_14_253.png" alt="" style="height: 3.51em;"/></figure>
          </li>
          <li class="numbered-l2">Compute the loss of the critic network and calculate the gradient as <img src="../Images/B15558_14_229.png" alt="" style="height: 3.07em;"/></li>
          <li class="numbered-l2">After computing the gradients, update the critic network parameter using gradient descent: <img src="../Images/B15558_14_255.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the gradient of the actor network <img src="../Images/B15558_11_014.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Update the actor network parameter by gradient ascent:<img src="../Images/B15558_14_257.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <img src="../Images/B15558_14_258.png" alt="" style="height: 1.2em;"/>, then:<p class="numbered-l2">Update the target critic and target actor network parameter using soft replacement as <img src="../Images/B15558_14_259.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_14_260.png" alt="" style="height: 1.2em;"/> respectively</p>
          </li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <img src="../Images/B15558_14_261.png" alt="" style="height: 1.11em;"/>, then:<p class="bullet-para-l2">Replicate the network weights to the actors</p>
          </li>
        </ol>
      </li>
    </ol>
    <p class="normal">And <a id="_idIndexMarker1394"/>we perform the following steps in the actor network:</p>
    <ol>
      <li class="numbered" value="1">Select action <em class="italic">a</em> based on the policy <img src="../Images/B15558_14_262.png" alt="" style="height: 1.4em;"/> and exploration noise, that is,<img src="../Images/B15558_14_233.png" alt="" style="height: 1.29em;"/></li>
      <li class="numbered">Perform the selected action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_14_264.png" alt="" style="height: 1.2em;"/>, get the reward <em class="italic">r</em>, and store the transition information in the replay buffer <img src="../Images/B15558_09_092.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 1</em> to <em class="italic">2</em> until the learner finishes</li>
    </ol>
    <p class="normal">Thus, we <a id="_idIndexMarker1395"/>have learned how D4PG works.</p>
    <h1 id="_idParaDest-400" class="title">Summary</h1>
    <p class="normal">We started the chapter by understanding how distributional reinforcement learning works. We learned that in distributional reinforcement learning, instead of selecting an action based on the expected return, we select the action based on the distribution of return, which is often called the value distribution or return distribution.</p>
    <p class="normal">Next, we learned about the categorical DQN algorithm, also known as C51, where we feed the state and support of the distribution as the input and the network returns the probabilities of the value distribution. We also learned how the projection step matches the support of the target and predicted the value distribution so that we can apply the cross entropy loss.</p>
    <p class="normal">Going ahead, we learned about quantile regression DQNs, where we feed the state and also the equally divided cumulative probabilities <img src="../Images/B15558_14_157.png" alt="" style="height: 0.84em;"/> as input to the network and it returns the support value of the distribution.</p>
    <p class="normal">At the end of the chapter, we learned about how D4PG works, and we also learned how it varies from DDPG. </p>
    <h1 id="_idParaDest-401" class="title">Questions</h1>
    <p class="normal">Let's test our knowledge of distributional reinforcement learning by answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">What is distributional reinforcement learning?</li>
      <li class="numbered">What is a categorical DQN?</li>
      <li class="numbered">Why is the categorical DQN called the C51 algorithm?</li>
      <li class="numbered">What is the quantile function?</li>
      <li class="numbered">How does a QR-DQN differ from a categorical DQN?</li>
      <li class="numbered">How does D4PG differ from DDPG?</li>
    </ol>
    <h1 id="_idParaDest-402" class="title">Further reading</h1>
    <p class="normal">For more information, refer to the following papers:</p>
    <ul>
      <li class="bullet"><strong class="keyword">A Distributional Perspective on Reinforcement Learning </strong>by <em class="italic">Marc G. Bellemare</em>, <em class="italic">Will Dabney</em>, <em class="italic">Remi Munos</em>, <a href="https://arxiv.org/pdf/1707.06887.pdf"><span class="url">https://arxiv.org/pdf/1707.06887.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Distributional Reinforcement Learning with Quantile Regression </strong>by <em class="italic">Will Dabney</em>, <em class="italic">Mark Rowland</em>, <em class="italic">Marc G. Bellemare</em>, <em class="italic">Rémi Munos</em>, <a href="https://arxiv.org/pdf/1710.10044.pdf"><span class="url">https://arxiv.org/pdf/1710.10044.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Distributed Distributional Deep Deterministic Policy Gradient </strong>by <em class="italic">Gabriel Barth-Maron</em>, <em class="italic">et al</em>., <a href="https://arxiv.org/pdf/1804.08617.pdf"><span class="url">https://arxiv.org/pdf/1804.08617.pdf</span></a></li>
    </ul>
  </div>
</body></html>