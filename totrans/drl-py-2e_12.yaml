- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning DDPG, TD3, and SAC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about interesting actor-critic methods,
    such as **Advantage Actor-Critic** (**A2C**) and **Asynchronous Advantage Actor-Critic**
    (**A3C**). In this chapter, we will learn several state-of-the-art actor-critic
    methods. We will start off the chapter by understanding one of the popular actor-critic
    methods called **Deep Deterministic Policy Gradient** (**DDPG**). DDPG is used
    only in continuous environments, that is, environments with a continuous action
    space. We will understand what DDPG is and how it works in detail. We will also
    learn the DDPG algorithm step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we will learn about the **Twin Delayed Deep Deterministic Policy
    Gradient** (**TD3**). TD3 is an improvement over the DDPG algorithm and includes
    several interesting features that solve the problems faced in DDPG. We will understand
    the key features of TD3 in detail and also look into the algorithm of TD3 step
    by step.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will learn about another interesting actor-critic algorithm, called
    **Soft Actor-Critic (SAC)**. We will learn what SAC is and how it works using
    the entropy term in the objective function. We will look into the actor and critic
    components of SAC in detail and then learn the algorithm of SAC step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep deterministic policy gradient (DDPG)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of DDPG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DDPG algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twin delayed deep deterministic policy gradient (TD3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key features of TD3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TD3 algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soft actor-critic (SAC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of SAC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SAC algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep deterministic policy gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DDPG is an off-policy, model-free algorithm, designed for environments where
    the action space is continuous. In the previous chapter, we learned how the actor-critic
    method works. DDPG is an actor-critic method where the actor estimates the policy
    using the policy gradient, and the critic evaluates the policy produced by the
    actor using the Q function.
  prefs: []
  type: TYPE_NORMAL
- en: DDPG uses the policy network as an actor and deep Q network as a critic. One
    important difference between the DPPG and actor-critic algorithms we learned in
    the previous chapter is that DDPG tries to learn a deterministic policy instead
    of a stochastic policy.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will get an intuitive understanding of how DDPG works and then we
    will look into the algorithm in detail.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of DDPG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DDPG is an actor-critic method that takes advantage of both the policy-based
    method and the value-based method. It uses a deterministic policy ![](img/B15558_12_001.png)
    instead of a stochastic policy ![](img/B15558_03_139.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that a deterministic policy tells the agent to perform one particular
    action in a given state, meaning a deterministic policy maps the state to one
    particular action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Whereas a stochastic policy maps the state to the probability distribution
    over the action space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_004.png)'
  prefs: []
  type: TYPE_IMG
- en: In a deterministic policy, whenever the agent visits the state, it always performs
    the same particular action. But with a stochastic policy, instead of performing
    the same action every time the agent visits the state, the agent performs a different
    action each time based on a probability distribution over the action space.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will look into an overview of the actor and critic networks in the DDPG
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Actor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The actor in DDPG is basically the policy network. The goal of the actor is
    to learn the mapping between the state and action. That is, the role of the actor
    is to learn the optimal policy that gives the maximum return. So, the actor uses
    the policy gradient method to learn the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Critic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The critic is basically the value network. The goal of the critic is to evaluate
    the action produced by the actor network. How does the critic network evaluate
    the action produced by the actor network? Let's suppose we have a Q function;
    can we evaluate an action using the Q function? Yes! First, let's take a little
    detour and recap the use of the Q function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the Q function gives the expected return that an agent would obtain
    starting from state *s* and performing an action *a* following a particular policy.
    The expected return produced by the Q function is often called the Q value. Thus,
    given a state and action, we obtain a Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: If the Q value is high, then we can say that the action performed in that state
    is a good action. That is, if the Q value is high, meaning the expected return
    is high when we perform an action *a* in state *s*, we can say that the action
    *a* is a good action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the Q value is low, then we can say that the action performed in that state
    is not a good action. That is, if the Q value is low, meaning the expected return
    is low when we perform an action *a* in state *s*, we can say that the action
    *a* is not a good action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Okay, now how can the critic network evaluate an action produced by the actor
    network based on the Q function (Q value)? Let's suppose the actor network performs
    a *down* action in state **A**. So, now, the critic computes the Q value of moving
    *down* in state **A**. If the Q value is high, then the critic network gives feedback
    to the actor network that the action *down* is a good action in state **A**. If
    the Q value is low, then the critic network gives feedback to the actor network
    that the *down* action is not a good action in state **A,** and so the actor network
    tries to perform a different action in state **A**.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, with the Q function, the critic network can evaluate the action performed
    by the actor network. But wait, how can the critic network learn the Q function?
    Because only if it knows the Q function can it evaluate the action performed by
    the actor. So, how does the critic network learn the Q function? Here is where
    we use the **deep Q network** (**DQN**). We learned that with the DQN, we can
    use the neural network to approximate the Q function. So, now, we use the DQN
    as the critic network to compute the Q function.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in a nutshell, DDPG is an actor-critic method and so it takes advantage
    of policy-based and value-based methods. DDPG consists of an actor that is a policy
    network and uses the policy gradient method to learn the optimal policy and the
    critic, which is a deep Q network, and it evaluates the action produced by the
    actor.
  prefs: []
  type: TYPE_NORMAL
- en: DDPG components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of how the DDPG algorithm works, let's
    go into further detail. We will understand how exactly the actor and critic networks
    work by looking at them separately.
  prefs: []
  type: TYPE_NORMAL
- en: Critic network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned that the critic network is basically the DQN and it uses the DQN
    to estimate the Q value. Now, let's learn how the critic network uses the DQN
    to estimate the Q value in more detail, along with a recap of the DQN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The critic evaluates the action produced by the actor. Thus, the input to the
    critic will be the state and also the action produced by the actor in that state,
    and the critic returns the Q value of the given state-action pair, as shown *Figure
    12.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: The critic network'
  prefs: []
  type: TYPE_NORMAL
- en: To approximate the Q value in the critic, we can use the deep neural network,
    and if we use the deep neural network to approximate the Q value, then the network
    is called the DQN. Since we are using the neural network to approximate the Q
    value in the critic, we can represent the Q function with ![](img/B15558_12_005.png),
    where ![](img/B15558_12_006.png) is the parameter of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in the critic network, we approximate the Q value using the DQN and the
    parameter of the critic network is represented by ![](img/B15558_12_006.png),
    as shown in *Figure 12.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: The critic network'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from *Figure 12.2*, given state *s* and the action *a* produced
    by the actor, the critic network returns the Q value.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how to obtain the action *a* produced by the actor. We learned
    that the actor is basically the policy network and it uses a policy gradient to
    learn the optimal policy. In DDPG, we learn a deterministic policy instead of
    a stochastic policy, so we can denote the policy with ![](img/B15558_12_008.png)
    instead of ![](img/B15558_12_009.png). The parameter of the actor network is represented
    by ![](img/B15558_11_043.png). So, we can represent our parameterized policy as
    ![](img/B15558_12_011.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a state *s* as the input, the actor network returns the action *a* to
    be performed in that state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the critic network takes state *s* and action ![](img/B15558_12_012.png)
    produced by the actor network in that state as input and returns the Q value,
    as shown in *Figure 12.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: The critic network'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we train the critic network (DQN)? We generally train the network
    by minimizing the loss as the difference between the target value and predicted
    value. So, we can train the critic network by minimizing the loss as the difference
    between the target Q value and the Q value predicted by the network. But how can
    we obtain the target Q value? The target Q value is the optimal Q value and we
    can obtain the optimal Q value using the Bellman equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that the optimal Q function (Q value) can be obtained by using the
    Bellman optimality equation. Thus, the optimal Q function can be obtained using
    the Bellman optimality equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that ![](img/B15558_12_015.png) represents the immediate reward *r*
    we obtain while performing an action *a* in state *s* and moving to the next state
    ![](img/B15558_12_016.png), so we can just denote ![](img/B15558_12_015.png) with
    *r*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we can remove the expectation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer and
    taking the average value. We will learn more about this in a while. So, we can
    express the target Q value as the sum of the immediate reward and discounted maximum
    Q value of the next state-action pair, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can represent the loss function of the critic network as the difference
    between the target value (optimal Bellman Q value) and the predicted value (the
    Q value predicted by the critic network):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the action *a* is the action produced by the actor network, that is,![](img/B15558_12_021.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using the loss as simply the difference between the target value
    and the predicted value, we can use the mean squared error as our loss function.
    We know that in the DQN, we use the replay buffer and store the transitions as
    ![](img/B15558_12_022.png). So, we randomly sample a minibatch of *K* number of
    transitions from the replay buffer and train the network by minimizing the mean
    squared loss between the target value (optimal Bellman Q value) and the predicted
    value (Q value predicted by the critic network). Thus, our loss function is given
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_023.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding equation, we can observe that both the target and predicted
    Q functions are parameterized by the same parameter ![](img/B15558_12_006.png).
    This will cause instability in the mean squared error and the network will learn
    poorly.
  prefs: []
  type: TYPE_NORMAL
- en: So, we introduce another neural network to learn the target value, and it is
    usually referred to as the target critic network. The parameter of the target
    critic network is represented by ![](img/B15558_12_025.png). Our main critic network,
    which is used to predict Q values, learns the correct parameter ![](img/B15558_12_006.png)
    using gradient descent. The target critic network parameter ![](img/B15558_12_025.png)
    is updated by just copying the parameter of the main critic network ![](img/B15558_12_006.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the loss function of the critic network can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_029.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember that the action *a*[i] in the preceding equation is the action produced
    by the actor network, that is, ![](img/B15558_12_030.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a small problem in the target value computation in our loss function
    due to the presence of the max term, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: The max term means that we compute the Q value of all possible actions ![](img/B15558_12_031.png)
    in state ![](img/B15558_12_016.png) and select the action ![](img/B15558_12_031.png)
    as the one that has the maximum Q value. But when the action space is continuous,
    we cannot compute the Q value of all possible actions ![](img/B15558_12_031.png)
    in state ![](img/B15558_12_016.png). So, we need to get rid of the max term in
    our loss function. How can we do that?
  prefs: []
  type: TYPE_NORMAL
- en: Just as we use the target network in the critic, we can use a target actor network,
    and the parameter of the target actor network is denoted by ![](img/B15558_12_042.png).
    Now, instead of selecting the action ![](img/B15558_12_031.png) as the one that
    has the maximum Q value, we can generate an action ![](img/B15558_12_031.png)
    using the target actor network, that is, ![](img/B15558_12_039.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, as shown in *Figure 12.4*, to compute the Q value of the next state-action
    pair in the target, we feed state ![](img/B15558_12_016.png) and the action ![](img/B15558_12_031.png)
    produced by the target actor network parameterized by ![](img/B15558_12_042.png)
    to the target critic network, and it returns the Q value of the next state-action
    pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: The target critic network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in our loss function, equation (1), we can remove the max term and instead
    of ![](img/B15558_12_031.png), we can write ![](img/B15558_12_044.png), as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_045.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To maintain a uniform notation, let''s represent the loss function with *J*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_046.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To reduce the clutter, we can denote the target value with *y* and write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_047.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *y*[i] is the target value of the critic, that is, ![](img/B15558_12_048.png),
    and the action *a*[i] is the action produced by the main actor network, that is,
    ![](img/B15558_12_030.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'To minimize the loss, we compute the gradients of the objective function ![](img/B15558_12_050.png)
    and update the main critic network parameter ![](img/B15558_12_006.png) by performing
    gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_052.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Okay, what about the target critic network parameter ![](img/B15558_12_025.png)?
    How can we update it? We can update the parameter of the target critic network
    by just copying the parameter of the main critic network parameter ![](img/B15558_09_098.png)
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_055.png)'
  prefs: []
  type: TYPE_IMG
- en: This is usually called the soft replacement and the value of ![](img/B15558_12_056.png)
    is often set to 0.001.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we learned how the critic network uses the DQN to compute the Q value
    to evaluate the action produced by the actor network. In the next section, we
    will learn how the actor network learns the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Actor network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already learned that the actor network is the policy network and it
    uses the policy gradient to compute the optimal policy. We also learned that we
    represent the parameter of the actor network with ![](img/B15558_11_043.png),
    and so the parameterized policy is represented with ![](img/B15558_12_011.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The actor network takes state *s* as an input and returns the action *a*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_059.png)'
  prefs: []
  type: TYPE_IMG
- en: One important point that we may want to note down here is that we are using
    a deterministic policy. Since we are using a deterministic policy, we need to
    take care of the exploration-exploitation dilemma, because we know that a deterministic
    policy always selects the same action and doesn't explore new actions, unlike
    a stochastic policy, which selects different actions based on the probability
    distribution over the action space.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we explore new actions while using a deterministic policy? Note
    that DDPG is designed for an environment where the action space is continuous.
    Thus, we are using a deterministic policy in the continuous action space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the discrete action space, in the continuous action space, we have continuous
    values. So, to explore new actions, we can just add some noise ![](img/B15558_12_060.png)
    to the action produced by the actor network since the action is a continuous value.
    We generate this noise using a process called the Ornstein-Uhlenbeck random process.
    So, our modified action can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_061.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, say the action ![](img/B15558_12_062.png) produced by the actor
    network is 13\. Suppose the noise ![](img/B15558_12_060.png) is 0.1, then our
    action becomes *a* = 13+0.1 = 13.1.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that the critic network is represented by ![](img/B15558_12_064.png)
    and it evaluates the action produced by the actor using the Q value. If the Q
    value is high, then the critic tells the actor that it has produced a good action
    but when the Q value is low, then the critic tells the actor that it has produced
    a bad action.
  prefs: []
  type: TYPE_NORMAL
- en: But wait! We learned that it is difficult to compute the Q value when the action
    space is continuous. That is, when the action space is continuous, it is difficult
    to compute the Q value of all possible actions in the state and take the maximum
    Q value. That is why we resorted to the policy gradient method. But now, we are
    computing the Q value with a continuous action space. How will this work?
  prefs: []
  type: TYPE_NORMAL
- en: Note that, here in DDPG, we are not computing the Q value of all possible state-action
    pairs. We simply compute the Q value of state *s* and action *a* produced by the
    actor network.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the actor is to make the critic tell that the action it has produced
    is a good action. That is, the actor wants to get good feedback from the critic
    network. When does the critic give good feedback to the actor? The critic gives
    good feedback when the action produced by the actor has a maximum Q value. That
    is, if the action produced by the actor has a maximum Q value, then the critic
    tells the actor that it has produced a good action. So, the actor tries to generate
    an action in such a way that it can maximize the Q value produced by the critic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the objective function of the actor is to generate an action that maximizes
    the Q value produced by the critic network. So, we can write the objective function
    of the actor as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_065.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the action ![](img/B15558_12_066.png). Maximizing the above objective
    function ![](img/B15558_12_067.png) implies that we are maximizing the Q value
    produced by the critic network. Okay, how can we maximize the preceding objective
    function? We can maximize the objective function by performing gradient ascent
    and update the actor network parameter as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_068.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Wait. Instead of updating the actor network parameter ![](img/B15558_11_043.png)
    just for a single state ![](img/B15558_12_070.png), we sample ![](img/B15558_12_071.png)
    number of states from the replay buffer ![](img/B15558_12_072.png) and update
    the parameter. So, now our objective function becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_073.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the action ![](img/B15558_12_074.png). Maximizing the preceding objective
    function implies that the actor tries to generate actions in such a way that it
    maximizes the Q value over all the sampled states. We can maximize the objective
    function by performing gradient ascent and update the actor network parameter
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_068.png)'
  prefs: []
  type: TYPE_IMG
- en: To summarize, the objective of the actor is to generate action in such a way
    that it maximizes the Q value produced by the critic. So, we perform gradient
    ascent and update the actor network parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, what about the parameter of the target actor network? How can we update
    it? We can update the parameter of the target actor network by just copying the
    parameter of the main actor network parameter ![](img/B15558_11_043.png) by soft
    replacement, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_077.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have understood how actor and critic networks work, let's get a
    good understanding of what we have learned so far and how DDPG works exactly by
    putting all the concepts together.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To avoid getting lost in notations, first, let''s recollect the notations to
    understand DDPG better. We use four networks, two actor networks and two critic
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: The main critic network parameter is represented by ![](img/B15558_12_006.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target critic network parameter is represented by ![](img/B15558_12_025.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main actor network parameter is represented by ![](img/B15558_11_043.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target actor network parameter is represented by ![](img/B15558_12_042.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that DDPG is an actor-critic method, and so its parameters will be updated
    at every step of the episode, unlike the policy gradient method, where we generate
    complete episodes and then update the parameter. Okay, let's get started and understand
    how DDPG works.
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize the main critic network parameter ![](img/B15558_12_006.png)
    and the main actor network parameter ![](img/B15558_11_043.png) with random values.
    We learned that the target network parameter is just a copy of the main network
    parameter. So, we initialize the target critic network parameter ![](img/B15558_12_025.png)
    by just copying the main critic network parameter ![](img/B15558_12_006.png).
    Similarly, we initialize the target actor network parameter ![](img/B15558_12_042.png)
    by just copying the main actor network parameter ![](img/B15558_11_043.png). We
    also initialize the replay buffer ![](img/B15558_12_072.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each step in the episode, first, we select an action, *a*, using the
    actor network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_059.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, instead of using the action *a* directly, to ensure exploration, we
    add some noise ![](img/B15558_12_060.png), and so the action becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_061.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_12_016.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_12_072.png).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we randomly sample a minibatch of *K* transitions (*s*, *a*, *r*, *s'*)
    from the replay buffer. These *K* transitions will be used for updating both our
    critic and actor network.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us compute the loss of the critic network. We learned that the loss
    function of the critic network is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_047.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *y*[i] is the target value of the critic, that is, ![](img/B15558_12_095.png),
    and the action *a*[i] is the action produced by the actor network, that is, ![](img/B15558_12_074.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the loss of the critic network, we compute the gradients ![](img/B15558_12_050.png)
    and update the critic network parameter ![](img/B15558_12_006.png) using gradient
    descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let us update the actor network. We learned that the objective function
    of the actor network is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_100.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that in the above equation, we are only using the state (*s*[i]) from
    the sampled *K* transitions (*s*, *a*, *r*, *s''*). The action *a* is selected
    by actor network, ![](img/B15558_12_074.png). Now, we need to maximize the preceding
    objective function. Maximizing the above objective function helps the actor to
    generate actions in such a way that it maximizes the Q value produced by the critic.
    We can maximize the objective function by computing the gradients of our objective
    function ![](img/B15558_10_093.png) and update the actor network parameter ![](img/B15558_11_043.png)
    using gradient ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_103.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And then, in the final step, we update the parameter of the target critic network
    ![](img/B15558_12_025.png) and the parameter of the target actor network ![](img/B15558_12_042.png)
    by soft replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_106.png)![](img/B15558_12_107.png)'
  prefs: []
  type: TYPE_IMG
- en: We repeat these steps for several episodes. Thus, for each step in the episode,
    we update the parameter of our networks. Since the parameter gets updated at every
    step, our policy will also be improved at every step in the episode.
  prefs: []
  type: TYPE_NORMAL
- en: To have a better understanding of how DDPG works, let's look into the DDPG algorithm
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – DDPG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DDPG algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main critic network parameter ![](img/B15558_12_006.png) and
    the main actor network parameter ![](img/B15558_11_043.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target critic network parameter ![](img/B15558_12_025.png) by
    just copying the main critic network parameter ![](img/B15558_12_006.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target actor network parameter ![](img/B15558_12_042.png) by
    just copying the main actor network parameter ![](img/B15558_12_042.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_12_072.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat steps 6 and 7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize an Ornstein-Uhlenbeck random process ![](img/B15558_12_060.png) for
    an action space exploration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0,…,*T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select action *a* based on the policy ![](img/B15558_12_062.png) and exploration
    noise, that is, ![](img/B15558_12_117.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_016.png),
    get the reward *r*, and store this transition information in the replay buffer
    ![](img/B15558_12_072.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_088.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value of the critic, that is, ![](img/B15558_12_095.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network, ![](img/B15558_12_047.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the loss ![](img/B15558_12_050.png) and update the critic
    network parameter using gradient descent, ![](img/B15558_12_099.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the actor network ![](img/B15558_10_093.png) and update
    the actor network parameter by gradient ascent, ![](img/B15558_12_126.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic and target actor network parameter as ![](img/B15558_12_127.png)
    and ![](img/B15558_12_107.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Swinging up a pendulum using DDPG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, let's implement the DDPG algorithm to train the agent to swing
    up a pendulum. That is, we will have a pendulum that starts swinging from a random
    position and the goal of our agent is to swing the pendulum up so it stays upright.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Creating the Gym environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s create a pendulum environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the state shape of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the action shape of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the pendulum is a continuous environment, and thus our action space
    consists of continuous values. Hence, we get the bounds of our action space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Defining the variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's define some of the important variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the discount factor, ![](img/B15558_05_056.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the value of ![](img/B15558_12_056.png), which is used for soft replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the size of our replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Defining the DDPG class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s define the class called `DDPG`, where we will implement the DDPG algorithm.
    To aid understanding, let''s look into the code line by line. You can also access
    the complete code from the GitHub repository of the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Defining the init method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, let''s define the `init` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the replay buffer for storing the transitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `num_transitions` to `0`, which means that the number of transitions
    in our replay buffer is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We learned that in DDPG, instead of selecting the action *a* directly, to ensure
    exploration, we add some noise ![](img/B15558_12_060.png) using the Ornstein-Uhlenbeck
    process. So, we first initialize the noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, initialize the state shape, action shape, and high action value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With the actor variable scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the main actor network, which is parameterized by ![](img/B15558_11_043.png).
    The actor network takes the state as an input and returns the action to be performed
    in that state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the target actor network that is parameterized by ![](img/B15558_12_042.png).
    The target actor network takes the next state as an input and returns the action
    to be performed in that state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'With the critic variable scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the main critic network, which is parameterized by ![](img/B15558_12_006.png).
    The critic network takes the state and also the action produced by the actor in
    that state as an input and returns the Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the target critic network, which is parameterized by ![](img/B15558_12_025.png).
    The target critic network takes the next state and also the action produced by
    the target actor network in that next state as an input and returns the Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the parameter of the main actor network ![](img/B15558_11_043.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the parameter of the target actor network ![](img/B15558_12_042.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the parameter of the main critic network ![](img/B15558_12_006.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the parameter of the target critic network ![](img/B15558_12_025.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the soft replacement, update the parameter of the target actor network
    as ![](img/B15558_12_107.png), and update the parameter of the target critic network
    as ![](img/B15558_12_127.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the target Q value. We learned that the target Q value can be computed
    as the sum of reward and the discounted Q value of the next state-action pair,
    ![](img/B15558_12_095.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compute the loss of the critic network. The loss of the critic
    network is the mean squared error between the target Q value and the predicted
    Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_047.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can define the mean squared error as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the critic network by minimizing the mean squared error using the Adam
    optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We learned that the objective function of the actor is to generate an action
    that maximizes the Q value produced by the critic network, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_100.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the action ![](img/B15558_12_074.png), and we can maximize this objective
    by computing gradients and by performing gradient ascent. However, it is a standard
    convention to perform minimization rather than maximization. So, we can convert
    the preceding maximization objective into a minimization objective by just adding
    a negative sign. Hence, we can define the actor network objective as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_145.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can minimize the actor network objective by computing gradients and
    by performing gradient descent. Thus, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the actor network by minimizing the loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all the TensorFlow variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Selecting the action
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `select_action` to select the action with the
    noise to ensure exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the actor network and get the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we generate a normal distribution with the mean as the action and the
    standard deviation as the noise and we randomly select an action from this normal
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to make sure that our action should not fall away from the action bound.
    So, we clip the action so that it lies within the action bound and then we return
    the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Defining the train function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s define the train function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the soft replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomly select indices from the replay buffer with the given batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the batch of transitions from the replay buffer with the selected indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the batch of states, actions, rewards, and next states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the actor network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the critic network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Storing the transitions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s store the transitions in the replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'First, stack the state, action, reward, and next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the transition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the number of transitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If the number of transitions is greater than the replay buffer, train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Building the actor network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We define a function called `build_actor_network` to build the actor network.
    The actor network takes the state and returns the action to be performed in that
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Building the critic network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We define a function called `build_critic_network` to build the critic network.
    The critic network takes the state and the action produced by the actor in that
    state and returns the Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Training the network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s start training the network. First, let''s create an object for
    our DDPG class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of time steps in each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'For each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'For every step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the transition in the replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'If the state is the terminal state, then break:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the state to the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the return for every 10 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'By rendering the environment, we can observe how the agent learns to swing
    up the pendulum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: The Gym pendulum environment'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how DDPG works and how to implement it, in the next
    section, we will learn about another interesting algorithm called twin delayed
    DDPG.
  prefs: []
  type: TYPE_NORMAL
- en: Twin delayed DDPG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will look into another interesting actor-critic algorithm, known as
    TD3\. TD3 is an improvement (and basically a successor) to the DDPG algorithm
    we just covered.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we learned how DDPG uses a deterministic policy to
    work on the continuous action space. DDPG has several advantages and has been
    successfully used in a variety of continuous action space environments.
  prefs: []
  type: TYPE_NORMAL
- en: We understood that DDPG is an actor-critic method where an actor is a policy
    network and it finds the optimal policy, while the critic evaluates the policy
    produced by the actor by estimating the Q function using a DQN.
  prefs: []
  type: TYPE_NORMAL
- en: One of the problems with DDPG is that the critic overestimates the target Q
    value. This overestimation causes several issues. We learned that the policy is
    improved based on the Q value given by the critic, but when the Q value has an
    approximation error, it causes stability issues to our policy and the policy may
    converge to local optima.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to combat this, TD3 proposes three important features, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Clipped double Q learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delayed policy updates
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Target policy smoothing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, we will understand how TD3 works intuitively, and then we will look at
    the algorithm in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Key features of TD3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TD3 is essentially the same as DDPG, except that it proposes three important
    features to mitigate the problems in DDPG. In this section, let''s first get a
    basic understanding of the key features of TD3\. The three key features of TD3
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clipped double Q learning**:Instead of using one critic network, we use two
    main critic networks to compute the Q value and also use two target critic networks
    to compute the target value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compute two target Q values using two target critic networks and use the
    minimum value of these two while computing the loss. This helps to prevent overestimation
    of the target Q value. We will learn more about this in detail in the next section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Delayed policy updates**: In DDPG, we learned that we update the parameter
    of both the actor (policy network) and critic (DQN) network at every step of the
    episode. Unlike DDPG, here we delay updating the parameter of the actor network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That is, the critic network parameter is updated at every step of the episode,
    but the actor network (policy network) parameter is delayed and updated only after
    every two steps of the episode.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Target policy smoothing**:TheDDPG method produces different target values
    even for the same action. Hence, the variance of the target value will be high
    even for the same action, so we reduce this variance by adding some noise to the
    target action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a basic idea of the key features of TD3, we will get into more
    detail and learn how exactly these three key features work and how they solve
    the problems associated with DDPG.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clipped double Q learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Remember in *Chapter 9*, *Deep Q Network and Its Variants*, while learning
    about the DQN, we discovered that it tends to overestimate the Q value of the
    next state-action pair in the target? It is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to mitigate the overestimation, we used double Q learning. With double
    Q learning, we use two different networks, in other words, two different Q functions,
    one for selecting an action and the other to compute the Q value, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_146.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, computing the target value by using the preceding equation prevents the
    overestimation of the Q value in the DQN.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that in DDPG, the critic network is the DQN, and so it also suffers
    from the overestimation of the Q value in the target. Can we employ double Q learning
    in DDPG and try to solve the overestimation bias? Yes! But the problem is that
    in the actor-critic method, the policy and target network parameter updates happen
    slowly, and this will not help us in removing the overestimation bias.
  prefs: []
  type: TYPE_NORMAL
- en: So, we will use a slightly different version of double Q learning called *clipped
    double Q learning*. In clipped double Q learning, we use two target critic networks
    to compute the Q value.
  prefs: []
  type: TYPE_NORMAL
- en: We use the two target critic networks and compute the two Q values and select
    the minimum value out of these two to compute the target value. This helps to
    prevent overestimation bias. Let's understand this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: If we need two target critic networks, then we also need two main critic networks.
    We know that the target network parameter is just a time-delayed copy of the main
    network parameter. So, we define two main critic networks with the parameters
    ![](img/B15558_12_147.png) and ![](img/B15558_12_148.png) to compute the two Q
    values, that is, ![](img/B15558_12_149.png) and ![](img/B15558_12_150.png), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We also define the two target critic networks with parameters ![](img/B15558_12_151.png)
    and ![](img/B15558_12_152.png) to compute the two Q values of next state-action
    pair in the target, that is, ![](img/B15558_12_153.png) and ![](img/B15558_12_154.png),
    respectively. Let's understand this clearly step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'In DDPG, we learned that we compute the target value as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_155.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Computing the Q value of the next state-action pair in the target in this way
    creates overestimation bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, to avoid this, in TD3, first, we compute the Q value of the next state-action
    pair in the target using the first target critic network with a parameter ![](img/B15558_12_151.png),
    that is, ![](img/B15558_12_153.png), and then we compute the Q value of the next
    state-action pair in the target using the second target critic network with a
    parameter ![](img/B15558_12_152.png), that is, ![](img/B15558_12_154.png). Then,
    we use the minimum of these two Q values to compute the target value as expressed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_160.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the action ![](img/B15558_12_161.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can express the preceding equation simply as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_162.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the action ![](img/B15558_12_161.png).
  prefs: []
  type: TYPE_NORMAL
- en: Computing the target value in this way prevents overestimation of the Q value
    of the next state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, we computed the target value. How do we compute the loss and update the
    critic network parameter? We learned that we use two main critic networks, so,
    first, we compute the loss of the first main critic network, parameterized by
    ![](img/B15558_12_147.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_165.png)'
  prefs: []
  type: TYPE_IMG
- en: After computing the loss, we compute the gradients and update the parameter
    ![](img/B15558_12_147.png)using gradient descent as ![](img/B15558_12_167.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the loss of the second main critic network, parameterized
    by ![](img/B15558_12_148.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_169.png)'
  prefs: []
  type: TYPE_IMG
- en: After computing the loss, we compute the gradients and update the parameter
    ![](img/B15558_12_148.png)using gradient descent as ![](img/B15558_12_171.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply express the preceding updates as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_172.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After updating the two main critic network parameters, ![](img/B15558_12_147.png)and![](img/B15558_12_148.png),we
    can update the two target critic network parameters, ![](img/B15558_12_151.png)
    and ![](img/B15558_12_152.png), by soft replacement, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_177.png)![](img/B15558_12_178.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can simply express the preceding updates as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_179.png)'
  prefs: []
  type: TYPE_IMG
- en: Delayed policy updates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Delayed policy updates imply that we update the parameters of our actor network
    (policy network) less frequently than the critic networks. But why do we want
    to do that? We learned that in DDPG, actor and critic network parameters are updated
    at every step of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: When the critic network parameter is not good, then it estimates the incorrect
    Q values. If the Q value estimated by the critic network is not correct, then
    the actor network cannot update its parameter correctly. That is, we learned that
    the actor network learns based on feedback from the critic network. This feedback
    is just the Q value. When the critic network gives incorrect feedback (incorrect
    Q value), then the actor network cannot learn the correct action and cannot update
    its parameter correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to avoid this, we hold updating the parameter of the actor network for
    a while and only update the critic network to make the critic estimate the correct
    Q value. That is, we update the parameter of the critic network at every step
    of the episode, and we delay updating the parameter of the actor network, and
    only update it for some specific steps of the episode because we don't want our
    actor to learn from the incorrect critic's feedback.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, the critic network parameter is updated at every step of the
    episode, but the actor network parameter update is delayed. We generally delay
    the update by two steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, in DDPG, we learned that the objective of the actor network (policy network)
    is to maximize the Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_100.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding objective of the actor network is the same in TD3 as well. That
    is, similar to DDPG, here, the objective of the actor is to generate actions in
    such a way that it maximizes the Q value produced by the critic. But wait! Unlike
    DDPG, here we have two Q values, ![](img/B15558_12_149.png) and ![](img/B15558_12_150.png),
    since we use two critic networks with parameters ![](img/B15558_12_147.png) and
    ![](img/B15558_12_148.png), respectively. So which Q value should our actor network
    maximize? Should it be ![](img/B15558_12_149.png) or ![](img/B15558_12_150.png)?
    We can take either of these and maximize one. So, we can take ![](img/B15558_12_149.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in TD3, the objective of the actor network is to maximize the Q value,
    ![](img/B15558_12_149.png), as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_189.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Remember that in the above equation, the action *a* is selected by actor network,
    ![](img/B15558_12_074.png). In order to maximize the objective function, we compute
    the gradients of our objective function, ![](img/B15558_10_093.png), and update
    the parameter of the network using gradient ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_103.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, instead of doing this parameter update of the actor network at every time
    step of the episode, we delay the updates and update the parameter only on every
    other step (every two steps). Let *t* be the time step of the episode and *d*
    denotes the number of time steps we want to delay the update by (usually *d* is
    set to 2); then we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If *t* mod *d* =0, then:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the actor network parameter using gradient ascent ![](img/B15558_12_103.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This will be made clearer when we look at the final algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Target policy smoothing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand this, let''s first recollect how we compute the target value
    in TD3\. We learned that in TD3, we update the target value using clipped double
    Q learning with two target critic networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_162.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the action ![](img/B15558_12_161.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can notice, we compute the target values with action ![](img/B15558_12_031.png)
    generated by the target actor network, ![](img/B15558_12_199.png). Instead of
    using the action given by the target actor network directly, we add some noise
    ![](img/B15558_12_200.png) to the action and modify the action to ![](img/B15558_12_201.png),
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_202.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, −*c* to +*c* indicates that noise is clipped, so that we can keep the
    target close to the actual action. Thus, our target value computation now becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_203.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the action ![](img/B15558_12_204.png).
  prefs: []
  type: TYPE_NORMAL
- en: But why are we doing this? Why do we need to add noise to the action and use
    it to compute the target value? Similar actions should have similar target values,
    right? However, the DDPG method produces target values with high variance even
    for similar actions. This is because deterministic policies overfit to the sharp
    peaks in the value estimate. So, we can smooth out these peaks for similar actions
    by adding some noise. Thus, target policy smoothing basically acts as a regularizer
    and reduces the variance in the target values.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the key features of the TD3 algorithm, let's get
    clarity on what we have learned so far and how the TD3 algorithm works by putting
    all the concepts together.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s recollect the notations to understand TD3 better. We use six
    networks—four critic networks and two actor networks:'
  prefs: []
  type: TYPE_NORMAL
- en: The two main critic network parameters are represented by ![](img/B15558_12_205.png)
    and ![](img/B15558_12_206.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two target critic network parameters are represented by ![](img/B15558_12_207.png)
    and ![](img/B15558_12_208.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main actor network parameter is represented by ![](img/B15558_12_209.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target actor network parameter is represented by ![](img/B15558_12_210.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TD3 is an actor-critic method, and so the parameters of TD3 will get updated
    at every step of the episode, unlike the policy gradient method where we generate
    complete episodes and then update the parameter. Now, let's get started and understand
    how TD3 works.
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize the two main critic network parameters, ![](img/B15558_12_211.png)
    and ![](img/B15558_12_148.png), and the main actor network parameter ![](img/B15558_12_213.png)
    with random values. We know that the target network parameter is just a copy of
    the main network parameter. So, we initialize the two target critic network parameters
    ![](img/B15558_12_214.png) and ![](img/B15558_12_208.png) by just copying ![](img/B15558_12_216.png)
    and ![](img/B15558_12_217.png), respectively. Similarly, we initialize the target
    actor network parameter ![](img/B15558_12_218.png) by just copying the main actor
    network parameter ![](img/B15558_12_219.png). We also initialize the replay buffer
    ![](img/B15558_12_220.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each step in the episode, first, we select an action *a* using the
    actor network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_059.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But instead of using the action *a* directly, to ensure exploration, we add
    some noise ![](img/B15558_12_222.png), where ![](img/B15558_12_223.png). Thus,
    our action now becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_224.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_02_004.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_12_226.png).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we randomly sample a minibatch of *K* transitions (*s*, *a*, *r*, *s'*)
    from the replay buffer. These *K* transitions will be used for updating both our
    critic and actor network.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us compute the loss of the critic networks. We learned that the
    loss function of the critic networks is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_227.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: The action *a*[i] is the action produced by the actor network, that is, ![](img/B15558_12_228.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[i] is the target value of the critic, that is, ![](img/B15558_12_230.png),
    and the action ![](img/B15558_12_231.png) is the action produced by the target
    actor network, that is, ![](img/B15558_12_232.png) where ![](img/B15558_12_233.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After computing the loss of the critic network, we compute the gradients ![](img/B15558_12_234.png)
    and update the critic network parameter using gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_235.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let us update the actor network. We learned that the objective function
    of the actor network is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_189.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that in the above equation, we are only using the state (*s*[i]) from
    the sampled *K* transitions (*s*, *a*, *r*, *s''*). The action *a* is selected
    by actor network, ![](img/B15558_12_074.png). In order to maximize the objective
    function, we compute gradients of our objective function ![](img/B15558_10_093.png)
    and update the parameters of the network using gradient ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_103.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of doing this parameter update of the actor network at every time step
    of the episode, we delay the updates. Let *t* be the time step of the episode
    and *d* denotes the number of time steps we want to delay the update by (usually
    *d* is set to 2); then we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If *t* mod *d* = 0, then:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the actor network parameter using gradient ascent ![](img/B15558_12_103.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we update the parameter of the target critic networks ![](img/B15558_12_214.png)
    and ![](img/B15558_12_208.png) and the parameter of the target actor network ![](img/B15558_12_243.png)
    by soft replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_244.png)![](img/B15558_12_245.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is a small change in updating the parameter of the target networks. Just
    like we delay updating the actor network parameter for *d* steps, we update the
    target network parameter for every *d* step; hence, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If *t* mod *d* = 0, then:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png) and
    update the actor network parameter using gradient ascent ![](img/B15558_12_247.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic network parameter and target actor network parameter
    as ![](img/B15558_12_248.png), and ![](img/B15558_12_107.png), respectively
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We repeat the preceding steps for several episodes and improve the policy. To
    get a better understanding of how TD3 works, let's look into the TD3 algorithm
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – TD3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The TD3 algorithm is exactly similar to the DDPG algorithm except that it includes
    the three key features we learned in the previous sections. So, before looking
    into the TD3 algorithm directly, you can revise all the key features of TD3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm of TD3 is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the two main critic network parameters ![](img/B15558_12_211.png)
    and ![](img/B15558_12_217.png) and the main actor network parameter ![](img/B15558_12_252.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the two target critic network parameters ![](img/B15558_12_214.png)
    and ![](img/B15558_12_152.png) by copying the main critic network parameters ![](img/B15558_12_255.png)
    and ![](img/B15558_12_148.png), respectively
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target actor network parameter ![](img/B15558_12_218.png) by
    copying the main actor network parameter ![](img/B15558_12_218.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_12_259.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat step 6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0,…,*T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action *a* based on the policy ![](img/B15558_12_062.png) and with
    exploration noise ![](img/B15558_12_261.png), that is, ![](img/B15558_12_224.png)
    where, ![](img/B15558_12_263.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_264.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_088.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_266.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action ![](img/B15558_12_267.png) to compute the target value, ![](img/B15558_12_268.png),
    where ![](img/B15558_12_269.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value of the critic, that is, ![](img/B15558_12_230.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network, ![](img/B15558_12_227.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients of the loss ![](img/B15558_12_234.png) and minimize the
    loss using gradient descent, ![](img/B15558_12_273.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod *d* =0, then:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_12_274.png) and
    update the actor network parameter using gradient ascent, ![](img/B15558_12_126.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic network parameter and target actor network parameter
    as ![](img/B15558_12_248.png), and ![](img/B15558_12_107.png), respectively
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned how TD3 works, in the next section, we will learn about
    another interesting algorithm, called SAC.
  prefs: []
  type: TYPE_NORMAL
- en: Soft actor-critic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will look into another interesting actor-critic algorithm, called SAC.
    This is an off-policy algorithm and it borrows several features from the TD3 algorithm.
    But unlike TD3, it uses a stochastic policy ![](img/B15558_03_139.png). SAC is
    based on the concept of entropy. So first, let's understand what is meant by entropy.
    Entropy is a measure of the randomness of a variable. It basically tells us the
    uncertainty or unpredictability of the random variable and is denoted by ![](img/B15558_12_279.png).
  prefs: []
  type: TYPE_NORMAL
- en: If the random variable always gives the same value every time, then we can say
    that its entropy is low because there is no randomness. But if the random variable
    gives different values, then we can say that its entropy is high.
  prefs: []
  type: TYPE_NORMAL
- en: For an example, consider a dice throw experiment. Every time a dice is thrown,
    if we get a different number, then we can say that the entropy is high because
    we are getting a different number every time and there is high uncertainty since
    we don't know which number will come up on the next throw. But if we are getting
    the same number, say 3, every time the dice is thrown, then we can say that the
    entropy is low, since there is no randomness here as we are getting the same number
    on every throw.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the policy ![](img/B15558_03_139.png) tells what action to perform
    in a given state. What happens when the entropy of the policy ![](img/B15558_12_281.png)
    is high or low? If the entropy of the policy is high, then this means that our
    policy performs different actions instead of performing the same action every
    time. But if the entropy of the policy is low, then this means that our policy
    performs the same action every time. As you may have guessed, increasing the entropy
    of a policy promotes exploration, while decreasing the entropy of the policy means
    less exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that, in reinforcement learning, our goal is to maximize the return.
    So, we can define our objective function as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_282.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_12_283.png) is the parameter of our stochastic policy ![](img/B15558_03_139.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the return of the trajectory is just the sum of rewards, that
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_115.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can rewrite our objective function by expanding the return as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_286.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Maximizing the preceding objective function maximizes the return. In the SAC
    method, we use a slightly modified version of the objective function with the
    entropy term as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_287.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, our objective function now has two terms; one is the reward and
    the other is the entropy of the policy. Thus, instead of maximizing only the reward,
    we also maximize the entropy of a policy. But what is the point of this? Maximizing
    the entropy of the policy allows us to explore new actions. But we don't want
    to explore actions that give us a bad reward. Hence, maximizing entropy along
    with maximizing reward means that we can explore new actions along with maintaining
    maximum reward. The preceding objective function is often referred to as **maximum
    entropy reinforcement learning**, or **entropy regularized reinforcement learning.**
    Adding an entropy term is also often referred to as an entropy bonus.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the term ![](img/B15558_09_143.png) in the objective function is called
    temperature and is used to set the importance of our entropy term, or we can say
    that it is used to control exploration. When ![](img/B15558_09_143.png) is high,
    we allow exploration in the policy, but when it is low, then we don't allow exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, now that we have a basic idea of SAC, let's get into some more details.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding soft actor-critic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SAC, as the name suggests, is an actor-critic method similar to DDPG and TD3
    we learned in the previous sections. Unlike DDPG and TD3, which use deterministic
    policies, SAC uses a stochastic policy. SAC works in a very similar manner to
    TD3\. We learned that in actor-critic architecture, the actor uses the policy
    gradient to find the optimal policy and the critic evaluates the policy produced
    by the actor using the Q function.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in SAC, the actor uses the policy gradient to find the optimal policy
    and the critic evaluates the policy produced by the actor. However, instead of
    using only the Q function to evaluate the actor's policy, the critic uses both
    the Q function and the value function. But why exactly do we need both the Q function
    and the value function to evaluate the actor's policy? This will be explained
    in detail in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: So, in SAC, we have three networks, one actor network (policy network) to find
    the optimal policy, and two critic networks—a value network and a Q network, to
    compute the value function and the Q function, respectively, to evaluate the policy
    produced by the actor.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, let's look at the modified version of the value function and
    the Q function with the entropy term.
  prefs: []
  type: TYPE_NORMAL
- en: V and Q functions with the entropy term
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that the value function (state value) is the expected return of the
    trajectory starting from state *s* following a policy ![](img/B15558_03_008.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_289.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We learned that the return is the sum of rewards of the trajectory, so we can
    rewrite the preceding equation by expanding the return as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_290.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can rewrite the value function by adding the entropy term as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_291.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that the Q function (state-action value) is the expected return of
    the trajectory starting from state *s* and action *a* following a policy ![](img/B15558_03_140.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_293.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Expanding the return of the trajectory, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_294.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can rewrite the Q function by adding the entropy term as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_295.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The modified Bellman equation for the preceding Q function with the entropy
    term is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_296.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the value function can be computed using the relation between the Q function
    and the value function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_297.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To learn how exactly we obtained the equations (2) and (3), you can check the
    derivation of soft policy iteration in the paper *Soft Actor-Critic: Off-Policy
    Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor*, by Tuomas
    Haarnoja et.al.: [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Components of SAC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a basic idea of SAC, let's go into more detail and understand
    how exactly each component of SAC works by looking at them separately.
  prefs: []
  type: TYPE_NORMAL
- en: Critic network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned that unlike other actor-critic methods we have seen earlier, the
    critic in SAC uses both the value function and the Q function to evaluate the
    policy produced by the actor network. But why is that?
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous algorithms, we used the critic network to compute the Q function
    for evaluating the action produced by the actor. Also, the target Q value in the
    critic is computed using the Bellman equation. We can do the same here. However,
    here we have modified the Bellman equation of the Q function due to the entropy
    term, as we learned in equation (2):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_298.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding equation, we can observe that in order to compute the Q function,
    first we need to compute the value function. So, we need to compute both the Q
    function and the value function in order to evaluate the policy produced by the
    actor. We can use a single network to approximate both the Q function and the
    value function. However, instead of using a single network, we use two different
    networks, the Q network to estimate the Q function, and the value network to estimate
    the value function. Using two different networks to compute the Q function and
    value function stabilizes the training.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the SAC paper, "*There is no need in principle to include a
    separate function approximator (neural network) for the state value since it is
    related to the Q function and policy according to Equation 2\. But in practice,
    including a separate function approximator (neural network) for the state value
    can stabilize training and is convenient to train simultaneously with the other
    networks*."
  prefs: []
  type: TYPE_NORMAL
- en: First, we will learn how the value network works and then we will learn about
    the Q network.
  prefs: []
  type: TYPE_NORMAL
- en: Value network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The value network is denoted by *V*, the parameter of the value network is denoted
    by ![](img/B15558_12_299.png), and the parameter of the target value network is
    denoted by ![](img/B15558_12_300.png).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, ![](img/B15558_12_301.png) implies that we approximate the value function
    (state value) using the neural network parameterized by ![](img/B15558_12_302.png).
    Okay, how can we train the value network? We can train the network by minimizing
    the loss between the target state value and the state value predicted by our network.
    How can we obtain the target state value? We can use the value function given
    in equation (3) to compute the target state value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that according to equation (3), the value of the state is computed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_303.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we can remove the expectation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer.
    So, we can compute the target state value *y*[v] using the preceding equation
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_304.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we look at the preceding equation, we have a Q function. In order to compute
    the Q function, we use a Q network parameterized by ![](img/B15558_09_098.png),
    and similarly, our policy is parameterized by ![](img/B15558_12_306.png), so we
    can rewrite the preceding equation with the parameterized Q function and policy
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But if we use the preceding equation to compute the target value, the Q value
    will overestimate. So, to avoid this overestimation, we use clipped double Q learning,
    just like we learned in TD3\. That is, we compute the two Q values using two Q
    networks parameterized by ![](img/B15558_12_211.png) and ![](img/B15558_12_217.png)
    and take the minimum value of these two, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_310.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe in the preceding equation, for clipped double Q learning,
    we are using the two main Q networks parameterized by ![](img/B15558_12_311.png)
    and ![](img/B15558_12_217.png), but in TD3, we used two target Q networks parameterized
    by ![](img/B15558_12_214.png) and ![](img/B15558_12_152.png). Why is that?
  prefs: []
  type: TYPE_NORMAL
- en: Because here, we are computing the Q value of a state-action pair ![](img/B15558_12_315.png)
    so we can use the two main Q networks parameterized by ![](img/B15558_12_216.png)
    and ![](img/B15558_12_217.png), but in TD3, we compute the Q value of the next
    state-action pair ![](img/B15558_05_091.png), so we used the two target Q networks
    parameterized by ![](img/B15558_12_214.png) and ![](img/B15558_12_320.png). Thus,
    here, we don't need target Q networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply express the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_321.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can define our objective function ![](img/B15558_12_322.png) of the
    value network as the mean squared difference between the target state value and
    the state value predicted by our network, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_323.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *K* denotes the number of transitions we sample from the replay buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the gradients of our objective function and then update our
    main value network parameter ![](img/B15558_12_302.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_325.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we are using ![](img/B15558_12_326.png) to represent the learning
    rate since we are already using ![](img/B15558_07_025.png) to denote the temperature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can update the parameter of the target value network ![](img/B15558_12_328.png)
    using soft replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_329.png)'
  prefs: []
  type: TYPE_IMG
- en: We will learn where exactly the target value network is used in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Q network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Q network is denoted by *Q* and it is parameterized by ![](img/B15558_12_330.png).
    Thus, ![](img/B15558_12_331.png) implies that we approximate the Q function using
    the neural network parameterized by ![](img/B15558_09_098.png). How can we train
    the Q network? We can train the network by minimizing the loss between the target
    Q value and the Q value predicted by the network. How can we obtain the target
    Q value? Here is where we use the Bellman equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that according to the Bellman equation (2), the Q value can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_298.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can remove the expectation in the preceding equation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer.
    So, we can compute the target Q value *y*[q] using the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_334.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we look at the preceding equation, we have a value of next state ![](img/B15558_12_335.png).
    In order to compute the value of next state ![](img/B15558_12_336.png), we use
    a target value network parameterized by ![](img/B15558_12_337.png), so we can
    rewrite the preceding equation with the parameterized value function as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_338.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can define our objective function ![](img/B15558_12_339.png) of the
    Q network as the mean squared difference between the target Q value and the Q
    value predicted by the network, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_340.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *K* denotes the number of transitions we sample from the replay buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we learned that we use two Q networks parameterized
    by ![](img/B15558_12_216.png) and ![](img/B15558_12_217.png) to prevent overestimation
    bias. So, first, we compute the loss of the first Q network, parameterized by
    ![](img/B15558_12_211.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_344.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we compute the gradients and update the parameter ![](img/B15558_12_211.png)using
    gradient descent as ![](img/B15558_12_346.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the loss of the second Q network, parameterized by ![](img/B15558_12_217.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_348.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we compute the gradients and update the parameter ![](img/B15558_12_217.png)using
    gradient descent as ![](img/B15558_12_350.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply express the preceding updates as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_351.png)'
  prefs: []
  type: TYPE_IMG
- en: Actor network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The actor network (policy network) is parameterized by ![](img/B15558_11_043.png).
    Let''s recall the objective function of the actor network we learned in TD3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_100.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *a* is the action produced by the actor.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding objective function means that the goal of the actor is to generate
    action in such a way that it maximizes the Q value computed by the critic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function of the actor network in SAC is the same as what we learned
    in TD3, except that here we use a stochastic policy ![](img/B15558_12_413.png),
    and also, we maximize the entropy. So, we can write the objective function of
    the actor network in SAC as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_354.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, how can we compute the derivative of the preceding objective function?
    Because, unlike TD3, here, our action is computed using a stochastic policy. It
    will be difficult to apply backpropagation and compute gradients of the preceding
    objective function with the action computed using a stochastic policy. So, we
    use the reparameterization trick. The reparameterization trick guarantees that
    sampling from our policy is differentiable. Thus, we can rewrite our action as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_355.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, we can observe that we parameterize the policy with
    a neural network *f* and ![](img/B15558_12_200.png) is the noise sampled from
    a spherical Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we can rewrite our objective function as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_358.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that in the preceding equation, our action is ![](img/B15558_12_355.png).
    Remember how we used two Q functions parameterized by ![](img/B15558_12_359.png)
    and ![](img/B15558_12_360.png) to avoid overestimation bias? Now, which Q function
    should we use in the preceding objective function? We can use either of the functions
    and so we use the Q function parameterized by ![](img/B15558_12_216.png) and write
    our final objective function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_362.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have understood how the SAC algorithm works, let's recap what we
    have learned so far and how the SAC algorithm works exactly by putting all the
    concepts together.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s recall the notations to understand SAC better. We use five networks—four
    critic networks (two value networks and two Q networks) and one actor network:'
  prefs: []
  type: TYPE_NORMAL
- en: The main value network parameter is represented by ![](img/B15558_12_363.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target value network parameter is represented by ![](img/B15558_12_364.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two main Q network parameters are represented by ![](img/B15558_12_216.png)
    and ![](img/B15558_12_206.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actor network (policy network) parameter is represented by ![](img/B15558_10_152.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target state value is represented by *y*[v], and the target Q value is represented
    by *y*[q]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAC is an actor-critic method, and so the parameters of SAC will get updated
    at every step of the episode. Now, let's get started and understand how SAC works.
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize the main network parameter of the value network ![](img/B15558_12_363.png),
    two Q network parameters ![](img/B15558_12_216.png) and ![](img/B15558_12_206.png),
    and the actor network parameter ![](img/B15558_12_371.png). Next, we initialize
    the target value network parameter ![](img/B15558_12_364.png) by just copying
    the main network parameter ![](img/B15558_12_302.png) and then we initialize the
    replay buffer ![](img/B15558_12_374.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each step in the episode, first, we select an action *a* using the
    actor network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_375.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_12_376.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_09_124.png).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we randomly sample a minibatch of *K* transitions from the replay buffer.
    These *K* transitions (*s*, *a*, *r*, *s'*) are used for updating our value, Q,
    and actor network.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us compute the loss of the value network. We learned that the loss
    function of the value network is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_323.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_12_379.png) is the target state value and it is given as
    ![](img/B15558_12_380.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the loss, we calculate the gradients and update the parameter
    ![](img/B15558_12_302.png) of the value network using gradient descent: ![](img/B15558_12_325.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we compute the loss of the Q networks. We learned that the loss function
    of the Q network is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_383.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_12_384.png) is the target Q value and it is given as ![](img/B15558_12_385.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the loss, we calculate the gradients and update the parameter
    of the Q networks using gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we update the actor network. We learned that the objective of the actor
    network is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_362.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we calculate gradients and update the parameter ![](img/B15558_12_283.png)
    of the actor network using gradient ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_389.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, in the end, we update the target value network parameter by soft replacement,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_390.png)'
  prefs: []
  type: TYPE_IMG
- en: We repeat the preceding steps for several episodes and improve the policy. To
    get a better understanding of how SAC works, let's look into the SAC algorithm
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – SAC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SAC algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main value network parameter ![](img/B15558_12_302.png), the
    Q network parameters ![](img/B15558_12_211.png) and ![](img/B15558_12_206.png),
    and the actor network parameter ![](img/B15558_12_371.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target value network ![](img/B15558_12_395.png) by just copying
    the main value network parameter ![](img/B15558_12_302.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_12_259.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat step 5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0,…, *T* – 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an action *a* based on the policy ![](img/B15558_12_398.png), that is,
    ![](img/B15558_12_399.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_376.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_124.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target state value ![](img/B15558_12_380.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of value network ![](img/B15558_12_323.png) and update the
    parameter using gradient descent, ![](img/B15558_12_325.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target Q value ![](img/B15558_12_385.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the Q networks ![](img/B15558_12_383.png) and update the
    parameter using gradient descent, ![](img/B15558_12_386.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute gradients of the actor objective function, ![](img/B15558_10_093.png)
    and update the parameter using gradient ascent, ![](img/B15558_12_409.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target value network parameter as ![](img/B15558_12_410.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Many congratulations on learning the several important state-of-the-art actor-critic
    algorithms, including DDPG, twin delayed DDPG, and SAC. In the next chapter, we will
    examine several state-of-the-art policy gradient algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding the DDPG algorithm. We learned that
    DDPG is an actor-critic algorithm where the actor estimates the policy using policy
    gradient and the critic evaluates the policy produced by the actor using the Q
    function. We learned how DDPG uses a deterministic policy and how it is used in
    environments with a continuous action space.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we looked into the actor and critic components of DDPG in detail and
    understood how they work, before finally learning about the DDPG algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we learned about the twin delayed DDPG, which is the successor to
    DDPG and constitutes an improvement to the DDPG algorithm. We learned the key
    features of TD3, including clipped double Q learning, delayed policy updates,
    and target policy smoothing, in detail and finally, we looked into the TD3 algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about the SAC algorithm. We learned that,
    unlike DDPG and TD3, the SAC method uses a stochastic policy. We also understood
    how SAC works with the entropy bonus in the objective function, and we learned
    what is meant by maximum entropy reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn the state-of-the-art policy gradient algorithms
    such as trust region policy optimization, proximal policy optimization, and actor-critic
    using Kronecker-factored trust region.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s put our knowledge of actor-critic methods to the test. Try answering
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of actor and critic networks in DDPG?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the critic in DDPG work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the key features of TD3?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need clipped double Q learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is target policy smoothing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is maximum entropy reinforcement learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of the critic network in SAC?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, refer to the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous Control with Deep Reinforcement Learning** by *Timothy P. Lillicrap,
    et al.*, [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Addressing Function Approximation Error in Actor-Critic Methods** by *Scott
    Fujimoto, Herke van Hoof, David Meger,* [https://arxiv.org/pdf/1802.09477.pdf](https://arxiv.org/pdf/1802.09477.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
    with a Stochastic Actor** by *Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey
    Levine*, [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
