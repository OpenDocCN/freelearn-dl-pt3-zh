- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Learning DDPG, TD3, and SAC
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习 DDPG、TD3 和 SAC
- en: In the previous chapter, we learned about interesting actor-critic methods,
    such as **Advantage Actor-Critic** (**A2C**) and **Asynchronous Advantage Actor-Critic**
    (**A3C**). In this chapter, we will learn several state-of-the-art actor-critic
    methods. We will start off the chapter by understanding one of the popular actor-critic
    methods called **Deep Deterministic Policy Gradient** (**DDPG**). DDPG is used
    only in continuous environments, that is, environments with a continuous action
    space. We will understand what DDPG is and how it works in detail. We will also
    learn the DDPG algorithm step by step.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们了解了有趣的演员-评论家方法，如 **优势演员-评论家** (**A2C**) 和 **异步优势演员-评论家** (**A3C**)。在本章中，我们将学习几种最先进的演员-评论家方法。我们将从理解一种流行的演员-评论家方法
    **深度确定性策略梯度** (**DDPG**) 开始。DDPG 仅在连续环境中使用，即具有连续动作空间的环境。我们将详细了解 DDPG 是什么以及它如何工作。我们还将逐步学习
    DDPG 算法。
- en: Going forward, we will learn about the **Twin Delayed Deep Deterministic Policy
    Gradient** (**TD3**). TD3 is an improvement over the DDPG algorithm and includes
    several interesting features that solve the problems faced in DDPG. We will understand
    the key features of TD3 in detail and also look into the algorithm of TD3 step
    by step.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解 **双延迟深度确定性策略梯度** (**TD3**)。TD3 是对 DDPG 算法的改进，包括解决 DDPG 中面临的几个问题的一些有趣特性。我们将详细了解
    TD3 的关键特性，还将逐步学习 TD3 的算法。
- en: Finally, we will learn about another interesting actor-critic algorithm, called
    **Soft Actor-Critic (SAC)**. We will learn what SAC is and how it works using
    the entropy term in the objective function. We will look into the actor and critic
    components of SAC in detail and then learn the algorithm of SAC step by step.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将学习另一种有趣的演员-评论家算法，称为 **软演员-评论家 (SAC)**。我们将学习 SAC 是什么以及它如何通过目标函数中的熵项工作。我们将详细了解
    SAC 的演员和评论家组件，然后逐步学习 SAC 算法。
- en: 'In this chapter, we will learn the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Deep deterministic policy gradient (DDPG)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度 (DDPG)
- en: The components of DDPG
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDPG 的组成部分
- en: The DDPG algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDPG 算法
- en: Twin delayed deep deterministic policy gradient (TD3)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双延迟深度确定性策略梯度 (TD3)
- en: The key features of TD3
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD3 的关键特性
- en: The TD3 algorithm
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD3 算法
- en: Soft actor-critic (SAC)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软演员-评论家（SAC）
- en: The components of SAC
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAC 的组成部分
- en: The SAC algorithm
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAC 算法
- en: Deep deterministic policy gradient
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: DDPG is an off-policy, model-free algorithm, designed for environments where
    the action space is continuous. In the previous chapter, we learned how the actor-critic
    method works. DDPG is an actor-critic method where the actor estimates the policy
    using the policy gradient, and the critic evaluates the policy produced by the
    actor using the Q function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 是一种离策略、无模型的算法，专为动作空间连续的环境设计。在前一章中，我们学习了演员-评论家方法的工作原理。DDPG 是一种演员-评论家方法，其中演员使用策略梯度估计策略，评论家使用
    Q 函数评估演员产生的策略。
- en: DDPG uses the policy network as an actor and deep Q network as a critic. One
    important difference between the DPPG and actor-critic algorithms we learned in
    the previous chapter is that DDPG tries to learn a deterministic policy instead
    of a stochastic policy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 使用策略网络作为演员和深度 Q 网络作为评论家。我们在前一章中学到的 DPPG 和演员-评论家算法之间的一个重要区别是，DDPG 尝试学习确定性策略而不是随机策略。
- en: First, we will get an intuitive understanding of how DDPG works and then we
    will look into the algorithm in detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将直观地了解 DDPG 的工作原理，然后详细学习算法。
- en: An overview of DDPG
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPG 概述
- en: DDPG is an actor-critic method that takes advantage of both the policy-based
    method and the value-based method. It uses a deterministic policy ![](img/B15558_12_001.png)
    instead of a stochastic policy ![](img/B15558_03_139.png).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 是一种演员-评论家方法，充分利用了基于策略的方法和基于值的方法。它使用确定性策略 ![](img/B15558_12_001.png) 而不是随机策略
    ![](img/B15558_03_139.png)。
- en: 'We learned that a deterministic policy tells the agent to perform one particular
    action in a given state, meaning a deterministic policy maps the state to one
    particular action:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，确定性策略告诉代理在给定状态下执行一种特定的动作，这意味着确定性策略将状态映射到一个特定的动作：
- en: '![](img/B15558_12_003.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_003.png)'
- en: 'Whereas a stochastic policy maps the state to the probability distribution
    over the action space:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 而随机策略将状态映射到动作空间上的概率分布：
- en: '![](img/B15558_12_004.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_004.png)'
- en: In a deterministic policy, whenever the agent visits the state, it always performs
    the same particular action. But with a stochastic policy, instead of performing
    the same action every time the agent visits the state, the agent performs a different
    action each time based on a probability distribution over the action space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定性策略中，每当智能体访问某个状态时，它总是执行相同的特定动作。但是在随机策略中，智能体不是每次访问状态时都执行相同的动作，而是根据动作空间中的概率分布每次执行不同的动作。
- en: Now, we will look into an overview of the actor and critic networks in the DDPG
    algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将概览DDPG算法中的演员和评论员网络。
- en: Actor
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 演员
- en: The actor in DDPG is basically the policy network. The goal of the actor is
    to learn the mapping between the state and action. That is, the role of the actor
    is to learn the optimal policy that gives the maximum return. So, the actor uses
    the policy gradient method to learn the optimal policy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG中的演员本质上就是策略网络。演员的目标是学习状态与动作之间的映射关系。也就是说，演员的作用是学习能带来最大回报的最优策略。因此，演员使用策略梯度方法来学习最优策略。
- en: Critic
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评论员
- en: The critic is basically the value network. The goal of the critic is to evaluate
    the action produced by the actor network. How does the critic network evaluate
    the action produced by the actor network? Let's suppose we have a Q function;
    can we evaluate an action using the Q function? Yes! First, let's take a little
    detour and recap the use of the Q function.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员本质上就是价值网络。评论员的目标是评估演员网络所产生的动作。评论员网络如何评估演员网络产生的动作呢？假设我们有一个Q函数，我们能否使用Q函数来评估一个动作呢？当然可以！首先，让我们稍微绕一下路，回顾一下Q函数的使用。
- en: 'We know that the Q function gives the expected return that an agent would obtain
    starting from state *s* and performing an action *a* following a particular policy.
    The expected return produced by the Q function is often called the Q value. Thus,
    given a state and action, we obtain a Q value:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，Q函数给出了一个智能体从状态*s*开始并执行动作*a*时，按照特定策略所获得的预期回报。Q函数产生的预期回报通常称为Q值。因此，给定一个状态和动作，我们可以得到一个Q值：
- en: If the Q value is high, then we can say that the action performed in that state
    is a good action. That is, if the Q value is high, meaning the expected return
    is high when we perform an action *a* in state *s*, we can say that the action
    *a* is a good action.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Q值很高，我们可以说在该状态下执行的动作是一个好的动作。也就是说，如果Q值很高，意味着当我们在状态*s*中执行动作*a*时，预期回报很高，我们可以说动作*a*是一个好的动作。
- en: If the Q value is low, then we can say that the action performed in that state
    is not a good action. That is, if the Q value is low, meaning the expected return
    is low when we perform an action *a* in state *s*, we can say that the action
    *a* is not a good action.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Q值很低，我们可以说在该状态下执行的动作不是一个好的动作。也就是说，如果Q值很低，意味着当我们在状态*s*中执行动作*a*时，预期回报很低，我们可以说动作*a*不是一个好的动作。
- en: Okay, now how can the critic network evaluate an action produced by the actor
    network based on the Q function (Q value)? Let's suppose the actor network performs
    a *down* action in state **A**. So, now, the critic computes the Q value of moving
    *down* in state **A**. If the Q value is high, then the critic network gives feedback
    to the actor network that the action *down* is a good action in state **A**. If
    the Q value is low, then the critic network gives feedback to the actor network
    that the *down* action is not a good action in state **A,** and so the actor network
    tries to perform a different action in state **A**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么评论网络如何基于Q函数（Q值）来评估演员网络所产生的动作呢？假设演员网络在状态**A**下执行了一个*down*动作。那么，现在，评论网络计算在状态**A**下执行*down*动作的Q值。如果Q值很高，那么评论网络会给演员网络反馈，表示在状态**A**下，*down*动作是一个好的动作。如果Q值很低，那么评论网络会给演员网络反馈，表示在状态**A**下，*down*动作不是一个好的动作，因此演员网络会尝试在状态**A**下执行一个不同的动作。
- en: Thus, with the Q function, the critic network can evaluate the action performed
    by the actor network. But wait, how can the critic network learn the Q function?
    Because only if it knows the Q function can it evaluate the action performed by
    the actor. So, how does the critic network learn the Q function? Here is where
    we use the **deep Q network** (**DQN**). We learned that with the DQN, we can
    use the neural network to approximate the Q function. So, now, we use the DQN
    as the critic network to compute the Q function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，借助 Q 函数，评论网络可以评估演员网络执行的动作。但是，等等，评论网络怎么学习 Q 函数呢？因为只有当它知道 Q 函数时，才能评估演员执行的动作。那么，评论网络如何学习
    Q 函数呢？这就是我们使用**深度 Q 网络**（**DQN**）的地方。我们了解到，利用 DQN，可以使用神经网络来近似 Q 函数。因此，现在我们使用 DQN
    作为评论网络来计算 Q 函数。
- en: Thus, in a nutshell, DDPG is an actor-critic method and so it takes advantage
    of policy-based and value-based methods. DDPG consists of an actor that is a policy
    network and uses the policy gradient method to learn the optimal policy and the
    critic, which is a deep Q network, and it evaluates the action produced by the
    actor.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，简而言之，DDPG 是一种演员-评论方法，它结合了基于策略和基于价值的方法。DDPG 由演员（一个策略网络）和评论（一个深度 Q 网络）组成，演员通过策略梯度方法来学习最优策略，评论则评估演员产生的动作。
- en: DDPG components
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPG 组件
- en: Now that we have a basic understanding of how the DDPG algorithm works, let's
    go into further detail. We will understand how exactly the actor and critic networks
    work by looking at them separately.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 DDPG 算法的基本工作原理有了了解，接下来我们将深入探讨。通过分别查看演员和评论网络，我们将更好地理解它们的具体工作原理。
- en: Critic network
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评论网络
- en: We learned that the critic network is basically the DQN and it uses the DQN
    to estimate the Q value. Now, let's learn how the critic network uses the DQN
    to estimate the Q value in more detail, along with a recap of the DQN.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，评论网络基本上就是 DQN，它利用 DQN 来估计 Q 值。现在，让我们更详细地了解评论网络如何使用 DQN 来估计 Q 值，并回顾一下 DQN。
- en: 'The critic evaluates the action produced by the actor. Thus, the input to the
    critic will be the state and also the action produced by the actor in that state,
    and the critic returns the Q value of the given state-action pair, as shown *Figure
    12.1*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 评论网络评估演员产生的动作。因此，评论的输入将是状态以及在该状态下由演员产生的动作，评论返回给定状态-动作对的 Q 值，如*图 12.1*所示：
- en: '![](img/B15558_12_01.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_01.png)'
- en: 'Figure 12.1: The critic network'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：评论网络
- en: To approximate the Q value in the critic, we can use the deep neural network,
    and if we use the deep neural network to approximate the Q value, then the network
    is called the DQN. Since we are using the neural network to approximate the Q
    value in the critic, we can represent the Q function with ![](img/B15558_12_005.png),
    where ![](img/B15558_12_006.png) is the parameter of the network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在评论中近似 Q 值，我们可以使用深度神经网络，如果我们用深度神经网络来近似 Q 值，那么这个网络就叫做 DQN。由于我们使用神经网络来近似评论中的
    Q 值，所以我们可以用 ![](img/B15558_12_005.png) 来表示 Q 函数，其中 ![](img/B15558_12_006.png)
    是网络的参数。
- en: 'Thus, in the critic network, we approximate the Q value using the DQN and the
    parameter of the critic network is represented by ![](img/B15558_12_006.png),
    as shown in *Figure 12.2*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在评论网络中，我们使用 DQN 来近似 Q 值，评论网络的参数由 ![](img/B15558_12_006.png) 表示，如*图 12.2*所示：
- en: '![](img/B15558_12_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_02.png)'
- en: 'Figure 12.2: The critic network'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：评论网络
- en: As we can observe from *Figure 12.2*, given state *s* and the action *a* produced
    by the actor, the critic network returns the Q value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如从*图 12.2*中可以观察到，给定状态 *s* 和由演员生成的动作 *a*，评论网络返回 Q 值。
- en: Now, let's look at how to obtain the action *a* produced by the actor. We learned
    that the actor is basically the policy network and it uses a policy gradient to
    learn the optimal policy. In DDPG, we learn a deterministic policy instead of
    a stochastic policy, so we can denote the policy with ![](img/B15558_12_008.png)
    instead of ![](img/B15558_12_009.png). The parameter of the actor network is represented
    by ![](img/B15558_11_043.png). So, we can represent our parameterized policy as
    ![](img/B15558_12_011.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何获得演员产生的动作 *a*。我们了解到，演员基本上是策略网络，它通过策略梯度方法来学习最优策略。在 DDPG 中，我们学习的是确定性策略，而不是随机策略，因此我们可以用
    ![](img/B15558_12_008.png) 来表示策略，而不是 ![](img/B15558_12_009.png)。演员网络的参数由 ![](img/B15558_11_043.png)
    表示。因此，我们可以将参数化策略表示为 ![](img/B15558_12_011.png)。
- en: 'Given a state *s* as the input, the actor network returns the action *a* to
    be performed in that state:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个状态 *s* 作为输入，演员网络返回在该状态下执行的动作 *a*：
- en: '![](img/B15558_12_012.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_012.png)'
- en: 'Thus, the critic network takes state *s* and action ![](img/B15558_12_012.png)
    produced by the actor network in that state as input and returns the Q value,
    as shown in *Figure 12.3*:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，评论家网络将状态 *s* 和演员网络在该状态下产生的动作 ![](img/B15558_12_012.png) 作为输入，返回 Q 值，如 *图
    12.3* 所示：
- en: '![](img/B15558_12_03.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_03.png)'
- en: 'Figure 12.3: The critic network'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：评论家网络
- en: Okay, how can we train the critic network (DQN)? We generally train the network
    by minimizing the loss as the difference between the target value and predicted
    value. So, we can train the critic network by minimizing the loss as the difference
    between the target Q value and the Q value predicted by the network. But how can
    we obtain the target Q value? The target Q value is the optimal Q value and we
    can obtain the optimal Q value using the Bellman equation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何训练评论家网络（DQN）呢？我们通常通过最小化损失来训练网络，损失是目标值和预测值之间的差异。因此，我们可以通过最小化损失来训练评论家网络，损失是目标
    Q 值与网络预测的 Q 值之间的差异。但我们如何获得目标 Q 值呢？目标 Q 值是最优 Q 值，我们可以使用贝尔曼方程来获得最优 Q 值。
- en: 'We learned that the optimal Q function (Q value) can be obtained by using the
    Bellman optimality equation. Thus, the optimal Q function can be obtained using
    the Bellman optimality equation as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，最优 Q 函数（Q 值）可以通过使用贝尔曼最优性方程来获得。因此，最优 Q 函数可以通过以下贝尔曼最优性方程来获得：
- en: '![](img/B15558_12_014.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_014.png)'
- en: 'We know that ![](img/B15558_12_015.png) represents the immediate reward *r*
    we obtain while performing an action *a* in state *s* and moving to the next state
    ![](img/B15558_12_016.png), so we can just denote ![](img/B15558_12_015.png) with
    *r*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，![](img/B15558_12_015.png) 表示我们在状态 *s* 中执行动作 *a* 并转移到下一个状态 ![](img/B15558_12_016.png)
    时获得的即时奖励 *r*，因此我们可以将 ![](img/B15558_12_015.png) 简单表示为 *r*：
- en: '![](img/B15558_12_018.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_018.png)'
- en: 'In the preceding equation, we can remove the expectation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer and
    taking the average value. We will learn more about this in a while. So, we can
    express the target Q value as the sum of the immediate reward and discounted maximum
    Q value of the next state-action pair, as shown here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的方程中，我们可以去除期望。我们将通过从重放缓冲区中采样 *K* 数量的过渡状态并取平均值来逼近期望。稍后我们将更详细地了解这个过程。所以，我们可以将目标
    Q 值表示为即时奖励和下一个状态-动作对的折扣最大 Q 值的和，如下所示：
- en: '![](img/B15558_12_019.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_019.png)'
- en: 'Thus, we can represent the loss function of the critic network as the difference
    between the target value (optimal Bellman Q value) and the predicted value (the
    Q value predicted by the critic network):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将评论家网络的损失函数表示为目标值（最优贝尔曼 Q 值）和预测值（评论家网络预测的 Q 值）之间的差异：
- en: '![](img/B15558_12_020.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_020.png)'
- en: Here, the action *a* is the action produced by the actor network, that is,![](img/B15558_12_021.png).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，动作 *a* 是由演员网络产生的动作，即 ![](img/B15558_12_021.png)。
- en: 'Instead of using the loss as simply the difference between the target value
    and the predicted value, we can use the mean squared error as our loss function.
    We know that in the DQN, we use the replay buffer and store the transitions as
    ![](img/B15558_12_022.png). So, we randomly sample a minibatch of *K* number of
    transitions from the replay buffer and train the network by minimizing the mean
    squared loss between the target value (optimal Bellman Q value) and the predicted
    value (Q value predicted by the critic network). Thus, our loss function is given
    as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不将损失作为目标值和预测值之间的差异，而是将均方误差作为我们的损失函数。我们知道，在 DQN 中，我们使用重放缓冲区并存储过渡状态为 ![](img/B15558_12_022.png)。因此，我们从重放缓冲区中随机抽取一个
    *K* 数量的过渡状态小批量，并通过最小化目标值（最优贝尔曼 Q 值）和预测值（评论家网络预测的 Q 值）之间的均方损失来训练网络。因此，我们的损失函数表示为：
- en: '![](img/B15558_12_023.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_023.png)'
- en: From the preceding equation, we can observe that both the target and predicted
    Q functions are parameterized by the same parameter ![](img/B15558_12_006.png).
    This will cause instability in the mean squared error and the network will learn
    poorly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程中，我们可以观察到目标 Q 函数和预测的 Q 函数都由相同的参数 ![](img/B15558_12_006.png) 参数化。这将导致均方误差的不稳定，网络的学习效果会很差。
- en: So, we introduce another neural network to learn the target value, and it is
    usually referred to as the target critic network. The parameter of the target
    critic network is represented by ![](img/B15558_12_025.png). Our main critic network,
    which is used to predict Q values, learns the correct parameter ![](img/B15558_12_006.png)
    using gradient descent. The target critic network parameter ![](img/B15558_12_025.png)
    is updated by just copying the parameter of the main critic network ![](img/B15558_12_006.png).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们引入了另一个神经网络来学习目标值，通常称为目标评论家网络。目标评论家网络的参数用![](img/B15558_12_025.png)表示。我们的主要评论家网络用于预测Q值，并通过梯度下降学习正确的参数![](img/B15558_12_006.png)。目标评论家网络参数![](img/B15558_12_025.png)通过直接复制主要评论家网络的参数![](img/B15558_12_006.png)来更新。
- en: 'Thus, the loss function of the critic network can be written as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，评论家网络的损失函数可以写作：
- en: '![](img/B15558_12_029.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_029.png)'
- en: Remember that the action *a*[i] in the preceding equation is the action produced
    by the actor network, that is, ![](img/B15558_12_030.png).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，前面方程中的动作*a*[i]是由演员网络生成的动作，即！[](img/B15558_12_030.png)。
- en: 'There is a small problem in the target value computation in our loss function
    due to the presence of the max term, as shown here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最大项的存在，在目标值计算中有一个小问题，如下所示：
- en: '![](img/B15558_12_06.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_06.png)'
- en: The max term means that we compute the Q value of all possible actions ![](img/B15558_12_031.png)
    in state ![](img/B15558_12_016.png) and select the action ![](img/B15558_12_031.png)
    as the one that has the maximum Q value. But when the action space is continuous,
    we cannot compute the Q value of all possible actions ![](img/B15558_12_031.png)
    in state ![](img/B15558_12_016.png). So, we need to get rid of the max term in
    our loss function. How can we do that?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最大项意味着我们计算所有可能动作的Q值！[](img/B15558_12_031.png)，在状态！[](img/B15558_12_016.png)下，并选择具有最大Q值的动作！[](img/B15558_12_031.png)。但是，当动作空间是连续时，我们无法计算所有可能动作！[](img/B15558_12_031.png)在状态！[](img/B15558_12_016.png)下的Q值。因此，我们需要去掉损失函数中的最大项。我们该如何做呢？
- en: Just as we use the target network in the critic, we can use a target actor network,
    and the parameter of the target actor network is denoted by ![](img/B15558_12_042.png).
    Now, instead of selecting the action ![](img/B15558_12_031.png) as the one that
    has the maximum Q value, we can generate an action ![](img/B15558_12_031.png)
    using the target actor network, that is, ![](img/B15558_12_039.png).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在评论家中使用目标网络一样，我们也可以使用目标演员网络，目标演员网络的参数用![](img/B15558_12_042.png)表示。现在，我们不再选择具有最大Q值的动作！[](img/B15558_12_031.png)，而是可以使用目标演员网络生成一个动作！[](img/B15558_12_031.png)，即！[](img/B15558_12_039.png)。
- en: 'Thus, as shown in *Figure 12.4*, to compute the Q value of the next state-action
    pair in the target, we feed state ![](img/B15558_12_016.png) and the action ![](img/B15558_12_031.png)
    produced by the target actor network parameterized by ![](img/B15558_12_042.png)
    to the target critic network, and it returns the Q value of the next state-action
    pair:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如*图12.4*所示，为了计算目标中的下一个状态-动作对的Q值，我们将状态！[](img/B15558_12_016.png)和由目标演员网络（参数化为！[](img/B15558_12_042.png)）生成的动作！[](img/B15558_12_031.png)输入到目标评论家网络中，它返回下一个状态-动作对的Q值：
- en: '![](img/B15558_12_04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_04.png)'
- en: 'Figure 12.4: The target critic network'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：目标评论家网络
- en: 'Thus, in our loss function, equation (1), we can remove the max term and instead
    of ![](img/B15558_12_031.png), we can write ![](img/B15558_12_044.png), as shown
    here:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的损失函数中，方程（1），我们可以去掉最大项，而是将![](img/B15558_12_031.png)写作![](img/B15558_12_044.png)，如下所示：
- en: '![](img/B15558_12_045.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_045.png)'
- en: 'To maintain a uniform notation, let''s represent the loss function with *J*:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持符号的一致性，我们用*J*表示损失函数：
- en: '![](img/B15558_12_046.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_046.png)'
- en: 'To reduce the clutter, we can denote the target value with *y* and write:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少杂乱，我们可以用*y*表示目标值，并写作：
- en: '![](img/B15558_12_047.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_047.png)'
- en: Where *y*[i] is the target value of the critic, that is, ![](img/B15558_12_048.png),
    and the action *a*[i] is the action produced by the main actor network, that is,
    ![](img/B15558_12_030.png).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y*[i]是评论家的目标值，即！[](img/B15558_12_048.png)，而动作*a*[i]是由主要演员网络生成的动作，即！[](img/B15558_12_030.png)。
- en: 'To minimize the loss, we compute the gradients of the objective function ![](img/B15558_12_050.png)
    and update the main critic network parameter ![](img/B15558_12_006.png) by performing
    gradient descent:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化损失，我们计算目标函数![](img/B15558_12_050.png)的梯度，并通过梯度下降更新主要的评论家网络参数![](img/B15558_12_006.png)：
- en: '![](img/B15558_12_052.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_052.png)'
- en: 'Okay, what about the target critic network parameter ![](img/B15558_12_025.png)?
    How can we update it? We can update the parameter of the target critic network
    by just copying the parameter of the main critic network parameter ![](img/B15558_09_098.png)
    as shown here:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那目标评论网络的参数![](img/B15558_12_025.png)呢？我们如何更新它？我们可以通过直接复制主评论网络参数![](img/B15558_09_098.png)的参数来更新目标评论网络的参数，如下所示：
- en: '![](img/B15558_12_055.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_055.png)'
- en: This is usually called the soft replacement and the value of ![](img/B15558_12_056.png)
    is often set to 0.001.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为软替代，并且![](img/B15558_12_056.png)的值通常设置为0.001。
- en: Thus, we learned how the critic network uses the DQN to compute the Q value
    to evaluate the action produced by the actor network. In the next section, we
    will learn how the actor network learns the optimal policy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们学到评论网络如何使用DQN来计算Q值，以评估演员网络产生的动作。在下一节中，我们将学习演员网络如何学习最优策略。
- en: Actor network
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 演员网络
- en: We have already learned that the actor network is the policy network and it
    uses the policy gradient to compute the optimal policy. We also learned that we
    represent the parameter of the actor network with ![](img/B15558_11_043.png),
    and so the parameterized policy is represented with ![](img/B15558_12_011.png).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到演员网络是策略网络，它使用策略梯度来计算最优策略。我们还学到我们通过![](img/B15558_11_043.png)表示演员网络的参数，因此参数化的策略表示为![](img/B15558_12_011.png)。
- en: 'The actor network takes state *s* as an input and returns the action *a*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 演员网络以状态 *s* 作为输入，并返回动作 *a*：
- en: '![](img/B15558_12_059.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_059.png)'
- en: One important point that we may want to note down here is that we are using
    a deterministic policy. Since we are using a deterministic policy, we need to
    take care of the exploration-exploitation dilemma, because we know that a deterministic
    policy always selects the same action and doesn't explore new actions, unlike
    a stochastic policy, which selects different actions based on the probability
    distribution over the action space.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们可能需要注意的一个重要点是，我们使用的是确定性策略。由于我们使用的是确定性策略，我们需要处理探索-开发困境，因为我们知道确定性策略总是选择相同的动作，而不会探索新的动作，这与基于动作空间概率分布选择不同动作的随机策略不同。
- en: Okay, how can we explore new actions while using a deterministic policy? Note
    that DDPG is designed for an environment where the action space is continuous.
    Thus, we are using a deterministic policy in the continuous action space.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，在使用确定性策略时，我们如何探索新的动作呢？请注意，DDPG是为动作空间连续的环境设计的。因此，我们在连续动作空间中使用确定性策略。
- en: 'Unlike the discrete action space, in the continuous action space, we have continuous
    values. So, to explore new actions, we can just add some noise ![](img/B15558_12_060.png)
    to the action produced by the actor network since the action is a continuous value.
    We generate this noise using a process called the Ornstein-Uhlenbeck random process.
    So, our modified action can be represented as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与离散动作空间不同，在连续动作空间中，我们有连续的值。因此，为了探索新的动作，我们可以直接在演员网络产生的动作上添加一些噪声![](img/B15558_12_060.png)，因为动作是一个连续值。我们通过一种叫做奥恩斯坦-乌伦贝克随机过程的方式生成这个噪声。因此，我们修改后的动作可以表示为：
- en: '![](img/B15558_12_061.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_061.png)'
- en: For example, say the action ![](img/B15558_12_062.png) produced by the actor
    network is 13\. Suppose the noise ![](img/B15558_12_060.png) is 0.1, then our
    action becomes *a* = 13+0.1 = 13.1.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设演员网络产生的动作![](img/B15558_12_062.png)是13。假设噪声![](img/B15558_12_060.png)是0.1，那么我们的动作变成了
    *a* = 13+0.1 = 13.1。
- en: We learned that the critic network is represented by ![](img/B15558_12_064.png)
    and it evaluates the action produced by the actor using the Q value. If the Q
    value is high, then the critic tells the actor that it has produced a good action
    but when the Q value is low, then the critic tells the actor that it has produced
    a bad action.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到评论网络通过![](img/B15558_12_064.png)表示，它使用Q值来评估演员产生的动作。如果Q值很高，那么评论网络告诉演员它产生了一个好的动作；但当Q值很低时，评论网络告诉演员它产生了一个不好的动作。
- en: But wait! We learned that it is difficult to compute the Q value when the action
    space is continuous. That is, when the action space is continuous, it is difficult
    to compute the Q value of all possible actions in the state and take the maximum
    Q value. That is why we resorted to the policy gradient method. But now, we are
    computing the Q value with a continuous action space. How will this work?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等！我们了解到当动作空间是连续时，计算 Q 值是困难的。也就是说，当动作空间是连续的时，计算状态中所有可能动作的 Q 值并取最大 Q 值是困难的。这就是为什么我们转向策略梯度方法。但现在，我们正在计算具有连续动作空间的
    Q 值。这会怎么样？
- en: Note that, here in DDPG, we are not computing the Q value of all possible state-action
    pairs. We simply compute the Q value of state *s* and action *a* produced by the
    actor network.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 DDPG 中，我们并不计算所有可能的状态-动作对的 Q 值。我们只是简单地计算演员网络生成的状态 *s* 和动作 *a* 的 Q 值。
- en: The goal of the actor is to make the critic tell that the action it has produced
    is a good action. That is, the actor wants to get good feedback from the critic
    network. When does the critic give good feedback to the actor? The critic gives
    good feedback when the action produced by the actor has a maximum Q value. That
    is, if the action produced by the actor has a maximum Q value, then the critic
    tells the actor that it has produced a good action. So, the actor tries to generate
    an action in such a way that it can maximize the Q value produced by the critic.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 演员的目标是让评论者说出它产生的动作是一个好动作。也就是说，演员希望从评论者网络获得良好的反馈。评论者何时会给演员良好的反馈？当演员产生的动作具有最大的
    Q 值时，评论者会给出良好的反馈。因此，演员试图以一种方式生成动作，使得它可以最大化评论者生成的 Q 值。
- en: 'Thus, the objective function of the actor is to generate an action that maximizes
    the Q value produced by the critic network. So, we can write the objective function
    of the actor as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，演员的目标函数是生成一个动作，最大化评论者网络生成的 Q 值。因此，我们可以将演员的目标函数写为：
- en: '![](img/B15558_12_065.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_065.png)'
- en: 'Where the action ![](img/B15558_12_066.png). Maximizing the above objective
    function ![](img/B15558_12_067.png) implies that we are maximizing the Q value
    produced by the critic network. Okay, how can we maximize the preceding objective
    function? We can maximize the objective function by performing gradient ascent
    and update the actor network parameter as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中动作为 ![](img/B15558_12_066.png)。最大化上述目标函数 ![](img/B15558_12_067.png) 意味着我们在最大化评论者网络生成的
    Q 值。好的，我们如何最大化前述目标函数呢？我们可以通过执行梯度上升来最大化目标函数，并更新演员网络参数如下：
- en: '![](img/B15558_12_068.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_068.png)'
- en: 'Wait. Instead of updating the actor network parameter ![](img/B15558_11_043.png)
    just for a single state ![](img/B15558_12_070.png), we sample ![](img/B15558_12_071.png)
    number of states from the replay buffer ![](img/B15558_12_072.png) and update
    the parameter. So, now our objective function becomes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。我们不仅仅为单个状态 ![](img/B15558_12_070.png) 更新演员网络参数 ![](img/B15558_11_043.png)，而是从重播缓冲区
    ![](img/B15558_12_072.png) 中采样 ![](img/B15558_12_071.png) 个状态并更新参数。因此，我们的目标函数现在变成：
- en: '![](img/B15558_12_073.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_073.png)'
- en: 'Where the action ![](img/B15558_12_074.png). Maximizing the preceding objective
    function implies that the actor tries to generate actions in such a way that it
    maximizes the Q value over all the sampled states. We can maximize the objective
    function by performing gradient ascent and update the actor network parameter
    as:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中动作为 ![](img/B15558_12_074.png)。最大化前述目标函数意味着演员试图以一种方式生成动作，使得在所有采样状态下最大化 Q 值。我们可以通过执行梯度上升来最大化目标函数，并更新演员网络参数如下：
- en: '![](img/B15558_12_068.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_068.png)'
- en: To summarize, the objective of the actor is to generate action in such a way
    that it maximizes the Q value produced by the critic. So, we perform gradient
    ascent and update the actor network parameter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，演员的目标是以一种方式生成动作，使得它最大化评论者生成的 Q 值。因此，我们执行梯度上升，并更新演员网络参数。
- en: 'Okay, what about the parameter of the target actor network? How can we update
    it? We can update the parameter of the target actor network by just copying the
    parameter of the main actor network parameter ![](img/B15558_11_043.png) by soft
    replacement, as shown here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么目标演员网络的参数如何更新？我们可以通过软替换仅复制主要演员网络参数 ![](img/B15558_11_043.png) 来更新目标演员网络参数，如下所示：
- en: '![](img/B15558_12_077.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_077.png)'
- en: Now that we have understood how actor and critic networks work, let's get a
    good understanding of what we have learned so far and how DDPG works exactly by
    putting all the concepts together.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了演员和评论家网络是如何工作的，让我们整理一下到目前为止所学的内容，并通过将所有概念整合起来，来深入理解 DDPG 是如何工作的。
- en: Putting it all together
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'To avoid getting lost in notations, first, let''s recollect the notations to
    understand DDPG better. We use four networks, two actor networks and two critic
    networks:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在符号中迷失，首先，让我们回顾一下符号，以便更好地理解 DDPG。我们使用四个网络，两个演员网络和两个评论家网络：
- en: The main critic network parameter is represented by ![](img/B15558_12_006.png)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主评论家网络参数表示为 ![](img/B15558_12_006.png)
- en: The target critic network parameter is represented by ![](img/B15558_12_025.png)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标评论家网络参数表示为 ![](img/B15558_12_025.png)
- en: The main actor network parameter is represented by ![](img/B15558_11_043.png)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主演员网络参数表示为 ![](img/B15558_11_043.png)
- en: The target actor network parameter is represented by ![](img/B15558_12_042.png)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标演员网络参数表示为 ![](img/B15558_12_042.png)
- en: Note that DDPG is an actor-critic method, and so its parameters will be updated
    at every step of the episode, unlike the policy gradient method, where we generate
    complete episodes and then update the parameter. Okay, let's get started and understand
    how DDPG works.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DDPG 是一种演员-评论家方法，因此它的参数将在每个回合的每一步都更新，这与策略梯度方法不同，后者是先生成完整的回合，然后再更新参数。好了，让我们开始，理解
    DDPG 是如何工作的。
- en: First, we initialize the main critic network parameter ![](img/B15558_12_006.png)
    and the main actor network parameter ![](img/B15558_11_043.png) with random values.
    We learned that the target network parameter is just a copy of the main network
    parameter. So, we initialize the target critic network parameter ![](img/B15558_12_025.png)
    by just copying the main critic network parameter ![](img/B15558_12_006.png).
    Similarly, we initialize the target actor network parameter ![](img/B15558_12_042.png)
    by just copying the main actor network parameter ![](img/B15558_11_043.png). We
    also initialize the replay buffer ![](img/B15558_12_072.png).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们用随机值初始化主评论家网络参数 ![](img/B15558_12_006.png) 和主演员网络参数 ![](img/B15558_11_043.png)。我们了解到，目标网络参数只是主网络参数的副本。因此，我们通过简单地复制主评论家网络参数
    ![](img/B15558_12_006.png) 来初始化目标评论家网络参数 ![](img/B15558_12_025.png)。类似地，我们通过复制主演员网络参数
    ![](img/B15558_11_043.png) 来初始化目标演员网络参数 ![](img/B15558_12_042.png)。我们还初始化了回放缓冲区
    ![](img/B15558_12_072.png)。
- en: 'Now, for each step in the episode, first, we select an action, *a*, using the
    actor network:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在每个回合的每一步，首先，我们使用演员网络选择一个动作 *a*：
- en: '![](img/B15558_12_059.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_059.png)'
- en: 'However, instead of using the action *a* directly, to ensure exploration, we
    add some noise ![](img/B15558_12_060.png), and so the action becomes:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了确保探索，我们不是直接使用动作 *a*，而是添加一些噪声 ![](img/B15558_12_060.png)，因此动作变为：
- en: '![](img/B15558_12_061.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_061.png)'
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_12_016.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_12_072.png).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们执行动作 *a*，移动到下一个状态 ![](img/B15558_12_016.png)，并获得奖励 *r*。我们将这个过渡信息存储在回放缓冲区
    ![](img/B15558_12_072.png) 中。
- en: Next, we randomly sample a minibatch of *K* transitions (*s*, *a*, *r*, *s'*)
    from the replay buffer. These *K* transitions will be used for updating both our
    critic and actor network.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从回放缓冲区随机采样一个 *K* 过渡（*s*，*a*，*r*，*s'*）的小批量。这些 *K* 个过渡将用于更新评论家和演员网络。
- en: 'First, let us compute the loss of the critic network. We learned that the loss
    function of the critic network is:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算评论家网络的损失。我们了解到，评论家网络的损失函数是：
- en: '![](img/B15558_12_047.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_047.png)'
- en: Where *y*[i] is the target value of the critic, that is, ![](img/B15558_12_095.png),
    and the action *a*[i] is the action produced by the actor network, that is, ![](img/B15558_12_074.png).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y*[i] 是评论家的目标值，即 ![](img/B15558_12_095.png)，而动作 *a*[i] 是由演员网络生成的动作，即 ![](img/B15558_12_074.png)。
- en: 'After computing the loss of the critic network, we compute the gradients ![](img/B15558_12_050.png)
    and update the critic network parameter ![](img/B15558_12_006.png) using gradient
    descent:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算评论家网络的损失后，我们计算梯度 ![](img/B15558_12_050.png)，并使用梯度下降法更新评论家网络参数 ![](img/B15558_12_006.png)：
- en: '![](img/B15558_12_099.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_099.png)'
- en: 'Now, let us update the actor network. We learned that the objective function
    of the actor network is:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更新演员网络。我们了解到，演员网络的目标函数是：
- en: '![](img/B15558_12_100.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_100.png)'
- en: 'Note that in the above equation, we are only using the state (*s*[i]) from
    the sampled *K* transitions (*s*, *a*, *r*, *s''*). The action *a* is selected
    by actor network, ![](img/B15558_12_074.png). Now, we need to maximize the preceding
    objective function. Maximizing the above objective function helps the actor to
    generate actions in such a way that it maximizes the Q value produced by the critic.
    We can maximize the objective function by computing the gradients of our objective
    function ![](img/B15558_10_093.png) and update the actor network parameter ![](img/B15558_11_043.png)
    using gradient ascent:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_103.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'And then, in the final step, we update the parameter of the target critic network
    ![](img/B15558_12_025.png) and the parameter of the target actor network ![](img/B15558_12_042.png)
    by soft replacement:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_106.png)![](img/B15558_12_107.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: We repeat these steps for several episodes. Thus, for each step in the episode,
    we update the parameter of our networks. Since the parameter gets updated at every
    step, our policy will also be improved at every step in the episode.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: To have a better understanding of how DDPG works, let's look into the DDPG algorithm
    in the next section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – DDPG
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DDPG algorithm is given as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main critic network parameter ![](img/B15558_12_006.png) and
    the main actor network parameter ![](img/B15558_11_043.png)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target critic network parameter ![](img/B15558_12_025.png) by
    just copying the main critic network parameter ![](img/B15558_12_006.png)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target actor network parameter ![](img/B15558_12_042.png) by
    just copying the main actor network parameter ![](img/B15558_12_042.png)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_12_072.png)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat steps 6 and 7
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize an Ornstein-Uhlenbeck random process ![](img/B15558_12_060.png) for
    an action space exploration
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0,…,*T* – 1:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select action *a* based on the policy ![](img/B15558_12_062.png) and exploration
    noise, that is, ![](img/B15558_12_117.png).
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_016.png),
    get the reward *r*, and store this transition information in the replay buffer
    ![](img/B15558_12_072.png).
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_088.png).
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value of the critic, that is, ![](img/B15558_12_095.png).
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network, ![](img/B15558_12_047.png).
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the loss ![](img/B15558_12_050.png) and update the critic
    network parameter using gradient descent, ![](img/B15558_12_099.png).
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the actor network ![](img/B15558_10_093.png) and update
    the actor network parameter by gradient ascent, ![](img/B15558_12_126.png).
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic and target actor network parameter as ![](img/B15558_12_127.png)
    and ![](img/B15558_12_107.png).
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Swinging up a pendulum using DDPG
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DDPG摆动一个摆锤
- en: In this section, let's implement the DDPG algorithm to train the agent to swing
    up a pendulum. That is, we will have a pendulum that starts swinging from a random
    position and the goal of our agent is to swing the pendulum up so it stays upright.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们实现DDPG算法来训练智能体摆动一个摆锤。也就是说，我们将有一个从随机位置开始摆动的摆锤，智能体的目标是使摆锤摆动起来并保持直立。
- en: 'First, let''s import the required libraries:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的库：
- en: '[PRE0]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating the Gym environment
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Gym环境
- en: 'Let''s create a pendulum environment using Gym:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Gym创建一个摆锤环境：
- en: '[PRE1]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Get the state shape of the environment:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的状态形状：
- en: '[PRE2]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Get the action shape of the environment:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的动作形状：
- en: '[PRE3]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that the pendulum is a continuous environment, and thus our action space
    consists of continuous values. Hence, we get the bounds of our action space:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，摆锤是一个连续环境，因此我们的动作空间由连续值组成。因此，我们获取动作空间的边界：
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Defining the variables
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义变量
- en: Now, let's define some of the important variables.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一些重要的变量。
- en: 'Set the discount factor, ![](img/B15558_05_056.png):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 设置折扣因子![](img/B15558_05_056.png)：
- en: '[PRE5]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Set the value of ![](img/B15558_12_056.png), which is used for soft replacement:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 设置![](img/B15558_12_056.png)的值，用于软替代：
- en: '[PRE6]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Set the size of our replay buffer:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 设置我们的重放缓冲区的大小：
- en: '[PRE7]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Set the batch size:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 设置批量大小：
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Defining the DDPG class
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义DDPG类
- en: 'Let''s define the class called `DDPG`, where we will implement the DDPG algorithm.
    To aid understanding, let''s look into the code line by line. You can also access
    the complete code from the GitHub repository of the book:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为`DDPG`的类，在其中实现DDPG算法。为了便于理解，让我们逐行查看代码。你也可以通过本书的GitHub仓库访问完整代码：
- en: '[PRE9]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Defining the init method
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义初始化方法
- en: 'First, let''s define the `init` method:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义`init`方法：
- en: '[PRE10]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the replay buffer for storing the transitions:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 定义重放缓冲区，用于存储转移：
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize `num_transitions` to `0`, which means that the number of transitions
    in our replay buffer is zero:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将`num_transitions`初始化为`0`，这意味着我们的重放缓冲区中的转移数为零：
- en: '[PRE12]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Start the TensorFlow session:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 启动TensorFlow会话：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We learned that in DDPG, instead of selecting the action *a* directly, to ensure
    exploration, we add some noise ![](img/B15558_12_060.png) using the Ornstein-Uhlenbeck
    process. So, we first initialize the noise:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在DDPG中，为了确保探索，而不是直接选择动作*a*，我们通过使用奥恩斯坦-乌伦贝克过程添加了一些噪声![](img/B15558_12_060.png)。因此，我们首先初始化噪声：
- en: '[PRE14]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, initialize the state shape, action shape, and high action value:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，初始化状态形状、动作形状和高动作值：
- en: '[PRE15]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the placeholder for the state:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 定义状态的占位符：
- en: '[PRE16]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the placeholder for the next state:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 定义下一个状态的占位符：
- en: '[PRE17]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the placeholder for the reward:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 定义奖励的占位符：
- en: '[PRE18]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With the actor variable scope:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在演员变量作用域内：
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the main actor network, which is parameterized by ![](img/B15558_11_043.png).
    The actor network takes the state as an input and returns the action to be performed
    in that state:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 定义主演员网络，该网络由![](img/B15558_11_043.png)进行参数化。演员网络以状态为输入，并返回在该状态下执行的动作：
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the target actor network that is parameterized by ![](img/B15558_12_042.png).
    The target actor network takes the next state as an input and returns the action
    to be performed in that state:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 定义目标演员网络，该网络由![](img/B15558_12_042.png)进行参数化。目标演员网络以下一个状态为输入，并返回在该状态下执行的动作：
- en: '[PRE21]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With the critic variable scope:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在评论变量作用域内：
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the main critic network, which is parameterized by ![](img/B15558_12_006.png).
    The critic network takes the state and also the action produced by the actor in
    that state as an input and returns the Q value:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 定义主评论网络，该网络由![](img/B15558_12_006.png)进行参数化。评论网络以状态和演员在该状态下产生的动作作为输入，并返回Q值：
- en: '[PRE23]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the target critic network, which is parameterized by ![](img/B15558_12_025.png).
    The target critic network takes the next state and also the action produced by
    the target actor network in that next state as an input and returns the Q value:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 定义目标评论网络，该网络由![](img/B15558_12_025.png)进行参数化。目标评论网络以下一个状态和目标演员网络在该下一个状态下产生的动作作为输入，并返回Q值：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Get the parameter of the main actor network ![](img/B15558_11_043.png):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 获取主演员网络的参数![](img/B15558_11_043.png)：
- en: '[PRE25]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Get the parameter of the target actor network ![](img/B15558_12_042.png):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 获取目标演员网络的参数![](img/B15558_12_042.png)：
- en: '[PRE26]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Get the parameter of the main critic network ![](img/B15558_12_006.png):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 获取主评论网络的参数![](img/B15558_12_006.png)：
- en: '[PRE27]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Get the parameter of the target critic network ![](img/B15558_12_025.png):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Perform the soft replacement, update the parameter of the target actor network
    as ![](img/B15558_12_107.png), and update the parameter of the target critic network
    as ![](img/B15558_12_127.png):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Compute the target Q value. We learned that the target Q value can be computed
    as the sum of reward and the discounted Q value of the next state-action pair,
    ![](img/B15558_12_095.png):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let''s compute the loss of the critic network. The loss of the critic
    network is the mean squared error between the target Q value and the predicted
    Q value:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_047.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'So, we can define the mean squared error as:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Train the critic network by minimizing the mean squared error using the Adam
    optimizer:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We learned that the objective function of the actor is to generate an action
    that maximizes the Q value produced by the critic network, as shown here:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_100.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: 'Where the action ![](img/B15558_12_074.png), and we can maximize this objective
    by computing gradients and by performing gradient ascent. However, it is a standard
    convention to perform minimization rather than maximization. So, we can convert
    the preceding maximization objective into a minimization objective by just adding
    a negative sign. Hence, we can define the actor network objective as:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_145.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can minimize the actor network objective by computing gradients and
    by performing gradient descent. Thus, we can write:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Train the actor network by minimizing the loss using the Adam optimizer:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Initialize all the TensorFlow variables:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Selecting the action
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `select_action` to select the action with the
    noise to ensure exploration:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Run the actor network and get the action:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we generate a normal distribution with the mean as the action and the
    standard deviation as the noise and we randomly select an action from this normal
    distribution:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We need to make sure that our action should not fall away from the action bound.
    So, we clip the action so that it lies within the action bound and then we return
    the action:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Defining the train function
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s define the train function:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Perform the soft replacement:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Randomly select indices from the replay buffer with the given batch size:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Select the batch of transitions from the replay buffer with the selected indices:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Get the batch of states, actions, rewards, and next states:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Train the actor network:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Train the critic network:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Storing the transitions
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s store the transitions in the replay buffer:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'First, stack the state, action, reward, and next state:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Get the index:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Store the transition:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Update the number of transitions:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If the number of transitions is greater than the replay buffer, train the network:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Building the actor network
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We define a function called `build_actor_network` to build the actor network.
    The actor network takes the state and returns the action to be performed in that
    state:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Building the critic network
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建评论员网络
- en: 'We define a function called `build_critic_network` to build the critic network.
    The critic network takes the state and the action produced by the actor in that
    state and returns the Q value:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为`build_critic_network`的函数来构建评论员网络。评论员网络接收状态和由演员在该状态下产生的动作，并返回Q值：
- en: '[PRE54]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Training the network
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'Now, let''s start training the network. First, let''s create an object for
    our DDPG class:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练网络。首先，我们创建一个DDPG类的对象：
- en: '[PRE55]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Set the number of episodes:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数：
- en: '[PRE56]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Set the number of time steps in each episode:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 设置每个回合的时间步数：
- en: '[PRE57]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'For each episode:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: '[PRE58]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境初始化状态：
- en: '[PRE59]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Initialize the return:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化返回值：
- en: '[PRE60]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'For every step:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步：
- en: '[PRE61]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Render the environment:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE62]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Select the action:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 选择动作：
- en: '[PRE63]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Perform the selected action:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 执行选定的动作：
- en: '[PRE64]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Store the transition in the replay buffer:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 将过渡存储在回放缓冲区中：
- en: '[PRE65]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Update the return:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 更新返回值：
- en: '[PRE66]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'If the state is the terminal state, then break:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态是终止状态，则中断：
- en: '[PRE67]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Update the state to the next state:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态更新到下一个状态：
- en: '[PRE68]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Print the return for every 10 episodes:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 打印每10个回合的返回值：
- en: '[PRE69]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'By rendering the environment, we can observe how the agent learns to swing
    up the pendulum:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 通过渲染环境，我们可以观察到代理是如何学习摆动摆钟的：
- en: '![](img/B15558_12_05.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_05.png)'
- en: 'Figure 12.5: The Gym pendulum environment'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：Gym摆钟环境
- en: Now that we have learned how DDPG works and how to implement it, in the next
    section, we will learn about another interesting algorithm called twin delayed
    DDPG.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了DDPG是如何工作的以及如何实现它，在下一节中，我们将了解另一个有趣的算法，称为双延迟DDPG。
- en: Twin delayed DDPG
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双延迟DDPG
- en: Now, we will look into another interesting actor-critic algorithm, known as
    TD3\. TD3 is an improvement (and basically a successor) to the DDPG algorithm
    we just covered.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入了解另一个有趣的演员-评论员算法，称为TD3。TD3是对我们刚才讨论的DDPG算法的改进（基本上是继任者）。
- en: In the previous section, we learned how DDPG uses a deterministic policy to
    work on the continuous action space. DDPG has several advantages and has been
    successfully used in a variety of continuous action space environments.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了DDPG如何使用确定性策略在连续动作空间上工作。DDPG有几个优点，并且已经成功地应用于各种连续动作空间环境。
- en: We understood that DDPG is an actor-critic method where an actor is a policy
    network and it finds the optimal policy, while the critic evaluates the policy
    produced by the actor by estimating the Q function using a DQN.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解到DDPG是一种演员-评论员方法，其中演员是一个策略网络，负责寻找最优策略，而评论员通过使用DQN估计Q函数来评估演员产生的策略。
- en: One of the problems with DDPG is that the critic overestimates the target Q
    value. This overestimation causes several issues. We learned that the policy is
    improved based on the Q value given by the critic, but when the Q value has an
    approximation error, it causes stability issues to our policy and the policy may
    converge to local optima.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG的一个问题是评论员高估了目标Q值。这种高估导致了几个问题。我们了解到，策略是基于评论员给出的Q值来改进的，但当Q值存在近似误差时，会导致策略的不稳定，且策略可能会收敛到局部最优解。
- en: 'Thus, to combat this, TD3 proposes three important features, which are as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了解决这个问题，TD3提出了三项重要的功能，分别是：
- en: Clipped double Q learning
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪双Q学习
- en: Delayed policy updates
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 延迟策略更新
- en: Target policy smoothing
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标策略平滑
- en: First, we will understand how TD3 works intuitively, and then we will look at
    the algorithm in detail.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将直观地了解TD3是如何工作的，然后再详细查看算法。
- en: Key features of TD3
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TD3的关键特点
- en: 'TD3 is essentially the same as DDPG, except that it proposes three important
    features to mitigate the problems in DDPG. In this section, let''s first get a
    basic understanding of the key features of TD3\. The three key features of TD3
    are:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: TD3本质上与DDPG相同，不同之处在于它提出了三项重要功能来缓解DDPG中的问题。在本节中，我们首先了解TD3的关键特点。TD3的三大关键特点是：
- en: '**Clipped double Q learning**:Instead of using one critic network, we use two
    main critic networks to compute the Q value and also use two target critic networks
    to compute the target value.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**裁剪双Q学习**：我们不再使用一个评论员网络，而是使用两个主要的评论员网络来计算Q值，同时使用两个目标评论员网络来计算目标值。'
- en: We compute two target Q values using two target critic networks and use the
    minimum value of these two while computing the loss. This helps to prevent overestimation
    of the target Q value. We will learn more about this in detail in the next section.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用两个目标Q值来计算两个目标评论网络，并在计算损失时使用这两个中的最小值。这有助于防止目标Q值的高估。我们将在下一节中更详细地学习这一点。
- en: '**Delayed policy updates**: In DDPG, we learned that we update the parameter
    of both the actor (policy network) and critic (DQN) network at every step of the
    episode. Unlike DDPG, here we delay updating the parameter of the actor network.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟策略更新**：在DDPG中，我们了解到我们在每个episode的每一步都更新演员（策略网络）和评论员（DQN）网络的参数。与DDPG不同，在这里我们延迟更新演员网络的参数。'
- en: That is, the critic network parameter is updated at every step of the episode,
    but the actor network (policy network) parameter is delayed and updated only after
    every two steps of the episode.
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 也就是说，评论员网络的参数在每个episode的每一步都更新，而演员网络（策略网络）的参数则延迟更新，仅在每两步之后更新一次。
- en: '**Target policy smoothing**:TheDDPG method produces different target values
    even for the same action. Hence, the variance of the target value will be high
    even for the same action, so we reduce this variance by adding some noise to the
    target action.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标策略平滑**：DDPG方法即使对于相同的动作也会产生不同的目标值。因此，即使对于相同的动作，目标值的方差也会很高，因此我们通过给目标动作添加一些噪声来减少这种方差。'
- en: Now that we have a basic idea of the key features of TD3, we will get into more
    detail and learn how exactly these three key features work and how they solve
    the problems associated with DDPG.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经对TD3的关键特性有了一个基本的了解，我们将深入探讨这三个关键特性是如何工作的，并了解它们是如何解决与DDPG相关的问题的。
- en: Clipped double Q learning
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 剪切双重Q学习
- en: 'Remember in *Chapter 9*, *Deep Q Network and Its Variants*, while learning
    about the DQN, we discovered that it tends to overestimate the Q value of the
    next state-action pair in the target? It is shown here:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在*第9章*，*深度Q网络及其变种*中，我们在学习DQN时发现，它倾向于高估目标状态-动作对的Q值吗？如图所示：
- en: '![](img/B15558_12_07.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_07.png)'
- en: 'In order to mitigate the overestimation, we used double Q learning. With double
    Q learning, we use two different networks, in other words, two different Q functions,
    one for selecting an action and the other to compute the Q value, as shown here:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻高估问题，我们使用了双重Q学习。通过双重Q学习，我们使用两个不同的网络，换句话说，两个不同的Q函数，一个用于选择动作，另一个用于计算Q值，如图所示：
- en: '![](img/B15558_12_146.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_146.png)'
- en: Thus, computing the target value by using the preceding equation prevents the
    overestimation of the Q value in the DQN.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过使用前面的公式计算目标值可以防止DQN中Q值的高估。
- en: We learned that in DDPG, the critic network is the DQN, and so it also suffers
    from the overestimation of the Q value in the target. Can we employ double Q learning
    in DDPG and try to solve the overestimation bias? Yes! But the problem is that
    in the actor-critic method, the policy and target network parameter updates happen
    slowly, and this will not help us in removing the overestimation bias.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到在DDPG中，评论员网络就是DQN，因此它也会遭遇目标中的Q值高估问题。那么我们能否在DDPG中应用双重Q学习来尝试解决高估偏差呢？当然可以！但问题是，在演员-评论员方法中，策略和目标网络的参数更新过程较慢，这将不会帮助我们消除高估偏差。
- en: So, we will use a slightly different version of double Q learning called *clipped
    double Q learning*. In clipped double Q learning, we use two target critic networks
    to compute the Q value.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将使用一种稍微不同的双重Q学习版本，称为*剪切双重Q学习*。在剪切双重Q学习中，我们使用两个目标评论网络来计算Q值。
- en: We use the two target critic networks and compute the two Q values and select
    the minimum value out of these two to compute the target value. This helps to
    prevent overestimation bias. Let's understand this in more detail.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个目标评论网络并计算两个Q值，从这两个Q值中选择最小值来计算目标值。这有助于防止高估偏差。让我们更详细地理解这一点。
- en: If we need two target critic networks, then we also need two main critic networks.
    We know that the target network parameter is just a time-delayed copy of the main
    network parameter. So, we define two main critic networks with the parameters
    ![](img/B15558_12_147.png) and ![](img/B15558_12_148.png) to compute the two Q
    values, that is, ![](img/B15558_12_149.png) and ![](img/B15558_12_150.png), respectively.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: We also define the two target critic networks with parameters ![](img/B15558_12_151.png)
    and ![](img/B15558_12_152.png) to compute the two Q values of next state-action
    pair in the target, that is, ![](img/B15558_12_153.png) and ![](img/B15558_12_154.png),
    respectively. Let's understand this clearly step by step.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'In DDPG, we learned that we compute the target value as:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_155.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: 'Computing the Q value of the next state-action pair in the target in this way
    creates overestimation bias:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_08.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: 'So, to avoid this, in TD3, first, we compute the Q value of the next state-action
    pair in the target using the first target critic network with a parameter ![](img/B15558_12_151.png),
    that is, ![](img/B15558_12_153.png), and then we compute the Q value of the next
    state-action pair in the target using the second target critic network with a
    parameter ![](img/B15558_12_152.png), that is, ![](img/B15558_12_154.png). Then,
    we use the minimum of these two Q values to compute the target value as expressed
    here:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_160.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: Where the action ![](img/B15558_12_161.png).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'We can express the preceding equation simply as:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_162.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
- en: Where the action ![](img/B15558_12_161.png).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Computing the target value in this way prevents overestimation of the Q value
    of the next state-action pair.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, we computed the target value. How do we compute the loss and update the
    critic network parameter? We learned that we use two main critic networks, so,
    first, we compute the loss of the first main critic network, parameterized by
    ![](img/B15558_12_147.png):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_165.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: After computing the loss, we compute the gradients and update the parameter
    ![](img/B15558_12_147.png)using gradient descent as ![](img/B15558_12_167.png).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the loss of the second main critic network, parameterized
    by ![](img/B15558_12_148.png):'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_169.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: After computing the loss, we compute the gradients and update the parameter
    ![](img/B15558_12_148.png)using gradient descent as ![](img/B15558_12_171.png).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply express the preceding updates as:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_172.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
- en: 'After updating the two main critic network parameters, ![](img/B15558_12_147.png)and![](img/B15558_12_148.png),we
    can update the two target critic network parameters, ![](img/B15558_12_151.png)
    and ![](img/B15558_12_152.png), by soft replacement, as shown here:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_177.png)![](img/B15558_12_178.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: 'We can simply express the preceding updates as:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_179.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
- en: Delayed policy updates
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Delayed policy updates imply that we update the parameters of our actor network
    (policy network) less frequently than the critic networks. But why do we want
    to do that? We learned that in DDPG, actor and critic network parameters are updated
    at every step of the episode.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: When the critic network parameter is not good, then it estimates the incorrect
    Q values. If the Q value estimated by the critic network is not correct, then
    the actor network cannot update its parameter correctly. That is, we learned that
    the actor network learns based on feedback from the critic network. This feedback
    is just the Q value. When the critic network gives incorrect feedback (incorrect
    Q value), then the actor network cannot learn the correct action and cannot update
    its parameter correctly.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to avoid this, we hold updating the parameter of the actor network for
    a while and only update the critic network to make the critic estimate the correct
    Q value. That is, we update the parameter of the critic network at every step
    of the episode, and we delay updating the parameter of the actor network, and
    only update it for some specific steps of the episode because we don't want our
    actor to learn from the incorrect critic's feedback.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, the critic network parameter is updated at every step of the
    episode, but the actor network parameter update is delayed. We generally delay
    the update by two steps.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, in DDPG, we learned that the objective of the actor network (policy network)
    is to maximize the Q value:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_100.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
- en: The preceding objective of the actor network is the same in TD3 as well. That
    is, similar to DDPG, here, the objective of the actor is to generate actions in
    such a way that it maximizes the Q value produced by the critic. But wait! Unlike
    DDPG, here we have two Q values, ![](img/B15558_12_149.png) and ![](img/B15558_12_150.png),
    since we use two critic networks with parameters ![](img/B15558_12_147.png) and
    ![](img/B15558_12_148.png), respectively. So which Q value should our actor network
    maximize? Should it be ![](img/B15558_12_149.png) or ![](img/B15558_12_150.png)?
    We can take either of these and maximize one. So, we can take ![](img/B15558_12_149.png).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in TD3, the objective of the actor network is to maximize the Q value,
    ![](img/B15558_12_149.png), as shown here:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_189.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: 'Remember that in the above equation, the action *a* is selected by actor network,
    ![](img/B15558_12_074.png). In order to maximize the objective function, we compute
    the gradients of our objective function, ![](img/B15558_10_093.png), and update
    the parameter of the network using gradient ascent:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_103.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
- en: 'Now, instead of doing this parameter update of the actor network at every time
    step of the episode, we delay the updates and update the parameter only on every
    other step (every two steps). Let *t* be the time step of the episode and *d*
    denotes the number of time steps we want to delay the update by (usually *d* is
    set to 2); then we can write the following:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'If *t* mod *d* =0, then:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png)
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the actor network parameter using gradient ascent ![](img/B15558_12_103.png)
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This will be made clearer when we look at the final algorithm.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Target policy smoothing
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand this, let''s first recollect how we compute the target value
    in TD3\. We learned that in TD3, we update the target value using clipped double
    Q learning with two target critic networks:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_162.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: Where the action ![](img/B15558_12_161.png).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can notice, we compute the target values with action ![](img/B15558_12_031.png)
    generated by the target actor network, ![](img/B15558_12_199.png). Instead of
    using the action given by the target actor network directly, we add some noise
    ![](img/B15558_12_200.png) to the action and modify the action to ![](img/B15558_12_201.png),
    as shown here:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_202.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
- en: 'Here, −*c* to +*c* indicates that noise is clipped, so that we can keep the
    target close to the actual action. Thus, our target value computation now becomes:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_203.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the action ![](img/B15558_12_204.png).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: But why are we doing this? Why do we need to add noise to the action and use
    it to compute the target value? Similar actions should have similar target values,
    right? However, the DDPG method produces target values with high variance even
    for similar actions. This is because deterministic policies overfit to the sharp
    peaks in the value estimate. So, we can smooth out these peaks for similar actions
    by adding some noise. Thus, target policy smoothing basically acts as a regularizer
    and reduces the variance in the target values.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the key features of the TD3 algorithm, let's get
    clarity on what we have learned so far and how the TD3 algorithm works by putting
    all the concepts together.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s recollect the notations to understand TD3 better. We use six
    networks—four critic networks and two actor networks:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: The two main critic network parameters are represented by ![](img/B15558_12_205.png)
    and ![](img/B15558_12_206.png)
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two target critic network parameters are represented by ![](img/B15558_12_207.png)
    and ![](img/B15558_12_208.png)
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main actor network parameter is represented by ![](img/B15558_12_209.png)
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target actor network parameter is represented by ![](img/B15558_12_210.png)
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TD3 is an actor-critic method, and so the parameters of TD3 will get updated
    at every step of the episode, unlike the policy gradient method where we generate
    complete episodes and then update the parameter. Now, let's get started and understand
    how TD3 works.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize the two main critic network parameters, ![](img/B15558_12_211.png)
    and ![](img/B15558_12_148.png), and the main actor network parameter ![](img/B15558_12_213.png)
    with random values. We know that the target network parameter is just a copy of
    the main network parameter. So, we initialize the two target critic network parameters
    ![](img/B15558_12_214.png) and ![](img/B15558_12_208.png) by just copying ![](img/B15558_12_216.png)
    and ![](img/B15558_12_217.png), respectively. Similarly, we initialize the target
    actor network parameter ![](img/B15558_12_218.png) by just copying the main actor
    network parameter ![](img/B15558_12_219.png). We also initialize the replay buffer
    ![](img/B15558_12_220.png).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each step in the episode, first, we select an action *a* using the
    actor network:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_059.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'But instead of using the action *a* directly, to ensure exploration, we add
    some noise ![](img/B15558_12_222.png), where ![](img/B15558_12_223.png). Thus,
    our action now becomes:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_224.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_02_004.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_12_226.png).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Next, we randomly sample a minibatch of *K* transitions (*s*, *a*, *r*, *s'*)
    from the replay buffer. These *K* transitions will be used for updating both our
    critic and actor network.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us compute the loss of the critic networks. We learned that the
    loss function of the critic networks is:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_227.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: The action *a*[i] is the action produced by the actor network, that is, ![](img/B15558_12_228.png)
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[i] is the target value of the critic, that is, ![](img/B15558_12_230.png),
    and the action ![](img/B15558_12_231.png) is the action produced by the target
    actor network, that is, ![](img/B15558_12_232.png) where ![](img/B15558_12_233.png)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After computing the loss of the critic network, we compute the gradients ![](img/B15558_12_234.png)
    and update the critic network parameter using gradient descent:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_235.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: 'Now, let us update the actor network. We learned that the objective function
    of the actor network is:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_189.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
- en: 'Note that in the above equation, we are only using the state (*s*[i]) from
    the sampled *K* transitions (*s*, *a*, *r*, *s''*). The action *a* is selected
    by actor network, ![](img/B15558_12_074.png). In order to maximize the objective
    function, we compute gradients of our objective function ![](img/B15558_10_093.png)
    and update the parameters of the network using gradient ascent:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_103.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
- en: 'Instead of doing this parameter update of the actor network at every time step
    of the episode, we delay the updates. Let *t* be the time step of the episode
    and *d* denotes the number of time steps we want to delay the update by (usually
    *d* is set to 2); then we can write the following:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'If *t* mod *d* = 0, then:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png)
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the actor network parameter using gradient ascent ![](img/B15558_12_103.png)
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we update the parameter of the target critic networks ![](img/B15558_12_214.png)
    and ![](img/B15558_12_208.png) and the parameter of the target actor network ![](img/B15558_12_243.png)
    by soft replacement:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_244.png)![](img/B15558_12_245.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
- en: 'There is a small change in updating the parameter of the target networks. Just
    like we delay updating the actor network parameter for *d* steps, we update the
    target network parameter for every *d* step; hence, we can write:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'If *t* mod *d* = 0, then:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png) and
    update the actor network parameter using gradient ascent ![](img/B15558_12_247.png)
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic network parameter and target actor network parameter
    as ![](img/B15558_12_248.png), and ![](img/B15558_12_107.png), respectively
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We repeat the preceding steps for several episodes and improve the policy. To
    get a better understanding of how TD3 works, let's look into the TD3 algorithm
    in the next section.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – TD3
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The TD3 algorithm is exactly similar to the DDPG algorithm except that it includes
    the three key features we learned in the previous sections. So, before looking
    into the TD3 algorithm directly, you can revise all the key features of TD3.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm of TD3 is given as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the two main critic network parameters ![](img/B15558_12_211.png)
    and ![](img/B15558_12_217.png) and the main actor network parameter ![](img/B15558_12_252.png)
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the two target critic network parameters ![](img/B15558_12_214.png)
    and ![](img/B15558_12_152.png) by copying the main critic network parameters ![](img/B15558_12_255.png)
    and ![](img/B15558_12_148.png), respectively
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target actor network parameter ![](img/B15558_12_218.png) by
    copying the main actor network parameter ![](img/B15558_12_218.png)
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_12_259.png)
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat step 6
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0,…,*T* – 1:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action *a* based on the policy ![](img/B15558_12_062.png) and with
    exploration noise ![](img/B15558_12_261.png), that is, ![](img/B15558_12_224.png)
    where, ![](img/B15558_12_263.png)
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_264.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_088.png)
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_266.png)
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action ![](img/B15558_12_267.png) to compute the target value, ![](img/B15558_12_268.png),
    where ![](img/B15558_12_269.png)
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value of the critic, that is, ![](img/B15558_12_230.png)
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network, ![](img/B15558_12_227.png)
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradients of the loss ![](img/B15558_12_234.png) and minimize the
    loss using gradient descent, ![](img/B15558_12_273.png)
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod *d* =0, then:'
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the objective function ![](img/B15558_12_274.png) and
    update the actor network parameter using gradient ascent, ![](img/B15558_12_126.png)
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic network parameter and target actor network parameter
    as ![](img/B15558_12_248.png), and ![](img/B15558_12_107.png), respectively
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned how TD3 works, in the next section, we will learn about
    another interesting algorithm, called SAC.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Soft actor-critic
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will look into another interesting actor-critic algorithm, called SAC.
    This is an off-policy algorithm and it borrows several features from the TD3 algorithm.
    But unlike TD3, it uses a stochastic policy ![](img/B15558_03_139.png). SAC is
    based on the concept of entropy. So first, let's understand what is meant by entropy.
    Entropy is a measure of the randomness of a variable. It basically tells us the
    uncertainty or unpredictability of the random variable and is denoted by ![](img/B15558_12_279.png).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: If the random variable always gives the same value every time, then we can say
    that its entropy is low because there is no randomness. But if the random variable
    gives different values, then we can say that its entropy is high.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: For an example, consider a dice throw experiment. Every time a dice is thrown,
    if we get a different number, then we can say that the entropy is high because
    we are getting a different number every time and there is high uncertainty since
    we don't know which number will come up on the next throw. But if we are getting
    the same number, say 3, every time the dice is thrown, then we can say that the
    entropy is low, since there is no randomness here as we are getting the same number
    on every throw.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: We know that the policy ![](img/B15558_03_139.png) tells what action to perform
    in a given state. What happens when the entropy of the policy ![](img/B15558_12_281.png)
    is high or low? If the entropy of the policy is high, then this means that our
    policy performs different actions instead of performing the same action every
    time. But if the entropy of the policy is low, then this means that our policy
    performs the same action every time. As you may have guessed, increasing the entropy
    of a policy promotes exploration, while decreasing the entropy of the policy means
    less exploration.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that, in reinforcement learning, our goal is to maximize the return.
    So, we can define our objective function as shown here:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_282.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_12_283.png) is the parameter of our stochastic policy ![](img/B15558_03_139.png).
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the return of the trajectory is just the sum of rewards, that
    is:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_10_115.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
- en: 'So, we can rewrite our objective function by expanding the return as:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_286.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
- en: 'Maximizing the preceding objective function maximizes the return. In the SAC
    method, we use a slightly modified version of the objective function with the
    entropy term as shown here:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_287.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
- en: As we can see, our objective function now has two terms; one is the reward and
    the other is the entropy of the policy. Thus, instead of maximizing only the reward,
    we also maximize the entropy of a policy. But what is the point of this? Maximizing
    the entropy of the policy allows us to explore new actions. But we don't want
    to explore actions that give us a bad reward. Hence, maximizing entropy along
    with maximizing reward means that we can explore new actions along with maintaining
    maximum reward. The preceding objective function is often referred to as **maximum
    entropy reinforcement learning**, or **entropy regularized reinforcement learning.**
    Adding an entropy term is also often referred to as an entropy bonus.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Also, the term ![](img/B15558_09_143.png) in the objective function is called
    temperature and is used to set the importance of our entropy term, or we can say
    that it is used to control exploration. When ![](img/B15558_09_143.png) is high,
    we allow exploration in the policy, but when it is low, then we don't allow exploration.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: Okay, now that we have a basic idea of SAC, let's get into some more details.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: Understanding soft actor-critic
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SAC, as the name suggests, is an actor-critic method similar to DDPG and TD3
    we learned in the previous sections. Unlike DDPG and TD3, which use deterministic
    policies, SAC uses a stochastic policy. SAC works in a very similar manner to
    TD3\. We learned that in actor-critic architecture, the actor uses the policy
    gradient to find the optimal policy and the critic evaluates the policy produced
    by the actor using the Q function.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in SAC, the actor uses the policy gradient to find the optimal policy
    and the critic evaluates the policy produced by the actor. However, instead of
    using only the Q function to evaluate the actor's policy, the critic uses both
    the Q function and the value function. But why exactly do we need both the Q function
    and the value function to evaluate the actor's policy? This will be explained
    in detail in the upcoming sections.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: So, in SAC, we have three networks, one actor network (policy network) to find
    the optimal policy, and two critic networks—a value network and a Q network, to
    compute the value function and the Q function, respectively, to evaluate the policy
    produced by the actor.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, let's look at the modified version of the value function and
    the Q function with the entropy term.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: V and Q functions with the entropy term
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that the value function (state value) is the expected return of the
    trajectory starting from state *s* following a policy ![](img/B15558_03_008.png):'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_289.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
- en: 'We learned that the return is the sum of rewards of the trajectory, so we can
    rewrite the preceding equation by expanding the return as:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_290.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can rewrite the value function by adding the entropy term as shown
    here:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_291.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
- en: 'We know that the Q function (state-action value) is the expected return of
    the trajectory starting from state *s* and action *a* following a policy ![](img/B15558_03_140.png):'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_293.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
- en: 'Expanding the return of the trajectory, we can write the following:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_294.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can rewrite the Q function by adding the entropy term as shown here:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_295.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
- en: 'The modified Bellman equation for the preceding Q function with the entropy
    term is given as:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_296.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
- en: 'Here, the value function can be computed using the relation between the Q function
    and the value function as:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_297.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
- en: 'To learn how exactly we obtained the equations (2) and (3), you can check the
    derivation of soft policy iteration in the paper *Soft Actor-Critic: Off-Policy
    Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor*, by Tuomas
    Haarnoja et.al.: [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: Components of SAC
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a basic idea of SAC, let's go into more detail and understand
    how exactly each component of SAC works by looking at them separately.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Critic network
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned that unlike other actor-critic methods we have seen earlier, the
    critic in SAC uses both the value function and the Q function to evaluate the
    policy produced by the actor network. But why is that?
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous algorithms, we used the critic network to compute the Q function
    for evaluating the action produced by the actor. Also, the target Q value in the
    critic is computed using the Bellman equation. We can do the same here. However,
    here we have modified the Bellman equation of the Q function due to the entropy
    term, as we learned in equation (2):'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_298.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
- en: From the preceding equation, we can observe that in order to compute the Q function,
    first we need to compute the value function. So, we need to compute both the Q
    function and the value function in order to evaluate the policy produced by the
    actor. We can use a single network to approximate both the Q function and the
    value function. However, instead of using a single network, we use two different
    networks, the Q network to estimate the Q function, and the value network to estimate
    the value function. Using two different networks to compute the Q function and
    value function stabilizes the training.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the SAC paper, "*There is no need in principle to include a
    separate function approximator (neural network) for the state value since it is
    related to the Q function and policy according to Equation 2\. But in practice,
    including a separate function approximator (neural network) for the state value
    can stabilize training and is convenient to train simultaneously with the other
    networks*."
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: First, we will learn how the value network works and then we will learn about
    the Q network.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: Value network
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The value network is denoted by *V*, the parameter of the value network is denoted
    by ![](img/B15558_12_299.png), and the parameter of the target value network is
    denoted by ![](img/B15558_12_300.png).
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: Thus, ![](img/B15558_12_301.png) implies that we approximate the value function
    (state value) using the neural network parameterized by ![](img/B15558_12_302.png).
    Okay, how can we train the value network? We can train the network by minimizing
    the loss between the target state value and the state value predicted by our network.
    How can we obtain the target state value? We can use the value function given
    in equation (3) to compute the target state value.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that according to equation (3), the value of the state is computed
    as:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_303.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we can remove the expectation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer.
    So, we can compute the target state value *y*[v] using the preceding equation
    as:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_304.png)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
- en: 'If we look at the preceding equation, we have a Q function. In order to compute
    the Q function, we use a Q network parameterized by ![](img/B15558_09_098.png),
    and similarly, our policy is parameterized by ![](img/B15558_12_306.png), so we
    can rewrite the preceding equation with the parameterized Q function and policy
    as shown here:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_307.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
- en: 'But if we use the preceding equation to compute the target value, the Q value
    will overestimate. So, to avoid this overestimation, we use clipped double Q learning,
    just like we learned in TD3\. That is, we compute the two Q values using two Q
    networks parameterized by ![](img/B15558_12_211.png) and ![](img/B15558_12_217.png)
    and take the minimum value of these two, as shown here:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_310.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
- en: As we can observe in the preceding equation, for clipped double Q learning,
    we are using the two main Q networks parameterized by ![](img/B15558_12_311.png)
    and ![](img/B15558_12_217.png), but in TD3, we used two target Q networks parameterized
    by ![](img/B15558_12_214.png) and ![](img/B15558_12_152.png). Why is that?
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: Because here, we are computing the Q value of a state-action pair ![](img/B15558_12_315.png)
    so we can use the two main Q networks parameterized by ![](img/B15558_12_216.png)
    and ![](img/B15558_12_217.png), but in TD3, we compute the Q value of the next
    state-action pair ![](img/B15558_05_091.png), so we used the two target Q networks
    parameterized by ![](img/B15558_12_214.png) and ![](img/B15558_12_320.png). Thus,
    here, we don't need target Q networks.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply express the preceding equation as:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_321.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can define our objective function ![](img/B15558_12_322.png) of the
    value network as the mean squared difference between the target state value and
    the state value predicted by our network, as shown here:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_323.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
- en: Where *K* denotes the number of transitions we sample from the replay buffer.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the gradients of our objective function and then update our
    main value network parameter ![](img/B15558_12_302.png) as:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_325.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
- en: Note that we are using ![](img/B15558_12_326.png) to represent the learning
    rate since we are already using ![](img/B15558_07_025.png) to denote the temperature.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: 'We can update the parameter of the target value network ![](img/B15558_12_328.png)
    using soft replacement:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_329.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
- en: We will learn where exactly the target value network is used in the next section.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: Q network
  id: totrans-533
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Q network is denoted by *Q* and it is parameterized by ![](img/B15558_12_330.png).
    Thus, ![](img/B15558_12_331.png) implies that we approximate the Q function using
    the neural network parameterized by ![](img/B15558_09_098.png). How can we train
    the Q network? We can train the network by minimizing the loss between the target
    Q value and the Q value predicted by the network. How can we obtain the target
    Q value? Here is where we use the Bellman equation.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that according to the Bellman equation (2), the Q value can be computed as:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_298.png)'
  id: totrans-536
  prefs: []
  type: TYPE_IMG
- en: 'We can remove the expectation in the preceding equation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer.
    So, we can compute the target Q value *y*[q] using the preceding equation as:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_334.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
- en: 'If we look at the preceding equation, we have a value of next state ![](img/B15558_12_335.png).
    In order to compute the value of next state ![](img/B15558_12_336.png), we use
    a target value network parameterized by ![](img/B15558_12_337.png), so we can
    rewrite the preceding equation with the parameterized value function as shown
    here:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_338.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can define our objective function ![](img/B15558_12_339.png) of the
    Q network as the mean squared difference between the target Q value and the Q
    value predicted by the network, as shown here:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_340.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
- en: Where *K* denotes the number of transitions we sample from the replay buffer.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we learned that we use two Q networks parameterized
    by ![](img/B15558_12_216.png) and ![](img/B15558_12_217.png) to prevent overestimation
    bias. So, first, we compute the loss of the first Q network, parameterized by
    ![](img/B15558_12_211.png):'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_344.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
- en: Then, we compute the gradients and update the parameter ![](img/B15558_12_211.png)using
    gradient descent as ![](img/B15558_12_346.png).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the loss of the second Q network, parameterized by ![](img/B15558_12_217.png):'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_348.png)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
- en: Then, we compute the gradients and update the parameter ![](img/B15558_12_217.png)using
    gradient descent as ![](img/B15558_12_350.png).
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simply express the preceding updates as:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_351.png)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
- en: Actor network
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The actor network (policy network) is parameterized by ![](img/B15558_11_043.png).
    Let''s recall the objective function of the actor network we learned in TD3:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_100.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
- en: Where *a* is the action produced by the actor.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: The preceding objective function means that the goal of the actor is to generate
    action in such a way that it maximizes the Q value computed by the critic.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function of the actor network in SAC is the same as what we learned
    in TD3, except that here we use a stochastic policy ![](img/B15558_12_413.png),
    and also, we maximize the entropy. So, we can write the objective function of
    the actor network in SAC as:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_354.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
- en: 'Now, how can we compute the derivative of the preceding objective function?
    Because, unlike TD3, here, our action is computed using a stochastic policy. It
    will be difficult to apply backpropagation and compute gradients of the preceding
    objective function with the action computed using a stochastic policy. So, we
    use the reparameterization trick. The reparameterization trick guarantees that
    sampling from our policy is differentiable. Thus, we can rewrite our action as
    shown here:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_355.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, we can observe that we parameterize the policy with
    a neural network *f* and ![](img/B15558_12_200.png) is the noise sampled from
    a spherical Gaussian distribution.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we can rewrite our objective function as shown here:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_358.png)'
  id: totrans-563
  prefs: []
  type: TYPE_IMG
- en: 'Note that in the preceding equation, our action is ![](img/B15558_12_355.png).
    Remember how we used two Q functions parameterized by ![](img/B15558_12_359.png)
    and ![](img/B15558_12_360.png) to avoid overestimation bias? Now, which Q function
    should we use in the preceding objective function? We can use either of the functions
    and so we use the Q function parameterized by ![](img/B15558_12_216.png) and write
    our final objective function as:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_362.png)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
- en: Now that we have understood how the SAC algorithm works, let's recap what we
    have learned so far and how the SAC algorithm works exactly by putting all the
    concepts together.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-567
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s recall the notations to understand SAC better. We use five networks—four
    critic networks (two value networks and two Q networks) and one actor network:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: The main value network parameter is represented by ![](img/B15558_12_363.png)
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target value network parameter is represented by ![](img/B15558_12_364.png)
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two main Q network parameters are represented by ![](img/B15558_12_216.png)
    and ![](img/B15558_12_206.png)
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actor network (policy network) parameter is represented by ![](img/B15558_10_152.png)
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target state value is represented by *y*[v], and the target Q value is represented
    by *y*[q]
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAC is an actor-critic method, and so the parameters of SAC will get updated
    at every step of the episode. Now, let's get started and understand how SAC works.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize the main network parameter of the value network ![](img/B15558_12_363.png),
    two Q network parameters ![](img/B15558_12_216.png) and ![](img/B15558_12_206.png),
    and the actor network parameter ![](img/B15558_12_371.png). Next, we initialize
    the target value network parameter ![](img/B15558_12_364.png) by just copying
    the main network parameter ![](img/B15558_12_302.png) and then we initialize the
    replay buffer ![](img/B15558_12_374.png).
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each step in the episode, first, we select an action *a* using the
    actor network:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_375.png)'
  id: totrans-577
  prefs: []
  type: TYPE_IMG
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_12_376.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_09_124.png).
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: Next, we randomly sample a minibatch of *K* transitions from the replay buffer.
    These *K* transitions (*s*, *a*, *r*, *s'*) are used for updating our value, Q,
    and actor network.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us compute the loss of the value network. We learned that the loss
    function of the value network is:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_323.png)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_12_379.png) is the target state value and it is given as
    ![](img/B15558_12_380.png).
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the loss, we calculate the gradients and update the parameter
    ![](img/B15558_12_302.png) of the value network using gradient descent: ![](img/B15558_12_325.png).'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we compute the loss of the Q networks. We learned that the loss function
    of the Q network is:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_383.png)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_12_384.png) is the target Q value and it is given as ![](img/B15558_12_385.png).
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the loss, we calculate the gradients and update the parameter
    of the Q networks using gradient descent:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_386.png)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
- en: 'Next, we update the actor network. We learned that the objective of the actor
    network is:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_362.png)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
- en: 'Now, we calculate gradients and update the parameter ![](img/B15558_12_283.png)
    of the actor network using gradient ascent:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_389.png)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
- en: 'Finally, in the end, we update the target value network parameter by soft replacement,
    as shown here:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_390.png)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
- en: We repeat the preceding steps for several episodes and improve the policy. To
    get a better understanding of how SAC works, let's look into the SAC algorithm
    in the next section.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – SAC
  id: totrans-596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SAC algorithm is given as follows:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main value network parameter ![](img/B15558_12_302.png), the
    Q network parameters ![](img/B15558_12_211.png) and ![](img/B15558_12_206.png),
    and the actor network parameter ![](img/B15558_12_371.png)
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target value network ![](img/B15558_12_395.png) by just copying
    the main value network parameter ![](img/B15558_12_302.png)
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_12_259.png)
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat step 5
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0,…, *T* – 1:'
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an action *a* based on the policy ![](img/B15558_12_398.png), that is,
    ![](img/B15558_12_399.png)
  id: totrans-603
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_376.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_124.png)
  id: totrans-604
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer
  id: totrans-605
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target state value ![](img/B15558_12_380.png)
  id: totrans-606
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of value network ![](img/B15558_12_323.png) and update the
    parameter using gradient descent, ![](img/B15558_12_325.png)
  id: totrans-607
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target Q value ![](img/B15558_12_385.png)
  id: totrans-608
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the Q networks ![](img/B15558_12_383.png) and update the
    parameter using gradient descent, ![](img/B15558_12_386.png)
  id: totrans-609
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute gradients of the actor objective function, ![](img/B15558_10_093.png)
    and update the parameter using gradient ascent, ![](img/B15558_12_409.png)
  id: totrans-610
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target value network parameter as ![](img/B15558_12_410.png)
  id: totrans-611
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Many congratulations on learning the several important state-of-the-art actor-critic
    algorithms, including DDPG, twin delayed DDPG, and SAC. In the next chapter, we will
    examine several state-of-the-art policy gradient algorithms.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-613
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding the DDPG algorithm. We learned that
    DDPG is an actor-critic algorithm where the actor estimates the policy using policy
    gradient and the critic evaluates the policy produced by the actor using the Q
    function. We learned how DDPG uses a deterministic policy and how it is used in
    environments with a continuous action space.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: Later, we looked into the actor and critic components of DDPG in detail and
    understood how they work, before finally learning about the DDPG algorithm.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we learned about the twin delayed DDPG, which is the successor to
    DDPG and constitutes an improvement to the DDPG algorithm. We learned the key
    features of TD3, including clipped double Q learning, delayed policy updates,
    and target policy smoothing, in detail and finally, we looked into the TD3 algorithm.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about the SAC algorithm. We learned that,
    unlike DDPG and TD3, the SAC method uses a stochastic policy. We also understood
    how SAC works with the entropy bonus in the objective function, and we learned
    what is meant by maximum entropy reinforcement learning.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn the state-of-the-art policy gradient algorithms
    such as trust region policy optimization, proximal policy optimization, and actor-critic
    using Kronecker-factored trust region.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s put our knowledge of actor-critic methods to the test. Try answering
    the following questions:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of actor and critic networks in DDPG?
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the critic in DDPG work?
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the key features of TD3?
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need clipped double Q learning?
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is target policy smoothing?
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is maximum entropy reinforcement learning?
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of the critic network in SAC?
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-628
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, refer to the following papers:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous Control with Deep Reinforcement Learning** by *Timothy P. Lillicrap,
    et al.*, [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Addressing Function Approximation Error in Actor-Critic Methods** by *Scott
    Fujimoto, Herke van Hoof, David Meger,* [https://arxiv.org/pdf/1802.09477.pdf](https://arxiv.org/pdf/1802.09477.pdf)'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
    with a Stochastic Actor** by *Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey
    Levine*, [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
