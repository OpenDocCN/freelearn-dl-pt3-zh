- en: Deep Feed-forward Neural Networks - Implementing Digit Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **feed-forward neural network** (**FNN**) is a special type of neural network
    wherein links/connections between neurons do not form a cycle. As such, it is
    different from other architectures in a neural network that we will get to study
    later on in this book (recurrent-type neural networks). The FNN is a widely used
    architecture and it was the first and simplest type of neural network.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go through the architecture of a typical ;FNN, and
    we will be using the TensorFlow library for this. After covering these concepts,
    we will give a practical example of digit classification. The question of this
    example is, *Given a set of images that contain handwritten digits, how can you
    classify these images into 10 different classes (0-9)*?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Hidden units and architecture design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MNIST dataset analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Digit classification - model building and training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden units and architecture design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next section, we'll recap artificial neural networks; they can do a good
    job in classification tasks such as classifying handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have the network shown in *Figure 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fff2d218-3570-4cd7-8ed1-e85114799780.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Simple FNN with one hidden layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the leftmost layer in this network is called the **input
    layer**, and the neurons within the layer are called **input neurons**. The rightmost
    or output layer contains the output neurons, or, as in this case, a single output
    neuron. The middle layer is called a **hidden layer**, since the neurons in this
    layer are neither inputs nor outputs. The term hidden perhaps sounds a little
    mysterious—the first time I heard the term, I thought it must have some deep philosophical
    or mathematical significance—but it really means *not an input and not an output*.
    It means nothing else. The preceding network has just a single hidden layer, but
    some networks have multiple hidden layers. For example, the following four-layer
    network has two hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01c99930-1ba8-40ec-ac97-bd50e301540a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Artificial neural network with more hidden layers'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture in which the input, hidden, and output layers are organized
    is very straightforward. For example, let's go through a practical example to
    see whether a specific handwritten image has the digit 9 in it or not.
  prefs: []
  type: TYPE_NORMAL
- en: So first, we will feed the pixels of the input image to the input layer; for
    example, in the MNIST dataset, we have monochrome images. Each one of them is
    28 by 28, so we need to have 28 × 28 = 784 neurons in the input layer to receive
    this input image.
  prefs: []
  type: TYPE_NORMAL
- en: In the output layer, we will need only 1 neuron, which produces a probability
    (or score) of whether this image has the digit 9 in it or not. For example, an
    output value of more than 0.5 means that this image has the digit 9, and if it's
    less than 0.5, then it means that the input image doesn't have the digit 9 in
    it.
  prefs: []
  type: TYPE_NORMAL
- en: So these types of networks, where the output from one layer is fed as an input
    to the next layer, are called FNNs. This kind of sequentiality in the layers means
    that there are no loops in it.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST dataset analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to get our hands dirty by implementing a classifier
    for handwritten images. This kind of implementation could be considered as the
    *Hello world!* of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'MNIST is a widely used dataset for benchmarking machine learning techniques.
    The dataset contains a set of handwritten digits like the ones shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/080b85fa-6251-42d9-b069-a96ac276eefe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Sample digits from the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: So, the dataset includes handwritten images and their corresponding labels as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to train a basic model on these images and the
    goal will be to tell which digit is handwritten in the input images.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you'll find out that we will be able to accomplish this classification
    task using very few lines of code, but the idea behind this implementation is
    to understand the basic bits and pieces for building a neural network solution.
    Moreover, we are going to cover the main concepts of neural networking in this
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MNIST data is hosted on Yann LeCun''s website ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
    Fortunately, TensorFlow provides some helper functions to download the dataset,
    so let''s start off by downloading the dataset using the following two lines of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The MNIST data is split into three parts: 55,000 data points of training data
    (`mnist.train`), 10,000 points of test data (`mnist.test`), and 5,000 points of
    validation data (`mnist.validation`). This split is very important; it''s essential
    in machine learning that we have separate data that we don''t learn from so that
    we can make sure that what we''ve learned actually generalizes!'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, every MNIST sample has two parts: an image of a handwritten
    digit and its corresponding label. Both the training set and test set contain
    images and their corresponding labels. For example, the training images are `mnist.train.images`
    and the training labels are `mnist.train.labels`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each image is 28 pixels by 28 pixels. We can interpret this as a big array
    of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67eb24ef-9a39-4345-b9ec-c1e6c2b020a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: MNIST digit in matrix representation (intensity values)'
  prefs: []
  type: TYPE_NORMAL
- en: In order to feed this matrix of pixel values to the input layer of the neural
    network, we need to flatten this matrix into a vector of 784 values. So, the final
    shape of the dataset will be a bunch of 784-dimensional vector space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is that `mnist.train.images` is a tensor with a shape of `(55000,
    784)`. The first dimension is an index of the list of images and the second dimension
    is the index for each pixel in each image. Each entry in the tensor is a pixel
    intensity between 0 and 1 for a particular pixel in a particular image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b211b0d9-6625-41f0-b873-a2ddab250156.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: MNIST data analysis'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, each image in the dataset has its corresponding
    label that ranges from 0 to 9.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purposes of this implementation, we''re going to encode our labels
    as one-hot vectors. A one-hot vector is a vector of all zeros except the index
    of the digit that this vector represents. For example, 3 would be [0,0,0,1,0,0,0,0,0,0].
    Consequently, `mnist.train.labels` is a `(55000, 10)` array of floats:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e497ee6-2d08-4004-a666-f1d242eb2060.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: MNIST data analysis'
  prefs: []
  type: TYPE_NORMAL
- en: Digit classification – model building and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's go ahead and build our model. So, we have 10 classes in our dataset
    0-9 and the goal is to classify any input image into one of these classes. Instead
    of giving a hard decision about the input image by saying only which class it
    could belong to, we are going to produce a vector of 10 possible values (because
    we have 10 classes). It'll represent the probabilities of each digit from 0-9
    being the correct class for the input image.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we feed the model with a specific image. The model might
    be 70% sure that this image is 9, 10% sure that this image is 8, and so on. So,
    we are going to use the softmax regression here, which will produce values between
    0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'A softmax regression has two steps: first we add up the evidence of our input
    being in certain classes, and then we convert that evidence into probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: To tally up the evidence that a given image is in a particular class, we do
    a weighted sum of the pixel intensities. The weight is negative if that pixel
    having a high intensity is evidence against the image being in that class, and
    positive if it is evidence in favor.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7* shows the weights one model learned for each of these classes. Red
    represents negative weights, while blue represents positive weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/650f626c-81a7-4e1c-a0c2-a756f5780c21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Weights one model learned for each of MNIST classes'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also add some extra evidence called a **bias**. Basically, we want to be
    able to say that some things are more likely independent of the input. The result
    is that the evidence for a class *i*, given an input, *x*, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a99b106-2fd3-404f-bb8f-b2c894db0e26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*W[i]* is the weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b[i]* is the bias for class *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*j* is an index for summing over the pixels in our input image *x*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then convert the evidence tallies into our predicted probabilities *y* using
    the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = softmax(evidence)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, softmax is serving as an activation or link function, shaping the output
    of our linear function into the form we want, in this case, a probability distribution
    over 10 cases (because we have 10 possible classes from 0-9). You can think of
    it as converting tallies of evidence into probabilities of our input being in
    each class. It''s defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*softmax(evidence) = normalize(exp(evidence))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you expand that equation, you get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a32b978-b54b-411e-b8fb-cf0132cbd402.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But it''s often more helpful to think of softmax the first way: exponentiating
    its inputs and then normalizing them. Exponentiation means that one more unit
    of evidence increases the weight given to any hypothesis exponentially. And conversely,
    having one less unit of evidence means that a hypothesis gets a fraction of its
    earlier weight. No hypothesis ever has zero or negative weight. Softmax then normalizes
    these weights so that they add up to one, forming a valid probability distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can picture our softmax regression as looking something like the following,
    although with a lot more *x*''*s*. For each output, we compute a weighted sum
    of the *x*''s, add a bias, and then apply softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66464f07-4b6d-4ba2-b7a6-28833847370b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Visualization of softmax regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we write that out as equations, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88eb5cd6-723b-4cef-807e-c578cc50020c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Equation representation of the softmax regression'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use vector notation for this procedure. This means that we''ll be turning
    it into a matrix multiplication and vector addition. This is very helpful for
    computational efficiency and readability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf06fcbb-5b11-4a96-a7f2-d6cd04e85b5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Vectorized representation of the softmax regression equation'
  prefs: []
  type: TYPE_NORMAL
- en: 'More compactly, we can just write:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = softmax(W[x] + b)*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's turn that into something that TensorFlow can use.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, let''s go ahead and start implementing our classifier. Let''s start off
    by importing the required packages for this implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we are going to define some helper functions to make us able to subset
    from the original dataset that we have downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we''re going to define two helper functions for displaying specific digits
    from the dataset or even display a flattened version of a subset of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's get down to business and start playing around with the dataset. So
    we are going to define the training and testing examples that we would like to
    load from the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll get down to the business of building and training our model. First,
    we define variables with how many training and test examples we would like to
    load. For now, we will load all the data, but we will change this value later
    on to save resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So now, we have a training set of 55,000 samples of handwritten digits, and
    each sample is 28 by 28 pixel images flattened to be a 784-dimensional vector.
    We also have their corresponding labels in a one-hot encoding format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `target_values_train` data are the associated labels for all the `input_values_train`
    samples. In the following example, the array represents a 7 in one-hot encoding
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/750c677e-5094-4b5b-b780-f544ae8a31ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: One hot encoding for the digit 7'
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s visualize a random image from the dataset and see how it looks like,
    so we are going to use our preceding helper function to display a random digit
    from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6f0caec6-5918-4765-ba6a-f783062558bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Output digit of the display_digit method'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also visualize a bunch of flattened images using the helper function
    defined before. Each value in the flattened vector represents a pixel intensity,
    so visualizing the pixels will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e51ef11a-ea66-4d8c-9e75-ec0806f9085b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: First 400 training examples'
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we haven''t started to build our computational graph for this classifier.
    Let''s start off by creating the session variable that will be responsible for
    executing the computational graph we are going to build:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we are going to define our model''s placeholders, which will be used
    to feed data into the computational graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When we specify `None` in our placeholder's first dimension, it means the placeholder
    can be fed as many examples as we like. In this case, our placeholder can be fed
    any number of examples, where each example has a `784` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to define another placeholder for feeding the image labels. Also
    we''ll be using this placeholder later on to compare the model predictions with
    the actual labels of the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define the `weights` and `biases`. These two variables will be
    the trainable parameters of our network and they will be the only two variables
    needed to make predictions on unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: I like to think of these `weights` as 10 cheat sheets for each number. This
    is similar to how a teacher uses a cheat sheet to grade a multiple choice exam.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now define our softmax regression, which is our classifier function.
    This particular classifier is called **multinomial logistic regression**, and
    we make the prediction by multiplying the flattened version of the digit by the
    weight and then adding the bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s ignore the softmax and look at what''s inside the softmax function.
    `matmul` is the TensorFlow function for multiplying matrices. If you know matrix
    multiplication ([https://en.wikipedia.org/wiki/Matrix_multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)),
    you''ll understand that this computes properly and that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/872f2c41-24e3-4684-a856-8ec72f4f0aa7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Will result in a number of training examples fed (**m**) × number of classes
    (**n**) matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da19af56-1dbd-4cf7-a371-c8af23bc0538.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Simple matrix multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can confirm it by evaluating `softmax_layer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's experiment with the computational graph that we have defined previously
    with three samples from the training set and see how it works. To execute the
    computational graph, we need to use the session variable that we defined before.
    And we need to initialize the variables using `tf.global_variables_initializer()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and only feed three samples to the computational graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can see the model predictions for the three training samples that
    fed to it. At the moment, the model has learned nothing about our task because
    we haven't gone through the training process yet, so it just outputs 10% probability
    of each digit being the correct class for the input samples.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, softmax is an activation function that squashes
    the output to be between 0 and 1, and the TensorFlow implementation of softmax
    ensures that all the probabilities of a single input sample sums up to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s experiment a bit with the softmax function of TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Next up, we need to define our loss function for this model, which will measure
    how good or bad our classifier is while trying to assign a class for the input
    images. The accuracy of our model is calculated by making a comparison between
    the actual values that we have in the dataset and the predictions that we got
    from the model.
  prefs: []
  type: TYPE_NORMAL
- en: The goal will be to reduce any misclassifications between the actual and predicted
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-entropy is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95cbcc07-4d8d-4902-bcdc-0cd64709a406.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* is our predicted probability distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y''* is the true distribution (the one-hot vector with the digit labels)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some rough sense, cross-entropy measures how inefficient our predictions
    are for describing the actual input.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement the cross-entropy function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes the log of all our predictions from `softmax_layer` (whose
    values range from 0 to 1) and multiplies them element-wise ([https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29))
    by the example''s true value, `output_values`. If the `log` function for each
    value is close to zero, it will make the value a large negative number (`-np.log(0.01)
    = 4.6`), and if it is close to one, it will make the value a small negative number
    (`-np.log(0.99) = 0.1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/559d0f5f-c295-4752-b80b-091aa75944ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Visualization for Y = log (x)'
  prefs: []
  type: TYPE_NORMAL
- en: We are essentially penalizing the classifier with a very large number if the
    prediction is confidently incorrect and a very small number if the prediction
    is confidently correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple Python example of a softmax prediction that is very confident
    that the digit is a 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create an array label of 3 as a ground truth to compare to our softmax
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Can you guess what value our loss function gives us? Can you see how the log
    of `j` would penalize a wrong answer with a large negative number? Try this to
    understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return nine zeros and a value of 0.1053; when they all are summed
    up, we can consider this a good prediction. Notice what happens when we make the
    same prediction for what is actually a 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, our `cross_entropy` function gives us 4.6051, which shows a heavily penalized,
    poorly made prediction. It was heavily penalized due to the fact the classifier
    was very confident that it was a 3 when it actually was a 2.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we begin to train our classifier. In order to train it, we have to develop
    appropriate values for W and b that will give us the lowest possible loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is where we can now assign custom variables for training if we
    wish. Any value that is in all caps as follows is designed to be changed and messed
    with. In fact, I encourage it! First, use these values, and then notice what happens
    when you use too few training examples or too high or low of a learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now initialize all variables so that they can be used by our TensorFlow
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to train the classifier using the gradient descent algorithm.
    So we first define our training method and some variables for measuring the model
    accuracy. The variable `train` will perform the gradient descent optimizer with
    a chosen learning rate in order to minimize the model loss function `model_cross_entropy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we'll define a loop that iterates `num_iterations` times. And for each
    loop, it runs training, feeding in values from `input_values_train` and `target_values_train`
    using `feed_dict`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to calculate accuracy, it will test the model against the unseen data
    in `input_values_test` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the loss was still decreasing near the end but our accuracy slightly
    went down! This shows that we can still minimize our loss and hence maximize the
    accuracy over our training data, but this may not help us predict the testing
    data used for measuring accuracy. This is also known as **overfitting** (not generalizing).
    With the default settings, we got an accuracy of about 91%. If I wanted to cheat
    to get 94% accuracy, I could've set the test examples to 100\. This shows how
    not having enough test examples can give you a biased sense of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that this is a very inaccurate way to calculate our classifier's
    performance. However, we did this on purpose for the sake of learning and experimentation.
    Ideally, when training with large datasets, you train using small batches of training
    data at a time, not all at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the interesting part. Now that we have calculated our weight cheat
    sheet, we can create a graph with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8ad5bc0f-e6ee-4843-8205-588cde3de594.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Visualization of our weights from 0-9'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows the model weights from 0-9, which is the most important
    side of our classifier. All this amount of work of machine learning is done to
    figure out what the optimal weights are. Once they are calculated based on an
    optimization criteria, you have the **cheat sheet** and can easily find your answers
    using the learned weights.
  prefs: []
  type: TYPE_NORMAL
- en: The learned model makes its prediction by comparing how similar or different
    the input digit sample is to the red and blue weights. The darker the red, the
    better the hit; white means neutral and blue means misses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use the cheat sheet and see how our model performs on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/265bc5f7-4cda-47a3-8382-3d3faef8af97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at our softmax predictor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will give us a 10-dimensional vector, with each column containing
    one probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `argmax` function to find out the most probable digit to be
    the correct classification for our input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now, we get a correct classification from our network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use our knowledge to define a helper function that can select a random
    image from the dataset and test the model against it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'And now try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fdc05ce6-73ba-4b58-ad42-38c3adad66f9.png)'
  prefs: []
  type: TYPE_IMG
- en: We've got a correct classification again!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through a basic implementation of a FNN for our digit
    classification task. We also did a recap of the terminologies used in the neural
    network context.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we will build a sophisticated version of the digit classification model
    using some modern best practices and some tips and tricks to enhance the model's
    performance.
  prefs: []
  type: TYPE_NORMAL
