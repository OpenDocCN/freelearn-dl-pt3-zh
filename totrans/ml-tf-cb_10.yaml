- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers are deep learning architectures introduced by Google in 2017 that
    are designed to process sequential data for downstream tasks such as translation,
    question answering, or text summarization. In this manner, they aim to solve a
    similar problem to RNNs discussed in *Chapter 9*, *Recurrent Neural Networks*,
    but Transformers have a significant advantage as they do not require processing
    the data in order. Among other advantages, this allows a higher degree of parallelization
    and therefore faster training.
  prefs: []
  type: TYPE_NORMAL
- en: Due to their flexibility, Transformers can be pretrained on large bodies of
    unlabeled data and then finetuned for other tasks. Two main groups of such pretrained
    models are **Bidirectional Encoder Representations from Transformers** (**BERTs**)
    and **Generative Pretrained Transformers** (**GPTs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text classification: sarcasm detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll begin by demonstrating the text generation capabilities of GPT-2 – one
    of the most popular Transformer architectures usable by a broader audience. While
    sentiment analysis can be handled by RNNs as well (as demonstrated in the previous
    chapter), it is the generative capabilities that most clearly demonstrate the
    impact of introducing Transformers into the Natural Language Processing stack.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first GPT model was introduced in a 2018 paper by Radford et al. from OpenAI
    – it demonstrated how a generative language model can acquire knowledge and process
    long-range dependencies thanks to pretraining on a large, diverse corpus of contiguous
    text. Two successor models (trained on more extensive corpora) were released in
    the following years: GPT-2 in 2019 (1.5 billion parameters) and GPT-3 in 2020
    (175 billion parameters). In order to strike a balance between demonstration capabilities
    and computation requirements, we will be working with GPT-2 – as of the time of
    writing, access to the GPT-3 API is limited.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll begin by demonstrating how to generate your own text based on a prompt
    given to the GPT-2 model without any finetuning.
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be making use of the excellent Transformers library created by Hugging
    Face ([https://huggingface.co/](https://huggingface.co/)). It abstracts away several
    components of the building process, allowing us to focus on the model performance
    and intended performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we begin by loading the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the advantages of the Transformers library – and a reason for its popularity,
    undoubtedly – is how easily we can download a specific model (and also define
    the appropriate tokenizer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It is usually a good idea to fix the random seed to ensure the results are
    reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For a proper description of the decoder architecture within Transformers, please
    refer to the *See also* section at the end of this section – for now, let us focus
    on the fact that how we decode is one of the most important decisions when using
    a GPT-2 model. Below, we review some of the methods that can be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: 'With **greedy search**, the word with the highest probability is predicted
    as the next word in the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our input sequence, we encode it and then call a `decode` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the results leave some room for improvement: the model starts
    repeating itself, because the high-probability words mask the less-likely ones
    so they cannot explore more diverse combinations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple remedy is **beam search**: we keep track of the alternative variants,
    so that more comparisons are possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is definitely more diverse – the message is the same, but at least the
    formulations look a little different from a style point of view.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can explore sampling – indeterministic decoding. Instead of following
    a strict path to find the end text with the highest probability, we rather randomly
    pick the next word by its conditional probability distribution. This approach
    risks producing incoherent ramblings, so we make use of the `temperature` parameter,
    which affects the probability mass distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Waxing poetic a bit. What happens if we increase the temperature?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is getting more interesting, although it still feels a bit like a train
    of thought – which is perhaps to be expected, given the content of our prompt.
    Let's explore some more ways to tune the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In **Top-K sampling**, the top *k* most likely next words are selected and
    the entire probability mass is shifted to these *k* words. So instead of increasing
    the chances of high-probability words occurring and decreasing the chances of
    low-probability words, we just remove low-probability words altogether:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This seems like a step in the right direction. Can we do better?
  prefs: []
  type: TYPE_NORMAL
- en: 'Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead
    of choosing the top *k* most likely words, we choose the smallest set of words
    whose total probability is larger than *p*, and then the entire probability mass
    is shifted to the words in this set. The main difference here is that with Top-K
    sampling, the size of the set of words is static (obviously), whereas in Top-P
    sampling, the size of the set can change. To use this sampling method, we just
    set `top_k = 0` and choose a `top_p` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can combine both approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Clearly, the more-sophisticated method''s settings can give us pretty impressive
    results. Let''s explore this avenue more – we''ll use the prompts taken from OpenAI''s
    GPT-2 website, where they feed them to a full-sized GPT-2 model. This comparison
    will give us an idea of how well we are doing with a local (smaller) model compared
    to a full one that was used for the original demos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For comparison, this is the output from a complete model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '0*: In a* *shocking finding, scientist discovered a herd of unicorns living
    in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising
    to the researchers was the fact that the unicorns spoke perfect English.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*"This is not only a scientific finding; it is also a very important finding
    because it will enable us to further study the* *phenomenon," said Dr. Jorge Llamas,
    from the* **National Institute of Anthropology and History** *(***INAH***) in
    Colombia, in a statement.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*"We have previously found that humans have used human voices to communicate
    with the animals. In this case, the animals are communicating with us. In other
    words, this is a breakthrough in the field of animal communication," added Llamas...*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In another example, it seems like the trepidations of the model authors were
    justified: GPT-2 can in fact generate fake news stories.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood
    Boulevard today. The star* *was spotted trying on three dresses before attempting
    to walk out of the store.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Abercrombie is one of a number of stores the star has frequented.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The singer was spotted walking into Abercrombie & Fitch in West Hollywood
    just after noon this afternoon before leaving the store.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The star is currently in the middle of a tour of Australia and New Zealand
    for her X Factor appearance on February 28....*'
  prefs: []
  type: TYPE_NORMAL
- en: What about riffing off literature classics like Tolkien?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing
    war cry.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Then the orcs* *made their move.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Great Orc Warband advanced at the sound of battle. They wore their weapons
    proudly on their chests, and they looked down upon their foes.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In the distance, the orcs could be heard shouting their orders in a low voice.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*But the battle was not yet over. The orcs'' axes and hammers slammed into
    the enemy ranks as though they were an army of ten thousand warriors, and their
    axes made the orcs bleed.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In the midst of the carnage, the Elven leader Aragorn cried out: "Come, brave.
    Let us fight the orcs!"*'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the examples above, a GPT-2 model working out of the box
    (without finetuning) can already generate plausible-looking long-form text. Assessing
    the future impact of this technology on the field of communication remains an
    open and highly controversial issue: on the one hand, there is fully justified
    fear of fake news proliferation (see the Miley Cyrus story above). This is particularly
    concerning because large-scale automated detection of generated text is an extremely
    challenging topic. On the other hand, GPT-2 text generation capabilities can be
    helpful for creative types: be it style experimentation or parody, an AI-powered
    writing assistant can be a tremendous help.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple excellent resources online for text generation with GPT-2:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original OpenAI post that introduced the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/
    )'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Top GPT-2 open source projects:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://awesomeopensource.com/projects/gpt-2](https://awesomeopensource.com/projects/gpt-2
    )'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Hugging Face documentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate
    )'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/model_doc/gpt2.html](https://huggingface.co/transformers/model_doc/gpt2.html
    )'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll demonstrate how DistilBERT – a lightweight version
    of BERT – can be used to handle a common problem of sentiment analysis. We will
    be using data from a Kaggle competition ([https://www.kaggle.com/c/tweet-sentiment-extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)):
    given a tweet and the sentiment (positive, neutral, or negative), participants
    needed to identify the part of the tweet that defines that sentiment. Sentiment
    analysis is typically employed in business as part of a system that helps data
    analysts gauge public opinion, conduct detailed market research, and track customer
    experience. An important application is medical: the effect of different treatments
    on patients'' moods can be evaluated based on their communication patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, we begin by loading the necessary packages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to streamline the code, we define some helper functions for cleaning
    the text: we remove website links, starred-out NSFW terms, and emojis.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Load the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_10_1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Sample of the tweet sentiment analysis data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The snapshot above demonstrates a sample of the data we will focus our analysis
    on: the complete text, the key phrase, and its associated sentiment (positive,
    negative, or neutral).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We proceed with fairly standard preprocessing of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`basic_cleaning` – to remove website URLs and non-characters and to replace
    * swear words with the word swear.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`remove_html`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`remove_emojis`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`remove_multiplechars` – this is for when there are more than 3 characters
    in a row in a word, for example, wayyyyy. The function removes all but one of
    the letters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As for labels, we one-hot encode the targets, tokenize them, and convert them
    into sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'DistilBERT is a light version of BERT: it has 40 pct fewer parameters, but
    achieves 97% of the performance. For the purpose of this recipe, we will use it
    primarily for its tokenizer and an embedding matrix. Although the matrix is trainable,
    we shall not utilize this option, in order to reduce the training time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We proceed with the usual steps for defining a model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the above output, the model converges quite rapidly and achieves
    a reasonable accuracy of 76% on the validation set already after 10 iterations.
    Further finetuning of hyperparameters and longer training can improve the performance,
    but even at this level, a trained model – for example, through the use of TensorFlow
    Serving – can provide a valuable addition to the sentiment analysis logic of a
    business application.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The best starting point is the documentation by Hugging Face: [https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/).'
  prefs: []
  type: TYPE_NORMAL
- en: Open-domain question answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a passage of text and a question related to that text, the idea of **Question
    Answering** (**QA**) is to identify the subset of the passage that answers the
    question. It is one of many tasks where Transformer architectures have been applied
    successfully. The Transformers library has a number of pretrained models for QA
    that can be applied even in the absence of a dataset to finetune on (a form of
    zero-shot learning).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, different models might fail at different examples and it might be
    useful to examine the reasons. In this section, we''ll demonstrate the TensorFlow
    2.0 GradientTape functionality: it allows us to record operations on a set of
    variables we want to perform automatic differentiation on. To explain the model''s
    output on a given input, we can:'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encode the input – unlike integer tokens (typically used in this context),
    a one-hot-encoding representation is differentiable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate GradientTape and watch our input variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute a forward pass through the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the gradients of the output of interest (for example, a specific class logit)
    with respect to the *watched* input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the normalized gradients as explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code in this section is adapted from the results published by Fast Forward
    Labs: [https://experiments.fastforwardlabs.com/](https://experiments.fastforwardlabs.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, we need some boilerplate: begin with a function for fetching pretrained
    QA models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Identify the span of the answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We need some functions for data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We'll compare the performance of a small set of models across a range of questions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Format the results for easier inspection.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_10_2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Sample records demonstrating the answers generated by different
    models'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can observe from the results data, even on this sample dataset there
    are marked differences between the models:'
  prefs: []
  type: TYPE_NORMAL
- en: DistilBERT (SQUAD1) can answer 5/8 questions, 2 correct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DistilBERT (SQUAD2) can answer 7/8 questions, 7 correct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT base can answer 5/8 questions, 5 correct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT large can answer 7/8 questions, 7 correct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the results above, we can gain some insight into the workings of BERT-based
    QA models:'
  prefs: []
  type: TYPE_NORMAL
- en: In a situation where a BERT model fails to produce an answer (for example, it
    only gives CLS), almost none of the input tokens have high normalized gradient
    scores. This suggests room for improvement in terms of the metrics used – going
    beyond explanation scores and potentially combining them with model confidence
    scores to gain a more complete overview of the situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the performance difference between the base and large variants of
    the BERT model suggests that the trade-off (better performance versus longer inference
    time) should be investigated further.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking into account the potential issues with our selection of the evaluation
    dataset, a possible conclusion is that DistilBERT (trained on SQuAD2) performs
    better than base BERT – which highlights issues around using SQuAD1 as a benchmark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
