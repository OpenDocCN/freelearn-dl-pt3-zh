<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Object Detection</h1>
                </header>
            
            <article>
                
<p class="mce-root">Detecting and classifying objects in images is a challenging problem. So far, we have treated the issue of image classification on a simple level; in a real-life scenario, we are unlikely to have pictures containing just one object. In industrial environments, it is possible to set up cameras <span>and mechanical supports</span> <span>to capture images of single objects. However, even in constrained environments, such as an industrial one, it is not always possible to have such a strict setup. Smartphone applications,</span> automated guided vehicles, and, more generally, any real-life application that uses images captured in a non-controlled environment require the simultaneous localization and classification of several objects in the input images. Object detection is the process of localizing an object into an image by predicting the coordinates of a bounding box that contains it, while at the same time correctly classifying it.</p>
<p class="mce-root">State-of-the-art methods to tackle object detection problems are based on convolutional neural networks that, as we will see in this chapter, can be used not only to extract meaningful classification features but also to regress the coordinates of the bounding box. Being a challenging problem, it is better to start with the foundations. Detecting and classifying more than one object at the same time requires the convolutional architecture to be designed and trained in a more complicated way than the one needed to solve the same problem with a single object. The tasks of regressing the bounding box coordinates of a single object and classifying the content are called <strong>localization and classification</strong>. Solving this task is the starting point to develop more complicated architectures that address the object detection task.</p>
<p>In this chapter, we will look at both problems; we start from the foundations, developing a regression network completely, and then extending it to perform both regression and classification. The chapter ends with an introduction to anchor-based detectors, since a complete implementation of an object detection network goes beyond the scope of this book.</p>
<p>The dataset used throughout the chapter is the PASCAL Visual Object Classes Challenge 2007.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Getting the data</li>
<li>Object localization</li>
<li>Classification and localization</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p class="mce-root">Object detection is a supervised learning problem that requires a considerable amount of data to reach good performance. The process of carefully annotating images by drawing bounding boxes around the objects and assigning them the correct labels is a time-consuming process that requires several hours of repetitive work.</p>
<p>Fortunately, there are already several datasets for object detection that are ready to use. The most famous is the ImageNet dataset, immediately followed by the PASCAL VOC 2007 dataset. To be able to use ImageNet, dedicated hardware is required since its size and number of labeled objects per image makes the object detection task hard to tackle.</p>
<p>PASCAL VOC 2007, instead, consists of only 9,963 images in total, each of them with a different number of labeled objects belonging to the 20 selected object classes. The twenty object classes are as follows:</p>
<ul>
<li><strong>Person</strong>:<span> P</span>erson</li>
<li><strong>Animal</strong>:<span> B</span>ird, cat, cow, dog, horse, sheep</li>
<li><strong>Vehicle</strong>:<span> A</span>irplane, bicycle, boat, bus, car, motorbike, train</li>
<li><strong>Indoor</strong>:<span> B</span>ottle, chair, dining table, potted plant, sofa, tv/monitor</li>
</ul>
<p>As described in the official dataset page (<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/">http://host.robots.ox.ac.uk/pascal/VOC/voc2007/</a>), the dataset already comes with three splits (train, validation, and test) ready to use. <span>The data has been split into 50% for training/validation and 50% for testing. The distributions of images and objects by class are approximately equal across the training/validation and test sets. In total there are 9,963 images, containing 24,640 annotated objects.</span></p>
<p>TensorFlow datasets allow us to download <span>the whole dataset </span>with a single line of code (approximately 869 MiB) and to obtain the <kbd>tf.data.Dataset</kbd> object of every split:</p>
<p><kbd>(tf2)</kbd></p>
<pre><span>import tensorflow as tf <br/>import tensorflow_datasets as tfds <br/><br/># Train, test, and validation are datasets for object detection: multiple objects per image. <br/>(train, test, validation), info = tfds.load( <br/> "voc2007", split=["train", "test", "validation"], with_info=True <br/>)<br/></span></pre>
<p class="mce-root">As usual, TensorFlow datasets give us a lot of useful information about the dataset format. The output that follows is the result of <kbd>print(info)</kbd>:</p>
<pre><span>tfds.core.DatasetInfo( <br/>    name='voc2007', <br/>    version=1.0.0, <br/>    description='This dataset contains the data from the PASCAL Visual Object Classes Challenge <br/>2007, a.k.a. VOC2007, corresponding to the Classification and Detection <br/>competitions. <br/>A total of 9,963 images are included in this dataset, where each image contains <br/>a set of objects, out of 20 different classes, making a total of 24,640 <br/>annotated objects. <br/>In the Classification competition, the goal is to predict the set of labels <br/>contained in the image, while in the Detection competition the goal is to <br/>predict the bounding box and label of each individual object. <br/>', <br/>    urls=['http://host.robots.ox.ac.uk/pascal/VOC/voc2007/'], <br/>    features=FeaturesDict({ <br/>        'image': Image(shape=(None, None, 3), dtype=tf.uint8), <br/>        'image/filename': Text(shape=(), dtype=tf.string, encoder=None), <br/>        'labels': Sequence(shape=(None,), dtype=tf.int64, feature=ClassLabel(shape=(), dtype=tf.int64, num_classes=20)), <br/>        'labels_no_difficult': Sequence(shape=(None,), dtype=tf.int64, feature=ClassLabel(shape=(), dtype=tf.int64, num_classes=20)), <br/>        'objects': SequenceDict({'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20), 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), 'pose': ClassLabel(shape=(), dtype=tf.int64, num_classes=5), 'is_truncated': Tensor(shape=(), dtype=tf.bool), 'is_difficult'<br/>: Tensor(shape=(), dtype=tf.bool)}) <br/>    }, <br/>    total_num_examples=9963, <br/>    splits={ <br/>        'test': &lt;tfds.core.SplitInfo num_examples=4952&gt;, <br/>        'train': &lt;tfds.core.SplitInfo num_examples=2501&gt;, <br/>        'validation': &lt;tfds.core.SplitInfo num_examples=2510&gt; <br/>    }, <br/>    supervised_keys=None, <br/>    citation='""" <br/>        @misc{pascal-voc-2007, <br/>          author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.", <br/>          title = "The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults", <br/>          howpublished = "http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html"} <br/>         <br/>    """',<br/>    redistribution_info=, <br/>)</span></pre>
<p>For every image, there is a <kbd>SequenceDict</kbd> object that contains the information of every labeled object present. Something handy when working with any data related project is to visualize the data. In this case in particular, since we are trying to solve a computer vision problem, visualizing the images and the bounding boxes can help us to have a better understanding of the difficulties the network should face during the training.</p>
<p>To visualize the labeled images, we use <kbd>matplotlib.pyplot</kbd> together with the usage of the <kbd>tf.image</kbd> package; the former is used to display the images, and the latter is used to draw the bounding boxes and to convert them to <kbd>tf.float32</kbd> (thus scaling the values in the [0,1] range). <span>Moreover, how to use the </span><kbd>tfds.ClassLabel.int2str</kbd><span> method is shown; this method is convenient since it allows us to get the text representation of a label from its numerical representation:</span></p>
<p><span> </span><kbd>(tf2)</kbd></p>
<pre>import matplotlib.pyplot as plt</pre>
<p><span>From the training set, take five images, draw the bounding box, and then print the class:</span></p>
<pre><span>with tf.device("/CPU:0"): <br/>    for row in train.take(5): <br/>        obj = row["objects"] <br/>        image = tf.image.convert_image_dtype(row["image"], tf.float32) <br/> <br/>        for idx in tf.range(tf.shape(obj["label"])[0]): <br/>            image = tf.squeeze( <br/>                tf.image.draw_bounding_boxes( <br/>                    images=tf.expand_dims(image, axis=[0]), <br/>                    boxes=tf.reshape(obj["bbox"][idx], (1, 1, 4)), <br/>                    colors=tf.reshape(tf.constant((1.0, 1.0, 0, 0)), (1, 4)), <br/>                ), <br/>                axis=[0], <br/>            ) <br/> <br/>            print( <br/>                "label: ", info.features["objects"]["label"].int2str(obj["label"][idx]) <br/>            ) <br/> </span></pre>
<p><span>Then, plot the image using the following code:</span></p>
<pre><span>            plt.imshow(image)<br/>            plt.show()</span></pre>
<p>The following images are a collage of the five images produced by the code snippet:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-942 image-border" src="assets/a0987746-b915-42a3-80cc-3e059f3178d9.png" style="width:34.92em;height:31.33em;"/></p>
<div class="mce-root packt_infobox"><span>Please note that since TensorFlow dataset shuffles the data when it creates the TFRecords, it is unlikely that the same execution on a different machine will produce the same sequence of images. </span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">It is also worth noting that partial objects are annotated as full objects; for instance, the human hand on the bottom-left image is labeled as a person, and <span>the rear wheel of a motorbike present in the bottom right of the picture</span> is marked as a motorbike .</p>
<p>The object detection task is challenging by its nature, but looking at the data, we can see that the data itself is hard to use. In fact, the labels printed to the standard output for the bottom-right image are:</p>
<ul>
<li>Person</li>
<li>Bird</li>
</ul>
<p>Thus, the dataset contains full objects annotated and labeled (bird) and also partial objects annotated and labeled as the whole object (for example, the human hand is labeled as a person). This small example shows how difficult object detection is: the network should be able to classify and localize, for example, people from their attributes (a hand) or from their complete shape while dealing with the problem of occlusion.</p>
<p>Looking at the data gives us a better idea of how challenging the problem is. However, before facing the challenge of object detection, it is better to start from the foundations by tackling the problem of localization and classification. Therefore, we have to filter the dataset objects to extract only the images that contain a single labeled object. For this reason, a simple function that accepts a <kbd>tf.data.Dataset</kbd> object as input and applies a filter on it can be defined and used. <span>Create a subset of the dataset by filtering the elements: we are interested in creating a dataset for object detection and classification, that is, a dataset of images with a single object annotated:</span></p>
<p><kbd>(tf2)</kbd></p>
<pre><span>def filter(dataset): <br/>    return dataset.filter(lambda row: tf.equal(tf.shape(row["objects"]["label"])[0], 1)) <br/> <br/>train, test, validation = filter(train), filter(test), filter(validation)<br/></span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Using the same snippet as earlier, we can visualize some of the images to check if everything goes as we expect:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-943 image-border" src="assets/1e52f0ef-e349-41e4-9cc1-69633671986f.png" style="width:37.92em;height:31.75em;"/></p>
<p class="CDPAlignLeft CDPAlign">We can see images that contain only a single object, sampled from the training set, drawn using the previous code snippet after applying the <kbd>filter</kbd> function. The <kbd>filter</kbd> function returns a new dataset that contains only the elements of the input dataset that contain a single bounding box, hence the perfect candidates to train a single network for classification and localization.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object localization </h1>
                </header>
            
            <article>
                
<p>Convolutional neural networks (CNNs) are extremely flexible objects—so far, we have used them to solve classification problems, making them learn to extract features specific to the task. As shown in <a href="699c6d94-72fa-4636-8e23-6da8928847b6.xhtml">Chapter 6</a>, <em>Image Classification Using TensorFlow Hub</em>, the standard architecture of CNNs designed to classify images is made of two parts—the feature extractor, which produces a feature vector, and a set of fully connected layers that classifies the feature vector in the (hopefully) correct class:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-944 image-border" src="assets/78335c1a-7738-41d9-8705-3b16ad17e97e.png" style="width:95.58em;height:32.67em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">The classifier placed on top of the feature vector can also be seen as the head of the network</div>
<p class="mce-root">The fact that, so far, CNNs have only been used to solve classification problems should not mislead us. These types of networks are extremely powerful, and, especially in their multilayer setting, they can be used to solve many different kinds of problems, extracting information from the visual input.</p>
<p class="mce-root">For this reason, solving the localization and classification problem is just a matter of adding a new head to the network, the localization head.</p>
<p class="mce-root">The input data is an image that contains a single object together with the four coordinates of the bounding box. So the idea is to use this information to solve the classification and localization problem at the same time by treating the localization as a regression problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Localization as a regression problem</h1>
                </header>
            
            <article>
                
<p>Ignoring for a moment the classification problem and focusing only on the localization part, we can think about the localization as the problem of regressing the four coordinates of the bounding box that contains the subject of the input image.</p>
<p class="mce-root"/>
<p class="mce-root">In practice, there is not much difference between training a CNN to solve a classification task or a regression task: the architecture of the feature extractor remains the same, while the classification head changes and becomes a regression head. In the very end, this only means to change the number of output neurons from the number of classes to 4, one neuron per coordinate of the bounding box.</p>
<p class="mce-root">The idea is that the regression head should learn to output the correct coordinates when certain input features are present.</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-945 image-border" src="assets/a0293f3f-3b42-4702-bb64-3599c35c2d81.png" style="width:92.50em;height:32.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The AlexNet architecture used as a feature extractor and the classification head replaced with a regression head with four output neurons</div>
<p>To make the network learn to regress the coordinates of the bounding box of an object, we have to express the input/output relationship between the neurons and the labels (that is, the four coordinates of the bounding box, present in the dataset) using a loss function.</p>
<p>The L2 distance can be effectively used as the loss function: the goal is to regress correctly all the four coordinates and thus minimize the distance between the predicted values and the real one, making it tend to zero:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/36609293-b950-431f-818c-ffc25791c50b.png" style="width:15.58em;height:2.08em;"/></p>
<p>Where the first tuple <sub><img class="fm-editor-equation" src="assets/e8fd1f5e-9e42-4b2c-8952-cec3d47769c6.png" style="width:7.67em;height:1.50em;"/></sub> is the regression head output, and the second tuple <sub><img class="fm-editor-equation" src="assets/be5cedad-3e23-4a0e-8dd2-acd229159641.png" style="width:6.58em;height:1.33em;"/></sub> represents the ground truth bounding box coordinates.</p>
<p>Implementing a regression network in TensorFlow 2.0 is straightforward. As shown in <a href="https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=31&amp;action=edit#post_30">Chapter 6</a><span>, </span><em>Image Classification Using TensorFlow Hub</em>, TensorFlow Hub can be used to speed up the training phase by using it to download and embed a pretrained feature extractor in our model.</p>
<div class="packt_infobox">A detail that is worth pointing out is the format that TensorFlow uses to represent the bounding box coordinates (and the coordinates in general)—the format used is <span><kbd>[ymin, xmin, ymax, xmax]</kbd> , and the coordinates are normalized in the [0,1] range, in order not to depend on the original image resolution.</span></div>
<p>Using TensorFlow 2.0 and TensorFlow Hub, we can define and train a coordinate regression network on the PASCAL VOC 2007 dataset in a few lines of code.</p>
<p>Using the Inception v3 network from TensorFlow Hub as the backbone of the coordinate regression network, defining the regression model is straightforward. Although the network has a sequential structure, we define it using the functional API since this will allow us to extend the model easily without the need to rewrite it:</p>
<p><kbd>(tf2)</kbd></p>
<pre>import tensorflow_hub as hub<br/><br/>inputs = tf.keras.layers.Input(shape=(299,299,3))<br/>net = hub.KerasLayer(<br/>        "https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2",<br/>        output_shape=[2048],<br/>        trainable=False,<br/>      )(inputs)<br/>net = tf.keras.layers.Dense(512)(net)<br/>net = tf.keras.layers.ReLU()(net)<br/>coordinates = tf.keras.layers.Dense(4, use_bias=False)(net)<br/><br/>regressor = tf.keras.Model(inputs=inputs, outputs=coordinates)</pre>
<p>Moreover, since we decided to use the inception network that needs a 299 x 299 input image resolution with values in the [0,1] range, we have to add an additional step to the input pipeline to prepare the data:</p>
<p><kbd>(tf2)</kbd></p>
<pre>def prepare(dataset):<br/>    def _fn(row):<br/>        row["image"] = tf.image.convert_image_dtype(row["image"], tf.float32)<br/>        row["image"] = tf.image.resize(row["image"], (299, 299))<br/>        return row<br/><br/>    return dataset.map(_fn)<br/><br/>train, test, validation = prepare(train), prepare(test), prepare(validation)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>As previously introduced, the loss function to use is the standard L2 loss, which already comes implemented in TensorFlow as a Keras loss that can be found in the <kbd>tf.losses</kbd> package. However, instead of using <kbd>tf.losses.MeanSquaredError</kbd>, it is worth defining the loss function ourselves since there is a detail to highlight.</p>
<p>If we decide to use the implemented <strong>Mean Squared Error</strong> (<strong>MSE</strong>) function, we have to take into account that under the hood, the <kbd>tf.subtract</kbd> operation is used. This operation simply computes the subtraction of the left-hand operation with the right-hand operand. This behavior is what we are looking for, of course, but the subtraction operation in TensorFlow follows the NumPy broadcasting semantic (as almost any mathematical operation). This particular semantic broadcasts the value of the left-side tensor to the right-side tensor, and if the right-side tensor has a dimension of 1 where the value of the left-side tensor is copied.</p>
<p>Since we selected the images with only one object inside, we have a single bounding box present in the <kbd>"bbox"</kbd> attribute. Hence, if we pick a batch size of 32, the tensor that contains the bounding box will have a shape of <kbd>(32, 1, 4)</kbd>. The 1 in the second position can cause problems in the loss computation and preventing the model from converging .</p>
<p>Thus, we have two options:</p>
<ul>
<li>Define the loss function using Keras, removing the unary dimension by using <kbd>tf.squeeze</kbd></li>
<li>Define the loss function manually</li>
</ul>
<p>In practice, defining the loss function manually allows us to place the <kbd>tf.print</kbd> statements in the body function, which can be used for a raw debugging process and, more importantly, to define the training loop in the standard way, making the loss function itself taking care of squeezing the unary dimension where needed:</p>
<p><kbd>(tf2)</kbd></p>
<pre># First option -&gt; this requires to call the loss l2, taking care of squeezing the input<br/># l2 = tf.losses.MeanSquaredError()<br/><br/># Second option, it is the loss function iself that squeezes the input<br/>def l2(y_true, y_pred):<br/>    return tf.reduce_mean(<br/>        tf.square(y_pred - tf.squeeze(y_true, axis=[1]))<br/>    )</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The training loop is straightforward, and it can be implemented in two different ways:</p>
<ul>
<li>Writing a custom training loop (thus using the <kbd>tf.GradientTape</kbd> object)</li>
<li>Using the <kbd>compile</kbd> and <kbd>fit</kbd> methods of the Keras model, since this is a standard training loop that Keras can build for us</li>
</ul>
<p>However, since we are interested in extending this solution in the next sections, it is better to start using the custom training loop, as it gives more freedom for customization. Moreover, we are interested in visualizing the ground truth and the predicted bounding box, by logging them on TensorBoard.</p>
<p>Therefore, before defining the training loop, it is worth defining a <kbd>draw</kbd> function that takes a dataset, the model, and the current step, and using them to draw both the ground truth and the predicted boxes:</p>
<p><kbd>(tf2)</kbd></p>
<pre>def draw(dataset, regressor, step):<br/>    with tf.device("/CPU:0"):<br/>        row = next(iter(dataset.take(3).batch(3)))<br/>        images = row["image"]<br/>        obj = row["objects"]<br/>        boxes = regressor(images)<br/>        tf.print(boxes)<br/><br/>        images = tf.image.draw_bounding_boxes(<br/>            images=images, boxes=tf.reshape(boxes, (-1, 1, 4))<br/>        )<br/>        images = tf.image.draw_bounding_boxes(<br/>            images=images, boxes=tf.reshape(obj["bbox"], (-1, 1, 4))<br/>        )<br/>        tf.summary.image("images", images, step=step)</pre>
<p>The training loop for our coordinate regressor (that can also be thought of as a region proposal, since it is now aware of the label of the object it's detecting in the images), which also logs on TensorBoard the training loss value and the prediction on three images sampled from the training and validation set (using the <kbd>draw</kbd> function), can be easily defined: </p>
<ol>
<li><span>D</span>efine the <kbd>global_step</kbd> variable, used to keep track of the training iterations, followed by the definition of the file writers, used to log the train and validation summaries:</li>
</ol>
<pre style="padding-left: 60px"><span>optimizer = tf.optimizers.Adam() <br/>epochs = 500 <br/>batch_size = 32 <br/> <br/>global_step = tf.Variable(0, trainable=False, dtype=tf.int64) <br/> <br/>train_writer, validation_writer = ( <br/>    tf.summary.create_file_writer("log/train"), <br/>    tf.summary.create_file_writer("log/validation"), <br/>) <br/>with validation_writer.as_default(): <br/>    draw(validation, regressor, global_step) <br/> </span></pre>
<ol start="2">
<li>Following the TensorFlow 2.0 best practice, we can define the training step as a function and convert it to its graph representation using <kbd>tf.function</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>@tf.function <br/>def train_step(image, coordinates): <br/>    with tf.GradientTape() as tape: <br/>        loss = l2(coordinates, regressor(image)) <br/>    gradients = tape.gradient(loss, regressor.trainable_variables) <br/>    optimizer.apply_gradients(zip(gradients, regressor.trainable_variables)) <br/>    return loss <br/> </span></pre>
<ol start="3">
<li>Define the training loop over the batches and invoke the <kbd>train_step</kbd> function on every iteration:</li>
</ol>
<pre style="padding-left: 60px"><span>train_batches = train.cache().batch(batch_size).prefetch(1) <br/>with train_writer.as_default(): <br/>    for _ in tf.range(epochs): <br/>        for batch in train_batches: <br/>            obj = batch["objects"] <br/>            coordinates = obj["bbox"] <br/>            loss = train_step(batch["image"], coordinates) <br/>            tf.summary.scalar("loss", loss, step=global_step) <br/>            global_step.assign_add(1) <br/>            if tf.equal(tf.mod(global_step, 10), 0): <br/>                tf.print("step ", global_step, " loss: ", loss) <br/>                with validation_writer.as_default(): <br/>                    draw(validation, regressor, global_step) <br/>                with train_writer.as_default(): <br/>                    draw(train, regressor, global_step)</span></pre>
<p>Although the Inception network is used as a fixed feature extractor, the training process can take a few hours on a CPU and almost half an hour on a GPU.</p>
<p>The following screenshot shows the visible trend of the loss function during the training:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-946 image-border" src="assets/065c98e0-6122-4e3a-a8f1-0ae70395309a.png" style="width:126.58em;height:37.67em;"/></p>
<p class="CDPAlignLeft CDPAlign">We see that<span> from the early training steps, the loss value is close to zero, although oscillations are present during the whole training process.</span></p>
<p>During the training process, in the images tab of TensorBoard, we can visualize the images with the regressed and ground truth bounding boxes drawn. Since we created two different summary writers (one for the training logs and the other for the validation logs), TensorFlow visualizes for us the images logged for the two different splits:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-947 image-border" src="assets/cf1356bb-95db-44f3-af09-41f9a77c51fd.png" style="width:117.17em;height:70.42em;"/></p>
<p class="CDPAlignLeft CDPAlign">The preceding images are samples from the training (first row) and validation (second row) set, with the ground truth and regressed bounding boxes. The regressed bounding boxes in the training set are close to the ground truth boxes, while the regressed boxes on the validation images are different.</p>
<p>The training loop previously defined has various problems:</p>
<ul>
<li>The only measured metric is the L2 loss</li>
<li>The validation set is never used to measure any numerical score</li>
<li>No check for overfitting is present</li>
<li>There is a complete lack of a metric that measures how good the regression of the bounding box is, measured on both the training and the validation set</li>
</ul>
<p>The training loop can, therefore, be improved by measuring an object detection metric; measuring the metric also reduces the training time since we can stop the training earlier. Moreover, from the visualization of the results, it is pretty clear that the model is overfitting the training set, and a regularization layer, such as dropout, can be added to address this problem. The problem of regressing a bounding box can be treated as a binary classification problem. In fact, there are only two possible outcomes: ground truth bounding box matched or not matched.</p>
<p>Of course, having a perfect match is not an easy task; for this reason, a function that measures how good the detected bounding box is <span>with a numerical score</span> (with respect to the ground truth) is needed. The most widely used function to measure the goodness of localization is the <strong>Intersection over Union</strong> (<strong>IoU</strong>), which we will explore in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intersection over Union</h1>
                </header>
            
            <article>
                
<p>Intersection over Union (IoU) is defined as the ratio between the area of overlap and the area of union. The following image is a graphical representation of IoU:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-948 image-border" src="assets/d078024a-428b-4e25-a723-f3d0c50838d3.png" style="width:36.17em;height:24.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Credits: Jonathan Hui (<a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173">https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173</a>)</div>
<p class="mce-root"/>
<p>In practice, the IoU measures <em>how much</em> the predicted bounding box overlaps with the ground truth. Since IoU is a metric that uses the areas of the objects, it can be easily expressed treating the ground truth and the detected area like sets. Let A be the set of the proposed object pixels and B the set of true object pixels; then IoU is defined as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/79a3faa7-f117-4f73-b9e1-878c13ebcc7e.png" style="width:5.25em;height:2.00em;"/></p>
<p>The IoU value is in the [0,1] range, where 0 is a no-match (no overlap), and 1 is the perfect match. The IoU value is used as an overlap criterion; usually, an IoU value greater than 0.5 is considered as a true positive (match), while any other value is regarded as a false positive. There are no true negatives.</p>
<p>Implementing the IoU formula in TensorFlow is straightforward. The only subtlety to take into account is that de-normalizing the coordinates is needed since the area should be computed in pixels. The conversion in pixel coordinates together with the coordinate swapping in a more friendly representation is implemented in the <kbd>_swap</kbd> closure:</p>
<p><kbd>(tf2)</kbd></p>
<pre>def iou(pred_box, gt_box, h, w):<br/>    """<br/>    Compute IoU between detect box and gt boxes<br/>    Args:<br/>        pred_box: shape (4,): y_min, x_min, y_max, x_max - predicted box<br/>        gt_boxes: shape (4,): y_min, x_min, y_max, x_max - ground truth<br/>        h: image height<br/>        w: image width<br/>    """</pre>
<p>Transpose the coordinates from <kbd>y_min</kbd>, <kbd>x_min</kbd>, <kbd>y_max</kbd>, and <kbd>x_max</kbd> in absolute coordinates to <kbd>x_min</kbd>, <kbd>y_min</kbd>, <kbd>x_max</kbd>, and <kbd>y_max</kbd> in pixel coordinates:</p>
<pre>    def _swap(box):<br/>        return tf.stack([box[1] * w, box[0] * h, box[3] * w, box[2] * h])<br/><br/>    pred_box = _swap(pred_box)<br/>    gt_box = _swap(gt_box)<br/><br/>    box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])<br/>    area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])<br/>    xx1 = tf.maximum(pred_box[0], gt_box[0])<br/>    yy1 = tf.maximum(pred_box[1], gt_box[1])<br/>    xx2 = tf.minimum(pred_box[2], gt_box[2])<br/>    yy2 = tf.minimum(pred_box[3], gt_box[3])</pre>
<p class="mce-root"/>
<p>Then, compute the width and height of the bounding box:</p>
<pre>    w = tf.maximum(0, xx2 - xx1)<br/>    h = tf.maximum(0, yy2 - yy1)<br/><br/>    inter = w * h<br/>    return inter / (box_area + area - inter)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Average precision</h1>
                </header>
            
            <article>
                
<p>A value of IoU greater than a specified threshold (usually 0.5) allows us to treat the bounding box regressed as a match.</p>
<p><span>In the case of single class prediction, having the number of <strong>true positives</strong> (<strong>TP</strong>) and <strong>false positives</strong> (<strong>FP</strong>) measured on the dataset allows us to compute the average precision as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/377c65ae-3390-47f8-8dbb-9235a35a05bd.png" style="width:7.67em;height:2.33em;"/></p>
<p>In the object detection challenges, the <strong>Average Precision</strong> (<strong>AP</strong>) is often measured for different values of IoU. The minimum requirement is to measure the AP for an IoU value of 0.5, but achieving good results with a half overlap is not sufficient in most real-life scenarios. Usually, in fact, a bounding box prediction is required to match at least a value of IoU of 0.75 or 0.85 to be useful.</p>
<p>So far we have treated the AP for the single-class case, but it is worth it to treat the more general multi-class object detection scenario.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean Average Precision</h1>
                </header>
            
            <article>
                
<p>In the case of a multi-class detection, where every bounding box regressed can contain one of the available classes, the standard metric used to evaluate the performance of the object detector is the <strong>Mean Average Precision</strong> (<strong>mAP</strong>).</p>
<p>Computing it is straightforward—the mAP is the average precision for each class in the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4fe09f9c-4399-4297-a2ed-385872534493.png" style="width:16.17em;height:2.75em;"/></p>
<p class="mce-root"/>
<p>Knowing the metrics to use for object detection, we can improve the training script by adding this measurement on the validation set at the end of every training epoch and measuring it on a batch of training data every ten steps. Since the model defined so far is just a coordinate regressor without classes, the measured metric will be the AP.</p>
<p>Implementing the mAP in TensorFlow is trivial since in the <kbd>tf.metrics</kbd> package there is an implementation ready to use. The first parameter of the <kbd>update_state</kbd> method is the true labels; the second parameter is the predicted labels. For instance, for a binary classification problem, a possible scenario can be as follows:</p>
<p><kbd>(tf2)</kbd></p>
<pre>m = tf.metrics.Precision()<br/><br/>m.update_state([0, 1, 1, 1], [1, 0, 1, 1])<br/>print('Final result: ', m.result().numpy()) # Final result: 0.66</pre>
<div class="packt_infobox">It should also be noted that the average precision and the IoU are not object-detection-specific metrics, but they can be used whenever a localization task is performed (the IoU) and the precision of the detection is measured (the mAP).</div>
<p>In <a href="51f4dcda-add6-4e58-a660-75f34a7e5593.xhtml">Chapter 8</a>, <em>Semantic Segmentation and Custom Dataset Builder</em>, dedicated to the semantic segmentation task, the same metrics are used to measure the segmentation model performance. The only difference is that the IoU is measured at the pixel level and not using a bounding box. The training loop can be improved; in the next section, a draft of an improved training script is presented, but the real improvement is left as an exercise. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving the training script</h1>
                </header>
            
            <article>
                
<p>Measuring the mean average precision (over a single class) requires you to fix a threshold for the IoU measurement and to define the <kbd>tf.metrics.Precision</kbd> object that computes the mean average precision over the batches.</p>
<p>In order not to change the whole code structure, the <kbd>draw</kbd> function is used not only to draw the ground truth and regressed boxes, but also to measure the IoU and log the mean average precision summary:</p>
<p><kbd>(tf2)</kbd></p>
<pre># IoU threshold<br/>threshold = 0.75<br/># Metric object<br/>precision_metric = tf.metrics.Precision()<br/><br/>def draw(dataset, regressor, step):<br/>    with tf.device("/CPU:0"):<br/>        row = next(iter(dataset.take(3).batch(3)))<br/>        images = row["image"]<br/>        obj = row["objects"]<br/>        boxes = regressor(images)<br/><br/>        images = tf.image.draw_bounding_boxes(<br/>            images=images, boxes=tf.reshape(boxes, (-1, 1, 4))<br/>        )<br/>        images = tf.image.draw_bounding_boxes(<br/>            images=images, boxes=tf.reshape(obj["bbox"], (-1, 1, 4))<br/>        )<br/>        tf.summary.image("images", images, step=step)<br/><br/>        true_labels, predicted_labels = [], []<br/>        for idx, predicted_box in enumerate(boxes):<br/>            iou_value = iou(predicted_box, tf.squeeze(obj["bbox"][idx]), 299, 299)<br/>            true_labels.append(1)<br/>            predicted_labels.append(1 if iou_value &gt;= threshold else 0)<br/><br/>        precision_metric.update_state(true_labels, predicted_labels)<br/>        tf.summary.scalar("precision", precision_metric.result(), step=step)</pre>
<p><span>As an exercise (see the <em>Exercises</em> section), you can use this code as a baseline and restructure them, in order to improve the code organization. After improving the code organization, you are also invited to retrain the model and analyze the precision plot.</span></p>
<p>Object localization alone, without the information about the class of the object being localized, has a limited utility, but, in practice, it is the basis of any object detection algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification and localization</h1>
                </header>
            
            <article>
                
<p>An architecture like the one defined so far that has no information about the class of the object it's localizing is called a<strong> region proposal</strong>.</p>
<p>It is possible to perform object detection and localization using a single neural network. In fact, there is nothing stopping us adding a second head on top of the feature extractor and training it to classify the image and at the same time training the regression head to regress the bounding box coordinates.</p>
<p>Solving multiple tasks at the same time is the goal of multitask learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multitask learning</h1>
                </header>
            
            <article>
                
<p>Rich Caruna defines multi-task learning in his paper <em>Multi-task learning</em> (1997):</p>
<div class="packt_quote">"Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better."</div>
<p>In practice, multi-task learning is a machine learning subfield with the explicit goal of solving multiple different tasks, exploiting commonalities and differences across tasks. It has been empirically shown that using the same network to solve multiple tasks usually results in improved learning efficiency and prediction accuracy compared to the performance achieved by the same network trained to solve the same tasks separately.</p>
<p>Multi-task learning also helps to fight the overfitting problem since the neural network is less likely to adapt its parameters to solve a specific task, so it has to learn how to extract meaningful features that can be useful to solve different tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Double-headed network</h1>
                </header>
            
            <article>
                
<p>In the past few years, several architectures for object detection and classification have been developed using a two-step process. The first process was to use a region proposal to get regions of the input image that are likely to contain an object. The second step was to use a simple classifier on the proposed regions to classify the content.</p>
<p>Using a double-headed neural network allows us to have faster inference time, since only a single forward pass of a single model is needed to achieve better performance overall.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>From the architectural side, supposing for simplicity that our feature extractor is AlexNet (when, instead, it is the more complex network Inception V3), adding a new head to the network changes the model architecture as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-949 image-border" src="assets/01503375-3d15-4fef-a4ff-4e12d4403912.png" style="width:97.83em;height:34.33em;"/></p>
<p class="CDPAlignLeft CDPAlign">The preceding screenshot is a representation of what a classification and localization network looks like. The feature extract part should be able to extract features general enough to make the two heads solve the two different tasks, using the same shared features.</p>
<p>From the code side, as we used the Keras functional style model definition, adding the additional output to the model is straightforward. In fact, it is only a matter of adding the desired numbers of layers that compose the new head and adding the final layer to the outputs list of the Keras model definition. As may be obvious at this point in the book, this second head must end with a number of neurons equal to the number of classes the model will be trained to classify. In our case, the PASCAL VOC 2007 dataset contains 20 different classes. Therefore, we only have to define the model as follows:</p>
<p><kbd>(tf2)</kbd></p>
<ol>
<li>First, start with the input layer definition:</li>
</ol>
<pre style="padding-left: 60px">inputs = tf.keras.layers.Input(shape=(299, 299, 3))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>Then, using TensorFlow Hub, we define the fixed (non-trainable) feature extractor:</li>
</ol>
<pre style="padding-left: 60px">net = hub.KerasLayer(<br/>    "https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2",<br/>    output_shape=[2048],<br/>    trainable=False,<br/>)(inputs)</pre>
<ol start="3">
<li>Then, we define the regression head, which is just a stack of fully connected layers that end with four linear neurons (one per bounding box coordinate):</li>
</ol>
<pre style="padding-left: 60px">regression_head = tf.keras.layers.Dense(512)(net)<br/>regression_head = tf.keras.layers.ReLU()(regression_head)<br/>coordinates = tf.keras.layers.Dense(4, use_bias=False)(regression_head)</pre>
<ol start="4">
<li>Next, we define the classification head, which is just a stack of fully connected layers that is trained to classify the features extracted by the fixed (non-trainable) feature extractor:</li>
</ol>
<pre style="padding-left: 60px">classification_head = tf.keras.layers.Dense(1024)(net)<br/>classification_head = tf.keras.layers.ReLU()(classificatio_head)<br/>classification_head = tf.keras.layers.Dense(128)(net)<br/>classification_head = tf.keras.layers.ReLU()(classificatio_head)<br/>num_classes = 20<br/>classification_head = tf.keras.layers.Dense(num_classes, use_bias=False)(<br/>    classification_head<br/>)</pre>
<ol start="5">
<li>Finally, we can define the Keras model that will perform classification and localization. Please note that the model has a single input and two outputs:</li>
</ol>
<pre style="padding-left: 60px">model = tf.keras.Model(inputs=inputs, outputs=[coordinates, classification_head])</pre>
<p>Using TensorFlow datasets, we have all the information needed to perform both the classification and localization easily since every row is a dictionary that contains the label for every bounding box present in the image. Moreover, since we filtered the dataset in order to have only images with a single object inside, we can treat the training of the classification head exactly as the training of the classification model, as shown in <a href="https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=31&amp;action=edit#post_30">Chapter 6</a><span>, </span><em>Image Classification Using TensorFlow Hub</em>.</p>
<p>The implementation of the training script is left as an exercise for you (see the <em>Exercises</em> section). The only peculiarity of the training process is the loss function to use. In order to effectively train the network to perform different tasks at the same time, the loss function should contain different terms for each different task.</p>
<p>Usually, the weighted sum of different terms is used as a loss function. In our case, one term is the classification loss that can easily be the sparse categorical cross-entropy loss, and the other is the regression loss (the L2 loss previously defined):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9395cba6-22ff-4db6-8f8c-3a9d5d3e36d1.png" style="width:22.75em;height:1.42em;"/></p>
<p>The multiplicative factors <sub><img class="fm-editor-equation" src="assets/0006eaf3-b30f-43d7-b932-d3fdf746e9e1.png" style="width:3.67em;height:1.33em;"/></sub> are hyperparameters used to give more or less <em>importance</em> (strength of the gradient update) to the different tasks.</p>
<p>Classifying images with a single object inside and regressing the coordinate of the only bounding box present can be applied only in limited real-life scenarios. More often, instead, given an input image, it is required to localize and classify multiple objects at the same time (the real object detection problem).</p>
<p>Over the years, several models for object detection have been proposed, and the ones that recently outperformed all the others are all based on the concept of anchor. We will explore anchor-based detectors in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Anchor-based detectors</h1>
                </header>
            
            <article>
                
<p>Anchor-based detectors rely upon the concept of anchor boxes to detect objects in images in a single pass, using a single architecture.</p>
<p>The intuitive idea of the anchor-based detectors is to split the input image into several regions of interests (the anchor boxes) and apply a localization and regression network to each of them. The idea is to make the network learn not only to regress the coordinates of a bounding box and classify its content, but also to use the same network to look at different regions of the image in a single forward pass.</p>
<p>To train these models, it is required not only to have a dataset with the annotated ground truth boxes, but also to add to every input image a new collection of boxes that overlap (with the desired amount of IoU) the ground truth boxes.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Anchor-boxes</h1>
                </header>
            
            <article>
                
<p>Anchor-boxes are a discretization of the input image in different regions, also called <strong>anchors</strong> or <strong>bounding boxes prior</strong>. The idea behind the concept of anchor-boxes is that the input can be discretized in different regions, each of them with a different appearance. An input image could contain big and small objects, and therefore the discretization should be made at different scales in order to detect the same time objects at different resolutions.</p>
<p>When discretizing the input in anchor boxes, the important parameters are as follows:</p>
<ul>
<li><strong>The grid size:</strong> How the input is evenly divided</li>
<li><strong>The box scale levels</strong>: Given the parent box, how to resize the current box</li>
<li><strong>The aspect ratio levels</strong>: For every box, the ratio between width and height</li>
</ul>
<p>The input image can be divided into a grid with cells of equal dimensions, for instance, a 4 x 4 grid. Each cell of this grid can then be resized with different scales (0.5, 1, 2, ...) and each of them with different levels of aspect ratios (0.5, 1, 2, ...). As an example, the following image shows how an image can be "covered" by anchor boxes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-950 image-border" src="assets/7922f964-ec85-43f2-90f6-def4ea0f519f.png" style="width:38.00em;height:25.83em;"/></p>
<p>The generation of the anchor boxes influences the network performance—the dimension of the smaller box represents the dimension of the smaller objects the network is able to detect. The same reasoning applies to the larger box as well.</p>
<p class="mce-root">In the last few years, anchor-based detectors have demonstrated they are capable of reaching astonishing detection performance, being not only accurate, but also faster.</p>
<p>The most famous anchor-based detector is <strong>You Only Look Once</strong> (<strong><span>YOLO</span></strong>), followed by <span><strong>Single Shot MultiBox Detector</strong> </span>(<strong><span>SSD</span></strong>). The following YOLO image<span> detects multiple objects in the image, at different scales, with a single forward pass:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-951 image-border" src="assets/79700b05-c3c0-4dde-beeb-f8472c0d7817.png" style="width:33.08em;height:23.83em;"/></p>
<p>The implementation of an anchor-based detector goes beyond the scope of this book due to the theory needed to understand the concepts and the complexity of these models. Therefore, only an intuitive idea of what happens when these models are used has been presented.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, the problem of object detection was introduced and some basic solutions were proposed. We first focused on the data required and used TensorFlow datasets to get the PASCAL VOC 2007 dataset ready to use in a few lines of code. Then, the problem of using a neural network to regress the coordinate of a bounding box was looked at, showing how a convolutional neural network can be easily used to produce the four coordinates of a bounding box, starting from the image representation. In this way, we build a region proposal, that is, a network able to suggest where in the input image a single object can be detected, without producing other information about the detected object.</p>
<p>After that, the concept of multi-task learning was introduced and how to add a classification head next to the regression head was shown by using the Keras functional API. Then, we covered a brief introduction about the anchor-based detectors. These detectors are used to solve the problem of object detection (the detection and classification of multiple objects in a single image) by discretizing the input in thousands of regions (anchors).</p>
<p>We used TensorFlow 2.0 and TensorFlow Hub together, allowing us to speed up the training process by using the Inception v3 model as a fixed feature extractor. Moreover, thanks to the quick execution, mixing pure Python and TensorFlow code simplified the way of defining the whole training process.</p>
<p>In the next chapter, we will learn about semantic segmentation and dataset builder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p><span>You can answer all the theoretical questions and, perhaps more importantly, struggle to solve all the code challenges that each exercise contains:</span></p>
<ol>
<li>In the <em>Getting the data</em> section, a filtering function was applied to the PASCAL VOC 2007 dataset to select only the images with a single object inside. The filtering process, however, doesn't take into account the class balancement.<br/>
Create a function that, given the three filtered datasets, merges them first and then creates three balanced splits (with a tolerable class imbalance, if it is not possible to have them perfectly balanced).</li>
<li>Use the splits created in the previous point to retrain the network for localization and classification defined in the chapter. How and why do the performances change?</li>
<li>What measures the Intersection over Union metric?</li>
</ol>
<ol start="4">
<li>What does an IoU value of 0.4 represent? A good or a bad match?</li>
<li>What is the Mean Average Precision? Explain the concept and write the formula.</li>
<li>What is multi-task learning?</li>
<li>Does multi-task learning improve or worsen the model performance on single tasks?</li>
<li>In the domain of object detection, what is an anchor?</li>
<li>Describe how an anchor-based detector looks at the input image during training and inference.</li>
<li>Are mAP and IoU object detection metrics only?</li>
<li>To improve the code of the object detection and localization network, add the support saving the model at the end of every epoch into a checkpoint and to restore the model (and the global step variable) status to continue the training process.</li>
<li>The code of the localization and regression networks explicitly use a <kbd>draw</kbd><span> </span>function that not only draws the bounding boxes but also measures the mAP. Improve the code quality by creating different functions for each different feature.</li>
<li>The code that measures network performance only uses three samples. This is wrong, can you explain the reason? Change the code in order to use a single training batch during the training and the whole validation set at the end of every epoch.</li>
<li>Define a training script for the model defined in the "Multi-Headed Network and Multi-Task learning": train the regression and classification head at the same time and measure the training and validation accuracy at the end of every training epoch.</li>
<li>Filter the PASCAL VOC train, validation, and test datasets to produce only images with at least a person inside (there can be other labeled objects present in the picture).</li>
<li>Replace the regression and classification head of the trained localization and classification network with two new heads. The classification head should now have a single neuron that represents the probability of the image to contain a person. The regression head should regress the coordinates of the objects labeled as a person.</li>
<li>Apply transfer learning to train the previously defined network. Stop the training process when the mAP on the person class stops increasing (with a tolerance of +/- 0.2) for 50 steps.</li>
<li>Create a Python script that generates anchor boxes at different resolutions and scales.</li>
</ol>


            </article>

            
        </section>
    </body></html>