<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Data Science - A Birds' Eye View</h1>
                </header>
            
            <article>
                
<p class="calibre2">Data science or machine learning is the process of giving the machines the ability to learn from a dataset without being told or programmed. For instance, it is extremely hard to write a program that can take a hand-written digit as an input image and outputs a value from 0-9 according to the image that's written. The same applies to the task of classifying incoming emails as spam or non-spam. For solving such tasks, data scientists use learning methods and tools from the field of data science or machine learning to teach the computer how to automatically recognize digits, by giving it some explanatory features that can distinguish one digit from another. The same for the spam/non-spam problem, instead of using regular expressions and writing hundred of rules to classify the incoming email, we can teach the computer through specific learning algorithms how to distinguish between spam and non-spam emails.</p>
<div class="packtinfobox">For the spam filtering application, you can code it by a rule-based approach, but it won't be good enough to be used in production, like the one in your mailing server. Building a learning system is an ideal solution for that.</div>
<p class="calibre2">You are probably using applications of data science on a daily basis, often without knowing it. For example, your country might be using a system to detect the ZIP code of your posted letter in order to automatically forward it to the correct area. If you are using Amazon, they often recommend things for you to buy and they do this by learning what sort of things you often search for or buy.</p>
<p class="calibre2">Building a learned/trained machine learning algorithm will require a base of historical data samples from which it's going to learn how to distinguish between different examples and to come up with some knowledge and trends from that data. After that, the learned/trained algorithm could be used for making predictions on unseen data. The learning algorithm will be using raw historical data and will try to come up with some knowledge and trends from that data.</p>
<p class="calibre2">In this chapter, we are going to have a bird's-eye view of data science, how it works as a black box, and the challenges that data scientists face on a daily basis. We are going to cover the following topics:</p>
<ul class="calibre7">
<li class="calibre8">Understanding data science by an example</li>
<li class="calibre8">Design procedure of data science algorithms</li>
<li class="calibre8">Getting to learn</li>
<li class="calibre8">Implementing the fish recognition/detection model</li>
<li class="calibre8">Different learning types</li>
<li class="calibre8">Data size and industry needs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding data science by an example</h1>
                </header>
            
            <article>
                
<p class="calibre2">To illustrate the life cycle and challenges of building a learning algorithm for specific data, let us consider a real example. The Nature Conservancy is working with other fishing companies and partners to monitor fishing activities and preserve fisheries for the future. So they are looking to use cameras in the future to scale up this monitoring process. The amount of data that will be produced from the deployment of these cameras will be cumbersome and very expensive to process manually. So the conservancy wants to develop a learning algorithm to automatically detect and classify different species of fish to speed up the video reviewing process.</p>
<p class="calibre2"><em class="calibre19">Figure 1.1</em> shows a sample of images taken by conservancy-deployed cameras. These images will be used to build the system.</p>
<div class="CDPAlignCenter">                           <img src="assets/46235fa5-ee68-43a2-b668-04a2d869461b.jpg" class="calibre22"/></div>
<div class="CDPAlignCenter1"> Figure 1.1: Sample of the conservancy-deployed cameras' output</div>
<p class="calibre2">So our aim in this example is to separate different species such as tunas, sharks, and more that fishing boats catch. As an illustrative example, we can limit the problem to only two classes, tuna and opah.</p>
<div class="CDPAlignCenter"><img src="assets/7ed0e228-c5ec-4727-a8be-5db5689cbdc0.png" class="calibre23"/></div>
<div class="CDPAlignCenter1">Figure 1.2: Tuna fish type (left) and opah fish type (right)</div>
<p class="calibre2">After limiting our problem to contain only two types of fish, we can take a sample of some  random images from our collection and start to note some physical differences between the two types. For example, consider the following physical differences:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Length</strong>: You can see that compared to the opah fish, the tuna fish is longer</li>
<li class="calibre8"><strong class="calibre1">Width</strong>: Opah is wider than tuna</li>
<li class="calibre8"><strong class="calibre1">Color</strong>: You can see that the opah fish tends to be more red while the tuna fish tends to be blue and white, and so on</li>
</ul>
<p class="calibre2">We can use these physical differences as features that can help our learning algorithm(classifier) to differentiate between these two types of fish.</p>
<div class="packtinfobox">Explanatory features of an object are something that we use in daily life to discriminate between objects that surround us. Even babies use these explanatory features to learn about the surrounding environment. The same for data science, in order to build a learned model that can discriminate between different objects (for example, fish type), we need to give it some explanatory features to learn from (<span>for example,</span> fish length). In order to make the model more certain and reduce the confusion error, we can increase (to some extent) the explanatory features of the objects.</div>
<p class="calibre2">Given that there are physical differences between the two types of fish, these two different fish populations have different models or descriptions. So the ultimate goal of our classification task is to get the classifier to learn these different models and then give an image of one of these two types as an input. The classifier will classify it by choosing the model (tuna model or opah model) that corresponds best to this image.</p>
<p class="calibre2">In this case, the collection of tuna and opah fish will act as the knowledge base for our classifier. Initially, the knowledge base (training samples) will be labeled/tagged, and for each image, you will know beforehand whether it's tuna or opah fish. So the classifier will use these training samples to model the different types of fish, and then we can use the output of the training phase to automatically label unlabeled/untagged fish that the classifier didn't see during the training phase.  This kind of unlabeled data is often called <strong class="calibre13">unseen</strong> <strong class="calibre13">data</strong>. The training phase of the life cycle is shown in the following diagram:</p>
<div class="packtinfobox">Supervised data science is all about learning from historical data with known target or output, such as the fish type, and then using this learned model to predict cases or data samples, for which we don't know the target/output.</div>
<div class="CDPAlignCenter"><img src="assets/5b6e7bf3-3fdf-4d67-9489-e84b3579b323.png" class="calibre24"/></div>
<div class="CDPAlignCenter1">Figure 1.3: Training phase life cycle</div>
<p class="calibre2">Let's have a look at how the training phase of the classifier will work:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Pre-processing</strong>: In this step, we will try to segment the fish from the image by using the relevant segmentation technique.</li>
<li class="calibre8"><strong class="calibre1">Feature extraction</strong>: After segmenting the fish from the image by subtracting the background, we will measure the physical differences (length, width, color, and so on) of each image. At the end, you will get something like <em class="calibre25">Figure 1.4</em>.</li>
</ul>
<p class="calibre2">Finally, we will feed this data into the classifier in order to model different fish types.</p>
<p class="calibre2">As we have seen, we can visually differentiate between tuna and opah fish based on the physical differences (features) that we proposed, such as length, width, and color.</p>
<p class="calibre2">We can use the length feature to differentiate between the two types of fish. So we can try to differentiate between the fish by observing their length and seeing whether it exceeds some value (<kbd class="calibre12">length*</kbd>) or not.</p>
<p class="calibre2">So, based on our training sample, we can derive the following rule:</p>
<pre class="calibre21">If length(fish)&gt; length* then label(fish) = Tuna<br class="title-page-name"/>Otherwise label(fish) = Opah </pre>
<p class="calibre2"><span class="calibre10">In order to find thi</span>s <kbd class="calibre12">length*</kbd> we ca<span class="calibre10">n somehow make length measurements based on our training samples. So, suppose we get these length measurements and obtain the histogram as follows:</span></p>
<div class="CDPAlignCenter">                   <img class="image-border" src="assets/4cb26cf7-26c3-4833-82ef-61e7e4d52aae.png"/></div>
<div class="CDPAlignCenter1">Figure 1.4: Histogram of the length  measurements for the two types of fish</div>
<p class="calibre2">In this case, we can derive a rule based on the length feature and differentiate the tuna and opah fish. In this particular example, we can tell that <kbd class="calibre12">length*</kbd> is <kbd class="calibre12">7</kbd>. So we can update the preceding rule to be:</p>
<pre class="calibre21">If length(fish)&gt; 7 then label(fish) = Tuna<br class="title-page-name"/>Otherwise label(fish) = Opah</pre>
<p class="calibre2">As you may notice, this is not a promising result because of the overlap between the two histograms, as the length feature is not a perfect one to use solely for differentiating between the two types. So we can try to incorporate more features such as the width and then combine them. So, if we somehow manage to measure the width of our training samples, we might get something like the histogram as follows:</p>
<div class="CDPAlignCenter"><img src="assets/0b538b23-036b-4def-8b53-49c40d475a53.png" class="calibre26"/></div>
<div class="CDPAlignCenter1">Figure 5: Histogram of width  measurements for the two types of fish</div>
<p class="calibre2">As you can see, depending on one feature only will not give accurate results and the output model will do lots of misclassifications. Instead, we can somehow combine the two features and come up with something that looks reasonable.</p>
<p class="calibre2">So if we combine both features, we might get something that looks like the following graph:</p>
<div class="CDPAlignCenter"><img class="image-border1" src="assets/63eb4b5d-7389-4bb0-9ebb-d4fd90b46a6f.png"/></div>
<div class="CDPAlignCenter1">Figure 1.6 : Combination between the subset of the length and width measurements for the two types of fish</div>
<p class="calibre2">Combining the readings for the <strong class="calibre13">length</strong> and <strong class="calibre13">width</strong> features, we will get a scatter plot like the one in the preceding graph. We have the red dots to represent the tuna fish and the green dots to represent the opah fish, and we can suggest this black line to be the rule or the decision boundary that will differentiate between the two types of fish.</p>
<p class="calibre2">For example, if the reading of one fish is above this decision boundary, then it's a tuna fish; otherwise, it will be predicted as an opah fish.</p>
<p class="calibre2">We can somehow try to increase the complexity of the rule to avoid any errors and get a decision boundary like the one in the following graph:</p>
<div class="CDPAlignCenter"><img class="image-border2" src="assets/f5b4dead-79fb-454c-8d30-6efc5b2f8a66.png"/></div>
<div class="CDPAlignCenter1">Figure 1.7: Increasing the complexity of the decision boundary to avoid misclassifications over the training data</div>
<p class="calibre2">The advantage of this model is that we get <span class="calibre10">almost</span><span class="calibre10"> </span><span class="calibre10">0 misclassifications over the training samples. But actually this is not the objective of using data science. The objective of data science is to build a model that will be able to generalize and perform well over the unseen data. In order to find out whether we built a model that will generalize or not, we are going to introduce a new phase called the <strong class="calibre13">testing phase</strong>, in which we give the trained model an unlabeled image and expect the model to assign the correct label (<strong class="calibre13">Tuna</strong> and <strong class="calibre13">Opah</strong>) to it.</span></p>
<div class="packttip">Data science's ultimate objective is to build a model that will work well in production, not over the training set. So don't be happy when you see your model is performing well on the training set, like the one in figure 1.7. Mostly, this kind of model will fail to work well in recognizing the fish type in the image. This incident of having your model work well only over the training set is called <strong class="calibre1">overfitting</strong>, and most practitioners fall into this trap.</div>
<p class="calibre2">Instead of coming up with such a complex model, you can drive a less complex one that will generalize in the testing phase. The following graph shows the use of a less complex model in order to get fewer misclassification errors and to generalize the unseen data as well:</p>
<div class="CDPAlignCenter"><img class="image-border3" src="assets/aeb1ceb4-39eb-4769-8653-5f4bdd6d8d1e.png"/></div>
<div class="CDPAlignCenter1">Figure 1.8: Using a less complex model in order to be able to generalize over the testing samples (unseen data)</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Design procedure of data science algorithms</h1>
                </header>
            
            <article>
                
<p class="calibre2">Different learning systems usually follow the same design procedure. They start by acquiring the knowledge base, selecting the relevant explanatory features from the data, going through a bunch of candidate learning algorithms while keeping an eye on the performance of each one, and finally the evaluation process, which measures how successful the training process was.</p>
<p class="calibre2">In this section, we are going to address all these different design steps in more detail:</p>
<div class="CDPAlignCenter"><img src="assets/4e2ddd2f-0dc8-485a-9b33-324696efa48f.png" class="calibre27"/></div>
<div class="CDPAlignCenter1">Figure 1.11: Model learning process outline</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data pre-processing</h1>
                </header>
            
            <article>
                
<p class="calibre2">This component of the learning cycle represents the knowledge base of our algorithm. So, in order to help the learning algorithm give accurate decisions about the unseen data, we need to provide this knowledge base in the best form. Thus, our data may need a lot of cleaning and pre-processing (conversions).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data cleaning</h1>
                </header>
            
            <article>
                
<p class="calibre2">Most datasets require this step, in which you get rid of errors, noise, and redundancies. We need our data to be accurate, complete, reliable, and unbiased, as there are lots of problems that may arise from using bad knowledge base, such as:</p>
<ul class="calibre7">
<li class="calibre8">Inaccurate and biased conclusions</li>
<li class="calibre8">Increased error</li>
<li class="calibre8">Reduced generalizability, which is the model's ability to perform well over the unseen data that it didn't train on previously</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data pre-processing</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this step, we apply some conversions to our data to make it consistent and concrete. There are lots of different conversions that you can consider while pre-processing your data:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Renaming</strong> (<strong class="calibre1">relabeling</strong>): This means converting categorical values to numbers, as categorical values are dangerous if used with some learning methods, and also numbers will impose an order between the values</li>
<li class="calibre8"><strong class="calibre1">Rescaling</strong> (<strong class="calibre1">normalization</strong>): Transforming/bounding continuous values to some range, typically <em class="calibre25">[-1, 1]</em> or <em class="calibre25">[0, 1]</em></li>
<li class="calibre8"><strong class="calibre1">New features</strong>: Making up new features from the existing ones. For example, <em class="calibre25">obesity-factor = weight/height</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection</h1>
                </header>
            
            <article>
                
<p class="calibre2">The number of explanatory features (input variables) of a sample can be enormous wherein you get <em class="calibre19">x<sub class="calibre28">i</sub>=(x<sub class="calibre28">i</sub><sup class="calibre29">1</sup>, x<sub class="calibre28">i</sub><sup class="calibre29">2</sup>, x<sub class="calibre28">i</sub><sup class="calibre29">3</sup>, ... , x<sub class="calibre28">i</sub><sup class="calibre29">d</sup>)</em> as a training sample (observation/example) and <em class="calibre19">d</em> is very large. An example of this can be a document classification task3, where you get 10,000 different words and the input variables will be the number of occurrences of different words.</p>
<p class="calibre2">This enormous number of input variables can be problematic and sometimes a curse because we have many input variables and few training samples to help us in the learning procedure. To avoid this curse of having an enormous number of input variables (curse of dimensionality), data scientists use dimensionality reduction techniques in order to select a subset from the input variables. For example, in the text classification task they can do the following:</p>
<ul class="calibre7">
<li class="calibre8">Extracting relevant inputs (for instance, mutual information measure)</li>
<li class="calibre8"><strong class="calibre1">Principal component analysis</strong> (<strong class="calibre1">PCA</strong>)</li>
<li class="calibre8">Grouping (cluster) similar words (this uses a similarity measure)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model selection</h1>
                </header>
            
            <article>
                
<p class="calibre2">This step comes after selecting a proper subset of your input variables by using any dimensionality reduction technique. Choosing the proper subset of the input variable will make the rest of the learning process very simple.</p>
<p class="calibre2">In this step, you are trying to figure out the right model to learn.</p>
<p class="calibre2">If you have any prior experience with data science and applying learning methods to different domains and different kinds of data, then you will find this step easy as it requires prior knowledge of how your data looks and what assumptions could fit the nature of your data, and based on this you choose the proper learning method. If you don't have any prior knowledge, that's also fine because you can do this step by guessing and trying different learning methods with different parameter settings and choose the one that gives you better performance over the test set.</p>
<p class="calibre2">Also, initial data analysis and visualization will help you to make a good guess about the form of the distribution and nature of your data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning process</h1>
                </header>
            
            <article>
                
<p class="calibre2">By learning, we mean the optimization criteria that you are going to use to select the best model parameters. There are various optimization criteria for that:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Mean square error</strong> (<strong class="calibre1">MSE</strong>)</li>
<li class="calibre8"><strong class="calibre1">Maximum likelihood</strong> (<strong class="calibre1">ML</strong>) criterion</li>
<li class="calibre8"><strong class="calibre1">Maximum a posterior probability</strong> (<strong class="calibre1">MAP</strong>)</li>
</ul>
<p class="calibre2">The optimization problem may be hard to solve, but the right choice of model and error function makes a difference.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating your model</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this step, we try to measure the generalization error of our model on the unseen data. Since we only have the specific data without knowing any unseen data beforehand, we can randomly select a test set from the data and never use it in the training process so that it acts like valid unseen data. There are different ways you can to evaluate the performance of the selected model:</p>
<ul class="calibre7">
<li class="calibre8">Simple holdout method, which is dividing the data into training and testing sets</li>
<li class="calibre8">Other complex methods, based on cross-validation and random subsampling</li>
</ul>
<p class="calibre2">Our objective in this step is to compare the predictive performance for different models trained on the same data and choose the one with a better (smaller) testing error, which will give us a better generalization error over the unseen data. You can also be more certain about the generalization error by using a statistical method to test the significance of your results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting to learn</h1>
                </header>
            
            <article>
                
<p class="calibre2">Building a machine learning system comes with some challenges and issues; we will try to address them in this section. Many of these issues are domain specific and others aren't.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges of learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">The following is an overview of the challenges and issues that you will typically face when trying to build a learning system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature extraction – feature engineering</h1>
                </header>
            
            <article>
                
<p class="calibre2">Feature extraction is one of the crucial steps toward building a learning system. If you did a good job in this challenge by selecting the proper/right number of features, then the rest of the learning process will be easy. Also, feature extraction is domain dependent and it requires prior knowledge to have a sense of what features could be important for a particular task. For example, the features for our fish recognition system will be different from the ones for spam detection or identifying fingerprints.</p>
<p class="calibre2">The feature extraction step starts from the raw data that you have. Then build derived variables/values (features) that are informative about the learning task and facilitate the next steps of learning and evaluation (generalization).</p>
<p class="calibre2">Some tasks will have a vast number of features and fewer training samples (observations) to facilitate the subsequent learning and generalization processes. In such cases, data scientists use dimensionality reduction techniques to reduce the vast number of features to a smaller set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Noise</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the fish recognition task, you can see that the length, weight, fish color, as well as the boat color may vary, and there could be shadows, images with low resolution, and other objects in the image. All these issues affect the significance of the proposed explanatory features that should be informative about our fish classification task.</p>
<p class="calibre2">Work-arounds will be helpful in this case. For example, someone might think of detecting the boat ID and mask out certain parts of the boat that most likely won't contain any fish to be detected by our system. This work-around will limit our search space.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overfitting</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we have seen in our fish recognition task, we have tried to enhance our model's performance by increasing the model complexity and perfectly classifying every single instance of the training samples. As we will see later, such models do not work over unseen data (such as the data that we will use for testing the performance of our model). Having trained models that work perfectly over the training samples but fail to perform well over the testing samples is called <strong class="calibre13">overfitting</strong>.</p>
<p class="calibre2">If you sift through the latter part of the chapter, we build a learning system with an objective to use the training samples as a knowledge base for our model in order to learn from it and generalize over the unseen data. Performance error of the <span class="calibre10">trained model</span> is of no interest to us over the training data; rather, we are interested in the performance (generalization) error of the trained model over the testing samples that haven't been involved in the training phase.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selection of a machine learning algorithm</h1>
                </header>
            
            <article>
                
<p class="calibre2">Sometimes you are unsatisfied with the execution of the model that you have utilized for a particular errand and you need an alternate class of models. Each learning strategy has its own presumptions about the information it will utilize as a learning base. As an information researcher, you have to discover which suspicions will fit your information best; by this you will have the capacity to acknowledge to attempt a class of models and reject another.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prior knowledge</h1>
                </header>
            
            <article>
                
<p class="calibre2">As discussed in the concepts of model selection and feature extraction, the two issues can be dealt with, if you have prior knowledge about:</p>
<ul class="calibre7">
<li class="calibre8">The appropriate feature </li>
<li class="calibre8">Model selection parts</li>
</ul>
<p class="calibre2">Having prior knowledge of the explanatory features in the fish recognition system enabled us to differentiate amid different types of fish. We can go promote by endeavoring to envision our information and get some feeling of the information types of the distinctive fish classifications. On the basis of this  prior knowledge, apt family of models can be chosen.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Missing values</h1>
                </header>
            
            <article>
                
<p class="calibre2">Missing features mainly occur because of a lack of data or choosing the prefer-not-to-tell option. How can we handle such a case in the learning process? For example, imagine we find the width of specific a fish type is missing for some reason. There are many ways to handle these missing features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the fish recognition/detection model</h1>
                </header>
            
            <article>
                
<p class="calibre2">To introduce the power of machine learning and deep learning in particular, we are going to implement the fish recognition example. No understanding of the inner details of the code will be required. The point of this section is to give you an overview of a typical machine learning pipeline.</p>
<p class="calibre2">Our knowledge base for this task will be a bunch of images, each one of them is labeled as opah or tuna. For this implementation, we are going to use one of the deep learning architectures that made a breakthrough in the area of imaging and computer vision in general. This architecture is called <strong class="calibre13">Convolution Neural Networks</strong> (<strong class="calibre13">CNNs</strong>). It is a family of deep learning architectures that use the convolution operation of image processing to extract features from the images that can explain the object that we want to classify. For now, you can think of it as a magic box that will take our images, learn from it how to distinguish between our two classes (opah and tuna), and then we will test the learning process of this box by feeding it with unlabeled images and see if it's able to tell which type of fish is in the image.</p>
<div class="packtinfobox">Different types of learning will be addressed in a later section, so you will understand later on why our fish recognition task is under the supervised learning category.</div>
<p class="calibre2">In this example, we will be using Keras. For the moment, you can think of Keras as an API that makes building and using deep learning way easier than usual. So let's get started! From the Keras website we have:</p>
<div class="packtquote">Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. <em class="calibre25">Being able to go from idea to result with the least possible delay is key to doing good research.<br class="title-page-name"/></em></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Knowledge base/dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we mentioned earlier, we need a historical base of data that will be used to teach the learning algorithm about the task that it's supposed to do later. But we also need another dataset for testing its ability to perform the task after the learning process. So to sum up, we need two types of datasets during the learning process:</p>
<ol class="calibre16">
<li class="calibre8">The first one is the knowledge base where we have the input data and their corresponding labels such as the fish images and their corresponding labels (opah or tuna). This data will be fed to the learning algorithm to learn from it and try to discover the patterns/trends that will help later on for classifying unlabeled images.</li>
<li class="calibre8">The second one is mainly for testing the ability of the model to apply what it learned from the knowledge base to unlabeled images or unseen data, in general, and see if it's working well.</li>
</ol>
<p class="calibre2">As you can see, we only have the data that we will use as a knowledge base for our learning method. All of the data we have at hand will have the correct output associated with it. So we need to somehow make up this data that does not have any correct output associated with it (the one that we are going to apply the model to).</p>
<p class="calibre2">While performing data science, we'll be doing the following:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Training phase</strong>: We present our data from our knowledge base and train our learning method/model by feeding the input data along with its correct output to the model.</li>
<li class="calibre8"><strong class="calibre1">Validation/test phase</strong>: In this phase, we are going to measure how well the trained model is doing. We also use different model property techniques in order to measure the performance of our trained model by using (R-square score for regression, classification errors for classifiers, recall and precision for IR models, and so on).</li>
</ul>
<p class="calibre2">The validation/test phase is usually split into two steps:</p>
<ol class="calibre16">
<li class="calibre8">In the first step, we use different learning methods/models and choose the best performing one based on our validation data (validation step)</li>
<li class="calibre8">Then we measure and report the accuracy of the selected model based on the test set (test step)</li>
</ol>
<p class="calibre2">Now let's see how we get this data to which we are going to apply the model and see how well trained it is.</p>
<p class="calibre2">Since we don't have any training samples without the correct output, we can make up one from the original training samples that we will be using. So we can split our data samples into three different sets (as shown in <em class="calibre19">Figure 1.9</em>):</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Train set</strong>: This will be used as a knowledge base for our model. Usually, will be 70% from the original data samples.</li>
<li class="calibre8"><strong class="calibre1">Validation set</strong>: This will be used to choose the best performing model among a set of models. Usually this will be 10% of the original data samples.</li>
<li class="calibre8"><strong class="calibre1">Test set</strong>: This will be used to measure and report the accuracy of the selected model. Usually, it will be as big as the validation set.</li>
</ul>
<div class="CDPAlignCenter2"><img src="assets/179a8f28-814b-41cc-bae1-e8629f3dac1a.png" class="calibre30"/></div>
<div class="CDPAlignCenter1">Figure 1.9: Splitting data into train, validation, and test sets</div>
<p class="calibre2">In case you have only one learning method that you are using, you can cancel the validation set and re-split your data to be train and test sets only. Usually, data scientists use 75/25 as percentages, or 70/30.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis pre-processing</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section we are going to analyze and preprocess the input images and have it in an acceptable format for our learning algorithm, which is the convolution neural networks here.</p>
<p class="calibre2">So let's start off by importing the required packages for this implementation:</p>
<pre class="calibre21">import numpy as np<br class="title-page-name"/>np.random.seed(2018)</pre>
<pre class="calibre21">import os<br class="title-page-name"/>import glob<br class="title-page-name"/>import cv2<br class="title-page-name"/>import datetime<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import time<br class="title-page-name"/>import warnings<br class="title-page-name"/>warnings.filterwarnings("ignore")</pre>
<pre class="calibre21">from sklearn.cross_validation import KFold<br class="title-page-name"/>from keras.models import Sequential<br class="title-page-name"/>from keras.layers.core import Dense, Dropout, Flatten<br class="title-page-name"/>from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D<br class="title-page-name"/>from keras.optimizers import SGD<br class="title-page-name"/>from keras.callbacks import EarlyStopping<br class="title-page-name"/>from keras.utils import np_utils<br class="title-page-name"/>from sklearn.metrics import log_loss<br class="title-page-name"/>from keras import __version__ as keras_version</pre>
<p class="calibre2">In order to use the images provided in the dataset, we need to get them to have the same size. OpenCV is a good choice for doing this, from the OpenCV website:</p>
<div class="packtquote">OpenCV (Open Source Computer Vision Library) is released under a BSD license and hence it’s free for both academic and commercial use. It has C++, C, Python and Java interfaces and supports Windows, Linux, Mac OS, iOS and Android. OpenCV was designed for computational efficiency and with a strong focus on real-time applications. Written in optimized C/C++, the library can take advantage of multi-core processing. Enabled with OpenCL, it can take advantage of the hardware acceleration of the underlying heterogeneous compute platform.</div>
<div class="packtinfobox">You can install OpenCV by using the python package manager by issuing, <kbd class="calibre12">pip install</kbd> <kbd class="calibre12">opencv-python</kbd></div>
<pre class="calibre21"># Parameters<br class="title-page-name"/># ----------<br class="title-page-name"/># img_path : path<br class="title-page-name"/>#    path of the image to be resized<br class="title-page-name"/>def rezize_image(img_path):<br class="title-page-name"/>   #reading image file<br class="title-page-name"/>   img = cv2.imread(img_path)<br class="title-page-name"/>   #Resize the image to to be 32 by 32<br class="title-page-name"/>   img_resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)<br class="title-page-name"/>return img_resized</pre>
<p class="calibre2">Now we need to load all the training samples of our dataset and resize each image, according to the previous function. So we are going to implement a function that will load the training samples from the different folders that we have for each fish type:</p>
<pre class="calibre21"># Loading the training samples and their corresponding labels<br class="title-page-name"/>def load_training_samples():<br class="title-page-name"/>    #Variables to hold the training input and output variables<br class="title-page-name"/>    train_input_variables = []<br class="title-page-name"/>    train_input_variables_id = []<br class="title-page-name"/>    train_label = []<br class="title-page-name"/>    # Scanning all images in each folder of a fish type<br class="title-page-name"/>    print('Start Reading Train Images')<br class="title-page-name"/>    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']<br class="title-page-name"/>    for fld in folders:<br class="title-page-name"/>       folder_index = folders.index(fld)<br class="title-page-name"/>       print('Load folder {} (Index: {})'.format(fld, folder_index))<br class="title-page-name"/>       imgs_path = os.path.join('..', 'input', 'train', fld, '*.jpg')<br class="title-page-name"/>       files = glob.glob(imgs_path)<br class="title-page-name"/>       for file in files:<br class="title-page-name"/>           file_base = os.path.basename(file)<br class="title-page-name"/>           # Resize the image<br class="title-page-name"/>           resized_img = rezize_image(file)<br class="title-page-name"/>           # Appending the processed image to the input/output variables of the classifier<br class="title-page-name"/>           train_input_variables.append(resized_img)<br class="title-page-name"/>           train_input_variables_id.append(file_base)<br class="title-page-name"/>           train_label.append(folder_index)<br class="title-page-name"/>     return train_input_variables, train_input_variables_id, train_label</pre>
<p class="calibre2">As we discussed, we have a test set that will act as the unseen data to test the generalization ability of our model. So we need to do the same with testing images; load them and do the resizing processing:</p>
<pre class="calibre21">def load_testing_samples():<br class="title-page-name"/>    # Scanning images from the test folder<br class="title-page-name"/>    imgs_path = os.path.join('..', 'input', 'test_stg1', '*.jpg')<br class="title-page-name"/>    files = sorted(glob.glob(imgs_path))<br class="title-page-name"/>    # Variables to hold the testing samples<br class="title-page-name"/>    testing_samples = []<br class="title-page-name"/>    testing_samples_id = []<br class="title-page-name"/>    #Processing the images and appending them to the array that we have<br class="title-page-name"/>    for file in files:<br class="title-page-name"/>       file_base = os.path.basename(file)<br class="title-page-name"/>       # Image resizing<br class="title-page-name"/>       resized_img = rezize_image(file)<br class="title-page-name"/>       testing_samples.append(resized_img)<br class="title-page-name"/>       testing_samples_id.append(file_base)<br class="title-page-name"/>    return testing_samples, testing_samples_id</pre>
<p class="calibre2">Now we need to invoke the previous function into another one that will use the <kbd class="calibre12">load_training_samples()</kbd> function in order to load and resize the training samples. Also, it will add a few lines of code to convert the training data into NumPy format, reshape that data to fit into our classifier, and finally convert it to float:</p>
<pre class="calibre21">def load_normalize_training_samples():<br class="title-page-name"/>    # Calling the load function in order to load and resize the training samples<br class="title-page-name"/>    training_samples, training_label, training_samples_id = load_training_samples()<br class="title-page-name"/>    # Converting the loaded and resized data into Numpy format<br class="title-page-name"/>    training_samples = np.array(training_samples, dtype=np.uint8)<br class="title-page-name"/>    training_label = np.array(training_label, dtype=np.uint8)<br class="title-page-name"/>    # Reshaping the training samples<br class="title-page-name"/>    training_samples = training_samples.transpose((0, 3, 1, 2))<br class="title-page-name"/>    # Converting the training samples and training labels into float format<br class="title-page-name"/>    training_samples = training_samples.astype('float32')<br class="title-page-name"/>    training_samples = training_samples / 255<br class="title-page-name"/>    training_label = np_utils.to_categorical(training_label, 8)<br class="title-page-name"/>    return training_samples, training_label, training_samples_id</pre>
<p class="calibre2">We also need to do the same with the test:</p>
<pre class="calibre21">def load_normalize_testing_samples():<br class="title-page-name"/>    # Calling the load function in order to load and resize the testing samples<br class="title-page-name"/>    testing_samples, testing_samples_id = load_testing_samples()<br class="title-page-name"/>    # Converting the loaded and resized data into Numpy format<br class="title-page-name"/>    testing_samples = np.array(testing_samples, dtype=np.uint8)<br class="title-page-name"/>    # Reshaping the testing samples<br class="title-page-name"/>    testing_samples = testing_samples.transpose((0, 3, 1, 2))<br class="title-page-name"/>    # Converting the testing samples into float format<br class="title-page-name"/>    testing_samples = testing_samples.astype('float32')<br class="title-page-name"/>    testing_samples = testing_samples / 255<br class="title-page-name"/>    return testing_samples, testing_samples_id</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model building</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now it's time to create the model. As we mentioned, we are going to use a deep learning architecture called CNN as a learning algorithm for this fish recognition task. Again, you are not required to understand any of the previous or the upcoming code in this chapter as we are only demonstrating how complex data science tasks can be solved by using only a few lines of code with the help of Keras and TensorFlow as a deep learning platform.</p>
<p class="calibre2">Also note that CNN and other deep learning architectures will be explained in greater detail in later chapters:</p>
<div class="CDPAlignCenter"><img src="assets/ef917a6b-5415-4d98-8da6-070ebbdbd663.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 1.10: CNN architecture</div>
<p class="calibre2">So let's go ahead and create a function that will be responsible for creating the CNN architecture that will be used in our fish recognition task:</p>
<pre class="calibre21">def create_cnn_model_arch():<br class="title-page-name"/>    pool_size = 2 # we will use 2x2 pooling throughout<br class="title-page-name"/>    conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...<br class="title-page-name"/>    conv_depth_2 = 64 # ...switching to 64 after the first pooling layer<br class="title-page-name"/>    kernel_size = 3 # we will use 3x3 kernels throughout<br class="title-page-name"/>    drop_prob = 0.5 # dropout in the FC layer with probability 0.5<br class="title-page-name"/>    hidden_size = 32 # the FC layer will have 512 neurons<br class="title-page-name"/>    num_classes = 8 # there are 8 fish types<br class="title-page-name"/>    # Conv [32] -&gt; Conv [32] -&gt; Pool<br class="title-page-name"/>    cnn_model = Sequential()<br class="title-page-name"/>    cnn_model.add(ZeroPadding2D((1, 1), input_shape=(3, 32, 32), dim_ordering='th'))<br class="title-page-name"/>    cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu', <br class="title-page-name"/>      dim_ordering='th'))<br class="title-page-name"/>    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))<br class="title-page-name"/>    cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu',<br class="title-page-name"/>      dim_ordering='th'))<br class="title-page-name"/>    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2),<br class="title-page-name"/>      dim_ordering='th'))<br class="title-page-name"/>    # Conv [64] -&gt; Conv [64] -&gt; Pool<br class="title-page-name"/>    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))<br class="title-page-name"/>    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu',<br class="title-page-name"/>      dim_ordering='th'))<br class="title-page-name"/>    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))<br class="title-page-name"/>    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu',<br class="title-page-name"/>      dim_ordering='th'))<br class="title-page-name"/>    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2),<br class="title-page-name"/>     dim_ordering='th'))<br class="title-page-name"/>    # Now flatten to 1D, apply FC then ReLU (with dropout) and finally softmax(output layer)<br class="title-page-name"/>    cnn_model.add(Flatten())<br class="title-page-name"/>    cnn_model.add(Dense(hidden_size, activation='relu'))<br class="title-page-name"/>    cnn_model.add(Dropout(drop_prob))<br class="title-page-name"/>    cnn_model.add(Dense(hidden_size, activation='relu'))<br class="title-page-name"/>    cnn_model.add(Dropout(drop_prob))<br class="title-page-name"/>    cnn_model.add(Dense(num_classes, activation='softmax'))<br class="title-page-name"/>    # initiating the stochastic gradient descent optimiser<br class="title-page-name"/>    stochastic_gradient_descent = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)    cnn_model.compile(optimizer=stochastic_gradient_descent,  # using the stochastic gradient descent optimiser<br class="title-page-name"/>                  loss='categorical_crossentropy')  # using the cross-entropy loss function<br class="title-page-name"/>    return cnn_model</pre>
<p class="calibre2">Before starting to train the model, we need to use a model assessment and validation method to help us assess our model and see its generalization ability. For this, we are going to use a method called <strong class="calibre13">k-fold cross-validation</strong>. Again, you are not required to understand this method or how it works as we are going to explain this method later in much detail.</p>
<p class="calibre2">So let's start and and create a function that will help us assess and validate the model:</p>
<pre class="calibre21">def create_model_with_kfold_cross_validation(nfolds=10):<br class="title-page-name"/>    batch_size = 16 # in each iteration, we consider 32 training examples at once<br class="title-page-name"/>    num_epochs = 30 # we iterate 200 times over the entire training set<br class="title-page-name"/>    random_state = 51 # control the randomness for reproducibility of the results on the same platform<br class="title-page-name"/>    # Loading and normalizing the training samples prior to feeding it to the created CNN model<br class="title-page-name"/>    training_samples, training_samples_target, training_samples_id = <br class="title-page-name"/>      load_normalize_training_samples()<br class="title-page-name"/>    yfull_train = dict()<br class="title-page-name"/>    # Providing Training/Testing indices to split data in the training samples<br class="title-page-name"/>    # which is splitting data into 10 consecutive folds with shuffling<br class="title-page-name"/>    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)<br class="title-page-name"/>    fold_number = 0 # Initial value for fold number<br class="title-page-name"/>    sum_score = 0 # overall score (will be incremented at each iteration)<br class="title-page-name"/>    trained_models = [] # storing the modeling of each iteration over the folds<br class="title-page-name"/>    # Getting the training/testing samples based on the generated training/testing indices by <br class="title-page-name"/>      Kfold<br class="title-page-name"/>    for train_index, test_index in kf:<br class="title-page-name"/>       cnn_model = create_cnn_model_arch()<br class="title-page-name"/>       training_samples_X = training_samples[train_index] # Getting the training input variables<br class="title-page-name"/>       training_samples_Y = training_samples_target[train_index] # Getting the training output/label variable<br class="title-page-name"/>       validation_samples_X = training_samples[test_index] # Getting the validation input variables<br class="title-page-name"/>       validation_samples_Y = training_samples_target[test_index] # Getting the validation output/label variable<br class="title-page-name"/>       fold_number += 1<br class="title-page-name"/>       print('Fold number {} from {}'.format(fold_number, nfolds))<br class="title-page-name"/>       callbacks = [<br class="title-page-name"/>           EarlyStopping(monitor='val_loss', patience=3, verbose=0),<br class="title-page-name"/>       ]<br class="title-page-name"/>       # Fitting the CNN model giving the defined settings<br class="title-page-name"/>       cnn_model.fit(training_samples_X, training_samples_Y, batch_size=batch_size,<br class="title-page-name"/>         nb_epoch=num_epochs,<br class="title-page-name"/>             shuffle=True, verbose=2, validation_data=(validation_samples_X,<br class="title-page-name"/>               validation_samples_Y),<br class="title-page-name"/>             callbacks=callbacks)<br class="title-page-name"/>       # measuring the generalization ability of the trained model based on the validation set<br class="title-page-name"/>       predictions_of_validation_samples = <br class="title-page-name"/>         cnn_model.predict(validation_samples_X.astype('float32'), <br class="title-page-name"/>         batch_size=batch_size, verbose=2)<br class="title-page-name"/>       current_model_score = log_loss(Y_valid, predictions_of_validation_samples)<br class="title-page-name"/>       print('Current model score log_loss: ', current_model_score)<br class="title-page-name"/>       sum_score += current_model_score*len(test_index)<br class="title-page-name"/>       # Store valid predictions<br class="title-page-name"/>       for i in range(len(test_index)):<br class="title-page-name"/>           yfull_train[test_index[i]] = predictions_of_validation_samples[i]<br class="title-page-name"/>       # Store the trained model<br class="title-page-name"/>       trained_models.append(cnn_model)<br class="title-page-name"/>    # incrementing the sum_score value by the current model calculated score<br class="title-page-name"/>    overall_score = sum_score/len(training_samples)<br class="title-page-name"/>    print("Log_loss train independent avg: ", overall_score)<br class="title-page-name"/>    #Reporting the model loss at this stage<br class="title-page-name"/>    overall_settings_output_string = 'loss_' + str(overall_score) + '_folds_' + str(nfolds) + <br class="title-page-name"/>      '_ep_' + str(num_epochs)<br class="title-page-name"/>    return overall_settings_output_string, trained_models</pre>
<p class="calibre2">Now, after building the model and using k-fold cross-validation method in order to assess and validate the model, we need to report the results of the trained model over the test set. In order to do this, we are also going to use k-fold cross-validation but this time over the test to see how good our trained model is.</p>
<p class="calibre2">So let's define the function that will take the trained CNN models as an input and then test them using the test set that we have:</p>
<pre class="calibre21">def test_generality_crossValidation_over_test_set( overall_settings_output_string, cnn_models):<br class="title-page-name"/>    batch_size = 16 # in each iteration, we consider 32 training examples at once<br class="title-page-name"/>    fold_number = 0 # fold iterator<br class="title-page-name"/>    number_of_folds = len(cnn_models) # Creating number of folds based on the value used in the training step<br class="title-page-name"/>    yfull_test = [] # variable to hold overall predictions for the test set<br class="title-page-name"/>    #executing the actual cross validation test process over the test set<br class="title-page-name"/>    for j in range(number_of_folds):<br class="title-page-name"/>       model = cnn_models[j]<br class="title-page-name"/>       fold_number += 1<br class="title-page-name"/>       print('Fold number {} out of {}'.format(fold_number, number_of_folds))<br class="title-page-name"/>       #Loading and normalizing testing samples<br class="title-page-name"/>       testing_samples, testing_samples_id = load_normalize_testing_samples()<br class="title-page-name"/>       #Calling the current model over the current test fold<br class="title-page-name"/>       test_prediction = model.predict(testing_samples, batch_size=batch_size, verbose=2)<br class="title-page-name"/>       yfull_test.append(test_prediction)<br class="title-page-name"/>    test_result = merge_several_folds_mean(yfull_test, number_of_folds)<br class="title-page-name"/>    overall_settings_output_string = 'loss_' + overall_settings_output_string \ + '_folds_' +<br class="title-page-name"/>      str(number_of_folds)<br class="title-page-name"/>    format_results_for_types(test_result, testing_samples_id, overall_settings_output_string)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training and testing</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now we are ready to start the model training phase by calling the main function <kbd class="calibre12">create_model_with_kfold_cross_validation()</kbd> for building and training the CNN model using 10-fold cross-validation; then we can call the testing function to measure the model's ability to generalize to the test set:</p>
<pre class="calibre21">if __name__ == '__main__':<br class="title-page-name"/>  info_string, models = create_model_with_kfold_cross_validation()<br class="title-page-name"/>  test_generality_crossValidation_over_test_set(info_string, models)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fish recognition – all together</h1>
                </header>
            
            <article>
                
<p class="calibre2">After explaining the main building blocks for our fish recognition example, we are ready to see all the code pieces connected together and see how we managed to build such a complex system with just a few lines of code. The full code is placed in the <em class="calibre19">Appendix</em> section of the book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Different learning types</h1>
                </header>
            
            <article>
                
<p class="calibre2">According to Arthur Samuel (<a href="https://en.wikipedia.org/wiki/Arthur_Samuel" target="_blank" class="calibre11">https://en.wikipedia.org/wiki/Arthur_Samuel</a>), <em class="calibre19">data science gives computers the ability to learn without being explicitly programmed</em>. So, any piece of software that will consume training examples in order to make decisions over unseen data without explicit programming is considered learning. Data science or learning comes in three different forms.</p>
<p class="calibre2">Figure 1.12 shows the commonly used types of data science/machine learning:</p>
<div class="CDPAlignCenter"><img src="assets/a11e2872-d4cf-49b7-bb27-ba07b7ab1fa8.png" class="calibre32"/></div>
<div class="CDPAlignCenter1">Figure 1.12: Different types of data science/machine learning.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">The majority of data scientists use supervised learning. Supervised learning is where you have some explanatory features, which are called input variables (<em class="calibre19">X</em>), and you have the labels that are associated with the training samples, which are called output variables (<em class="calibre19">Y</em>). The objective of any supervised learning algorithm is to learn the mapping function from the input variables (<em class="calibre19">X</em>) to the output variables (<em class="calibre19">Y</em>):</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation" src="assets/103d2d2d-9b0c-4c73-bef8-bdc975e1be87.png"/></div>
<p class="calibre2">So the supervised learning algorithm will try to learn approximately the mapping from the input variables (<em class="calibre19">X</em>) to the output variables (<em class="calibre19">Y</em>), such that it can be used later to predict the <em class="calibre19">Y</em> values of an unseen sample.</p>
<p class="calibre2"><em class="calibre19">Figure 1.13</em> shows a typical workflow for any supervised data science system:</p>
<div class="CDPAlignCenter"><img src="assets/44109720-d5a2-4f63-a905-809aa5261c5a.png" class="calibre33"/></div>
<div class="CDPAlignCenter1">Figure 1.13: A typical supervised learning workflow/pipeline. The top part shows the training process that starts with feeding the raw data into a feature extraction module where we will select meaningful explanatory feature to represent our data. After that, the extracted/selected explanatory feature gets combined with the training set and we feed it to the learning algorithm in order to learn from it. Then we do some model evaluation to tune the parameters and get the learning algorithm to get the best out of the data samples.</div>
<p class="calibre2">This kind of learning is called <strong class="calibre13">supervised learning</strong> because you are getting the label/output of each training sample associated with it. In this case, we can say that the learning process is supervised by a supervisor. The algorithm makes decisions on the training samples and is corrected by the supervisor, based on the correct labels of the data. The learning process will stop when the supervised learning algorithm achieves an acceptable level of accuracy.</p>
<p class="calibre2">Supervised learning tasks come in two different forms; regression and classification:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Classification</strong>: A classification task is when the label or the output variable is a category, such as <em class="calibre25">tuna</em> or <em class="calibre25">Opah</em> or <em class="calibre25">spam</em> and <em class="calibre25">non spam</em></li>
<li class="calibre8"><strong class="calibre1">Regression</strong>: A regression task is when the output variable is a real value, such as <em class="calibre25">house prices</em> or <em class="calibre25">height</em></li>
</ul>
<p class="calibre2"> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">Unsupervised learning is viewed as the second most common kind of learning that is utilized by information researchers. In this type of learning, only the explanatory features or the input variables (<em class="calibre19">X</em>) are given, without any corresponding label or output variable.</p>
<p class="calibre2">The target of unsupervised learning algorithms is to take in the hidden structures and examples in the information. This kind of learning is called <strong class="calibre13">unsupervised</strong> in light of the fact that there aren't marks related with the training samples. So it's a learning procedure without corrections, and the algorithm will attempt to find the basic structure on its own.</p>
<p class="calibre2">Unsupervised learning can be further broken into two forms—clustering and association tasks:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Clustering</strong>: A clustering task is where you want to discover similar groups of training samples and group them together, such as grouping documents by topic</li>
<li class="calibre8"><strong class="calibre1">Association</strong>: An association rule learning task is where you want to discover some rules that describe the relationships in your training samples, such as people who watch movie <em class="calibre25">X</em> also tend to watch movie <em class="calibre25">Y</em></li>
</ul>
<p class="calibre2"><em class="calibre19">Figure 1.14</em> shows a trivial example of unsupervised learning where we have got scattered documents and we are trying to group <em class="calibre19">similar</em> ones together:</p>
<div class="CDPAlignCenter"><img src="assets/4871de97-c0c1-4780-b921-56c7a8663f2e.png" class="calibre34"/></div>
<div class="CDPAlignCenter1">Figure 1.14: Shows how unsupervised use similarity measure such as Euclidean distance to group similar documents to together and draw a decision boundaries for them</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">Semi-supervised learning is a type of learning that sits in between supervised and unsupervised learning, where you have got training examples with input variables (<em class="calibre19">X</em>), but only some of them are labeled/tagged with the output variable (<em class="calibre19">Y</em>).</p>
<p class="calibre2">A good example of this type of learning is Flickr (<a href="https://www.flickr.com/" class="calibre11">https://www.flickr.com/</a>), where you have got lots of images uploaded by users but only some of them are labeled (such as sunset, ocean, and dog) and the rest are unlabeled.</p>
<p class="calibre2">To solve the tasks that fall into this type of learning, you can use one of the following or a combination of them:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Supervised learning</strong>: Learn/train the learning algorithm to give predictions about the unlabeled data and then feed the entire training samples back to learn from it and predict the unseen data</li>
<li class="calibre8"><strong class="calibre1">Unsupervised learning</strong>: Use the unsupervised learning algorithms to learn the underlying structure of the explanatory features or the input variables as if you don't have any tagged training samples</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">The last type of learning in machine learning is reinforcement learning, in which there's no supervisor but only a reward signal.</p>
<p class="calibre2">So the reinforcement learning algorithm will try to make a decision and then a reward signal will be there to tell whether this decision is right or wrong. Also, this supervision feedback or reward signal may not come instantaneously but get delayed for a few steps. For example, the algorithm will take a decision now, but only after many steps will the reward signal tell whether decision was good or bad.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data size and industry needs</h1>
                </header>
            
            <article>
                
<p class="calibre2">Data is the information base of our learning calculations; any uplifting and imaginative thoughts will be nothing with the absence of information. So in the event that you have a decent information science application with the right information, at that point you are ready to go.</p>
<p class="calibre2">Having the capacity to investigate and extricate an incentive from your information is obvious these days notwithstanding to the structure of your information, however since enormous information is turning into the watchword of the day then we require information science apparatuses and advancements that can scale with this immense measure of information in an unmistakable learning time. These days everything is producing information and having the capacity to adapt to it is a test. Huge organizations, for example, Google, Facebook, Microsoft, IBM, and so on, manufacture their own adaptable information science arrangements keeping in mind the end goal to deal with the tremendous amount of information being produced once a day by their clients.</p>
<p class="calibre2">TensorFlow, is a machine intelligence/data science platform that was released as an open source library on November 9, 2016 by Google. It is a scalable analytics platform that enables data scientists to build complex systems with a vast amount of data in visible time and it also enables them to use greedy learning methods that require lots of data to get a good performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we went through building a learning system for fish recognition; we also saw how we can build complex applications, such as fish recognition, using a few lines of code with the help of TensorFlow and Keras. This coding example was not meant to be understood from your side, rather to demonstrate the visibility of building complex systems and how data science in general and specifically deep learning became an easy-to-use tool.</p>
<p class="calibre2">We saw the challenges that you might encounter in your daily life as a data scientist while building a learning system.</p>
<p class="calibre2">We also looked at the typical design cycle for building a learning system and explained the overall idea of each component involved in this cycle.</p>
<p class="calibre2">Finally, we went through different learning types, having big data generated daily by big and small companies, and how this vast amount of data raises a red alert to build scalable tools to be able to analyze and extract value from this data.</p>
<p class="calibre2">At this point, the reader may be overwhelmed by all the information mentioned so far, but most of what we explained in this chapter will be addressed in other chapters, including data science challenges and the fish recognition example. The whole purpose of this chapter was to get an overall idea about data science and its development cycle, without any deep understanding of the challenges and the coding example. The coding example was mentioned in this chapter to break the fear of most newcomers in the field of data science and show them how complex systems such as fish recognition can be done in a few lines of code.</p>
<p class="calibre2">Next up, we will start our <em class="calibre19">by example</em> journey, by addressing the basic concepts of data science through an example. The next part will mainly focus on preparing you for the later advanced chapters, by going through the famous Titanic example. Lots of concepts will be addressed, including different learning methods for regression and classification, different types of performance errors and which one to care about most, and more about tackling some of the data science challenges and handling different forms of the data samples.</p>


            </article>

            
        </section>
    </body></html>