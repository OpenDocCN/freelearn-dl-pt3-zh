["```\nimport tensorflow as tf \nimport tensorflow_datasets as tfds \n\n# Train, test, and validation are datasets for object detection: multiple objects per image. \n(train, test, validation), info = tfds.load( \n \"voc2007\", split=[\"train\", \"test\", \"validation\"], with_info=True \n)\n\n```", "```\ntfds.core.DatasetInfo( \n    name='voc2007', \n    version=1.0.0, \n    description='This dataset contains the data from the PASCAL Visual Object Classes Challenge \n2007, a.k.a. VOC2007, corresponding to the Classification and Detection \ncompetitions. \nA total of 9,963 images are included in this dataset, where each image contains \na set of objects, out of 20 different classes, making a total of 24,640 \nannotated objects. \nIn the Classification competition, the goal is to predict the set of labels \ncontained in the image, while in the Detection competition the goal is to \npredict the bounding box and label of each individual object. \n', \n    urls=['http://host.robots.ox.ac.uk/pascal/VOC/voc2007/'], \n    features=FeaturesDict({ \n        'image': Image(shape=(None, None, 3), dtype=tf.uint8), \n        'image/filename': Text(shape=(), dtype=tf.string, encoder=None), \n        'labels': Sequence(shape=(None,), dtype=tf.int64, feature=ClassLabel(shape=(), dtype=tf.int64, num_classes=20)), \n        'labels_no_difficult': Sequence(shape=(None,), dtype=tf.int64, feature=ClassLabel(shape=(), dtype=tf.int64, num_classes=20)), \n        'objects': SequenceDict({'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20), 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), 'pose': ClassLabel(shape=(), dtype=tf.int64, num_classes=5), 'is_truncated': Tensor(shape=(), dtype=tf.bool), 'is_difficult'\n: Tensor(shape=(), dtype=tf.bool)}) \n    }, \n    total_num_examples=9963, \n    splits={ \n        'test': <tfds.core.SplitInfo num_examples=4952>, \n        'train': <tfds.core.SplitInfo num_examples=2501>, \n        'validation': <tfds.core.SplitInfo num_examples=2510> \n    }, \n    supervised_keys=None, \n    citation='\"\"\" \n        @misc{pascal-voc-2007, \n          author = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\", \n          title = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\", \n          howpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"} \n\n    \"\"\"',\n    redistribution_info=, \n)\n```", "```\nimport matplotlib.pyplot as plt\n```", "```\nwith tf.device(\"/CPU:0\"): \n    for row in train.take(5): \n        obj = row[\"objects\"] \n        image = tf.image.convert_image_dtype(row[\"image\"], tf.float32) \n\n        for idx in tf.range(tf.shape(obj[\"label\"])[0]): \n            image = tf.squeeze( \n                tf.image.draw_bounding_boxes( \n                    images=tf.expand_dims(image, axis=[0]), \n                    boxes=tf.reshape(obj[\"bbox\"][idx], (1, 1, 4)), \n                    colors=tf.reshape(tf.constant((1.0, 1.0, 0, 0)), (1, 4)), \n                ), \n                axis=[0], \n            ) \n\n            print( \n                \"label: \", info.features[\"objects\"][\"label\"].int2str(obj[\"label\"][idx]) \n            ) \n\n```", "```\n            plt.imshow(image)\n            plt.show()\n```", "```\ndef filter(dataset): \n    return dataset.filter(lambda row: tf.equal(tf.shape(row[\"objects\"][\"label\"])[0], 1)) \n\ntrain, test, validation = filter(train), filter(test), filter(validation)\n\n```", "```\nimport tensorflow_hub as hub\n\ninputs = tf.keras.layers.Input(shape=(299,299,3))\nnet = hub.KerasLayer(\n        \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\n        output_shape=[2048],\n        trainable=False,\n      )(inputs)\nnet = tf.keras.layers.Dense(512)(net)\nnet = tf.keras.layers.ReLU()(net)\ncoordinates = tf.keras.layers.Dense(4, use_bias=False)(net)\n\nregressor = tf.keras.Model(inputs=inputs, outputs=coordinates)\n```", "```\ndef prepare(dataset):\n    def _fn(row):\n        row[\"image\"] = tf.image.convert_image_dtype(row[\"image\"], tf.float32)\n        row[\"image\"] = tf.image.resize(row[\"image\"], (299, 299))\n        return row\n\n    return dataset.map(_fn)\n\ntrain, test, validation = prepare(train), prepare(test), prepare(validation)\n```", "```\n# First option -> this requires to call the loss l2, taking care of squeezing the input\n# l2 = tf.losses.MeanSquaredError()\n\n# Second option, it is the loss function iself that squeezes the input\ndef l2(y_true, y_pred):\n    return tf.reduce_mean(\n        tf.square(y_pred - tf.squeeze(y_true, axis=[1]))\n    )\n```", "```\ndef draw(dataset, regressor, step):\n    with tf.device(\"/CPU:0\"):\n        row = next(iter(dataset.take(3).batch(3)))\n        images = row[\"image\"]\n        obj = row[\"objects\"]\n        boxes = regressor(images)\n        tf.print(boxes)\n\n        images = tf.image.draw_bounding_boxes(\n            images=images, boxes=tf.reshape(boxes, (-1, 1, 4))\n        )\n        images = tf.image.draw_bounding_boxes(\n            images=images, boxes=tf.reshape(obj[\"bbox\"], (-1, 1, 4))\n        )\n        tf.summary.image(\"images\", images, step=step)\n```", "```\noptimizer = tf.optimizers.Adam() \nepochs = 500 \nbatch_size = 32 \n\nglobal_step = tf.Variable(0, trainable=False, dtype=tf.int64) \n\ntrain_writer, validation_writer = ( \n    tf.summary.create_file_writer(\"log/train\"), \n    tf.summary.create_file_writer(\"log/validation\"), \n) \nwith validation_writer.as_default(): \n    draw(validation, regressor, global_step) \n\n```", "```\n@tf.function \ndef train_step(image, coordinates): \n    with tf.GradientTape() as tape: \n        loss = l2(coordinates, regressor(image)) \n    gradients = tape.gradient(loss, regressor.trainable_variables) \n    optimizer.apply_gradients(zip(gradients, regressor.trainable_variables)) \n    return loss \n\n```", "```\ntrain_batches = train.cache().batch(batch_size).prefetch(1) \nwith train_writer.as_default(): \n    for _ in tf.range(epochs): \n        for batch in train_batches: \n            obj = batch[\"objects\"] \n            coordinates = obj[\"bbox\"] \n            loss = train_step(batch[\"image\"], coordinates) \n            tf.summary.scalar(\"loss\", loss, step=global_step) \n            global_step.assign_add(1) \n            if tf.equal(tf.mod(global_step, 10), 0): \n                tf.print(\"step \", global_step, \" loss: \", loss) \n                with validation_writer.as_default(): \n                    draw(validation, regressor, global_step) \n                with train_writer.as_default(): \n                    draw(train, regressor, global_step)\n```", "```\ndef iou(pred_box, gt_box, h, w):\n    \"\"\"\n    Compute IoU between detect box and gt boxes\n    Args:\n        pred_box: shape (4,): y_min, x_min, y_max, x_max - predicted box\n        gt_boxes: shape (4,): y_min, x_min, y_max, x_max - ground truth\n        h: image height\n        w: image width\n    \"\"\"\n```", "```\n    def _swap(box):\n        return tf.stack([box[1] * w, box[0] * h, box[3] * w, box[2] * h])\n\n    pred_box = _swap(pred_box)\n    gt_box = _swap(gt_box)\n\n    box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n    area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n    xx1 = tf.maximum(pred_box[0], gt_box[0])\n    yy1 = tf.maximum(pred_box[1], gt_box[1])\n    xx2 = tf.minimum(pred_box[2], gt_box[2])\n    yy2 = tf.minimum(pred_box[3], gt_box[3])\n```", "```\n    w = tf.maximum(0, xx2 - xx1)\n    h = tf.maximum(0, yy2 - yy1)\n\n    inter = w * h\n    return inter / (box_area + area - inter)\n```", "```\nm = tf.metrics.Precision()\n\nm.update_state([0, 1, 1, 1], [1, 0, 1, 1])\nprint('Final result: ', m.result().numpy()) # Final result: 0.66\n```", "```\n# IoU threshold\nthreshold = 0.75\n# Metric object\nprecision_metric = tf.metrics.Precision()\n\ndef draw(dataset, regressor, step):\n    with tf.device(\"/CPU:0\"):\n        row = next(iter(dataset.take(3).batch(3)))\n        images = row[\"image\"]\n        obj = row[\"objects\"]\n        boxes = regressor(images)\n\n        images = tf.image.draw_bounding_boxes(\n            images=images, boxes=tf.reshape(boxes, (-1, 1, 4))\n        )\n        images = tf.image.draw_bounding_boxes(\n            images=images, boxes=tf.reshape(obj[\"bbox\"], (-1, 1, 4))\n        )\n        tf.summary.image(\"images\", images, step=step)\n\n        true_labels, predicted_labels = [], []\n        for idx, predicted_box in enumerate(boxes):\n            iou_value = iou(predicted_box, tf.squeeze(obj[\"bbox\"][idx]), 299, 299)\n            true_labels.append(1)\n            predicted_labels.append(1 if iou_value >= threshold else 0)\n\n        precision_metric.update_state(true_labels, predicted_labels)\n        tf.summary.scalar(\"precision\", precision_metric.result(), step=step)\n```", "```\ninputs = tf.keras.layers.Input(shape=(299, 299, 3))\n```", "```\nnet = hub.KerasLayer(\n    \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\n    output_shape=[2048],\n    trainable=False,\n)(inputs)\n```", "```\nregression_head = tf.keras.layers.Dense(512)(net)\nregression_head = tf.keras.layers.ReLU()(regression_head)\ncoordinates = tf.keras.layers.Dense(4, use_bias=False)(regression_head)\n```", "```\nclassification_head = tf.keras.layers.Dense(1024)(net)\nclassification_head = tf.keras.layers.ReLU()(classificatio_head)\nclassification_head = tf.keras.layers.Dense(128)(net)\nclassification_head = tf.keras.layers.ReLU()(classificatio_head)\nnum_classes = 20\nclassification_head = tf.keras.layers.Dense(num_classes, use_bias=False)(\n    classification_head\n)\n```", "```\nmodel = tf.keras.Model(inputs=inputs, outputs=[coordinates, classification_head])\n```"]