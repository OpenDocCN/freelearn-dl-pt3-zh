["```\nimport functools\nfrom collections import deque\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow.keras.layers import Concatenate, Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\ntf.keras.backend.set_floatx(“float64”)\n```", "```\n    def actor(state_shape, action_shape, units=(512, 256, 64)):\n        state_shape_flattened = \\\n            functools.reduce(lambda x, y: x * y, state_shape)\n        state = Input(shape=state_shape_flattened)\n        x = Dense(units[0], name=”L0”, activation=”relu”)\\\n                 (state)\n        for index in range(1, len(units)):\n            x = Dense(units[index], name=”L{}”.format(index),\n                      activation=”relu”)(x)\n        actions_mean = Dense(action_shape[0], \\\n                             name=”Out_mean”)(x)\n        actions_std = Dense(action_shape[0], \n                             name=”Out_std”)(x)\n        model = Model(inputs=state, outputs=[actions_mean,\n                      actions_std])\n        return model\n    ```", "```\n    def critic(state_shape, action_shape, units=(512, 256, 64)):\n        state_shape_flattened = \\\n            functools.reduce(lambda x, y: x * y, state_shape)\n        inputs = [Input(shape=state_shape_flattened), \\\n                  Input(shape=action_shape)]\n        concat = Concatenate(axis=-1)(inputs)\n        x = Dense(units[0], name=”Hidden0”, \\\n                  activation=”relu”)(concat)\n        for index in range(1, len(units)):\n            x = Dense(units[index], \\\n                      name=”Hidden{}”.format(index), \\\n                      activation=”relu”)(x)\n        output = Dense(1, name=”Out_QVal”)(x)\n        model = Model(inputs=inputs, outputs=output)\n        return model\n    ```", "```\n    def update_target_weights(model, target_model, tau=0.005):\n        weights = model.get_weights()\n        target_weights = target_model.get_weights()\n        for i in range(len(target_weights)):  \n        # set tau% of target model to be new weights\n            target_weights[i] = weights[i] * tau + \\\n                                target_weights[i] * (1 - tau)\n        target_model.set_weights(target_weights)\n    ```", "```\n    class SAC(object):\n        def __init__(\n            self,\n            observation_shape,\n            action_space,\n            lr_actor=3e-5,\n            lr_critic=3e-4,\n            actor_units=(64, 64),\n            critic_units=(64, 64),\n            auto_alpha=True,\n            alpha=0.2,\n            tau=0.005,\n            gamma=0.99,\n            batch_size=128,\n            memory_cap=100000,\n        ):\n    ```", "```\n            self.state_shape = observation_shape  # shape of \n            # observations\n            self.action_shape = action_space.shape  # number \n            # of actions\n            self.action_bound = \\\n                (action_space.high - action_space.low) / 2\n            self.action_shift = \\\n                (action_space.high + action_space.low) / 2\n            self.memory = deque(maxlen=int(memory_cap))\n    ```", "```\n            # Define and initialize actor network\n            self.actor = actor(self.state_shape, \n                               self.action_shape, \n                               actor_units)\n            self.actor_optimizer = \\\n                Adam(learning_rate=lr_actor)\n            self.log_std_min = -20\n            self.log_std_max = 2\n            print(self.actor.summary())\n    ```", "```\n            # Define and initialize critic networks\n            self.critic_1 = critic(self.state_shape, \n                                   self.action_shape, \n                                   critic_units)\n            self.critic_target_1 = critic(self.state_shape,\n                                          self.action_shape,\n                                          critic_units)\n            self.critic_optimizer_1 = \\\n                Adam(learning_rate=lr_critic)\n            update_target_weights(self.critic_1, \n                                  self.critic_target_1, \n                                  tau=1.0)\n            self.critic_2 = critic(self.state_shape, \n                                   self.action_shape, \n                                   critic_units)\n            self.critic_target_2 = critic(self.state_shape,\n                                          self.action_shape,\n                                          critic_units)\n            self.critic_optimizer_2 = \\\n                Adam(learning_rate=lr_critic)\n            update_target_weights(self.critic_2, \n                                  self.critic_target_2, \n                                  tau=1.0)\n            print(self.critic_1.summary())\n    ```", "```\n            # Define and initialize temperature alpha and \n            # target entropy\n            self.auto_alpha = auto_alpha\n            if auto_alpha:\n                self.target_entropy = \\\n                    -np.prod(self.action_shape)\n                self.log_alpha = tf.Variable(0.0, \n                                            dtype=tf.float64)\n                self.alpha = tf.Variable(0.0, \n                                         dtype=tf.float64)\n                self.alpha.assign(tf.exp(self.log_alpha))\n                self.alpha_optimizer = \\\n                    Adam(learning_rate=lr_actor)\n            else:\n                self.alpha = tf.Variable(alpha, \n                                         dtype=tf.float64)\n    ```", "```\n            # Set hyperparameters\n            self.gamma = gamma  # discount factor\n            self.tau = tau  # target model update\n            self.batch_size = batch_size\n            # Tensorboard\n            self.summaries = {}\n    ```", "```\n        def process_actions(self, mean, log_std, test=False, \n        eps=1e-6):\n            std = tf.math.exp(log_std)\n            raw_actions = mean\n            if not test:\n                raw_actions += tf.random.normal(shape=mean.\\\n                               shape, dtype=tf.float64) * std\n            log_prob_u = tfp.distributions.Normal(loc=mean,\n                             scale=std).log_prob(raw_actions)\n            actions = tf.math.tanh(raw_actions)\n            log_prob = tf.reduce_sum(log_prob_u - \\\n                         tf.math.log(1 - actions ** 2 + eps))\n            actions = actions * self.action_bound + \\\n                      self.action_shift\n            return actions, log_prob\n    ```", "```\n        def act(self, state, test=False, use_random=False):\n            state = state.reshape(-1)  # Flatten state\n            state = np.expand_dims(state, axis=0).\\\n                                         astype(np.float64)\n            if use_random:\n                a = tf.random.uniform(\n                    shape=(1, self.action_shape[0]), \n                           minval=-1, maxval=1, \n                           dtype=tf.float64\n                )\n            else:\n                means, log_stds = self.actor.predict(state)\n                log_stds = tf.clip_by_value(log_stds, \n                                            self.log_std_min,\n                                            self.log_std_max)\n                a, log_prob = self.process_actions(means,\n                                                   log_stds, \n                                                   test=test)\n            q1 = self.critic_1.predict([state, a])[0][0]\n            q2 = self.critic_2.predict([state, a])[0][0]\n            self.summaries[“q_min”] = tf.math.minimum(q1, q2)\n            self.summaries[“q_mean”] = np.mean([q1, q2])\n            return a\n    ```", "```\n        def load_actor(self, a_fn):\n            self.actor.load_weights(a_fn)\n            print(self.actor.summary())\n        def load_critic(self, c_fn):\n            self.critic_1.load_weights(c_fn)\n            self.critic_target_1.load_weights(c_fn)\n            self.critic_2.load_weights(c_fn)\n            self.critic_target_2.load_weights(c_fn)\n            print(self.critic_1.summary())\n    ```", "```\n    import sys\n    import os\n    from gym.envs.registration import register\n    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n    _AVAILABLE_ENVS = {\n        “CryptoTradingEnv-v0”: {\n            “entry_point”: \\\n              “tradegym.crypto_trading_env:CryptoTradingEnv”,\n            “description”: “Crypto Trading RL environment”,\n        },\n        “StockTradingContinuousEnv-v0”: {\n            “entry_point”: “tradegym.stock_trading_\\\n                 continuous_env:StockTradingContinuousEnv”,\n            “description”: “Stock Trading RL environment with continous action space”,\n        },\n    }\n    for env_id, val in _AVAILABLE_ENVS.items():\n        register(id=env_id, entry_point=val.get(\n                                         “entry_point”))\n    ```", "```\n    import argparse\n    import json\n    import logging\n    import os\n    import sys\n    import uuid\n    import numpy as np\n    import six\n    from flask import Flask, jsonify, request\n    import gym\n    ```", "```\n    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n    import tradegym  # Register tradegym envs with OpenAI Gym \n    # registry\n    ```", "```\n    class Envs(object):\n        def __init__(self):\n            self.envs = {}\n            self.id_len = 8  # Number of chars in instance_id\n    ```", "```\n        def _lookup_env(self, instance_id):\n            \"\"\"Lookup environment based on instance_id and \n               throw error if not found\"\"\"\n        def _remove_env(self, instance_id):\n            \"\"\"Delete environment associated with \n               instance_id\"\"\"\n    ```", "```\n        def create(self, env_id, seed=None):\n            \"\"\"Create (make) an instance of the environment \n               with `env_id` and return the instance_id\"\"\"\n        def list_all(self):\n            \"\"\"Return a dictionary of all the active \n               environments with instance_id as keys\"\"\"\n        def reset(self, instance_id):\n            \"\"\"Reset the environment pointed to by the \n               instance_id\"\"\"\n\n        def env_close(self, instance_id):\n            \"\"\"Call .close() on the environment and remove \n               instance_id from the list of all envs\"\"\"\n    ```", "```\n        def step(self, instance_id, action, render):\n            \"\"\"Perform a single step in the environment \n               pointed to by the instance_id and return \n               observation, reward, done and info\"\"\"\n        def get_action_space_contains(self, instance_id, x):\n            \"\"\"Check if the given environment’s action space \n               contains x\"\"\"\n        def get_action_space_info(self, instance_id):\n            \"\"\"Return the observation space infor for the \n               given environment instance_id\"\"\"\n        def get_action_space_sample(self, instance_id):\n            \"\"\"Return a sample action for the environment \n               referred by the instance_id\"\"\"\n        def get_observation_space_contains(self, instance_id, \n        j):\n            \"\"\"Return true is the environment’s observation \n               space contains `j`. False otherwise\"\"\"\n        def get_observation_space_info(self, instance_id):\n            \"\"\"Return the observation space for the \n               environment referred by the instance_id\"\"\"\n        def _get_space_properties(self, space):\n            \"\"\"Return a dictionary containing the attributes \n               and values of the given Gym Spce (Discrete, \n               Box etc.)\"\"\"\n    ```", "```\n    app = Flask(__name__)\n    envs = Envs()\n    ```", "```\n    @app.route(“/v1/envs/”, methods=[“POST”])\n    def env_create():\n        env_id = get_required_param(request.get_json(), \n                                    “env_id”)\n        seed = get_optional_param(request.get_json(), \n                                   “seed”, None)\n        instance_id = envs.create(env_id, seed)\n        return jsonify(instance_id=instance_id)\n    ```", "```\n    @app.route(“/v1/envs/<instance_id>/reset/”, \n               methods=[“POST”])\n    def env_reset(instance_id):\n        observation = envs.reset(instance_id)\n        if np.isscalar(observation):\n            observation = observation.item()\n        return jsonify(observation=observation)\n    ```", "```\n    @app.route(“/v1/envs/<instance_id>/step/”,\n               methods=[“POST”])\n    def env_step(instance_id):\n        json = request.get_json()\n        action = get_required_param(json, “action”)\n        render = get_optional_param(json, “render”, False)\n        [obs_jsonable, reward, done, info] = envs.step(instance_id, action, render)\n        return jsonify(observation=obs_jsonable, \n                       reward=reward, done=done, info=info)\n    ```", "```\n    if __name__ == “__main__”:\n        parser = argparse.ArgumentParser(description=”Start a\n                                        Gym HTTP API server”)\n        parser.add_argument(“-l”,“--listen”, help=”interface\\\n                            to listen to”, default=”0.0.0.0”)\n        parser.add_argument(“-p”, “--port”, default=6666, \\\n                            type=int, help=”port to bind to”)\n        args = parser.parse_args()\n        print(“Server starting at: “ + \\\n               “http://{}:{}”.format(args.listen, args.port))\n        app.run(host=args.listen, port=args.port, debug=True)\n    ```", "```\n    import json\n    import logging\n    import os\n    import requests\n    import six.moves.urllib.parse as urlparse\n    ```", "```\n    class Client(object):\n        def __init__(self, remote_base):\n            self.remote_base = remote_base\n            self.session = requests.Session()\n            self.session.headers.update({“Content-type”: \\\n                                         “application/json”})\n    ```", "```\n        def env_create(self, env_id):\n            route = “/v1/envs/”\n            data = {“env_id”: env_id}\n            resp = self._post_request(route, data)\n            instance_id = resp[“instance_id”]\n            return instance_id\n    ```", "```\n        def env_reset(self, instance_id):\n            route = “/v1/envs/{}/reset/”.format(instance_id)\n            resp = self._post_request(route, None)\n            observation = resp[“observation”]\n            return observation\n    ```", "```\n        def env_step(self, instance_id, action, \n        render=False):\n            route = “/v1/envs/{}/step/”.format(instance_id)\n            data = {“action”: action, “render”: render}\n            resp = self._post_request(route, data)\n            observation = resp[“observation”]\n            reward = resp[“reward”]\n            done = resp[“done”]\n            info = resp[“info”]\n            return [observation, reward, done, info]\n    ```", "```\n    if __name__ == “__main__”:\n        remote_base = “http://127.0.0.1:6666”\n        client = Client(remote_base)\n        # Create environment\n        env_id = “StockTradingContinuousEnv-v0”\n        # env_id = “CartPole-v0”\n        instance_id = client.env_create(env_id)\n        # Check properties\n        all_envs = client.env_list_all()\n        logger.info(f”all_envs:{all_envs}”)\n        action_info = \\\n            client.env_action_space_info(instance_id)\n        logger.info(f”action_info:{action_info}”)\n        obs_info = \\\n            client.env_observation_space_info(instance_id)\n        # logger.info(f”obs_info:{obs_info}”)\n        # Run a single step\n        init_obs = client.env_reset(instance_id)\n        [observation, reward, done, info] = \\\n            client.env_step(instance_id, 1, True)\n        logger.info(f”reward:{reward} done:{done} \\\n                      info:{info}”)\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python tradegym_http_server.py\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python tradegym_http_client.py\n    ```", "```\n    all_envs:{‘114c5e8f’: ‘StockTradingContinuousEnv-v0’, ‘6287385e’: ‘StockTradingContinuousEnv-v0’, ‘d55c97c0’: ‘StockTradingContinuousEnv-v0’, ‘fd355ed8’: ‘StockTradingContinuousEnv-v0’}\n    action_info:{‘high’: [1.0], ‘low’: [-1.0], ‘name’: ‘Box’, ‘shape’: [1]}\n    reward:0.0 done:False info:{}\n    ```", "```\nimport datetime\nimport os\nimport sys\nimport logging\nimport gym.spaces\nimport numpy as np\nimport tensorflow as tf\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom tradegym_http_client import Client\nfrom sac_agent_base import SAC\n```", "```\n    # Create an App-level child logger\n    logger = logging.getLogger(“TFRL-cookbook-ch7-training-with-sim-server”)\n    # Set handler for this logger to handle messages\n    logger.addHandler(logging.StreamHandler())\n    # Set logging-level for this logger’s handler\n    logger.setLevel(logging.DEBUG)\n    ```", "```\n    current_time = datetime.datetime.now().strftime(“%Y%m%d-%H%M%S”)\n    train_log_dir = os.path.join(“logs”, “TFRL-Cookbook-Ch4-SAC”, current_time)\n    summary_writer = tf.summary.create_file_writer(train_log_dir)\n    ```", "```\n    if __name__ == “__main__”:\n        # Set up client to connect to sim server\n        sim_service_address = “http://127.0.0.1:6666”\n        client = Client(sim_service_address)\n    ```", "```\n        # Set up training environment\n        env_id = “StockTradingContinuousEnv-v0”\n        instance_id = client.env_create(env_id)\n    ```", "```\n        # Set up agent\n        observation_space_info = \\\n            client.env_observation_space_info(instance_id)\n        observation_shape = \\\n            observation_space_info.get(“shape”)\n        action_space_info = \\\n            client.env_action_space_info(instance_id)\n        action_space = gym.spaces.Box(\n            np.array(action_space_info.get(“low”)),\n            np.array(action_space_info.get(“high”)),\n            action_space_info.get(“shape”),\n        )\n        agent = SAC(observation_shape, action_space)\n    ```", "```\n        # Configure training\n        max_epochs = 30000\n        random_epochs = 0.6 * max_epochs\n        max_steps = 100\n        save_freq = 500\n        reward = 0\n        done = False\n        done, use_random, episode, steps, epoch, \\\n        episode_reward = (\n            False,\n            True,\n            0,\n            0,\n            0,\n            0,\n        )\n    ```", "```\n        cur_state = client.env_reset(instance_id)\n        # Start training\n        while epoch < max_epochs:\n            if steps > max_steps:\n                done = True\n    ```", "```\n            if done:\n                episode += 1\n                logger.info(\n                    f”episode:{episode} \\\n                     cumulative_reward:{episode_reward} \\\n                     steps:{steps} epochs:{epoch}”)\n                with summary_writer.as_default():\n                    tf.summary.scalar(“Main/episode_reward”, \n                                episode_reward, step=episode)\n                    tf.summary.scalar(“Main/episode_steps”,\n                                       steps, step=episode)\n                summary_writer.flush()\n                done, cur_state, steps, episode_reward = (\n                    False, \n                client.env_reset(instance_id), 0, 0,)\n                if episode % save_freq == 0:\n                    agent.save_model(\n                        f”sac_actor_episode{episode}_\\\n                          {env_id}.h5”,\n                        f”sac_critic_episode{episode}_\\\n                          {env_id}.h5”,\n                    )\n    ```", "```\n            if epoch > random_epochs:\n                use_random = False\n            action = agent.act(np.array(cur_state), \n                               use_random=use_random)\n            next_state, reward, done, _ = client.env_step(\n                instance_id, action.numpy().tolist()\n            )\n            agent.train(np.array(cur_state), action, reward,\n                        np.array(next_state), done)\n    ```", "```\n            cur_state = next_state\n            episode_reward += reward\n            steps += 1\n            epoch += 1\n            # Update Tensorboard with Agent’s training status\n            agent.log_status(summary_writer, epoch, reward)\n            summary_writer.flush()\n    ```", "```\n        agent.save_model(\n            f”sac_actor_final_episode_{env_id}.h5”, \\\n            f”sac_critic_final_episode_{env_id}.h5”\n        )\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python 3_training_rl_agents_using_remote_sims.py\n    ```", "```\n    Failed to establish a new connection: [Errno 111] Connection refused’))\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python tradegym_http_server.py\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python 3_training_rl_agents_using_remote_sims.py\n    ```", "```\n    ...\n    Total params: 16,257\n    Trainable params: 16,257\n    Non-trainable params: 0\n    __________________________________________________________________________________________________\n    None\n    episode:1 cumulative_reward:370.45421418744525 steps:9 epochs:9\n    episode:2 cumulative_reward:334.52956448599605 steps:9 epochs:18\n    episode:3 cumulative_reward:375.27432450733943 steps:9 epochs:27\n    episode:4 cumulative_reward:363.7160827166332 steps:9 epochs:36\n    episode:5 cumulative_reward:363.2819222532322 steps:9 epochs:45\n    ...\n    ```", "```\n#!/bin/env/python\nimport os\nimport sys\nfrom argparse import ArgumentParser\nimport imageio\nimport gym\n```", "```\n    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n    import tradegym  # Register tradegym envs with OpenAI Gym registry\n    from sac_agent_runtime import SAC\n    ```", "```\n    parser = ArgumentParser(prog=”TFRL-Cookbook-Ch7-Evaluating-RL-Agents”)\n    parser.add_argument(“--agent”, default=”SAC”, help=”Name of Agent. Default=SAC”)\n    ```", "```\n    parser.add_argument(\n        “--env”,\n        default=”StockTradingContinuousEnv-v0”,\n        help=”Name of Gym env. Default=StockTradingContinuousEnv-v0”,\n    )\n    parser.add_argument(\n        “--num-episodes”,\n        default=10,\n        help=”Number of episodes to evaluate the agent.\\\n              Default=100”,\n    )\n    ```", "```\n    parser.add_argument(\n        “--trained-models-dir”,\n        default=”trained_models”,\n        help=”Directory contained trained models. Default=trained_models”,\n    )\n    parser.add_argument(\n        “--model-version”,\n        default=”episode100”,\n        help=”Trained model version. Default=episode100”,\n    )\n    ```", "```\n    args = parser.parse_args()\n    ```", "```\n    if __name__ == “__main__”:\n        # Create an instance of the evaluation environment\n        env = gym.make(args.env)\n    ```", "```\n        if args.agent != “SAC”:\n            print(f”Unsupported Agent: {args.agent}. Using \\\n                    SAC Agent”)\n            args.agent = “SAC”\n        # Create an instance of the Soft Actor-Critic Agent\n        agent = SAC(env.observation_space.shape, \\\n                    env.action_space)\n    ```", "```\n        # Load trained Agent model/brain\n        model_version = args.model_version\n        agent.load_actor(\n            os.path.join(args.trained_models_dir, \\\n                         f”sac_actor_{model_version}.h5”)\n        )\n        agent.load_critic(\n            os.path.join(args.trained_models_dir, \\\n                         f”sac_critic_{model_version}.h5”)\n        )\n        print(f”Loaded {args.agent} agent with trained \\\n                model version:{model_version}”)\n    ```", "```\n        # Evaluate/Test/Rollout Agent with trained model/\n        # brain\n        video = imageio.get_writer(“agent_eval_video.mp4”,\\\n                                    fps=30)\n        avg_reward = 0\n        for i in range(args.num_episodes):\n            cur_state, done, rewards = env.reset(), False, 0\n            while not done:\n                action = agent.act(cur_state, test=True)\n                next_state, reward, done, _ = \\\n                                    env.step(action[0])\n                cur_state = next_state\n                rewards += reward\n                if render:\n                    video.append_data(env.render(mode=\\\n                                                ”rgb_array”))\n            print(f”Episode#:{i} cumulative_reward:\\\n                    {rewards}”)\n            avg_reward += rewards\n        avg_reward /= args.num_episodes\n        video.close()\n        print(f”Average rewards over {args.num_episodes} \\\n                episodes: {avg_reward}”)\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python 4_evaluating_rl_agents.py\n    ```", "```\n    ...\n    ==================================================================================================\n    Total params: 16,257\n    Trainable params: 16,257\n    Non-trainable params: 0\n    __________________________________________________________________________________________________\n    None\n    Loaded SAC agent with trained model version:episode100\n    Episode#:0 cumulative_reward:382.5117154452246\n    Episode#:1 cumulative_reward:359.27720004181674\n    Episode#:2 cumulative_reward:370.92829808499664\n    Episode#:3 cumulative_reward:341.44002189086007\n    Episode#:4 cumulative_reward:364.32631211784394\n    Episode#:5 cumulative_reward:385.89219327764476\n    Episode#:6 cumulative_reward:365.2120387185878\n    Episode#:7 cumulative_reward:339.98494537310785\n    Episode#:8 cumulative_reward:362.7133769241483\n    Episode#:9 cumulative_reward:379.12388043270073\n    Average rewards over 10 episodes: 365.1409982306931\n    ...\n    ```", "```\nimport os\nimport sys\nfrom argparse import ArgumentParser\nimport gym.spaces\nfrom flask import Flask, request\nimport numpy as np\n```", "```\n    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n    from sac_agent_runtime import SAC\n    ```", "```\n    parser = ArgumentParser(\n        prog=”TFRL-Cookbook-Ch7-Packaging-RL-Agents-For-Cloud-Deployments”\n    )\n    parser.add_argument(“--agent”, default=”SAC”, help=”Name of Agent. Default=SAC”)\n    ```", "```\n    parser.add_argument(\n        “--host-ip”,\n        default=”0.0.0.0”,\n        help=”IP Address of the host server where Agent  \n              service is run. Default=127.0.0.1”,\n    )\n    parser.add_argument(\n        “--host-port”,\n        default=”5555”,\n        help=”Port on the host server to use for Agent \n              service. Default=5555”,\n    )\n    ```", "```\n    parser.add_argument(\n        “--trained-models-dir”,\n        default=”trained_models”,\n        help=”Directory contained trained models. \\\n              Default=trained_models”,\n    )\n    parser.add_argument(\n        “--model-version”,\n        default=”episode100”,\n        help=”Trained model version. Default=episode100”,\n    )\n    ```", "```\n    parser.add_argument(\n        “--observation-shape”,\n        default=(6, 31),\n        help=”Shape of observations. Default=(6, 31)”,\n    )\n    parser.add_argument(\n        “--action-space-low”, default=[-1], help=”Low value \\\n         of action space. Default=[-1]”\n    )\n    parser.add_argument(\n        “--action-space-high”, default=[1], help=”High value\\\n         of action space. Default=[1]”\n    )\n    parser.add_argument(\n        “--action-shape”, default=(1,), help=”Shape of \\\n        actions. Default=(1,)”\n    )\n    ```", "```\n    args = parser.parse_args()\n    if __name__ == “__main__”:\n    ```", "```\n        if args.agent != “SAC”:\n            print(f”Unsupported Agent: {args.agent}. Using \\\n                    SAC Agent”)\n            args.agent = “SAC”\n        # Set Agent’s runtime configs\n        observation_shape = args.observation_shape\n        action_space = gym.spaces.Box(\n            np.array(args.action_space_low),\n            np.array(args.action_space_high),\n            args.action_shape,\n        )\n    ```", "```\n        # Create an instance of the Agent\n        agent = SAC(observation_shape, action_space)\n        # Load trained Agent model/brain\n        model_version = args.model_version\n        agent.load_actor(\n            os.path.join(args.trained_models_dir, \\\n                         f”sac_actor_{model_version}.h5”)\n        )\n        agent.load_critic(\n            os.path.join(args.trained_models_dir, \\\n                         f”sac_critic_{model_version}.h5”)\n        )\n        print(f”Loaded {args.agent} agent with trained model\\\n                 version:{model_version}”)\n    ```", "```\n        # Setup Agent (http) service\n        app = Flask(__name__)\n        @app.route(“/v1/act”, methods=[“POST”])\n        def get_action():\n            data = request.get_json()\n            action = agent.act(np.array(data.get(\n                               “observation”)), test=True)\n            return {“action”: action.numpy().tolist()}\n    ```", "```\n        # Launch/Run the Agent (http) service\n        app.run(host=args.host_ip, port=args.host_port, \n                debug=True)\n    ```", "```\n    FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\n    # TensorFlow2.x Reinforcement Learning Cookbook\n    # Chapter 7: Deploying Deep RL Agents to the cloud\n    LABEL maintainer=”emailid@domain.tld”\n    ```", "```\n    RUN apt-get install -y wget git make cmake zlib1g-dev && rm -rf /var/lib/apt/lists/*\n    ```", "```\n    ENV PATH=”/root/miniconda3/bin:${PATH}”\n    ARG PATH=”/root/miniconda3/bin:${PATH}”\n    RUN apt-get update\n    RUN wget \\\n        https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n        && mkdir /root/.conda \\\n        && bash Miniconda3-latest-Linux-x86_64.sh -b \\\n        && rm -f Miniconda3-latest-Linux-x86_64.sh\n    # conda>4.9.0 is required for `--no-capture-output`\n    RUN conda update -n base conda\n    ```", "```\n    ADD . /root/tf-rl-cookbook/ch7\n    WORKDIR /root/tf-rl-cookbook/ch7\n    RUN conda env create -f “tfrl-cookbook.yml” -n “tfrl-cookbook”\n    ```", "```\n    ENTRYPOINT [ “conda”, “run”, “--no-capture-output”, “-n”, “tfrl-cookbook”, “python” ]\n    CMD [ “5_packaging_rl_agents_for_deployment.py” ]\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$docker build -f Dockerfile -t tfrl-cookbook/ch7-trading-bot:latest\n    ```", "```\n    Sending build context to Docker daemon  1.793MB\n    Step 1/13 : FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\n     ---> a3bd8cb789b0\n    Step 2/13 : LABEL maintainer=”emailid@domain.tld”\n     ---> Using cache\n     ---> 4322623c24c8\n    Step 3/13 : ENV PATH=”/root/miniconda3/bin:${PATH}”\n     ---> Using cache\n     ---> e9e8c882662a\n    Step 4/13 : ARG PATH=”/root/miniconda3/bin:${PATH}”\n     ---> Using cache\n     ---> 31d45d5bcb05\n    Step 5/13 : RUN apt-get update\n     ---> Using cache\n     ---> 3f7ed3eb3c76\n    Step 6/13 : RUN apt-get install -y wget git make cmake zlib1g-dev && rm -rf /var/lib/apt/lists/*\n     ---> Using cache\n     ---> 0ffb6752f5f6\n    Step 7/13 : RUN wget     https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh     && mkdir /root/.conda     && bash Miniconda3-latest-Linux-x86_64.sh -b     && rm -f Miniconda3-latest-Linux-x86_64.sh\n     ---> Using cache\n    ```", "```\n    Step 9/13 : ADD . /root/tf-rl-cookbook/ch7\n     ---> ed8541c42ebc\n    Step 10/13 : WORKDIR /root/tf-rl-cookbook/ch7\n     ---> Running in f5a9c6ad485c\n    Removing intermediate container f5a9c6ad485c\n     ---> 695ca00c6db3\n    Step 11/13 : RUN conda env create -f “tfrl-cookbook.yml” -n “tfrl-cookbook”\n     ---> Running in b2a9706721e7\n    Collecting package metadata (repodata.json): ...working... done\n    Solving environment: ...working... done...\n    ```", "```\n    Step 13/13 : CMD [ “2_packaging_rl_agents_for_deployment.py” ]\n     ---> Running in 336e442b0218\n    Removing intermediate container 336e442b0218\n     ---> cc1caea406e6\n    Successfully built cc1caea406e6\n    Successfully tagged tfrl-cookbook/ch7:latest\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$docker build -f Dockerfile -t tfrl-cookbook/ch7-trading-bot:latest\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$docker run -it -p 5555:5555 tfrl-cookbook/ch7-trading-bot\n    ```", "```\n    ...==================================================================================================\n    Total params: 16,257\n    Trainable params: 16,257\n    Non-trainable params: 0\n    __________________________________________________________________________________________________\n    None\n    Loaded SAC agent with trained model version:episode100\n     * Debugger is active!\n     * Debugger PIN: 604-104-903\n    ...\n    ```", "```\n    #Simple test script for the deployed Trading Bot-as-a-Service\n    import os\n    import sys\n    import gym\n    import requests\n    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n    import tradegym  # Register tradegym envs with OpenAI Gym # registry\n    host_ip = “127.0.0.1”\n    host_port = 5555\n    endpoint = “v1/act”\n    env = gym.make(“StockTradingContinuousEnv-v0”)\n    post_data = {“observation”: env.observation_space.sample().tolist()}\n    res = requests.post(f”http://{host_ip}:{host_port}/{endpoint}”, json=post_data)\n    if res.ok:\n        print(f”Received Agent action:{res.json()}”)\n    ```", "```\n    (tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$python test_agent_service.py\n    ```", "```\n    172.17.0.1 - - [00/Mmm/YYYY hh:mm:ss] “POST /v1/act HTTP/1.1” 200 -\n    ```", "```\n    Received Agent action:{‘action’: [[0.008385116065491426]]}\n    ```", "```\n    sudo snap install --classic heroku\n    ```", "```\n    heroku container:login\n    ```", "```\n    tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$heroku create\n    ```", "```\n    Creating app... !\n         Invalid credentials provided.\n     ›   Warning: heroku update available from 7.46.2 to \n    7.47.0.\n    heroku: Press any key to open up the browser to login or q to exit:\n    ```", "```\n    Creating salty-fortress-4191... done, stack is heroku-18\n    https://salty-fortress-4191.herokuapp.com/ | https://git.heroku.com/salty-fortress-4191.git\n    ```", "```\n    heroku container:push web\n    ```", "```\n    heroku container:release web\n    ```"]