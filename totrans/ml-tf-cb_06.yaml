- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce neural networks and how to implement them
    in TensorFlow. Most of the subsequent chapters will be based on neural networks,
    so learning how to use them in TensorFlow is very important.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are currently breaking records in tasks such as image and speech
    recognition, reading handwriting, understanding text, image segmentation, dialog
    systems, autonomous car driving, and so much more. While some of these tasks will
    be covered in later chapters, it is important to introduce neural networks as
    a general-purpose, easy-to-implement machine learning algorithm, so that we can
    expand on it later.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a neural network has been around for decades. However, it only
    recently gained traction because we now have the computational power to train
    large networks because of advances in processing power, algorithm efficiency,
    and data sizes.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network is, fundamentally, a sequence of operations applied to a matrix
    of input data. These operations are usually collections of additions and multiplications
    followed by the application of non-linear functions. One example that we have
    already seen is logistic regression, which we looked at in *Chapter 4*, *Linear
    Regression*. Logistic regression is the sum of partial slope-feature products
    followed by the application of the sigmoid function, which is non-linear. Neural
    networks generalize this a little more by allowing any combination of operations
    and non-linear functions, which includes the application of absolute values, maximums,
    minimums, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The most important trick to neural networks is called **backpropagation**. Backpropagation
    is a procedure that allows us to update model variables based on the learning
    rate and the output of the loss function. We used backpropagation to update our
    model variables in *Chapter 3*, *Keras*, and *Chapter 4*, *Linear Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: Another important feature to take note of regarding neural networks is the non-linear
    activation function. Since most neural networks are just combinations of addition
    and multiplication operations, they will not be able to model non-linear datasets.
    To address this issue, we will use non-linear activation functions in our neural
    networks. This will allow the neural network to adapt to most non-linear situations.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that, as we have seen in many of the algorithms
    covered, neural networks are sensitive to the hyperparameters we choose. In this
    chapter, we will explore the impact of different learning rates, loss functions,
    and optimization procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few more resources I would recommend to you for learning about
    neural networks that cover the topic in greater depth and more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The seminal paper describing backpropagation is *Efficient Back Prop* by Yann
    LeCun et al. The PDF is located here: [http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CS231, *Convolutional Neural Networks for Visual Recognition*, by Stanford University.
    Class resources are available here: [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CS224d, *Deep Learning for Natural Language Processing*, by Stanford University.
    Class resources are available here: [http://cs224d.stanford.edu/](http://cs224d.stanford.edu/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning*, a book by the MIT Press. Goodfellow, et al. 2016\. The book
    is located here: [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The online book *Neural Networks and Deep Learning* by Michael Nielsen, which
    is located here: [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a more pragmatic approach and introduction to neural networks, Andrej Karpathy
    has written a great summary with JavaScript examples called *A Hacker's Guide
    to Neural Networks*. The write-up is located here: [http://karpathy.github.io/neuralnets/](http://karpathy.github.io/neuralnets/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another site that summarizes deep learning well is called *Deep Learning for
    Beginners* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. The web page
    can be found here: [http://randomekek.github.io/deep/deeplearning.html](http://randomekek.github.io/deep/deeplearning.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start by introducing the basic concepts of neural networking before
    working up to multilayer networks. In the last section, we will create a neural
    network that will learn how to play Tic-Tac-Toe.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing operational gates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with gates and activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a one-layer neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing different layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a multilayer neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving the predictions of linear models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to play Tic-Tac-Toe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reader can find all of the code from this chapter online at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook), and
    on the Packt repository at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing operational gates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most fundamental concepts of neural networks is its functioning as
    an operational gate. In this section, we will start with a multiplication operation
    as a gate, before moving on to consider nested gate operations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first operational gate we will implement is *f*(*x*) = *a* · *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: To optimize this gate, we declare the *a* input as a variable and *x* as the
    input tensor of our model. This means that TensorFlow will try to change the *a* value
    and not the *x* value. We will create the loss function as the difference between
    the output and the target value, which is 50.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second, nested, operational gate will be *f*(*x*) = *a* · *x* + *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, we will declare *a* and *b* as variables and *x* as the input tensor
    of our model. We optimize the output toward the target value of 50 again. The
    interesting thing to note is that the solution for this second example is not
    unique. There are many combinations of model variables that will allow the output
    to be 50\. With neural networks, we do not care so much about the values of the
    intermediate model variables, but instead place more emphasis on the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement the first operational gate, *f*(*x*) = *a* · *x*, in TensorFlow
    and train the output toward the value of 50, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start off by loading TensorFlow as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will need to declare our model variable and input data. We make our
    input data equal to the value 5, so that the multiplication factor to get 50 will
    be 10 (that is, 5*10=50), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create a lambda layer that computes the operation, and we create a
    functional Keras model with the following input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now declare our optimizing algorithm as the stochastic gradient descent
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now optimize our model output toward the desired value of 50\. We will
    use the loss function as the L2 distance between the output and the desired target
    value of 50\. We do this by continually feeding in the input value of 5 and backpropagating
    the loss to update the model variable toward the value of 10, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will do the same with the two-nested operational gate, *f*(*x*) = *a*
    · *x* + *b*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will start in exactly the same way as the preceding example, but will initialize
    two model variables, `a` and `b`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now optimize the model variables to train the output toward the target value
    of 50, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is important to note here that the solution to the second example is not
    unique. This does not matter as much in neural networks, as all parameters are
    adjusted toward reducing the loss. The final solution here will depend on the
    initial values of a and b. If these were randomly initialized, instead of to the
    value of 1, we would see different ending values for the model variables for each
    iteration.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We achieved the optimization of a computational gate via TensorFlow's implicit
    backpropagation. TensorFlow keeps track of our model's operations and variable
    values and makes adjustments in respect of our optimization algorithm specification
    and the output of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: We can keep expanding the operational gates while keeping track of which inputs
    are variables and which inputs are data. This is important to keep track of, because
    TensorFlow will change all variables to minimize the loss but not the data.
  prefs: []
  type: TYPE_NORMAL
- en: The implicit ability to keep track of the computational graph and update the
    model variables automatically with every training step is one of the great features
    of TensorFlow and what makes it so powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Working with gates and activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we can link together operational gates, we want to run the computational
    graph output through an activation function. In this section, we will introduce
    common activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will compare and contrast two different activation functions: **sigmoid** and **rectified
    linear unit** (**ReLU**). Recall that the two functions are given by the following
    equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_06_03.png)![](img/B16254_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we will create two one-layer neural networks with the same
    structure, except that one will feed through the sigmoid activation and one will
    feed through the ReLU activation. The loss function will be governed by the L2
    distance from the value 0.75\. We will randomly pull batch data and then optimize
    the output toward 0.75.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the necessary libraries. This is also a good point
    at which we can bring up how to set a random seed with TensorFlow. Since we will
    be using a random number generator from NumPy and TensorFlow, we need to set a
    random seed for both. With the same random seeds set, we should be able to replicate
    the results. We do this with the following input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to declare our batch size, model variables, and data model inputs.
    Our computational graph will consist of feeding in our normally distributed data
    into two similar neural networks that differ only by the activation function at
    the end, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll declare our two models, the sigmoid activation model and the ReLU
    activation model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to declare our optimization algorithm and initialize our variables,
    shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ll loop through our training for 750 iterations for both models, as
    shown in the following code block. The loss functions will be the average L2 norm
    between the model output and the value of 0.75\. We will also save the loss output
    and the activation output values for plotting later on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To plot the loss and the activation outputs, we need to input the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The activation output needs to be plotted as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/B8A59E6E.tmp](img/B16254_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Computational graph outputs from the network with the sigmoid activation
    and a network with the ReLU activation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two neural networks work with a similar architecture and target (0.75)
    but with two different activation functions, sigmoid and ReLU. It is important
    to notice how much more rapidly the ReLU activation network converges to the desired
    target of 0.75 than the sigmoid activation, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/F46E84AC.tmp](img/B16254_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: This figure depicts the loss value of the sigmoid and the ReLU
    activation networks. Notice how extreme the ReLU loss is at the beginning of the
    iterations'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because of the form of the ReLU activation function, it returns the value of
    zero much more often than the sigmoid function. We consider this behavior as a
    type of sparsity. This sparsity results in a speeding up of convergence, but a
    loss of controlled gradients. On the other hand, the sigmoid function has very
    well-controlled gradients and does not risk the extreme values that the ReLU activation
    does, as illustrated in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Activation function | Advantages | Disadvantages |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid | Less extreme outputs | Slower convergence |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU | Quick convergence | Extreme output values possible |'
  prefs: []
  type: TYPE_TB
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we compared the ReLU activation function and the sigmoid activation
    function for neural networks. There are many other activation functions that are
    commonly used for neural networks, but most fall into either one of two categories;
    the first category contains functions that are shaped like the sigmoid function,
    such as arctan, hypertangent, heaviside step, and so on; the second category contains
    functions that are shaped like the ReLU function, such as softplus, leaky ReLU,
    and so on. Most of what we discussed in this section about comparing the two functions
    will hold true for activations in either category. However, it is important to
    note that the choice of activation function has a big impact on the convergence
    and the output of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a one-layer neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have all of the tools needed to implement a neural network that operates
    on real data, so in this section, we will create a neural network with one layer
    that operates on the `Iris` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will implement a neural network with one hidden layer. It
    will be important to understand that a fully connected neural network is based
    mostly on matrix multiplication. As such, it is important that the dimensions
    of the data and matrix are lined up correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a regression problem, we will use **mean squared error** (**MSE**)
    as the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the computational graph, we''ll start by loading the following necessary
    libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ll load the `Iris` data and store the length as the target value with
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since the dataset is smaller, we will want to set a seed to make the results
    reproducible, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To prepare the data, we''ll create a 80-20 train-test split and normalize the
    `x` features to be between `0` and `1` via min-max scaling, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will declare the batch size and the data model input with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The important part is to declare our model variables with the appropriate shape.
    We can declare the size of our hidden layer to be any size we wish; in the following
    code block, we have set it to have five hidden nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll now declare our model in two steps. The first step will be creating
    the hidden layer output and the second will be creating the `final_output` of
    the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a note, our model goes from three input features to five hidden nodes, and
    finally to one output value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ll declare our optimizing algorithm with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we loop through our training iterations. We''ll also initialize two lists
    in which we can store our `train` and `test_loss` functions. In every loop, we
    also want to randomly select a batch from the training data for fitting to the
    model, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can plot the losses with `matplotlib` and the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We proceed with the recipe by plotting the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/EE3BD051.tmp](img/B16254_06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: We plot the loss (MSE) of the train and test set'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also see that the train set loss is not as smooth as that
    in the test set. This is because of two reasons: the first is that we are using
    a smaller batch size than the test set, although not by much; the second cause
    is the fact that we are training on the train set, and the test set does not impact
    the variables of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our model has now been visualized as a neural network diagram, as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: A neural network diagram'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure is a visualization of our neural network that has five
    nodes in the hidden layer. We are feeding in three values: the **sepal length**
    (**S.L.**), the **sepal width** (**S.W.**), and the **petal length** (**P.L.**).
    The target will be the petal width. In total, there will be 26 total variables
    in the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing different layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to know how to implement different layers. In the preceding
    recipe, we implemented fully connected layers. In this recipe, we will further
    expand our knowledge of various layers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have explored how to connect data inputs and a fully connected hidden layer,
    but there are more types of layers available as built-in functions inside TensorFlow.
    The most popular layers that are used are convolutional layers and maxpool layers.
    We will show you how to create and use such layers with input data and with fully
    connected data. First, we will look at how to use these layers on one-dimensional
    data, and then on two-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: While neural networks can be layered in any fashion, one of the most common
    designs is to use convolutional layers and fully connected layers to first create
    features. If we then have too many features, it is common to use a maxpool layer.
  prefs: []
  type: TYPE_NORMAL
- en: After these layers, non-linear layers are commonly introduced as activation
    functions. **Convolutional neural networks** (**CNNs**), which we will consider
    in *Chapter 8*, *Convolutional Neural Networks, *usually have convolutional, maxpool,
    and activation layers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first look at one-dimensional data. We need to generate a random array
    of data for this task using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by loading the libraries we need, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ll initialize some parameters and we''ll create the input data layer
    with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define a convolutional layer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For our example data, we have a batch size of `1`, a width of `1`, a height
    of `25`, and a channel size of `1`. Also note that we can calculate the output
    dimensions of convolutional layers with the `output_size=(W-F+2P)/S+1` formula,
    where `W` is the input size, `F` is the filter size, `P` is the padding size,
    and `S` is the stride size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we add a ReLU activation layer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ll add a maxpool layer. This layer will create a `maxpool` on a moving
    window across our one-dimensional vector. For this example, we will initialize
    it to have a width of 5, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TensorFlow's `maxpool` arguments are very similar to those of the convolutional
    layer. While a `maxpool` argument does not have a filter, it does have size, stride,
    and padding options. Since we have a window of 5 with valid padding (no zero padding),
    then our output array will have 4 fewer entries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final layer that we will connect is the fully connected layer. Here, we
    will use a dense layer, as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ll create the model, and print the output of each of the layers, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: One-dimensional data is very important to consider for neural networks. Time
    series, signal processing, and some text embeddings are considered to be one-dimensional
    and are frequently used in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now consider the same types of layer in an equivalent order but for
    two-dimensional data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by initializing the variables, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we will initialize our input data layer. Since our data has a height and
    width already, we just need to expand it in two dimensions (a batch size of 1,
    and a channel size of 1) as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Just as in the one-dimensional example, we now need to add a 2D convolutional
    layer. For the filter, we will use a random 2x2 filter, a stride of 2 in both
    directions, and valid padding (in other words, no zero padding). Because our input
    matrix is 10x10, our convolutional output will be 5x5, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we add a ReLU activation layer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our maxpool layer is very similar to the one-dimensional case, except we have
    to declare a width and height for the maxpool window and the stride. In our case,
    we will use the same value for all spatial dimensions so we will set integer values,
    shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our fully connected layer is very similar to the one-dimensional output. We
    use a dense layer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we''ll create the model, and print the output of each of the layers, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We should now know how to use the convolutional and maxpool layers in TensorFlow
    with one-dimensional and two-dimensional data. Regardless of the shape of the
    input, we ended up with outputs of the same size. This is important for illustrating
    the flexibility of neural network layers. This section should also impress upon
    us again the importance of shapes and sizes in neural network operations.
  prefs: []
  type: TYPE_NORMAL
- en: Using a multilayer neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now apply our knowledge of different layers to real data by using a
    multilayer neural network on the low birth weight dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know how to create neural networks and work with layers, we will
    apply this methodology with the aim of predicting birth weights in the low birth
    weight dataset. We'll create a neural network with three hidden layers. The low
    birth weight dataset includes the actual birth weights and an indicator variable
    for whether the given birth weight is above or below 2,500 grams. In this example,
    we'll make the target the actual birth weight (regression) and then see what the
    accuracy is on the classification at the end. At the end, our model should be
    able to identify whether the birth weight will be <2,500 grams.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the libraries as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll now load the data from the website using the `requests` module. After
    this, we will split the data into features of interest and the target value, shown
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To help with repeatability, we now need to set the random seed for both NumPy
    and TensorFlow. Then we declare our batch size as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we split the data into an 80-20 train-test split. After this, we need
    to normalize our input features so that they are between 0 and 1 with min-max
    scaling, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Normalizing input features is a common feature transformation and is especially
    useful for neural networks. It will help with convergence if our data is centered
    between 0 and 1 for the activation functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since we have multiple layers that have similar initialized variables, we now
    need to create a function to initialize both the weights and the bias. We do that
    with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now need to initialize our input data layer. There will be seven input features.
    The output will be the birth weight in grams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The fully connected layer will be used three times for all three hidden layers.
    To prevent repeated code, we will create a layer function for use when we initialize
    our model, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now it''s time to create our model. For each layer (and output layer), we will
    initialize a weight matrix, bias matrix, and the fully connected layer. For this
    example, we will use hidden layers of sizes 25, 10, and 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model that we are using will have 522 variables to fit. To arrive at this
    number, we can see that between the data and the first hidden layer we have 7*25+25=200 variables.
    If we continue in this way and add them up, we'll have 200+260+33+4=497 variables.
    This is significantly larger than the nine variables that we used in the logistic
    regression model on this data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll now declare our optimizer (using Adam optimization) and loop through
    our training iterations. We will use the L1 loss function (the absolute value).
    We''ll also initialize two lists in which we can store our `train` and `test_loss` functions.
    In every loop, we also want to randomly select a batch from the training data
    for fitting to the model and print the status every 25 generations, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is a snippet of code that plots the train and test loss with `matplotlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We proceed with the recipe by plotting the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/AE276A4A.tmp](img/B16254_06_09.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.5: In the preceding figure, we plot the train and test losses for
    our neural network that we trained to predict birth weight in grams. Notice that
    we have arrived at a good model after approximately 30 generations'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we need to output the train and test regression results and turn them
    into classification results by creating an indicator for if they are above or
    below 2,500 grams. To find out the model''s accuracy, we need to use the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, both the train set accuracy and the test set accuracy are quite
    good and the models learn without under- or overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we created a regression neural network with three fully connected
    hidden layers to predict the birth weight of the low birth weight dataset. In
    the next recipe, we will try to improve our logistic regression by making it a
    multiple-layer, logistic-type neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the predictions of linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will attempt to improve our logistic model by increasing
    the accuracy of the low birth weight prediction. We will use a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will load the low birth weight data and use a neural network
    with two hidden fully connected layers with sigmoid activations to fit the probability
    of a low birth weight.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the libraries and initializing our computational graph
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load, extract, and normalize our data as in the preceding recipe,
    except that here we are going to be using the low birth weight indicator variable
    as our target instead of the actual birth weight, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to declare our batch size, our seed in order to have reproductible
    results, and our input data layer as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As previously, we now need to declare functions that initialize a variable
    and a layer in our model. To create a better logistic function, we need to create
    a function that returns a logistic layer on an input layer. In other words, we
    will just use a fully connected layer and return a sigmoid element for each layer.
    It is important to remember that our loss function will have the final sigmoid
    included, so we want to specify on our last layer that we will not return the
    sigmoid of the output, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will declare three layers (two hidden layers and an output layer). We
    will start by initializing a weight and bias matrix for each layer and defining
    the layer operations as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define a loss function (cross-entropy) and declare the optimization
    algorithm, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Cross-entropy is a way of measuring distances between probabilities. Here, we
    want to measure the difference between certainty (0 or 1) and our model probability
    (0 < x < 1). TensorFlow implements cross-entropy with the built-in sigmoid function.
    This is also important as part of the hyperparameter tuning, as we are more likely
    to find the best loss function, learning rate, and optimization algorithm for
    the problem at hand. For brevity in this recipe, we do not include hyperparameter
    tuning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to evaluate and compare our model to previous models, we need to create
    a prediction and accuracy operation on the graph. This will allow us to feed in
    the whole test set and determine the accuracy, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to start our training loop. We will train for 1,500 generations
    and save the model loss and train and test accuracies for plotting later. Our
    training loop is started with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code blocks illustrate how to plot the cross-entropy loss and
    train and test set accuracies with `matplotlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the plot for cross-entropy loss per generation as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/E3D23CF0.tmp](img/B16254_06_10.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.6: Training loss over 1,500 iterations'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within approximately 150 generations, we have reached a good model. As we continue
    to train, we can see that very little is gained over the remaining iterations,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/D849E07E.tmp](img/B16254_06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Accuracy for the train set and test set'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding diagram, we arrived at a good model very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When considering using neural networks to model data, you have to consider the
    advantages and disadvantages. While our model has converged faster than previous
    models, and perhaps with greater accuracy, this comes with a price; we are training
    many more model variables and there is a greater chance of overfitting. To check
    if overfitting is occurring, we look at the accuracy of the test and train sets.
    If the accuracy of the training set continues to increase while the accuracy on
    the test set stays the same or even decreases slightly, we can assume overfitting
    is occurring.
  prefs: []
  type: TYPE_NORMAL
- en: To combat underfitting, we can increase our model depth or train the model for
    more iterations. To address overfitting, we can add more data or add regularization
    techniques to our model.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that our model variables are not as interpretable
    as a linear model. Neural network models have coefficients that are harder to
    interpret than linear models, as they explain the significance of features within
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Learning to play Tic-Tac-Toe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To show how adaptable neural networks can be, we will now attempt to use a neural
    network in order to learn the optimal moves for Tic-Tac-Toe. We will approach
    this knowing that Tic-Tac-Toe is a deterministic game and that the optimal moves
    are already known.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train our model, we will use a list of board positions followed by the optimal
    response for a number of different boards. We can reduce the amount of boards
    to train on by considering only board positions that are different with regard
    to symmetry. The non-identity transformations of a Tic-Tac-Toe board are a rotation
    (in either direction) of 90 degrees, 180 degrees, and 270 degrees, a horizontal
    reflection, and a vertical reflection. Given this idea, we will use a shortlist
    of boards with the optimal move, apply two random transformations, and then feed
    that into our neural network for learning.
  prefs: []
  type: TYPE_NORMAL
- en: Since Tic-Tac-Toe is a deterministic game, it is worth noting that whoever goes
    first should either win or draw. We will hope for a model that can respond to
    our moves optimally and ultimately result in a draw.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we denote Xs using 1, Os using -1, and empty spaces using 0, then the following
    diagram illustrates how we can consider a board position and an optimal move as
    a row of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_06_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Here, we illustrate how to consider a board and an optimal move
    as a row of data. Note that X = 1, O = -1, empty spaces are 0, and we start indexing
    at 0'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the model loss, to check how our model is performing we will
    do two things. The first check we will perform is to remove a position and an
    optimal move row from our training set. This will allow us to see if the neural
    network model can generalize a move it hasn't seen before. The second way to evaluate
    our model is to actually play a game against it at the end.
  prefs: []
  type: TYPE_NORMAL
- en: The list of possible boards and optimal moves can be found in the GitHub directory
    for this recipe at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/tree/master/ch6/08_Learning_Tic_Tac_Toe](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/tree/master/ch6/08_Lea) and
    in the Packt repository at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to start by loading the necessary libraries for this script, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we declare the following batch size for training our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To make visualizing the boards a bit easier, we will create a function that
    outputs Tic-Tac-Toe boards with Xs and Os. This is done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we have to create a function that will return a new board and an optimal
    response position under a transformation. This is done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The list of boards and their optimal responses is in a `.csv` file in the directory
    available in the GitHub repository at [https://github.com/nfmcclure/tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook) or
    the Packt repository at [https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition](https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition).
    We will create a function that will load the file with the boards and responses
    and will store it as a list of tuples, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to tie everything together to create a function that will return
    a randomly transformed board and response. This is done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load our data and create a training set as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remember that we want to remove one board and an optimal response from our
    training set to see if the model can generalize making the best move. The best
    move for the following board will be to play at index number 6:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now initialize the weights and bias and create our models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we create our model. Note that we do not include the `softmax()` activation
    function in the following model because it is included in the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will declare our optimizer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now loop through the training of our neural network with the following
    code. Note that our `loss` function will be the average softmax of the final output
    logits (unstandardized output):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the code needed to plot the loss over the model training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should get the following plot for the loss per generation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/2B562542.tmp](img/B16254_06_13.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.9: A Tic-Tac-Toe train set loss over 10,000 iterations'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the preceding diagram, we have plotted the loss over the training steps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To test the model, we need to see how it performs on the test board that we
    removed from the training set. We are hoping that the model can generalize and
    predict the optimal index for moving, which will be index number 6\. Most of the
    time the model will succeed, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to evaluate our model, we need to play against our trained model.
    To do this, we have to create a function that will check for a win. This way,
    our program will know when to stop asking for more moves. This is done with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can loop through and play a game with our model. We start with a blank
    board (all zeros), we ask the user to input an index (0-8) of where to play, and
    we then feed that into the model for a prediction. For the model''s move, we take
    the largest available prediction that is also an open space. From this game, we
    can see that our model is not perfect, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding step should result in the following interactive output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, a human player beats the machine very quickly and easily.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we trained a neural network to play Tic-Tac-Toe by feeding
    in board positions and a nine-dimensional vector, and predicted the optimal response.
    We only had to feed in a few possible Tic-Tac-Toe boards and apply random transformations
    to each board to increase the training set size.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test our algorithm, we removed all instances of one specific board and saw
    whether our model could generalize to predict the optimal response. Finally, we
    played a sample game against our model. This model isn''t perfect yet. Using more
    data or applying a more complex neural network architecture could be done to improve
    it. But the better thing to do is to change the type of learning: instead of using
    supervised learning, we''re better off using a reinforcement learning-based approach.'
  prefs: []
  type: TYPE_NORMAL
