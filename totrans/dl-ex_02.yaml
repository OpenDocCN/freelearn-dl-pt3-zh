- en: Data Modeling in Action - The Titanic Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear models are the basic learning algorithms in the field of data science.
    Understanding how a linear model works is crucial in your journey of learning
    data science because it's the basic building block for most of the sophisticated
    learning algorithms out there, including neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to dive into a famous problem in the field of
    data science, which is the Titanic example. The purpose of this example is to
    introduce linear models for classification and see a full machine learning system
    pipeline, starting from data handling and exploration up to model evaluation.
    We are going to cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear models for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear models for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Titanic example—model building and training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear models for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression models are the most basic type of regression models and are
    widely used in predictive data analysis. The overall idea of regression models
    is to examine two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Does a set of explanatory features / input variables do a good job at predicting
    an output variable? Is the model using features that account for the variability
    in changes to the dependent variable (output variable)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which features in particular are significant ones of the dependent variable?
    And in what way do they impact the dependent variable (indicated by the magnitude
    and sign of the parameters)? These regression parameters are used to explain the
    relationship between one output variable (dependent variable) and one or more
    input features (independent variables).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A regression equation will formulate the impact of the input variables (independent
    variables) on the output variable (dependent variable). The simplest form of this
    equation, with one input variable and one output variable, is defined by this
    formula *y = c + b*x.* Here, *y *= estimated dependent score, *c *= constant,
    *b *= regression parameter/coefficients, and *x *= input (independent) variable.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression models are the building blocks of many learning algorithms,
    but this is not the only reason behind their popularity. The following are the
    key factors behind their popularity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Widely used**: Linear regression is the oldest regression technique and it''s
    widely used in many applications, such as forecasting and financial analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runs fast**: Linear regression algorithms are very simple and don''t include
    mathematical computations which are too expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy to use** (**not a lot of tuning required**): Linear regression is very
    easy to use, and mostly it''s the first learning method to learn about in the
    machine learning or data science class as you don''t have too many hyperparameters
    to tune in order to get better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Highly interpretable**: Because of its simplicity and ease of inspecting
    the contribution of each predictor-coefficient pair, linear regression is highly
    interpretable; you can easily understand the model behavior and interpret the
    model output for non-technical guys. If a coefficient is zero, the associated
    predictor variable contributes nothing. If a coefficient is not zero, the contribution
    due to the specific predictor variable can easily be ascertained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Basis for many other methods**: Linear regression is considered the underlying
    foundation for many learning methods, such as neural networks and its growing
    part, deep learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advertising – a financial example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to better understand linear regression models, we will go through an
    example advertisement. We will try to predict the sales of some companies given
    some factors related to the amount of money spent by these companies on advertising
    in TV, radio, and newspapers.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To model our advertising data samples using linear regression, we will be using
    the Stats models library to get nice characteristics for linear models, but later
    on, we will be using scikit-learn, which has very useful functionality for data
    science in general.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data with pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are lots of libraries out there in Python that you can use to read, transform,
    or write data. One of these libraries is pandas ([http://pandas.pydata.org/](http://pandas.pydata.org/)).
    Pandas is an open source library and has great functionality and tools for data
    analysis as well as very easy-to-use data structures.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily get pandas in many different ways. The best way to get pandas
    is to install it via `conda` ([http://pandas.pydata.org/pandas-docs/stable/install.html#installing-pandas-with-anaconda](http://pandas.pydata.org/pandas-docs/stable/install.html#installing-pandas-with-anaconda)).
  prefs: []
  type: TYPE_NORMAL
- en: “conda is an open source package management system and environment management
    system for installing multiple versions of software packages and their dependencies
    and switching easily between them. It works on Linux, OS X and Windows, and was
    created for Python programs but can package and distribute any software.” – conda
    website.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily get conda by installing Anaconda, which is an open data science
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s have a look and see how to use pandas in order to read advertising
    data samples. First off, we need to import `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we can use the `pandas.read_csv` method in order to load our data
    into an easy-to-use pandas data structure called **DataFrame**. For more information
    about `pandas.read_csv` and its parameters, you can refer to the pandas documentation
    for this method ([https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first argument passed to the `pandas.read_csv` method is a string value
    representing the file path. The string can be a URL that includes `http`, `ftp`,
    `s3`, and `file`. The second argument passed is the index of the column that will
    be used as a label/name for the data rows.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have the data DataFrame, which contains the advertising data provided
    in the URL and each row is labeled by the first column. As mentioned earlier,
    pandas provides easy-to-use data structures that you can use as containers for
    your data. These data structures have some methods associated with them and you
    will be using these methods to transform and/or operate on your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s have a look at the first five rows of the advertising data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **TV** | **Radio** | **Newspaper** | **Sales** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 230.1 | 37.8 | 69.2 | 22.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 44.5 | 39.3 | 45.1 | 10.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17.2 | 45.9 | 69.3 | 9.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 151.5 | 41.3 | 58.5 | 18.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 180.8 | 10.8 | 58.4 | 12.9 |'
  prefs: []
  type: TYPE_TB
- en: Understanding the advertising data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This problem falls into the supervised learning type, in which we have explanatory
    features (input variables) and the response (output variable).
  prefs: []
  type: TYPE_NORMAL
- en: What are the features/input variables?
  prefs: []
  type: TYPE_NORMAL
- en: '**TV**: Advertising dollars spent on TV for a single product in a given market
    (in thousands of dollars)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Radio**: Advertising dollars spent on radio'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Newspaper**: Advertising dollars spent on newspapers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the response/outcome/output variable?
  prefs: []
  type: TYPE_NORMAL
- en: '**Sales**: The sales of a single product in a given market (in thousands of
    widgets)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also use the `DataFrame` method shape to know the number of samples/observations
    in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So, there are 200 observations in the advertising data.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to understand the underlying form of the data, the relationship between
    the features and response, and more insights, we can use different types of visualization.
    To understand the relationship between the advertising data features and response,
    we are going to use a scatterplot.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make different types of visualizations of your data, you can use
    Matplotlib ([https://matplotlib.org/](https://matplotlib.org/)), which is a Python
    2D library for making visualizations. To get Matplotlib, you can follow their
    installation instructions at: [https://matplotlib.org/users/installing.html](https://matplotlib.org/users/installing.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the visualization library Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use a scatterplot to visualize the relationship between the advertising
    data features and response variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45e7442f-7b9e-474c-836e-04228777d540.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Scatter plot for understanding the relationship between the advertising
    data features and the response variable'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to see how the ads will help increase the sales. So, we need to
    ask ourselves a couple of questions about that. Worthwhile questions to ask will
    be something like the relationship between the ads and sales, which kind of ads
    contribute more to the sales, and the approximate effect of each type of ad on
    the sales. We will try to answer such questions using a simple linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Simple regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The linear regression model is a learning algorithm that is concerned with predicting
    a **quantitative **(also known as **numerical**) **response** using a combination
    of **explanatory** **features** (or **inputs** or **predictors**).
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple linear regression model with only one feature takes the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = beta[0] + beta[1]x*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* is the predicted numerical value (response) → **sales**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the the feature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beta[0]* is called the **intercept**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beta[1]* is the coefficient of the feature *x* → **TV ad**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both *beta[0]* and *beta[1]* are considered as model **coefficients**. In order
    to create a model that can predict the value of sales in the advertising example,
    we need to learn these coefficients because *beta[1]* will be the learned effect
    of the feature *x* on the response *y*. For example, if *beta[1] = 0.04*, it means
    that an additional $100 spent on TV ads is **associated with** an increase in
    sales by four widgets. So, we need to go ahead and see how can we learn these
    coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Learning model coefficients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to estimate the coefficients of our model, we need to fit the data
    with a regression line that gives a similar answer to the actual sales. To get
    a regression line that best fits the data, we will use a criterion called **least
    squares**. So, we need to find a line that minimizes the difference between the
    predicted value and the observed(actual) one. In other words, we need to find
    a regression line that minimizes the **sum of squared residuals** (**SSresiduals**).
    *Figure 2* illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d507cac-b66c-4975-9771-a6e6f64f3732.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Fitting the data points (sample of TV ads) with a regression line
    that minimizes the sum of the squared residuals (difference between the predicted
    and observed value)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the elements that exist in *Figure 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Black dots** represent the actual or observed values of *x* (TV ad) and *y*
    (sales)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The blue line** represents the least squares line (regression line)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The red line** represents the residuals, which are the differences between
    the predicted and the observed (actual) values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, this is how our coefficients relate to the least squares line (regression
    line):'
  prefs: []
  type: TYPE_NORMAL
- en: '*beta[0]* is the **intercept**, which is the value of *y* when *x =0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beta[1]* is the **slope**, which represents the change in *y* divided by the
    change in *x*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 3* presents a graphical explanation of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2a18228-02ac-44bb-8be4-2a2c3b844def.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The relation between the least squares line and the model coefficients'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go ahead and start to learn these coefficients using **Statsmodels**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned, one of the advantages of linear regression models is that they
    are easy to interpret, so let's go ahead and interpret the model.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting model coefficients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to interpret the coefficients of the model, such as the TV ad
    coefficient (*beta[1]*):'
  prefs: []
  type: TYPE_NORMAL
- en: A unit increase in the input/feature (TV ad) spending is **associated** with
    a `0.047537` unit increase in Sales (response). In other words, an additional
    $100 spent on TV ads is **associated with** an increase in sales of 4.7537 widgets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of building a learned model from the TV ad data is to predict the sales
    for unseen data. So, let's see how we can use the learned model in order to predict
    the value of sales (which we don't know) based on a given value of a TV ad.
  prefs: []
  type: TYPE_NORMAL
- en: Using the model for prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say we have unseen data of TV ad spending and that we want to know their
    corresponding impact on the sales of the company. So, we need to use the learned
    model to do that for us. Let's suppose that we want to know how much sales will
    increase from $50000 of TV advertising.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use our learned model coefficients to make such a calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = 7.032594 + 0.047537 x 50*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use Statsmodels to make the prediction for us. First, we need to
    provide the TV ad value in a pandas DataFrame since the Statsmodels interface
    expects it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **TV** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 50000 |'
  prefs: []
  type: TYPE_TB
- en: 'Now, we can go ahead and use the predict function to predict the sales value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how the learned least squares line looks. In order to draw the line,
    we need two points, with each point represented by this pair: (`x, predict_value_of_x`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s take the minimum and maximum values for the TV ad feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **TV** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 296.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s get the corresponding predictions for these two values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s plot the actual data and then fit it with the least squares line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2a7c8e3-54d6-4cf8-9b81-ec0167a4cd4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Plot of the actual data and the least squares line'
  prefs: []
  type: TYPE_NORMAL
- en: Extensions of this example and further explanations will be explained in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to go through logistic regression, which is one
    of the widely used algorithms for classification.
  prefs: []
  type: TYPE_NORMAL
- en: What's logistic regression? The simple definition of logistic regression is
    that it's a type of classification algorithm involving a linear discriminant.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to clarify this definition in two points:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike linear regression, logistic regression doesn't try to estimate/predict
    the value of the numeric variable given a set of features or input variables.
    Instead, the output of the logistic regression algorithm is the probability that
    the given sample/observation belongs to a specific class. In simpler words, let's
    assume that we have a binary classification problem. In this type of problem,
    we have only two classes in the output variable, for example, diseased or not
    diseased. So, the probability that a certain sample belongs to the diseased class
    is *P[0]* and the probability that a certain sample belongs to the not diseased
    class is *P[1] = 1 - P[0]*. Thus, the output of the logistic regression algorithm
    is always between 0 and 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As you probably know, there are a lot of learning algorithms for regression
    or classification, and each learning algorithm has its own assumption about the
    data samples. The ability to choose the learning algorithm that fits your data
    will come gradually with practice and good understanding of the subject. Thus,
    the central assumption of the logistic regression algorithm is that our input/feature
    space could be separated into two regions (one for each class) by a linear surface,
    which could be a line if we only have two features or a plane if we have three,
    and so on. The position and orientation of this boundary will be determined by
    your data. If your data satisfies this constraint that is separating them into
    regions corresponding to each class with a linear surface, then your data is said
    to be linearly separable. The following figure illustrates this assumption. In
    *Figure 5*, we have three dimensions, inputs, or features and two possible classes:
    diseased (red) and not diseased (blue). The dividing place that separates the
    two regions from each other is called a **linear discriminant**, and that’s because
    it''s linear and it helps the model to discriminate between samples belonging
    to different classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/16e03c8c-c3e2-4526-851c-0ead591812ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Linear decision surface separating two classes'
  prefs: []
  type: TYPE_NORMAL
- en: If your data samples aren't linearly separable, you can make them so by transforming
    your data into higher dimensional space, by adding more features.
  prefs: []
  type: TYPE_NORMAL
- en: Classification and logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned how to predict continuous quantities (for
    example, the impact of TV advertising on company sales) as linear functions of
    input values (for example, TV, Radio, and newspaper advertisements). But for other
    tasks, the output will not be continuous quantities. For example, predicting whether
    someone is diseased or not is a classification problem and we need a different
    learning algorithm to perform this. In this section, we are going to dig deeper
    into the mathematical analysis of logistic regression, which is a learning algorithm
    for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In linear regression, we tried to predict the value of the output variable *y^((i))* for
    the *i^(th)* sample *x^((i))* in that dataset using a linear model function *y
    = h[θ](x)=θ^Τ x*. This is not really a great solution for classification tasks
    such as predicting binary labels *(y^((i)) ∈ {0,1})*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression is one of the many learning algorithms that we can use
    for classification tasks, whereby we use a different hypothesis class while trying
    to predict the probability that a specific sample belongs to the one class and
    the probability that it belongs to the zero class. So, in logistic regression,
    we will try to learn the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/621ad68c-251e-4148-ab09-8075642e6eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: The function ![](img/aae68862-cda0-4cec-abca-621bc46268f8.png) is often called
    a **sigmoid** or **logistic** function, which squashes the value of *θ^Τx* into
    a fixed range *[0,1]*, as shown in the following graph. Because the value will
    be squashed between [0,1], we can then interpret *h[θ](x)* as a probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to search for a value of the parameters *θ* so that the probability
    *P(y = 1|x) = h[θ](x))* is large when the input sample *x* belongs to the one
    class and small when *x* belongs to the zero class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e319d30b-e5e6-4b80-a840-02107358b9ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Shape of the sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, suppose we have a set of training samples with their corresponding binary
    labels *{(x^((i)),y^((i))): i = 1,...,m}.* We will need to minimize the following
    cost function, which measures how good a given *h[θ]* does:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57740d69-403e-45af-b411-841990cbbb5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we have only one of the two terms of the equation's summation as non-zero
    for each training sample (depending on whether the value of the label *y^((i))* is
    *0* or ). When *y^((i)) = 1*, minimizing the model cost function means we need
    to make *h[θ](x^((i)))* large, and when *y^((i)) = 0![](img/7851cec8-7d58-4c09-887f-6177ca8b48dc.png)*,
    we want to make *1-h[θ] large*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a cost function that calculates how well a given hypothesis *h[θ]* fits
    our training samples. We can learn to classify our training samples by using an
    optimization technique to minimize *J(θ)* and find the best choice of parameters *θ*.
    Once we have done this, we can use these parameters to classify a new test sample
    as 1 or 0, checking which of these two class labels is most probable. If *P(y
    = 1|x) < P(y = 0|x)* then we output 0, otherwise we output 1, which is the same
    as defining a threshold of 0.5 between our classes and checking whether *h[θ](x)
    > 0.5*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To minimize the cost function *J(θ),* we can use an optimization technique
    that finds the best value of *θ* that minimizes the cost function. So, we can
    use a calculus tool called **gradient**, which tries to find the greatest rate
    of increase of the cost function. Then, we can take the opposite direction to
    find the minimum value of this function; for example, the gradient of *J(θ)* is
    denoted by *∇[θ]J(θ),* which means taking the gradient for the cost function with
    respect to the model parameters. Thus, we need to provide a function that computes
    *J(θ)* and *∇[θ]J(θ)* for any requested choice of *θ*. If we derived the gradient
    or derivative of the cost function above *J(θ)* with respect to *θ[j]*, we will
    get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bac9e1fa-8c45-49fa-81e4-0335d4c5c858.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Which can be written in a vector form as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1fd5510-fc07-4c56-985b-b3a942947b8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have a mathematical understanding of the logistic regression, so let's
    go ahead and use this new learning method for solving a classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Titanic example – model building and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sinking of the ship, Titanic, is one of the most infamous events in history.
    This incident led to the deaths of 1,502 passengers and crew out of 2,224\. In
    this problem, we will use data science to predict whether the passenger will survive
    this tragedy or not and then test the performance of our model based on the actual
    statistics of the tragedy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow up with the Titanic example, you need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download this repository in a ZIP file by clicking on [https://github.com/ahmed-menshawy/ML_Titanic/archive/master.zip](https://github.com/ahmed-menshawy/ML_Titanic/archive/master.zip)
    or execute from the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Git clone: [https://github.com/ahmed-menshawy/ML_Titanic.git](https://github.com/ahmed-menshawy/ML_Titanic.git)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install `[virtualenv]`: ([http://virtualenv.readthedocs.org/en/latest/installation.html](http://virtualenv.readthedocs.org/en/latest/installation.html))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the directory where you unzipped or cloned the repo and create a
    virtual environment with `virtualenv ml_titanic`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Activate the environment with `source ml_titanic/bin/activate`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the required dependencies with `pip install -r requirements.txt`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the `ipython notebook` from the command line or terminal
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the example code in the chapter
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you're done, deactivate the virtual environment with `deactivate`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data handling and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to do some data preprocessing and analysis. Data
    exploration and analysis is considered one of the most important steps while applying
    machine learning and might also be considered as the most important one, because
    at this step, you get to know the friend, Data, which is going to stick with you
    during the training process. Also, knowing your data will enable you to narrow
    down the set of candidate algorithms that you might use to check which one is
    the best for your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start off by importing the necessary packages for our implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s read the Titanic passengers and crew data using Pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, let''s check the dimensions of our dataset and see how many examples
    we have and how many explanatory features are describing our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have a total of 891 observations, data samples, or passenger/crew records,
    and 12 explanatory features for describing this record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the data of some samples/observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/185f0d5b-b29b-409a-b5a8-1f815ed60bc1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Samples from the titanic dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a Pandas DataFrame that holds the information of 891 passengers
    that we need to analyze. The columns of the DataFrame represent the explanatory
    features about each passenger/crew, like name, sex, or age.
  prefs: []
  type: TYPE_NORMAL
- en: Some of these explanatory features are complete without any missing values,
    such as the survived feature, which has 891 entries. Other explanatory features
    contain missing values, such as the age feature, which has only 714 entries. Any
    missing value in the DataFrame is represented as NaN.
  prefs: []
  type: TYPE_NORMAL
- en: If you explore all of the dataset features, you will find that the ticket and
    cabin features have many missing values (NaNs), and so they won't add much value
    to our analysis. To handle this, we will drop them from the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following line of code to drop the `ticket` and `cabin` features entirely
    from the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: There are a lot of reasons to have such missing values in our dataset. But in
    order to preserve the integrity of the dataset, we need to handle such missing
    values. In this specific problem, we will choose to drop them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following line of code in order to remove all `NaN` values from all
    the remaining features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a sort of compete dataset that we can use to do our analysis. If
    you decided to just delete all the NaNs without deleting the **ticket** and **cabin**
    features first, you will find that most of the dataset is removed, because the `.dropna()`
    method removes an observation from the DataFrame, even if it has only one NaN
    in one of the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do some data visualization to see the distribution of some features and
    understand the relationship between the explanatory features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cdec217d-50f1-41fc-b1ef-bb9b49c18ead.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Basic visualizations for the Titanic data samples'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, the purpose of this analysis is to predict if a specific passenger
    will survive the tragedy based on the available feature, such as traveling class
    (called `pclass` in the data), **Sex**, **Age**, and **Fare Price**. So, let's
    see if we can get a better visual understanding of the passengers who survived
    and died.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s draw a bar plot to see the number of observations in each class
    (survived/died):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2f98e69d-7965-40fa-b36b-c53188be57d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Survival breakdown'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get some more understanding of the data by breaking down the previous
    graph by gender:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c97d7af6-5437-4acf-ac37-66f45ed2462e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Further breakdown for the Titanic data by the gender feature'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have more information about the two possible classes (survived and died).
    The exploration and visualization step is necessary because it gives you more
    insight into the structure of the data and helps you to choose the suitable learning
    algorithm for your problem. As you can see, we started with very basic plots and
    then increased the complexity of the plot to discover more about the data that
    we were working with.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis – supervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of this analysis is to predict the survivors. So, the outcome will
    be survived or not, which is a binary classification problem; in it, you have
    only two possible classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are lots of learning algorithms that we can use for binary classification
    problems. Logistic regression is one of them. As explained by Wikipedia:'
  prefs: []
  type: TYPE_NORMAL
- en: '*In statistics, logistic regression or logit regression is a type of regression
    analysis used for predicting the outcome of a categorical dependent variable (a
    dependent variable that can take on a limited number of values, whose magnitudes
    are not meaningful but whose ordering of magnitudes may or may not be meaningful)
    based on one or more predictor variables. That is, it is used in estimating empirical
    values of the parameters in a qualitative response model. The probabilities describing
    the possible outcomes of a single trial are modeled, as a function of the explanatory
    (predictor) variables, using a logistic function. Frequently (and subsequently
    in this article) "logistic regression" is used to refer specifically to the problem
    in which the dependent variable is binary—that is, the number of available categories
    is two—and problems with more than two categories are referred to as multinomial
    logistic regression or, if the multiple categories are ordered, as ordered logistic
    regression. Logistic regression measures the relationship between a categorical
    dependent variable and one or more independent variables, which are usually (but
    not necessarily) continuous, by using probability scores as the predicted values
    of the dependent variable.[1] As such it treats the same set of problems as does
    probit regression using similar techniques.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use logistic regression, we need to create a formula that tells
    our model the type of features/inputs we''re giving it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a2254f5c-9ef2-4f13-8658-962474fc88c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Logistic regression results'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s plot the prediction of our model versus actual ones and also the
    residuals, which is the difference between the actual and predicted values of
    the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c020d1bf-3a74-496d-9d74-4b581068d655.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Understanding the logit regression model'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have built our logistic regression model, and prior to that, we have
    done some analysis and exploration of the dataset. The preceding example shows
    you the general pipelines for building a machine learning solution.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, practitioners fall into some technical pitfalls because they
    lack experience of understanding the concepts of machine learning. For example,
    someone might get an accuracy of 99% over the test set, and then without doing
    any investigation of the distribution of classes in the data (such as how many
    samples are negative and how many samples are positive), they deploy the model.
  prefs: []
  type: TYPE_NORMAL
- en: To highlight some of these concepts and differentiate between different kinds
    of errors that you need to be aware of and which ones you should really care about,
    we'll move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, there are two types of errors, and as a newcomer to data
    science, you need to understand the crucial difference between both of them. If
    you end up minimizing the wrong type of error, the whole learning system will
    be useless and you won’t be able to use it in practice over unseen data. To minimize
    this kind of misunderstanding between practitioners about these two types of errors,
    we are going to explain them in the following two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Apparent (training set) error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This the first type of error that you don't have to care about minimizing. Getting
    a small value for this type of error doesn't mean that your model will work well
    over the unseen data (generalize). To better understand this type of error, we'll
    give a trivial example of a class scenario. The purpose of solving problems in
    the classroom is not to be able to solve the same problem again in the exam, but
    to be able to solve other problems that won’t necessarily be similar to the ones
    you practiced in the classroom. The exam problems could be from the same family
    of the classroom problems, but not necessarily identical.
  prefs: []
  type: TYPE_NORMAL
- en: Apparent error is the ability of the trained model to perform on the training
    set for which we already know the true outcome/output. If you manage to get 0
    error over the training set, then it is a good indicator for you that your model
    (mostly) won't work well on unseen data (won't generalize). On the other hand,
    data science is about using a training set as a base knowledge for the learning
    algorithm to work well on future unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 3*, the red curve represents the **apparent** error. Whenever you
    increase the model''s ability to memorize things (such as increasing the model
    complexity by increasing the number of explanatory features), you will find that
    this apparent error approaches zero. It can be shown that if you have as many
    features as observations/samples, then the **apparent** error will be zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04bc2373-a824-45fc-b4d1-7beb0ab33bfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Apparent error (red curve) and generalization/true error (light
    blue)'
  prefs: []
  type: TYPE_NORMAL
- en: Generalization/true error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the second and more important type of error in data science. The whole
    purpose of building learning systems is the ability to get a smaller generalization
    error on the test set; in other words, to get the model to work well on a set
    of observation/samples that haven't been used in the training phase. If you still
    consider the class scenario from the previous section, you can think of generalization
    error as the ability to solve exam problems that weren’t necessarily similar to
    the problems you solved in the classroom to learn and get familiar with the subject.
    So, generalization performance is the model's ability to use the skills (parameters)
    that it learned in the training phase in order to correctly predict the outcome/output
    of unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 13*, the light blue line represents the generalization error. You
    can see that as you increase the model complexity, the generalization error will
    be reduced, until some point when the model will start to lose its increasing
    power and the generalization error will decrease. This part of the curve where
    you get the generalization error to lose its increasing generalization power,
    is called **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: The takeaway message from this section is to minimize the generalization error
    as much as you can.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A linear model is a very powerful tool that you can use as an initial learning
    algorithm if your data matches its assumptions. Understanding linear models will
    help you to understand more sophisticated models that use linear models as building
    blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we will continue using the Titanic example by addressing model complexity
    and assessment in more detail. Model complexity is a very powerful tool and you
    need to use it carefully in order to enhance the generalization error. Misunderstanding
    it will lead to overfitting problems.
  prefs: []
  type: TYPE_NORMAL
