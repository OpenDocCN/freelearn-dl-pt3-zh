["```\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import scale\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dense, Flatten, Dropout, Embedding, BatchNormalization\nfrom keras.layers.convolutional import Conv1D,MaxPooling1D\nfrom keras.layers import LSTM\nfrom keras.layers.merge import concatenate\nfrom keras.layers import GaussianNoise\nfrom pickle import load\nfrom keras import optimizers\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import precision_recall_fscore_support\nimport pandas_profiling\n```", "```\npandas_profiling.ProfileReport(df)\n```", "```\ntrainDF = pd.read_csv(\"trainingData.csv\",header = 0)\n```", "```\nprofile = pandas_profiling.ProfileReport(trainDF) \n```", "```\nrejected_variables = profile.get_rejected_variables(threshold=0.9)\n```", "```\nprofile.to_file(outputfile=\"Report.html\")\n```", "```\nfeatureDF = np.asarray(trainDF.iloc[:,0:520]) # First 520 features \nfeatureDF[featureDF == 100] = -110\nfeatureDF = (featureDF - featureDF.mean()) / featureDF.var()\n```", "```\nlabelDF = np.asarray(trainDF[\"BUILDINGID\"].map(str) + trainDF[\"FLOOR\"].map(str)) \nlabelDF = np.asarray(pd.get_dummies(labelDF))\n```", "```\ntrain_x = featureDF\ntrain_y = labelDF\nprint(train_x.shape)\nprint(train_x.shape[1])\n```", "```\ntestDF = pd.read_csv(\"validationData.csv\",header = 0)\ntest_featureDF = np.asarray(testDF.iloc[:,0:520])\ntest_featureDF[test_featureDF == 100] = -110\ntest_x = (test_featureDF - test_featureDF.mean()) / test_featureDF.var()\ntest_labelDF = np.asarray(testDF[\"BUILDINGID\"].map(str) + testDF[\"FLOOR\"].map(str))\ntest_y = np.asarray(pd.get_dummies(test_labelDF))\nprint(test_x.shape)\nprint(test_y.shape[1])\n```", "```\nnumber_epochs = 100\nbatch_size = 32\ninput_size = train_x.shape[1] # 520\nnum_classes = train_y.shape[1] # 13\n```", "```\ndef encoder():\n    model = Sequential()\n    model.add(Dense(256, input_dim=input_size, activation='relu', use_bias=True))\n    model.add(Dense(128, activation='relu', use_bias=True))\n    model.add(Dense(64, activation='relu', use_bias=True))    \n    return model\n```", "```\ndef decoder(encoder):   \n    encoder.add(Dense(128, input_dim=64, activation='relu', use_bias=True))\n    encoder.add(Dense(256, activation='relu', use_bias=True))\n    encoder.add(Dense(input_size, activation='relu', use_bias=True))\n    encoder.compile(optimizer='adam', loss='mse')\n    return encoder \n```", "```\nencoderModel = encoder() # Encoder\nauto_encoder = decoder(encoderModel) # The autoencoder \nauto_encoder.summary()\n```", "```\nauto_encoder.fit(train_x, train_x, epochs = 100, batch_size = batch_size, \n                 validation_split=0.1, verbose = 1)\n```", "```\nTrain on 17943 samples, validate on 1994 samples\nEpoch 1/100\n17943/17943 [==============================] - 5s 269us/step - loss: 0.0109 - val_loss: 0.0071\nEpoch 2/100\n17943/17943 [==============================] - 4s 204us/step - loss: 0.0085 - val_loss: 0.0066\nEpoch 3/100\n17943/17943 [==============================] - 3s 185us/step - loss: 0.0081 - val_loss: 0.0062\nEpoch 4/100\n17943/17943 [==============================] - 4s 200us/step - loss: 0.0077 - val_loss: 0.0062\nEpoch 98/100\n17943/17943 [==============================] - 6s 360us/step - loss: 0.0067 - val_loss: 0.0055\n.......\nEpoch 99/100\n17943/17943 [==============================] - 5s 271us/step - loss: 0.0067 - val_loss: 0.0055\nEpoch 100/100\n17943/17943 [==============================] - 7s 375us/step - loss: 0.0067 - val_loss: 0.0055\n```", "```\nX_train_re = encoderModel.predict(train_x)\nX_test_re = encoderModel.predict(test_x)\n```", "```\nfor layer in auto_encoder.layers[0:3]:\n    layer.trainable = True  \n```", "```\nfor i in range(number_of_layers_to_remove):\n    auto_encoder.pop()\n```", "```\nauto_encoder.add(Dense(128, input_dim=64, activation='relu', use_bias=True)) \nauto_encoder.add(BatchNormalization())                     \nauto_encoder.add(Dense(64, activation='relu', kernel_initializer = 'he_normal', use_bias=True)) \nauto_encoder.add(BatchNormalization())\nauto_encoder.add(Dropout(0.2))    \nauto_encoder.add(Dense(32, activation='relu', kernel_initializer = 'he_normal', use_bias=True))\nauto_encoder.add(GaussianNoise(0.1))\nauto_encoder.add(Dropout(0.1))  \nauto_encoder.add(Dense(num_classes, activation = 'softmax', use_bias=True))\n```", "```\nfull_model = autoEncoderClassifier(auto_encoder)\n```", "```\ndef autoEncoderClassifier(auto_encoder):\n    for layer in auto_encoder.layers[0:3]:\n        layer.trainable = True        \n\n    auto_encoder.add(Dense(128, input_dim=64, activation='relu', use_bias=True)) \n    auto_encoder.add(BatchNormalization())                     \n    auto_encoder.add(Dense(64, activation='relu', kernel_initializer = 'he_normal', use_bias=True)) \n    auto_encoder.add(BatchNormalization())\n    auto_encoder.add(Dropout(0.2))    \n    auto_encoder.add(Dense(32, activation='relu', kernel_initializer = 'he_normal', use_bias=True))\n    auto_encoder.add(GaussianNoise(0.1))\n    auto_encoder.add(Dropout(0.1))  \n    auto_encoder.add(Dense(num_classes, activation = 'softmax', use_bias=True))\n    return auto_encoder\n\nfull_model = autoEncoderClassifier(auto_encoder)\n```", "```\nfull_model.compile(loss = 'categorical_crossentropy', optimizer = optimizers.adam(lr = 0.001), metrics = ['accuracy'])\n```", "```\nhistory = full_model.fit(X_train_re, train_y, epochs = 50, batch_size = 200, validation_split = 0.2, verbose = 1)\n```", "```\nTrain on 15949 samples, validate on 3988 samples\nEpoch 1/50\n15949/15949 [==============================] - 10s 651us/step - loss: 0.9263 - acc: 0.7086 - val_loss: 1.4313 - val_acc: 0.5747\nEpoch 2/50\n15949/15949 [==============================] - 5s 289us/step - loss: 0.6103 - acc: 0.7749 - val_loss: 1.2776 - val_acc: 0.5619\nEpoch 3/50\n15949/15949 [==============================] - 5s 292us/step - loss: 0.5499 - acc: 0.7942 - val_loss: 1.3871 - val_acc: 0.5364\n.......\nEpoch 49/50\n15949/15949 [==============================] - 5s 342us/step - loss: 1.3861 - acc: 0.4662 - val_loss: 1.8799 - val_acc: 0.2706\nEpoch 50/50\n15949/15949 [==============================] - 5s 308us/step - loss: 1.3735 - acc: 0.4805 - val_loss: 2.1081 - val_acc: 0.2199\n```", "```\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['Training loss', 'Validation loss'], loc='upper left')\nplt.show()\n```", "```\nimport os\nfrom pickle import load\nfrom keras.models import load_model\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\nfrom keras.utils.vis_utils import plot_model\n\nplot_model(full_model, show_shapes=True, to_file='Localization.png')\n# save the model\nfull_model.save('model.h5')\n# load the model\nmodel = load_model('model.h5') \n```", "```\nresults = full_model.evaluate(X_test_re, test_y)\nprint('Test accuracy: ', results[1])\n```", "```\n1111/1111 [==============================] - 0s 142us/step\nTest accuracy: 0.8874887488748875\n```", "```\npredicted_classes = full_model.predict(test_x)\npred_y = np.argmax(np.round(predicted_classes),axis=1)\ny = np.argmax(np.round(test_y),axis=1)\np, r, f1, s = precision_recall_fscore_support(y, pred_y, average='weighted')\nprint(\"Precision: \" + str(p*100) + \"%\")\nprint(\"Recall: \" + str(r*100) + \"%\")\nprint(\"F1-score: \" + str(f1*100) + \"%\")\n```", "```\nPrecision: 90.29611866225324%\nRecall: 88.11881188118812%\nF1-score: 88.17976604784566%\n```", "```\nprint(classification_report(y, pred_y))\n```", "```\nprint(confusion_matrix(y, pred_y))\n```"]