["```\nbatch_size = 4096 # Data points in a single batch\nembedding_size = 128 # Dimension of the embedding vector.\nwindow_size=1 # We use a window size of 1 on either side of target word\nepochs = 5 # Number of epochs to train for\n# We pick a random validation set to sample nearest neighbors\nvalid_size = 16 # Random set of words to evaluate similarity on.\n# We sample valid datapoints randomly from a large window without always \n# being deterministic\nvalid_window = 250\n# When selecting valid examples, we select some of the most frequent words # as well as some moderately rare words as well\nnp.random.seed(54321)\nrandom.seed(54321)\nvalid_term_ids = np.array(random.sample(range(valid_window), valid_size))\nvalid_term_ids = np.append(\n    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_ \n    size),\n    axis=0\n) \n```", "```\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Embedding, Dot, Add\nfrom tensorflow.keras.models import Model\nK.clear_session() \n```", "```\n# Define two input layers for context and target words\nword_i = Input(shape=())\nword_j = Input(shape=()) \n```", "```\n# Each context and target has their own embeddings (weights and biases)\n# Embedding weights\nembeddings_i = Embedding(n_vocab, embedding_size, name='target_embedding')(word_i)\nembeddings_j = Embedding(n_vocab, embedding_size, name='context_embedding')(word_j)\n# Embedding biases\nb_i = Embedding(n_vocab, 1, name='target_embedding_bias')(word_i)\nb_j = Embedding(n_vocab, 1, name='context_embedding_bias')(word_j) \n```", "```\n# Compute the dot product between embedding vectors (i.e. w_i.w_j)\nij_dot = Dot(axes=-1)([embeddings_i,embeddings_j])\n# Add the biases (i.e. w_i.w_j + b_i + b_j )\npred = Add()([ij_dot, b_i, b_j]) \n```", "```\n# The final model\nglove_model = Model(\n    inputs=[word_i, word_j],outputs=pred,\nname='glove_model'\n) \n```", "```\n# Glove has a specific loss function with a sound mathematical\n# underpinning\n# It is a form of mean squared error\nglove_model.compile(loss=\"mse\", optimizer = 'adam') \n```", "```\ndef glove_data_generator(\n    sequences, window_size, batch_size, vocab_size, cooccurrence_matrix,\n    x_max=100.0, alpha=0.75, seed=None\n): \n```", "```\n # Shuffle the data so that, every epoch, the order of data is\n    # different\n    rand_sequence_ids = np.arange(len(sequences))\n    np.random.shuffle(rand_sequence_ids) \n```", "```\n sampling_table = \n    tf.keras.preprocessing.sequence.make_sampling_table(vocab_size) \n```", "```\n # For each story/article\n    for si in rand_sequence_ids:\n\n        # Generate positive skip-grams while using sub-sampling \n        positive_skip_grams, _ = tf.keras.preprocessing.sequence.\n        skipgrams(\n            sequences[si], \n            vocabulary_size=vocab_size, \n            window_size=window_size, \n            negative_samples=0.0, \n            shuffle=False,   \n            sampling_table=sampling_table,\n            seed=seed\n        ) \n```", "```\n # Take targets and context words separately\n        targets, context = zip(*positive_skip_grams)\n        targets, context = np.array(targets).ravel(),\n        np.array(context).ravel() \n```", "```\n x_ij = np.array(cooccurrence_matrix[targets, \n        context].toarray()).ravel() \n```", "```\n # Compute log - Introducing an additive shift to make sure we\n        # don't compute log(0)\n        log_x_ij = np.log(x_ij + 1)\n\n        # Sample weights \n        # if x < x_max => (x/x_max)**alpha / else => 1        \n        sample_weights = np.where(x_ij < x_max, (x_ij/x_max)**alpha, 1) \n```", "```\n # If seed is not provided generate a random one\n        if not seed:\n            seed = random.randint(0, 10e6)\n\n        # Shuffle data\n        np.random.seed(seed)\n        np.random.shuffle(context)\n        np.random.seed(seed)\n        np.random.shuffle(targets)\n        np.random.seed(seed)\n        np.random.shuffle(log_x_ij)\n        np.random.seed(seed)\n        np.random.shuffle(sample_weights) \n```", "```\n # Generate a batch or data in the format \n        # ((target words, context words), log(X_ij) <- true targets,\n        # f(X_ij) <- sample weights)\n        for eg_id_start in range(0, context.shape[0], batch_size):            \n            yield (\n                targets[eg_id_start: min(eg_id_start+batch_size, \n                targets.shape[0])], \n                context[eg_id_start: min(eg_id_start+batch_size, \n                context.shape[0])]\n            ), log_x_ij[eg_id_start: min(eg_id_start+batch_size, \n            log_x_ij.shape[0])], \\\n            sample_weights[eg_id_start: min(eg_id_start+batch_size, \n            sample_weights.shape[0])] \n```", "```\nglove_validation_callback = ValidationCallback(valid_term_ids, glove_model, tokenizer)\n# Train the model for several epochs\nfor ei in range(epochs):\n\n    print(\"Epoch: {}/{} started\".format(ei+1, epochs))\n\n    news_glove_data_gen = glove_data_generator(\n        news_sequences, window_size, batch_size, n_vocab\n    )\n\n    glove_model.fit(\n        news_glove_data_gen, epochs=1, \n        callbacks=glove_validation_callback,\n    ) \n```", "```\nelection: attorney, posters, forthcoming, november's, month's\nmonths: weeks, years, nations, rbs, thirds\nyou: afford, we, they, goodness, asked\nmusic: cameras, mp3, hp's, refuseniks, divide\nbest: supporting, category, asante, counterparts, actor\nmr: ron, tony, bernie, jack, 63\nleave: pay, need, unsubstantiated, suited, return\n5bn: 8bn, 2bn, 1bn, 3bn, 7bn\ndebut: solo, speakerboxxx, youngster, nasty, toshack\nimages: 117, pattern, recorder, lennon, unexpectedly\nchampions: premier, celtic, football, representatives, neighbour\nindividual: extra, attempt, average, improvement, survived\nbusinesses: medium, sell, redder, abusive, handedly\ndeutsche: central, austria's, donald, ecb, austria\nmachine: unforced, wireless, rapid, vehicle, workplace \n```", "```\ndef save_embeddings(model, tokenizer, vocab_size, save_dir):\n\n    os.makedirs(save_dir, exist_ok=True)\n\n    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()),\n    key=lambda x: x[0])[:vocab_size-1])\n\n    words_sorted = [None] + list(words_sorted)\n\n    context_embedding_weights = model.get_layer(\"context_embedding\").get_\n    weights()[0]\n    context_embedding_bias = model.get_layer(\"context_embedding_bias\").\n    get_weights()[0]\n    context_embedding = np.concatenate([context_embedding_weights,\n    context_embedding_bias], axis=1)\n\n    target_embedding_weights = model.get_layer(\"target_embedding\").get_\n    weights()[0]\n    target_embedding_bias = model.get_layer(\"target_embedding_bias\").get_\n    weights()[0]\n    target_embedding = np.concatenate([target_embedding_weights, target_\n    embedding_bias], axis=1)\n\n    pd.DataFrame(\n        context_embedding, \n        index = words_sorted\n    ).to_pickle(os.path.join(save_dir, \"context_embedding_and_bias.pkl\"))\n\n    pd.DataFrame(\n        target_embedding, \n        index = words_sorted\n    ).to_pickle(os.path.join(save_dir, \"target_embedding_and_bias.pkl\"))\n\nsave_embeddings(glove_model, tokenizer, n_vocab, save_dir='glove_embeddings') \n```", "```\n# Not allocating full GPU memory upfront\n%env TF_FORCE_GPU_ALLOW_GROWTH=true\n# Making sure we cache the models and are not downloaded all the time\n%env TFHUB_CACHE_DIR=./tfhub_modules \n```", "```\nimport tensorflow_hub as hub \n```", "```\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nK.clear_session() \n```", "```\nelmo_layer = hub.KerasLayer(\n    \"https://tfhub.dev/google/elmo/3\", \n    signature=\"tokens\",signature_outputs_as_dict=True\n) \n```", "```\n{\n    'tokens': [\n        ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n        ['the', 'mat', 'sat', '', '', '']\n    ], \n    'sequence_len': [6, 3]\n} \n```", "```\ndef format_text_for_elmo(texts, lower=True, split=\" \", max_len=None):\n\n    \"\"\" Formats a given text for the ELMo model (takes in a list of\n    strings) \"\"\"\n\n    token_inputs = [] # Maintains individual tokens\n    token_lengths = [] # Maintains the length of each sequence\n\n    max_len_inferred = 0 \n    # We keep a variable to maintain the max length of the input\n    # Go through each text (string)\n    for text in texts:    \n\n        # Process the text and get a list of tokens\n        tokens = tf.keras.preprocessing.text.text_to_word_sequence(text, \n        lower=lower, split=split)\n\n        # Add the tokens \n        token_inputs.append(tokens)                   \n\n        # Compute the max length for the collection of sequences\n        if len(tokens)>max_len_inferred:\n            max_len_inferred = len(tokens)\n\n    # It's important to make sure the maximum token length is only as\n    # large as the longest input in the sequence\n    # Here we make sure max_len is only as large as the longest input\n    if max_len and max_len_inferred < max_len:\n        max_len = max_len_inferred\n    if not max_len:\n        max_len = max_len_inferred\n\n    # Go through each token sequence and modify sequences to have same\n    # length\n    for i, token_seq in enumerate(token_inputs):\n\n        token_lengths.append(min(len(token_seq), max_len))\n\n        # If the maximum length is less than input length, truncate\n        if max_len < len(token_seq):\n            token_seq = token_seq[:max_len]            \n        # If the maximum length is greater than or equal to input length,\n        # add padding as needed\n        else:            \n            token_seq = token_seq+[\"\"]*(max_len-len(token_seq))\n\n        assert len(token_seq)==max_len\n\n        token_inputs[i] = token_seq\n\n    # Return the final output\n    return {\n        \"tokens\": tf.constant(token_inputs), \n        \"sequence_len\": tf.constant(token_lengths)\n    } \n```", "```\n #InvalidArgumentError:  Incompatible shapes: [2,6,1] vs. [2,10,1024]\n    #    [[node mul (defined at .../python3.6/site-packages/tensorflow_\n    hub/module_v2.py:106) ]] [Op:__inference_pruned_3391] \n```", "```\nprint(format_text_for_elmo([\"the cat sat on the mat\", \"the mat sat\"], max_len=10)) \n```", "```\n{'tokens': <tf.Tensor: shape=(2, 6), dtype=string, numpy=\narray([[b'the', b'cat', b'sat', b'on', b'the', b'mat'],\n       [b'the', b'mat', b'sat', b'', b'', b'']], dtype=object)>, 'sequence_len': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 3], dtype=int32)>} \n```", "```\n# Titles of 001.txt - 005.txt in bbc/business\nelmo_inputs = format_text_for_elmo([\n    \"Ad sales boost Time Warner profit\",\n    \"Dollar gains on Greenspan speech\",\n    \"Yukos unit buyer faces loan claim\",\n    \"High fuel prices hit BA's profits\",\n    \"Pernod takeover talk lifts Domecq\"\n]) \n```", "```\n# Get the result from ELMo\nelmo_result = elmo_layer(elmo_inputs) \n```", "```\n# Print the result\nfor k,v in elmo_result.items():    \n    print(\"Tensor under key={} is a {} shaped Tensor\".format(k, v.shape)) \n```", "```\nTensor under key=sequence_len is a (5,) shaped Tensor\nTensor under key=elmo is a (5, 6, 1024) shaped Tensor\nTensor under key=default is a (5, 1024) shaped Tensor\nTensor under key=lstm_outputs1 is a (5, 6, 1024) shaped Tensor\nTensor under key=lstm_outputs2 is a (5, 6, 1024) shaped Tensor\nTensor under key=word_emb is a (5, 6, 512) shaped Tensor \n```", "```\ndef read_data(data_dir):\n\n    # This will contain the full list of stories\n    news_stories = []    \n    filenames = []\n    print(\"Reading files\")\n\n    i = 0 # Just used for printing progress\n    for root, dirs, files in os.walk(data_dir):\n\n        for fi, f in enumerate(files):\n\n            # We don't read the readme file\n            if 'README' in f:\n                continue\n\n            # Printing progress\n            i += 1\n            print(\".\"*i, f, end='\\r')\n\n            # Open the file\n            with open(os.path.join(root, f), encoding='latin-1') as text_\n            file:\n                story = []\n                # Read all the lines\n                for row in text_file:\n                    story.append(row.strip())\n\n                # Create a single string with all the rows in the doc\n                story = ' '.join(story)                        \n                # Add that to the list\n                news_stories.append(story)  \n                filenames.append(os.path.join(root, f))\n\n        print('', end='\\r')\n\n    print(\"\\nDetected {} stories\".format(len(news_stories)))\n    return news_stories, filenames\nnews_stories, filenames = read_data(os.path.join('data', 'bbc')) \n```", "```\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nn_vocab = 15000 + 1\ntokenizer = Tokenizer(\n    num_words=n_vocab - 1,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_'{|}~\\t\\n',\n    lower=True, split=' ', oov_token=''\n)\ntokenizer.fit_on_texts(news_stories) \n```", "```\nlabels_ser = pd.Series(filenames, index=filenames) \n```", "```\nlabels_ser = labels_ser.str.split(os.path.sep, expand=True) \n```", "```\nlabels_ser = labels_ser.iloc[:, -2] \n```", "```\nlabels_ser = labels_ser.map({'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}) \n```", "```\ndata/bbc/tech/272.txt    4\ndata/bbc/tech/127.txt    4\ndata/bbc/tech/370.txt    4\ndata/bbc/tech/329.txt    4\ndata/bbc/tech/240.txt    4\nName: 2, dtype: int64 \n```", "```\nlabels_ser = pd.Series(filenames, index=filenames).str.split(os.path.sep, expand=True).iloc[:, -2].map(\n    {'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3,\n    'tech': 4}\n) \n```", "```\nfrom sklearn.model_selection import train_test_split\ntrain_labels, test_labels = train_test_split(labels_ser, test_size=0.33) \n```", "```\ndef generate_document_embeddings(texts, filenames, tokenizer, embeddings):\n\n    \"\"\" This function takes a sequence of tokens and compute the mean\n    embedding vector from the word vectors of all the tokens in the\n    document \"\"\"\n\n    doc_embedding_df = []\n    # Contains document embeddings for all the articles\n    assert isinstance(embeddings, pd.DataFrame), 'embeddings must be a \n    pd.DataFrame'\n\n    # This is a trick we use to quickly get the text preprocessed by the\n    # tokenizer\n    # We first convert text to a sequences, and then back to text, which\n    # will give the preprocessed tokens\n    sequences = tokenizer.texts_to_sequences(texts)    \n    preprocessed_texts = tokenizer.sequences_to_texts(sequences)\n\n    # For each text,\n    for text in preprocessed_texts:\n        # Make sure we had matches for tokens in the embedding matrx\n        assert embeddings.loc[text.split(' '), :].shape[0]>0\n        # Compute mean of all the embeddings associated with words\n        mean_embedding = embeddings.loc[text.split(' '), :].mean(axis=0)\n        # Add that to list\n        doc_embedding_df.append(mean_embedding)\n\n    # Save the doc embeddings in a dataframe\n    doc_embedding_df = pd.DataFrame(doc_embedding_df, index=filenames)\n\n    return doc_embedding_df \n```", "```\n# Load the skip-gram embeddings context and target\nskipgram_context_embeddings = pd.read_pickle(\n    os.path.join('../Ch03-Word-Vectors/skipgram_embeddings',\n    'context_embedding.pkl')\n)\nskipgram_target_embeddings = pd.read_pickle(\n    os.path.join('../Ch03-Word-Vectors/skipgram_embeddings',\n    'target_embedding.pkl')\n)\n# Compute the mean of context & target embeddings for better embeddings\nskipgram_embeddings = (skipgram_context_embeddings + skipgram_target_embeddings)/2\n# Generate the document embeddings with the average context target\n# embeddings\nskipgram_doc_embeddings = generate_document_embeddings(news_stories, filenames, tokenizer, skipgram_embeddings) \n```", "```\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\ndef get_classification_accuracy(doc_embeddings, train_labels, test_labels, n_trials):\n    \"\"\" Train a simple MLP model for several trials and measure test \n    accuracy\"\"\"\n\n    accuracies = [] # Store accuracies across trials\n\n    # For each trial\n    for trial in range(n_trials):\n        # Create a MLP classifier\n        lr_classifier = LogisticRegression(multi_class='multinomial', \n        max_iter=500)\n\n        # Fit the model on training data\n        lr_classifier.fit(doc_embeddings.loc[train_labels.index],\n        train_labels)\n\n        # Get the predictions for test data\n        predictions = lr_classifier.predict(doc_embeddings.loc[test_\n        labels.index])\n\n        # Compute accuracy\n        accuracies.append(accuracy_score(predictions, test_labels))\n\n    return accuracies\n# Get classification accuracy for skip-gram models\nskipgram_accuracies = get_classification_accuracy(\n    skipgram_doc_embeddings, train_labels, test_labels, n_trials=5\n)\nprint(\"Skip-gram accuracies: {}\".format(skipgram_accuracies)) \n```", "```\nSkip-gram accuracies: [0.882…, 0.882…, 0.881…, 0.882…, 0.884…] \n```"]