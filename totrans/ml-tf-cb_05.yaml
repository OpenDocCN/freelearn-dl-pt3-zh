- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosted Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we describe boosted trees: the **TensorFlow** (**TF**) approach
    to gradient boosting. It is a class of ML algorithms that produce a prediction
    model in the form of an ensemble of weak prediction models, typically decision
    trees. The model is constructed in a stage-wise fashion and generalized by utilizing
    an arbitrary (differentiable) loss function. Gradient boosted trees are an extremely
    popular class of algorithms, as they can be parallelized (at the tree construction
    stage), can natively handle missing values and outliers, and require minimal data
    preprocessing.'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we briefly demonstrate how to approach a binary classification
    problem using `BoostedTreesClassifier`. We will apply the technique to solve a
    realistic business problem using a popular educational dataset: predicting which
    customers are likely to cancel their bookings. The data for this problem – and
    several other business problems – comes in tabular format, and typically contains
    a mixture of different feature types: numeric, categorical, dates, and so on.
    In the absence of sophisticated domain knowledge, gradient boosting methods are
    a good first choice for creating an interpretable solution that works out of the
    box. In the next section, the relevant modeling steps will be demonstrated with
    code: data preparation, structuring into functions, fitting a model through the
    `tf.estimator` functionality, and interpretation of results.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by loading the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In principle, categorical variables could be simply recoded into integers (using
    a function such as `LabelEncoder` from scikit-learn) and a gradient boosting model
    would work just fine – these minimal requirements on data preprocessing are one
    of the reasons behind the popularity of ensembles of trees. However, in this recipe,
    we want to focus on demonstrating the interpretability of the model and therefore
    we want to analyze individual indicator values. For that reason, we create a function
    performing one-hot encoding in a TF-friendly format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in the introduction, for this recipe we will be using the hotel
    cancellations dataset available from the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.sciencedirect.com/science/article/pii/S2352340918315191](https://www.sciencedirect.com/science/article/pii/S2352340918315191)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We choose this dataset because it is fairly realistic for a typical business
    prediction problem a reader might encounter: there''s a time dimension present,
    and a mixture of numeric and categorical features. At the same time, it is fairly
    clean (no missing values), which means we can focus on the actual modeling and
    not on data wrangling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset has a time dimension, so a natural training/validation split can
    be made on `reservation_status_date`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Separate the features from the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We separate the columns into numerical and categorical ones and encode them
    in the TF-expected format. We skip some columns that could perhaps improve the
    model performance, but due to their nature they introduce a risk of leakage: introducing
    information that might improve the model performance in training but will fail
    when predicting on unseen data. In our situation, one such variable is `arrival_date_year`:
    if the model uses this variable very strongly, it will fail if we present it with
    a dataset further into the future (where a specific value of the variable will
    obviously be absent).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We remove some additional variables from our training data – this step can
    either be conducted based on expert judgment prior to the modeling procedure,
    or it can be automated. The latter approach would involve running a small model
    and examining the global feature importance: if the results show one very important
    feature dominating over others, it is a potential source of leakage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step required is creating the input functions for the boosted trees
    algorithm: we specify how data will be read into our model for both training and
    inference. We use the `from_tensor_slices` method in the `tf.data` API to read
    in data directly from pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now build the actual BoostedTrees model. We set up a minimal list of
    parameters (`max_depth` being one of the most important ones) – the ones not specified
    in the definition are left at their default values, which can be found through
    the help functions in the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have trained a model, we can evaluate the performance with respect
    to different metrics. `BoostedTreesClassifier` contains an `evaluate` method and
    the output covers a wide range of possible metrics; which ones are used for guidance
    depends on the specific application, but those outputted by default already allow
    us to evaluate the model from various angles (for example, if we are dealing with
    a highly imbalanced dataset, `auc` can be somewhat misleading and we should evaluate
    the loss as well). For a more detailed explanation, the reader is referred to
    documentation at [https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The results you see should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can evaluate the results at different levels of generality – details of
    the difference between global and local are given as follows. Let''s start with
    the **receiver operating characteristic** (**ROC**) curve: a graph showing the
    performance of a classification model at all possible classification thresholds.
    We plot the false positive rate versus the true positive rate: a random classifier
    would be a diagonal line from (0,0) to (1,1), and the further away we move from
    that scenario toward the upper-left corner, the better our classifier is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: ROC for the trained classifier'
  prefs: []
  type: TYPE_NORMAL
- en: 'Local interpretability refers to an understanding of a model''s predictions
    at the individual example level: we will create and visualize per-instance contributions.
    This is particularly useful if model predictions need to be explained to audiences
    exhibiting technical cognitive diversity. We refer to these values as **directional
    feature contributions** (**DFCs**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: The complete summary of the complete DFC DataFrame can be somewhat overwhelming
    at first glance, and in practice, one is most likely to focus on a subset of the
    columns. What we get in each row are summary statistics (`mean`, `std`, and so
    on) of the directional contributions of a feature (`arrival_date_week_number`
    in the first row, `arrival_date_day_of_month` for the second, and so on) across
    all observations in the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following code block demonstrates the steps necessary to extract the feature
    contributions to a prediction for a particular record. For convenience and reusability,
    we define a function plotting a chosen record first (for easier interpretation,
    we want to plot feature importances using different colors, depending on whether
    their contribution is positive or negative):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With the boilerplate code defined, we plot the detailed graph for a specific
    record in a straightforward manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Which delivers the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: How different features contribute to predicted probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides analyzing the feature relevance on the level of individual observation,
    we can also take a global (aggregate) view. Global interpretability refers to
    an understanding of the model as a whole: we will retrieve and visualize gain-based
    feature importances and permutation feature importances and also show aggregated
    DFCs.'
  prefs: []
  type: TYPE_NORMAL
- en: Gain-based feature importances measure the loss change when splitting on a particular
    feature, while permutation feature importances are computed by evaluating the
    model performance on the evaluation set by shuffling each feature one by one and
    attributing the change in model performance to the shuffled feature.
  prefs: []
  type: TYPE_NORMAL
- en: In general, permutation feature importance is preferred to gain-based feature
    importance, though both methods can be unreliable in situations where potential
    predictor variables vary in their scale of measurement or their number of categories
    and when features are correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function calculating permutation importances is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the following function to display the most relevant columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: The permutation feature importance of different features'
  prefs: []
  type: TYPE_NORMAL
- en: 'And we use the following function to display the gain feature importance columns
    in the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: The gain feature importance of different features'
  prefs: []
  type: TYPE_NORMAL
- en: 'The absolute values of DFCs can be averaged to understand the impact at a global
    level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: The mean directional feature contributions of different features'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we have introduced the TF implementation of `GradientBoostingClassifier`:
    a flexible model architecture applicable to a wide range of tabular data problems.
    We built a model to solve a real business problem: predicting the probability
    that a customer might cancel their hotel booking and, in the process, we introduced
    all the relevant components of the TF Boosted Trees pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the data for use with the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure the `GradientBoostingClassifier` with `tf.estimator`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the feature importance and model interpretability, both on a global
    and local level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a plethora of articles introducing the gradient boosting family of
    algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: An excellent Medium post at [https://medium.com/analytics-vidhya/introduction-to-the-gradient-boosting-algorithm-c25c653f826b](https://medium.com/analytics-vidhya/introduction-to-the-gradient-boosting-algorithm-c25c653f826b)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official XGBoost documentation: [https://xgboost.readthedocs.io/en/latest/tutorials/model.html](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The LightGBM documentation: [https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
