<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Learning on Microsoft Azure Using Python</h1>
                </header>
            
            <article>
                
<p>We are going to end our cloud API e<span>xploration</span> <span>journ</span><span>ey with this chapter</span><span>. So far, we have gently introduced ourselves to the wonderful world of APIs, specifically the APIs that let us carry out deep learning with ease. We have seen how to consume REST APIs and use them programmatically. Like <strong>Google Cloud Platform</strong> (<strong>GCP</strong>) and <strong>Amazon Web Services</strong> (<strong>AWS</strong>), Microsoft also offers its own cloud service platform, which is called Azure. As in previous chapters, we will only be focusing on the deep learning-based solutions that Azure has to offer. We will be shifting gears a bit and will also take a look at Microsoft's <strong>Cognitive Toolkit</strong> (<strong>CNTK</strong>), which is a deep learning framework like Keras.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li style="font-weight: 400">Setting up your account in Azure</li>
<li style="font-weight: 400">A quick walk-through of the deep learning solutions offered by Azure</li>
<li style="font-weight: 400">Using the Face API in Python</li>
<li style="font-weight: 400">Using the Text Analytics API in Python</li>
<li style="font-weight: 400">An introduction to CNTK</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can access the code for this chapter from <a href="https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter8">https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter8</a>.</p>
<p>To run the code used in this chapter, you'll need the following software:</p>
<ul>
<li>Python 3.6+</li>
<li>The Python PIL library</li>
<li>The Matplotlib library</li>
</ul>
<p>All other installations, such as CNTK and Django, will be described during the course of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up your account in Azure</h1>
                </header>
            
            <article>
                
<p>From your previous cloud platform usage experience, you may have realized that it all starts with setting up your account and billing in a cloud provider. This is a pretty standard workflow and Azure is no exception. So, let's head over to <a href="https://azure.microsoft.com/">https://azure.microsoft.com</a> and follow these steps:</p>
<ol>
<li>Click on the <span class="packt_screen">Start free</span> button, as shown:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1326 image-border" src="assets/01908bbd-6abc-4f1b-8b26-2d05cab0472d.png" style="width:39.58em;height:12.58em;"/></p>
<div class="packt_infobox">Note that you will need a Microsoft account to proceed with the following steps. So, if you do not have one, create one at <a href="https://account.microsoft.com/account">https://account.microsoft.com/account</a>.</div>
<ol start="2">
<li>You will be redirected to another page, where you will again see another <span class="packt_screen">Start free</span> button. Click on it.</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1327 image-border" src="assets/e67d8ec9-75a8-48a3-aa96-a76668954eb7.png" style="width:26.75em;height:9.58em;"/></p>
<ol start="3">
<li>You will be asked to log in to your Microsoft account to proceed. Give the credentials accordingly and you should land on a page as in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1328 image-border" src="assets/89e7d103-9c26-4a97-aaa8-78989fd0b407.png" style="width:57.33em;height:34.50em;"/></p>
<p style="padding-left: 60px">If you are a first-time user, you will get $200 of credit (depending on your currency) for free for 30 days to explore different services offered by Azure.</p>
<ol start="4">
<li>Fill in your details, which will also include verification of your identity by card.</li>
</ol>
<p style="padding-left: 60px">You might be charged a very nominal fee for this. Be sure to review the terms and conditions of the Azure free tier, which you will find at <a href="https://azure.microsoft.com/en-in/offers/ms-azr-0003p/">https://azure.microsoft.com/en-in/offers/ms-azr-0003p/</a>.</p>
<p>Once this process is complete, you are all set up and ready to move to your Azure portal (<a href="https://portal.azure.com/#home">https://portal.azure.com</a>), which acts in the same way as the GCP and AWS consoles that you have seen in previous chapters.</p>
<p>The Azure portal looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1329 image-border" src="assets/be9a0fa6-08a0-474a-8d62-b28113ea7fcf.png" style="width:59.42em;height:27.00em;"/></p>
<p>Now that you have set up your Azure account, let's explore the deep learning-based offerings of Azure in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A walk-through of the deep learning services provided by Azure</h1>
                </header>
            
            <article>
                
<p>Azure's deep learning- (and general machine learning-) based offerings are broadly divided into three parts:</p>
<ul>
<li><strong>The Azure Machine Learning service</strong> (<a href="https://azure.microsoft.com/en-in/services/machine-learning-service/">https://azure.microsoft.com/en-in/services/machine-learning-service/</a>), which provides an end-to-end machine learning life cycle, including model building, training, and deployment:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1330 image-border" src="assets/5e49fe53-bc9e-41e1-b544-97bdfb6d9e0a.png" style="width:30.33em;height:10.67em;"/></p>
<ul>
<li><strong>Machine Learning APIs</strong> (<a href="https://gallery.azure.ai/machineLearningAPIs">https://gallery.azure.ai/machineLearningAPIs</a>), which provide APIs for a wide range of learning tasks, such as content moderation, translation, anomaly detection, and so on:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1331 image-border" src="assets/3172ed87-5c07-4de6-bcb3-8a1c49c9ac02.png" style="width:42.25em;height:8.42em;"/></p>
<ul>
<li><strong>Azure AI</strong> (<a href="https://azure.microsoft.com/en-in/overview/ai-platform/">https://azure.microsoft.com/en-in/overview/ai-platform/</a>), which focuses on topics such as <strong>knowledge mining</strong>, <strong>decision mining</strong>, and many other similar machine learning capabilities in the domains of computer vision and language modeling:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1332 image-border" src="assets/a33df20e-bf48-44a2-b065-44b02e2bf715.png" style="width:40.17em;height:14.50em;"/></p>
<p>We will now study two APIs for a computer vision task and a natural language understanding task, respectively. We will also look at how to use these APIs from Python. Let's dive in.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object detection using the Face API and Python</h1>
                </header>
            
            <article>
                
<p>Object detection is a classic use case of computer vision and is widely applied to a number of real-world problems, such as video surveillance systems. In this section, we will be using the Face API to detect faces from a given image. This has direct use when designing video surveillance systems. You can learn more about the Face API from its official page at <a href="https://azure.microsoft.com/en-us/services/cognitive-services/face/">https://azure.microsoft.com/en-us/services/cognitive-services/face/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The initial setup</h1>
                </header>
            
            <article>
                
<p>Azure lets you try this API for free for a duration of 7 days, as well. But since you already have an Azure account (with free credit, I am assuming), we can do it another way, as shown:</p>
<ol>
<li>Sign in to your Azure account.</li>
<li style="font-weight: 400">Go to <a href="https://azure.microsoft.com/en-us/services/cognitive-services/face/">https://azure.microsoft.com/en-us/services/cognitive-services/face/</a>.</li>
<li style="font-weight: 400">Click on <span class="packt_screen">Already using Azure? Try this service for free now.</span></li>
</ol>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">You should now have a window as in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1333 image-border" src="assets/f1b88b0e-e960-4830-9440-3e4ffa4ab083.png" style="width:22.33em;height:21.83em;"/></p>
<ol start="4">
<li>Fill in the details accordingly and hit <span class="packt_screen">Create</span> once you are done. You will get a popup that reads <span class="packt_screen">Submitting deployment</span>.</li>
</ol>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">Once the deployment is completed, you should land on a page <span>as in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1334 image-border" src="assets/8d44a941-a39a-4c25-8b07-1457bef65c63.png" style="width:39.83em;height:19.58em;"/></p>
<ol start="5">
<li><span>C</span><span>lick on <span class="packt_screen">Go to resource</span> an</span><span>d you shou</span><span>ld be redirected to the resources page, which contains a bunch of</span> <span>details on it:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1335 image-border" src="assets/08af55b4-5b80-44b4-9943-282a9618ff8d.png" style="width:41.92em;height:20.83em;"/></p>
<p style="padding-left: 60px">Just scroll down a bit and you will be able to see the endpoint of the Face API. Note that it will vary depending on the configuration details you entered while creating the deployment. The endpoint looks like <a href="https://eastus.api.cognitive.microsoft.com/face/v1.0">https://eastus.api.cognitive.microsoft.com/face/v1.0</a>. Note this down.</p>
<p style="padding-left: 60px">Now, to be able to use the Face API programmatically, you need to create the respective API keys. On that same page, there is a section at the top that says <strong><span class="packt_screen">Grab your keys</span></strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1336 image-border" src="assets/15e48360-dd9c-48f2-b123-c6e1680e2514.png" style="width:55.25em;height:10.92em;"/></p>
<ol start="6">
<li>Under <span>that section, click <span class="packt_screen">Keys</span> and you will see something as in the following screenshot:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1337 image-border" src="assets/6166e10e-a211-4053-9236-f678e04822cc.png" style="width:52.17em;height:28.67em;"/></p>
<p>Now that you have the API keys for the Face API, you are ready to use it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consuming the Face API from Python code</h1>
                </header>
            
            <article>
                
<p>When your program includes security credentials such as API keys, it is often a good practice to define those keys as environmental variables and then call them in your program. So, go ahead and create an environment variable to store one of the API keys of the Face API.</p>
<div class="packt_infobox">To add an environment variable to your computer, you can follow this article at <a href="https://www.twilio.com/blog/2017/01/how-to-set-environment-variables.html">https://www.twilio.com/blog/2017/01/how-to-set-environment-variables.html</a>.</div>
<p>In my case, I have named the environment variable <kbd>face_api_key</kbd>. You can put any image that contains faces in it. For this example, I will be using this image:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1339 image-border" src="assets/5429ceba-947a-43c2-abbe-7e92043a3fdf.jpg" style="width:28.83em;height:21.67em;"/></p>
<p>Create a new Jupyter notebook and follow these steps:</p>
<ol>
<li>Let's now load up the environment variable using Python, as shown:</li>
</ol>
<pre style="padding-left: 60px"><strong>import os</strong><br/><strong>face_api_key = os.environ['face_api_key']</strong></pre>
<ol start="2">
<li>Now, assign your Face API endpoint (for object detection) to a variable.</li>
<li>Also, upload the image you want to test to an online file server, such as Imgur, and retrieve the URL that allows fetching the raw image from Imgur.</li>
</ol>
<p style="padding-left: 60px">In my case, I have uploaded the image to a GitHub repository and used the respective URL:</p>
<pre style="padding-left: 60px"><strong>face_api_url = 'https://eastus.api.cognitive.microsoft.com/face/v1.0/detect'</strong><br/><br/><strong>image_url= 'https://raw.githubusercontent.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/master/Chapter8/sample_image.jpg'</strong></pre>
<p style="padding-left: 60px">Note that in the preceding API, only the endpoint name at the end of the URL changes. In most cases, the part before the endpoint name will remain constant throughout your use of Cognitive Services, unless a change is required by the Azure platform itself.</p>
<ol start="4">
<li>Now, import the <kbd>requests</kbd> module and set up the API payload as shown:</li>
</ol>
<pre style="padding-left: 60px"><strong>import requests</strong><br/><strong>params = {</strong><br/><strong>'returnFaceId': 'true',</strong><br/><strong>'returnFaceLandmarks': 'false',</strong><br/><strong>'returnFaceAttributes': 'age,gender',</strong><br/><strong>}</strong></pre>
<ol start="5">
<li>Now, we are in a position to make a request to the Face API.</li>
</ol>
<p style="padding-left: 60px">The following lines of code do this for you:</p>
<pre style="padding-left: 60px"><strong># Define the header param</strong><br/><strong>headers = { 'Ocp-Apim-Subscription-Key': face_api_key }</strong><br/><strong># Define the body params</strong><br/><strong>params = {</strong><br/><strong>'returnFaceId': 'true',</strong><br/><strong>'returnFaceLandmarks': 'false',</strong><br/><strong>'returnFaceAttributes': 'age,gender',</strong><br/><strong>}</strong></pre>
<ol start="6">
<li>We can now display the response received from the API:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong># Make the call to the API</strong><br/><strong>response = requests.post(face_api_url, params=params, headers=headers, json={"url": image_url})</strong><br/><strong># Get the response and log</strong><br/><strong>faces = response.json()</strong><br/><strong>print('There are {} faces im the given image'.format(str(len(faces))))</strong></pre>
<p>In this case, the code returned is as follows:</p>
<pre><strong>There are 2 faces in the given image</strong></pre>
<p>Pay attention to the <kbd>returnFaceAttributes</kbd> <span>body parameter,</span> which lets you specify several attributes of faces and the Face API will analyze the given faces with respect to those attributes. To find out more about these attributes, check out the documentation at <a href="http://bit.ly/2J3j6nM">http://bit.ly/2J3j6nM</a>.</p>
<p>Let's embed the response we got from the API in the image in a presentable manner. We will show the probable gender and probable age of the detected faces in the image. We will do this using the <kbd>matplotlib</kbd>, <kbd>PIL</kbd>, and <kbd>io</kbd> libraries and we'll be using a Jupyter notebook to work on the following segments of code in this section. We will start by importing the libraries:</p>
<pre>%matplotlib inline #Only for Jupyter Notebook<br/>import matplotlib.pyplot as plt<br/>from PIL import Image<br/>from matplotlib import patches<br/>from io import BytesIO</pre>
<p>To display overlays on the image with the response received from the API, we use the following method:</p>
<ol>
<li class="mce-root">Store <span>the API response:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>response = requests.get(image_url)</strong></pre>
<ol start="2">
<li class="mce-root">Create <span>an image from the response content:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>image = Image.open(BytesIO(response.content))</strong></pre>
<ol start="3">
<li class="mce-root">Create <span>an empty figure:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>plt.figure(figsize=(8,8))</strong></pre>
<ol start="4">
<li class="mce-root">Show <span>the image created with the response:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>ax = plt.imshow(image, alpha=0.6)</strong></pre>
<ol start="5">
<li style="font-weight: 400">Iterate over the faces specified in the earlier section and extract the necessary information:</li>
</ol>
<pre style="padding-left: 60px"><span>for face in faces:<br/> # Extract the information<br/></span> fr = face["faceRectangle"]<br/><br/> fa = face["faceAttributes"]<br/> origin = (fr["left"], fr["top"])<br/> p = patches.Rectangle(origin, fr["width"], fr["height"], fill=False, <br/> linewidth=2, color='b')<br/> ax.axes.add_patch(p)<br/> plt.text(origin[0], origin[1], "%s, %d"%(fa["gender"].capitalize(), fa["age"]), <br/> fontsize=20, weight="bold", va="bottom")<br/># Turn off the axis<br/><span>_ = plt.axis("off")</span> <br/>plt.show()</pre>
<p>You should have an image like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1340 image-border" src="assets/6602861e-76f7-4378-afcc-db74b1161cf8.png" style="width:32.83em;height:24.92em;"/></p>
<p>You are encouraged to play around with the different parameters that the API provides. We will now study a <strong>Natural Language Understanding</strong> <span>(</span><strong><span>NLU</span></strong><span>)</span> <span>API.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting text information using the Text Analytics API and Python</h1>
                </header>
            
            <article>
                
<p>Whether knowingly or unknowingly, we <span>must</span> <span>all have encountered some of the astonishing use cases of natural language processing. Be it autocorrect, the next word suggestion, or language translation, these use cases are too important to neglect. In this section, we are going to use the Text Analytics API (</span><a href="https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/">https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/</a><span>) to extract meaningful information from a given piece of text.</span></p>
<p>You can try the API for free using the previously mentioned link and see its power. In the following example, I entered the phrase <kbd>I want to attend NeurIPS someday and present a paper there</kbd> and the Text Analytics API extracted four meaningful pieces of information from it:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1342 image-border" src="assets/e67aa8f4-cd9d-4d88-922a-4c94bd55d0b0.png" style="width:127.33em;height:45.58em;"/></p>
<p>Observe how gracefully the API was able to extract all the key pieces of information from the phrase.</p>
<p>We will now see how to do this programmatically using Python. The setup steps are going to be exactly the same as the preceding ones. Just go to <a href="https://portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics">https://portal.azure.com/#create/Microsoft.CognitiveServicesTextAnalytics</a> and follow the steps there. Once you have the respective API keys to consume the Text Analytics API, move on to the following subsection. Do not forget to note down the respective endpoint, as well. The endpoint should start with <a href="https://eastus.api.cognitive.microsoft.com/text/analytics/v2.0">https://eastus.api.cognitive.microsoft.com/text/analytics/v2.0</a>. This URL will not work alone; it needs to have a suffix pointing to the right method to be invoked.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Text Analytics API from Python code</h1>
                </header>
            
            <article>
                
<p>This section will show you how to use the Text Analytics API in your own Python code. The following are the steps for using it:</p>
<ol>
<li>We will begin this section by importing the libraries we need:</li>
</ol>
<pre style="padding-left: 60px"><strong>import requests</strong><br/><strong>import os</strong><br/><strong>from pprint import pprint</strong></pre>
<ol start="2">
<li>We will then load the API key for the Text Analytics API from the environment variable:</li>
</ol>
<pre style="padding-left: 60px"><strong>api_key = os.environ['text_api_key']</strong></pre>
<ol start="3">
<li>Let's now specify a few URLs to store the API endpoints:</li>
</ol>
<pre style="padding-left: 60px"><strong>text_analytics_base_url = \</strong><br/><strong>'https://eastus.api.cognitive.microsoft.com/text/analytics/v2.0'</strong><br/><strong>language_api_url = text_analytics_base_url + "/languages"</strong><br/><strong>sentiment_api_url = text_analytics_base_url + "/sentiment"</strong><br/><strong>key_phrase_api_url = text_analytics_base_url + "/keyPhrases"</strong></pre>
<ol start="4">
<li>Let's now define the <kbd>headers</kbd> parameter by supplying the API key:</li>
</ol>
<pre style="padding-left: 60px"><strong>headers = {"Ocp-Apim-Subscription-Key": api_key}</strong></pre>
<ol start="5">
<li>Let's also define the body parameter. In my case, I will keep the same phrase I showed earlier in the GUI-based demo:</li>
</ol>
<pre style="padding-left: 60px"><strong>documents = { 'documents': [</strong><br/><strong>{ 'id': '1', 'text': 'I want to attend NeurIPS someday and present a paper there.' }</strong><br/><strong>]}</strong></pre>
<ol start="6">
<li>We can now make calls to the respective APIs of Text Analytics. Let's start by detecting the language:</li>
</ol>
<pre style="padding-left: 60px"><strong>response = requests.post(language_api_url, headers=headers, json=documents)</strong><br/><strong>language = response.json()</strong><br/><strong>pprint(language)</strong></pre>
<p>We get the response accordingly, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1343 image-border" src="assets/16b55881-1edf-4f83-9814-5cd3c785a8da.png" style="width:25.00em;height:5.17em;"/></p>
<p>Note that I have highlighted the language. Now, let's move on to the sentiment analysis part:</p>
<pre><strong>response = requests.post(sentiment_api_url, headers=headers, json=documents)</strong><br/><strong>sentiment = response.json()</strong><br/><strong>pprint(sentiment)</strong></pre>
<p>The sentiment displayed is <span>as shown</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1344 image-border" src="assets/37049f4f-493f-4703-a41c-52f3a69aeb73.png" style="width:39.33em;height:1.67em;"/></p>
<p>Note that the phrase used here contains <span>neither</span> <span>a positive sentiment nor a negative sentiment, hence the score. We will now extract the key phrases from the given text:</span></p>
<pre><strong>response = requests.post(key_phrase_api_url, headers=headers, json=documents)</strong><br/><strong>phrases = response.json()</strong><br/><strong>print(phrases)</strong></pre>
<p>The key phrases are <span>as shown</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1345 image-border" src="assets/6e957a31-82f6-4670-8ad7-eaded45163f1.png" style="width:37.50em;height:2.25em;"/></p>
<p>Notice how the endpoints have changed with respect to the tasks. You can explore more about the different parameters of the endpoints we used in the preceding example at <a href="http://bit.ly/2JjLRfi">http://bit.ly/2JjLRfi</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An introduction to CNTK</h1>
                </header>
            
            <article>
                
<p>CNTK is an offering by Microsoft. The framework is a part of the ONNX format initiative, which allows easy conversion of models between different neural toolkit frameworks. The framework is responsible for a huge portion of the deep learning production workload on Microsoft software and platforms. Launched in 2016, the framework has been a contender to popular frameworks such as TensorFlow, PyTorch, and so on. The framework is completely open source and can be found at <a href="http://github.com/microsoft/CNTK">https://github.com/microsoft/CNTK</a>.</p>
<p>CNTK powers enterprise services, such as Cortana and Bing, and advertisements, such as Skype Translate, Microsoft Cognitive Services, and several others. It has been proven to work faster than competitors such as TensorFlow and PyTorch on several applications.</p>
<p class="CDPAlignLeft CDPAlign">In this section, we will study some fundamentals of CNTK and then proceed to create a Django application to carry over the CNTK-based model to the web.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with CNTK</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">CNTK is one of the easiest frameworks to get started with, thanks to its simple syntax and ability to work without the concept of sessions, as is the case in TensorFlow, which is confusing to most learners. Let's see how we can set up CNTK on our local machines or on Google Colaboratory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation on a local machine</h1>
                </header>
            
            <article>
                
<p>The CNTK framework supports both 64-bit and 32-bit architecture machines. However, it only supports Python versions up to version 3.6, at the time of writing this book. You can verify the latest supported versions at <a href="https://pypi.org/project/cntk/">https://pypi.org/project/cntk/</a>. Furthermore, CNTK is not available as a built binary on macOS, currently.</p>
<p>To install the framework, you can either use the <kbd>pip</kbd> package manager or install it using compiled binaries on Anaconda. Assuming a Python environment is set up, you can use the following commands to install CNTK on both Windows and Linux:</p>
<ul>
<li class="mce-root"><span>Without Anaconda, use the following for the CPU version:</span></li>
</ul>
<pre style="padding-left: 60px"><strong><span># For CPU version</span></strong><br/><strong><span>pip install cntk</span></strong></pre>
<ul>
<li>Use the following for the GPU-enabled version:</li>
</ul>
<pre style="padding-left: 60px"><strong><span># For the GPU enabled version</span></strong><br/><strong><span>pip install cntk-gpu</span></strong></pre>
<ul>
<li class="mce-root"><span>On Anaconda-enabled machines, the CNTK framework can be installed using <kbd>pip</kbd> with the following command:</span></li>
</ul>
<pre style="color: black;padding-left: 60px"><strong>pip install &lt;url&gt;</strong></pre>
<p class="mce-root"><span><kbd>&lt;url&gt;</kbd></span><span> </span><span>can be obtained from the CNTK website at</span> <a href="http://tiny.cc/cntk">http://tiny.cc/cntk</a><span>.</span></p>
<p class="mce-root"><span>The command will resemble the following:</span></p>
<pre style="color: black"><strong>pip install https://cntk.ai/PythonWheel/CPU-Only/cntk-2.6-cp35-cp35m-win_amd64.whl</strong></pre>
<p>We can now begin with its installation on Google Colaboratory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation on Google Colaboratory</h1>
                </header>
            
            <article>
                
<p>The CNTK framework is not available on the Google Colaboratory platform by default and so must be installed along with other requisite modules. To install CNTK on a Google Colaboratory runtime, use the following command at the top of the script:</p>
<pre><strong>!apt-get install --no-install-recommends openmpi-bin libopenmpi-dev libopencv-dev python3-opencv python-opencv &amp;&amp; ln -sf /usr/lib/x86_64-linux-gnu/libmpi_cxx.so /usr/lib/x86_64-linux-gnu/libmpi_cxx.so.1 &amp;&amp; ln -sf /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so.12 &amp;&amp; ln -sf /usr/lib/x86_64-linux-gnu/libmpi.so /usr/lib/x86_64-linux-gnu/libmpi.so.12 &amp;&amp; pip install cntk</strong></pre>
<div class="packt_infobox">Note that the preceding command is a single-line command. If you break it up into multiple lines, you should make sure you add the required changes to the command.</div>
<p><span><span>Once the preceding step</span></span> runs successfully, you will not need to use this command again in that runtime. So, the command can be commented out in future runs of the program.</p>
<p>It is conventional to import CNTK to Python projects by the <kbd>C</kbd> <span>alias</span><span>. We use the following code to import the library to the project:</span></p>
<pre>import cntk as C</pre>
<p>We can check the version of CNTK installed using the following line:</p>
<pre>print(C.__version__)</pre>
<p>With CNTK imported to the project, we're ready to proceed with the precursory requirements of creating a deep learning model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a CNTK neural network model</h1>
                </header>
            
            <article>
                
<p>In this section, we'll complete the steps <span>required</span> <span>before creating a predictive neural network and then we will create the neural network itself:</span></p>
<ol>
<li class="mce-root"><span>We begin by importing the necessary modules to the project:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>import numpy as np<br/>from sklearn.datasets import fetch_openml<br/>import random<br/><br/>import cntk.tests.test_utils<br/>from sklearn.preprocessing import OneHotEncoder<br/><br/>import cntk as C # if you have not done this before in the project</pre>
<p style="padding-left: 60px" class="mce-root"><span>The</span> <kbd>fetch_openml()</kbd> <span>method of the <kbd>sklearn</kbd> modules helps us directly download the dataset used in this example</span> <span>to the project</span><span>—the MNIST Handwritten Digits dataset. The</span> <kbd>OneHotEncoder</kbd> <span>method is used for the one-hot encoding of the labels.</span></p>
<ol start="2">
<li class="mce-root">Next<span>, the few constants that are required during the program execution are set up:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">num_samples = 60000<br/>batch_size = 64<br/>learning_rate = 0.1</pre>
<p style="padding-left: 60px" class="mce-root"><span>We will perform the training on 60,000 samples with an initial learning rate of <kbd>0.1</kbd>. This rate can be dynamically updated during the training.</span></p>
<ol start="3">
<li class="mce-root">We <span>then need to create a method for generating random mini-batches for the training:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">class Batch_Reader(object):<br/>    def __init__(self, data , label):<br/>        self.data = data<br/>        self.label = label<br/>        self.num_sample = data.shape[0]<br/><br/>    def next_batch(self, batch_size):<br/>        index = random.sample(range(self.num_sample), batch_size)<br/>        return self.data[index,:].astype(float),self.label[index,:].astype(float)</pre>
<p style="padding-left: 60px" class="mce-root"><span>The preceding method on each call generates batches equal to the size set in the previous step—for example, 64 samples in each batch. These samples are taken randomly from the dataset.</span></p>
<ol start="4">
<li class="mce-root">The <span>dataset now needs to be fetched; to do so, we use the following line of code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">mnist = fetch_openml('mnist_784')</pre>
<p style="padding-left: 60px" class="mce-root"><span>Once the data has been fetched, it can be separated into training and test datasets, as shown:</span></p>
<pre style="color: black;padding-left: 60px">train_data = mnist.data[:num_samples,:]<br/>train_label = mnist.target[:num_samples]<br/>test_data = mnist.data[num_samples:,:]<br/>test_label = mnist.target[num_samples:]</pre>
<ol start="5">
<li class="mce-root">Labels <span>in the datasets need to be one-hot encoded before being fed into the training model. To do so, we use the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">enc = OneHotEncoder()<br/>enc.fit(train_label[:,None])<br/>train_encoded = enc.transform(train_label[:,None]).toarray()</pre>
<ol start="6">
<li class="mce-root">We <span>can now create a generator object for the training batches generator, as shown:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">train_reader = Batch_Reader(train_data, train_encoded)</pre>
<ol start="7">
<li class="mce-root">Let'<span>s quickly carry out the preceding steps for the <kbd>test</kbd> dataset, too:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">enc = OneHotEncoder()<br/>enc.fit(test_label[:,None])<br/>test_encoded = enc.transform(test_label[:,None]).toarray()<br/><br/>test_reader = Batch_Reader(test_data, test_encoded)</pre>
<ol start="8">
<li class="mce-root">Now<span>, let's create a CNTK neural network model. We first begin by defining some constants:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">dimensions = 784<br/>classes = 10<br/>hidden_layers = 3<br/>hidden_layers_neurons = 400</pre>
<p class="mce-root"/>
<p style="padding-left: 60px" class="mce-root"><span>We define the dimensions of the input data as <kbd>784</kbd>. Recall our example from <a href="a33bccdc-8664-4ae7-be82-b066e5b64850.xhtml">Chapter 3</a>, <em>Creating Your First Deep Learning Web Application</em>, where we used the MNIST dataset. The images in the MNIST dataset are stored in the format of single-dimension arrays containing 28 x 28 values in the range of <kbd>0</kbd> to <kbd>255</kbd>. The images belong to 10 different classes, corresponding to each digit in the Arabic numeral system. We keep a provision of 3 hidden layers, each with 400 neurons in them.</span></p>
<ol start="9">
<li class="mce-root">We <span>then create two CNTK <kbd>input</kbd> variables to use while creating the model. This is one of the most important concepts of CNTK.</span></li>
</ol>
<pre style="color: black;padding-left: 60px">input = C.input_variable(dimensions)<br/>label = C.input_variable(classes)</pre>
<p style="padding-left: 60px">An <kbd>input</kbd> variable in CNTK is essentially a placeholder that we use to fill samples during model training and evaluation or testing. The shape of the input from the dataset must exactly match the dimensions declared in the <kbd>input</kbd> variables declaration in this step. It is important to mention here that a lot of people confuse the dimensions of input with the number of features a dataset has. A dataset that has <em>N</em> number of features and <em>M</em> number of samples has an (<em>M</em>, <em>N</em>) <span>shape</span> <span>and so the dimensions of this dataset is simply <kbd>2</kbd>:</span></p>
<pre style="padding-left: 60px">def create_model(features):<br/>    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):<br/>​<br/>            hidden_out = features<br/>​<br/>            for _ in range(hidden_layers):<br/>                hidden_out = C.layers.Dense(hidden_layers_neurons)(hidden_out)<br/>​<br/>            network_output = C.layers.Dense(classes, activation = None)(hidden_out)<br/>            return network_output</pre>
<ol start="10">
<li>We create the <kbd>create_model()</kbd> method, which takes the input of the features as the argument.</li>
</ol>
<p style="padding-left: 60px">First, the defaults are set for the model to use the uniform distribution of the initialization of weights and other values. The default activation function is set to <kbd>ReLU</kbd>.</p>
<p style="padding-left: 60px">The first layer contains the features themselves and the final layer contains a vector with a dimension equal to the number of classes. All the layers in between contain a completely connected network of 3 hidden layers with 400 neurons each and ReLU activation:</p>
<pre>model = create_model(input/255.0)</pre>
<p>Finally, we create the model using the previous function. Dividing by <kbd>255</kbd> provides normalization to the dataset, rendering the values in the image arrays between <kbd>0</kbd> and <kbd>1</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the CNTK model</h1>
                </header>
            
            <article>
                
<p>With the model created, we can now move on to training the model and making it learn to predict. To do so, we need to use the CNTK model object and fit the samples in the dataset to it. We can, at the same time, log <kbd>loss</kbd> and other evaluation metrics. We need to carry out the following steps to train our model:</p>
<ol start="1">
<li class="mce-root">Create <span>placeholders for <kbd>loss</kbd> and the classification error:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">loss = C.cross_entropy_with_softmax(model, label)<br/>label_error = C.classification_error(model, label)</pre>
<ol start="2">
<li class="mce-root">Now<span>, we can set up a <kbd>trainer</kbd> object for the CNTK framework, which is used to perform the actual training:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">lrs = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)<br/>learner = C.sgd(model.parameters, lrs)<br/>trainer = C.Trainer(model, (loss, label_error), [learner])</pre>
<ol start="3">
<li class="mce-root">Let'<span>s perform the training now:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">epochs = 10<br/>num_iters = (num_samples * epochs) / batch_size<br/><br/>for i in range(int(num_iters)):<br/><br/>    batch_data, batch_label = train_reader.next_batch(batch_size=batch_size)<br/><br/>    arguments = {input: batch_data, label: batch_label}<br/>    trainer.train_minibatch(arguments=arguments)<br/><br/>    if i % 1000 == 0:<br/>        training_loss = False<br/>        evalaluation_error = False<br/>        training_loss = trainer.previous_minibatch_loss_average<br/>        evalaluation_error = trainer.previous_minibatch_evaluation_average<br/>        print("{0}: , Loss: {1:.3f}, Error: {2:.2f}%".format(i, training_loss, evalaluation_error * 100))</pre>
<p class="mce-root"><span>We set the number of epochs for training as <kbd>10</kbd> to allow quick training and evaluations. You can set it to a higher value for more accuracy in training; however, this may lead to no better training or overfitting, in some cases. At every 1,000th iteration, we display the loss and evaluation error obtained up to then. The general trend for these should be toward decline.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing and saving the CNTK model</h1>
                </header>
            
            <article>
                
<p>Before continuing with turning this project into a web application using the Django framework, let's quickly test the accuracy obtained in this training of the model. We will carry out the following to make predictions from the model:</p>
<pre>predicted_label_probs = model.eval({input: test_data})</pre>
<p>This creates a NumPy array of probabilities for each label in the dataset. This has to be converted into indices and compared to the labels of the test data. We do this as shown:</p>
<pre>predictions = np.argmax(predicted_label_probs, axis=1)<br/>actual = np.argmax(test_encoded, axis=1)<br/>correct = np.sum(predictions == actual)<br/>print(correct / len(actual))</pre>
<p>We find around 98% accuracy in the prediction. This is a very good value and we will move on to saving the model and using it through Django. To save the CNTK model, we do the following:</p>
<pre>model.save("cntk.model")</pre>
<p>With the model saved successfully, you will have to download the <kbd>model</kbd> file to your local system if you've used Colaboratory to build the model. Next, we can move on to deploying the model on a Django-based server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief introduction to Django web development</h1>
                </header>
            
            <article>
                
<p>Django is one of the most popular frameworks for web development using Python. The framework is lightweight, robust, and actively maintained by the community, which quickly patches security holes and adds new features. In this book, w<span>e've</span> covered the Flask framework, which is essentially a bare-bones framework for Python web development. However, Django comes with a lot of built-in features that implement state-of-the-art methods and practices.</p>
<p>A Django project is initially structured in the following manner:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1346 image-border" src="assets/220c2b95-9ed4-4521-9ee1-9937f6e28fec.png" style="width:18.08em;height:23.08em;"/></p>
<p>These files are auto-generated when you create a new Django project using the <kbd>django-admin</kbd> tool. The top-level directory, <kbd>mysite</kbd>, represents the name of the Django project. Each Django project contains apps. Apps are similar to the concept of modules in software development. They are usually independent pieces of the complete project and are put together by the <kbd>mysite</kbd> master app within the project directory. Each project can have several apps inside it.</p>
<p>Let's learn how to get started with Django and create a new project!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with Django</h1>
                </header>
            
            <article>
                
<p>The foremost step before using Django is to install it. Fortunately, the framework is easily installable as a module from the Python PIP repository. It is also available on the Conda repository. To install Django, open a new terminal window and use the following command:</p>
<pre><strong>conda install django</strong></pre>
<p>Alternatively, if you prefer PIP, use the following command:</p>
<pre><strong>pip install django</strong></pre>
<p>This will install the Django module to your Python environment.</p>
<p>To check whether it has been successfully installed, use the following command in the terminal:</p>
<pre><strong>python -m django --version</strong></pre>
<p>This should produce an output of a version number—for example, <kbd>- 2.0.8</kbd>. If not, check your installation of Django.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new Django project</h1>
                </header>
            
            <article>
                
<p>Django provides a handy utility named the <kbd>django-admin</kbd> tool, which can be used to generate the boilerplate code required for a Django project. To create a new project named<span>, say,</span> <kbd>cntkdemo</kbd><span>, use the following code:</span></p>
<pre><strong>django-admin startproject cntkdemo</strong></pre>
<p>This will create all the boilerplate folders and files. However, we must create at least one app within the project. Change your active working directory to the <kbd>cntkdemo</kbd> folder using the terminal. Use the following command to create an app inside this project:</p>
<pre><strong>python manage.py startapp api</strong></pre>
<p>So, we have created a folder named <kbd>api</kbd> with the following folders inside it; all the files are auto-generated with placeholder code and documentation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1347 image-border" src="assets/3d297b46-da99-422e-aab5-4dfb5cbc90e6.png" style="width:16.50em;height:26.25em;"/></p>
<p>We can now proceed with the coding of the initial UI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the home page template</h1>
                </header>
            
            <article>
                
<p>Let's now create a web page that loads when the <kbd>/</kbd> route is accessed. Remember the <kbd>api</kbd> app that we created in the project? Let's make the index page a part of this app for the sake of simplicity. While it is possible to create this route in the <kbd>urls.py</kbd> file of the <kbd>mysite</kbd> app, we will provide the <kbd>api</kbd> app with its own route handling file.</p>
<p>Let's begin with the steps for setting up the home page template:</p>
<ol>
<li class="mce-root"><span>Create a file,</span> <kbd>urls.py</kbd><span>, inside the</span> <kbd>api</kbd> <span>folder. The complete path of this file relative to the project directory would be</span> <kbd>mysite/api/urls.py</kbd><span>. Inside this file, let's add the</span> <kbd>/</kbd> <span>route, using the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>from django.urls import path</strong><br/><br/><strong>from . import views</strong><br/><br/><strong>urlpatterns = [</strong><br/><strong>  path('', views.indexView), # This line handles the '/' route.</strong><br/><strong>]</strong></pre>
<ol start="2">
<li class="mce-root">Save this <span>file. The preceding code essentially adds a new path, <kbd>/</kbd>, to the <kbd>api</kbd> app (note, not to the project!). It imports all the views available in the <kbd>views.py</kbd> file of the <kbd>api</kbd> app. Note that <kbd>indexView</kbd> still does not exist. We will create this view after the next step.</span></li>
<li class="mce-root">The <span><kbd>api</kbd> app is not linked to the main project app. We need to add the following lines to the <kbd>mysite/mysite/urls.py</kbd> file to enable the route handling by the <kbd>api</kbd> app's route handler:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>from django.contrib import admin</strong><br/><strong>from django.urls import path</strong><br/><strong>from django.urls import include # -- Add this line!</strong><br/><br/><strong>urlpatterns = [</strong><br/><strong>  path('', include('api.urls')), # -- Add this line!</strong><br/><strong>  path('admin/', admin.site.urls),</strong><br/><strong>]</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The first line imports a utility for including app-specific routing settings to the project app. We use this to include the <kbd>urls.py</kbd> file inside the <kbd>api</kbd> app using the <kbd>api.urls</kbd> string. This automatically converts the strings to lines of code that try to find and include the correct file.</span></p>
<ol start="4">
<li class="mce-root">In <span>the <kbd>views.py</kbd> file inside the <kbd>api</kbd> app directory, add the following lines:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>from django.http import HttpResponse</strong><br/><strong>from django.template import loader</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The <kbd>HttpResponse</kbd> method allows the <kbd>view</kbd> method to return an HTML response. The <kbd>loader</kbd> class provides us with methods to load HTML templates from the disk.</span></p>
<ol start="5">
<li class="mce-root">Let'<span>s now create the</span> <kbd>indexView</kbd> <span>method:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>def indexView(request):</strong><br/><strong>  template = loader.get_template('api/index.html')</strong><br/><strong>  context = {}</strong><br/><strong>  return HttpResponse(template.render(context, request))</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The <kbd>indexView</kbd> method loads the <kbd>api/index.html</kbd></span> <span>template file</span> <span>and renders it with the variables provided in the</span> <kbd>context</kbd> <span>dictionary, along with the</span> <kbd>request</kbd> <span>parameters available to the template. Currently, we pass a blank context because we do not have any values to send to the template. But again, the</span> <kbd>api/index.html</kbd> <span>file defined previously does not exist.</span></p>
<ol start="6">
<li class="mce-root">Let'<span>s create the folder for holding templates and link it to the project settings. To do so, go to the root directory of the project and create a folder named <kbd>templates</kbd>. We need the project to be able to recognize this folder as the directory for the templates. To do so, we need to modify the <kbd>TEMPLATES</kbd> settings in the <kbd>mysite/mysite/settings.py</kbd> file:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>TEMPLATES = [</strong><br/><strong> {</strong><br/><strong> 'BACKEND': 'django.template.backends.django.DjangoTemplates',</strong><br/><strong> 'DIRS': [os.path.join(BASE_DIR, 'templates')], # -- Add this line!</strong><br/><strong> 'APP_DIRS': True,</strong><br/><strong> 'OPTIONS': {</strong><br/><strong> 'context_processors': [</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>Upon adding the preceding line, the project will look for the templates inside the <kbd>mysite/templates/</kbd> folder.</span></p>
<ol start="7">
<li class="mce-root">Create <span>the <kbd>index.html</kbd> template file.</span><br/>
<br/>
<span>Notice that our template file route in step 4 exists within an <kbd>api</kbd> directory. Create a folder named <kbd>api</kbd> inside the <kbd>templates</kbd> directory. Inside this, create the <kbd>index.html</kbd> file with the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">{% load static %}<br/>...<br/>        &lt;div class="jumbotron"&gt;<br/>            &lt;h3 class="jumbotronHeading"&gt;Draw here!&lt;/h3&gt;<br/>            ...<br/>        &lt;/div&gt;<br/>        &lt;div class="jumbotron"&gt;<br/>            &lt;h3&gt;Prediction Results&lt;/h3&gt; <br/>            &lt;p id="result"&gt;&lt;/p&gt;<br/>        &lt;/div&gt;<br/>        &lt;div id="csrf"&gt;{% csrf_token %}&lt;/div&gt;<br/>    &lt;/div&gt;<br/>    &lt;script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'&gt;&lt;/script&gt;<br/>    &lt;script src="{% static "/index.js" %}"&gt;&lt;/script&gt;<br/>...</pre>
<p style="padding-left: 60px" class="mce-root"><span>We've included some required scripts at the end of the preceding code block, including a script to fetch the CSRF token from the backend.</span></p>
<ol start="8">
<li class="mce-root">Now<span>, let's add a <kbd>canvas</kbd> element to <kbd>div</kbd> with the <kbd>jumbotron</kbd> class in the previous code block, where we will draw the digits. We'll also add a slider for selecting the width of the drawing pen, as shown:</span></li>
</ol>
<pre style="color: black">        &lt;div class="jumbotron"&gt;<br/>            &lt;h3 class="jumbotronHeading"&gt;Draw here!&lt;/h3&gt;        <br/>            &lt;div class="slidecontainer"&gt;<br/>                &lt;input type="range" min="10" max="50" value="15" id="myRange"&gt;<br/>                &lt;p&gt;Value: &lt;span id="sliderValue"&gt;&lt;/span&gt;&lt;/p&gt;<br/>            &lt;/div&gt;<br/>            &lt;div class="canvasDiv"&gt;<br/>                &lt;canvas id="canvas" width="350" height="350"&gt;&lt;/canvas&gt;<br/>                &lt;p style="text-align:center;"&gt;<br/>                    &lt;button class="btn btn-success" id="predict-btn" role="button"&gt;Predict&lt;/button&gt;<br/>                    &lt;button class="btn btn-primary" id="clearButton" role="button"&gt;Clear&lt;/button&gt;<br/>                &lt;/p&gt;<br/>            &lt;/div&gt;<br/>        &lt;/div&gt;</pre>
<p style="padding-left: 60px" class="mce-root"><span>The <kbd>template</kbd> file also includes two static files—<kbd>style.css</kbd> and <kbd>script.js</kbd>. We will be creating these files in the upcoming section. We have not yet created the script for sending the data to the server and rendering the response received.</span></p>
<ol start="9">
<li class="mce-root">Now<span>, we will begin adding the JavaScript code required to communicate with the backend APIs. First, we create a method to check whether we need a CSRF token to communicate with the backend. This is only a utility function and is not related to calling the backend APIs, which may, at times, be designed to accept requests without CSRF tokens. We create this function as shown:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">&lt;script type="text/javascript"&gt;<br/>    function csrfSafeMethod(method) {<br/>        return (/^(GET|HEAD|OPTIONS|TRACE)$/.test(method));<br/>    }</pre>
<ol start="10">
<li class="mce-root">Then<span>, we create a <kbd>click</kbd> handler for the <kbd>Predict</kbd> button. This handler function first sets up the proper headers required to make the call to the backend APIs and then converts the drawing present on the canvas into a data URL string:</span></li>
</ol>
<pre style="color: black">    $("#predict-btn").click(function() {<br/><br/>        var csrftoken = $('input[name=csrfmiddlewaretoken]').val();<br/><br/>        $.ajaxSetup({<br/>            beforeSend: function(xhr, settings) {<br/>                if (!csrfSafeMethod(settings.type) &amp;&amp; !this.crossDomain) {<br/>                    xhr.setRequestHeader("X-CSRFToken", csrftoken);<br/>                }<br/>            }<br/>        });<br/><br/>        $('#predict-btn').prop('disabled', true);<br/><br/>        var canvasObj = document.getElementById("canvas");<br/>        var img = canvasObj.toDataURL();<br/>        <br/>        // MORE CODE TO BE ADDED BELOW THIS LINE<br/><br/>        // MORE CODE TO BE ADDED ABOVE THIS LINE<br/>    });<br/>    &lt;/script&gt;</pre>
<ol start="11">
<li class="mce-root">Finally<span>, we add the code to the <kbd>click</kbd> handler function of the <kbd>Predict</kbd> button to make the Ajax call to the backend with the data from the canvas, as shown:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">$("#predict-btn").click(function() {<br/>...<br/>        // MORE CODE TO BE ADDED BELOW THIS LINE<br/>        $.ajax({<br/>            type: "POST",<br/>            url: "/predict",<br/>            data: img,<br/>            success: function(data) {<br/>                console.log(data);<br/>                var tb = "&lt;table class='table table-hover'&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Item&lt;/th&gt;&lt;th&gt;Confidence&lt;/th&gt;&lt;/thead&gt;&lt;tbody&gt;";<br/>                var res = JSON.parse(data);<br/>                console.log(res);<br/>                <br/>                $('#result').empty.append(res.data);<br/>                $('#predict-btn').prop('disabled', false);<br/>            }<br/>        });<br/>        // MORE CODE TO BE ADDED ABOVE THIS LINE<br/>...<br/>});<br/>    &lt;/script&gt;</pre>
<ol start="12">
<li class="mce-root">Before <span>we can create the static files, we need to create a folder for them and link it to the project. This is similar to how we created the <kbd>templates</kbd> folder. First, create a folder, <kbd>static</kbd>, in the project directory with a <kbd>mysite/static/</kbd> path. Then, modify the <kbd>STATIC</kbd> configuration in the <kbd>mysite/mysite/settings.py</kbd> file, as shown:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>STATIC_URL = '/static/'</strong><br/><br/><strong>STATICFILES_DIRS = [</strong><br/><strong>    os.path.join(BASE_DIR, "static"), # -- Add this line!</strong><br/><strong>]</strong></pre>
<p class="mce-root"/>
<p style="color: black;padding-left: 60px">We can now create and load static files into the project templates using the <kbd>{% load static %}</kbd> directive at the top of the template files, as we did in the <kbd>index.html</kbd> file.</p>
<ol start="13">
<li class="mce-root">Creat<span>e</span> <kbd>style.css</kbd> <span>and</span> <kbd>script.js</kbd><span>—since these files are not explicitly relevant to the context of this book, you can download them directly from</span> <a href="http://tiny.cc/cntk-demo">http://tiny.cc/cntk-demo</a><span>.</span></li>
</ol>
<div class="packt_infobox">Please note here that without the <kbd>script.js</kbd> file, the project will not run.</div>
<p>We have created the setup for the prediction of the images drawn on a canvas present in the <kbd>index.html</kbd> template file. However, the <kbd>/predict</kbd> route is yet to be created. Let's see how CNTK models can be loaded and used in Django in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making predictions using CNTK from the Django project</h1>
                </header>
            
            <article>
                
<p>In this section, we'll first set the required route, the view, and the imports for the CNTK model to work with Django. We will then load the CNTK model from the saved file and make predictions using it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the predict route and view</h1>
                </header>
            
            <article>
                
<p>Recall how we created the <kbd>/</kbd> route and its corresponding view in the <kbd>api</kbd> app:</p>
<ol>
<li>First, add the following line to <kbd>mysite/api/urls.py</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>urlpatterns = [</strong><br/><strong>  path('', views.indexView),</strong><br/><strong>  path('predict', views.predictView), # -- Add this line!</strong><br/><strong>]</strong></pre>
<p style="padding-left: 60px">This creates the <kbd>/predict</kbd> route. However, the view, <kbd>predictView</kbd>, is not yet created.</p>
<ol start="2">
<li>Add the following lines to the <kbd>views.py</kbd> file in the <kbd>api</kbd> app:</li>
</ol>
<pre style="padding-left: 60px"><strong>from django.http import JsonResponse</strong><br/><br/><strong>def predictView(request):</strong><br/><strong>    # We will add more code below this line</strong><br/><br/><strong>    # We will add more code above this line</strong><br/><strong>    return JsonResponse({"data": -1})</strong></pre>
<p>Notice the placeholders in the preceding lines. We'll add more here in the next steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making the necessary module imports</h1>
                </header>
            
            <article>
                
<p>Now, let's load all the modules required to make predictions with the CNTK model, as in the following steps:</p>
<ol>
<li>Add the following lines of imports to the <kbd>views.py</kbd> file of the <kbd>api</kbd> app:</li>
</ol>
<pre style="padding-left: 60px"><strong>import os</strong><br/><strong>from django.conf import settings</strong></pre>
<ol start="2">
<li>We'll need the preceding imports to load the model from the disk:</li>
</ol>
<pre style="padding-left: 60px"><strong>import cntk as C</strong><br/><strong>from cntk.ops.functions import load_model</strong></pre>
<p style="padding-left: 60px">The preceding lines import the CNTK module to the Django project. The <kbd>load_model</kbd> method will help us load the saved CNTK model file.</p>
<p style="padding-left: 60px"><span>The following modules are used to manipulate the images that the predictions will be made on:</span></p>
<pre style="padding-left: 60px" class="mce-root"><strong>from PIL import Image</strong><br/><strong>import numpy as np</strong></pre>
<p style="padding-left: 60px"><span>The following modules provide utility for handling Base64-encoded strings, which is the format that the <kbd>index.html</kbd> page sends the canvas data in the request:</span></p>
<pre style="padding-left: 60px" class="mce-root"><strong>import re</strong><br/><strong>import base64</strong><br/><strong>import random </strong><br/><strong>import string</strong></pre>
<p>The other libraries will be explained when they are used in the upcoming sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading and predicting using the CNTK model</h1>
                </header>
            
            <article>
                
<p>We will now further edit the <kbd>predictView</kbd> view by following these steps:</p>
<ol>
<li class="mce-root"><span>First, read the Base64-encoded image string data to a variable using the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>def predictView(request):</strong><br/><strong>  # We will add more code below this line</strong><br/><br/><strong>  post_data = request.POST.items()</strong><br/><strong>  pd = [p for p in post_data]</strong><br/><strong>  imgData = pd[1][0].replace(" ", "+")</strong><br/><strong>  imgData += "=" * ((4 - len(imgData) % 4) % 4)</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The Base64-decoded string does not have proper padding and contains spaces that need to be converted into <kbd>+</kbd>. The last two lines in the previous code block perform the same manipulations on the string.</span></p>
<ol start="2">
<li class="mce-root">Next<span>, we will convert this Base64-encoded string into a PNG image and save it to disk with the following lines:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>filename = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(32)])</strong><br/><br/><strong>convertImage(imgData, filename)</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The first line creates a 32-character-long random string for the filename. The next line calls the <kbd>convertImage</kbd></span> <span>method,</span> <span>which stored the <kbd>base64</kbd> string as the filename provided.</span></p>
<ol start="3">
<li class="mce-root">However, the <kbd>convertImage</kbd> method has not yet been defined. Outside of the <kbd>predictView</kbd> method, add the definition for the function, as shown:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>def convertImage(imgData, filename):</strong><br/><strong>  imgstr = re.search(r'base64,(.*)', str(imgData)).group(1)</strong><br/><strong>  img = base64.b64decode(imgstr)</strong><br/><strong>  with open(filename+'.png', 'wb') as output:</strong><br/><strong>    output.write(img)</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The method strips out the extra metadata from the string. It then decodes the string and saves it as a PNG file.</span></p>
<ol start="4">
<li class="mce-root">Let's <span>return back to the <kbd>predictView</kbd> method. We will first load the saved <kbd>image</kbd> file:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>image = Image.open(filename+'.png').convert('1')</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>We will also convert the image into a black and white channel only. This reduces the number of channels in the image from 3 to 1.</span></p>
<ol start="5">
<li class="mce-root">Recall <span>that all images in the MNIST dataset have dimensions of 28 x 28. We must resize our current image to the same dimensions. We do so with the following line:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>image.thumbnail((28,28), Image.ANTIALIAS)</strong></pre>
<ol start="6">
<li class="mce-root">Now<span>, we convert the image into a NumPy array with the following lines:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>image_np = np.array(image.getdata()).astype(int)</strong><br/><strong>image_np_expanded = np.expand_dims(image_np, axis = 0)</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span><kbd>np.expanded_dims</kbd> is a simple utility in NumPy used to add an extra dimension to the array for proper compatibility with most machine learning libraries.</span></p>
<ol start="7">
<li class="mce-root">Load <span>the CNTK model.</span> <span>First, create a folder named <kbd>data</kbd> in the root directory of the project and copy the saved <kbd>model</kbd> file there in <kbd>mysite/data/cntk.model</kbd>.</span><br/>
<br/>
<span>We now load the CNTK model in the <kbd>predictView</kbd> method, as shown:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>model = load_model(os.path.join(settings.BASE_DIR, "data/cntk.model"))</strong></pre>
<ol start="8">
<li class="mce-root">Finally<span>, we can predict the label of the image, as shown:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>predicted_label_probs = model.eval({model.arguments[0]: image_np_expanded})</strong><br/><strong>data = np.argmax(predicted_label_probs, axis=1)</strong></pre>
<p style="padding-left: 60px" class="mce-root"><span>The <kbd>eval</kbd> method, in its first argument, expects the NumPy array of the image and returns a list of probabilities of each output class. The <kbd>np.argmax</kbd> method is used to find the index of the class with the highest probability.</span></p>
<ol start="9">
<li class="mce-root">To <span>return the output, modify the <kbd>return</kbd> part of the <kbd>predictView</kbd> method, as shown:</span></li>
</ol>
<pre style="color: black"><strong>    # We will add more code above this line</strong><br/><strong>    return JsonResponse({"data": str(data[0])})</strong></pre>
<p class="mce-root"><span>The predicted label for the image is sent as a digit contained in the <kbd>data</kbd> variable of the JSON response, which is displayed on the page.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the web app</h1>
                </header>
            
            <article>
                
<p>Finally, we can test the CNTK + Django app we have developed. To do so, open the terminal and direct it to the root directory of the project.</p>
<p>Use the following command to start the Django server:</p>
<pre><strong>python manage.py runserver</strong></pre>
<p>The server starts at <a href="http://localhost:8000">http://localhost:8000</a> if the port is free. Open the page in a web browser. Draw your digit on the canvas provided and click on the <span class="packt_screen">Predict</span> button. You will be able to see the result from the model at the bottom of the page, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1348 image-border" src="assets/b717132a-d629-41e8-b622-7424137ccf76.png" style="width:35.25em;height:26.17em;"/></p>
<p>Notice that the model returns the correct output in the preceding screenshot, which is <span class="packt_screen">2</span>. Hence, we conclude the deployment of CNTK models using Django.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered the offerings from Microsoft AI and the Azure cloud for performing deep learning on websites. We saw how the Face API can be used to predict the gender and age of people in images, as well as how the Text Analytics API can be used to predict the language of a given text and the key phrases in the provided text or the sentiment of any sentence. Finally, we created a deep learning model using CNTK on the MNIST dataset. We saw how the model can be saved and then deployed via a Django-based web application in the form of an API. This deployment of the saved model via Django can be <span>easily</span> <span>adapted for other deep learning frameworks, such as TensorFlow or PyTorch.</span></p>
<p>In the next chapter, we will discuss a generalized framework for building production-grade deep learning applications using Python.</p>


            </article>

            
        </section>
    </body></html>