<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Nuts and Bolts of Neural Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">In this chapter, we'll discuss some of the intricacies of neural networks (<strong>NNs</strong>)<span>—</span>the cornerstone of <strong>deep learning</strong> (<strong>DL</strong>). We'll talk about their <span>mathemat</span>ical apparatus, structure, and training. <span>Our main goal is to provide you</span> with a systematic understanding of <span>NNs. Often, we approach them from a computer science perspective—as a machine learning (<strong>ML</strong>) algorithm (or even a special entity) composed of a number of different steps/components. We gain our intuition by thinking in terms of neurons, layers, and so on (at least I did this when I first learned about this field). This is a perfectly valid way to do things and we can still do impressive things at this level of understanding. Perhaps this is not the correct approach, though.</span></p>
<p class="mce-root"><span>NNs have solid mathematical foundations and if we approach them from this point of view, we'll be able to define and understand them in a more fundamental and elegant way. Therefore, in this chapter, we'll try to underscore the analogy between NNs from mathematical and computer science points of view. If you are already familiar with these topics, you can skip this chapter. Still, I hope that you'll find some interesting bits you didn't know about already (we'll do our best to keep this chapter interesting!)</span>.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>The mathematical apparatus of NNs</li>
<li>A short introduction to NNs</li>
<li>Training NNs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The mathematical apparatus of NNs</h1>
                </header>
            
            <article>
                
<p>In the next few sections, we'll discuss the mathematical branches related to NNs. Once we've done this, we'll connect them to NNs themselves.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear algebra</h1>
                </header>
            
            <article>
                
<p><span>Linear algebra deals with linear equations such as </span><span><sub><img class="fm-editor-equation" src="assets/6f58b451-029b-4567-901b-7d8cee5287f8.png" style="width:15.75em;height:1.17em;"/> </sub></span><span>and linear transformations (or linear functions) and their representations, such as matrices and vectors.</span></p>
<p>Linear algebra identifies the following mathematical objects:</p>
<ul>
<li><strong>Scalars</strong>: A single number.</li>
<li><strong>Vectors</strong>: A one-dimensional array of numbers (or components). Each component of the array has an index. In literature, we will see vectors denoted either with a superscript arrow (<sub><img class="fm-editor-equation" src="assets/12778eab-f899-4ef1-86a4-963419135aae.png" style="width:0.67em;height:1.00em;"/></sub>) or in bold (<strong>x</strong>). The following is an example of a vector:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6081dcdb-46c7-42cc-96ac-9ca33280b3ad.png" style="width:8.75em;height:7.67em;"/></p>
<div class="packt_tip">Throughout this book, we'll mostly use <span>the bold (</span><strong>x</strong><span>) graph notations. But in some instances, we'll use formulas from different sources and we'll try to retain their original notation.</span></div>
<p style="padding-left: 60px"><span>We can visually represent an </span><em>n</em><span>-dimensional vector as the coordinates of a point in an </span><em>n</em><span>-dimensional Euclidean space,</span> <span><sub><img class="fm-editor-equation" src="assets/d51dca7b-23f1-4beb-8870-7c33c790cae0.png" style="width:1.17em;height:0.83em;"/></sub></span> <span>(equivalent to a coordinate system). In this case, the vector is referred to as Euclidean and each vector component represents the coordinate along the corresponding axis, as shown in the following diagram: </span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1081 image-border" src="assets/6b7d4b80-53be-48f8-94b6-734fc3067210.png" style="width:18.50em;height:15.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Vector representation in <sub><img class="fm-editor-equation" src="assets/0d21b02f-f4ac-400c-9389-ec8cf53ab570.png" style="width:1.00em;height:0.83em;"/></sub> space</div>
<p style="padding-left: 60px">However, the Euclidean vector is more than just a point and we can <span>also represent it with the following two properties:</span></p>
<ul>
<li style="padding-left: 60px"><strong>Magnitude</strong> (or <strong>length</strong>) <span>is a generalization of the Pythagorean theorem for an </span><em>n</em><span>-dimensional space</span>:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7858b8a3-ac6c-4d81-84f7-bea53f11c884.png" style="width:13.17em;height:2.50em;"/></p>
<ul>
<li style="padding-left: 60px"><strong>Direction</strong> <span>is the angle of the vector along each axis of the vector space.</span></li>
</ul>
<ul>
<li><strong>Matrices</strong>: This is a two-dimensional array of numbers. Each element is identified by two indices (row and column). A matrix is usually denoted with a bold capital letter; for example, <strong>A</strong>. Each matrix element is denoted with the small matrix letter and a subscript index; for example, <em>a<sub>ij</sub></em>. <span>Let's look at an example of the matrix notation in the following formula</span>:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/95cc7e76-7279-4ffd-840e-7c01ea75404b.png" style="width:15.75em;height:7.42em;"/></p>
<p style="padding-left: 60px">We can represent a vector as a single-column <em>n×1</em> matrix (referred to as a column matrix) or a single -ow <em>1×n</em> matrix (referred to as a row matrix).</p>
<ul>
<li><strong>Tensors</strong>: Before we explain them, we have to start with a disclaimer. Tensors originally come from mathematics and physics, where they have existed long before we started using them in ML. The tensor definition in these fields differs from the ML one. For the purposes of this book, we'll only consider tensors in the ML context. Here, a tensor is a multi-dimensional array with the following properties:
<ul>
<li><strong>Rank</strong>: Indicates the number of array dimensions. For example, a tensor of rank 2 is a matrix, a tensor of rank 1 is a vector, and a tensor of rank 0 is a scalar. However, the tensor has no limit on the number of dimensions. Indeed, some types of NNs use tensors of rank 4. </li>
<li><strong>Shape</strong>: The size of each dimension.</li>
<li><strong>The data type</strong> of the tensor elements. These can vary between libraries, but typically include 16-, 32-, and 64-bit float and 8-, 16-, 32-, and 64-bit integers.</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">Contemporary DL libraries such as TensorFlow and PyTorch use tensors as their main data structure.</p>
<div class="packt_tip">You can find a thorough discussion on the nature of tensors here: <a href="https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors">https://stats.stackexchange.com/questions/198061/why-the-sudden-fascination-with-tensors</a>. You can also check the TensorFlow (<a href="https://www.tensorflow.org/guide/tensors">https://www.tensorflow.org/guide/tensors</a>) and PyTorch (<a href="https://pytorch.org/docs/stable/tensors.html">https://pytorch.org/docs/stable/tensors.html</a>) tensor definitions.</div>
<p>Now that we've introduced the types of objects in linear algebra, in the next section, we'll discuss some operations that can be applied to them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Vector and matrix operations</h1>
                </header>
            
            <article>
                
<p>In this section, we'll discuss the vector and matrix operations that are relevant to NNs. Let's start:</p>
<ul>
<li><strong>Vector addition</strong> is the operation of adding two or more vectors together into an output vector sum. The output is another vector and is computed with the following formula:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0f5341c5-fb2c-45f4-b0e0-75306d43a081.png" style="width:20.58em;height:1.42em;"/></p>
<ul>
<li>The <strong>dot</strong> (<strong>or scalar</strong>) <strong>product</strong> takes two vectors and outputs a scalar value. We can compute the dot product with the following formula:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dcfaa2c5-6467-44b6-87b0-69669e7d4f4e.png" style="width:8.83em;height:1.25em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">Here, |<em>a</em>| and |<em>b</em>| are the vector magnitudes and θ is the angle between the two vectors. Let's assume that the two vectors are<span> </span><em>n</em>-dimensional and that their components are<span> </span><em>a<sub>1</sub></em>,<span> </span><em>b<sub>1</sub></em>,<span> </span><em>a<sub>2</sub></em>,<span> </span><em>b<sub>2</sub></em><span>, </span>and so on. Here, the preceding formula is equivalent to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/46537fc8-349f-4046-9417-0543b0524012.png" style="width:17.25em;height:1.42em;"/></p>
<p style="padding-left: 60px"><span>The dot product of two two-dimensional vectors, <strong>a</strong> and <strong>b</strong>, is illustrated in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1274 image-border" src="assets/0922a3d2-af6c-4c3b-bcbe-df8ca2fbbdd4.png" style="width:33.50em;height:13.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The dot product of vectors. Top: vector components; Bottom: dot product of the two vectors</div>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><span>The dot product acts as a kind of similarity measure between the two vectors—if the angle </span><span>θ between the two vectors is small (the vectors have sim</span>ilar directions), the<span>n their dot product will be higher because of </span><sub><img class="fm-editor-equation" src="assets/2020afd0-3989-43cc-a5d2-d7bdcfb9f1db.png" style="width:2.33em;height:1.00em;"/></sub><span>. </span></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><span>Following this idea, we can define a <strong>cosine similarity</strong> between two vectors as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f82ad3d3-1446-4ef6-af18-997490b7eca9.png" style="width:6.50em;height:2.58em;"/></p>
<ul>
<li>The <strong>cross</strong> (<strong>or <span>vector</span></strong>)<strong><span> </span>product</strong> takes two vectors and outputs another vector, which is perpendicular to both initial vectors. We can compute the magnitude of the cross product output vector with the following formula:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/22fef02f-a59c-4443-ba72-a4ee23e950d3.png" style="width:8.67em;height:1.17em;"/></p>
<p style="padding-left: 60px">The following diagram shows an example of a cross product between two two-dimensional vectors:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1275 image-border" src="assets/78e303b7-c5ed-4a04-b205-36d8a4dbb1cf.png" style="width:14.25em;height:8.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Cross product of two two-dimensional vectors</div>
<p style="padding-left: 60px">As we mentioned previously, the output vector is perpendicular to the input vectors, which also means that the vector is normal to the plane containing them. The magnitude of the output vector is equal to the area of the parallelogram with the vectors <strong>a</strong> and <strong>b</strong> for sides (denoted in the preceding diagram).</p>
<div class="packt_infobox"><span>We can also define a vector through <strong>vector space</strong>, which is a collection of objects (in our case, vectors) that can be added together and multiplied by a scalar value. The vector space will allow us to define a <strong>linear transformation</strong> as a function, <em>f</em>, which can transform each vector (point) of vector space, <em><strong>V</strong></em>, into a vector (point) of another vector space, <em><strong>W</strong></em> : <sub><img class="fm-editor-equation" src="assets/5c14d1cd-c51d-4987-9a10-9b12d667bf94.png" style="width:4.75em;height:1.08em;"/></sub></span>. <em>f</em> has to satisfy the following requirements for any two vectors,<span> </span><sub><img class="fm-editor-equation" src="assets/e192b6b7-1389-41ae-a87e-3b72678b2572.png" style="width:3.58em;height:1.00em;"/></sub>:
<ul>
<li><span>Additivity: <sub><img class="fm-editor-equation" src="assets/7244c5ff-977f-4d71-aa10-073bfcb5bdef.png" style="width:10.58em;height:1.17em;"/></sub></span></li>
<li><span>H</span><span>omogeneity: <sub><img class="fm-editor-equation" src="assets/64244355-8911-4913-9b28-54a739f130b8.png" style="width:6.33em;height:1.17em;"/></sub></span>, where<span> </span><em>c</em><span> </span>is a scalar</li>
</ul>
</div>
<ul>
<li><strong>Matrix transpose</strong>: Here, we flip the matrix along its main diagonal (the main diagonal is the collection of matrix elements, <em>a<sub>ij</sub></em>, where <em>i = j</em>). The transpose operation is denoted with superscript, <sup><img class="fm-editor-equation" src="assets/197f61f9-0945-4ea1-bb90-ce72f751af65.png" style="width:0.50em;height:0.58em;"/></sup>. To clarify, the cell <sub><img class="fm-editor-equation" src="assets/7c130bda-b7e7-4a0d-a965-0cc338c6cb57.png" style="width:1.42em;height:1.00em;"/></sub> of <img class="fm-editor-equation" src="assets/57dd28ea-d409-49c2-8032-9fad6261d4c7.png" style="width:1.25em;height:0.92em;"/> is equal to the cell <sub><img class="fm-editor-equation" src="assets/d40013ee-5d21-4cee-83f0-781b154ed773.png" style="width:1.75em;height:1.25em;"/></sub> of <sub><img class="fm-editor-equation" src="assets/6c02e597-82aa-4af4-8811-f17e6164c93a.png" style="width:0.92em;height:0.92em;"/></sub>:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/47f10040-ad04-4cf0-bd8e-eb26768f9d0b.png" style="width:7.42em;height:1.83em;"/></p>
<p style="padding-left: 60px">The transpose of an <em>m×n</em> matrix is an <em>n×m</em> matrix. The following are a few transpose examples:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1667 image-border" src="assets/f70469ff-c9d5-4f61-b017-8bebbc0e8abc.png" style="width:27.58em;height:15.08em;"/></p>
<ul>
<li><strong>Matrix-scalar multiplication</strong> is the multiplication of a matrix by a scalar value. In the following example, <sub><img class="fm-editor-equation" src="assets/4931bab7-9012-44c1-a6a0-5708f01cc23a.png" style="width:0.58em;height:1.00em;"/></sub> is a scalar:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ece33a00-58a4-462e-89f6-5249e8f3e5f9.png" style="width:22.83em;height:3.25em;"/></p>
<ul>
<li><strong>Matrix-matrix addition</strong> is the element-wise addition of one matrix with another. For this operation, both matrices must have the same size. The following is an example:</li>
</ul>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8790d323-7f9e-4c7b-bf79-bedba3f4a110.png" style="width:33.58em;height:3.17em;"/></p>
<ul>
<li><strong>Matrix-vector multiplication</strong> is the multiplication of a matrix by a vector. For this operation to be valid, the number of matrix columns must be equal to the vector length. The result of multiplying the <em>m×n</em> matrix and an <em>n</em>-dimensional vector is an <em>m</em>-dimensional vector. The following is an example:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1668 image-border" src="assets/335d2d08-a422-4d14-84fc-2ccb407decf5.png" style="width:25.67em;height:8.50em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><span>We can think of each row of the matrix as a separate </span><em>n</em><span>-dimensional vector. Here, each element of the output vector is the dot product between the corresponding matrix row and <strong>x</strong>. The following is a numerical example:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a390abf7-9e62-4fb5-a511-ca25849b9306.png" style="width:23.42em;height:3.00em;"/></p>
<ul>
<li><strong>Matrix multiplication</strong> is the multiplication of one matrix with another. To be valid, the number of columns of the first matrix has to be equal to the number of rows of the second (this is a non-commutative operation). We can think of this operation as multiple matrix-vector multiplications, where each column of the second matrix is one vector. The result of an <em>m×n</em> matrix multiplied by an <em>n×p</em> matrix is an <em>m×p</em> matrix. The following is an example:</li>
</ul>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><img class="aligncenter size-full wp-image-1669 image-border" src="assets/59628c27-e81f-4945-b00a-b3555d20046e.png" style="width:60.08em;height:12.08em;"/></p>
<p style="padding-left: 60px">If we consider two vectors as row matrices, we can represent a vector dot product as matrix multiplication, that is, <img class="fm-editor-equation" src="assets/b94c318e-0c3f-4c68-be0a-bec4d81fc96b.png" style="width:4.67em;height:1.00em;"/>.</p>
<p>This concludes our introduction to linear algebra. In the next section, we'll introduce the probability theory. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to probability</h1>
                </header>
            
            <article>
                
<p>In this section, we'll discuss some of the aspects of probability and statistics that are relevant to NNs.</p>
<p>Let's start by introducing the concept of a <strong>statistical experiment</strong>, which has the following properties:</p>
<ul>
<li>Consists of multiple independent trials.</li>
<li>The outcome of each trial is non-deterministic; that is, it's determined by chance.</li>
<li>It has more than one possible outcome. These outcomes are known as <strong>events</strong> (we'll also discuss events in the context of sets in the following section).</li>
<li>All the possible outcomes of the experiment are known in advance.</li>
</ul>
<p>One example of a statistical experiment is a coin toss, which has two possible outcomes—heads or tails. Another example is a dice throw with six possible outcomes: 1, 2, 3, 4, 5, and 6. </p>
<p>We'll define <strong>probability</strong> as<span> the likelihood that some event,</span> <strong>e</strong>, <span>would occur and we'll denote it with</span> <strong>P(e)</strong><span>. The probability is a number in the range of [0, 1], where 0 indicates that the event cannot occur and 1 indicates that it will always occur. If <em>P(e) = 0.5</em>, there is a 50-50 chance the event would occur, and so on.</span></p>
<p><span>There are two ways we can approach probability:</span></p>
<ul>
<li><strong>Theoretical</strong>: The event we're interested in compared to the total number of possible events. All the events are equally as likely:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/49b6defa-690f-4e59-a266-fd4d9f6b2c1f.png" style="width:20.33em;height:2.33em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">To understand this, let's use the coin toss example with two possible outcomes. The theoretical probability of each possible outcome is P(heads) = P(tails) = 1/2. The theoretical probability for each of the sides of a dice throw would be 1/6. </p>
<ul>
<li><strong>Empirical</strong>: This is the number of times an event we're interested in occurs compared to the total number of trials:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f0d0450e-9cfc-4b25-b874-eef2ccc77de7.png" style="width:17.75em;height:2.33em;"/></p>
<p style="padding-left: 60px">The result of the experiment may show that the events aren't equally likely. For example, let's say that we toss a coin 100 times and that we observe heads 56 times. Here, the empirical probability for heads is P(heads) = 56 / 100 = 0.56. The higher the number of trials, the more accurate the calculated probability is (this is known as the law of large numbers).</p>
<p>In the next section, we'll discuss probability in the context of sets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Probability and sets</h1>
                </header>
            
            <article>
                
<p><span>The collection of all possible outcomes (events) of an experiment is called,</span><span> </span><strong>sample space</strong><span>. We can think of the sample space as a mathematical</span> <strong>set. </strong><span>It is usually denoted with a capital letter and we can list all the set outcomes with {} (the same as Python sets). For example, the sample space of coin toss events is S</span><sub>c</sub><span> </span><span>= {heads, tails}, while for dice rows it's S</span><sub>d</sub><span> </span><span>= {1, 2, 3, 4, 5, 6}. A single outcome of the set (for example, heads) is called a <strong>sample point</strong>. An <strong>e</strong></span><span><strong>vent</strong> is an outcome (sample point) or a combination of outcomes (subset) of the sample space. An example of a combined event is for the dice to land on an even number, that is, {2, 4, 6}.</span></p>
<p><span>Let's assume that we have a sample space S = {1, 2, 3, 4, 5} and two subsets (events) A = {1, 2, 3} and B = {3, 4, 5}. Here, we can do the following operations with them:</span></p>
<ul>
<li><strong>Intersection</strong>: The result is a new set that contains only the elements found in both sets:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9750f3fa-904e-4d19-8929-29b91192e4f8.png" style="width:5.92em;height:1.25em;"/></p>
<p style="padding-left: 60px">Sets whose intersections are empty sets {} are <strong>disjoint</strong>.</p>
<ul>
<li><strong>Complement</strong>: The result is a new set that contains all the elements of the sample space that aren't included in a given set:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9faf6393-251d-4453-9452-2a4a88467b1b.png" style="width:10.67em;height:1.25em;"/></p>
<ul>
<li><strong>Union:</strong> The result is a new set that contains the elements that can be found in either set:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0d19f6d-0d13-44f8-99e9-4b63f679d7b0.png" style="width:9.58em;height:1.25em;"/></p>
<p>The following Venn diagrams illustrate these different set relationships:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1082 image-border" src="assets/fa553edf-4bc6-447f-970b-1fb73643b7cd.png" style="width:36.75em;height:7.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Venn diagrams of the possible set relationships</span></div>
<p><span>We can transfer the set properties to events and their probabilities. We'll assume that the events are <strong>independent</strong></span>—t<span>he occurrence of one event doesn't affect the probability of the occurrence of another. For example, the outcomes of the different coin tosses are independ</span><span>ent of one another. That being said, let's learn how to translate the set operations in the events domain:</span></p>
<ul>
<li>The intersection of two events is a subset of the outcomes, contained in both events. The probability of the intersection is called <strong>joint probability</strong> and is computed via the following formula:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4de15857-c77b-4a52-84e0-53c12fdc63d4.png" style="width:11.08em;height:1.17em;"/></p>
<p style="padding-left: 60px">Let's say that we want to compute the probability of a card being red (either hearts or diamonds) and a Jack. The probability for red is <em>P(red) = 26/52 = 1/2</em>. The probability for getting a Jack is <em>P(Jack) = 4/52 = 1/13</em>. Therefore, the joint probability is <em>P(red, Jack) = (1/2) * (1/13) = 1/26</em>. In this example, we assumed that the two events are independent. However, the two events occur at the same time (we draw a single card). Had they occurred successively, for example, two card draws, where one is a Jack and the other is red, we would enter the realm of conditional probability. This joint probability is also denoted as P(A, B) or P(AB).</p>
<p style="padding-left: 60px">The probability of the occurrence of a single event P(A) is also known as <strong>marginal probability </strong><span>(as opposed to joint probability)</span>.</p>
<ul>
<li>Two events are disjoint (or <strong>mutually exclusive</strong>) if they don't share any outcomes. That is, their respective sample space subsets are disjoint. For example, the events of odd or even dice rows are disjoint. The following is true for the probability of disjoint events: <br/>
<ul>
<li>The joint probability of disjoint events (the probability for these events to occur simultaneously) is P(A∩B) = 0.</li>
<li>The sum of the probabilities of disjoint events is <sub><img class="fm-editor-equation" src="assets/82424ff7-15fe-4381-a40a-034f8c46b5d0.png" style="width:11.17em;height:1.17em;"/></sub>.</li>
</ul>
</li>
<li>If the subsets of multiple events contain the whole sample space between themselves, they are <strong>jointly exhaustive</strong>. Events A and B from the preceding example are jointly exhaustive because, together, they fill up the whole sample space (1 through 5). <span>The following is true for the probability of jointly exhaustive events:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f27611cd-f808-40d6-a0c4-cc09f3d0f911.png" style="width:15.75em;height:1.58em;"/></p>
<p style="padding-left: 60px">If we only have two events that are disjoint and jointly exhaustive at the same time, the events are <strong>complement</strong>. For example, odd and even dice throw events are complement. </p>
<ul>
<li>We'll refer to outcomes coming from either A or B (not necessarily in both) as the union of A and B. The probability of this union is as follows:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a7739b11-08b3-46e4-80a9-bc940ada1c1c.png" style="width:15.75em;height:1.08em;"/></p>
<p>So far, we've discussed independent events. In the next section, we'll focus on dependent ones.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conditional probability and the Bayes rule</h1>
                </header>
            
            <article>
                
<p>If the occurrence of event A changes the probability of the occurrence of event B, where A occurs before B, then the two are dependent. To illustrate this concept, let's imagine that we draw multiple cards sequentially from the deck. When the deck is full, the probability to draw hearts is <em>P(hearts) = 13/52 = 0.25</em>. But once we've drawn the first card, the probability to pick hearts on the second turn changes. Now, we only have 51 cards and one less heart. We'll call the probability of the second draw conditional probability and we'll denote it with P(B|A). T<span>his is the probability of event B (second draw), given that event A has occurred (first draw). To continue with our example</span>, the probability of picking hearts on the second draw becomes <em>P(hearts<sub>2</sub>|hearts<sub>1</sub>) = 12/51 = 0.235</em>.</p>
<p>Next, we can extend the joint probability formula (introduced in the preceding section) in terms of dependent events. The formula is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2ccab1c9-ab2d-4913-a6a3-72e19662ec07.png" style="width:11.42em;height:1.17em;"/></p>
<p>However, the preceding equation is just a special case for two events. We can extend this further for multiple events, A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>n</sub>. This new generic formula is known as the chain rule of probability:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4e7799cb-67c2-454f-adc4-d50a7d9ef1dd.png" style="width:32.58em;height:1.25em;"/></p>
<p>For example, the chain rule for three events is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e68d065f-658a-4e69-af1d-e453133b3156.png" style="width:24.17em;height:2.58em;"/></p>
<p>We can also derive the formula for the conditional probability itself:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5d06c169-d129-491f-93ba-dbaefbd62344.png" style="width:8.67em;height:2.50em;"/></p>
<p>This formula makes sense for the following reasons:</p>
<ul>
<li><strong>P(A ∩ B)</strong> states that we're interested in the occurrences of B, given that A has already occurred. In other words, we're interested in the joint occurrence of the events, hence the joint probability.</li>
<li><strong>P(A)</strong> states that we're interested only in the subset of outcomes when event A has occurred. We already know that A has occurred and therefore we restrict our observations to these outcomes. </li>
</ul>
<p>The following holds true for dependent events:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ca82fc58-9f78-4a59-b73c-00851157a8c8.png" style="width:39.08em;height:2.42em;"/></p>
<p>Using this equation, we can replace the value of <span>P(A</span><span>∩B) in the conditional probability formula to come up with the following:</span></p>
<p class="CDPAlignLeft CDPAlign"><img class="aligncenter size-full wp-image-1671 image-border" src="assets/97ce1d41-4f63-4420-ba47-2c307235e44d.png" style="width:39.58em;height:2.83em;"/></p>
<p>The preceding formula gives us the ability to compute the conditional probability, P(B|A), if we know the opposite conditional probability, <span>P(B|A)</span>. This equation is known as the <strong>Bayes rule</strong> and is frequently used in ML. In the context of Bayesian statistics, P(A) and P(B|A) are <span>known as prior and posterior probability, respectively.</span></p>
<p>The Bayes rule can be illustrated in the realm of medical testing. Let's say that we want to determine whether a patient has a particular disease or not. We conduct a medical test, which comes out positive. But this doesn't necessarily mean that the patient has the disease. <span>Most tests have a reliability value, which is the percentage chance of the test being positive when administered on people with a particular disease. Using t</span>his information, we'l<span>l apply the Bayes rule to compute the actual</span> probability of the patient having the disease, given that the test is positive. We get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3f66a8ac-fd8c-4842-8d7a-664b37690ab0.png" style="width:37.00em;height:2.92em;"/></p>
<p>Here, <em>P(has disease)</em> is the general probability of the disease without any prior conditions. Think of this as the probability of the disease in the general population.</p>
<p>Next, let's make some assumptions about the disease and the test's accuracy:</p>
<ul>
<li><span>The test is 98% reliable, that is, if the test is positive, it will also be positive in 98% of cases: <em>P(test=positive|has disease)</em> = 0.98.</span></li>
<li>Only 2% of the people under 50 have this kind of disease: <em>P(has disease)</em> = 0.02.</li>
<li>The test that's administered on people under 50 is positive only for 3.9% of the population: <em>P(test=positive)</em> = 0.039.</li>
</ul>
<p>We can ask the following question: if a test is 98% accurate for cancer and if a 45-year-old person took the test, which turned out to be positive, what is the probability that they may have the disease? Using the preceding formula, we can calculate the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0ba37462-d1ef-4608-bea0-3f1ab81484c8.png" style="width:45.58em;height:2.83em;"/></p>
<p>In the next section, we'll go beyond probabilities and we'll discuss random variables and probability distributions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random variables and probability distributions</h1>
                </header>
            
            <article>
                
<p class="mce-root">In statistics, we define a variable as an attribute that describes a given entity. The value of the attribute can vary between entities. For example, we can describe the height of a person with a variable, which would differ for different people. But let's say that we take the height measurement of the same person multiple times. We can expect to obtain slightly different values each time due to some random factors, such as the person's pose or inaccuracy in our own measurements. Therefore, the value of the variable height would differ, despite the fact that we are measuring the same thing. To account for these changes, we'll introduce random variables. These are variables whose values are determined by some random event. Unlike regular variables, a random variable can take multiple values and each of these values is associated with some probability. </p>
<p class="mce-root">There are two types of random variables:</p>
<ul>
<li>
<p><strong>Discrete</strong>, which can take distinct separate values. For example, the number of goals in a football match is a discrete variable.</p>
</li>
<li>
<p><strong>Continuous</strong>, which can take any value within a given interval. For example, a height measurement is a continuous variable.</p>
</li>
</ul>
<p>Random variables are denoted with capital letters and the probability of a certain value <em>x</em> for random variable <em>X</em> is denoted with either <em>P(X = x)</em> or <em>p(x)</em>. The collection of probabilities for each possible value of a random variable is called the <strong>probability distribution</strong>. Depending on the variable type, we have two types of probability distributions:</p>
<ul>
<li><strong>Probability mass function</strong> (<strong>PMF</strong>) for discrete variables. The following is an example of a PMF. The <em>x </em>axis shows the possible values and the <em>y </em>axis shows the probability for each value:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1083 image-border" src="assets/b624542d-ad4c-4de3-b8cd-705432da3571.png" style="width:17.17em;height:11.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">An example of a PMF</div>
<p style="padding-left: 60px">The PMF is only defined for the possible values of the random variable. All the values of a PMF are non-negative and their sum is 1. That is, the events of the PMF are mutually exclusive and jointly exhaustive. We'll denote PMF with P(X), where X is the random variable.</p>
<ul>
<li><strong>Probability density function</strong> (<strong>PDF</strong>) for continuous variables. Unlike PMF, the PDF is uninterrupted (defined for every possible value) in the interval between two values, thereby reflecting the nature of the continuous variable. The following is an example of a PDF:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1084 image-border" src="assets/9af07727-a3be-4e97-becd-2e28cf781e10.png" style="width:18.67em;height:12.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">An example of a PDF</div>
<p style="padding-left: 60px">In the PDF, the probability is computed for a value interval and is given by the surface area under the curve, enclosed by that interval (this is the marked area in the preceding diagram). <span>The total area under the curve is 1.</span> We'll denote PDF with <em>f<sub>X</sub></em>, where X is the random variable.</p>
<p>Next, let's focus on some of the properties of random variables:</p>
<ul>
<li>The <strong>mean</strong> (or <strong>expected value</strong>) is the expected outcome of an experiment over many observations. We'll denote it with μ or <sub><img class="fm-editor-equation" src="assets/0e27cda7-f298-4638-bb45-baf9d20f900d.png" style="width:0.75em;height:1.00em;"/></sub>. For a discrete variable, the mean is the weighted sum of all possible values, multiplied by their probabilities:</li>
</ul>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c0657eab-a8d9-4164-a902-80831922cf56.png" style="width:39.25em;height:3.17em;"/></p>
<p style="padding-left: 60px">Let's use the preceding discrete variable example as an example, where we defined a random variable with six possible values (0, 1, 2, 3, 4, 5) and their respective probabilities (0.1, 0.2, 0.3, 0.2, 0.1, 0.1). Here, the mean is <em>μ = 0*0.1 + 1*0.2 + 2*0.3 + 3*0.2 + 4*0.1 + 5*0.1 = 2.3.</em></p>
<p style="padding-left: 60px">The mean for a continuous variable is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/795d34b0-3db0-4bb7-b329-ba6fb5340ce0.png" style="width:12.67em;height:2.50em;"/></p>
<p style="padding-left: 60px">While with a discrete variable we can think of the PMF as a lookup table, the PDF may be more complex (an actual function or equation), which is why there's different notation between the two. We won't go into further details about the mean of continuous variables.</p>
<ul>
<li><strong>Variance</strong> is defined as the expected value of the squared deviation from the mean, μ, of a random variable:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f8904fe0-0265-47a6-a282-560d4650cef1.png" style="width:11.33em;height:1.50em;"/></p>
<p style="padding-left: 60px">In other words, the variance measures how the values of a random variable differ from its mean value. </p>
<p style="padding-left: 60px">The variance of a discrete random variable is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5af276dd-d6ad-45d1-9d18-6e96bde7e162.png" style="width:16.00em;height:3.17em;"/></p>
<p style="padding-left: 60px">Let's use the preceding example, where we calculated the mean value to be 2.3. The new variance would be <em>Var(X)</em> = <em>(0 - 2.3)<sup>2</sup> * 0 + <span>(1 - 2.3)</span><sup>2</sup><span> </span></em><span><em>* 1 + ... + (5- 2.3)<sup>2</sup> * 5 = 2.01</em>.</span></p>
<p style="padding-left: 60px">The variance of a continuous variable is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ab614d73-6d72-434e-bb88-e4aaca8c203c.png" style="width:14.42em;height:2.58em;"/></p>
<ul>
<li>The <strong>standard deviation</strong> measures the degree to which the values of the random variable differ from the expected value. If this definition sounds similar to variance, it's because it is. In fact, the formula for standard deviation is as follows:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/aa212f96-53f9-4902-b157-a7794dae687a.png" style="width:6.75em;height:2.08em;"/></p>
<p style="padding-left: 60px">We can also define the variance in terms of standard deviation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/95089bd7-c563-4642-9f9e-9ffd955b814c.png" style="width:5.42em;height:1.25em;"/></p>
<p style="padding-left: 60px">The difference between standard deviation and variance is that the standard deviation is expressed in the same units as the mean value, while the variance uses squared units.</p>
<p><span>In this section, we defined what a probability distribution is. Next, let's discuss different types of probability distributions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Probability distributions</h1>
                </header>
            
            <article>
                
<p>We'll start with the <strong>binomial distribution</strong> for discrete variables in binomial experiments. A binomial experiment has only two possible outcomes: success or failure. It also satisfies the following requirements:</p>
<ul>
<li>Each trial is independent of the others.</li>
<li>The probability of success is always the same.</li>
</ul>
<p>An example of a binomial experiment is the coin toss experiment.</p>
<p class="mce-root">Now, let's assume that the experiment consists of <em>n</em> trials. <em>x</em> of them are successful, while the probability of success at each trial is <em>p</em>. The formula for a binomial PMF of variable X (not to be confused with <em>x</em>) is as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ecbfa31a-d56f-4dcb-9901-471b1326841d.png" style="width:15.08em;height:2.75em;"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="assets/54708b4a-3bf1-4a19-9f3b-5c06c5cd6035.png" style="width:6.67em;height:1.17em;"/></sub> is the binomial coefficient. This is the number of combinations of <em>x</em> successful trials, which we can select from the <em>n</em> total trials. If <em>n=1</em>, then we have a special case of binomial distribution called <strong>Bernoulli distribution</strong>.</p>
<p>Next, let's discuss the normal (or Gaussian) distribution for continuous variables, which closely approximates many natural processes. The normal distribution is defined with the following exponential PDF formula, known as normal equation (one of the most popular notations):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/27332c0a-c327-44ac-a6de-6fc4308703ea.png" style="width:21.42em;height:6.50em;"/></p>
<p>Here, <em>x</em> is the value of the random variable, <em>μ</em> is the mean, <em>σ</em> is the standard deviation, and <em>σ<sup>2</sup></em> is the variance. The preceding equation produces a bell-shaped curve, which is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1085 image-border" src="assets/299a86be-c077-4a2e-8565-9e5bd48b9636.png" style="width:21.75em;height:15.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Normal distribution</div>
<p>Let's discuss some of the properties of the normal distribution, in no particular order:</p>
<ul>
<li>The curve is symmetric along its center, which is also the maximum value.</li>
<li>The shape and location of the curve are fully described by the mean and standard deviation, where we have the following:<br/>
<ul>
<li>The center of the curve (and its maximum value) is equal to the mean. That is, the mean determines the location of the curve along the <em>x</em> axis.</li>
<li>The width of the curve is determined by the standard deviation. </li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">In the following diagram, we can see examples of normal distributions with different <em>μ</em> and <em>σ</em> values:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1086 image-border" src="assets/966244d3-6838-4232-8d95-efcc2db6718b.png" style="width:28.08em;height:17.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Examples of normal distributions with different <em>μ</em><span> and </span><em>σ</em> values</div>
<ul>
<li><span>The normal distribution approaches 0 toward +/- infinity, but it never becomes 0. Therefore, a random variable under normal distribution can have any value (albeit some values with a tiny probability).</span></li>
<li>The surface area under the curve is equal to 1, which is ensured by the constant, <sub><img class="fm-editor-equation" src="assets/02c31240-b7e4-4022-86be-883f26fb9ced.png" style="width:3.17em;height:1.17em;"/></sub>, being before the exponent.</li>
<li><sub><img class="fm-editor-equation" src="assets/6c9b2a78-408e-49a3-a1bc-d1462d3c6964.png" style="width:2.67em;height:2.08em;"/></sub> (located in the exponent) is called the standard score (or z-score). A standardized normal variable has a mean of 0 and a standard deviation of 1. Once transformed, the random variable participates in the equation in its standardized form.</li>
</ul>
<p>In the next section, we'll introduce the multidisciplinary field of information theory, which will help us use probability theory in the context of NNs. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Information theory</h1>
                </header>
            
            <article>
                
<p>Information theory attempts to determine the amount of information an event has. The amount of information is guided by the following principles:</p>
<ul>
<li>The higher the probability of an event, the less informative the event is considered. Conversely, if the probability is lower, the event carries more informational content. For example, the outcome of a coin flip (with a probability of 1/2) provides less information than the outcome of a dice throw (with a probability of 1/6). </li>
<li>The information that's carried by independent events is the sum of their individual information contents. For example, two dice rows that come up on the same side of the dice (let's say, 4) are twice as informative as the individual rows.</li>
</ul>
<p>We'll define the amount of information (or self-information) of event <em>x</em> as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bc0a3a5c-f8c5-416c-860a-9ff406d5bddc.png" style="width:7.17em;height:1.08em;"/></p>
<p class="mce-root">Here, <em>log</em> is the natural logarithm. For example, if the probability of event is <em>P(x) = 0.8</em>, then <em>I(x) = 0.22</em>. Alternatively, if <em>P(x)</em><span><em> = 0.2</em>, then <em>I(x) = 1.61</em>. We can see that the event information content is opposite to the event probability. The amount of self-information I(x) is measured in natural units of information (<strong>nat</strong>). We can also compute I(x) with a base 2 logarithm <sub><img class="fm-editor-equation" src="assets/c6e7e6d6-823e-41fd-b98b-53f85660ed0b.png" style="width:8.17em;height:1.08em;"/></sub>, in which case we measure it in bits. There is no principal difference between the two versions. For the purposes of this book, we'll stick with the natural logarithm version.</span></p>
<div class="packt_infobox"><span>Let's discuss why we use logarithm in the preceding formula, even though a negative probability would also satisfy the reciprocity between self-information and probability. The main reason is the product and division rules of logarithms:</span>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/df24a7c6-d531-45f0-8349-f55b4cc9fe81.png" style="width:16.17em;height:3.00em;"/></p>
Here, <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em> are scalar values. Without going into too much detail, note that these properties allow us to easily minimize the error function during network training.</div>
<p>So far, we've defined the information content of a single outcome. But what about other outcomes? To measure them, we have to measure the amount of information over the probability distribution of the random variable. Let's denote it with I(<em>X</em>), where <em>X</em> is a random discrete variable <span>(we'll focus on discrete variables here)</span><em><span>.</span></em> Recall that, in the <em>Random variables and probability distributions</em> section, we defined the mean (or expected value) of a discrete random variable as <span>the weighted sum of all possible values, multiplied by their probabilities. We'll do something similar here, but we'll multiply the information content of each event by the probability of that event.</span></p>
<p><span>This measure is called Shannon entropy (or just entropy) and is defined as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b5be33a0-5072-4c44-a32f-f4afe5eb8a38.png" style="width:32.25em;height:2.83em;"/></p>
<p>Here, <em>x<sub>i</sub></em> represents the discrete variable values.<span> Events with higher probabilities will carry more weight compared to low-probability ones. We can think of entropy as the expected (mean) amount of information about the events (outcomes) of the probability distribution. To understand this, let's try to compute the entropy of the familiar coin toss experiment. We'll calculate two examples:<br/></span></p>
<ul>
<li>First, let's assume that <em>P(heads) = P(tails) = 0.5</em>. In this case, the entropy is as follows:</li>
</ul>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ea728c03-e98b-46d7-a382-4c5a4721f411.png" style="width:46.58em;height:1.25em;"/></p>
<ul>
<li>Next, let's assume that, for some reason, the outcomes are not equally likely and that the probability distribution is <em>P(heads) = 0.2 and P(tails) = 0.8</em>. The entropy is as follows:</li>
</ul>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8f2e8145-8134-4095-9957-dab1e32d88d4.png" style="width:43.42em;height:1.17em;"/></p>
<p>We can see that the entropy is highest when the outcomes are equally likely and decreases when one outcome becomes prevalent. In a sense, we can think of entropy as a measurement of uncertainty or chaos. The following diagram shows a graph of the entropy <strong>H(X)</strong> ov<span>er a binary event (such as the coin toss), depending on the probability distribution of the two outcomes:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1087 image-border" src="assets/3048ac36-93c3-4e88-8c6b-5465ce9619c7.png" style="width:34.50em;height:18.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Left: entropy with natural logarithm; right: entropy with base 2 logarithm</div>
<p><span>Next, let's imagine that we have a discrete random variable, <em>X</em>, and two different probability distributions over it. This is usually the scenario where a NN produces some output probability distribution <em>Q</em>(<em>X</em>) and we compare it to a target distribution, <em>P</em>(<em>X</em>), during training. We can measure the difference between these two distributions with <strong>cross-entropy</strong>, which is defined as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/286a0d80-e65e-4094-9d51-5ed9a35d3c5a.png" style="width:19.33em;height:3.08em;"/></p>
<p>For example, let's calculate the cross entropy between the two probability distributions of the preceding coin toss scenario. We have predicted distribution <em>Q(heads) = 0.2, Q(tails) = 0.8</em> and the target (or true) distribution <em>P(heads) = 0.5, P(tails) = 0.5</em>. The cross entropy is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f2d9c169-6917-4baa-a640-e127e574dd99.png" style="width:46.92em;height:1.17em;"/></p>
<p>Another measure of the difference between two probability distributions is the <strong>Kullback–Leibler divergence</strong> (<strong>KL divergence</strong>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1670 image-border" src="assets/cd443f93-4762-4798-97b1-2fad942ff6a8.png" style="width:33.67em;height:11.17em;"/></p>
<p><span>The product rule of logarithms helped us to</span> transform the first-row formula into a more intuitive form on the second row. It is easier to see that the KL divergence measures the difference between the target and predicted log probabilities. If we derive the equation further, we can also see the relationship between the entropy, cross-entropy, and KL divergence.</p>
<p>The KL divergence of the coin toss example scenario is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cb7873f1-4967-4cc2-aa2b-9847a61eafeb.png" style="width:43.50em;height:2.83em;"/></p>
<p class="mce-root">In the next section, we'll discuss the field of differential calculus, which will help us with training NNs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differential calculus</h1>
                </header>
            
            <article>
                
<p>In ML, we are often interested in how to approximate some target function by adjusting the parameters of ML algorithms. If we think of the ML algorithm itself as a mathematical function (which is the case for NNs), we would like to know how the output of that function changes when we change some of its parameters (weights). Thankfully, differential calculus<span> deals with the rate of change of a function with respect to a variable that the function depends on. </span>The following is a (very) short introduction to derivatives.</p>
<p>Let's say that we have a function, <em>f(x)</em>, with a single parameter, <em>x</em>, which has the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1088 image-border" src="assets/6af6853f-95e6-4295-9928-8072b8b3cc4e.png" style="width:26.92em;height:15.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The graph of <em>f(x)</em> and the slope (red dot-dashed line)</div>
<p>We can get a relative idea of how <em>f(x)</em> changes with respect to <em>x</em> at any value of <em>x</em> by calculating the slope of the function at that point. If the slope is positive, the function increases. Conversely, if it's negative, it decreases. We can calculate the slope with the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4747290f-0134-4f25-9a76-8cc959826bc4.png" style="width:15.83em;height:2.50em;"/></p>
<p>The idea here is simple<span>—</span>we calculate the difference between two values of <em>f</em> at <em>x</em> and <em>x+Δx: Δy = f(x + Δx) - f(x)</em>. Then, we calculate the ratio between <em>Δy</em><span> and <em>Δx</em> to get the slope</span><span>. But</span> if <em>Δx</em> is too big, the measurement won't be very accurate, because the part of the function graph enclosed between <em>x</em> and <em>x+Δx</em> may change drastically. We can use a smaller <em>Δx</em> to minimize this error; here, we can focus on a smaller part of the graph. If <em>Δx</em> approaches 0, we can assume that the slope reflects a single point of the graph. In this case, we call the slope the <strong>first derivative</strong> of <em>f(x)</em>. We can express this in mathematical terms via the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1346b5cc-2850-455b-b2c2-d08ac1436d63.png" style="width:17.08em;height:2.50em;"/></p>
<p>Here, <em>f'(x)</em> and <em>dy</em>/<em>dx</em> are Lagrange's and Leibniz's notations for derivatives, respectively. <sub><img class="fm-editor-equation" src="assets/daf6028e-6f96-422b-85ad-2958c188a7de.png" style="width:1.75em;height:1.33em;"/></sub> is the mathematical concept of the limit<span>—</span>we can think of it as <em>Δx</em> approaches 0. The process of finding the derivative of <em>f</em> is called <strong>differentiation</strong>. The following diagram shows slopes at different values of <em>x</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1089 image-border" src="assets/9be91093-11dc-41c7-934a-35a65f6c27c3.png" style="width:38.67em;height:19.08em;"/></p>
<p>We can see that the slopes at the <strong>local minimum</strong> and <strong>local maximum</strong> of <em>f</em> are 0—at these points (known as saddle points), <em>f</em> neither increases nor decreases as we change <em>x</em>. </p>
<p>Next, let's assume that we have a function of multiple parameters, <sub><img class="fm-editor-equation" src="assets/12ed27f9-3790-42db-8686-9074a9898e0a.png" style="width:7.33em;height:1.17em;"/></sub>. The derivative of <em>f</em> with respect to any of the parameters, <em>x<sub>i</sub></em>, is called a partial derivative and is denoted by <sub><img class="fm-editor-equation" src="assets/727e193f-e698-4224-a40f-e4f2df518a13.png" style="width:2.58em;height:1.08em;"/></sub>. When computing the partial derivative, we assume that all the other parameters, <sub><img class="fm-editor-equation" src="assets/97873945-7233-4a21-b250-4dd3f2ce4a5f.png" style="width:3.17em;height:1.17em;"/></sub>, are constants. We'll denote the partial derivatives of the components of a vector with <sub><img class="fm-editor-equation" src="assets/cf689fe9-76b8-4538-b626-f1a2716c9281.png" style="width:8.83em;height:1.75em;"/></sub>.</p>
<p>Finally, let's mention some useful rules for differentiation:</p>
<ul>
<li><strong>Chain rule</strong>: Let's say that <em>f</em> <span>and</span> <em>g</em> <span>are some functions and</span><span> </span><em>h(x)= f(g(x)).</em> Here, <span>the derivative of </span><em>f</em><span> with respect to </span><em>x</em><span> for any <em>x</em> is as follows:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/657a48ea-67e9-4c27-8ae3-1ca8c386e28c.png" style="width:22.17em;height:5.58em;"/></p>
<ul>
<li><strong>Sum rule</strong>: <span>Let's say that </span><em>f</em><span> and </span><em>g</em><span> are some functions and</span><span> </span><em>h(x) = f(x) + g(x)</em>. The sum rule states the following:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e856188c-d37a-44e0-919c-99764acc1029.png" style="width:19.67em;height:1.42em;"/></p>
<ul>
<li><strong>Common functions</strong>:
<ul>
<li><em>x' = 1</em></li>
<li><em>(ax)' = a</em>, where <em>a</em> is scalar</li>
<li><em>a' = 0</em>, where <em>a</em> is scalar</li>
<li><em>x<sup>2</sup> = 2x</em></li>
<li><em>(e<sup>x</sup>)' = e<sup>x</sup></em></li>
</ul>
</li>
</ul>
<p>The mathematical apparatus of NNs and NNs themselves form a sort of knowledge hierarchy. If we think of implementing a NN as building a house, then the mathematical apparatus is like mixing concrete. We can learn how to mix the concrete independently of how to build a house. In fact, we can mix concrete for a variety of purposes other than the specific goal of building a house. However, we need to know how to mix concrete before building the house. To continue with our analogy, now that we know how to mix concrete (mathematical apparatus), we'll focus on actually building the house (NNs). </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A short introduction to NNs</h1>
                </header>
            
            <article>
                
<p>A NN is a function (let's denote it with <em>f</em>) that tries to approximate another target function, <em>g</em>. We can describe this relationship with the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0d8ad81-d35d-4aca-938c-a995ad09c5cf.png" style="width:5.25em;height:1.08em;"/></p>
<p>Here, <em>x</em> is the input data and <em>θ</em> are the NN parameters (weights). The goal is to find such <em>θ</em> parameters with the best approximate, <em>g</em>. This generic definition applies for both regression (approximating the exact value of <em>g</em>) and classification (assigning the input to one of multiple possible classes) tasks. Alternatively, the NN function can be denoted as <img class="fm-editor-equation" src="assets/04acee79-fa2e-435c-b648-1ae896c2e1d0.png" style="width:2.67em;height:1.08em;"/>.</p>
<p>We'll start our discussion from the smallest building block of the NN<span>—</span>the neuron.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neurons</h1>
                </header>
            
            <article>
                
<p>The preceding definition is a bird's-eye view of a NN. Now, let's discuss the basic building blocks of a NN, namely the neurons (or <strong>units</strong>). Units are mathematical functions that can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f16f17a0-7360-485c-a4ca-463feb633388.png" style="width:9.83em;height:3.42em;"/></p>
<p>Here, we have the following:</p>
<ul>
<li><em>y</em> is the unit output (single value).</li>
<li><em>f</em> is the non-linear differentiable activation function. The activation function is the source of non-linearity in a NN<span>—</span>if the NN was entirely linear, it would only be able to approximate other linear functions.</li>
<li>The argument of the activation function is the weighted sum (with weights <em>w<sub>i</sub></em>) of all the unit inputs <em>x<sub>i</sub></em> (<em>n</em> total inputs) and the bias weight <em>b</em>. The inputs <em>x<sub>i</sub></em> can be either the data input values or outputs of other units.</li>
</ul>
<p>Alternatively, we can substitute <em>x<sub>i</sub></em> and <em>w<sub>i</sub></em> with their vector representations, where <sub><img class="fm-editor-equation" src="assets/70c30cb5-49ed-4523-915a-7ad17f211850.png" style="width:8.17em;height:1.08em;"/></sub> and <sub><img class="fm-editor-equation" src="assets/5463e3ea-224a-4be3-9db7-87f4172b806d.png" style="width:8.83em;height:1.08em;"/></sub>. Here, the formula will use the dot product of the two vectors:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d9748d38-e79a-4d30-9a70-0558d40a3a9a.png" style="width:7.25em;height:1.17em;"/></p>
<p>The following diagram (left) shows a unit:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1090 image-border" src="assets/082ab20e-1d14-4d08-9d03-cf22b368c695.png" style="width:39.75em;height:13.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Left: A unit and its equivalent formula; right: A geometric representation of a perceptron. </div>
<p><span>The input</span><span> vector </span><strong>x</strong><span> will be perpendicular to the weight vector </span><strong>w</strong><span> if</span><span> <strong>x• w = 0</strong>. Therefore, all vectors </span><strong>x</strong><span> where <strong>x• w = 0</strong> </span><span>define a hyperplane in the vector space <sub><img class="fm-editor-equation" src="assets/4c44a65f-c096-4021-a553-a0ce228357cb.png" style="width:1.50em;height:1.08em;"/></sub></span><span>, where <em>n</em> is the dimension of </span><em>x</em><span>. In the case of two-dimensional input <em>(x<sub>1</sub>, x<sub>2</sub>)</em>, we can represent the hyperplane as a line. This could be illustrated with the perceptron (or binary classifier)—</span><span>a u</span>nit with a <strong>threshold</strong> activat<span>ion function </span><img style="font-size: 1em;width:8.17em;height:2.50em;" class="fm-editor-equation" src="assets/1ece5653-1f1a-458a-87b4-0d9c75eff95e.png"/><span> that classifies its input in one of the two classes. The geometric representation of the perceptron with two inputs</span> <em>(x<sub>1</sub>, x<sub>2</sub>)</em> <span>is a line (or decision boundary) separating the two classes (to the right in the preceding diagram). This imposes a serious limitation on the neuron because it cannot classify linearly inseparable problems—even simple ones such as XOR.</span></p>
<p><span>A unit with an identity activation function (<em>f(x) = x</em>) is equivalent to multiple linear regression, while a unit with a sigmoid activation function is equivalent to logistic regression.</span></p>
<p><span>Next, let's learn how to organize the neurons in layers.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Layers as operations</h1>
                </header>
            
            <article>
                
<p>The next level in the NN organizational structure is the layers of units, where we combine the scalar outputs of multiple units in a single output vector. The units in a layer are not connected to each other. This organizational structure makes sense for the following reasons:</p>
<ul>
<li>We can generalize multivariate regression to a layer, as opposed to only linear or logistic regression for a single unit. In other words, we can approximate multiple values with a layer as opposed to a single value with a unit. This happens in the case of classification output, where each output unit represents the probability the input belongs to a certain class.</li>
<li><span>A unit can convey limited information because its output is a scalar. By combining the unit outputs, instead of a single activation, we can now consider the vector in its entirety. In this way, we can convey a lot more information, not only because the vector has multiple values, but also because the relative ratios between them carry additional meaning.</span></li>
<li>Because the units in a layer have no connections to each other, we can parallelize the computation of their outputs (thereby increasing the computational speed). This ability is one of the major reasons for the success of DL in recent years.</li>
</ul>
<p>In classical NNs (that is, NNs before DL, when they were just one of many ML algorithms), the primary type of layer is the <strong>fully connected</strong> (<strong>FC</strong>) layer. In this layer, every unit receives weighted input from all the components of the input vector, <strong>x</strong>. Let's assume that the size of the input vector is <em>m</em> and that the FC layer has <em>n</em> units and an activation function <em>f</em>, which is the same for all the units. Each of the <em>n</em> units will have <em>m</em> weights: one for each of the <em>m</em> inputs. The following is a formula we can use for the output of a single unit <em>j</em> of an FC layer. It's the same as the formula we defined in the <em>Neurons</em> section, but we'll include the unit index here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3d58e01e-f4a0-4d06-85ec-4bab1f5954c5.png" style="width:11.92em;height:3.67em;"/></p>
<p>Here, <em>w<sub>ij</sub></em> is the weight between the <em>j</em>-th layer unit and the <em>i</em>-th input component. We can represent the weights connecting the input vector to the units as an <em>m<span>×</span>n</em> matrix <strong>W</strong>. Each matrix column represents the weight vector of all the inputs to one layer unit. In this case, the output vector of the layer is the result of matrix-vector multiplication. However, we can also combine multiple input samples, <strong>x</strong><em><sub>i</sub></em>, in an input matrix (or <strong>batch</strong>) <strong>X</strong>, which will be passed through the layer simultaneously. In this case, we have matrix-matrix multiplication and the layer output is also a matrix. The following diagram shows an example of an FC layer, as well as its equivalent formulas in the batch and single sample scenarios:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1091 image-border" src="assets/9eca7a6e-1de6-4761-827d-8ad3056d0cd2.png" style="width:46.33em;height:15.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">An FC layer with vector/matrix inputs and outputs and its equivalent formulas</div>
<p class="mce-root">We have explicitly separated the bias and input weight matrices, but in practice, the underlying implementation may use a shared weight matrix and append an additional row of 1s to the input data.</p>
<p>Contemporary DL is not limited to FC layers. We have many other types, such as convolutional, pooling, and so on. Some of the layers have trainable weights (FC, convolutional), while others don't (pooling). We can also use the terms functions or operations interchangeably with the layer. For example, in TensorFlow and PyTorch, the FC layer we just described is a combination of two sequential operations. First, we perform the weighted sum of the weights and inputs and then we feed the result as an input to the activation function operation. In practice (that is, when working with DL libraries), the basic building block of a NN is not the unit but an operation that takes one or more tensors as input and outputs one or more tensors:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1092 image-border" src="assets/3c10ceb6-f86f-412b-b227-6761c88280d4.png" style="width:12.67em;height:9.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">A function with input and output tensors</div>
<p>Next, let's discuss how to combine the layer operations in a NN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NNs</h1>
                </header>
            
            <article>
                
<p>In the <em>Neurons</em> section, we demonstrated that a neuron (also valid for a layer) can only classify linearly separable classes. To overcome this limitation, we have to combine multiple layers in a NN. We'll define the NN as a directed graph of operations (or layers). The graph nodes are the operations, and the edges between them determine the data flow. If two operations are connected, then the output tensor of the first will serve as input to the second, which is determined by the edge direction. A NN can have multiple inputs and outputs<span>—</span>the input nodes only have outgoing edges, while the outputs only have incoming edges.</p>
<p>Based on this definition, we can identify two main types of NNs:</p>
<ul>
<li><strong>Feed-forward</strong>, which are represented by <strong>acyclic</strong> graphs. </li>
<li><strong>Recurrent</strong> (<strong>RNN</strong>), which are represented by <strong>cyclic</strong> graphs. The recurrence is temporal; the loop connection in the graph propagates the output of an operation at moment <em>t-1</em> and feeds it back into the network at the next moment, <em>t</em>. The RNN maintains an internal state, which represents a kind of summary of all the previous network inputs. This summary, along with the latest input,<span> is fed to the RNN. The network produces some output but also updates its internal state and waits for the next input value. In this way, the RNN can take inputs with variable lengths, such as text sequences or time series.</span></li>
</ul>
<p>The following is an example of the two types of networks:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1093 image-border" src="assets/5659f4a0-a7f8-4824-8f4f-06316abc9594.png" style="width:33.17em;height:6.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Left: Feed-forward network; Right: Recurrent network</div>
<p>Let's assume that, when an operation receives input from more than one operation, we use the element-wise sum to combine the multiple input tensors. Then, we can represent the NN as a series of nested functions/operations. We'll denote a NN operation with <sub><img class="fm-editor-equation" src="assets/02ac42df-f4ea-4c94-baa7-4f4784483f3e.png" style="width:2.75em;height:1.33em;"/></sub>, where<span> </span><em>i</em><span> </span>is some index that helps us differentiate between multiple operations. For example, the equivalent formula for the feed-forward network on the left is as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1efd317b-048c-4ad8-b062-7b8f76925308.png" style="width:43.67em;height:1.83em;"/></p>
<p>The formula for the RNN on the right is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7dea3971-d074-4813-8c6e-813f7564454a.png" style="width:20.92em;height:1.92em;"/></p>
<p>We'll also denote the parameters (weights) of an operation with the same index as the operation itself. Let's take an FC network layer with index <em>l</em>, which takes its input from a previous layer with index <em>l-1</em>. The following are the layer formulas for a single unit and vector/matrix layer representations with layer indexes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fbb5a8e3-b244-48ce-b989-38f6dae8e12b.png" style="width:17.92em;height:7.92em;"/></p>
<p>Now that we're familiar with the full NN architecture, let's discuss the different types of activation functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p>Let's discuss the different types of activation functions, starting with the classics:</p>
<ul>
<li><strong>Sigmoid</strong>: Its output is bounded between 0 and 1 and can be interpreted stochastically as the probability of the neuron activating. Because of these properties, the sigmoid was the most popular activation function for a long time. However, it also has some less desirable properties (more on that later), which led to its decline in popularity. The following diagram shows the sigmoid formula, its derivative, and their graphs (the derivative will be useful when we discuss backpropagation):</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1094 image-border" src="assets/4e410206-7352-455d-80de-cf5fcf8ffcb4.png" style="width:27.08em;height:10.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Sigmoid activation function</div>
<ul>
<li><strong>Hyperbolic tangent</strong> (<strong>tanh</strong>): The name speaks for itself. The principal difference with the sigmoid is that the tanh is in the (-1, 1) range. The following diagram shows the tanh formula, its derivative, and their graphs:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1095 image-border" src="assets/a197e86d-c3e1-4e94-9430-061a91ecc65c.png" style="width:29.42em;height:11.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The hyperbolic tangent activation function</div>
<p>Next, let's focus on the new kids on the block<span>—</span>the *LU (<strong>LU</strong> stands for <strong>linear unit</strong>) family of functions. We'll start with the rectified linear unit (<strong>ReLU</strong>), which was first successfully used in 2011 (<em>Deep Sparse Rectifier Neural Networks</em>, <a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf</a>). The following diagram shows the ReLU formula, its derivative, and their graphs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1097 image-border" src="assets/727a2c5f-91fc-45ce-8943-7d095f3cf119.png" style="width:28.92em;height:11.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">ReLU activation function</div>
<p>As we can see, the ReLU repeats its input when <strong>x &gt; 0</strong> and stays at 0 otherwise. This activation has several important advantages over sigmoid and tanh:</p>
<ul>
<li>Its derivative helps prevent vanishing gradients (more on that in the <em>Weights initialization</em> section). Strictly speaking, the derivative ReLU at value 0 is undefined, which makes the ReLU only semi-differentiable (more information about this can be found at <a href="https://en.wikipedia.org/wiki/Semi-differentiability">https://en.wikipedia.org/wiki/Semi-differentiability</a>). But in practice, it works well enough.</li>
</ul>
<ul>
<li>It's idempotent<span>—</span>if we pass a value through an arbitrary number of ReLU activations, it will not change; for example, <em>ReLU(2) = 2</em>, <em>ReLU(ReLU(2)) = 2</em>, and so on. This is not the case for a sigmoid, where the value is <em>squashed</em> on each pass: <em>σ(</em><span><em>σ(2)) = 0.707</em>.</span> The following is an example of the activation of three consecutive sigmoid activations:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1098 image-border" src="assets/9b449d44-5940-42ea-888a-5500e5cc386d.png" style="width:19.58em;height:13.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Consecutive sigmoid activations "squash" the data</div>
<p style="padding-left: 60px"><span>The idempotence of ReLU makes it theoretically possible to create networks with more layers compared to the sigmoid.</span></p>
<ul>
<li>It creates sparse activations<span>—</span>let's assume that the weights of the network are initialized randomly through normal distribution. Here, there is a 0.5 chance that the input for each ReLU unit is &lt; 0. Therefore, the output of about half of all activations will also be 0. The sparse activations have a number of advantages, which we can roughly summarize as the Occam's razor in the context of NNs<span>—</span>it's better to achieve the same result with a simpler data representation than a complex one. </li>
<li>It's faster to compute in both the forward and backward passes. </li>
</ul>
<p>However, during training, the network weights can be updated in such a way that some of the ReLU units in a layer will always receive inputs smaller than 0, which in turn will cause them to permanently output 0 as well. This phenomenon is known as <strong>dying</strong> ReLUs. To solve this, a number of ReLU modifications have been proposed. The following is a non-exhaustive list:</p>
<ul>
<li><strong>Leaky ReLU</strong>: When the input is larger than 0, leaky ReLU repeats its input in the same way as the regular ReLU does. However, when <strong>x &lt; 0</strong>, the leaky ReLU outputs <em>x</em> multiplied by some constant <em>α<span> (0 &lt; </span><span>α &lt; 1)</span></em>, instead of 0. The f<span>ollowing diagram shows the leaky ReLU formula, its derivative, and their graphs for α=0.2:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1099 image-border" src="assets/1a2ece75-3d35-421d-b429-0a00a302ff39.png" style="width:29.42em;height:11.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Leaky ReLU activation function</div>
<ul>
<li><strong>Parametric ReLU</strong> (<strong>PReLU</strong>, <em>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</em>, <a href="https://arxiv.org/abs/1502.01852">https://arxiv.org/abs/1502.01852</a>): This activation is the same as the leaky ReLU, but the parameter <span>α is tunable and is adjusted during training. </span></li>
<li><strong>Exponential linear units</strong> (<strong>ELU</strong>, <em>Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</em>, <a href="https://arxiv.org/abs/1511.07289">https://arxiv.org/abs/1511.07289</a>): <span>When the input is larger than 0, ELU repeats its input in the same way</span> as ReLU does. However, when <span><em>x &lt; 0</em>, the ELU output becomes <sub><img class="fm-editor-equation" src="assets/7facdd5f-4b7c-496b-9faa-7325f05759f9.png" style="width:7.08em;height:1.08em;"/></sub>, where α is a tun</span>able parameter. The follo<span>wing diagram shows the ELU formula, its derivative, and their graphs for α=0.2:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1100 image-border" src="assets/4dec517e-6475-4ca9-acc6-f6beecdbcdd2.png" style="width:30.42em;height:12.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">ELU activation function</div>
<ul>
<li><strong>Scaled exponential linear units</strong> (<strong>SELU</strong>, <em>Self-Normalizing Neural Networks</em>, <a href="https://arxiv.org/abs/1706.02515">https://arxiv.org/abs/1706.02515</a>): This activation is similar to ELU, except that the output (both smaller and larger than 0) is scaled with an additional training parameter, λ. The SELU is part of a larger concept called self-normalizing NNs (SNNs), which is described in the source paper. <span>The following is the SELU formula:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dde66d94-9f7d-42ba-892c-7985c308e100.png" style="width:12.33em;height:2.50em;"/></p>
<p>Finally, we'll mention the <strong>softmax</strong>, which is the activation function of the output layer in classification problems. Let's assume that the output of the final network layer is a vector, <sub><img class="fm-editor-equation" src="assets/1d8758ff-5740-4a18-9ced-3c25f706c7e2.png" style="width:7.83em;height:1.08em;"/></sub>, where each of the <em>n</em> components represents the probability that the input data belongs to one of <em>n</em> possible classes. Here, the softmax output for each of the vector components is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9f9f8dd8-dcbe-4f3f-96fd-54a569fe8803.png" style="width:10.75em;height:3.17em;"/></p>
<p>The denominator in this formula acts as a normalizer. The softmax output has some important properties:</p>
<ul>
<li>Every value <em>f(z<sub>i</sub>)</em> is in the [0, 1] range.</li>
<li>The total sum of values of <strong>z</strong> is equal to 1: <sub><img class="fm-editor-equation" src="assets/8c9d0b4e-4c25-4547-b7b5-16f7f149ef92.png" style="width:5.50em;height:1.33em;"/>.</sub></li>
<li>An added bonus (in fact, obligatory) is that the function is differentiable.</li>
</ul>
<p>In other words, we can interpret the softmax output as a probability distribution of a discrete random variable. However, it also has one more subtle property. Before we normalize the data, we transform each vector component exponentially with <sub><img class="fm-editor-equation" src="assets/2ddaa9ce-bb9f-4662-b18b-864359b51d72.png" style="width:1.33em;height:1.08em;"/></sub>. Let's imagine that two of the vector components are <em>z<sub>1</sub> = 1</em> and <em>z<sub>2</sub><span> </span>= 2</em>. Here, we would have <em>exp(1) = 2.7</em> and <em>exp(2) = 7.39</em>. As we can see, the ratios between the components before and after the transformation are very different<span>—</span>0.5 and 0.36. In effect, the softmax increases the probability of the higher scores compared to lower ones. </p>
<p>In the next section, we'll shift our attention from the building blocks of the NN and focus on its entirety instead. More specifically, we'll demonstrate how NNs can approximate any function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The universal approximation theorem</h1>
                </header>
            
            <article>
                
<p>The universal approximation theorem was first proved in 1989 for a NN with sigmoid activation functions and then in 1991 for NNs with arbitrary non-linear activation functions. It states that <span>any continuous function on compact subsets of</span> <sub><span><img class="fm-editor-equation" src="assets/95737d8a-6d84-4a7a-89d1-ec9b27ea62b0.png" style="width:1.50em;height:1.08em;"/></span></sub><span> </span><span>can be approximated to an arbitrary degree of accuracy by a feedforward NN with at least one hidden layer with a finite number of units and a non-linear activation. Although a NN with a single hidden layer won't perform well in many tasks, the theorem still tells us that there are no theoretical insurmountable limitations in terms of NNs. The formal proof of the theorem is too complex to be explained here, but we'll attempt to provide an intuitive explanation using some basic mathematics.</span></p>
<div class="packt_tip"><span>The idea for the following example was inspired by Michael A. Nielsen's book <em>Neural Networks and </em><em>Deep Learning </em>(<a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a>).<br/></span></div>
<p><span>We'll implement a NN that approximates the boxcar function (shown on the right in the following diagram), which is a simple type of step function. </span>Since <span>a series of step functions can approximate any continuous function on a compact subset of</span><span> </span><em>R</em><span>, this will give us an idea of why the universal approximation theorem holds:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-1101 image-border" src="assets/c6e00a40-0299-4a5d-b3f6-62404071d039.png" style="width:31.17em;height:10.75em;"/></div>
<div style="padding-left: 60px" class="packt_figref CDPAlignCenter CDPAlign">The diagram on the left depicts continuous function approximation with a series of step functions, while the diagram on the right illustrates a single boxcar step function.</div>
<p>To understand how this approximation works, we'll start with a single unit with a single scalar input <em>x</em> and sigmoid activation. The following is a visualization of the <span>unit</span> and its equivalent formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1102 image-border" src="assets/5ba8fa8c-d68c-46bf-babc-7b4593d951fd.png" style="width:23.58em;height:5.50em;"/></p>
<p>In the following diagrams, we can see the graph of the formula for different values of <em>b</em> and <em>w</em> for inputs in the range of [-10: 10]:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1103 image-border" src="assets/79c3e582-7a3f-40ae-b2bd-b65568584257.png" style="width:29.50em;height:18.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The neuron output based on different values of <em>w</em> and <em>b</em>. The network input <em>x</em> is represented on the x axis.</div>
<p>Upon closer inspection of the formula and the graph, we can see that the steepness of the sigmoid function is determined by the weight, <em>w</em>. Also, the translation of the function along the <em>x</em> axis is determined by the formula <em>t = -b/w</em>. Let's discuss the different scenarios in the preceding diagram:</p>
<ul>
<li>The top-left graph shows the regular sigmoid.</li>
<li>The top-right <span>graph</span> demonstrates that a large weight <em>w</em> amplifies the input <em>x</em> to a point, where the unit output resembles threshold activation.</li>
<li>The bottom-left <span>graph</span> shows how the bias <em>b</em> translates the unit activation along the <em>x </em>axis.</li>
<li>The bottom-right <span>graph</span> shows that we can simultaneously reverse the activation with negative weight <em>w</em> and translate the activation along the <em>x </em>axis with the bias <em>b</em>.</li>
</ul>
<p>We can intuitively see that the preceding graphs contain all the ingredients of the box function. We can combine the different scenarios with the help of a NN with one hidden layer, which contains two of the aforementioned units. The following diagram shows the network architecture, along with the weights and biases of the units, as well as the box function that's produced by the network:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1104 image-border" src="assets/617c2e78-f676-443e-af6d-f35cb87771f8.png" style="width:30.75em;height:11.92em;"/></p>
<p>Here's how it works:</p>
<ul>
<li>First, the top unit activates for the upper step of the function and stays active. </li>
<li>The bottom unit activates afterward for the bottom step of the function and stays active. The outputs of the hidden units cancel each other out because of the weights in the output layer, which are the same but with opposite signs.</li>
<li>The weights of the output layer determine the height of the boxcar rectangle. </li>
</ul>
<p>The output of this network isn't 0, but only in the (-5, 5) interval. Therefore, we can approximate additional boxes by adding more units to the hidden layer in a similar manner.</p>
<p>Now that we're familiar with the structure of a NN, let's focus on the training process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training NNs</h1>
                </header>
            
            <article>
                
<p><span>In this section, we'll define training a NN as the process of adjusting its parameter</span>s (weights) <em>θ</em> i<span>n a way that minimizes the cost function </span><em>J(θ)</em><span>. The cost function is some performance measurement over a training set that consists of multiple samples, represented as vectors. Each vector has an associated label (supervised learning). Most commonly, the cost function measures the difference between the network output and the label.</span></p>
<p>We'll start this section with a short recap of the gradient descent optimization algorithm. If you're already familiar with it, you can skip this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient descent</h1>
                </header>
            
            <article>
                
<p>For the purposes of this section, we'll use a NN with a single regression output and <strong>mean square error</strong> (<strong>MSE</strong>) cost function, which is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e50aabbd-24c7-4044-8422-09de5c713e04.png" style="width:17.42em;height:3.83em;"/></p>
<p>Here, we have the following:</p>
<ul>
<li><em>f<sub>θ</sub></em>(<strong>x</strong><sup>(<em>i</em>)</sup>) is the output of the NN<span>.</span></li>
<li><em>n</em> is the total number of samples in the training set.</li>
<li><strong>x</strong><sup>(<em>i</em>)</sup> are the vectors for the training samples, where the superscript <em>i</em> indicates the <em>i</em>-th sample of the dataset. We use superscript because <strong>x</strong><sup>(<em>i</em>)</sup> is a vector and the subscript is reserved for each of the vector components. For example, <sub><img class="fm-editor-equation" src="assets/6a45d5e7-f486-48b8-b2aa-69e1c8f94188.png" style="width:1.00em;height:1.08em;"/></sub> is the <em>j</em>-th component of the <em>i</em>-th training sample.</li>
<li><em>t</em><sup>(<em>i</em>)</sup> is the label associated with sample <strong>x</strong><sup>(<em>i</em>)</sup>.</li>
</ul>
<div class="packt_tip"><span>We shouldn't confuse the <em>(i)</em> superscript index of the <em>i</em>-th training sample with the <em>(l)</em> superscript, which represents the layer index of the NN. We'll only use the (<em>i</em>) sample index notation in the <em>Gradient descent</em> and <em>Cost functions</em> sections, and elsewhere we'll use the <em>(l)</em> notation for the layer index<em>.</em></span></div>
<p>First, gradient descent computes the derivative (gradient) of <em>J(θ)</em> with respect to all the network weights. The gradient gives us an indication of how <em>J(θ)</em><span> changes with respect to ev</span>ery weight. Then, the algorithm uses this information to update the weights in a way that will minimize<span> </span><em>J(θ)</em> in future occurrences of the same input/target pairs. The goal is to gradually reach the global minimum of the cost function. The following is a visualization of gradient descent for MSE and a NN with a single weight:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1105 image-border" src="assets/7a7fada6-aac8-4594-b4da-9e415a4cf357.png" style="width:32.75em;height:22.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A visualization of gradient descent for MSE</div>
<p>Let's go over the step-by-step execution of gradient descent:</p>
<ol>
<li>Initialize the network weights <em>θ</em> with random values<em>.</em></li>
<li>Repeat until the cost function falls below a certain threshold:
<ol>
<li>Forward pass: compute the MSE <em>J(θ)</em> cost function for all the samples of the training set using the preceding formula.</li>
<li>Backward pass: compute the derivative of <em>J(θ)</em> with respect to all the network weights using the chain rule:</li>
</ol>
</li>
</ol>
<p style="padding-left: 30px"><img src="assets/d391519c-5add-4895-b0ed-536e1ced832c.png" style="width:43.75em;height:3.33em;"/></p>
<div class="packt_infobox"><span>Let's analyze the derivative </span><sub><img class="fm-editor-equation" src="assets/cdaeaebd-a07c-4e59-96a0-d899abe8f63a.png" style="width:4.00em;height:1.08em;"/></sub><span>.</span><span> </span><em>J</em><span> </span><span>is a function of </span><em>θ<sub>j</sub></em><span> </span><span>by being a function of the network output. Therefore, it is also a function of the NN function itself, that is, </span><sub><img class="fm-editor-equation" src="assets/fa690c9c-f79a-4e9f-977d-d708662e3522.png" style="width:3.08em;height:1.08em;"/></sub><span>. Then, by following the chain rule, we get</span><span> </span><sub><img class="fm-editor-equation" src="assets/d806c39d-4564-477f-9d35-55fd146e442f.png" style="width:14.33em;height:2.25em;"/></sub>.</div>
<ol>
<li style="list-style-type: none">
<ol start="3">
<li>Use these derivatives to update each of the network weights:</li>
</ol>
</li>
</ol>
<p style="padding-left: 30px" class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0f30d60f-7e45-4be6-9a2c-53cd1d36156f.png" style="width:30.50em;height:3.67em;"/></p>
<p style="padding-left: 120px">Here, η is the learning rate.</p>
<p>Gradient descent updates<span> </span>the weights by accumulating the error across all the training samples. In practice, we would use two of its modifications:</p>
<ul>
<li><strong>Stochastic</strong> (<strong>or online</strong>) <strong>gradient descent<em><span> </span></em></strong><span>(<strong>SGD</strong>) </span>updates the weights after every training sample.</li>
<li><strong>Mini-batch gradient descent </strong>accumulates the error for every <em>n</em><span> </span>samples (one mini-batch)<strong> </strong>and performs one weight update.</li>
</ul>
<p>Next, let's discuss the different cost functions we can use with SGD.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cost functions</h1>
                </header>
            
            <article>
                
<p>Besides MSE, there are also a few other loss functions that are commonly used in regression problems. The following is a non-exhaustive list:</p>
<ul>
<li><strong>Mean absolute error</strong> (<strong>MAE</strong>) is the mean of the absolute differences (not squared) between the network output and the target. The following is the MAE graph and formula:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1106 image-border" src="assets/32429f23-4286-46c2-8d93-85eb26c7c50d.png" style="width:32.50em;height:8.25em;"/></p>
<p style="padding-left: 60px">One advantage of MAE over MSE is that it deals with outlier samples better. With MSE, if the difference of a sample is <sub><img class="fm-editor-equation" src="assets/7e8d1ad7-eed6-4166-a47f-686c534cd901.png" style="width:6.58em;height:1.33em;"/></sub>, it increases exponentially (because of the square). We'll get an outsized weight of this sample compared to the others, which may skew the results. With MAE, the difference is not exponential and this issue is less pronounced.</p>
<p style="padding-left: 60px">On the other hand, the MAE<span> gradient</span> will have the same value until we reach the minimum, where it will become 0 <span>immediately</span>. This makes it harder for the algorithm to anticipate how close the cost function minimum is. Compare this to MSE, where the slope gradually decreases as we get close to the cost minimum. This makes MSE easier to optimize. In conclusion, unless the training data is corrupted with outliers, it is usually recommended to use MSE over MAE.</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><span><strong>Huber loss</strong> attempts to fix the problems of both MAE and MSE by combining their properties. In short, when the absolute difference between the output and the target data falls below the value of a fixed parameter, δ, the Huber loss behaves like MSE. Conversely, when the difference is greater than δ, it resembles MAE. In this way, it is less sensitive to outliers (when the difference is big) and at the same time, the minimum of the function is properly differentiable. The following is the Huber loss graph for three values of δ and its formula for a single training sample, which reflects its dualistic nature:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1107 image-border" src="assets/9e3e7435-9aaa-40b2-a4c7-503bfa747d7a.png" style="width:37.25em;height:9.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Huber loss</div>
<p>Next, let's focus on cost functions for classification problems. The following is a non-exhaustive list:</p>
<ul>
<li><strong>Cross-entropy</strong> loss: We have our work cut out for us here as we already defined cross-entropy in the <em>Information theory</em> section. This loss is usually applied over the output of a softmax function. The two work very well together. First, the softmax converts the network output into a probability distribution. Then, cross-entropy measures the difference between the network output (Q) and the true distribution (P), which is provided as a training label. Another nice property is that the derivative of <em>H(P, Q<sub><span>softmax</span></sub>)</em> is quite straightforward (although the computation isn't):</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d38c6fac-8614-4e00-92a4-b5ca8514070f.png" style="width:21.42em;height:1.67em;"/></p>
<p style="padding-left: 60px">Here, <em>x<sup>(i)</sup>/t<sup>(i)</sup></em> is the <em>i</em>-th input/label training pair.</p>
<ul>
<li><strong>KL Divergence</strong> loss: Like cross-entropy loss, we already did the grunt work in the <em>Information theory</em> <span>section, </span>where we derived the relationship between KL divergence and cross-entropy loss. From their relationship, we can state that if we use either of the two as a loss function, we implicitly use the other one as well.</li>
</ul>
<div class="packt_tip"><span>Sometimes, we may encounter the terms loss function and cost function being used interchangeably. It is usually accepted that they differ slightly. We'll refer to the loss function as the difference between the network output and target dat</span>a for a <strong>single</strong> sa<span>mple of the training set. The cost function is the same thing but is averaged (or summed) over multiple samples (batch) of the training set.</span></div>
<p>Now that we have looked at different cost functions, let's focus on propagating the error gradient through the network with backpropagation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagation</h1>
                </header>
            
            <article>
                
<p>In this section, we'll discuss how to update the network weights in order to minimize the cost function. As we demonstrated in the <em>Gradient descent</em> section, this means finding the derivative of the cost function <em>J(θ)</em> with respect to each network weight<em>. </em>We already took a step in this direction with the help of the chain rule:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2e6ef655-6bb5-4a77-92ab-f54faf307fae.png" style="width:15.50em;height:2.42em;"/></p>
<p>Here, <em>f(θ)</em> is the network output and <em>θ<sub>j</sub></em> is the <em>j</em>-th network weight<em>.</em> In this section, we'll push the envelope further and we'll learn how to derive the NN function itself for all the network weights (hint: chain rule). We'll do this by propagating the error gradient backward through the network (hence the name). Let's start with a few assumptions:</p>
<ul>
<li>For the sake of simplicity, we'll work with a sequential feed-forward NN. Sequential means that each layer takes input from the preceding layer and sends its output to the following layer.</li>
<li>We'll define<span> </span><em>w<sub>ij</sub></em><span> </span>as the weight between the<span> </span><em>i</em><span>-th</span><span> </span>neuron of layer<span> </span><em>l</em> and the<span> </span><em>j-</em>th neuron of layer<span> </span><em>l+1. </em>In other words, <span>we use subscripts</span><span> </span><em>i</em><span> </span><span>and</span><span> </span><em>j</em><span>, where the element with subscript</span><span> </span><em>i</em><span> </span><span>belongs to the preceding layer, which is the layer containing the element with subscript</span><span> </span><em>j</em><span>. </span><span><span>In a multi-layer network, <em>l</em> and <em>l+1</em> can be any two consecutive layers, including input, hidden, and output layers.</span></span></li>
<li>We'll <span>denote the output of the <em>i</em>-th unit of layer l with <sub><img class="fm-editor-equation" src="assets/47df128a-d1f0-403d-9aa3-b8d5351f475b.png" style="width:1.50em;height:1.75em;"/></sub>and the output of the <em>j</em>-th unit of layer l+1 with <sub><img class="fm-editor-equation" src="assets/af8df4bf-f929-49c3-94e8-738416ee0df6.png" style="width:2.17em;height:1.58em;"/></sub>.</span></li>
<li>We'll denote the input to the activation function (that is, the weighted sum of the inputs before activation) of unit <em>j</em> of layer <em>l</em> with <em>a<sub>j</sub><sup>(l)</sup></em>.</li>
</ul>
<p>The following diagram shows all the notations we introduced:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-1108 image-border" src="assets/ad7c70ac-37b4-4ce6-b1e5-7cc5923cebc0.png" style="width:24.33em;height:18.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Here, layer l represents the input, layer l+1 represents the output, and <em>w<sub>ij</sub></em> connects the <em>y<sub>i</sub><sup><sub>(l)</sub></sup></em> activation in layer l to the inputs of the j-th neuron of layer l+1</div>
<p>Armed with this great knowledge, let's get down to business:</p>
<ol>
<li>First, we'll assume that <em>l</em> and <em>l+1</em> are the second-to-last and the last (output) network layers, respectively. Knowing this, the derivative of J with respect to <em>w<sub>ij</sub></em> is as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2b12510e-19c8-4740-82c3-45b27e4270a0.png" style="width:15.58em;height:3.75em;"/></p>
<ol start="2">
<li>Let's focus on <sub><img class="fm-editor-equation" src="assets/3c0fe6b4-4c70-4de4-9a24-c723eafaaea8.png" style="width:5.75em;height:1.75em;"/></sub>. Here, we compute the partial derivative of the weighted sum of the output of layer <em>l</em> with respect to one of the weights, <em>w<sub>ij</sub></em>. As we discussed in the <em>Differential calculus</em> section, in partial derivatives, we'll consider all the function parameters except <em>w<sub>ij</sub></em> constants. When we derive <em>a<sub>j</sub><sup><sub>(l+1)</sub></sup></em>, they all become 0 and we're only left with <sub><img class="fm-editor-equation" src="assets/638298cb-bba0-48fe-923d-28fd64be27e3.png" style="width:8.50em;height:1.33em;"/></sub>. Therefore, we get the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/687176f6-b628-434b-9438-8f8617082829.png" style="width:6.33em;height:3.33em;"/></p>
<ol start="3">
<li>The formula from point 1 holds for any two consecutive hidden layers, <em>l</em> and <em>l+1</em>, of the network. We know that <span><sub><img style="color: #333333;width:8.50em;height:1.33em;" class="fm-editor-equation" src="assets/0faa558f-675d-4698-9454-12d5360204f9.png"/></sub></span><span>,</span><span> </span><span>and we also know that <sub><img class="fm-editor-equation" src="assets/6f4c364f-79c1-45fe-99c9-bc175617827e.png" style="width:5.08em;height:1.33em;"/></sub></span><span> is the derivative of the activation function, which we can calculate (see the</span> <em>Activation functions</em> section<span>)</span><span>. All we need to do is calculate the derivative</span> <span><sub><img style="color: #333333;width:4.00em;height:1.42em;" class="fm-editor-equation" src="assets/bd64bfc9-cd1a-479d-98b5-9b5a61f76faf.png"/></sub></span><span> (recall that, here,</span> <em>l+1</em> <span>is some hidden layer). Let's note that this is the derivative of the error with respect to the activation function in layer</span> <em>l+1</em><span>. </span><span>We can now calculate all the derivatives, starting from the last layer and moving backward, because the following apply:</span></li>
</ol>
<ul>
<li style="padding-left: 30px">We can calculate this derivative for the last layer.</li>
<li style="padding-left: 30px">We have a formula that allows us to calculate the derivative for one layer, assuming that we can calculate the derivative for the next.</li>
</ul>
<ol start="4">
<li>With these points in mind, we get the following equation by applying the chain rule:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9fa73368-1629-499f-aacf-6e8f707914b8.png" style="width:28.42em;height:4.17em;"/></p>
<p style="padding-left: 60px" class="mce-root">The sum over<span> </span><em>j</em><span> </span>reflects the fact that, in the feedforward part of the network, the output<em> <img class="fm-editor-equation" src="assets/178b40b8-5edb-4888-aea4-61ab1d0bdf82.png" style="width:1.50em;height:1.67em;"/> </em>is fed to all the neurons in layer <em>l+1</em>. Therefore, they all contribute to <em>y<sub>i</sub><sup><sub>(l)</sub></sup></em><sub><span> </span></sub>when the error is propagated backward.</p>
<p style="padding-left: 60px">Once again, we can calculate <sub><img class="fm-editor-equation" src="assets/3e0a68a5-2579-4d38-8687-6634162d3520.png" style="width:5.50em;height:1.42em;"/></sub>. Following the same logic that we followed in <em>step 3</em>, we can compute that <sub><img class="fm-editor-equation" src="assets/bc64c3d6-ee8a-4c98-b0a3-28c92ea47bde.png" style="width:7.17em;height:1.42em;"/></sub>. Therefore, once we know that <sub><img class="fm-editor-equation" src="assets/a35b6e03-6a0e-4b88-abeb-f5a509057bd0.png" style="width:4.00em;height:1.42em;"/></sub> <span>, </span>we can calculate <sub><img class="fm-editor-equation" src="assets/61ecb327-6a60-4342-bb49-e1bdb135e3d5.png" style="width:3.33em;height:1.42em;"/></sub>. Since we can calculate <img class="fm-editor-equation" src="assets/85b5edb6-75f7-467d-80fb-a18b93c3df98.png" style="width:3.92em;height:1.42em;"/> for the last layer, we can move backward and calculate <sub><img class="fm-editor-equation" src="assets/3c0ace54-6609-4270-9270-3fa338a59359.png" style="width:3.42em;height:1.42em;"/></sub> for any layer, and therefore <sub><img class="fm-editor-equation" src="assets/944cd03c-040e-45c8-9bb1-1547d1008bd7.png" style="width:4.25em;height:1.33em;"/></sub> for any layer.</p>
<ol start="5">
<li>To summarize, let's say we have a sequence of layers where the following applies:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fe64b72a-110b-4725-9b1e-36259211544c.png" style="width:8.50em;height:1.42em;"/></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">Here, we have the following fundamental equations:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/12977aa0-3f9f-4e34-80ec-35760bb18f06.png" style="width:15.75em;height:4.17em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9fa73368-1629-499f-aacf-6e8f707914b8.png" style="width:28.42em;height:4.17em;"/></p>
<p style="padding-left: 60px">By using these two equations, we can calculate the derivatives for the cost with respect to each layer.</p>
<ol start="6">
<li>If we set <img class="fm-editor-equation" src="assets/cdeb3c79-172c-4362-bd0d-0a524fb30cbe.png" style="width:9.83em;height:3.50em;"/> , then <span>δ</span><em><sub>j</sub><sup><sub>(l+1)</sub></sup></em> represents the variation in cost with respect to the activation value, and we can think of δ<em><sub>j</sub><sup><sub>(l+1)</sub></sup></em> <span>as the error at</span><span> </span><span>neuron <em>y<sub>j</sub><sup><sub>(l+1)</sub></sup></em></span><span>. </span>We can rewrite these equations as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a622a08e-7657-4b31-8480-de8e99875315.png" style="width:35.33em;height:4.08em;"/></p>
<p style="padding-left: 60px">Following this, we can write the following equation:</p>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/19ad376c-3380-41ba-8234-1c7ba5b8bdbe.png" style="width:16.08em;height:3.75em;"/></p>
<p style="padding-left: 60px">These two equations provide us with an alternative view of backpropagation since there is a variation in cost with respect to the activation value. They provide us with a<span> way to calculate the variation for any layer <em>l</em> once we know the variation for the following layer, <em>l+1</em>.</span></p>
<ol start="7">
<li>We can combine these equations to show the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6e2b1ad1-9a70-419e-acb7-ad1b25411f4b.png" style="width:13.83em;height:3.00em;"/></p>
<ol start="8">
<li>The updated rule for the weights of each layer is given by the following equation:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b266473d-8d0e-4086-9f57-69b057f810d5.png" style="width:12.75em;height:2.08em;"/></p>
<p>Now that we're familiar with backpropagation, let's discuss another component of the training process: weight initialization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weight initialization</h1>
                </header>
            
            <article>
                
<p>One key component of training deep networks is the random weight initialization. This matters because some activation functions, such as sigmoid and ReLU, produce meaningful outputs and gradients if their inputs are within a certain range.</p>
<p>A famous example is the vanishing gradient problem. To understand it, let's take an FC layer with sigmoid activation <span>(this example is also valid for tanh)</span>. We saw the sigmoid's graph (blue) and its derivative (green) in the <em>Activation functions</em> section. If the weighted sum of the inputs falls roughly outside the <span>(-5, 5)</span> range, the sigmoid activation will be effectively 0 or 1. In essence, it saturates. This is visible during the backward pass where we derive the sigmoid (the formula is <em>σ' = </em><span><em>σ(1 - σ)</em>)</span>. We can see that the derivative is larger than 0 within the same (-5, 5) range of the input. Therefore, whatever error we try to propagate back to the previous layers, it will vanish if the activation doesn't fall within this range (hence the name).</p>
<div class="packt_tip">Besides the tight meaningful range of the sigmoid derivative, let's note that, even under the best conditions, its maximum value is 0.25. When we propagate the gradient through the sigmoid derivative, it will be four times smaller a<span>t best</span> once it passes through. Because of this, the gradient may vanish in just a few layers, even if we don't fall outside the desired range. This is one of the major disadvantages of sigmoid over the *LU family of functions, where the gradient is 1 in most cases.</div>
<p>One way to solve this problem is to use *LU activations. But even so, it still makes sense to use better weight initialization since it can speed up the training process. One popular technique is the Xavier/<span>Glorot initializer (often found under either</span> of the two names: <span><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a>). In short, this technique takes the number of input and output connections of the unit into account. There are two variations:</span></p>
<ul>
<li><span><strong>Xavier uniform initializer</strong>, which draws samples from a uniform distribution in the range [-a, a]. The parameter, a, is defined as follows:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span><img class="fm-editor-equation" src="assets/1d4472a0-1575-4512-9897-6b8d20a85b7d.png" style="width:9.50em;height:4.17em;"/></span></p>
<p style="padding-left: 60px">Here, <em>n<sub>in</sub></em> and <em>n<sub>out</sub></em> are the number of inputs and outputs, respectively (that is, the number of units that send their output to the current unit and the number of units the current unit sends its output to). </p>
<ul>
<li><span><strong>Xavier normal initializer</strong>, which draws samples from a nor</span>mal distribution (see the <span><em>Probability distributions</em></span> section<span>) with a mean of 0 and variance as follows:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span><img class="fm-editor-equation" src="assets/3391fd8c-4eb8-4a03-a3f3-a554a21786e3.png" style="width:13.67em;height:3.33em;"/></span></p>
<p>The Xavier/Glorot initialization is recommended for sigmoid or tanh activations functions. The paper <em>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</em> (<a href="https://arxiv.org/abs/1502.01852">https://arxiv.org/abs/1502.01852</a>) proposes a similar technique that's better suited for ReLU activations. Again, there are two variations:</p>
<ul>
<li><span><strong>He uniform initializer</strong>, which draws samples from a uniform distribution in the range [-a, a]. The parameter, a, is defined as follows:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f7816ef0-6745-4756-b693-71c54dc13181.png" style="width:5.58em;height:3.25em;"/></p>
<ul>
<li><span><strong>He normal initializer</strong>, which draws samples from a normal distribution with a mean of 0 and variance as follows:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/798e9c12-982e-491f-b9e7-16d7a31ea90c.png" style="width:8.83em;height:3.08em;"/></p>
<p style="padding-left: 60px">The ReLU output is always 0 when the input is negative. If we assume that the initial inputs of the ReLU units are centered around 0, half of them will produce 0 outputs. The He initializer compensates this by increasing the variance twice, compared to the Xavier initialization.</p>
<p>In the next section, we'll discuss some improvements in the weight update rule over the standard SGD. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SGD improvements</h1>
                </header>
            
            <article>
                
<p><span>We'll start with</span> <strong>momentum</strong><span>, which extends vanilla SGD by adjusting the current weight update with the values of the previous weight updates. That is, if the weight update at step</span> <em>t-1</em> <span>was big, it will also increase the weight update of step</span> <em>t</em><span>. We can explain momentum with an analogy. Think of the </span><span>loss function surface as the surface of a hill. Now, imagine that we are holding a ball at the top of the hill (maximum). If we drop the ball, thanks to the Earth's gravity, it will start rolling toward the bottom of the hill (minimum). The more distance it travels, the more its speed will increase. In other words, it will gain momentum (hence the name of the optimization). </span></p>
<p><span>Now, let's look at how to implement momentum in the weight update rule. Recall the update rule that we introduced in</span> the <em>Gradient descent</em> secti<span>on, that is, <sub><img class="fm-editor-equation" src="assets/2547d3e8-4eab-46b2-be9f-56163d2f81b2.png" style="width:9.25em;height:1.17em;"/></sub>. Let's assume that we are at step <em>t</em> of the training process:</span></p>
<ol>
<li>First, we'll calculate the current weight update value <em>v<sub>t</sub></em> by also including the <strong>velocity</strong> of the previous update <em>v<sub>t-1</sub></em>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ce482d72-124d-4535-93d0-de41f1f1c0c6.png" style="width:13.75em;height:1.50em;"/> </p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">Here, μ is a hyperparameter <span>in the [0:1] range</span> called the momentum rate. <em>v<sub>t</sub></em> is initialized as 0 during the first iteration.</p>
<ol start="2">
<li>Then, we perform the actual weight update:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1716068c-45f5-4605-b5c7-3c9b42665c54.png" style="width:7.08em;height:1.50em;"/></p>
<p>An improvement over the basic momentum is the <strong>Nesterov momentum</strong>. It relies on the observation that the momentum from step <em>t-1</em> may not reflect the conditions at step <em>t</em>. For example, let's say that the gradient at <em>t-1</em> is steep and therefore the momentum is high. However, after the <em>t-1</em><span> </span>weight update, we actually reach the cost function minimum and require only a minor weight update at <em>t</em>. Despite that, we'll still get the large momentum from <em>t-1</em>, which may lead the adjusted weight to jump over the minimum. Nesterov momentum proposes a change in the way we compute the velocity of the weight update. We'll calculate <em>v<sub>t</sub></em> based on the gradient of the cost function that's computed by the potential future value of the weight <em>θ<sub>j</sub></em>. The following is the updated velocity formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/03baa9fb-5d82-45f6-8740-095ed92c6f88.png" style="width:19.83em;height:1.50em;"/></p>
<p><span>If the momentum at <em>t-1</em> is incorrect with respect to <em>t</em>, the modified gradient will compensate for this error in the same update step.</span></p>
<p>Next, let's discuss the Adam adaptive learning rate algorithm (<em>Adam: A Method for Stochastic Optimization</em>, <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>). It calculates individual and adaptive learning rates for every weight based on previous weight updates (momentum). Let's see how that works:</p>
<ol>
<li>First, we need to compute the first moment (or mean) and the <span>second moment (or variance) of the gradient:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/274ca96c-8518-4c6e-b03e-b1a12f9d7993.png" style="width:16.92em;height:6.17em;"/></p>
<p style="padding-left: 60px">Here, β<sub>1</sub> and β<sub>2</sub> are hyperparameters with default values of 0.9 and 0.999, respectively. <em>m<sub>t</sub></em> and <em>v<sub>t</sub></em> act as moving-average values of the gradient, somewhat similar to momentum. They are initialized with 0 during the first iteration.</p>
<ol start="2">
<li>Since <em>m<sub>t</sub></em> and <em>v<sub>t</sub></em> start as 0, they will have a bias toward 0 in the initial phase of the training. For example, let's say that, at <em>t=1, β1 = 0.9</em> and <img src="assets/3d8d6c89-32c0-4cb3-b1f4-92cd4aec489a.png" style="width:4.33em;height:1.17em;"/> = 10. Here, <em>m1 = 0.9 * 0 + (1 - 0.9) * 10 = 1</em>, which is a lot less than the actual gradient of 10. To compensate for this bias, we'll compute the bias-corrected versions of <em>m<sub>t</sub></em> and <em>v<sub>t</sub></em>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8f26742b-e907-4cd6-8414-31dd3745e614.png" style="width:8.33em;height:6.50em;"/></p>
<ol start="3">
<li>Finally, we need to perform the weight update using the following formula:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f47df8a1-7ac4-49a8-8a56-a0337c443779.png" style="width:11.42em;height:3.33em;"/></p>
<p style="padding-left: 60px"><span>Here, η is the learning rate and ε is some small value to prevent division by 0.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We started this chapter with a tutorial on the mathematical apparatus that forms the foundation of NNs. Then, we recapped on NNs and their architecture. Along the way, we tried to explicitly connect the mathematical concepts with the various components of the NNs. <span>We paid special attention to the various types of activation functions. </span>Finally, we took a comprehensive look at the NN training process. We discussed gradient descent<span>, cost functions</span>, backpropagation, weights initialization, and SGD optimization techniques.</p>
<p>In the next chapter, we'll discuss the intricacies of convolutional networks and their applications in the computer vision domain. </p>


            </article>

            
        </section>
    </body></html>