- en: TensorFlow Graph Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most concise and complete explanation of what TensorFlow is can be found
    on the project home page ([https://www.tensorflow.org/](https://www.tensorflow.org/))
    and it highlights every important part of the library. TensorFlow is an open source
    software library for high-performance numerical computation. Its flexible architecture
    allows easy deployment of computation across a variety of platforms (CPUs, GPUs,
    and TPUs), from desktops to clusters of servers, to mobile and edge devices. Originally
    developed by researchers and engineers from the Google Brain team within Google's
    AI organization, it comes with strong support for machine learning and deep learning,
    and the flexible numerical computation core is used across many other scientific
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow''s strengths and most important features can be summarized in the
    following three points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High-performance numerical computation library**: TensorFlow can be used
    in many different applications just by importing it. It is written in C++ and
    it offers bindings for several languages. The most complete, high-level, and widely
    used binding is the Python one. TensorFlow is a high-performance computational
    library that can be used in several domains (not only machine learning!) to execute
    numerical computation efficiently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible architecture**: TensorFlow has been designed to work on different
    hardware (GPUs, CPUs, and TPUs) and different network architectures; its abstraction
    level is so high that (almost) the same code can train a model on a single computer
    or a cluster of machines in a data center.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production-oriented**: TensorFlow has been developed by the Google Brain
    team as a tool for developing and serving machine learning models at scale. It
    was designed with the idea of simplifying the whole design-to-production pipeline;
    the library already comes with several APIs ready to be used in a production environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow, thus, is a numerical computational library—keep that in mind. You
    can use it to perform any mathematical operation it offers, leveraging the power
    of all the hardware you have at your disposal, without doing anything ML-related.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you''ll learn everything you need to know about the TensorFlow
    architecture: what TensorFlow is, how to set up your environment to test both
    versions 1.x and 2.0 to see the differences, and you will learn a lot about how
    a computational graph is built; in the process, you will also learn how to use
    TensorBoard to visualize graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you'll (finally!) start reading some code. Please don't just
    read the code and the related explanations; write all the code you read and try
    to execute it. Follow the instructions on how to set up the two virtual environments
    we need and get your hands dirty with the code. At the end of this chapter, you'll
    be familiar with the fundamentals of TensorFlow that are valid for every TensorFlow
    version.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataflow graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model definition and training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with the graph using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand the structure of TensorFlow, all the examples presented
    in this chapter will use the latest TensorFlow 1.x release: 1.15; however, we
    will also set up everything needed to run TensorFlow 2.0 since we are going to
    use it in the next chapter, [Chapter 4](655b734e-1636-4e11-b944-a71fafacb977.xhtml),
    *TensorFlow 2.0 Architecture.*'
  prefs: []
  type: TYPE_NORMAL
- en: All the examples presented in this book specify the version of TensorFlow to
    use when running it. Being a library, we can just install it specifying the version
    we need. Of course, having two different versions of the same library installed
    on one system would be a mistake. In order to be able to switch between versions,
    we are going to use two different *Python virtual environments.*
  prefs: []
  type: TYPE_NORMAL
- en: 'An explanation of what a **virtual environment** (**virtualenv**) is and why
    it perfectly fits our needs follows here, from the official introduction to virtual
    environments ([https://docs.Python.org/3/tutorial/venv.html#introduction](https://docs.Python.org/3/tutorial/venv.html#introduction)):'
  prefs: []
  type: TYPE_NORMAL
- en: Python applications will often use packages and modules that don't come as part
    of the standard library. Applications will sometimes need a specific version of
    a library, because the application may require that a particular bug has been
    fixed or the application may be written using an obsolete version of the library's
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: This means it may not be possible for one Python installation to meet the requirements
    of every application. If application A needs version 1.0 of a particular module,
    but application B needs version 2.0, then the requirements are in conflict and
    installing either version 1.0 or 2.0 will leave one application unable to run.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this problem is to create a virtual environment, a self-contained
    directory tree that contains a Python installation for a particular version of
    Python, plus a number of additional packages.
  prefs: []
  type: TYPE_NORMAL
- en: Different applications can then use different virtual environments. To resolve
    the earlier example of conflicting requirements, application A can have its own
    virtual environment with version 1.0 installed, while application B has another
    virtual environment with version 2.0\. If application B requires a library to
    be upgraded to version 3.0, this will not affect application A's environment.
  prefs: []
  type: TYPE_NORMAL
- en: In order to create virtual environments in the easiest way, we use `pipenv`: the
    definitive tool for `virtualenv` creation and management; follow the installation
    guide at [https://github.com/pypa/pipenv](https://github.com/pypa/pipenv). Being
    a cross-platform tool, using Windows, Mac, or Linux makes no difference. Having
    installed `pipenv`, we just need to create these two separate virtual environments
    for the two different TensorFlow versions.
  prefs: []
  type: TYPE_NORMAL
- en: We'll install TensorFlow without GPU support because `tensorflow-gpu` depends
    on CUDA and a recent NVIDIA GPU is required to use the GPU acceleration provided
    by the `CUDA` package. If you own a recent NVIDIA GPU, you can install the `tensorflow-gpu` package,
    but you have to take care to install the version of CUDA required by the TensorFlow
    package you are installing (TensorFlow 2.0 and TensorFlow 1.15 require CUDA 10).
    Moreover, you have to ensure that both the `tensorflow-gpu` packages installed
    in the `virtualenvs` depend on the same CUDA version (CUDA 10); otherwise, one
    installation will work and the other won't. However, if you stick with versions
    2.0 and 1.15 of TensorFlow, both are compiled with CUDA 10 support, hence, installing
    them in their GPU version and having CUDA 10 installed on your system should work
    perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 1.x environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a folder, `tf1`, step inside it, and run the following commands to create
    an environment, activate it, and install TensorFlow using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using Python 3.7 is not strictly mandatory; TensorFlow comes with support for
    Python 3.5, 3.6, and 3.7. Hence, if you are using a distribution/operating system
    that ships an older Python version, such as Python 3.5, you just have to change
    the Python version in the `pipenv` command.
  prefs: []
  type: TYPE_NORMAL
- en: So far, so good. Right now, you are in an environment that uses Python 3.7 and
    has `tensorflow==1.15` installed. In order to create a new environment for TensorFlow
    2.0, we have to first exit from the `pipenv shell` created for us, which we're
    currently using. As a general rule, to switch from one `virtualenv` to another,
    we activate it using `pipenv shell` and deactivate it, exiting the session from
    the shell, by typing `exit`.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, before creating the second virtual environment, just close the currently
    running shell by typing `exit`.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0 environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the same manner as with the TensorFlow 1.x environment, create a folder, `tf2`,
    step inside it, and run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the rest of the book, whether the TensorFlow 1.x or 2.0 environment should
    be used is indicated by the `(tf1)` or `(tf2)` symbol before the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now start digging inside the TensorFlow structure, analyzing, and describing
    something that was explicit in TensorFlow 1.x and hidden in TensorFlow 2.0 (but
    still present!): the data flow graph. Since the analysis that follows looks at
    the details of how a graph is built and how various low-level operations can be
    used to build graphs, almost every code snippet uses the TensorFlow 1.x environment.
    If you are interested in version 2.0 only because you already know and use TensorFlow
    1.x, you can skip this section; although, reading it is also recommended for the
    experienced user.'
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to use only the `tensorflow 2.0` environment and replace every
    call to the `tensorflow` package, `tf`, using the compatibility module present
    in TensorFlow 2; therefore, to have a single `(tf2)` environment, you must replace
    every `tf.` with `tf.compat.v1.` and disable eager execution by adding the `tf.compat.v1.disable_eager_execution()` line
    just after importing the TensorFlow package.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our environment setup complete, let's move on to dataflow graphs
    and learn how to start working on some practical code.
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to be a highly efficient, flexible, and production-ready library, TensorFlow
    uses dataflow graphs to represent computation in terms of the relationships between
    individual operations. Dataflow is a programming model widely used in parallel
    computing and, in a dataflow graph, the nodes represent units of computation while
    the edges represent the data consumed or produced by a computation unit.
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the previous chapter, [Chapter 2](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml),
    *Neural Networks and Deep Learning, *representing computation using graphs comes
    with the advantage of being able to run the forward and backward passes required
    to train a parametric machine learning model via gradient descent, applying the
    chain rule to compute the gradient as a local process to every node; however,
    this is not the only advantage of using graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reducing the abstraction level and thinking about the implementation details
    of representing computation using graphs brings the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelism**: Using nodes to represent operations and edges that represent
    their dependencies, TensorFlow is able to identify operations that can be executed
    in parallel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computation optimization**: Being a graph, a well-known data structure, it
    is possible to analyze it with the aim of optimizing execution speed. For example,
    it is possible to detect unused nodes in the graph and remove them, hence optimizing
    it for size; it is also possible to detect redundant operations or sub-optimal
    graphs and replace them with the best alternatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Portability**: A graph is a language-neutral and platform-neutral representation
    of computation. TensorFlow uses **Protocol Buffers** (**Protobuf**), which is
    a simple language-neutral, platform-neutral, and extensible mechanism for serializing
    structured data to store graphs. This, in practice, means that a model defined
    in Python using TensorFlow can be saved in its language-neutral representation
    (Protobuf) and then used inside another program written in another language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed execution**: Every graph''s node can be placed on an independent
    device and on a different machine. TensorFlow will take care of the communication
    between the nodes and ensure that the execution of a graph is correct. Moreover,
    TensorFlow itself is able to partition a graph across multiple devices, knowing
    that certain operations perform better on certain devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s describe our first dataflow graph to compute a product and a sum between
    matrices and a vector; save the graphical representation and use TensorBoard to
    visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In these few lines, there are a lot of peculiarities of TensorFlow and its way
    of building a computational graph. This graph represents the matrix product between
    the constant tensor identified by the `A` Python variable and the constant tensor
    identified by the `x` Python variable and the sum of the resulting matrix with
    the tensor identified by the `b` Python variable.
  prefs: []
  type: TYPE_NORMAL
- en: The result of the computation is represented by the `y` Python variable, also
    known as the output of the `tf.add` node named `result` in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note the separation between the concept of a Python variable and a node
    in the graph: we''re using Python only to describe the graph; the name of the
    Python variable means nothing in the graph definition.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we created `tf.summary.SummaryWriter` to save a graphical representation
    of the graph we've built. The `writer` object has been created, specifying the
    path in which to store the representation (`log/matmul`) and a `tf.Graph` object
    obtained using the `tf.get_default_graph` function call that returns the default
    graph since at least one graph is always present in any TensorFlow application.
  prefs: []
  type: TYPE_NORMAL
- en: You can now visualize the graph using TensorBoard, the data visualization tool
    that comes free with TensorFlow. TensorBoard works by reading the log files placed
    in the specified `--logdir` and creates a web server so we're able to visualize
    our graph by using a browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute TensorBoard and visualize the graph, just type the command that
    follows and open a web browser at the address indicated by TensorBoard itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the built graph, as seen in TensorBoard, and
    the detail of the node result. The screenshot allows an understanding of how TensorFlow
    represents the nodes and which features every node has:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca188909-c582-473a-9fbe-16c999310c42.png)'
  prefs: []
  type: TYPE_IMG
- en: The computational graph that describes the operation y = Ax +b. The result node
    is highlighted in red and its details are shown in the right-hand column.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that **we are just describing the graph**—the calls to the TensorFlow
    API are just adding operations (nodes) and connections (edges) among them; there
    is **no computation** performed in this phase. In TensorFlow 1.x, the following
    approach needs to be followed—static graph definition and execution, while this
    is no longer mandatory in 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Since the computational graph is the fundamental building block of the framework
    (in every version), it is mandatory to understand it in depth, since even after
    transitioning to 2.0, having an understanding of what's going on under the hood
    makes the difference (and it helps a lot with debugging!).
  prefs: []
  type: TYPE_NORMAL
- en: The main structure – tf.Graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As stated in the previous section, there''s no relation between the Python
    variables'' name and the names of the nodes. Always keep in mind that TensorFlow
    is a C++ library and we''re using Python to build a graph in an easy way. Python
    simplifies the graph description phase since it even creates a graph without the
    need to explicitly define it; in fact, there are two different ways to define
    a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implicit**: Just define a graph using the `tf.*` methods. If a graph is not
    explicitly defined, TensorFlow always defines a default `tf.Graph`, accessible
    by calling `tf.get_default_graph`. The implicit definition limits the expressive
    power of a TensorFlow application since it is constrained to using a single graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explicit**: It is possible to explicitly define a computational graph and
    thus have more than one graph per application. This option has more expressive
    power, but is usually not needed since applications that need more than one graph
    are not common.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to explicitly define a graph, TensorFlow allows the creation of `tf.Graph`
    objects that, through the `as_default` method, create a context manager; every
    operation defined inside the context is placed inside the associated graph. In
    practice, a `tf.Graph` object defines a namespace for the `tf.Operation` objects
    it contains.
  prefs: []
  type: TYPE_NORMAL
- en: The second peculiarity of the `tf.Graph` structure is its **graph c****ollections**.
    Every `tf.Graph` uses the collection mechanism to store metadata associated with
    the graph structure. A collection is uniquely identified by a key and its content
    is a list of objects/operations. The user does not usually need to worry about the
    existence of a collection since they are used by TensorFlow itself to correctly
    define a graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when defining a parametric machine learning model, the graph must
    know which `tf.Variable` objects are the variables to update during the learning
    phase and which other variables are not part of the model but are something else
    (such as moving the mean/variance computed during the training process—these are
    variables but not trainable). In this case, when, as we will see in the following
    section, a `tf.Variable` is created, it is added by default to two collections:
    the global variable and trainable variable collections.'
  prefs: []
  type: TYPE_NORMAL
- en: Graph definition – from tf.Operation to tf.Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A dataflow graph is the representation of a computation where the nodes represent
    units of computation, and the edges represent the data consumed or produced by
    the computation.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of `tf.Graph`, every API call defines `tf.Operation` (node) that
    can have multiple inputs and outputs `tf.Tensor` (edges). For instance, referring
    to our main example, when calling `tf.constant([[1, 2], [3, 4]], dtype=tf.float32)`,
    a new node (`tf.Operation`) named `Const` is added to the default `tf.Graph` inherited
    from the context. This node returns a `tf.Tensor` (edge) named `Const:0`.
  prefs: []
  type: TYPE_NORMAL
- en: Since each node in a graph is unique, if there is already a node named *Const*
    in the graph (that is the default name given to all the constants), TensorFlow
    will make it unique by appending the suffix '_1', '_2', and so on to the name.
    If a name is not provided, as in our example, TensorFlow gives a default name
    to each operation added and adds the suffix to make them unique in this case too.
  prefs: []
  type: TYPE_NORMAL
- en: The output `tf.Tensor` has the same name as the associated `tf.Operation`, with
    the addition of the *:ID* suffix. The *ID* is a progressive number that indicates
    how many outputs the operation produces. In the case of `tf.constant`, the output
    is just a single tensor, therefore *ID=0*; but there can be operations with more
    than one output, and in this case, the suffixes *:0, :1,* and so on are added
    to the `tf.Tensor` name generated by the operation.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to add a name scope prefix to all operations created within
    a context—a context defined by the `tf.name_scope` call. The default name scope
    prefix is a `/` delimited list of names of all the active `tf.name_scope` context
    managers. In order to guarantee the uniqueness of the operations defined within
    the scopes and the uniqueness of the scopes themselves, the same suffix appending
    rule used for `tf.Operation` holds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how our baseline example can be wrapped into
    a separate graph, how a second independent graph can be created in the same Python
    script, and how we can change the node names, adding a prefix, using `tf.name_scope`.
    First, we import the TensorFlow library:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define two `tf.Graph` objects (the scoping system allows you to use
    multiple graphs easily):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, we define two summary writers. We need to use two different `tf.summary.FileWriter`
    objects to log two separate graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Run the example and use TensorBoard to visualize the two graphs, using the left-hand
    column on TensorBoard to switch between "runs."
  prefs: []
  type: TYPE_NORMAL
- en: Nodes with the same name, `x` in the example, can live together in the same
    graph, but they have to be under different scopes. In fact, being under different
    scopes makes the nodes completely independent and completely different objects.
    The node name, in fact, is not only the parameter `name` passed to the operation
    definition, but its full path, complete with all of the prefixes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, running the script, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the full names are different and we also have other information
    about the tensors produced. In general, every tensor has a name, a type, a rank,
    and a shape:'
  prefs: []
  type: TYPE_NORMAL
- en: The **name** uniquely identifies the tensor in the computational graphs. Using
    `tf.name_scope`, we can prefix tensor names, thus changing their full path. We
    can also specify the name using the `name` attribute of every `tf.*` API call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **type** is the data type of the tensor; for example, `tf.float32`, `tf.int8`,
    and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **rank**, in the TensorFlow world (this is different from the strictly mathematical
    definition), is just the number of dimensions of a tensor; for example, a scalar
    has rank 0, a vector has rank 1, a matrix has rank 2, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **shape** is the number of elements in each dimension; for example, a scalar
    has rank 0 and an empty shape of `()`, a vector has rank 1 and a shape of `(D0)`,
    a matrix has rank 2 and a shape of `(D0, D1)`, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, it is possible to see a shape with a dimension of `-1`. This is a
    particular syntax that tells TensorFlow to infer from the other, well-defined,
    dimensions of the tensor which value should be placed in that position. Usually,
    a negative shape is used in the `tf.reshape` operation, which is able to change
    the shape of a tensor if the requested one is compatible with the number of elements
    of the tensor.
  prefs: []
  type: TYPE_NORMAL
- en: When defining a tensor, instead, it is possible to see one or more dimensions
    with the value of `None`. In this case, the full shape definition is delegated
    to the execution phase, since using `None` instructs TensorFlow to expect a value
    in that position known only at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Being a C++ library, TensorFlow is strictly statically typed. This means that
    the type of every operation/tensor must be known at graph definition time. Moreover,
    this also means that it is not possible to execute an operation among incompatible
    types.
  prefs: []
  type: TYPE_NORMAL
- en: Looking closely at the baseline example, it is possible to see that both matrix
    multiplication and addition operations are performed on tensors with the same
    type, `tf.float32`. The tensors identified by the Python variables `A` and `b`
    have been defined, making the type clear in the operation definition, while tensor `x`
    has the same `tf.float32` type; but in this case, it has been inferred by the
    Python bindings, which are able to look inside the constant value and infer the
    type to use when creating the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Another peculiarity of Python bindings is their simplification in the definition
    of some common mathematical operations using operator overloading. The most common
    mathematical operations have their counterpart as `tf.Operation`; therefore, using
    operator overloading to simplify the graph definition is natural.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the available operators overloaded in the TensorFlow
    Python API:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Python operator** | **Operation name** |'
  prefs: []
  type: TYPE_TB
- en: '| `__neg__` | unary `-` |'
  prefs: []
  type: TYPE_TB
- en: '| `__abs__` | `abs()` |'
  prefs: []
  type: TYPE_TB
- en: '| `__invert__` | unary `~` |'
  prefs: []
  type: TYPE_TB
- en: '| `__add__` | binary `+` |'
  prefs: []
  type: TYPE_TB
- en: '| `__sub__` | binary `-` |'
  prefs: []
  type: TYPE_TB
- en: '| `__mul__` | binary elementwise `*` |'
  prefs: []
  type: TYPE_TB
- en: '| `__floordiv__` | binary `//` |'
  prefs: []
  type: TYPE_TB
- en: '| `__truediv__` | binary `/` |'
  prefs: []
  type: TYPE_TB
- en: '| `__mod__` | binary `%` |'
  prefs: []
  type: TYPE_TB
- en: '| `__pow__` | binary `**` |'
  prefs: []
  type: TYPE_TB
- en: '| `__and__` | binary `&` |'
  prefs: []
  type: TYPE_TB
- en: '| `__or__` | binary `&#124;` |'
  prefs: []
  type: TYPE_TB
- en: '| `__xor__` | binary `^` |'
  prefs: []
  type: TYPE_TB
- en: '| `__le__` | binary `<` |'
  prefs: []
  type: TYPE_TB
- en: '| `__lt__` | binary `<=` |'
  prefs: []
  type: TYPE_TB
- en: '| `__gt__` | binary `>` |'
  prefs: []
  type: TYPE_TB
- en: '| `__ge__` | binary `<=` |'
  prefs: []
  type: TYPE_TB
- en: '| `__matmul__` | binary `@` |'
  prefs: []
  type: TYPE_TB
- en: 'Operator overloading allows a faster graph definition and is completely equivalent
    to their `tf.*` API call (for example, using `__add__` is the same as using the `tf.add`
    function). There is only one case in which it is beneficial to use the TensorFlow
    API call instead of the associated operator overload: when a name for the operation
    is needed. Usually, when defining a graph, we''re interested in giving meaningful
    names only to the input and output nodes, while any other node can just be automatically
    named by TensorFlow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using overloaded operators, we can''t specify the node name and thus the output
    tensor''s name. In fact, in the baseline example, we defined the addition operation
    using the `tf.add` method, because we wanted to give the output tensor a meaningful
    name (result). In practice, these two lines are equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned at the beginning of this section, TensorFlow itself can place specific
    nodes on different devices better suited to the operation execution. The framework
    is so flexible that it allows the user to manually place operations on different
    local and remote devices just using the `tf.device` context manager.
  prefs: []
  type: TYPE_NORMAL
- en: Graph placement – tf.device
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`tf.device` creates a context manager that matches a device. The function allows
    the user to request that all operations created within the context it creates
    are placed on the same device. The devices identified by `tf.device` are more
    than physical devices; in fact, it is capable of identifying devices such as remote
    servers, remote devices, remote workers, and different types of physical devices
    (GPUs, CPUs, and TPUs). It is required to follow a device specification to correctly
    instruct the framework to use the desired device. A device specification has the
    following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Broken down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<JOB_NAME>` is an alpha-numeric string that does not start with a number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<DEVICE_TYPE>` is a registered device type (such as GPU or CPU)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<TASK_INDEX>` is a non-negative integer representing the index of the task
    in the job named `<JOB_NAME>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<DEVICE_NAME>` is a non-negative integer representing the index of the device;
    for example, `/GPU:0` is the first GPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no need to specify every part of a device specification. For example,
    when running a single-machine configuration with a single GPU, you might use `tf.device`
    to pin some operations to the CPU and GPU.
  prefs: []
  type: TYPE_NORMAL
- en: We can thus extend our baseline example to place the operations on the device
    we choose. Thus, it is possible to place the matrix multiplication on the GPU,
    since it is hardware optimized for this kind of operation, while keeping all the
    other operations on the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that since this is only a graph description, there''s no need to
    physically have a GPU or to use the `tensorflow-gpu` package. First, we import
    the TensorFlow library:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use the context manager to place operations on different devices, first,
    on the first CPU of the local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, on the first GPU of the local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When the device is not forced by a scope, TensorFlow decides which device is
    better to place the operation on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the summary writer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the generated graph, we''ll see that it is identical to the one
    generated by the baseline example, with two main differences:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of having a meaningful name for the output tensor, we have just the
    default one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clicking on the matrix multiplication node, it is possible to see (in TensorBoard)
    that this operation must be executed in the first GPU of the local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `matmul` node is placed on the first GPU of the local machine, while any
    other operation is executed in the CPU. TensorFlow takes care of communication
    among different devices in a transparent manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4982764b-9b4f-4813-af3f-ed57cec12b46.png)'
  prefs: []
  type: TYPE_IMG
- en: Please also note that even though we have defined constant operations that produce
    constant tensors, their values are not visible among the attributes of the node
    nor among the input/output properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the static-graph and session execution parading, the execution is
    completely separated from the graph definition. This is no longer true in eager
    execution, but since, in this chapter, the focus is on the TensorFlow architecture,
    it is worth also focusing on the execution part using `tf.Session`: in TensorFlow
    2.0, the session is still present, but hidden, as we will see in the next chapter,
    [Chapter 4](655b734e-1636-4e11-b944-a71fafacb977.xhtml), *TensorFlow 2.0 Architecture*.'
  prefs: []
  type: TYPE_NORMAL
- en: Graph execution – tf.Session
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`tf.Session` is a class that TensorFlow provides to represent a connection
    between the Python program and the C++ runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.Session` object is the only object able to communicate directly with
    the hardware (through the C++ runtime), placing operations on the specified devices,
    using the local and distributed TensorFlow runtime, with the goal of concretely
    building the defined graph. The `tf.Session` object is highly optimized and, once
    correctly built, caches `tf.Graph` in order to speed up its execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Being the owner of physical resources, the `tf.Session` object must be used
    as a file descriptor to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Acquire the resources by creating a `tf.Session` (the equivalent of the `open` operating
    system call)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the resources (the equivalent of using the `read/write` operation on the
    file descriptor)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release the resources with `tf.Session.close` (the equivalent of the `close`
    call)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, instead of manually defining a session and taking care of its creation
    and destruction, a session is used through a context manager that automatically
    closes the session at the block exit.
  prefs: []
  type: TYPE_NORMAL
- en: The constructor of `tf.Session` is fairly complex and highly customizable since
    it is used to configure and create the execution of the computational graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the simplest and most common scenario, we just want to use the current local
    hardware to execute the previously described computational graph as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'There are more complex scenarios in which we wouldn''t want to use the local
    execution engine, but use a remote TensorFlow server that gives access to all
    the devices it controls. This is possible by specifying the `target` parameter
    of `tf.Session` just by using the URL (`grpc://`) of the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: By default, the `tf.Session` will capture and use the default `tf.Graph` object,
    but when working with multiple graphs, it is possible to specify which graph to
    use by using the `graph` parameter. It's easy to understand why working with multiple
    graphs is unusual, since even the `tf.Session` object is able to work with only
    a single graph at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third and last parameter of the `tf.Session` object is the hardware/network
    configuration specified through the `config` parameter. The configuration is specified
    through the `tf.ConfigProto` object, which is able to control the behavior of
    the session. The `tf.ConfigProto` object is fairly complex and rich with options,
    the most common and widely used being the following two (all the others are options
    used in distributed, complex environments):'
  prefs: []
  type: TYPE_NORMAL
- en: '`allow_soft_placement`: If set to `True`, it enables a soft device placement.
    Not every operation can be placed indifferently on the CPU and GPU, because the
    GPU implementation of the operation may be missing, for example, and using this
    option allows TensorFlow to ignore the device specification made via `tf.device`
    and place the operation on the correct device when an unsupported device is specified
    at graph definition time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gpu_options.allow_growth`: If set to `True`, it changes the TensorFlow GPU
    memory allocator; the default allocator allocates all the available GPU memory
    as soon as the `tf.Session` is created, while the allocator used when `allow_growth`
    is `True` gradually increases the amount of memory allocated. The default allocator
    works in this way because, in production environments, the physical resources
    are completely dedicated to the `tf.Session` execution, while, in a standard research
    environment, the resources are usually shared (the GPU is a resource that can
    be used by other processes while the TensorFlow `tf.Session` is in execution).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The baseline example can now be extended to not only define a graph, but to
    proceed on to an effective construction and the execution of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first `sess.run` call evaluates the three `tf.Tensor` objects, `A, x, b`,
    and returns their values as `numpy` arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second call, `sess.run(y)`, works in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '`y` is an output node of an operation: backtrack to its inputs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recursively backtrack through every node until all the nodes without a parent
    are found
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the input; in this case, the `A, x, b` tensors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Follow the dependency graph: the multiplication operation must be executed
    before the addition of its result with `b`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the matrix multiplication
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the addition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The addition is the entry point of the graph resolution (Python variable `y`)
    and the computation ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first print call, therefore, produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The third `sess.run` call shows how it is possible to inject into the computational
    graph values from the outside, as `numpy` arrays, overwriting a node. The `feed_dict`
    parameter allows you to do this: usually, inputs are passed to the graph using
    the `feed_dict` parameter and through the overwriting of the `tf.placeholder`
    operation created exactly for this purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.placeholder` is just a placeholder created with the aim of throwing an
    error when values from the outside are not injected inside the graph. However,
    the `feed_dict` parameter is more than just a way to feed the placeholders. In
    fact, the preceding example shows how it can be used to overwrite any node. The
    result produced by the overwriting of the node identified by the Python variable, `b`,
    with a `numpy` array that must be compatible, in terms of both type and shape,
    with the overwritten variable, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The baseline example has been updated in order to show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to build a graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to save a graphical representation of the graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create a session and execute the defined graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we have used graphs with constant values and used the `feed_dict` parameter
    of the `sess.run` call to overwrite a node parameter. However, since TensorFlow
    is designed to solve complex problems, the concept of `tf.Variable` has been introduced:
    every parametric machine learning model can be defined and trained with TensorFlow.'
  prefs: []
  type: TYPE_NORMAL
- en: Variables in static graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A variable is an object that maintains a state in the graph across multiple
    calls to `sess.run`. A variable is added to `tf.Graph` by constructing an instance
    of the `tf.Variable` class.
  prefs: []
  type: TYPE_NORMAL
- en: A variable is completely defined by the pair (type, shape), and variables created
    by calling `tf.Variable` can be used as input for other nodes in the graph; in
    fact, the `tf.Tensor` and `tf.Variable` objects can be used in the same manner
    when building a graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variables have more attributes with respect to tensors: a variable object must
    be initialized and thus have its initializer; a variable is, by default, added
    to the global variables and trainable variable graph collections. If a variable
    is set as non-trainable, it can be used by the graph to store the state, but the
    optimizers will ignore it when performing the learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways of declaring a variable in a graph: `tf.Variable` and `tf.get_variable`.
    Using `tf.Variable` is easier but less powerful—the second way is more complex
    to use, but has more expressive power.'
  prefs: []
  type: TYPE_NORMAL
- en: tf.Variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a variable by calling `tf.Variable` will always create a new variable
    and it always requires an initial value to be specified. In the following lines,
    the creation of a variable named `W` with shape `(5, 5, size_in, size_out)` and
    a variable, `B`, with shape `(size_out)` is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `w` initial value is generated by the `tf.truncated_normal` operation, which
    samples from a normal distribution with 0 mean and a standard deviation of 0.1
    the `5 x 5 x size_in x size_out` (total number) values required to initialize
    the tensor, while `b` is initialized using the constant value of 0.1 generated
    by the `tf.constant` operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since each call to `tf.Variable` creates a new variable in the graph, it is
    the perfect candidate for the creation of layers: every layer (for example, a
    convolutional layer/a fully connected layer) definition requires the creation
    of a new variable. For instance, the following lines of code show the definition
    of two functions that can be used to define a convolutional neural network and/or
    a fully connected neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first function creates a 2D convolutional layer (with a 5 x 5 kernel) followed
    by a max-pool operation to halve the output''s spatial extent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The second function defines a fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Both functions also use the `tf.summary` module to log the histograms of the
    weight, bias, and activation values, which can change during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The call to a `tf.summary` method automatically adds the summaries to a global
    collection that is used by `tf.Saver` and `tf.SummaryWriter` objects to log every
    summary value in the TensorBoard log directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A layer definition made in this way is perfect for the most common scenarios
    in which a user wants to define a deep learning model composed by a stack of several
    layers and train it given a single input that flows from the first to the last
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: What if, instead, the training phase is not standard and there is the need to
    share the variable's values among different inputs?
  prefs: []
  type: TYPE_NORMAL
- en: We need to use the TensorFlow feature called **variable sharing**, which is not
    possible using a layer definition made with `tf.Variable`, so we have to instead
    use the most powerful method, `tf.get_variable`.
  prefs: []
  type: TYPE_NORMAL
- en: tf.get_variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like `tf.Variable`, `tf.get_variable` also allows the definition and creation
    of new variables. The main difference is that its behavior changes if the variable
    has already been defined.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.get_variable` is always used together with `tf.variable_scope` since it
    enables the variable sharing capabilities of `tf.get_variable` through its `reuse`
    parameter. The following example clarifies the concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the Python variables `a` and `c` point to the same
    graph variable, named `scope/v:0`. Hence, a layer that uses `tf.get_variable` to
    define variables can be used in conjunction with `tf.variable_scope` to define
    or reuse the layer's variables. This is extremely useful and powerful when training
    generative models using adversarial training, as we will see in [Chapter 9](66948c53-131c-43ef-a7fc-3d242d1e0664.xhtml), *Generative
    Adversarial Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from `tf.Variable`, in this case, we can''t pass an initial value
    in a raw way (passing the value directly as input to the `call `method ); we always
    have to explicitly use an initializer. The previously defined layer can be written
    using `tf.get_variable` (and this is the recommended way to define variables)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoking `conv2D` or `fc` defines the variables needed to define a layer in
    the current scope; hence, to define two convolutional layers without having naming
    conflicts, `tf.variable_scope` must be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Manually defining layers is a good exercise, and knowing that TensorFlow has
    all the primitives required to define every ML layer is something every ML practitioner
    should know. However, manually defining every single layer is tedious and repetitive
    (we need fully connected, convolutional, dropout, and batch normalization layers
    in almost every project), and, for this reason, TensorFlow already comes with
    a module named `tf.layers`, which contains all the most common and widely used
    layers, defined using `tf.get_variable` under the hood, and therefore, layers
    can be used in conjunction with `tf.variable_scope` to share their variables.
  prefs: []
  type: TYPE_NORMAL
- en: Model definition and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Disclaimer: the layer module has been completely removed in TensorFlow 2.0,
    and the layer definition using `tf.keras.layers` is the new standard; however,
    an overview of `tf.layers` is still worth reading because it shows how reasoning
    layer by layer to define deep models is the natural way to proceed and it also
    gives us an idea of the reasons behind the migration from `tf.layers` to `tf.keras.layers`.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining models with tf.layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown in the previous section, TensorFlow provides all the primitive features
    to define a neural network layer: the user should take care when defining the
    variables, the operation nodes, the activation functions, and the logging, and
    define a proper interface to handle all cases (adding, or not, the bias term,
    adding regularization to the layer parameters, and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.layers` module in TensorFlow 1.x and the `tf.keras.layers` module in
    TensorFlow 2.0 provide an excellent API to define machine learning models in a
    convenient and powerful way. Every layer in `tf.layers`, defines variables using `tf.get_variable`,
    and therefore, each layer defined in this way can use the variable-sharing features
    provided by `tf.variable_scope`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previously manually defined 2D convolution and fully connected layers are
    clearly present and well-defined in `tf.layers` and using them to define a LeNet-like
    CNN is easy, as shown. First, we define a convolutional neural network for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we flatten the data to a 1D vector so that we can use a fully connected
    layer. Please note how the new shape is computed and the negative dimension in
    the batch size position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Being high-level wrappers on primitive TensorFlow operations, there is no need
    to detail what every layer does in this book since it is pretty clear from the
    layer names themselves and from the documentation. The reader is invited to become
    familiar with the official TensorFlow documentation, and, in particular, to try
    to define their own classification model using layers: [https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers)[.](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers)
  prefs: []
  type: TYPE_NORMAL
- en: Using the baseline example and replacing the graph with this CNN definition,
    using TensorFlow, it is possible to see how every layer has its own scope, how
    the layers are connected among them, and, as shown in the second diagram, by double-clicking
    on a layer, it is possible to see its contents to understand how it is implemented
    without having to look at the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the architecture of the defined LeNet-like CNN.
    The whole architecture is placed under the *cnn* scope; the input node is a placeholder.
    It is possible to visualize how the layers are connected and how TensorFlow added
    the `_1` suffix to blocks with the same name to avoid conflicts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6aedc60-0542-4150-8377-b04cad633ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Double-clicking on the `conv2d` block allows you to analyze how the various
    components defined by the layers are connected to each other. Please note how,
    different from our layer implementation, the TensorFlow developers used an operation
    named `BiasAdd` to add the bias and not the raw `Add` operation. The behavior
    is the same, but the semantics are clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40e6d1cd-926e-4334-bc2c-1f8b77660c67.png)'
  prefs: []
  type: TYPE_IMG
- en: As an exercise, you can try to extend the baseline by defining a CNN like the
    one just presented to visualize and understand the layer structure.
  prefs: []
  type: TYPE_NORMAL
- en: We always have to keep in mind that TensorFlow 1.x follows the graph definition
    and session execution approach. This means that even the training phase should
    be described within the same `tf.Graph` object before being executed.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic differentiation – losses and optimizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow uses automatic differentiation—a differentiator is an object that
    contains all the rules required to build a new graph that takes the derivative
    of each node it traverses. The `tf.train` module in TensorFlow 1.x contains the
    most widely used type of differentiator, called optimizers here. In the module,
    among the other optimizers, it is possible to find the ADAM optimizer as `tf.train.AdamOptimizer` and
    the standard gradient descent optimizer as `tf.train.GradientDescentOptimizer`.
    Each optimizer is an object that implements a common interface. The interface
    standardizes how to use an optimizer to train a model. Performing a mini-batch
    gradient descent step is just a matter of executing a train operation in a Python
    loop; that is, an operation returned by the `.minimize` method of every optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will know from the theory presented in the previous chapter, [Chapter
    2](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml), *Neural Networks and Deep Learning, *to
    train a classifier using cross-entropy loss, it is necessary to one-hot encode
    the labels. TensorFlow has a module, `tf.losses`, that contains the most commonly
    used loss functions that are also capable of performing the one-hot encoding of
    labels by themselves. Moreover, every loss function expects the `logits` tensor
    as input; that is, the linear output of the model without the application of the
    softmax/sigmoid activation function. The name of the `logits` tensor is a TensorFlow
    design choice: it is called in this way even if no sigmoidal transformation has
    been applied to it (a better choice would be naming this parameter `unscaled_logits`).'
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this choice is to let the user focus on the network design without
    having to worry about the numerical instability problems that could arise when
    computing certain loss functions; in fact, every loss defined in the `tf.losses`
    module is numerically stable.
  prefs: []
  type: TYPE_NORMAL
- en: In order to have a complete understanding of the topic and to show that an optimizer
    just builds a graph connected to the previous one (it only adds nodes in practice),
    it is possible to mix the baseline example that logs the graph together with the
    example that defines the network with its loss function and an optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the previous example can be modified as follows. To define the input
    placeholder for the labels, we can define the loss function (`tf.losses.sparse_softmax_cross_entropy`)
    and instantiate the ADAM optimizer to minimize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'TensorBoard allows us to visualize the graph built as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8f0e238-3feb-4ad1-9816-bf5f0c3d6fc2.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows the structure of the graph when a loss function
    is defined, and the `.minimize` method is invoked.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ADAM optimizer is a separate block that only has inputs—the model (*cnn*),
    the gradients, and the nontrainable *beta1* and *beta2* parameters used by ADAM
    (and specified in its constructor, left at their default values in this case).
    The gradients, as you will know from the theory, are computed with respect to
    the model parameters to minimize the loss function: the graph on the left perfectly
    describes this construction. The gradient block created by the minimize method
    invocation is a named scope, and as such, it can be analyzed by double-clicking
    on it just like any other block in TensorBoard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the gradient block expanded: it contains a mirrored
    structure of the graph used for the forward model. Every block the optimizer uses
    to optimize the parameters is an input of the gradient block. The gradients are
    the input of the optimizer (ADAM):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/932f5a8d-ca86-41f6-96c6-51aa85c5dba8.png)'
  prefs: []
  type: TYPE_IMG
- en: Following on from the theory, the differentiator (optimizer) created a new graph
    that mirrors the original graph; this second graph, inside the gradient block,
    performs the gradient calculation. The optimizer uses the gradients produced to
    apply the variables update rule it defines and implements the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'A brief recap on what we have seen so far for the static-graph and session
    execution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the model inputs using placeholders or other optimized methods, as shown
    in later chapters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the model as a function of the input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the loss function as a function of the model output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the optimizer and invoke the `.minimize` method to define the gradient
    computation graph
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These four steps allow us to define a simple training loop and train our model.
    However, we''re skipping some important parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Model performance measurement on training and validation sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving the model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moreover, since the input is defined using placeholders, we have to take care
    of everything related to the input: splitting the dataset, creating the mini-batches,
    keeping track of the training epochs, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2.0 with `tfds` (TensorFlow Datasets) simplified and standardized
    the input pipeline definition, as we will see in later chapters; however, having
    a clear idea about what happens under the hood is always an advantage, therefore,
    it's a good exercise for the reader to continue with the following low-level use
    of placeholders in order to have a better understanding of the problems `tfds` solves.
  prefs: []
  type: TYPE_NORMAL
- en: So far, you should have a clear understanding of the operations that must be
    executed in a computational graph, and you should have understood that Python
    is used only to build a graph and to do non-learning related operations (hence,
    it's not Python that performs the variable update, but it is the execution within
    a session of a Python variable that represents the training operation that triggers
    all the required operations to make the model learn).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, the previous CNN example is extended, adding all the functionality
    on the Python-side that is required to perform model selection (saving the model,
    measuring performance, making the training loop, and feeding the input placeholders).
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with the graph using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is the language of choice to train a TensorFlow model; however, after
    defining a computational graph in Python, there are no constraints regarding using
    it with another language to execute the learning operations defined.
  prefs: []
  type: TYPE_NORMAL
- en: Always keep in mind that we use Python to define a graph and this definition
    can be exported in a portable and language-agnostic representation (Protobuf)—this
    representation can then be used in any other language to create a concrete graph
    and using it within a session.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Python API is complete and easy to use. Therefore, we can extend
    the previous example to measure the accuracy (defining the accuracy measurement
    operation in the graph) and use this metric to perform model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best model means storing the model parameters at the end of each
    epoch and moving the parameters that produced the highest metric value in a different
    folder. To do this, we have to define the input pipeline in Python and use the
    Python interpreter to interact with the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding placeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the previous section, placeholders are the easiest to use but
    are also the least performant and most error-prone way to build a data input pipeline.
    In later chapters, a better, highly efficient solution will be presented. This
    highly efficient solution has the complete input pipeline completely defined inside
    the graph. However, the placeholder solution is not only the easiest but also
    the only one that can be used in certain scenarios (for example, when training
    reinforcement learning agents, input via a placeholder is the preferred solution).
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml), *What is Machine
    Learning?*, the Fashion-MNIST dataset was described, and we're now going to use
    it as the input dataset for our model—the previously defined CNN will be used
    to classify the fashion items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, we don''t have to worry about the dataset download and processing
    part, since TensorFlow, in its `keras` module, already has a function that downloads
    and processes the dataset for us to have the training images and the test images
    together with their labels in the expected form (28 x 28 images):'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`train_x` and `test_x` contain the whole dataset—training a model using a single
    batch containing the complete dataset is not tractable on a standard computer;
    therefore, using Python, we have to take care when splitting the dataset and building
    mini-batches to make the training process affordable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we want to train the model for 10 epochs using batches of 32 elements
    each; it is easy to compute the number of batches needed to train the model for
    an epoch and then run a training loop that iterates over the batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Of course, since we need to perform model selection, we need to first define
    an operation that computes the accuracy as a function of the model and the input
    and then use the `tf.summary.SummaryWriter` object to write the train and validation
    accuracy on the same graph.
  prefs: []
  type: TYPE_NORMAL
- en: Writing summaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The baseline example already uses a `tf.summary.SummaryWriter` object to write
    the graph in the log directory and make it appear in the graph section of TensorBoard.
    However, `SummaryWriter` can be used to write not only the graph but also a histogram,
    scalar values, distributions, log images, and many other data types.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.summary` package is filled with easy-to-use methods to log any data.
    For instance, we are interested in logging the loss value; the loss value is a
    scalar and, therefore, `tf.summary.scalar` is the method to use. The package is
    well-documented, and you should take the time to explore it: [https://www.tensorflow.org/versions/r1.15/api_docs/python/tf](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf)[.](https://www.tensorflow.org/versions/r1.15/api_docs/Python/tf)
  prefs: []
  type: TYPE_NORMAL
- en: To extend the previous example, we can define the accuracy operation as a function
    of the input placeholders. In this way, we can run the same operation, changing
    the input when needed. For instance, we could be interested in measuring both
    the training and validation accuracy at the end of each training epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same reasoning applies to the loss value: defining the loss as a function
    of the model and the model as a function of a placeholder, we are able to measure
    how the loss changes on the training and validation input just by changing the
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: A single `tf.train.FileWriter` object is associated with a unique path on the
    disk, called **run**. A run represents a different configuration of the current
    experiment. For example, the default run is usually the training phase. At this
    phase, hence at this run, the metrics attached (loss, accuracy, logs of images,
    and so on) are measured during the training phase, on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: A different run can be created by creating a new `tf.train.FileWriter` with
    a different path associated with it, but with the same root of the other (training) `FileWriter`.
    In this way, using TensorBoard, we can visualize different curves on the same
    graph; for example, visualizing the validation accuracy and the training accuracy
    on the same plot. This feature is of extreme importance when analyzing the behavior
    of an experiment and when you are interested in comparing different experiments
    at a glance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, since we want to visualize the training and the validation curves on
    the same plot, we can create two different writers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The first one is the train phase writer; the second, the validation phase one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, potentially, we could measure the validation accuracy and the training
    accuracy, just by running the `accuracy` tensor, changing the input placeholder
    values accordingly; this means that we are already able to perform model selection:
    the model with the highest validation accuracy is the one to select.'
  prefs: []
  type: TYPE_NORMAL
- en: To save the model parameters, a `tf.Saver` object is required.
  prefs: []
  type: TYPE_NORMAL
- en: Saving model parameters and model selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Saving model parameters is important since it's the only way to continue to
    train a model after an interruption, and the only way to checkpoint a model status
    for any reason—training finished, the model reached the best validation performance.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Saver` is the object the TensorFlow Python API provides to save the current
    model variables. Please note that the `tf.Saver` object saves the variables only
    and not the graph structure!'
  prefs: []
  type: TYPE_NORMAL
- en: To save both the graph structure and variables, a `SavedModel` object is required;
    however, since the `SavedModel` object is more connected with putting a trained
    model into production, its definition and usage are demanded to the paragraph
    dedicated to the production.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.Saver` object saves the list of the trainable variables plus any other
    nontrainable variables specified in its constructor. Once created, the object
    provides the `save` method, which accepts the path used to store the variables.
    A single `Saver` object can be used to create several checkpoints and thus save
    the model that reached the top performance on the validation metric in a different
    path, in order to perform model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the `Saver` object offers the `restore` method, which can be used
    to populate the variables of the previously defined graph, before starting to
    train them, to restart an interrupted training phase. Eventually, it is possible
    to specify the list of the variables to restore from the checkpoint in the restore
    call, making it possible to use pre-trained layers and fine-tune them. The `tf.Saver`
    is the main object involved when doing transfer learning and fine-tuning a model.
  prefs: []
  type: TYPE_NORMAL
- en: The previous example can thus be extended to perform logging of the measured
    training/validation accuracy in TensorBoard (in the code, the accuracy is measured
    on a batch of 128 elements at the end of each epoch), the training/validation
    loss, and to perform model selection using the measured validation accuracy and
    a new saver.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are invited to analyze and run the complete example to completely understand
    how every presented object works in detail. For any additional tests, always keep
    the TensorFlow API reference and documentation open and try everything:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The result, as seen in TensorBoard, is shown in the following two screenshots.
    The first one shows that, by using two different writers, it is possible to write
    two different curves on the same plot; while the second one shows the graph tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45b985d8-952d-479c-bf17-40455e27ab0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Using two `SummaryWriter`, it's possible to draw different curves on the same
    plot. The graph on top is the validation graph; the one on the bottom is the loss
    graph. Orange is the color of the training run, while blue is validation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76b43e22-a414-4379-b2a2-6768af93ee5e.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting graph—please note how proper use of the variable scopes makes
    the graph easy to read and understand
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that, even if trained for only a few epochs, the model defined
    already reaches notable performance, although it should be clear from the accuracy
    plot that it suffers from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we analyzed how TensorFlow works under the hood—the separation
    between the graph definition phase and its execution within a session, how to
    use the Python API to interact with a graph, and how to define a model and measure
    the metrics during training.
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that this chapter analyzed how TensorFlow works in its static
    graph version, which is no longer the default in TensorFlow 2.0; however, the
    graph is still present and even when used in eager mode, every API call produces
    operations that can be executed inside a graph to speed up execution. As will
    be shown in the next chapter, TensorFlow 2.0 still allows models to be defined
    in static graph mode, especially when defining models using the Estimator API.
  prefs: []
  type: TYPE_NORMAL
- en: Having knowledge of graph representation is of fundamental importance, and having
    at least an intuitive idea about the advantages that representing computation
    using dataflow graphs brings should make it clear why TensorFlow scales so well,
    even in huge, complex environments such as Google data centers.
  prefs: []
  type: TYPE_NORMAL
- en: The exercise section is incredibly important—it asks you to solve problems not
    introduced in the previous sections because this is the only way to become familiar
    with the TensorFlow documentation and code base. Keep track of the time it takes
    you to solve every exercise and try to figure out the solution by yourself with
    only the help of the TensorFlow documentation and some Stack Overflow questions!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, [Chapter 4](655b734e-1636-4e11-b944-a71fafacb977.xhtml), *TensorFlow
    2.0 Architecture**,* you''ll deep dive into the TensorFlow 2.0 world: eager mode;
    automatic graph conversion; a better, cleaner code base; and a Keras-based approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is it possible to assess that the model suffers from overfitting only by
    looking at the graph?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extend the baseline example to place the matrix multiplication operation on
    a remote device at IP 192.168.1.12; visualize the result on TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it necessary to have a remote device to place an operation on?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extend the CNN architecture defined in the `define_cnn` method: add a batch
    normalization layer (from `tf.layers`) between the output of the convolutional
    layer and its activation function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Try to train the model with the extended CNN architecture: the batch normalization
    layer adds two update operations that must be executed before running the training
    operation. Become familiar with the `tf.control_dependencies` method to force
    the execution of the operations contained inside the collection `tf.GraphKeys.UPDATE_OPS`,
    to be executed before the train operation (look at the documentation of `tf.control_dependencies` and `tf.get_collection`!).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log the training and validation images in TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the model selection in the last example been performed correctly ? Probably
    not. Extend the Python script to measure the accuracy on the complete dataset
    and not just a batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the accuracy measurement performed manually with the accuracy operation
    provided in the `tf.metrics` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Process the fashion-MNIST dataset and make it a binary dataset: all the items
    with a label different from 0 are now labeled as 1\. The dataset is unbalanced
    now. Which metric should you use to measure the model performance and perform
    model selection? Give reasons for your answer (see [Chapter 1](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml), *What
    is Machine Learning?*) and implement the metric manually.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the manually implemented metric using the same metric defined in the `tf.metrics` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
