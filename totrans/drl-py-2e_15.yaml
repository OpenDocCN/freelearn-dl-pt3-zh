- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imitation Learning and Inverse RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning from demonstration is often called imitation learning. In the imitation
    learning setting, we have expert demonstrations and train our agent to mimic those
    expert demonstrations. Learning from demonstrations has many benefits, including
    helping an agent to learn more quickly. There are several approaches to perform
    imitation learning, and two of them are **supervised imitation learning** and
    **Inverse Reinforcement Learning** (**IRL**).
  prefs: []
  type: TYPE_NORMAL
- en: First, we will understand how we can perform imitation learning using supervised
    learning, and then we will learn about an algorithm called **Dataset Aggregation**
    (**DAgger**). Next, we will learn how to use demonstration data in a DQN using
    an algorithm called **Deep Q Learning from Demonstrations** (**DQfD**).
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we will learn about IRL and how it differs from reinforcement learning.
    We will learn about one of the most popular IRL algorithms called **maximum entropy
    IRL**. Toward the end of the chapter, we will understand how **Generative Adversarial
    Imitation Learning** (**GAIL**) works.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised imitation learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAgger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q learning from demonstrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inverse reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum entropy inverse reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative adversarial imitation learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin our chapter by understanding how supervised imitation learning works.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised imitation learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the imitation learning setting, our goal is to mimic the expert. Say, we
    want to train our agent to drive a car. Instead of training the agent from scratch
    by having them interact with the environment, we can train them with expert demonstrations.
    Okay, what are expert demonstrations? An expert demonstrations are a set of trajectories
    consisting of state-action pairs where each action is performed by the expert.
  prefs: []
  type: TYPE_NORMAL
- en: We can train an agent to mimic the actions performed by the expert in various
    respective states. Thus, we can view expert demonstrations as training data used
    to train our agent. The fundamental idea of imitation learning is to imitate (learn)
    the behavior of an expert.
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest and most naive ways to perform imitation learning is to
    treat the imitation learning task as a supervised learning task. First, we collect
    a set of expert demonstrations, and then we train a classifier to perform the
    same action performed by the expert in the respective states. We can view this
    as a big multiclass classification problem and train our agent to perform the
    action performed by the expert in the respective states.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to minimize the loss ![](img/B15558_15_001.png) where ![](img/B15558_04_122.png)
    is the expert action and ![](img/B15558_15_003.png) denotes the action performed
    by our agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in supervised imitation learning, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect the set of expert demonstrations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a policy ![](img/B15558_10_044.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn the policy by minimizing the loss function ![](img/B15558_15_001.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, there exist several challenges and drawbacks with this method. The
    knowledge of the agent is limited only to the expert demonstrations (training
    data), so if the agent comes across a new state that is not present in the expert
    demonstrations, then the agent will not know what action to perform in that state.
  prefs: []
  type: TYPE_NORMAL
- en: Say, we train an agent to drive a car using supervised imitation learning and
    let the agent perform in the real world. If the training data has no state where
    the agent encounters a traffic signal, then our agent will have no clue about
    the traffic signal.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the accuracy of the agent is highly dependent on the knowledge of the
    expert. If the expert demonstrations are poor or not optimal, then the agent cannot
    learn correct actions or the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the challenges in supervised imitation learning, we introduce a
    new algorithm called DAgger. In the next section, we will learn how DAgger works
    and how it overcomes the limitations of supervised imitation learning.
  prefs: []
  type: TYPE_NORMAL
- en: DAgger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DAggeris one of the most-used imitation learning algorithms. Let's understand
    how DAgger works with an example. Let's revisit our example of training an agent
    to drive a car. First, we initialize an empty dataset ![](img/B15558_09_124.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**In the first iteration**, we start off with some policy ![](img/B15558_03_153.png)
    to drive the car. Thus, we generate a trajectory ![](img/B15558_14_143.png) using
    the policy ![](img/B15558_03_153.png). We know that the trajectory consists of
    a sequence of states and actions—that is, states visited by our policy ![](img/B15558_15_010.png)
    and actions made in those states using our policy ![](img/B15558_15_010.png).
    Now, we create a new dataset ![](img/B15558_15_012.png) by taking only the states
    visited by our policy ![](img/B15558_03_153.png) and we use an expert to provide
    the actions for those states. That is, we take all the states from the trajectory
    and ask the expert to provide actions for those states.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we combine the new dataset ![](img/B15558_15_014.png) with our initialized
    empty dataset ![](img/B15558_12_259.png) and update ![](img/B15558_12_259.png)
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_017.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we train a classifier on this updated dataset ![](img/B15558_12_259.png)
    and learn a new policy ![](img/B15558_03_159.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**In the second iteration**, we use the new policy ![](img/B15558_03_158.png)
    to generate trajectories, create a new dataset ![](img/B15558_15_021.png) by taking
    only the states visited by the new policy ![](img/B15558_03_159.png), and ask
    the expert to provide the actions for those states.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we combine the dataset ![](img/B15558_15_023.png) with ![](img/B15558_09_088.png)
    and update ![](img/B15558_12_259.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_026.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we train a classifier on this updated dataset ![](img/B15558_15_027.png)
    and learn a new policy ![](img/B15558_15_028.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**In the third iteration**, we use the new policy ![](img/B15558_04_094.png)
    to generate trajectories and create a new dataset ![](img/B15558_15_030.png) by
    taking only the states visited by the new policy ![](img/B15558_15_031.png), and
    then we ask the expert to provide the actions for those states.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we combine the dataset ![](img/B15558_15_030.png) with ![](img/B15558_09_075.png)
    and update ![](img/B15558_09_075.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_035.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we train a classifier on this updated dataset ![](img/B15558_15_036.png)
    and learn a new policy ![](img/B15558_15_037.png). In this way, DAgger works in
    a series of iterations until it finds the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of Dagger; let's go into more detail
    and learn how DAgger finds the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DAgger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's suppose we have a human expert, and let's denote the expert policy with
    ![](img/B15558_15_038.png). We initialize an empty dataset ![](img/B15558_09_075.png)
    and also a novice policy ![](img/B15558_15_040.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first iteration, we create a new policy ![](img/B15558_15_041.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation implies that we create a new policy ![](img/B15558_04_086.png)
    by taking some amount of expert policy ![](img/B15558_15_044.png) and some amount
    of novice policy ![](img/B15558_15_045.png). How much of the expert policy and
    novice policy we take is decided by the parameter ![](img/B15558_06_030.png).
    The value of ![](img/B15558_15_047.png) is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_048.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The value of *p* is chosen between 0.1 and 0.9\. Since we are in iteration
    1, substituting *i* = 1, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, substituting ![](img/B15558_15_050.png) in equation (1), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_051.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe, in the first iteration, the policy ![](img/B15558_15_052.png)
    is just an expert policy ![](img/B15558_15_053.png). Now, we use this policy ![](img/B15558_15_041.png)
    and generate trajectories. Next, we create a new dataset ![](img/B15558_15_055.png)
    by collecting all the states visited by our policy ![](img/B15558_03_153.png)
    and ask the expert to provide actions of those states. So, our dataset will consist
    of ![](img/B15558_15_057.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we combine the dataset ![](img/B15558_15_058.png) with our initialized
    empty dataset ![](img/B15558_09_124.png) and update ![](img/B15558_12_259.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_017.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have an updated dataset ![](img/B15558_12_259.png), we train a classifier
    on this new dataset and extract a new policy. Let the new policy be ![](img/B15558_15_063.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second iteration, we create a new policy ![](img/B15558_15_064.png)
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_065.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation implies that we create a new policy ![](img/B15558_03_159.png)
    by taking some amount of expert policy ![](img/B15558_15_053.png) and some amount
    of policy ![](img/B15558_15_068.png) that we obtained in the previous iteration.
    We know that the value of beta is chosen as: ![](img/B15558_15_069.png). Thus,
    we have ![](img/B15558_15_070.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we use this policy ![](img/B15558_15_071.png) and generate trajectories.
    Next, we create a new dataset ![](img/B15558_15_072.png) by collecting all the
    states visited by our policy ![](img/B15558_15_073.png) and ask the expert to
    provide actions of those states. So, our dataset will consist of ![](img/B15558_15_074.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we combine the dataset ![](img/B15558_15_075.png) with ![](img/B15558_15_036.png)
    and update ![](img/B15558_12_259.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_026.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have an updated dataset ![](img/B15558_09_075.png), we train a classifier
    on this new dataset and extract a new policy. Let that new policy be ![](img/B15558_15_080.png).
  prefs: []
  type: TYPE_NORMAL
- en: We repeat these steps for several iterations to obtain the optimal policy. As
    we can observe in each iteration, we aggregate our dataset ![](img/B15558_09_088.png)
    and train a classifier to obtain the new policy. Notice that the value of ![](img/B15558_09_151.png)
    is decaying exponentially. This makes sense as over a series of iterations, our
    policy will become better and so we can reduce the importance of the expert policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how DAgger works, in the next section, we will look
    into the algorithm of DAgger for a better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – DAgger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The algorithm of DAgger is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize an empty dataset ![](img/B15558_12_259.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a policy ![](img/B15558_15_045.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For iterations *i* = 1 to *N*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a policy ![](img/B15558_15_085.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a trajectory using the policy ![](img/B15558_15_086.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dataset ![](img/B15558_15_087.png) by collecting states visited by
    the policy ![](img/B15558_15_086.png) and actions of those states provided by
    the expert ![](img/B15558_15_089.png). Thus, ![](img/B15558_15_090.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate the dataset as ![](img/B15558_15_091.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a classifier on the updated dataset ![](img/B15558_12_259.png) and extract
    a new policy ![](img/B15558_15_093.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned the DAgger algorithm, in the next section, we will
    learn about DQfD.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q learning from demonstrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned that in imitation learning, we try to learn from expert demonstrations.
    Can we make use of expert demonstrations in DQN and perform better? Yes! In this section,
    we will learn how to make use of expert demonstrations in DQN using an algorithm
    calledDQfD.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we have learned about several types of DQN. We started
    off with vanilla DQN, and then we explored various improvements to the DQN, such
    as double DQN, dueling DQN, prioritized experience replay, and more. In all these
    methods, the agent tries to learn from scratch by interacting with the environment.
    The agent interacts with the environment and stores their interaction experience
    in a buffer called a replay buffer and learns based on their experience.
  prefs: []
  type: TYPE_NORMAL
- en: In order for the agent to perform better, it has to gather a lot of experience
    from the environment, add it to the replay buffer, and train itself. However,
    this method costs us a lot of training time. In all the previous methods we have
    learned so far, we have trained our agent in a simulator, so the agent gathers
    experience in the simulator environment to perform better. To learn the optimal
    policy, the agent has to perform a lot of interactions with the environment, and
    some of these interactions give the agent a very bad reward. This is tolerable
    in a simulator environment. But how can we train the agent in a real-world environment?
    We can't train the agent by directly interacting with the real-world environment
    and by making a lot of bad actions in the real-world environment.
  prefs: []
  type: TYPE_NORMAL
- en: So, in those cases, we can train the agent in a simulator that corresponds to
    the particular real-world environment. But the problem is that it is hard to find
    an accurate simulator corresponding to the real-world environment for most use
    cases. However, we can easily obtain expert demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's suppose we want to train an agent to play chess. Let's assume
    we don't find an accurate simulator to train the agent to play chess. But we can
    easily obtain good expert demonstrations of an expert playing chess.
  prefs: []
  type: TYPE_NORMAL
- en: Now, can we make use of these expert demonstrations and train our agent? Yes!
    Instead of learning from scratch by interacting with the environment, if we add
    the expert demonstrations directly to the replay buffer and pre-train our agent
    based on these expert demonstrations, then the agent can learn better and faster.
  prefs: []
  type: TYPE_NORMAL
- en: This is the fundamental idea behind DQfD. We fill the replay buffer with expert
    demonstrations and pre-train the agent. Note that these expert demonstrations
    are used only for pre-training the agent. Once the agent is pre-trained, the agent
    will interact with the environment and gather more experience and make use of
    it for learning. Thus DQfD consists of two phases, which are pre-training and
    training.
  prefs: []
  type: TYPE_NORMAL
- en: First, we pre-train the agent based on the expert demonstrations, and then we
    train the agent by interacting with the environment. When the agent interacts
    with the environment, it collects some experience, and the agent's experience
    (self-generated data) also gets added to the replay buffer. The agent makes use
    of both the expert demonstrations and also the self-generated data for learning.
    We use a prioritized experience replay buffer and give more priority to the expert
    demonstrations than the self-generated data. Now that we have a basic understanding
    of DQfD, let's go into detail and learn how exactly it works.
  prefs: []
  type: TYPE_NORMAL
- en: Phases of DQfD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DQfD consists of two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-training phase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the pre-training phase, the agent does not interact with the environment.
    We directly add the expert demonstrations to the replay buffer and the agent learns
    by sampling the expert demonstrations from the replay buffer.
  prefs: []
  type: TYPE_NORMAL
- en: The agent learns from expert demonstrations by minimizing the loss *J*(*Q*)
    using gradient descent. However, pre-training with expert demonstrations alone
    is not sufficient for the agent to perform better because the expert demonstrations
    will not contain all possible transitions. But the pretraining with expert demonstrations
    acts as a good starting point to train our agent. Once the agent is pre-trained
    with demonstrations, then during the training phase, the agent will perform better
    actions in the environment from the initial iteration itself instead of performing
    random actions, and so the agent can learn quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Training phase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the agent is pre-trained, we start the training phase, where the agent
    interacts with the environment and learns based on its experience. Since the agent
    has already learned some useful information from the expert demonstrations in
    the pre-training phase, it will not perform random actions in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase, the agent interacts with the environment and stores
    its transition information (experience) in the replay buffer. We learned that
    our replay buffer will be pre-filled with the expert demonstrations data. So,
    now, our replay buffer will consist of a mixture of both expert demonstrations
    and the agent's experience (self-generated data). We sample a minibatch of experience
    from the replay buffer and train the agent. Note that here we use a prioritized
    replay buffer, so while sampling, we give more priority to the expert demonstrations
    than the agent-generated data. In this way, we train the agent by sampling experience
    from the replay buffer and minimize the loss using gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that the agent interacts with the environment and stores the experience
    in the replay buffer. If the replay buffer is full, then we overwrite the buffer
    with new transition information generated by the agent. However, we won't overwrite
    the expert demonstrations. So, the expert demonstrations will always remain in
    the replay buffer so that the agent can make use of expert demonstrations for
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have learned how to pre-train and train an agent with expert demonstrations.
    In the next section, we will learn about the loss function of DQfD.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function of DQfD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The loss function of DQfD comprises the sum of four losses:'
  prefs: []
  type: TYPE_NORMAL
- en: Double DQN loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: N-step double DQN loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Supervised classification loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: L2 loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we will look at each of these losses.
  prefs: []
  type: TYPE_NORMAL
- en: '**Double DQN loss** – ![](img/B15558_15_094.png) represents the 1-step double
    DQN loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '**N-step double DQN loss** – ![](img/B15558_15_095.png) represents the n-step
    double DQN loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised classification loss** – ![](img/B15558_15_096.png) represents
    the supervised classification loss. It is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_097.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[E] is the action taken by the expert.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*l*(*a*[E], *a*) is known as the margin function or margin loss. It will be
    0 when the action taken is equal to the expert action *a* = *a*[E]; else, it is
    positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 regularization loss** – ![](img/B15558_15_098.png) represents the L2 regularization
    loss. It prevents the agent from overfitting to the demonstration data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the final loss function will be the sum of all the preceding four losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_099.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the value of ![](img/B15558_15_100.png) acts as a weighting factor and
    helps us to control the importance of the respective loss.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how DQfD works, we will look into the algorithm of
    DQfD in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – DQfD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The algorithm of DQfD is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_10_037.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_15_102.png) by copying
    the main network parameter ![](img/B15558_09_123.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_12_259.png) with the expert demonstrations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set *d*: the number of time steps we want to delay updating the target network
    parameter'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pre-training phase**: For steps ![](img/B15558_15_105.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a minibatch of experience from the replay buffer ![](img/B15558_12_259.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss *J*(*Q*)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameter of the network using gradient descent
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod *d* = 0:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target network parameter ![](img/B15558_09_084.png) by copying the
    main network parameter ![](img/B15558_09_118.png)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Training phase**: For steps *t* = 1, 2, ..., *T*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an action
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action and move to the next state, observe the reward,
    and store this transition information in the replay buffer ![](img/B15558_09_075.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a minibatch of experience from the replay buffer ![](img/B15558_12_259.png)
    with prioritization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss *J*(*Q*)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameter of the network using gradient descent
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod *d* = 0:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target network parameter ![](img/B15558_09_059.png) by copying the
    main network parameter ![](img/B15558_09_118.png)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: That's it! In the next section, we will learn about a very interesting concept
    called IRL.
  prefs: []
  type: TYPE_NORMAL
- en: Inverse reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Inverse Reinforcement Learning (IRL)** is one of the most exciting fields
    of reinforcement learning. In reinforcement learning, our goal is to learn the
    optimal policy. That is, our goal is to find the optimal policy that gives the
    maximum return (sum of rewards of the trajectory). In order to find the optimal
    policy, first, we should know the reward function. A reward function tells us
    what reward we obtain by performing an action *a* in the state *s*. Once we have
    the reward function, we can train our agent to learn the optimal policy that gives
    the maximum reward. But the problem is that designing the reward function is not
    that easy for complex tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider designing the reward function for tasks such as an agent learning to
    walk, self-driving cars, and so on. In these cases, designing the reward function
    is not that handy and involves assigning rewards to a variety of agent behaviors.
    For instance, consider designing the reward function for an agent learning to
    drive a car. In this case, we need to assign a reward for every behavior of the
    agent. For example, we can assign a high reward if the agent follows the traffic
    signal, avoids pedestrians, doesn't hit any objects, and so on. But designing
    the reward function in this way is not optimal, and there is also a good chance
    that we might miss out on several behaviors of an agent.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, now the question is can we learn the reward function? Yes! If we have
    expert demonstrations, then we can learn the reward function from the expert demonstrations.
    But how can we do that exactly? Here is where IRL helps us. As the name suggests,
    IRL is the inverse of reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: In RL, we try to find the optimal policy given the reward function, but in IRL,
    we try to learn the reward function given the expert demonstrations. Once we have
    derived the reward function from the expert demonstrations using IRL, we can use
    the reward function to train our agent to learn the optimal policy using any reinforcement
    learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: IRL consists of several interesting algorithms. In the next section, we will
    learn one of the most popular IRL algorithms, called maximum entropy IRL.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum entropy IRL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will learn how to extract a reward function from the given
    set of expert demonstrations using an IRL algorithm called maximum entropy IRL.
    Before diving into maximum entropy IRL, let's learn some of the important terms
    that are required to understand how maximum entropy IRL works.
  prefs: []
  type: TYPE_NORMAL
- en: Key terms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Feature vector** – We can represent the state by a feature vector *f*. Let''s
    say we have a state *s*; its feature vector can then be defined as *f*[s].'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature count** –Say we have a trajectory ![](img/B15558_14_164.png); the
    feature count of the trajectory is then defined as the sum of the feature vectors
    of all the states in the trajectory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_114.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_15_115.png) denotes the feature count of the trajectory
    ![](img/B15558_14_164.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**Reward function** – The reward function can be defined as the linear combination
    of the features, that is, the sum of feature vectors multiplied by a weight ![](img/B15558_09_118.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_118.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_09_123.png) denotes the weight and *f*[s] denotes the feature
    vector. Note that this ![](img/B15558_09_118.png) is what we are trying to learn.
    When we obtain the optimal ![](img/B15558_15_121.png), then we will have a correct
    reward function. We will learn how we can find the optimal ![](img/B15558_09_106.png)
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent the preceding equation in sigma notation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_123.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that the feature count is the sum of feature vectors of all the states
    in the trajectory, so from (2), we can rewrite the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_124.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the reward of the trajectory is just the weight multiplied by the feature
    count of the trajectory.
  prefs: []
  type: TYPE_NORMAL
- en: Back to maximum entropy IRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's learn how maximum entropy IRL works. Consider that we have expert
    demonstrations ![](img/B15558_15_125.png). Our goal is to learn the reward function
    from the given expert demonstrations. How can we do that?
  prefs: []
  type: TYPE_NORMAL
- en: We have already learned that the reward function is given as ![](img/B15558_15_126.png).
    Finding the optimal parameter ![](img/B15558_09_098.png) helps us to learn the
    correct reward function. So, we will sample a trajectory ![](img/B15558_14_140.png)
    from the expert demonstrations ![](img/B15558_15_036.png) and try to find the
    reward function by finding the optimal parameter ![](img/B15558_09_106.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability of a trajectory being sampled from the expert demonstrations
    is directly proportional to the exponential of the reward function. That is, trajectories
    that obtain more rewards are more likely to be sampled from our demonstrations
    than those trajectories that obtain less rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_131.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability should be in the range of 0 to 1, right? But the value of ![](img/B15558_15_132.png)
    will not be in the range of 0 to 1\. So, in order to normalize that, we introduce
    *z*, which acts as the normalization constant and is given as ![](img/B15558_15_133.png).
    We can rewrite our preceding equation with *z* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_134.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, our objective is to maximize ![](img/B15558_15_135.png), that is, to maximize
    the log probability of selecting trajectories that give more rewards. So, we can
    define our objective function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_136.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *M* denotes the number of demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Substituting (3) in (4), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_137.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the log rule, ![](img/B15558_15_138.png), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_139.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The logarithmic and exponential terms cancel each other out, so the preceding
    equation becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_140.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that ![](img/B15558_15_133.png); substituting the value of *z*, we
    can rewrite the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that ![](img/B15558_15_126.png); substituting the value of ![](img/B15558_15_144.png),
    our final simplified objective function is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_145.png)'
  prefs: []
  type: TYPE_IMG
- en: To find the optimal parameter ![](img/B15558_09_087.png), we compute the gradient
    of the preceding objective function ![](img/B15558_15_147.png) and update the
    value of ![](img/B15558_09_106.png) as ![](img/B15558_15_149.png). In the next
    section, we will learn how to compute the gradient ![](img/B15558_09_043.png).
  prefs: []
  type: TYPE_NORMAL
- en: Computing the gradient
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We learned that our objective function is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_151.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we compute the gradient of the objective function with respect to ![](img/B15558_15_152.png).
    After computation, our gradient is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_153.png)![](img/B15558_15_154.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The average of the feature count is just the feature expectation ![](img/B15558_15_155.png),
    so we can substitute ![](img/B15558_15_156.png) and rewrite the preceding equation
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_157.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite the preceding equation by combining all the states of the trajectories
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_158.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, using the preceding equation, we compute gradients and update the parameter
    ![](img/B15558_13_233.png). If you look at the preceding equation, we can easily
    compute the first term, which is just the feature expectation ![](img/B15558_15_160.png),
    but what about the ![](img/B15558_15_161.png) in the second term? ![](img/B15558_15_162.png)
    is called the state visitation frequency and it represents the probability of
    being in a given state. Okay, how can we compute ![](img/B15558_15_163.png)?
  prefs: []
  type: TYPE_NORMAL
- en: If we have a policy ![](img/B15558_03_082.png), then we can use the policy to
    compute the state visitation frequency. But we don't have any policy yet. So,
    we can use a dynamic programming method, say, value iteration, to compute the
    policy. However, in order to compute the policy using the value iteration method,
    we require a reward function. So, we just feed our reward function ![](img/B15558_15_165.png)
    and extract the policy using the value iteration. Then, using the extracted policy,
    we compute the state visitation frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in computing the state visitation frequency using the policy
    ![](img/B15558_03_008.png) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the probability of visiting a state *s* at a time *t* be ![](img/B15558_15_167.png).
    We can write the probability of visiting the initial state *s*[1] at a first time
    step *t* = 1 as: ![](img/B15558_15_168.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then for time steps *t* = 1 to *T*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute ![](img/B15558_15_169.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the state visitation frequency as ![](img/B15558_15_170.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get a clear understanding of how maximum entropy IRL works, let's look into
    the algorithm in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – maximum entropy IRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The algorithm of maximum entropy IRL is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the parameter ![](img/B15558_09_118.png) and gather the demonstrations
    ![](img/B15558_15_125.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *N* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the reward function ![](img/B15558_15_126.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the policy using value iteration with the reward function obtained in
    the previous step
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute state visitation frequency ![](img/B15558_15_174.png) using the policy
    obtained in the previous step
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient with respect to ![](img/B15558_14_004.png), that is, ![](img/B15558_15_176.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value of ![](img/B15558_09_054.png) as ![](img/B15558_13_286.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, over a series of iterations, we will find an optimal parameter ![](img/B15558_09_054.png).
    Once we have ![](img/B15558_09_087.png), we can use it to define the correct reward
    function ![](img/B15558_15_181.png). In the next section, we will learn about
    GAIL.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial imitation learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative Adversarial Imitation Learning (GAIL)** is another very popular
    IRL algorithm. As the name suggests, it is based on **Generative Adversarial Networks
    (GANs)**, which we learned about in *Chapter 7*, *Deep Learning Foundations*.
    To understand how GAIL works, we should first recap how GANs work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a GAN, we have two networks: one is the generator and the other is the discriminator.
    The role of the generator is to generate new data points by learning the distribution
    of the input dataset. The role of the discriminator is to classify whether a given
    data point is generated by the generator (learned distribution) or whether it
    is from the real data distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the loss function of a GAN implies minimizing the **Jensen Shannon
    (JS)** divergence between the real data distribution and the fake data distribution
    (learned distribution). The JS divergence is used to measure how two probability
    distributions differ from each other. Thus, when the JS divergence between the
    real and fake distributions is zero, it means that the real and fake data distributions
    are equal, that is, our generator network has successfully learned the real distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's learn how to make use of GANs in an IRL setting. First, let's introduce
    a new term called **occupancy measure**.It is defined as the distribution of the
    states and actions that our agent comes across while exploring the environment
    with some policy ![](img/B15558_03_008.png). In simple terms, it is basically
    the distribution of state-action pairs following a policy ![](img/B15558_03_084.png).
    The occupancy measure of a policy ![](img/B15558_03_008.png) is denoted by ![](img/B15558_15_185.png).
  prefs: []
  type: TYPE_NORMAL
- en: In the imitation learning setting, we have an expert policy, and let's denote
    the expert policy by ![](img/B15558_15_186.png). Similarly, let's denote the agent's
    policy by ![](img/B15558_10_111.png). Now, our goal is to make our agent learn
    the expert policy. How can we do that? If we make the occupancy measure of the
    expert policy and the agent policy equal, then it implies that our agent has successfully
    learned the expert policy. That is, the occupancy measure is the distribution
    of the state-action pairs following a policy. If we can make the distribution
    of state-action pairs of the agent's policy equal to the distribution of state-action
    pairs of the expert's policy, then it means that our agent has learned the expert
    policy. Let's explore how we can do this using GANs.
  prefs: []
  type: TYPE_NORMAL
- en: We can perceive the occupancy measure of the expert policy as the real data
    distribution and the occupancy measure of the agent policy as the fake data distribution.
    Thus, minimizing the JS divergence between the occupancy measure of the expert
    policy ![](img/B15558_15_188.png) and the occupancy measure of the agent policy
    ![](img/B15558_15_189.png) implies that the agent will learn the expert policy.
  prefs: []
  type: TYPE_NORMAL
- en: With GANs, we know that the role of the generator is to generate new data points
    by learning the distribution of a given dataset. Similarly, in GAIL, the role
    of the generator is to generate a new policy by learning the distribution (occupancy
    measure) of the expert policy. The role of the discriminator is to classify whether
    the given policy is the expert policy or the agent policy.
  prefs: []
  type: TYPE_NORMAL
- en: With GANs, we know that, for a generator, the optimal discriminator is the one
    that is not able to distinguish between the real and fake data distributions;
    similarly, in GAIL, the optimal discriminator is the one that is unable to distinguish
    whether the generated state-action pair is from the agent policy or the expert
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make ourselves clear, let''s understand the terms we use in GAIL by relating
    to GAN terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real data distribution** – Occupancy measure of the expert policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fake data distribution** – Occupancy measure of the agent policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real data** – State-action pair generated by the expert policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fake data** – State-action pair generated by the agent policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a nutshell, we use the generator to generate the state-action pair in a
    way that the discriminator is not able to distinguish whether the state-action
    pair is generated using the expert policy or the agent policy. Both the generator
    and discriminator are neural networks. We train the generator to generate a policy
    similar to the expert policy using TRPO. The discriminator is a classifier, and
    it is optimized using Adam. Thus, we can define the objective function of GAIL
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_190.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_09_054.png) is the parameter of the generator and ![](img/B15558_15_192.png)
    is the parameter of the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an understanding of how GAIL works, let's get into more detail
    and learn how the preceding equation is derived.
  prefs: []
  type: TYPE_NORMAL
- en: Formulation of GAIL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we explore the math of GAIL and see how it works. You can
    skip this section if you are not interested in math. We know that in reinforcement
    learning, our objective is to find the optimal policy that gives the maximum reward.
    It can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_193.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite our objective function by adding the entropy of a policy as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_194.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation tells us that we can maximize the entropy of the policy
    while also maximizing the reward. Instead of defining the objective function in
    terms of the reward, we can also define the objective function in terms of cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, we can define our RL objective function in terms of cost, as our objective
    is to find an optimal policy that minimizes the cost; this can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_195.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *c* is the cost. Thus, given the cost function, our goal is to find the
    optimal policy that minimizes the cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s talk about IRL. We learned that in IRL, our objective is to find
    the reward function from the given set of expert demonstrations. We can also define
    the objective of IRL in terms of cost instead of reward. That is, we can define
    our IRL objective function in terms of cost, as our objective is to find the cost
    function under which the expert demonstration is optimal. The objective can be
    expressed using maximum causal entropy IRL as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_196.png)'
  prefs: []
  type: TYPE_IMG
- en: What does the preceding equation imply? In the IRL setting, our goal is to learn
    the cost function given the expert demonstrations (expert policy). We know that
    the expert policy performs better than the other policy, so we try to learn the
    cost function *c*, which assigns low cost to the expert policy and high cost to
    other policies. Thus, the preceding objective function implies that we try to
    find a cost function that assigns low cost to the expert policy and high cost
    to other policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce overfitting, we introduce the regularizer ![](img/B15558_15_197.png)
    and rewrite the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_198.png)'
  prefs: []
  type: TYPE_IMG
- en: From equation (6), we learned that in a reinforcement learning setting, given
    a cost, we obtain the optimal policy, and from (7), we learned that in an IRL
    setting, given an expert policy (expert demonstration), we obtain the cost. Thus,
    from (6) and (7), we can observe that the output of IRL can be sent as an input
    to the RL. That is, IRL results in the cost function and we can use this cost
    function as an input in RL to learn the optimal policy. Thus, we can write ![](img/B15558_15_199.png),
    which implies that the result of IRL is fed as an input to the RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can express this in a functional composition form as ![](img/B15558_15_200.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_201.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In equation (8), the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_202.png) is the convex conjugate of the regularizer ![](img/B15558_15_203.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B15558_15_204.png) is the occupancy measure of the agent policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_15_205.png) is the occupancy measure of the expert policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn how exactly the equation (8) is derived, you can refer to the GAIL
    paper given in the *Further reading* section at the end of the chapter. The objective
    function (equation (8)) implies that we try to find the optimal policy whose occupancy
    measure is close to the occupancy measure of the expert policy. The occupancy
    measure between the agent policy and the expert policy is measured by ![](img/B15558_15_206.png).
    There are several choices for the regularizer ![](img/B15558_15_207.png). We use
    a generative adversarial regularizer ![](img/B15558_15_208.png) and write our
    equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_209.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, minimizing ![](img/B15558_15_210.png) basically implies that we minimize
    the JS divergence between the occupancy measure of the agent policy ![](img/B15558_15_211.png)
    and the expert policy ![](img/B15558_15_212.png). Thus, we can rewrite the RHS
    of the equation (9) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_213.png)![](img/B15558_15_214.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/B15558_12_326.png) is just the policy regularizer. We know that
    the JS divergence between the occupancy measure of the agent policy ![](img/B15558_15_185.png)
    and the expert policy ![](img/B15558_15_217.png) is minimized using the GAN, so
    we can just replace ![](img/B15558_15_218.png) in the preceding equation with
    the GAN objective function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_219.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_09_087.png) is the parameter of the generator and ![](img/B15558_15_221.png)
    is the parameter of the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, our final objective function of GAIL becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_15_222.png)'
  prefs: []
  type: TYPE_IMG
- en: The objective equation implies that we can find the optimal policy by minimizing
    the occupancy measure of the expert policy and the agent policy, and we minimize
    that using GANs.
  prefs: []
  type: TYPE_NORMAL
- en: The role of the generator is to generate a policy by learning the occupancy
    measure of the expert policy, and the role of the discriminator is to classify
    whether the generated policy is from the expert policy or the agent policy. So,
    we train the generator using TRPO and the discriminator is basically a neural
    network that tells us whether the policy generated by the generator is the expert
    policy or the agent policy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by understanding what imitation learning is and how supervised
    imitation learning works. Next, we learned about the DAgger algorithm, where we
    aggregate the dataset obtained over a series of iterations and learn the optimal
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: After looking at DAgger, we learned about DQfD, where we prefill the replay
    buffer with expert demonstrations and pre-train the agent with expert demonstrations
    before the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we learned about IRL. We understood that in reinforcement learning,
    we try to find the optimal policy given the reward function, but in IRL, we try
    to learn the reward function given the expert demonstrations. When we have derived
    the reward function from the expert demonstrations using IRL, we can use the reward
    function to train our agent to learn the optimal policy using any reinforcement
    learning algorithm. We then explored how to learn the reward function using the
    maximum entropy IRL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about GAIL, where we used GANs to learn
    the optimal policy. In the next chapter, we will explore a reinforcement learning
    library called Stable Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assess our understanding of imitation learning and IRL. Try answering
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How does supervised imitation learning work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does DAgger differ from supervised imitation learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the different phases of DQfD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need IRL?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a feature vector?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does GAIL work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, refer to the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A Reduction of Imitation Learning and Structured Prediction to No-Regret
    Online Learning** by *Stephane Ross*, *Geoffrey J. Gordon*, *J. Andrew Bagnell*,
    [https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Q-learning from Demonstrations** by *Todd Hester*, *et al*., [https://arxiv.org/pdf/1704.03732.pdf](https://arxiv.org/pdf/1704.03732.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maximum Entropy Inverse Reinforcement Learning** by *Brian D. Ziebart*, *Andrew
    Maas*, *J.Andrew Bagnell*, *Anind K. Dey*, [https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf](https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative Adversarial Imitation Learning** by *Jonathan Ho*, *Stefano Ermon*,
    [https://arxiv.org/pdf/1606.03476.pdf](https://arxiv.org/pdf/1606.03476.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
