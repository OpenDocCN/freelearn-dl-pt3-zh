- en: Understanding Recurrent Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts
    of Neural Networks*, and [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, we took an in-depth look at the properties
    of general feedforward networks and their specialized incarnation, **Convolutional
    Neural Networks** (**CNNs**). In this chapter, we'll close this story arc with **Recurrent
    Neural Networks** (**RNNs**). The NN architectures we discussed in the previous
    chapters take in a fixed-sized input and provide a fixed-sized output. RNNs lift
    this constraint with their ability to process input sequences of a variable length
    by defining a recurrent relationship over these sequences (hence the name). If
    you are familiar with some of the topics that will be discussed in this chapter,
    you can skip them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing long short-term memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing gated recurrent units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are neural networks that can process sequential data with a variable length.
    Examples of such data include the words of a sentence or the price of stock at
    various moments in time. By using the word sequential, we imply that the elements
    of the sequence are related to each other and that their order matters. For example,
    if we take a book and randomly shuffle all of the words in it, the text will lose
    its meaning, even though we'll still know the individual words. Naturally, we
    can use RNNs to solve tasks that relate to sequential data. Examples of such tasks
    are language translation, speech recognition, predicting the next element of a
    time series, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs get their name because they apply the same function over a sequence recurrently.
    We can define an RNN as a recurrence relation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaf28f6c-9839-430b-b02a-7b0580618ccf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *f* is a differentiable function, **s***[t]* is a vector of values called
    the internal network state (at step *t*), and **x***[t]* is the network input
    at step *t*. Unlike regular networks, where the state only depends on the current
    input (and network weights), here, **s***[t]* is a function of both the current
    input as well as the previous state, **s***[t-1]*. You can think of **s***[t-1]* as
    the network''s summary of all of the previous inputs. This is unlike the regular
    feedforward networks (including CNNs), which take only the current input sample
    as input. The recurrence relationship defines how the state evolves step by step
    over the sequence via a feedback loop over previous states, as illustrated in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4773e34-6b4f-4007-8316-601d70c94e8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: Visual illustration of the RNN recurrence relation: s*[t] =*Ws*[t-1] + *Ux*[t;]* The
    final output will be y*[t] =*Vs*[t] . *Right: The RNN states are recurrently unfolded
    over the sequence *t-1, t, t+1*. Note that the parameters U, V, and W are shared
    between all steps'
  prefs: []
  type: TYPE_NORMAL
- en: 'The RNN has three sets of parameters (or weights):'
  prefs: []
  type: TYPE_NORMAL
- en: '**U** transforms the input, **x***[t]*, into the state, **s***[t].*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**W** transforms the previous state, **s***[t-1]*, into the current state, **s***[t].*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V** maps the newly computed internal state, **s***[t]*, to the output, **y***[t].*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**U**, **V**, and **W** apply linear transformation over their respective inputs.
    The most basic case of such a transformation is the familiar weighted sum we know
    and love. We can now define the internal state and the network output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/584cff1d-869f-4eae-a3e4-6d928b765678.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0d0c0977-4890-4872-8e65-ad1e5cef01af.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *f* is the non-linear activation function (such as tanh, sigmoid, or ReLU).
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a word-level language model, the input, *x*, will be a sequence
    of words encoded in input vectors *(***x***[1] ...* **x***[t] ...)*. The state, *s*, will
    be a sequence of state vectors *(***s***[1] ...* **s***[t] ... )*. Finally, the
    output, *y*, will be a sequence of probability vectors *(***y***[1] ...* **y***[t] ...
    )* of the next words in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in an RNN, each state is dependent on all of the previous computations
    through this recurrence relation. An important implication of this is that RNNs
    have memory over time because the states, *s*, contain information based on the
    previous steps. In theory, RNNs can remember information for an arbitrarily long
    period of time, but in practice, they are limited to looking back only a few steps.
    We will address this issue in more detail in the *Vanishing and exploding gradients* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RNN we described here is somewhat equivalent to a single layer regular
    neural network (with an additional recurrence relationship). As we now know from
    [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts of
    Neural Networks*, a network with a single layer has some serious limitations.
    Fear not! As with regular networks, we can stack multiple RNNs to form a **stacked
    RNN**. The cell state, **s***^l[t]*, of an RNN cell at level *l* at time *t* will
    take the output, **y***[t]^(l-1)*, of the RNN cell from level *l-1* and the previous cell
    state, **s***^l[t-1]*, of the cell at the same level, *l*, as the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10bc347d-66bb-471a-a2a7-476e5854a08c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following diagram, we can see an unfolded, stacked RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bede0919-4493-44ef-987c-42c64ddaa706.png)'
  prefs: []
  type: TYPE_IMG
- en: Stacked RNN
  prefs: []
  type: TYPE_NORMAL
- en: The RNN we've discussed so far takes the preceding elements of the sequence
    to produce an output. This makes sense for tasks such as time series prediction,
    where we want to predict the next element of the series based on the previous
    elements. But it also imposes unnecessary limitations on other tasks, such as
    the ones from the NLP domain. As we saw in [Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml),
    *Language Modeling*, we can obtain a lot of information about a word by its context
    and it makes sense to extract that context from both the preceding and succeeding
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extend the regular RNN to the so-called **bidirectional RNN** to cover
    this scenario, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb274fc3-b90e-4e5d-bb80-4e9bd1fc1e1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Bidirectional RNN
  prefs: []
  type: TYPE_NORMAL
- en: 'This network has two propagation loops working in both directions, that is,
    left to right from steps *t* to *t+1* and right to left from steps *t+1* to *t*.
    We''ll denote the right to left propagation related notations with the prim symbol
    (not to be confused with derivatives). At each time step, *t*, the network maintains
    two internal state vectors: **s***[t]* for the left to right propagation and **s***''[t]*
    for the right to left propagation. The right to left phase has its own set of
    input weights, *U''* and *W''* , mirroring the weights, **U** and **W**, for the
    left to right phase. The formula for the right to left hidden state vector is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c13ef94c-4676-406e-8c86-ff41b28e8cb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of the network, **y***[t]*, is a combination of the internal states, **s***[t]*
    and **s***[t+1]*. One way to combine them is with concatenation. In this case,
    we''ll denote the weight matrix of the concatenated states to the output with
    **V**. Here, the formula for the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18d9dcef-80b9-42f3-a675-907cc3c52d53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, we can simply sum the two state vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/598c746e-ca6e-4734-85bb-84b909c5ca84.png)'
  prefs: []
  type: TYPE_IMG
- en: Because RNNs are not limited to processing fixed-size inputs, they really expand
    the possibilities of what we can compute with neural networks, such as sequences
    of different lengths or images of varied sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go over some different combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**One**-**to**-**one**: This is non-sequential processing, such as feedforward neural
    networks and CNNs. Note that, there isn''t much difference between a feedforward
    network and applying an RNN to a single time step. An example of one-to-one processing
    is image classification, which we looked at in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, and [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One**-**to**-**many**: This processing generates a sequence based on a single
    input, for example, caption generation from an image (*Show and Tell: A Neural
    Image Caption Generator*, [https://arxiv.org/abs/1411.4555](https://arxiv.org/abs/1411.4555)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many**-**to**-**one**: This processing outputs a single result based on a
    sequence, for example, sentiment classification of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many**-**to**-**many indirect**: A sequence is encoded into a state vector,
    after which this state vector is decoded into a new sequence, for example, language
    translation (*Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation, *[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078) and *Sequence
    to Sequence Learning with Neural Networks*, [http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many**-**to**-**many direct:** This outputs a result for each input step,
    for example, frame phoneme labeling in speech recognition*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The many-to-many models are often referred to as **sequence-to-sequence** (**seq2seq**)
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a graphical representation of the preceding input-output combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b66a358c-a343-4da9-b294-a1430120170c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'RNN input-output combinations: Inspired by http://karpathy.github.io/2015/05/21/rnn-effectiveness/.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've introduced RNNs, in the next section, we'll implement a simple
    RNN example from scratch to improve our knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: RNN implementation and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding section, we briefly discussed what RNNs are and what problems
    they can solve. Let''s dive into the details of an RNN and how to train it with
    a very simple toy example: counting ones in a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: In this problem, we will teach a basic RNN how to count the number of ones in
    the input and then output the result at the end of the sequence. This is an example
    of a many-to-one relationship, which we defined in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll implement this example with Python (no DL libraries) and NumPy. An example
    of the input and output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The RNN we''ll use is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c817848d-b9e9-4fb0-928d-02becdbb5127.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic RNN for counting ones in the input
  prefs: []
  type: TYPE_NORMAL
- en: 'The network will have only two parameters: an input weight, **U**, and a recurrence
    weight, **W**. The output weight, **V**, is set to 1 so that we just read out
    the last state as the output, **y**.'
  prefs: []
  type: TYPE_NORMAL
- en: Since *s[t]*, *x[t]*, *U*, and *W* are scalar values, we won't use the matrix
    notation (bold capital letters) in the *RNN implementation and training* section
    and its subsections. However, note that the generic versions of these formulas
    use matrix and vector parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue, let''s add some code so that our example can be executed.
    We''ll import `numpy` and define our training and data, `x`, and labels, `y`. `x` is
    two-dimensional since the first dimension represents the sample in the mini-batch.
    For the sake of simplicity, we''ll use a mini-batch with a single sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The recurrence relation defined by this network is [![](img/6a316df2-66ff-4de3-ac56-4e852c351973.png)].
    Note that this is a linear model since we don''t apply a non-linear function in
    this formula. We can implement a recurrence relationship as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The states, *s[t]*, and the weights, *W* and *U*, are single scalar values. A
    good solution to this is to just get the sum of the inputs across the sequence.
    If we set *U=1*, then whenever input is received, we will get its full value.
    If we set *W=1*, then the value we would accumulate would never decay. So, for
    this example, we would get the desired output: 3.'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, let's use this simple example to network the training and implementation
    of this neural network. This will be interesting, as we will see in the rest of
    this section. First, let's look at how we can get this result through backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Backpropagation through time is the typical algorithm we use to train recurrent
    networks (*Backpropagation Through Time: What It Does and How to Do It*, [http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf](http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf)).
    As the name suggests, it''s based on the backpropagation algorithm we discussed
    in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts
    of Neural Networks*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main difference between regular backpropagation and backpropagation through
    time is that the recurrent network is unfolded through time for a certain number
    of time steps (as illustrated in the preceding diagram). Once the unfolding is
    complete, we end up with a model that is quite similar to a regular multi-layer
    feedforward network, that is, one hidden layer of that network represents one
    step through time. The only differences are that each layer has multiple inputs:
    the previous state, *s[t-1]*, and the current input, *x[t]*. The parameters *U* and *W* are
    shared between all of the hidden layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward pass unwraps the RNN along the sequence and builds a stack of states
    for each step. In the following code block, we can see an implementation of the
    forward pass, which returns the activation, *s*, for each recurrent step and each
    sample in the batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our forward step and loss function, we can define how the gradient
    is propagated backward. Since the unfolded RNN is equivalent to a regular feedforward
    network, we can use the backpropagation chain rule we introduced in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Because the weights, *W* and *U*, are shared across the layers, we'll accumulate
    the error derivatives for each recurrent step, and in the end, we'll update the
    weights with the accumulated value.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to get the gradient of the output, **s***[t]*, with respect
    to the loss function (*∂J/∂s*). Once we have it, we''ll propagate it backward
    through the stack of activities we built during the forward step. This backward
    pass pops activities off of the stack to accumulate their error derivatives at
    each time step. The recurrence relation to propagate this gradient through the
    network can be written as follows (chain rule):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b2df19a-bc53-40d8-97f4-44a99dce9073.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *J* is the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradients of the weights, *U* and *W*, are accumulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e86ed439-4010-4d3c-bb46-ae9c1d94a108.png)![](img/21a0bab0-78dc-4092-9285-083b28271940.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is an implementation of the backward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradients for `U` and `W` are accumulated in `gU` and `gW`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now try to use gradient descent to optimize our network. We compute `gradients` (using
    mean square error) with the help of the `backward` function and we use them to
    update the `weights` value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll implement the related `plot_training` function, which displays
    the `loss` function and the gradients for each weight over the epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8b9ef4b-3dd9-449e-a387-15c4cd59b827.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The RNN loss: the uninterrupted line represents the loss, where the dashed
    lines represent the weight gradients during training'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've learned about backpropagation through time, let's discuss how
    the familiar vanishing and exploding gradient problems affect it.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing and exploding gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding example has an issue, though. Let''s run the training process
    with a longer sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The reason for these warnings is that the final parameters, *U* and *W*, end
    up as **Not a Number** (**NaN**). To display the gradients properly, we'll need
    to change the scale of the gradient axis in the `plot_training` function from
    `ax1.set_ylim(-3, 20)` to `ax1.set_ylim(-3, 600)`, as well as the scale of the
    loss axis from `ax2.set_ylim(-3, 10)` to `ax2.set_ylim(-3, 200)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the program will produce the following diagram of the new loss and gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d20b08b7-7629-4068-9b31-bc8201873414.png)'
  prefs: []
  type: TYPE_IMG
- en: Parameters and loss function during exploding gradients scenario
  prefs: []
  type: TYPE_NORMAL
- en: 'In the initial epochs, the gradients slowly increase, similar to the way they
    increased for the shorter sequence. However, when they get to epoch 23 (the exact
    epoch is unimportant, though), the gradient becomes so large that it goes out
    of the range of the `float` variable and becomes NaN (as illustrated by the jump
    in the plot). This problem is known as exploding gradients. We can stumble upon
    exploding gradients in a regular feedforward NN, but it is especially pronounced
    in RNNs. To understand why, let''s recall the recurrent gradient propagation chain
    rule for the two consecutive sequence steps we defined in the *Backpropagation
    through time* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fba12913-286f-4108-8ddf-650abdee55f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Depending on the sequence''s length, an unfolded RNN can be much deeper compared
    to a regular network. At the same time, the weights, *W*, of an RNN are shared
    across all of the steps. Therefore, we can generalize this formula to compute
    the gradient between two non-consecutive steps of the sequence. Because *W* is
    shared, the equation forms a geometric progression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d8f4eb3-d708-4568-938b-1815bf786810.png)'
  prefs: []
  type: TYPE_IMG
- en: In our simple linear RNN, the gradient grows exponentially if *|W|* > *1* (exploding
    gradient), where *W* is a single scalar weight, for example, 50 time steps over
    W=1.5 is *W^(50) ≈ 637621500*. The gradient shrinks exponentially if *|W| <1* (vanishing
    gradient), for example, 10 time steps over *W=0.6* is *W^(20) = 0.00097*. If the
    weight parameter, **W**, is a matrix instead of a scalar, this exploding or vanishing
    gradient is related to the largest eigenvalue (*ρ*) of **W** (also known as a
    spectral radius). It is sufficient for *ρ* < *1* for the gradients to vanish,
    and it is necessary for *ρ* > *1* for them to explode.
  prefs: []
  type: TYPE_NORMAL
- en: The vanishing gradients problem, which we first mentioned in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The
    Nuts and Bolts of Neural Networks*, has another more subtle effect in RNNs. The
    gradient decays exponentially over the number of steps to a point where it becomes
    extremely small in the earlier states. In effect, they are overshadowed by the
    larger gradients from more recent time steps, and the network's ability to retain
    the history of these earlier states vanishes. This problem is harder to detect
    because the training will still work and the network will produce valid outputs
    (unlike with exploding gradients). It just won't be able to learn long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are familiar with some of the problems of RNNs. This knowledge will
    serve us well because, in the next section, we'll discuss how to solve these problems
    with the help of a special type of RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing long short-term memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber studied the problems of vanishing and exploding gradients
    extensively and came up with a solution called **Long Short-Term Memory** (**LSTM**, [https://www.bioinf.jku.at/publications/older/2604.pdf](https://www.bioinf.jku.at/publications/older/2604.pdf)).
    LSTMs can handle long-term dependencies due to a specially crafted memory cell.
    In fact, they work so well that most of the current accomplishments in training
    RNNs on a variety of problems are due to the use of LSTMs. In this section, we'll
    explore how this memory cell works and how it solves the vanishing gradients issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea of LSTM is the cell state, **c***[t]* (in addition to the hidden
    RNN state, **h***[t]*), where the information can only be explicitly written in
    or removed so that the state stays constant if there is no outside interference. The
    cell state can only be modified by specific gates, which are a way to let information
    pass through. These gates are composed of a sigmoid function and element-wise
    multiplication. Because the sigmoid only outputs values between 0 and 1, the multiplication
    can only reduce the value running through the gate. A typical LSTM is composed
    of three gates: a forget gate, an input gate, and an output gate. The cell state,
    input, and output are all vectors so that the LSTM can hold a combination of different
    information blocks at each time step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a diagram of an LSTM cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d5075a0-c9b0-4eb5-9145-f44374225915.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Top: LSTM cell; bottom: Unfolded LSTM cell: Inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we continue, let's introduce some notations. **x***[t]*, **c***[t]*,
    and **h***[t]* are the LSTM's input, cell memory state, and output (or hidden
    state) vectors in moment *t*. **c***'[t]* is the candidate cell state vector (more
    on that later). The input, **x***[t]*, and the previous cell output, **h***[t-]*, are
    connected to each gate and the candidate cell vector with sets of fully connected
    weights, **W** and **U**, respectively*. ***f***[t]*, **i***[t]*, and **o***[t]* are
    the forget, input, and output gates of the LSTM cell. These gates are fully connected
    layers with sigmoid activations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the forget gate, **f***[t]*. As the name suggests, it decides
    whether we want to erase parts of the existing cell state or not. It bases its
    decision on the weighted vector sum of the output of the previous cell, **h***[t-1]*, and
    the current input, **x***[t]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cd9c1ec-6f5b-4f43-a1e4-ca82b60dbb4a.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding diagram, we can see that the forget gate applies element-wise
    sigmoid activations on each element of the previous state vector, **c***[t-1]*: **f**[*t* ]***
    c**[*t*-1]. Again, note that because the operation is element-wise, the values
    of this vector are squashed in the [0, 1] range. An output of 0 erases a specific **c***[t-1]* cell
    block completely and an output of 1 allows the information in that cell block
    to pass through. This means that the LSTM can get rid of irrelevant information
    in its cell state vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The forget gate was not in the original LSTM that was proposed by Hochreiter.
    Instead, it was proposed in *Learning to Forget: Continual Prediction with* *LSTM*
    ([http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&rep=rep1&type=pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input gate, **i***[t]*, decides what new information is going to be added
    to the memory cell in a multi-step process. The first step determines whether
    any information is going to be added. As in the forget gate, it bases its decision
    on **h***[t-1]* and **x***[t]*: it outputs 0 or 1 through the sigmoid function
    for each cell of the candidate state vector. An output of 0 means that no information
    is added to that cell block''s memory. As a result, the LSTM can store specific
    pieces of information in its cell state vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cb87dbb-950c-4bee-9bc1-b88634ab8942.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, we compute the new candidate cell state, **c***''[t]*. It
    is based on the previous output, **h***[t-1]*, and the current input, **x***[t]*,
    and is transformed via a tanh function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d895b63-4b75-4f0a-b827-bb48c7556b0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, *c'[t]* is combined with the sigmoid outputs of the input gate via element-wise
    multiplication, [![](img/9f951b19-2361-4769-a807-039a1bab3515.png)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, the forget and input gates decide what information to forget and
    include from the previous and candidate cell states, respectively. The final version
    of the new cell state, *c[t]*, is just an element-wise sum between these two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3774867d-3a9b-4dc8-90a8-4fd0ed6a5c91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s focus on the output gate, which decides what the total cell output
    is going to be. It takes **h***[t-1]* and **x***[t]* as inputs and outputs, that
    is, 0 or 1 (via the sigmoid function), for each block of the cell''s memory. Like
    before, 0 means that the block doesn''t output any information and 1 means that
    the block can pass through as a cell''s output. Therefore, the LSTM can output
    specific blocks of information from its cell state vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e95c81e-cf77-4e91-b9c8-0a636838726d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the LSTM cell output is transferred by a tanh function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de7b058d-73fd-40e3-82be-1dc81faf8045.png)'
  prefs: []
  type: TYPE_IMG
- en: Because all of these formulas are derivable, we can chain LSTM cells together,
    just like when we chain simple RNN states together and train the network via backpropagation
    through time.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how does the LSTM protect us from vanishing gradients? Let''s start with
    the forward phase. Notice that the cell state is copied identically from step
    to step if the forget gate is 1 and the input gate is 0: [![](img/dbc2d8d2-a890-4200-a527-fe635ba668e8.png)].
    Only the forget gate can completely erase the cell''s memory. As a result, the
    memory can remain unchanged over a long period of time. Also, note that the input
    is a tanh activation that''s been added to the current cell''s memory. This means
    that the cell''s memory doesn''t blow up and is quite stable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use an example to demonstrate how a LSTM cell is unfolded. For the sake
    of simplicity, we''ll assume that it has one-dimensional (single scalar value)
    input, state, and output vectors. Because the values are scalar, we won''t use
    vector notation for the rest of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/020ba50e-a95e-4095-9cff-1cae9504ff52.png)'
  prefs: []
  type: TYPE_IMG
- en: Unrolling an LSTM through time: Inspired by http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we have a value of 3 as a candidate state. The input gate is set to *f[i]
    = 1* and the forget gate is set to *f[t] = 0*. This means that the previous state, *c[t-1 ]=
    N*, is erased and is replaced with the new state, [![](img/c33ecfc0-f1db-45ae-b1bc-e6c0bb7238bf.png)].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the next two time steps, the forget gate is set to 1, while the input gate
    is set to 0\. By doing this, all of the information is kept throughout these steps
    and no new information is added because the input gate is set to 0: [![](img/b9053fd0-79ab-4ead-afba-eff0e9fbfba0.png)].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the output gate is set to *o[t] = 1* and 3 is output and remains unchanged.
    We have successfully demonstrated how the internal state is stored across multiple
    steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, let''s focus on the backward phase. The cell state, *c[t]*, can mitigate
    the vanishing/ exploding gradients as well with the help of the forget gate, *f[t]*.
    Like the regular RNN, we can use the chain rule to compute the partial derivative, [![](img/d4892443-090e-4c1c-ab29-1bdac99af414.png)],
    for two consecutive steps. Following the formula [![](img/5571ece6-2c96-43b6-8702-a073dcdfb7c2.png)] and
    without going into details, its partial derivative is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02048859-47b3-49b0-8762-a8f98ea96f59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can generalize this to non-consecutive steps as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e03905d-1642-4d01-ac62-bf7c04e91213.png)'
  prefs: []
  type: TYPE_IMG
- en: If the forget gate values are close to 1, gradient information can pass back
    through the network states almost unchanged. This is because *f[t]* uses sigmoid
    activation and information flow is still subject to the vanishing gradient that's
    specific to sigmoid activations ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The
    Nuts and Bolts of Neural Networks*). But unlike the gradients in the regular RNN, *f[t]* has
    a different value at each time step. Therefore, this is not a geometric progression
    and the vanishing gradient effect is less pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can stack LSTM cells in the same way as we stack regular RNNs, with the
    exception that a cell state of step *t* at one level serves as an input to the
    cell state of the same level at step *t+1*. The following diagram shows an unfolded
    stacked LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84fa3898-e1b6-4bf3-b4b6-cb75e50b1b7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Stacked LSTM
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've introduced LSTM, let's solidify our knowledge by implementing
    it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll implement an LSTM cell with PyTorch 1.3.1\. First,
    let''s note that PyTorch already has an LSTM implementation, which is available
    at `torch.nn.LSTM`. However, our goal is to understand how the LSTM cell works,
    so we''ll implement our own version from scratch instead. The cell will be a subclass
    of `torch.nn.Module` and we''ll use it as a building block for larger models.
    The source code for this example is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_cell.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_cell.py).
    Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll do the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll implement the class and the `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To understand the role of the fully connected layers, `self.x_fc` and `self.h_fc`,
    let's recall that the candidate cell state and the input, forget, and output gates
    all depend on the weighted vector sum of the input, **x**[*t*], and the previous
    cell output, **h***[t-1]*. Therefore, instead of having eight separate [![](img/4ef433cf-f877-43bd-beec-91ec7b59eb90.png)] and
    [![](img/c78961c9-01c2-45ed-89a8-39a10cdc5a2d.png)] operations for each cell,
    we can combine these and make two large fully connected layers, `self.x_fc` and `self.h_fc`,
    each with an output size of `4 * hidden_size`. Once we need the output for a specific
    gate, we can extract the necessary slice from either of the two tensor outputs
    of the fully connected layers (we'll see how to do that in the implementation
    of the `forward` method).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue with the `reset_parameters` method, which initializes all of
    the weights of the network with the LSTM-specific Xavier initializer (if you copy
    and paste this code directly, you may have to check the indentation):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll start implementing the `forward` method, which contains all of
    the LSTM execution logic we described in the *Introducing long short-term memory*
    section. It takes the current mini-batch at step *t*, as well as a tuple that
    contains the cell output and cell state at step *t-1*, as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll continue by computing activations for all three gates and the candidate
    state simultaneously. It''s as simple as doing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll split the output for each gate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll apply the `activation` functions over them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll compute the new cell state, **c***[t]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll compute the cell output, `ht`, and we''ll return it along with
    the new cell state, *c[t]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the LSTM cell, we can apply it to the same task of counting the
    ones in a sequence, like we did with the regular RNN. We''ll only include the
    most relevant parts of the source code, but the full example is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py).
    This time, we''ll use a full training set of 10,000 binary sequences that have
    a length of 20 (these are arbitrary numbers). The premise of the implementation
    is similar to the RNN example: we feed the binary sequence to the LSTM in a recurrent
    manner and the cell outputs the predicted count of the ones as a single scalar
    value (regression task). However, our `LSTMCell` implementation has two limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: It only covers a single step of the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It outputs the cell state and the network output vector. This is a regression
    task and we have a single output value, but the cell state and network output
    have more dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve these problems, we'll implement a custom `LSTMModel` class, which extends
    `LSTMCell`. It feeds the `LSTMCell` instance with all of the elements of the sequence
    and handles the transition of the cell state and network output from one element
    of the sequence to the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the final output has been produced, it is fed to a fully connected layer,
    which transforms it into a single scalar value that represents the network''s
    prediction of the number of ones. The following is the implementation of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll jump straight to the train/test setup stage (recall that this is
    just a snippet of the full source code):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll generate the training and testing datasets. The `generate_dataset`
    function returns an instance of `torch.utils.data.TensorDataset`. It contains
    `TRAINING_SAMPLES = 10000` two-dimensional tensors of binary sequences with a
    length of `SEQUENCE_LENGTH = 20` and scalar value labels for the number of ones
    in each sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll instantiate the model with `HIDDEN_UNITS = 20`. The model takes a single
    input (each sequence element) and outputs a single value (number of ones):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll instantiate the `MSELoss` function (because of the regression)
    and the Adam optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run the training/testing cycle for `EPOCHS = 10`. The `train_model`
    and `test_model` functions are the same as the ones we implemented in the *Implementing
    transfer learning with PyTorch* section of [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding
    Convolutional Networks*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If we run this example, the network will achieve 100% test accuracy in 5-6 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've learned about LSTMs, let's shift our attention to gated recurrent
    units. This is another type of recurrent block that tries to replicate the properties
    of LSTM, but with a simplified structure.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing gated recurrent units
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **Gated Recurrent Unit **(**GRU**) is a type of recurrent block that was
    introduced in 2014 (*Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation*, [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078) and *Empirical
    Evaluation of Gated Recurrent Neural Networks on Sequence Modeling*, [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555))
    as an improvement over LSTM. A GRU unit usually has similar or better performance
    than an LSTM, but it does so with fewer parameters and operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a0ea6ee-84f5-4e7a-8d8f-d9a8deaf2e10.png)'
  prefs: []
  type: TYPE_IMG
- en: A GRU cell
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the *classic* RNN, a GRU cell has a single hidden state, **h***[t]*.
    You can think of it as a combination of the hidden and cell states of an LSTM.
    The GRU cell has two gates:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An update gate, **z***[t]*, which combines the input and forget LSTM gates. It
    decides what information to discard and what new information to include in its
    place, based on the network input, **x***[t]*, and the previous cell hidden state, **h***[t-1]*.
    By combining the two gates, we can ensure that the cell will forget information,
    but only when we are going to include new information in its place:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c9f1074d-3c22-42ef-bf8b-885f7c8246f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A reset gate, **r***[t]*, which uses the previous cell state, **h***[t-1]*,
    and the network input, **x***[t]*, to decide how much of the previous state to
    pass through:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/63df3872-1fcc-41dd-aeff-9dd0960de4af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we have the candidate state, **h***''[t]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20e8ab88-8f11-4a81-86b9-a9a7a57952ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the GRU output, **h***[t]*, at time *t* is an element-wise sum between the
    previous output, **h***[t−1]*, and the candidate output, **h***''[t]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4f6da4f-4787-48f8-891f-0194aad6be2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the update gate allows us to both forget and store data, it is directly
    applied over the previous output, **h[t]***[−1]*, and applied over the candidate
    output, **h***'[t]*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing GRUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll implement a GRU cell with PyTorch 1.3.1 by following
    the blueprint from the *Implementing LSTM* section. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll do the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll write the class definition and the `init` method. In LSTM, we
    were able to create a shared fully connected layer for all gates, because each
    gate required the same input combination of **x**[*t*] and **h**[*t-1*]. The GRU
    gates use different inputs, so we''ll create separate fully connected operations
    for each GRU gate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We'll omit the definition of `reset_parameters` because it's the same as it
    is in `LSTMCell`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we''ll implement the `forward` method with the cell by following the
    steps we described in the *Gated recurrent units* section. The method takes the
    current input vector, **x***[t]*, and the previous cell state/output, **h***[t-1]*,
    as input. First, we''ll compute the forget and update gates, similar to how we
    computed the gates in the LSTM cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll compute the new candidate state/output, which uses the reset gate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll compute the new output based on the candidate state and the
    update gate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We can implement the counting of ones task with a GRU cell in the same way that
    we did with LSTM. To avoid repetition, we won't include the implementation here,
    but it is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/lstm_gru_count_1s.py).
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion about various types of RNNs. Next, we'll channel
    this knowledge by implementing a text sentiment analysis example.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's recap on this chapter so far. We started by implementing an RNN using
    only `numpy`. Then, we continued with an LSTM implementation using primitive PyTorch
    operations. We'll conclude this arc by training the default PyTorch 1.3.1 LSTM
    implementation for a text classification problem. This example also requires the
    `torchtext` 0.4.0 package. Text classification (or categorization) refers to the
    task of assigning categories (or labels) depending on its contents. Text classification
    tasks include spam detection, topic labeling, and sentiment analysis. This type
    of problem is an example of a *many-to-one* relationship, which we defined in
    the *Introduction to RNNs* section.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll implement a sentiment analysis example over the Large
    Movie Review Dataset ([http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)),
    which consists of 25,000 training and 25,000 testing reviews of popular movies.
    Each review has a binary label that indicates whether it is positive or negative.
    Besides PyTorch, we'll use the `torchtext` package ([https://torchtext.readthedocs.io/](https://torchtext.readthedocs.io/)).
    It consists of data processing utilities and popular datasets for natural language.
    You'll also need to install the `spacy` open source software library ([https://spacy.io](https://spacy.io))
    for advanced NLP, which we'll use to tokenize the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentiment analysis algorithm is displayed in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a5f7310-9b5a-4cce-b31d-90cdd382136c.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentiment analysis with word embeddings and LSTM
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s describe the algorithm steps (these are valid for any text classification
    algorithm):'
  prefs: []
  type: TYPE_NORMAL
- en: Each word of the sequence is replaced with its embedding vector ([Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml),
    *Language Modeling*). These embeddings can be produced with word2vec, fastText,
    GloVe, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The word embedding is fed as input to the LSTM cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cell output, **h***[t]*, serves as input to a fully connected layer with
    a single output unit. The unit uses sigmoid activation, which represents the probability
    of the review to be positive (1) or negative (0). If the problem is multinomial
    (and not binary), we can replace the sigmoid with softmax.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The network output for the final element of the sequence is taken as a result
    for the whole sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have provided an overview of the algorithm, let's implement it.
    We'll only include the interesting portions of the code, but the full implementation
    is available at [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py)[. ](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Python/tree/master/Chapter07/sentiment_analysis.py)
  prefs: []
  type: TYPE_NORMAL
- en: This example is partially based on [https://github.com/bentrevett/pytorch-sentiment-analysis](https://github.com/bentrevett/pytorch-sentiment-analysis).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll add the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll instantiate a `torchtext.data.Field` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This object declares a text processing pipeline, which starts with the raw text
    and outputs a tensor representation of the text. More specifically, it uses the
    `spacy` tokenizer, converts all of the letters into lowercase, and includes the
    length (in words) of each movie review.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we''ll do the same for the labels (positive or negative):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll instantiate the training and testing dataset splits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The movie review dataset is included in `torchtext` and we don't need to do
    any additional work. The `splits` method takes the `TEXT` and `LABEL` fields as
    parameters. By doing this, the specified pipelines are applied over the selected
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we''ll instantiate the vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The vocabulary presents a mechanism for the numerical representation of the
    words. In this case, the numerical representation of the `TEXT` field is a pretrained
    100d GloVe vector. On the other hand, the labels in the dataset have a string
    value of either `pos` or `neg`. The role of the vocabulary here is to assign numbers
    (0 and 1) to these two labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll define iterators for the training and testing datasets, where
    `device` represents either GPU or CPU. The iterators will return one mini-batch
    at each call:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll proceed by implementing and instantiating the `LSTMModel` class. This
    is at the core of the program, which implements the algorithm steps we defined
    in the diagram at the beginning of this section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`LSTMModel` processes a mini-batch of sequences (in this case, movie reviews)
    with varying lengths. However, the mini-batch is a tensor, which assigns slices
    with equal length for each sequence. Because of this, all of the sequences are
    padded in advance with a special symbol to reach the length of the longest sequence
    in the batch. The `padding_idx` parameter in the constructor of `torch.nn.Embedding`
    represents the index of the padding symbol in the vocabulary. But using sequences
    with padding will lead to unnecessary calculations for the padded portions. Because
    of this, the forward propagation of the model takes both the `text` mini-batch
    and `text_lengths` of each sequence as parameters. They are fed to the `pack_padded_sequence`
    function, which transforms them into a `packed_sequence` object. We do all of
    this because the `self.rnn` object (the instance of `torch.nn.LSTM`) has a special
    routine for processing packed sequences, which optimizes the computation with
    respect to the padding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll copy the GloVe word embedding vectors to the embedding layer of the
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll set the embedding entries for the padding and unknown tokens to
    zeros so that they don''t influence the propagation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run the whole thing with the following code (the `train_model`
    and `test_model` functions are the same as they were previously):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: If everything works as intended, the model will achieve a test accuracy of around
    88%.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed RNNs. First, we started with the RNN and backpropagation
    through time theory. Then, we implemented an RNN from scratch to solidify our
    knowledge on the subject. Next, we moved on to more complex LSTM and GRU cells
    using the same pattern: a theoretical explanation, followed by a practical PyTorch
    implementation. Finally, we combined our knowledge from [Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml),
    *Language Modeling*, with the new material from this chapter for a full-featured
    sentiment analysis task implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss seq2seq models and their variations—an exciting
    new development in sequence processing.
  prefs: []
  type: TYPE_NORMAL
