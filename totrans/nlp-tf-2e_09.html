<html><head></head><body>
  <div id="_idContainer514" class="Basic-Text-Frame">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-216" class="chapterTitle">Sequence-to-Sequence Learning – Neural Machine Translation</h1>
    <p class="normal">Sequence-to-sequence learning is the term used for tasks that require mapping an arbitrary-length sequence to another arbitrary-length sequence. This is one of the most sophisticated tasks in NLP, which involves learning many-to-many mappings. Examples of this task include <strong class="keyWord">Neural Machine Translation</strong> (<strong class="keyWord">NMT</strong>) and creating chatbots. NMT is where we translate a sentence from one language (source language) to another (target language). Google Translate is an example of an NMT system. Chatbots (that is, software that can communicate with/answer a person) are able to converse with humans in a realistic manner. This is especially useful for various service providers, as chatbots can be used to find answers to easily solvable questions that customers might have, instead of redirecting them to human operators.</p>
    <p class="normal">In this chapter, we will learn how to implement an NMT system. However, before diving directly into such recent advances, we will first briefly visit some <strong class="keyWord">Statistical Machine Translation</strong> (<strong class="keyWord">SMT</strong>) methods, which preceded NMT and were the state-of-the-art systems until NMT caught up. Next, we will walk through the steps required for building an NMT. Finally, we will learn how to implement a real NMT system that translates from German to English, step by step.</p>
    <p class="normal">Specifically, this chapter will cover the following main topics:</p>
    <ul>
      <li class="bulletList">Machine translation</li>
      <li class="bulletList">A brief historical tour of machine translation</li>
      <li class="bulletList">Understanding neural machine translation </li>
      <li class="bulletList">Preparing data for the NMT system </li>
      <li class="bulletList">Defining the model </li>
      <li class="bulletList">Training the NMT </li>
      <li class="bulletList">The BLEU score – evaluating the machine translation systems </li>
      <li class="bulletList">Visualizing attention patterns </li>
      <li class="bulletList">Inference with NMT </li>
      <li class="bulletList">Other applications of Seq2Seq models – chatbots</li>
    </ul>
    <h1 id="_idParaDest-217" class="heading-1">Machine translation</h1>
    <p class="normal">Humans often communicate<a id="_idIndexMarker834"/> with each other by means of a language, compared to other communication methods (for example, gesturing). Currently, more than 6,000 languages are spoken worldwide. Furthermore, learning a language to a level where it is easily understandable to a native speaker of that language is a difficult task to master. However, communication is essential for sharing knowledge, socializing, and expanding your network. Therefore, language acts as a barrier to communicating with people in different parts of the world. This is where <strong class="keyWord">Machine Translation</strong> (<strong class="keyWord">MT</strong>) comes in. MT systems allow the user to input<a id="_idIndexMarker835"/> a sentence in their own tongue (known as the source language) and output a sentence in a desired target language.</p>
    <p class="normal">The problem with MT can be formulated as follows. Say we are given a sentence (or a sequence of words) <em class="italic">W</em><sub class="subscript">s</sub> belonging to a source language <em class="italic">S</em>, defined by the following: </p>
    <p class="center"><img src="../Images/B14070_09_001.png" alt="" style="height: 1.15em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_09_002.png" alt="" style="height: 1.15em !important; vertical-align: -0.31em !important;"/>.</p>
    <p class="normal">The source language would be translated to a sentence <img src="../Images/B14070_09_003.png" alt="" style="height: 1.15em !important; vertical-align: -0.18em !important;"/>, where <em class="italic">T</em> is the target language and is given by the following: </p>
    <p class="center"><img src="../Images/B14070_09_004.png" alt="" style="height: 1.15em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_09_005.png" alt="" style="height: 1.15em !important; vertical-align: -0.31em !important;"/>.</p>
    <p class="normal"><img src="../Images/B14070_09_006.png" alt="" style="height: 1.15em !important; vertical-align: -0.25em !important;"/> is obtained through the MT system, which outputs the following: </p>
    <p class="center"><img src="../Images/B14070_09_007.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_09_008.png" alt="" style="height: 1.15em !important; vertical-align: -0.16em !important;"/> is the pool of possible translation candidates found by the algorithm for the source sentence. Also, the best candidate from the pool of candidates is given by the following equation: </p>
    <p class="center"><img src="../Images/B14070_09_009.png" alt="" style="height: 1.45em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_09_073.png" alt="" style="height: 1.05em !important; vertical-align: -0.19em !important;"/> is the model parameters. During training, we optimize the model to maximize the probability of some known target translations for a set of corresponding source translations (that is, training data).</p>
    <p class="normal">So far, we have discussed the formal setup<a id="_idIndexMarker836"/> of the language translation problem that we’re interested in solving. Next, we will walk through the history of MT to get a feel of how people tried solving this in the early days.</p>
    <h1 id="_idParaDest-218" class="heading-1">A brief historical tour of machine translation</h1>
    <p class="normal">Here, we will discuss the history<a id="_idIndexMarker837"/> of MT. The inception of MT involved rule-based systems. Then, more statistically sound MT systems emerged. <strong class="keyWord">Statistical Machine Translation</strong> (<strong class="keyWord">SMT</strong>) used various measures of statistics <a id="_idIndexMarker838"/>of a language to produce translations to another language. Then came the era of NMT. NMT currently holds state-of-the-art performance in most machine learning tasks compared with other methods.</p>
    <h2 id="_idParaDest-219" class="heading-2">Rule-based translation</h2>
    <p class="normal">NMT came long after statistical<a id="_idIndexMarker839"/> machine learning, and statistical machine learning has been around for more than half a century now. The inception of SMT methods dates back to 1950-60, when during one of the first recorded projects, the <em class="italic">Georgetown-IBM experiment</em>, more than 60 Russian sentences were translated to English. o give some perspective, this attempt is almost as old as the invention of the transistor.</p>
    <p class="normal">One of the initial techniques for MT was word-based machine translation. This system performed word-to-word translations using bilingual dictionaries. However, as you can imagine, this method has serious limitations. The obvious limitation is that word-to-word translation is not a one-to-one mapping between different languages. In addition, word-to-word translation may lead to incorrect results as it does not consider the context of a given word. The translation of a given word in the source language can change depending<a id="_idIndexMarker840"/> on the context in which it is used. To understand this with a concrete example, let’s look at the translation example from English to French in <em class="italic">Figure 9.1</em>. You can see that in the given two English sentences, a single word changes. However, this creates drastic changes in the translation:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_01.png" alt="Rule-based translation"/></figure>
    <p class="packt_figref">Figure 9.1: Translations (English to French) between languages are not one-to-one mappings between words</p>
    <p class="normal">In the 1960s, the <strong class="keyWord">Automatic Language Processing Advisory Committee</strong> (<strong class="keyWord">ALPAC</strong>) released a report, <em class="italic">Languages and machines: computers in translation and linguistics,</em> <em class="italic">National Academy of the Sciences (1966)</em>, on MT’s prospects. The conclusion was this:</p>
    <p class="center"><em class="italic">There is no immediate or predictable prospect of useful machine translation.</em></p>
    <p class="normal">This was because<a id="_idIndexMarker841"/> MT was slower, less accurate, and more expensive than human translation at the time. This delivered a huge blow to MT advancements, and almost a decade passed in silence.</p>
    <p class="normal">Next came corpora-based MT, where an algorithm was trained using tuples of source sentences, and the corresponding target sentence was obtained through a parallel corpus, that is, the parallel corpus would be of the format <code class="inlineCode">[(&lt;source_sentence_1&gt;, &lt;target_sentence_1&gt;), (&lt;source_sentence_2&gt;, &lt;target_sentence_2&gt;), …]</code>. The parallel corpus is a large text corpus formed as tuples, consisting of text from the source language and the corresponding translation of that text. An illustration of this is shown in <em class="italic">Table 9.1</em>. It should be noted that building a parallel corpus is much easier than building bilingual dictionaries and they are more accurate because the training data is richer than word-to-word training data. Furthermore, instead of directly relying on manually created bilingual dictionaries, a bilingual dictionary (that is, the transition models) of two languages can be built using the parallel corpus. A transition model shows how likely a target word/phrase is to be the correct translation, given the current source word/phrase. In addition to learning the transition model, corpora-based MT also learns the word alignment models. A word alignment model can represent how words in a phrase from the source language correspond to the translation of that phrase. An example of parallel corpora and a word alignment model is depicted in <em class="italic">Figure 9.2</em>: </p>
    <figure class="mediaobject"><img src="../Images/B14070_09_02.png" alt="Rule-based translation"/></figure>
    <p class="packt_figref">Figure 9.2: Word alignment between two different languages</p>
    <p class="normal">An illustration of an example parallel corpora<a id="_idIndexMarker842"/> is shown in <em class="italic">Table 9.1</em>:</p>
    <table id="table001-4" class="table-container">
      <thead>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Source language sentences (English)</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Target language sentences (French)</strong></p>
          </td>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal">I went home</p>
          </td>
          <td class="table-cell">
            <p class="normal">Je suis allé à la maison</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">John likes to play guitar</p>
          </td>
          <td class="table-cell">
            <p class="normal">John aime jouer de la guitare</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">He is from England</p>
          </td>
          <td class="table-cell">
            <p class="normal">Il est d’Angleterre</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">…</p>
          </td>
          <td class="table-cell">
            <p class="normal">….</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 9.1: Parallel corpora for English and French sentences</p>
    <p class="normal">Another approach was interlingual machine translation, which involved translating the source sentence into a language-neutral <strong class="keyWord">interlingua</strong> (that is, a metalanguage), and then generating the translated sentence out of the interlingua. More specifically, an interlingual machine translation system consists of two important components, an analyzer and a synthesizer. The analyzer will take the source sentence and identify agents (for example, nouns), actions (for example, verbs), and so on, and also how they interact with each other. Next, these identified elements are represented by means of an interlingual lexicon. An example of an interlingual lexicon can be made with the synsets (that is, the group of synonyms sharing a common meaning) available in WordNet. Then, from this interlingual representation, the synthesizer will create the translation. Since the synthesizer knows the nouns, verbs, and so on through the interlingual representation, it can generate the translation<a id="_idIndexMarker843"/> in the target language by incorporating language-specific grammar rules.</p>
    <h2 id="_idParaDest-220" class="heading-2">Statistical Machine Translation (SMT)</h2>
    <p class="normal">Next, more statistically<a id="_idIndexMarker844"/> sound systems started emerging. One of the pioneering models of this era was IBM Models 1-5, which did word-based translation. However, as we discussed earlier, word translations do not match one to one from the source language to a target language (for example, compound words and morphology). Eventually, researchers started experimenting with phrase-based translation systems, which made some notable advances in machine translation.</p>
    <p class="normal">Phrase-based translation works in a similar<a id="_idIndexMarker845"/> way to word-based translation, except that it uses phrases of a language as the atomic units of translation instead of individual words. This is a more sensible approach as it makes modeling the one-to-many, many-to-one, or many-to-many relationships between words easier. The main goal of phrase-based translation is to learn a phrase-translation model that contains a probability distribution of different candidate target phrases for a given source phrase. As you can imagine, this method involves maintaining huge databases of various phrases in two languages. A reordering step for phrases is also performed as there is no monotonic ordering of words between a sentence from one language and one in another. </p>
    <p class="normal">An example of this is shown in <em class="italic">Figure 9.2</em>; if the words were monotonically ordered between languages, there would not be crosses between word mappings.</p>
    <p class="normal">One of the limitations of this approach is that the decoding process (finding the best target phrase for a given source phrase) is expensive. This is due to the size of the phrase database, as well as the fact that a source phrase often contains multiple target language phrases. To alleviate the burden, syntax-based translations arose.</p>
    <p class="normal">In syntax-based translation, the source<a id="_idIndexMarker846"/> sentence is<a id="_idIndexMarker847"/> represented<a id="_idIndexMarker848"/> by a syntax tree. In <em class="italic">Figure 9.3</em>, <strong class="keyWord">NP</strong> represents a noun phrase, <strong class="keyWord">VP</strong> a verb phrase, and <strong class="keyWord">S</strong> a sentence. Then a <strong class="keyWord">reordering phase</strong> takes place, where the tree nodes<a id="_idIndexMarker849"/> are reordered to change<a id="_idIndexMarker850"/> the order of subject, verb, and object, depending on the target language. This is because the sentence structure can change depending on the language (for example, in English it is <em class="italic">subject-verb-object</em>, whereas in Japanese it is <em class="italic">subject-object-verb</em>). The reordering is decided <a id="_idIndexMarker851"/>according to something known as the <strong class="keyWord">r-table</strong>. The r-table contains the likelihood probabilities for the tree nodes to be changed to some other order:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_03.png" alt="Statistical Machine Translation (SMT)"/></figure>
    <p class="packt_figref">Figure 9.3: Syntax tree for a sentence</p>
    <p class="normal">An <strong class="keyWord">insertion phase</strong> then takes place. In the insertion<a id="_idIndexMarker852"/> phase, we stochastically insert a word into each node of the tree. This is due to the assumption that there is an invisible <code class="inlineCode">NULL</code> word, and it generates target words at random positions of the tree. Also, the probability of inserting<a id="_idIndexMarker853"/> a word is determined by something called the <strong class="keyWord">n-table</strong>, which is a table that contains probabilities of inserting a particular word into the tree.</p>
    <p class="normal">Next, the <strong class="keyWord">translation phase</strong> occurs, where each leaf <a id="_idIndexMarker854"/>node is translated to the target word in a word<a id="_idIndexMarker855"/>-by-word manner. Finally, the translated sentence is read off the syntax tree, to construct the target sentence.</p>
    <h2 id="_idParaDest-221" class="heading-2">Neural Machine Translation (NMT)</h2>
    <p class="normal">Finally, around the year 2014, NMT<a id="_idIndexMarker856"/> systems were introduced. NMT is an end-to-end system that takes a full sentence as an input, performs certain transformations, and then outputs the translated sentence for the corresponding source sentence. </p>
    <p class="normal">Therefore, NMT eliminates the need for the feature engineering required for machine translation, such as building phrase translation models and building syntax trees, which is a big win for the NLP community. Also, NMT has outperformed all the other popular MT techniques in a very short period, just two to three years. In <em class="italic">Figure 9.4</em>, we depict the results of various MT systems reported in the MT literature. For example, 2016 results are obtained from Sennrich and others in their paper <em class="italic">Edinburgh Neural Machine Translation Systems for WMT 16, Association for Computational Linguistics, Proceedings of the First Conference on Machine Translation, August 2016: 371-376</em>, and from Williams and others in their paper <em class="italic">Edinburgh’s Statistical Machine Translation Systems for WMT16, Association for Computational Linguistics, Proceedings of the First Conference on Machine Translation, August 2016: 399-410</em>. All the MT systems<a id="_idIndexMarker857"/> are evaluated with the BLEU score. The BLEU score denotes the number of n-grams (for example, unigrams and bigrams) of candidate translation that matched in the reference translation. So the higher the BLEU score, the better the MT system. We’ll discuss the BLEU metric in detail later in the chapter. There is no need to highlight that NMT is a clear-cut winner:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_04.png" alt="Neural Machine Translation (NMT)"/></figure>
    <p class="packt_figref">Figure 9.4: Comparison of statistical machine translation system to NMT systems. Courtesy of Rico Sennrich</p>
    <p class="normal">A case study assessing the potential of NMT systems is available in <em class="italic">Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions, Junczys-Dowmunt, Hoang and Dwojak, Proceedings of the Ninth International Workshop on Spoken Language Translation</em>, <em class="italic">Seattle (2016)</em>. </p>
    <p class="normal">The study looks at the performance of different systems on several translation tasks between various languages (English, Arabic, French, Russian, and Chinese). The results also support that NMT systems (NMT 1.2M and NMT 2.4M) perform better than SMT systems (PB-SMT and Hiero).</p>
    <p class="normal"><em class="italic">Figure 9.5</em> shows several statistics<a id="_idIndexMarker858"/> for a set from a 2017 state-of-the-art machine translator. This is from a presentation, <em class="italic">State of the Machine Translation, Intento, Inc, 2017</em>, produced by Konstantin Savenkov, cofounder and CEO of Intento. We can see that the performance of the MT produced by DeepL (<a href="https://www.deepl.com"><span class="url">https://www.deepl.com</span></a>) appears to be competing<a id="_idIndexMarker859"/> closely with other MT giants, including Google. The comparison includes MT systems such as DeepL (NMT), Google (NMT), Yandex (NMT-SMT hybrid), Microsoft (has both SMT and NMT), IBM (SMT), Prompt (rule-based), and SYSTRAN (rule-based/SMT hybrid). The graph clearly shows that NMT systems are leading the current MT advancements. The LEPOR score is used to assess different systems. LEPOR is a more advanced metric than BLEU, and it attempts to solve the <strong class="keyWord">language bias problem</strong>. The language bias problem refers to the phenomenon that some evaluation metrics (such as BLEU) perform well for certain languages, but perform poorly for others.</p>
    <p class="normal">However, it should also be noted that the results do contain some bias due to the averaging mechanism used in this comparison. For example, Google Translate has been averaged over a larger set of languages (including difficult translation tasks), whereas DeepL has been averaged over a smaller and relatively easier subset of languages. Therefore, we should not conclude that the DeepL MT system is always better than the Google MT system. Nevertheless, the overall results provide a general comparison of the performance of the current NMT and SMT systems:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_05.png" alt="Neural Machine Translation (NMT)"/></figure>
    <p class="packt_figref">Figure 9.5: Performance of various MT systems. Courtesy of Intento, Inc.</p>
    <p class="normal">We saw that NMT has already<a id="_idIndexMarker860"/> outperformed SMT systems in very few years, and is the current state of the art. We will now move on to discussing the details and architecture of an NMT system. Finally, we will be implementing an NMT system from scratch.</p>
    <h1 id="_idParaDest-222" class="heading-1">Understanding neural machine translation</h1>
    <p class="normal">Now that we have an appreciation for how machine translation has evolved over time, let’s try to understand how state-of-the-art NMT works. First, we will take a look at the model architecture used by neural machine translators and then move on to understanding the actual training algorithm.</p>
    <h2 id="_idParaDest-223" class="heading-2">Intuition behind NMT systems</h2>
    <p class="normal">First, let’s understand the intuition<a id="_idIndexMarker861"/> underlying an NMT system’s design. Say you are a fluent English and German speaker and were asked to translate the following sentence into German:</p>
    <p class="center"><em class="italic">I went home</em></p>
    <p class="normal">This sentence translates to the following:</p>
    <p class="center"><em class="italic">Ich ging nach Hause</em></p>
    <p class="normal">Although it might not have taken more than a few seconds for a fluent person to translate this, there is a certain process that produces the translation. First, you read the English sentence, and then you create a thought or concept about what this sentence represents or implies, in your mind. And finally, you translate the sentence into German. The same idea is used for building NMT systems (see <em class="italic">Figure 9.6</em>). The encoder reads the source sentence (that is, similar to you reading the English sentence). Then the encoder outputs a context vector (the context vector corresponds to the thought/concept you imagined after reading the sentence). Finally, the decoder takes in the context vectors and outputs the translation in German:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_06.png" alt="Intuition behind NMT"/></figure>
    <p class="packt_figref">Figure 9.6: Conceptual architecture of an NMT system</p>
    <h2 id="_idParaDest-224" class="heading-2">NMT architecture</h2>
    <p class="normal">Now we will look at the architecture<a id="_idIndexMarker862"/> in more detail. The sequence-to-sequence approach was originally proposed by Sutskever, Vinyals, and Le in their paper <em class="italic">Sequence to Sequence Learning with Neural Networks, Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2: 3104-3112.</em> </p>
    <p class="normal">From the diagram in <em class="italic">Figure 9.6</em>, we can see that there are two major components in the NMT architecture. These are called the encoder and decoder. In other words, NMT can be seen as an encoder-decoder<a id="_idIndexMarker863"/> architecture. The <strong class="keyWord">encoder</strong> converts a sentence from a given source language into a thought vector (i.e. a contextualized representation), and the <strong class="keyWord">decoder</strong> decodes or translates<a id="_idIndexMarker864"/> the thought into a target language. As you can see, this shares some features<a id="_idIndexMarker865"/> with the interlingual machine translation method we briefly talked about. This explanation is illustrated in <em class="italic">Figure 9.7</em>. The left-hand side of the context vector denotes the encoder (which takes a source sentence in word by word to train a time-series model). The right-hand side denotes the decoder, which outputs word by word (while using the previous word as the current input) the corresponding translation of the source sentence. We will also use embedding layers (for both the source and target languages) where the semantics of the individual tokens will be learned and fed as inputs to the models:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_07.png" alt="NMT architecture"/></figure>
    <p class="packt_figref">Figure 9.7: Unrolling the source and target sentences over time</p>
    <p class="normal">With a basic understanding of what NMT looks like, let’s formally define the objective of the NMT. The ultimate objective of an NMT system is to maximize the log likelihood, given a source sentence <em class="italic">x</em><sub class="subscript">s</sub> and its corresponding <em class="italic">y</em><sub class="subscript">t</sub>. That is, to maximize the following: </p>
    <p class="center"><img src="../Images/B14070_09_010.png" alt="" style="height: 3.33em !important;"/></p>
    <p class="normal">Here, <em class="italic">N</em> refers to the number of source and target sentence inputs we have as training data.</p>
    <p class="normal">Then, during inference, for a given source sentence, <img src="../Images/B14070_09_011.png" alt="" style="height: 1.25em !important; vertical-align: -0.29em !important;"/>, we will find the <img src="../Images/B14070_09_012.png" alt="" style="height: 1.25em !important; vertical-align: -0.31em !important;"/> translation using the following: </p>
    <p class="center"><img src="../Images/B14070_09_013.png" alt="" style="height: 3.33em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_09_014.png" alt="" style="height: 1.25em !important; vertical-align: -0.35em !important;"/> is the predicted token at the <em class="italic">i</em><sup class="superscript">th</sup> time step and <img src="../Images/B14070_09_015.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> is the set of possible candidate sentences.</p>
    <p class="normal">Before we examine each part<a id="_idIndexMarker866"/> of the NMT architecture, let’s define the mathematical notation to understand<a id="_idIndexMarker867"/> the system more concretely. As our sequential model, we will choose a <strong class="keyWord">Gated Recurrent Unit </strong>(<strong class="keyWord">GRU</strong>), as it is simpler than an LSTM and performs comparatively well.</p>
    <p class="normal">Let’s define the encoder GRU as <img src="../Images/B14070_09_016.png" alt="" style="height: 1.15em !important; vertical-align: -0.24em !important;"/> and the decoder GRU as <img src="../Images/B14070_09_017.png" alt="" style="height: 1.15em !important; vertical-align: -0.17em !important;"/>. At the time step <img src="../Images/B14070_09_018.png" alt="" style="height: 1.05em !important; vertical-align: -0.16em !important;"/>, let’s define the output state of a general GRU as <em class="italic">h</em><sub class="subscript">t</sub>. That is, feeding the input <em class="italic">x</em><sub class="subscript">t</sub> into the GRU produces <em class="italic">h</em><sub class="subscript">t</sub>:</p>
    <p class="center"><img src="../Images/B14070_09_020.png" alt="" style="height: 1.15em !important;"/></p>
    <p class="normal">Now, we will talk about the embedding layer, the encoder, the context vector, and finally, the decoder.</p>
    <h3 id="_idParaDest-225" class="heading-3">The embedding layer</h3>
    <p class="normal">We have already seen the power<a id="_idIndexMarker868"/> of word embeddings. Here, we<a id="_idIndexMarker869"/> can also leverage embeddings to improve model performance. We will be using two-word embedding layers, <img src="../Images/B14070_09_021.png" alt="" style="height: 1.15em !important; vertical-align: -0.24em !important;"/>, for the source language and <img src="../Images/B14070_09_022.png" alt="" style="height: 1.15em !important; vertical-align: -0.15em !important;"/> for the target language. So, instead of feeding <em class="italic">x</em><sub class="subscript">t</sub> directly into the GRU, we will be getting <img src="../Images/B14070_09_023.png" alt="" style="height: 1.15em !important; vertical-align: -0.30em !important;"/>. However, to avoid excessive notation, we will assume <img src="../Images/B14070_09_024.png" alt="" style="height: 1.15em !important; vertical-align: -0.15em !important;"/>.</p>
    <h3 id="_idParaDest-226" class="heading-3">The encoder</h3>
    <p class="normal">As mentioned earlier, the encoder<a id="_idIndexMarker870"/> is responsible for generating a thought<a id="_idIndexMarker871"/> vector or a context vector that represents what is meant by the source language. For this, we will use a GRU-based network (see <em class="italic">Figure 9.8</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.8: A GRU cell</p>
    <p class="normal">The encoder is initialized<a id="_idIndexMarker872"/> with <em class="italic">h</em> at time step 0 (<em class="italic">h</em><sub class="subscript">0</sub>) with a zero vector by default. The encoder<a id="_idIndexMarker873"/> takes a sequence of words, <img src="../Images/B14070_09_025.png" alt="" style="height: 1.15em !important; vertical-align: -0.28em !important;"/>, as the input and calculates a context vector, <img src="../Images/B14070_09_026.png" alt="" style="height: 1.15em !important; vertical-align: -0.24em !important;"/>, where <em class="italic">v</em> is the final external hidden state obtained after processing the final element <img src="../Images/B14070_09_027.png" alt="" style="height: 1.15em !important; vertical-align: -0.07em !important;"/>, of the sequence <em class="italic">x</em><sub class="subscript">s</sub>. We represent this as the following:</p>
    <p class="center"><img src="../Images/B14070_09_028.png" alt="" style="height: 1.15em !important;"/></p>
    <p class="center"><img src="../Images/B14070_09_029.png" alt="" style="height: 1.15em !important;"/></p>
    <h3 id="_idParaDest-227" class="heading-3">The context vector</h3>
    <p class="normal">The idea of the context vector (<em class="italic">v</em>) is to represent<a id="_idIndexMarker874"/> a sentence of a source language concisely. Also, in contrast<a id="_idIndexMarker875"/> to how the encoder’s state is initialized (that is, it is initialized with zeros), the context vector becomes the initial state for the decoder GRU. In other words, the decoder GRU doesn’t start with an initial state of zeros, but with the context vector as its initial state. This creates a linkage between the encoder and the decoder and makes the whole model end-to-end differentiable. We will talk about this in more detail next.</p>
    <h3 id="_idParaDest-228" class="heading-3">The decoder</h3>
    <p class="normal">The decoder is responsible for decoding<a id="_idIndexMarker876"/> the context vector into the desired translation. Our<a id="_idIndexMarker877"/> decoder is an RNN as well. Though it is possible for the encoder and decoder to share the same set of weights, it is usually better to use two different networks for the encoder and the decoder. This increases the number of parameters in our model, allowing us to learn the translations more effectively.</p>
    <p class="normal">First, the decoder’s states are initialized with the context vector, i.e. <img src="../Images/B14070_09_030.png" alt="" style="height: 1.15em !important; vertical-align: -0.24em !important;"/>, as shown here: <img src="../Images/B14070_09_031.png" alt="" style="height: 1.15em !important; vertical-align: -0.33em !important;"/>.</p>
    <p class="normal">Here, <img src="../Images/B14070_09_032.png" alt="" style="height: 1.15em !important; vertical-align: -0.24em !important;"/> is the initial state vector of the decoder (<img src="../Images/B14070_09_033.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>).</p>
    <p class="normal">This (<em class="italic">v</em>) is the crucial link that connects the encoder with the decoder to form an end-to-end computational chain (see in <em class="italic">Figure 9.6 </em>that the only thing shared by the encoder and decoder is <em class="italic">v</em>). Also, this is the only piece of information that is available to the decoder about the source sentence.</p>
    <p class="normal">Then we will compute the <em class="italic">m</em><sup class="superscript">th</sup> prediction of the translated sentence with the following:</p>
    <p class="center"><img src="../Images/B14070_09_034.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_09_035.png" alt="" style="height: 1.45em !important;"/></p>
    <p class="normal">The full NMT system with the details of how the GRU cell in the encoder connects to the GRU cell in the decoder, and how the softmax layer is used to output predictions, is shown in <em class="italic">Figure 9.9</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.9: The encoder-decoder architecture with the GRUs. Both the encoder and the decoder have a separate GRU component. Additionally, the decoder has a fully-connected (dense) layer and a softmax layer that produce the final predictions.</p>
    <p class="normal">In the next section, we will go<a id="_idIndexMarker878"/> through the steps required to prepare data<a id="_idIndexMarker879"/> for our model.</p>
    <h1 id="_idParaDest-229" class="heading-1">Preparing data for the NMT system</h1>
    <p class="normal">In this section, we will understand<a id="_idIndexMarker880"/> the data and learn about the process for preparing data<a id="_idIndexMarker881"/> for training and predicting from the NMT system. First, we will talk about how to prepare training data (that is, the source sentence and target sentence pairs) to train the NMT system, followed by inputting a given source sentence to produce the translation of the source sentence.</p>
    <h2 id="_idParaDest-230" class="heading-2">The dataset</h2>
    <p class="normal">The dataset we’ll be using<a id="_idIndexMarker882"/> for this chapter is the WMT-14 English-German translation data from <a href="https://nlp.stanford.edu/projects/nmt/"><span class="url">https://nlp.stanford.edu/projects/nmt/</span></a>. There are ~4.5 million sentence pairs available. However, we will use only 250,000 sentence pairs due to computational feasibility. The vocabulary consists of the 50,000 most common English words and the 50,000 most common German words, and words not found in the vocabulary will be replaced with a special token, <code class="inlineCode">&lt;unk&gt;</code>. You will need to download the following files:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">train.de</code> – File containing German sentences</li>
      <li class="bulletList"><code class="inlineCode">train.en</code> – File containing English sentences</li>
      <li class="bulletList"><code class="inlineCode">vocab.50K.de</code> – File containing German vocabulary </li>
      <li class="bulletList"><code class="inlineCode">vocab.50K.en</code> – File containing English vocabulary </li>
    </ul>
    <p class="normal"><code class="inlineCode">train.de</code> and <code class="inlineCode">train.en</code> contain<a id="_idIndexMarker883"/> parallel sentences in German and English, respectively. Once downloaded we will load the sentences as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">n_sentences = <span class="hljs-number">250000</span>
<span class="hljs-comment"># Loading English sentences</span>
original_en_sentences = []
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'train.en'</span>), <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> en_file:
    <span class="hljs-keyword">for</span> i,row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(en_file):
        <span class="hljs-keyword">if</span> i &gt;= n_sentences: <span class="hljs-keyword">break</span>
        original_en_sentences.append(row.strip().split(<span class="hljs-string">" "</span>))
        
<span class="hljs-comment"># Loading German sentences</span>
original_de_sentences = []
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'train.de'</span>), <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> de_file:
    <span class="hljs-keyword">for</span> i, row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(de_file):
        <span class="hljs-keyword">if</span> i &gt;= n_sentences: <span class="hljs-keyword">break</span>
        original_de_sentences.append(row.strip().split(<span class="hljs-string">" "</span>))
</code></pre>
    <p class="normal">If you print the data you just loaded, for the two languages, you would have sentences like the following:</p>
    <pre class="programlisting con"><code class="hljs-con">English: a fire restant repair cement for fire places , ovens , open fireplaces etc . 
German: feuerfester Reparaturkitt für Feuerungsanlagen , Öfen , offene Feuerstellen etc.
English: Construction and repair of highways and ... 
German: Der Bau und die Reparatur der Autostraßen ...
English: An announcement must be commercial character . 
German: die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen .
</code></pre>
    <h3 id="_idParaDest-231" class="heading-3">Adding special tokens</h3>
    <p class="normal">The next step is to add a few special<a id="_idIndexMarker884"/> tokens to the start and end of our sentences. We will add <code class="inlineCode">&lt;s&gt;</code> to mark the start of a sentence and <code class="inlineCode">&lt;/s&gt;</code> to mark the end of a sentence. We can easily achieve this using the following list comprehension:</p>
    <pre class="programlisting code"><code class="hljs-code">en_sentences = [[<span class="hljs-string">"&lt;s&gt;"</span>]+sent+[<span class="hljs-string">"&lt;/s&gt;"</span>] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> original_en_sentences]
de_sentences = [[<span class="hljs-string">"&lt;s&gt;"</span>]+sent+[<span class="hljs-string">"&lt;/s&gt;"</span>] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> original_de_sentences]
</code></pre>
    <p class="normal">This will give us:</p>
    <pre class="programlisting con"><code class="hljs-con">English: &lt;s&gt; a fire restant repair cement for fire places , ovens , open fireplaces etc . &lt;/s&gt; 
German: &lt;s&gt; feuerfester Reparaturkitt für Feuerungsanlagen , Öfen , offene Feuerstellen etc. &lt;/s&gt;
English: &lt;s&gt; Construction and repair of highways and ... &lt;/s&gt; 
German: &lt;s&gt; Der Bau und die Reparatur der Autostraßen ... &lt;/s&gt;
English: &lt;s&gt; An announcement must be commercial character . &lt;/s&gt; 
German: &lt;s&gt; die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen . &lt;/s&gt;
</code></pre>
    <p class="normal">This is a very important step for Seq2Seq models. <code class="inlineCode">&lt;s&gt;</code> and <code class="inlineCode">&lt;/s&gt;</code> tokens serve an extremely important role during model inference. As you will see, at inference time, we will be using the decoder to predict one word at a time, by using the output of the previous time step as an input. This way we can predict for an arbitrary number of time steps. Using <code class="inlineCode">&lt;s&gt;</code> as the starting token gives us a way to signal to the decoder that it should start predicting tokens from the target language. Next, if we do not use the <code class="inlineCode">&lt;/s&gt;</code> token to mark the end of a sentence, we cannot signal the decoder to end a sentence. This can lead the model to enter an infinite loop of predictions.</p>
    <h3 id="_idParaDest-232" class="heading-3">Splitting training, validation, and testing datasets</h3>
    <p class="normal">We need to split our dataset<a id="_idIndexMarker885"/> into three parts: a training set, a validation<a id="_idIndexMarker886"/> set, and a testing set. Specifically, let’s<a id="_idIndexMarker887"/> use 80% of sentences to train the model, 10% as validation data, and the remaining 10% as testing data: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
train_en_sentences, valid_test_en_sentences, train_de_sentences, valid_test_de_sentences = train_test_split(
    np.array(en_sentences), np.array(de_sentences), test_size=<span class="hljs-number">0.2</span>
)
valid_en_sentences, valid_de_sentences, test_en_sentences, test_de_sentences = train_test_split(
    valid_test_en_sentences, valid_test_de_sentences, test_size=<span class="hljs-number">0.5</span>)
</code></pre>
    <h3 id="_idParaDest-233" class="heading-3">Defining sequence lengths for the two languages</h3>
    <p class="normal">A key statistic we have to understand<a id="_idIndexMarker888"/> at this point is how long, generally, the sentences in our corpus are. It is quite likely that the two languages will have different sentence lengths. To learn the statistics of this, we’ll be using the pandas library in the following way:</p>
    <pre class="programlisting code"><code class="hljs-code">pd.Series(train_en_sentences).<span class="hljs-built_in">str</span>.<span class="hljs-built_in">len</span>().describe(percentiles=[<span class="hljs-number">0.05</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.95</span>])
</code></pre>
    <p class="normal">Here, we are first converting the <code class="inlineCode">train_en_sentences</code> to a <code class="inlineCode">pd.Series</code> object. <code class="inlineCode">pd.Series</code> is an indexed series of values (an array). Here, each value is a list of tokens belonging to each sentence. Calling <code class="inlineCode">.str.len()</code> will give us the length of each list of tokens. Finally, the <code class="inlineCode">describe</code> method will give important statistics such as mean, standard deviation, and percentiles. Here. we are specifically asking for 5%, 50%, and 95% percentiles. </p>
    <p class="normal">Note that we are only using the training data for this calculation. If you include validation or test datasets in these calculations, we may be leaking data about validation and test data. Therefore, it’s best to only use the training dataset for these calculations.</p>
    <p class="normal">The result from the previous code gives us:</p>
    <pre class="programlisting con"><code class="hljs-con">Sequence lengths (English)
count    40000.000000
mean        25.162625
std         13.857748
min          6.000000
5%           9.000000
50%         22.000000
95%         53.000000
max        100.000000
dtype: float64
</code></pre>
    <p class="normal">We can get the same for the German<a id="_idIndexMarker889"/> sentences the following way:</p>
    <pre class="programlisting code"><code class="hljs-code">pd.Series(train_de_sentences).<span class="hljs-built_in">str</span>.<span class="hljs-built_in">len</span>().describe(percentiles=[<span class="hljs-number">0.05</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.95</span>])
</code></pre>
    <p class="normal">This gives us:</p>
    <pre class="programlisting con"><code class="hljs-con">Sequence lengths (German)
count    40000.000000
mean        22.882550
std         12.574325
min          6.000000
5%           9.000000
50%         20.000000
95%         47.000000
max        100.000000
dtype: float64
</code></pre>
    <p class="normal">Here we can see that 95% of English sentences have 53 tokens, where 95% of German sentences have 47 tokens.</p>
    <h3 id="_idParaDest-234" class="heading-3">Padding the sentences</h3>
    <p class="normal">Next, we need to pad our<a id="_idIndexMarker890"/> sentences. For this, we will use the <code class="inlineCode">pad_sequences()</code> function provided in Keras. This function takes in values for the following arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">sequences</code> – A list of strings/IDs representing the text corpus. Each document can either be a list of strings or a list of integers</li>
      <li class="bulletList"><code class="inlineCode">maxlen</code> – The maximum length to pad for (defaults to <code class="inlineCode">None</code>)</li>
      <li class="bulletList"><code class="inlineCode">dtype</code> – The type of data (defaults to <code class="inlineCode">'int32'</code>)</li>
      <li class="bulletList"><code class="inlineCode">padding</code> – The side to pad short sequences (defaults to <code class="inlineCode">'pre'</code>)</li>
      <li class="bulletList"><code class="inlineCode">truncating</code> – The side to truncate long sequences from (defaults to <code class="inlineCode">'pre'</code>)</li>
      <li class="bulletList"><code class="inlineCode">value</code> – The values to pad with (defaults to <code class="inlineCode">0.0</code>)</li>
    </ul>
    <p class="normal">We will use this function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences
train_en_sentences_padded = pad_sequences(train_en_sentences, maxlen=n_en_seq_length, value=unk_token, dtype=<span class="hljs-built_in">object</span>, truncating=<span class="hljs-string">'post'</span>, padding=<span class="hljs-string">'post'</span>)
valid_en_sentences_padded = pad_sequences(valid_en_sentences, maxlen=n_en_seq_length, value=unk_token, dtype=<span class="hljs-built_in">object</span>, truncating=<span class="hljs-string">'post'</span>, padding=<span class="hljs-string">'post'</span>)
test_en_sentences_padded = pad_sequences(test_en_sentences, maxlen=n_en_seq_length, value=unk_token, dtype=<span class="hljs-built_in">object</span>, truncating=<span class="hljs-string">'post'</span>, padding=<span class="hljs-string">'</span><span class="hljs-string">post'</span>)
train_de_sentences_padded = pad_sequences(train_de_sentences, maxlen=n_de_seq_length, value=unk_token, dtype=<span class="hljs-built_in">object</span>, truncating=<span class="hljs-string">'post'</span>, padding=<span class="hljs-string">'post'</span>)
valid_de_sentences_padded = pad_sequences(valid_de_sentences, maxlen=n_de_seq_length, value=unk_token, dtype=<span class="hljs-built_in">object</span>, truncating=<span class="hljs-string">'post'</span>, padding=<span class="hljs-string">'post'</span>)
test_de_sentences_padded = pad_sequences(test_de_sentences, maxlen=n_de_seq_length, value=unk_token, dtype=<span class="hljs-built_in">object</span>, truncating=<span class="hljs-string">'post'</span>, padding=<span class="hljs-string">'post'</span>)
</code></pre>
    <p class="normal">We are padding all<a id="_idIndexMarker891"/> of the training, validation, and test sentences in both English and German. We will use the recently found sequence lengths as the padding/truncating length.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reversing the source sentence</strong></p>
      <p class="normal">We can also perform<a id="_idIndexMarker892"/> a special trick on the source sentences. Say we have the sentence <em class="italic">ABC</em> in the source language, which we want to translate to <img src="../Images/B14070_09_036.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> in the target language. We will first reverse the source sentences so that the sentence <em class="italic">ABC</em> is read as <em class="italic">CBA</em>. This means that in order to translate <em class="italic">ABC</em> to <img src="../Images/B14070_09_036.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/>, we need to feed in <em class="italic">CBA</em>. This improves the performance of our model significantly, especially when the source and target languages share the same sentence structure (for example, <em class="italic">subject-verb-object</em>).</p>
      <p class="normal">Let’s try to understand why this helps. Mainly, it helps to build good <em class="italic">communication</em> between the encoder and the decoder. Let’s start from the previous example. We will concatenate the source and target sentences: </p>
      <p class="center"><img src="../Images/B14070_09_038.png" alt="" style="height: 1.15em !important;"/></p>
      <p class="normal">If you calculate the distance (that is, the number of words separating two words) from <em class="italic">A</em> to <img src="../Images/B14070_09_039.png" alt="" style="height: 0.95em !important;"/> or <em class="italic">B</em> to <img src="../Images/B14070_09_040.png" alt="" style="height: 1.15em !important;"/>, they will be the same. However, consider this when you reverse the source sentence, as shown here: </p>
      <p class="center"><img src="../Images/B14070_09_041.png" alt="" style="height: 1.15em !important;"/></p>
      <p class="normal">Here, <em class="italic">A</em> is very close to <img src="../Images/B14070_09_039.png" alt="" style="height: 0.95em !important;"/> and so on. Also, to build good translations, building good communications at the very start is important. This simple trick can possibly help NMT systems to improve their performance.</p>
      <p class="normal">Note that the source sentence reversing<a id="_idIndexMarker893"/> step is a subjective preprocessing step. This might not be necessary for some translational tasks. For example, if your translation task is to translate from Japanese (which is often written in <em class="italic">subject-object-verb</em> format) to Filipino (often written <em class="italic">verb-subject-object</em>), then reversing the source sentence might actually cause harm rather than helping. This is because by reversing the text in Japanese, you are increasing the distance between the starting element of the target sentence (that is, the verb (Japanese)) and the corresponding source language entity (that is, the verb (Filipino)).</p>
    </div>
    <p class="normal">Next let’s define<a id="_idIndexMarker894"/> our encoder-decoder model.</p>
    <h1 id="_idParaDest-235" class="heading-1">Defining the model</h1>
    <p class="normal">In this section, we will define<a id="_idIndexMarker895"/> the model from end to end. </p>
    <p class="normal">We are going to implement an encoder-decoder based NMT model equipped with additional techniques to boost performance. Let’s start off by converting our string tokens to IDs.</p>
    <h2 id="_idParaDest-236" class="heading-2">Converting tokens to IDs</h2>
    <p class="normal">Before we jump<a id="_idIndexMarker896"/> to the<a id="_idIndexMarker897"/> model, we have<a id="_idIndexMarker898"/> one more text processing operation remaining, that is, converting the processed text tokens into numerical IDs. We are going to use a <code class="inlineCode">tf.keras.layers.Layer</code> to do this. Particularly, we’ll be using the <code class="inlineCode">StringLookup</code> layer to create a layer in our model that converts each token into a numerical ID. As the first step, let us load the vocabulary files provided in the data. Before doing so, we will define the variable <code class="inlineCode">n_vocab</code> to denote the size of the vocabulary for each language:</p>
    <pre class="programlisting code"><code class="hljs-code">n_vocab = <span class="hljs-number">25000</span> + <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Originally, each vocabulary<a id="_idIndexMarker899"/> contains 50,000 tokens. However, we’ll take<a id="_idIndexMarker900"/> only half of this to reduce the memory<a id="_idIndexMarker901"/> requirement. Note that we allow one extra token as there’s a special token <code class="inlineCode">&lt;unk&gt;</code> to denote <strong class="keyWord">out-of-vocabulary</strong> (<strong class="keyWord">OOV</strong>) words. With a 50,000-token vocabulary, it is quite easy to run out of memory due to the size of the final prediction layer we’ll build. While cutting back on the size of the vocabulary, we have to make sure that we preserve the most common 25,000 words. Fortunately, each vocabulary file is organized such that words are ordered by their frequency of occurrence (high to low). Therefore, we just need to read the first 25,001 lines of text from the file:</p>
    <pre class="programlisting code"><code class="hljs-code">en_vocabulary = []
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'vocab.50K.en'</span>), <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> en_file:
    <span class="hljs-keyword">for</span> ri, row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(en_file):
        <span class="hljs-keyword">if</span> ri  &gt;= n_vocab: <span class="hljs-keyword">break</span>
        
        en_vocabulary.append(row.strip())
</code></pre>
    <p class="normal">Then we do the same for the German vocabulary: </p>
    <pre class="programlisting code"><code class="hljs-code">de_vocabulary = []
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'</span><span class="hljs-string">vocab.50K.de'</span>), <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> de_file:
    <span class="hljs-keyword">for</span> ri, row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(de_file):
        <span class="hljs-keyword">if</span> ri &gt;= n_vocab: <span class="hljs-keyword">break</span>
        
        de_vocabulary.append(row.strip())
</code></pre>
    <p class="normal">Each of the vocabularies contain the special OOV token <code class="inlineCode">&lt;unk&gt;</code> as the first line. We’ll pop that out of the <code class="inlineCode">en_vocabulary</code> and <code class="inlineCode">de_vocabulary</code> lists as we need this for the next step:</p>
    <pre class="programlisting code"><code class="hljs-code">en_unk_token = en_vocabulary.pop(<span class="hljs-number">0</span>)
de_unk_token = de_vocabulary.pop(<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Here’s how we can define our English <code class="inlineCode">StringLookup</code> layer:</p>
    <pre class="programlisting code"><code class="hljs-code">en_lookup_layer = tf.keras.layers.StringLookup(
    vocabulary=en_vocabulary, oov_token=en_unk_token, 
    mask_token=pad_token, pad_to_max_tokens=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">Let’s understand the arguments provided to this layer:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">vocabulary</code> – Contains a list of words that are found in the corpus (except certain special tokens that will be discussed below)</li>
      <li class="bulletList"><code class="inlineCode">oov_token</code> – A special out-of-vocabulary token that will be used to replace tokens not listed in the vocabulary</li>
      <li class="bulletList"><code class="inlineCode">mask_token</code> – A special token that will be used to mask inputs (e.g. uninformative padded tokens)</li>
      <li class="bulletList"><code class="inlineCode">pad_to_max_tokens</code> – If padding should occur to bring arbitrary-length sequences in a batch of data to the same length</li>
    </ul>
    <p class="normal">Similarly, we define a lookup layer for the German language:</p>
    <pre class="programlisting code"><code class="hljs-code">de_lookup_layer = tf.keras.layers.StringLookup(
    vocabulary=de_vocabulary, oov_token=de_unk_token, 
    mask_token=pad_token, pad_to_max_tokens=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">With the groundwork<a id="_idIndexMarker902"/> laid out, we can <a id="_idIndexMarker903"/>start building<a id="_idIndexMarker904"/> the encoder.</p>
    <h2 id="_idParaDest-237" class="heading-2">Defining the encoder</h2>
    <p class="normal">We start the encoder<a id="_idIndexMarker905"/> with an input layer. The input layer will take in a batch of sequences of tokens. Each sequence of tokens is <code class="inlineCode">n_en_seq_length</code> elements long. Remember that we padded or truncated the sentences to make sure all of them have a fixed length of <code class="inlineCode">n_en_seq_length</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)
</code></pre>
    <p class="normal">Next we use the previously defined <code class="inlineCode">StringLookup</code> layer to convert the string tokens into word IDs. As we saw, the <code class="inlineCode">StringLookup</code> layer can take a list of unique words (i.e. a vocabulary) and create a lookup operation to convert a given token into a numerical ID: </p>
    <pre class="programlisting code"><code class="hljs-code">encoder_wid_out = en_lookup_layer(encoder_input)
</code></pre>
    <p class="normal">With the tokens converted<a id="_idIndexMarker906"/> into IDs, we route the generated word IDs to a token embedding layer. We pass in the size of the vocabulary (derived from the <code class="inlineCode">en_lookup_layer</code>'s <code class="inlineCode">get_vocabulary()</code> method) and the embedding size (128) and finally we ask the layer to mask any zero-valued inputs as they don’t contain any information: </p>
    <pre class="programlisting code"><code class="hljs-code">en_full_vocab_size = <span class="hljs-built_in">len</span>(en_lookup_layer.get_vocabulary())
encoder_emb_out = tf.keras.layers.Embedding(en_full_vocab_size, <span class="hljs-number">128</span>, mask_zero=<span class="hljs-literal">True</span>)(encoder_wid_out)
</code></pre>
    <p class="normal">The output of the embedding layer is stored in <code class="inlineCode">encoder_emb_out</code>. Next we define a GRU layer to process the sequence of English token embeddings:</p>
    <pre class="programlisting code"><code class="hljs-code">encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(<span class="hljs-number">256</span>, return_sequences=<span class="hljs-literal">True</span>, return_state=<span class="hljs-literal">True</span>)(encoder_emb_out)
</code></pre>
    <p class="normal">Note how we are setting both the <code class="inlineCode">return_sequences</code> and <code class="inlineCode">return_state</code> arguments to <code class="inlineCode">True</code>. To recap, <code class="inlineCode">return_sequences</code> returns the full sequence of hidden states as the output (instead of returning only the last), where <code class="inlineCode">return_state</code> returns the last state of the model as an additional output. We need both these outputs to build the rest of our model. For example, we need to pass the last state of the encoder to the decoder as the initial state. For that, we need the last state of the encoder (stored in <code class="inlineCode">encoder_gru_last_state</code>). We will discuss the purpose of this in more detail as we go. We now have everything to define the encoder part of our model. It takes in a batch of sequences of string tokens and returns the full sequence of GRU hidden states as the output.</p>
    <pre class="programlisting code"><code class="hljs-code">encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)
</code></pre>
    <p class="normal">With the encoder defined, let’s build the decoder.</p>
    <h2 id="_idParaDest-238" class="heading-2">Defining the decoder</h2>
    <p class="normal">Our decoder will be more complex<a id="_idIndexMarker907"/> than the encoder. The objective of the decoder is, given the last encoder state and the previous token the decoder predicted, predict the next token. For example, for the German sentence: </p>
    <p class="center"><em class="italic">&lt;s&gt; ich ging zum Laden &lt;/s&gt;</em></p>
    <p class="normal">We define:</p>
    <table id="table002" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Input</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">&lt;s&gt;</p>
          </td>
          <td class="table-cell">
            <p class="normal">ich</p>
          </td>
          <td class="table-cell">
            <p class="normal">ging</p>
          </td>
          <td class="table-cell">
            <p class="normal">zum</p>
          </td>
          <td class="table-cell">
            <p class="normal">Laden</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Output</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal">ich</p>
          </td>
          <td class="table-cell">
            <p class="normal">ging</p>
          </td>
          <td class="table-cell">
            <p class="normal">zum</p>
          </td>
          <td class="table-cell">
            <p class="normal">Laden</p>
          </td>
          <td class="table-cell">
            <p class="normal">&lt;/s&gt;</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">This technique is known as <strong class="keyWord">teacher forcing</strong>. In other words, the decoder<a id="_idIndexMarker908"/> is leveraging previous tokens of the target itself to predict the next token. This makes the translation task easier for the model. We can understand this phenomenon as follows. Say a teacher asks a kindergarten student to complete the following sentence, given just the first word:</p>
    <p class="center"><em class="italic">I ___ ____ ___ ___ ____ ____</em></p>
    <p class="normal">This means that the child needs to pick a subject, verb, and object; know the syntax of the language; understand the grammar rules of the language; and so on. Therefore, the likelihood of the child producing an incorrect sentence is high.</p>
    <p class="normal">However, if we ask the child to produce it word by word, they might do a better job at coming up with a sentence. In other words, we ask the child to produce the next word given the following:</p>
    <p class="center"><em class="italic">I ____</em></p>
    <p class="normal">Then we ask them to fill in the blank given:</p>
    <p class="center"><em class="italic">I like ____</em></p>
    <p class="normal">And continue in the same fashion:</p>
    <p class="center"><em class="italic">I like to ___, I like to fly ____, I like to fly kites ____</em></p>
    <p class="normal">This way, the child can do a better job at producing a correct and meaningful sentence. We can adopt the same approach to alleviate the difficulty of the translation task, as shown in <em class="italic">Figure 9.10</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.10: The teacher forcing mechanism. The darker arrows in the inputs depict newly introduced input connections to the decoder. The right-hand side figure shows how the decoder GRU cell changes.</p>
    <p class="normal">To feed in previous tokens predicted<a id="_idIndexMarker909"/> by the decoder, we need an input layer for the decoder. When formulating the decoder inputs and outputs this way, for a sequence of tokens with length <em class="italic">n</em>, the input and output are <em class="italic">n-1</em> tokens long: </p>
    <pre class="programlisting code"><code class="hljs-code">decoder_input = tf.keras.layers.Input(shape=(n_de_seq_length-<span class="hljs-number">1</span>,), dtype=tf.string)
</code></pre>
    <p class="normal">Next, we use the <code class="inlineCode">de_lookup_layer</code> defined earlier to convert tokens to IDs:</p>
    <pre class="programlisting code"><code class="hljs-code">decoder_wid_out = de_lookup_layer(decoder_input)
</code></pre>
    <p class="normal">Similar to the encoder, let’s define an embedding layer for the German language: </p>
    <pre class="programlisting code"><code class="hljs-code">de_full_vocab_size = <span class="hljs-built_in">len</span>(de_lookup_layer.get_vocabulary())
decoder_emb_out = tf.keras.layers.Embedding(de_full_vocab_size, <span class="hljs-number">128</span>, mask_zero=<span class="hljs-literal">True</span>)(decoder_wid_out)
</code></pre>
    <p class="normal">We define a GRU layer in the decoder that will take the token embeddings and produce hidden outputs: </p>
    <pre class="programlisting code"><code class="hljs-code">decoder_gru_out = tf.keras.layers.GRU(<span class="hljs-number">256</span>, return_sequences=<span class="hljs-literal">True</span>)(decoder_emb_out, initial_state=encoder_gru_last_state)
</code></pre>
    <p class="normal">Note that we are passing the encoder’s last state to a special argument called <code class="inlineCode">initial_state</code> in the GRU’s <code class="inlineCode">call()</code> method. This ensures that the decoder uses the encoder’s last state to initialize its memory.</p>
    <p class="normal">The next step of our journey takes us to one of the most important concepts in machine learning, ‘attention.’ So far, the decoder had to rely on the encoder’s last state as the ‘only’ input/signal about the source<a id="_idIndexMarker910"/> language. This is like asking to summarize a sentence using a single word. Generally, when doing so, you lose a lot of the meaning and message in this conversion. Attention alleviates this problem.</p>
    <h2 id="_idParaDest-239" class="heading-2">Attention: Analyzing the encoder states</h2>
    <p class="normal">Instead of relying just<a id="_idIndexMarker911"/> on the encoder’s last state, attention enables the decoder to analyze the complete history of state outputs. The decoder does this at every step of the prediction and creates a weighted average of all the state outputs depending on what it needs to produce at that step. For example, in the translation <em class="italic">I went to the shop -&gt; ich ging zum Laden</em>, when predicting the word <em class="italic">ging</em>, the decoder will pay more attention to the first part of the English sentence than the latter.</p>
    <p class="normal">There have been many different implementations of attention over the years. It’s important to properly emphasize the need for attention in NMT systems. As you have learned previously, the context, or thought vector, that resides between the encoder and the decoder is a performance bottleneck (see <em class="italic">Figure 9.11</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_11.png" alt="Breaking the context vector bottleneck"/></figure>
    <p class="packt_figref">Figure 9.11: The encoder-decoder architecture</p>
    <p class="normal">To understand why this is a bottleneck, let’s imagine translating the following English sentence:</p>
    <p class="center"><em class="italic">I went to the flower market to buy some flowers</em></p>
    <p class="normal">This translates to the following:</p>
    <p class="center"><em class="italic">Ich ging zum Blumenmarkt, um Blumen zu kaufen</em></p>
    <p class="normal">If we are to compress this into a fixed-length vector, the resulting vector needs to contain these:</p>
    <ul>
      <li class="bulletList">Information about the subject (<em class="italic">I</em>)</li>
      <li class="bulletList">Information about the verbs (<em class="italic">buy </em>and <em class="italic">went</em>)</li>
      <li class="bulletList">Information about the objects (<em class="italic">flowers </em>and <em class="italic">flower market</em>)</li>
      <li class="bulletList">Interaction of the subjects, verbs, and objects with each other in the sentence</li>
    </ul>
    <p class="normal">Generally, the context<a id="_idIndexMarker912"/> vector has a size of 128 or 256 elements. Reliance on the context vector to store all this information with a small-sized vector is very impractical and an extremely difficult requirement for the system. Therefore, most of the time, the context vector fails to provide the complete information required to make a good translation. This results in an underperforming decoder that suboptimally translates a sentence.</p>
    <p class="normal">To make the problem worse, during decoding the context vector is observed only in the beginning. Thereafter, the decoder GRU must memorize the context vector until the end of the translation. This becomes more and more difficult for long sentences.</p>
    <p class="normal">Attention sidesteps this issue. With attention, the decoder will have access to the full state history of the encoder for each decoding time step. This allows the decoder to access a very rich representation of the source sentence. Furthermore, the attention mechanism introduces a softmax layer that allows the decoder to calculate a weighted mean of the past observed encoder states, which will be used as the context vector for the decoder. This allows the decoder to pay different amounts of attention to different words at different decoding steps. </p>
    <p class="normal"><em class="italic">Figure 9.12 </em>shows a conceptual breakdown of the attention mechanism:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_12.png" alt="The attention mechanism in detail"/></figure>
    <p class="packt_figref">Figure 9.12: Conceptual attention mechanism in NMT</p>
    <p class="normal">Next, let’s look at how<a id="_idIndexMarker913"/> we can compute attention.</p>
    <h3 id="_idParaDest-240" class="heading-3">Computing Attention</h3>
    <p class="normal">Now let’s investigate<a id="_idIndexMarker914"/> the actual implementation of the attention mechanism in detail. For this, we will use the Bahdanau attention mechanism introduced in the paper <em class="italic">Neural Machine Translation by Learning to Jointly Align and Translate</em>, by Bahdanau et al. We will discuss the original attention mechanism here. However, we’ll be implementing a slightly different version of it, due to the limitations of TensorFlow. For consistency with the paper, we will use the following notations:</p>
    <ul>
      <li class="bulletList">Encoder’s <em class="italic">j</em><sup class="superscript">th</sup> hidden state: <em class="italic">h</em><sub class="subscript">j</sub></li>
      <li class="bulletList"><em class="italic">i</em><sup class="superscript">th</sup> target token: <em class="italic">y</em><sub class="subscript">i</sub></li>
      <li class="bulletList"><em class="italic">i</em><sup class="superscript">th</sup> decode hidden state in the <em class="italic">i</em><sup class="superscript">th</sup> time step: <em class="italic">s</em><sub class="subscript">i</sub></li>
      <li class="bulletList">Context vector: <em class="italic">c</em><sub class="subscript">i</sub></li>
    </ul>
    <p class="normal">Our decoder GRU is a function of an input <em class="italic">y</em><sub class="subscript">i</sub> and a previous step’s hidden state <img src="../Images/B14070_09_043.png" alt="" style="height: 1.15em !important; vertical-align: -0.18em !important;"/>. This can be represented as follows:</p>
    <p class="center"><img src="../Images/B14070_09_044.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Here, <em class="italic">f</em> represents the actual update rules<a id="_idIndexMarker915"/> used to calculate <em class="italic">y</em><sub class="subscript">i</sub> and <em class="italic">s</em><sub class="subscript">i-1</sub>. With the attention mechanism, we are introducing a new time-dependent context vector <em class="italic">c</em><sub class="subscript">i</sub> for the <em class="italic">i</em><sup class="superscript">th</sup> decoding step. The <em class="italic">c</em><sub class="subscript">i</sub> vector is a weighted mean of the hidden states of all the unrolled encoder steps. A higher weight will be given to the <em class="italic">j</em><sup class="superscript">th</sup> hidden state of the encoder if the <em class="italic">j</em><sup class="superscript">th</sup> word is more important for translating the <em class="italic">i</em><sup class="superscript">th</sup> word in the target language. This means the model can learn which words are important at which time step, regardless of the directionality of the two languages or alignment mismatches. Now the decoder GRU becomes this:</p>
    <p class="center"><img src="../Images/B14070_09_045.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Conceptually, the attention mechanism can be thought of as a separate layer and illustrated as in <em class="italic">Figure 9.13</em>. As shown, attention functions as a layer. The attention layer is responsible for producing <em class="italic">c</em><sub class="subscript">i</sub> for the <em class="italic">i</em><sup class="superscript">th</sup> time step of the decoding process.</p>
    <p class="normal">Let’s now see how to calculate <em class="italic">c</em><sub class="subscript">i</sub>: </p>
    <p class="center"><img src="../Images/B14070_09_046.png" alt="" style="height: 3.55em !important;"/></p>
    <p class="normal">Here, <em class="italic">L</em> is the number of words in the source sentence, and <img src="../Images/B14070_09_047.png" alt="" style="height: 1.35em !important; vertical-align: -0.35em !important;"/> is a normalized weight representing the importance of the <em class="italic">j</em><sup class="superscript">th</sup> encoder hidden state for calculating the <em class="italic">i</em><sup class="superscript">th</sup> decoder prediction. This<a id="_idIndexMarker916"/> is calculated using what is known as an energy value. We represent <em class="italic">e</em><sub class="subscript">ij</sub> as the energy of the encoder’s <em class="italic">j</em><sup class="superscript">th</sup> position for predicting the decoder’s <em class="italic">i</em><sup class="superscript">th</sup> position. <em class="italic">e</em><sub class="subscript">ij</sub> is computed using a small fully connected network as follows: </p>
    <p class="center"><img src="../Images/B14070_09_048.png" alt="" style="height: 1.45em !important;"/></p>
    <p class="normal">In other words, <img src="../Images/B14070_09_049.png" alt="" style="height: 1.35em !important; vertical-align: -0.35em !important;"/> is calculated with a multilayer perceptron whose weights are <em class="italic">v</em><sub class="subscript">a</sub>, <em class="italic">W</em><sub class="subscript">a</sub>, and <em class="italic">U</em><sub class="subscript">a</sub>, and <img src="../Images/B14070_09_050.png" alt="" style="height: 1.15em !important; vertical-align: -0.15em !important;"/> (decoder’s previous hidden state from (<em class="italic">i-1</em>)<sup class="superscript">th</sup> time step) and <em class="italic">h</em><sub class="subscript">j</sub> (encoder’s <em class="italic">j</em><sup class="superscript">th</sup> hidden output) are the inputs to the network. Finally, we compute the normalized energy values (i.e. weights) using softmax normalization over all encoder timesteps: </p>
    <p class="center"><img src="../Images/B14070_09_051.png" alt="" style="height: 2.83em !important;"/></p>
    <p class="normal">The attention mechanism<a id="_idIndexMarker917"/> is shown in <em class="italic">Figure 9.13</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_13.png" alt="The attention mechanism in detail"/></figure>
    <p class="packt_figref">Figure 9.13: The attention mechanism</p>
    <h3 id="_idParaDest-241" class="heading-3">Implementing Attention</h3>
    <p class="normal">We said above that<a id="_idIndexMarker918"/> we’ll be implementing a slightly different variation of Bahdanau attention. This is because TensorFlow currently does not support an attention mechanism that can be iteratively computed for each time step, similar to how an RNN works. Therefore, we are going to decouple the attention mechanism from the GRU model and have it computed separately. We will concatenate the attention output with the hidden output of the GRU layer and feed it to the final prediction layer. In other words, we are not feeding attention output to the GRU model, but directly to the prediction layer. This is depicted in <em class="italic">Figure 9.14</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.14: The attention mechanism employed in this chapter</p>
    <p class="normal">To implement attention, we are going to use the sub-classing API of Keras. We’ll define a class called <code class="inlineCode">BahdanauAttention</code> (which inherits from the <code class="inlineCode">Layer</code> class) and override two functions in that:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">__init__()</code> – Defines the layer’s initialization logic</li>
      <li class="bulletList"><code class="inlineCode">call()</code> – Defines the computational logic of the layer</li>
    </ul>
    <p class="normal">Our defined class would<a id="_idIndexMarker919"/> look like this. But don’t worry, we’ll be going through those two functions in detail below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">BahdanauAttention</span>(tf.keras.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, units</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-comment"># Weights to compute Bahdanau attention</span>
        self.Wa = tf.keras.layers.Dense(units, use_bias=<span class="hljs-literal">False</span>)
        self.Ua = tf.keras.layers.Dense(units, use_bias=<span class="hljs-literal">False</span>)
        self.attention = 
        tf.keras.layers.AdditiveAttention(use_scale=<span class="hljs-literal">True</span>)
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, query, key, value, mask, </span>
<span class="hljs-keyword">    </span><span class="hljs-params">return_attention_scores=</span><span class="hljs-literal">False</span>):
        <span class="hljs-comment"># Compute 'Wa.ht'.</span>
        wa_query = self.Wa(query)
        <span class="hljs-comment"># Compute 'Ua.hs'.</span>
        ua_key = self.Ua(key)
        <span class="hljs-comment"># Compute masks</span>
        query_mask = tf.ones(tf.shape(query)[:-<span class="hljs-number">1</span>], dtype=<span class="hljs-built_in">bool</span>)
        value_mask = mask
        <span class="hljs-comment"># Compute the attention</span>
        context_vector, attention_weights = self.attention(
            inputs = [wa_query, value, ua_key],
            mask=[query_mask, value_mask, value_mask],
            return_attention_scores = <span class="hljs-literal">True</span>,
        )
        
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_attention_scores:
            <span class="hljs-keyword">return</span> context_vector
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> context_vector, attention_weights
</code></pre>
    <p class="normal">First, we’ll be looking at the <code class="inlineCode">__init__()</code> function.</p>
    <p class="normal">Here, you can see<a id="_idIndexMarker920"/> that we are defining three layers: weight matrix <code class="inlineCode">W_a</code>, weight matrix <code class="inlineCode">U_a</code>, and finally the <code class="inlineCode">AdditiveAttention</code> layer, which contains the attention computation logic we discussed above. The <code class="inlineCode">AdditiveAttention</code> layer takes in a query, value and a key. The query is the decoder states, and the value and key are all of encoder states produced. </p>
    <p class="normal">We will discuss this layer in more detail soon. We’ll discuss the details of this layer below. Next let’s look at the computations<a id="_idIndexMarker921"/> defined in the <code class="inlineCode">call()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, query, key, value, mask, return_attention_scores=</span><span class="hljs-literal">False</span>):
        <span class="hljs-comment"># Compute 'Wa.ht'</span>
        wa_query = self.Wa(query)
        <span class="hljs-comment"># Compute 'Ua.hs'</span>
        ua_key = self.Ua(key)
        <span class="hljs-comment"># Compute masks</span>
        query_mask = tf.ones(tf.shape(query)[:-<span class="hljs-number">1</span>], dtype=<span class="hljs-built_in">bool</span>)
        value_mask = mask
        <span class="hljs-comment"># Compute the attention</span>
        context_vector, attention_weights = self.attention(
            inputs = [wa_query, value, ua_key],
            mask=[query_mask, value_mask, value_mask],
            return_attention_scores = <span class="hljs-literal">True</span>,
        )
        
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> return_attention_scores:
            <span class="hljs-keyword">return</span> context_vector
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> context_vector, attention_weights
</code></pre>
    <p class="normal">The first thing to note is that this function takes a query, a key, and a value. These three elements will drive the attention computation. In Bahdanau attention, you can think of the key and value as being the same thing. The query will represent each decoder GRU’s hidden states for each time step, and the value (or key) will represent each encoder GRU’s hidden states for each time step. In other words, we are querying an output for each decoder position based on values provided by the encoder’s hidden states. </p>
    <p class="normal">Let’s recap the computations we have to perform:</p>
    <p class="center"><img src="../Images/B14070_09_048.png" alt="" style="height: 1.45em !important;"/></p>
    <p class="center"><img src="../Images/B14070_09_051.png" alt="" style="height: 2.83em !important;"/></p>
    <p class="center"><img src="../Images/B14070_09_046.png" alt="" style="height: 3.55em !important;"/></p>
    <p class="normal">First we compute <code class="inlineCode">wa_query</code> (represents <img src="../Images/B14070_09_055.png" alt="" style="height: 1.15em !important; vertical-align: -0.23em !important;"/>) and <code class="inlineCode">ua_key</code> (represents <img src="../Images/B14070_09_056.png" alt="" style="height: 1.35em !important; vertical-align: -0.45em !important;"/>). Next, we propagate these<a id="_idIndexMarker922"/> values to the attention layer. The <code class="inlineCode">AdditiveAttention</code> layer (<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention</span></a>) performs the following<a id="_idIndexMarker923"/> steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Reshapes <code class="inlineCode">wa_query</code> from <code class="inlineCode">[batch_size, Tq, dim]</code> to shape<code class="inlineCode">[batch_size, Tq, 1, dim]</code> and <code class="inlineCode">ua_key</code> from <code class="inlineCode">[batch_size, Tv, dim]</code> shape to <code class="inlineCode">[batch_size, 1, Tv, dim]</code>.</li>
      <li class="numberedList">Calculates scores with shape <code class="inlineCode">[batch_size, Tq, Tv]</code> as: <code class="inlineCode">scores = tf.reduce_sum(tf.tanh(query + key), axis=-1)</code>.</li>
      <li class="numberedList">Uses scores to calculate a distribution with shape <code class="inlineCode">[batch_size, Tq, Tv]</code> using softmax activation: <code class="inlineCode">distribution = tf.nn.softmax(scores)</code>.</li>
      <li class="numberedList">Uses <code class="inlineCode">distribution</code> to create a linear combination of <code class="inlineCode">value</code> with shape <code class="inlineCode">[batch_size, Tq, dim]</code>.</li>
      <li class="numberedList">Returns <code class="inlineCode">tf.matmul(distribution, value)</code>, which represents a weighted average of all encoder states (i.e. <code class="inlineCode">value</code>)</li>
    </ol>
    <p class="normal">Here, you can see that <em class="italic">step 2</em> performs the first equation, <em class="italic">step 3</em> performs the second equation, and finally <em class="italic">step 4</em> performs the third equation. Another thing worth noting is that <em class="italic">step 2</em> does not mention <img src="../Images/B14070_09_057.png" alt="" style="height: 1.15em !important; vertical-align: -0.18em !important;"/> from the first equation. <img src="../Images/B14070_09_058.png" alt="" style="height: 1.15em !important; vertical-align: -0.14em !important;"/> is essentially a weight matrix with which we compute the dot product. We can introduce this weight matrix by setting <code class="inlineCode">use_scale=True</code> when defining the <code class="inlineCode">AdditiveAttention</code> layer:</p>
    <pre class="programlisting code"><code class="hljs-code">self.attention = tf.keras.layers.AdditiveAttention(use_scale=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Another important<a id="_idIndexMarker924"/> argument is the <code class="inlineCode">return_attention_scores</code> argument when calling the <code class="inlineCode">AdditiveAttention</code> layer. This gives us the distribution weight matrix defined in <em class="italic">step 3</em>. We will use this to visualize where the model was paying attention when decoding the translation.</p>
    <h2 id="_idParaDest-242" class="heading-2">Defining the final model</h2>
    <p class="normal">With the attention mechanism<a id="_idIndexMarker925"/> understood and implemented, let’s continue our implementation of the decoder. We will get the attention output sequence, with one attended output for each time step. </p>
    <p class="normal">Moreover, we’ll get the attention weights distribution matrix, which we’ll use to visualize attention patterns against inputs and outputs:</p>
    <pre class="programlisting code"><code class="hljs-code">decoder_attn_out, attn_weights = BahdanauAttention(<span class="hljs-number">256</span>)(
    query=decoder_gru_out, key=encoder_gru_out, value=encoder_gru_out,
    mask=(encoder_wid_out != <span class="hljs-number">0</span>),
    return_attention_scores=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">When defining attention, we’ll also<a id="_idIndexMarker926"/> pass a mask that denotes which tokens need to be ignored when computing outputs (e.g. padded tokens). Combine the attention output and the decoder’s GRU output to create a single concatenated input for the prediction layer:</p>
    <pre class="programlisting code"><code class="hljs-code">context_and_rnn_output = tf.keras.layers.Concatenate(axis=-<span class="hljs-number">1</span>)([decoder_attn_out, decoder_gru_out])
</code></pre>
    <p class="normal">Finally, the prediction layer takes the concatenated attention’s context vector and the GRU output to produce probability distributions over the German tokens for each timestep:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Final prediction layer (size of the vocabulary)</span>
decoder_out = tf.keras.layers.Dense(full_de_vocab_size, activation=<span class="hljs-string">'softmax'</span>)(context_and_rnn_output)
</code></pre>
    <p class="normal">With the encoder and the decoder fully defined, let’s define the end-to-end model:</p>
    <pre class="programlisting code"><code class="hljs-code">seq2seq_model = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input], outputs=decoder_out)
seq2seq_model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'</span><span class="hljs-string">sparse_categorical_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>, metrics=<span class="hljs-string">'accuracy'</span>)
</code></pre>
    <p class="normal">We are also going to define a secondary model called the <code class="inlineCode">attention_visualizer</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">attention_visualizer = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out])
</code></pre>
    <p class="normal">The <code class="inlineCode">attention_visualizer</code> can generate attention patterns for a given set of inputs. This is a handy way to know if the model is paying attention to the correct words during the decoding process. This visualizer model<a id="_idIndexMarker927"/> will be used once the full model is trained. We will now look at how we can train our model.</p>
    <h1 id="_idParaDest-243" class="heading-1">Training the NMT</h1>
    <p class="normal">Now that we have defined <a id="_idIndexMarker928"/>the NMT architecture and preprocessed the training data, it is quite straightforward to train the model. Here, we will define and illustrate (see <em class="italic">Figure 9.15</em>) the exact process used for training:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.15: The training procedure for NMT</p>
    <p class="normal">For the model training, we’re going to define a custom training loop, as there is a special metric we’d like to track. Unfortunately, this metric is not a readily available TensorFlow metric. But before that, there are several utility functions we need to define:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_data</span>(<span class="hljs-params">de_lookup_layer, train_xy, valid_xy, test_xy</span>):
    <span class="hljs-string">""" Create a data dictionary from the dataframes containing data </span>
<span class="hljs-string">    """</span>
    
    data_dict = {}
    <span class="hljs-keyword">for</span> label, data_xy <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([<span class="hljs-string">'train'</span>, <span class="hljs-string">'valid'</span>, <span class="hljs-string">'test'</span>], [train_xy, 
<span class="hljs-keyword">    </span>valid_xy, test_xy]):
        
        data_x, data_y = data_xy
        en_inputs = data_x
        de_inputs = data_y[:,:-<span class="hljs-number">1</span>]
        de_labels = de_lookup_layer(data_y[:,<span class="hljs-number">1</span>:]).numpy()
        data_dict[label] = {<span class="hljs-string">'encoder_inputs'</span>: en_inputs, 
        <span class="hljs-string">'decoder_inputs'</span>: de_inputs, <span class="hljs-string">'decoder_labels'</span>: de_labels}
    
    <span class="hljs-keyword">return</span> data_dict
</code></pre>
    <p class="normal">The <code class="inlineCode">prepare_data()</code> function<a id="_idIndexMarker929"/> takes the source sentence and target sentence pairs and generates encoder and decoder inputs and decoder labels. Let’s understand the arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">de_lookup_layer</code> – The <code class="inlineCode">StringLookup</code> layer of the German language</li>
      <li class="bulletList"><code class="inlineCode">train_xy</code> – A tuple containing tokenized English sentences and tokenized German sentences in the training set, respectively</li>
      <li class="bulletList"><code class="inlineCode">valid_xy</code> – Similar to <code class="inlineCode">train_xy</code> but for validation data</li>
      <li class="bulletList"><code class="inlineCode">test_xy</code> – Similar to <code class="inlineCode">train_xy</code> but for test data</li>
    </ul>
    <p class="normal">For each training, validation, and test dataset, this function generates the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">encoder_inputs</code> – Tokenized English sentences as in the preprocessed dataset</li>
      <li class="bulletList"><code class="inlineCode">decoder_inputs</code> – All tokens except the last of each German sentence</li>
      <li class="bulletList"><code class="inlineCode">decoder_labels</code> – All token IDs except the first of each German sentence, where token IDs are generated by the <code class="inlineCode">de_lookup_layer</code></li>
    </ul>
    <p class="normal">So, you can see that <code class="inlineCode">decoder_labels</code> will be <code class="inlineCode">decoder_inputs</code> shifted one token to the left. Next we define the <code class="inlineCode">shuffle_data()</code> function, which will shuffle a provided set of data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">shuffle_data</span>(<span class="hljs-params">en_inputs, de_inputs, de_labels, shuffle_inds=</span><span class="hljs-literal">None</span>): 
    <span class="hljs-string">""" Shuffle the data randomly (but all of inputs and labels at </span>
<span class="hljs-string">    ones)"""</span>
        
    <span class="hljs-keyword">if</span> shuffle_inds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        <span class="hljs-comment"># If shuffle_inds are not passed create a shuffling </span>
<span class="hljs-comment">        automatically</span>
        shuffle_inds = 
        np.random.permutation(np.arange(en_inputs.shape[<span class="hljs-number">0</span>]))
    <span class="hljs-keyword">else</span>:
        <span class="hljs-comment"># Shuffle the provided shuffle_inds</span>
        shuffle_inds = np.random.permutation(shuffle_inds)
    
    <span class="hljs-comment"># Return shuffled data</span>
    <span class="hljs-keyword">return</span> (en_inputs[shuffle_inds], de_inputs[shuffle_inds], 
<span class="hljs-keyword">    </span>de_labels[shuffle_inds]), shuffle_inds
</code></pre>
    <p class="normal">The logic here is quite straightforward. We take the <code class="inlineCode">encoder_inputs</code>, <code class="inlineCode">decoder_inputs</code>, and <code class="inlineCode">decoder_labels</code> (generated by the <code class="inlineCode">prepare_data()</code> step) with <code class="inlineCode">shuffle_inds</code>. If <code class="inlineCode">shuffle_inds</code> is <code class="inlineCode">None</code>, we generate<a id="_idIndexMarker930"/> a random permutation of the indices. Otherwise, we generate a random permutation of the <code class="inlineCode">shuffle_inds</code> provided. Finally, we index all of the data according to the shuffled index. We can then train the model:</p>
    <pre class="programlisting code"><code class="hljs-code">Def train_model(model, en_lookup_layer, de_lookup_layer, train_xy, valid_xy, test_xy, epochs, batch_size, shuffle=<span class="hljs-literal">True</span>, predict_bleu_at_training=<span class="hljs-literal">False</span>):
    <span class="hljs-string">""" Training the model and evaluating on validation/test sets """</span>
    
    <span class="hljs-comment"># Define the metric</span>
    bleu_metric = BLEUMetric(de_vocabulary)
    <span class="hljs-comment"># Define the data</span>
    data_dict = prepare_data(de_lookup_layer, train_xy, valid_xy, 
    test_xy)
    shuffle_inds = <span class="hljs-literal">None</span>
    
    
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
        <span class="hljs-comment"># Reset metric logs every epoch</span>
        <span class="hljs-keyword">if</span> predict_bleu_at_training:
            blue_log = []
        accuracy_log = []
        loss_log = []
        <span class="hljs-comment"># ========================================================== #</span>
        <span class="hljs-comment">#                     Train Phase                            #</span>
        <span class="hljs-comment"># ========================================================== #</span>
        <span class="hljs-comment"># Shuffle data at the beginning of every epoch</span>
        <span class="hljs-keyword">if</span> shuffle:
            (en_inputs_raw,de_inputs_raw,de_labels), shuffle_inds  = 
            shuffle_data(
                data_dict[<span class="hljs-string">'train'</span>][<span class="hljs-string">'encoder_inputs'</span>],
                data_dict[<span class="hljs-string">'train'</span>][<span class="hljs-string">'decoder_inputs'</span>],
                data_dict[<span class="hljs-string">'train'</span>][<span class="hljs-string">'</span><span class="hljs-string">decoder_labels'</span>],
                shuffle_inds
            )
        <span class="hljs-keyword">else</span>:
            (en_inputs_raw,de_inputs_raw,de_labels)  = (
                data_dict[<span class="hljs-string">'train'</span>][<span class="hljs-string">'encoder_inputs'</span>],
                data_dict[<span class="hljs-string">'train'</span>][<span class="hljs-string">'decoder_inputs'</span>],
                data_dict[<span class="hljs-string">'train'</span>][<span class="hljs-string">'decoder_labels'</span>],
            )
        <span class="hljs-comment"># Get the number of training batches</span>
        n_train_batches = en_inputs_raw.shape[<span class="hljs-number">0</span>]//batch_size
        
        prev_loss = <span class="hljs-literal">None</span>
        <span class="hljs-comment"># Train one batch at a time</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_train_batches):
            <span class="hljs-comment"># Status update</span>
            <span class="hljs-built_in">print</span>(<span class="hljs-string">"Training batch {}/{}"</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>, n_train_batches), 
<span class="hljs-built_in">            </span>end=<span class="hljs-string">'\r'</span>)
            <span class="hljs-comment"># Get a batch of inputs (english and german sequences)</span>
            x = [en_inputs_raw[i*batch_size:(i+<span class="hljs-number">1</span>)*batch_size], 
            de_inputs_raw[i*batch_size:(i+<span class="hljs-number">1</span>)*batch_size]]
            <span class="hljs-comment"># Get a batch of targets (german sequences offset by 1)</span>
            y = de_labels[i*batch_size:(i+<span class="hljs-number">1</span>)*batch_size]
            
            loss, accuracy = model.evaluate(x, y, verbose=<span class="hljs-number">0</span>)
            
            <span class="hljs-comment"># Check if any samples are causing NaNs</span>
            check_for_nans(loss, model, en_lookup_layer, 
            de_lookup_layer)
                
            <span class="hljs-comment"># Train for a single step</span>
            model.train_on_batch(x, y)    
            
            <span class="hljs-comment"># Update the epoch's log records of the metrics</span>
            loss_log.append(loss)
            accuracy_log.append(accuracy)
            
            <span class="hljs-keyword">if</span> predict_bleu_at_training:
                <span class="hljs-comment"># Get the final prediction to compute BLEU</span>
                pred_y = model.predict(x)
                bleu_log.append(bleu_metric.calculate_bleu_from_
                predictions(y, pred_y))
        
        <span class="hljs-built_in">print</span>(<span class="hljs-string">""</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"\nEpoch {}/{}"</span>.<span class="hljs-built_in">format</span>(epoch+<span class="hljs-number">1</span>, epochs))
        <span class="hljs-keyword">if</span> predict_bleu_at_training:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"\t(train) loss: </span><span class="hljs-subst">{np.mean(loss_log)}</span><span class="hljs-string"> - accuracy: </span>
<span class="hljs-built_in">            </span><span class="hljs-subst">{np.mean(accuracy_log)}</span><span class="hljs-string"> - bleu: </span><span class="hljs-subst">{np.mean(bleu_log)}</span><span class="hljs-string">"</span>)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"\t(train) loss: </span><span class="hljs-subst">{np.mean(loss_log)}</span><span class="hljs-string"> - accuracy: </span>
<span class="hljs-built_in">            </span><span class="hljs-subst">{np.mean(accuracy_log)}</span><span class="hljs-string">"</span>)
        <span class="hljs-comment"># ========================================================== #</span>
        <span class="hljs-comment">#                     Validation Phase                       #</span>
        <span class="hljs-comment"># ========================================================== #</span>
        
        val_en_inputs = data_dict[<span class="hljs-string">'valid'</span>][<span class="hljs-string">'encoder_inputs'</span>]
        val_de_inputs = data_dict[<span class="hljs-string">'valid'</span>][<span class="hljs-string">'decoder_inputs'</span>]
        val_de_labels = data_dict[<span class="hljs-string">'valid'</span>][<span class="hljs-string">'decoder_labels'</span>]
            
        val_loss, val_accuracy, val_bleu = evaluate_model(
            model, de_lookup_layer, val_en_inputs, val_de_inputs, 
            val_de_labels, batch_size
        )
            
        <span class="hljs-comment"># Print the evaluation metrics of each epoch</span>
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"\t(valid) loss: {} - accuracy: {} - bleu: </span>
<span class="hljs-built_in">        </span><span class="hljs-string">{}"</span>.<span class="hljs-built_in">format</span>(val_loss, val_accuracy, val_bleu))
    
    <span class="hljs-comment"># ============================================================== #</span>
    <span class="hljs-comment">#                      Test Phase                                #</span>
    <span class="hljs-comment"># ============================================================== #</span>
    
    test_en_inputs = data_dict[<span class="hljs-string">'test'</span>][<span class="hljs-string">'encoder_inputs'</span>]
    test_de_inputs = data_dict[<span class="hljs-string">'</span><span class="hljs-string">test'</span>][<span class="hljs-string">'decoder_inputs'</span>]
    test_de_labels = data_dict[<span class="hljs-string">'test'</span>][<span class="hljs-string">'decoder_labels'</span>]
            
    test_loss, test_accuracy, test_bleu = evaluate_model(
            model, de_lookup_layer, test_en_inputs, test_de_inputs, 
            test_de_labels, batch_size
    )
    
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"\n(test) loss: {} - accuracy: {} - bleu: </span>
<span class="hljs-built_in">    </span><span class="hljs-string">{}"</span>.<span class="hljs-built_in">format</span>(test_loss, test_accuracy, test_bleu))
</code></pre>
    <p class="normal">During model training, we<a id="_idIndexMarker931"/> do the following:</p>
    <ul>
      <li class="bulletList">Prepare encoder and decoder inputs and decoder outputs using the <code class="inlineCode">prepare_data()</code> function</li>
      <li class="bulletList">For each epoch:<ul>
          <li class="bulletList">Shuffle the data if the flag <code class="inlineCode">shuffle</code> is set to <code class="inlineCode">True</code></li>
          <li class="bulletList">For each<a id="_idIndexMarker932"/> iteration: <ul>
              <li class="bulletList">Get a batch of data from prepared inputs and outputs</li>
              <li class="bulletList">Evaluate that batch using <code class="inlineCode">model.evaluate</code> to get the loss and accuracy</li>
              <li class="bulletList">Check if any of the samples are giving <code class="inlineCode">nan</code> values (useful as a debugging step)</li>
              <li class="bulletList">Train on the batch of data</li>
              <li class="bulletList">Compute the BLEU score if the flag <code class="inlineCode">predict_bleu_at_training</code> is set to <code class="inlineCode">True</code></li>
            </ul>
          </li>
          <li class="bulletList">Evaluate the model on validation data to get validation loss and accuracy</li>
          <li class="bulletList">Compute the validation BLEU score</li>
        </ul>
      </li>
      <li class="bulletList">Compute the loss, accuracy, and BLEU score on test data</li>
    </ul>
    <p class="normal">You can see that we are<a id="_idIndexMarker933"/> computing a new metric called the BLEU metric. BLEU is a special metric used to measure performance in sequence-to-sequence problems. It tries to maximize the correctness of n-grams of tokens, rather than measuring it on individual tokens (e.g. accuracy). The higher the BLEU score, the better. You will learn more about how the BLEU score is calculated in the next section. You can see the logic defined in the <code class="inlineCode">BLEUMetric</code> object in the code. </p>
    <p class="normal">In this, we are mostly doing the preprocessing of text to remove uninformative tokens, so that the BLEU score is not overestimated. For example, if we include the <code class="inlineCode">&lt;pad&gt;</code> token, you will see high BLEU scores, as there are long sequences of <code class="inlineCode">&lt;pad&gt;</code> tokens for short sentences. To compute the BLEU score, we’ll be using a third-party implementation available at <a href="https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py"><span class="url">https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py</span></a>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">If you have a large batch size, you may see TensorFlow throwing an exception starting out as:</p>
      <pre class="programlisting con"><code class="hljs-con">Resource exhausted: OOM when allocating tensor with ...
</code></pre>
      <p class="normal">In this case, you may need to restart the notebook kernel, reduce the batch size, and rerun the code.</p>
    </div>
    <p class="normal">Another thing we do, but haven’t discussed, is check<a id="_idIndexMarker934"/> for <code class="inlineCode">NaN</code> (i.e. not-a-number) values. It can be very frustrating to see your loss value being <code class="inlineCode">NaN</code> at the end of a training cycle. This is done by using the <code class="inlineCode">check_for_nan()</code> function. This function will print out any specific data points that caused <code class="inlineCode">NaN</code> values, so you have a much better idea of what caused it. You can find the implementation of the <code class="inlineCode">check_for_nan()</code> function in the code.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">In 2021, the current state-of-the-art BLEU score for German to English translation is 35.14 (<a href="https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german"><span class="url">https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german</span></a>).</p>
    </div>
    <p class="normal">Once the model is fully trained, you should see a BLEU score of around 15 for validation and test data. This is quite good, given that we used a very small proportion of the data (i.e. 250,000 sentences from more than 4 million) and a relatively simpler model compared to the state-of-the-art models.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Improving NMT performance with deep GRUs</strong></p>
      <p class="normal">One obvious improvement<a id="_idIndexMarker935"/> we can do is to increase the number<a id="_idIndexMarker936"/> of layers by stacking GRUs on top of each other, thereby creating a deep GRUs. For example, the Google NMT system uses eight LSTM layers stacked upon each other (<em class="italic">Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, Wu and others, Technical Report (2016)</em>). Though this hampers the computational efficiency, having more layers greatly improves the neural network’s ability to learn the syntax and other linguistic characteristics of the two languages.</p>
    </div>
    <p class="normal">Next, let’s understand how the BLEU score is calculated in detail.</p>
    <h1 id="_idParaDest-244" class="heading-1">The BLEU score – evaluating the machine translation systems</h1>
    <p class="normal"><strong class="keyWord">BLEU</strong> stands for <strong class="keyWord">Bilingual Evaluation Understudy</strong> and is a way of automatically<a id="_idIndexMarker937"/> evaluating machine translation<a id="_idIndexMarker938"/> systems. This metric was first introduced in the paper <em class="italic">BLEU: A Method for Automatic Evaluation of Machine Translation, Papineni and others, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002: 311-318</em>. We will be using an implementation of the BLEU score found at <a href="https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py"><span class="url">https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py</span></a>. Let’s understand how this is calculated in the context of machine translation.</p>
    <p class="normal">Let’s consider an example to learn the calculations of the BLEU score. Say we have two candidate sentences (that is, a sentence predicted by our MT system) and a reference sentence (that is, the corresponding actual translation) for some given source sentence:</p>
    <ul>
      <li class="bulletList">Reference 1: <em class="italic">The cat sat on the mat</em></li>
      <li class="bulletList">Candidate 1: <em class="italic">The cat is on the mat</em></li>
    </ul>
    <p class="normal">To see how good the translation<a id="_idIndexMarker939"/> is, we can use one measure, <strong class="keyWord">precision</strong>. Precision is a measure of how many words in the candidate are actually present in the reference. In general, if you consider a classification problem with two classes (denoted by negative and positive), precision is given by the following formula: </p>
    <p class="center"><img src="../Images/B14070_09_059.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">Let’s now calculate the precision for candidate 1:</p>
    <p class="center"><em class="italic">Precision = # of times each word of candidate appeared in reference/# of words in candidate</em></p>
    <p class="normal">Mathematically, this can be given by the following formula: </p>
    <p class="center"><img src="../Images/B14070_09_060.png" alt="" style="height: 2.60em !important;"/></p>
    <p class="center"><em class="italic">Precision for candidate 1 = 5/6</em></p>
    <p class="normal">This is also known as 1-gram precision since we consider<a id="_idIndexMarker940"/> a single word at a time.</p>
    <p class="normal">Now let’s introduce a new candidate:</p>
    <ul>
      <li class="bulletList">Candidate 2: <em class="italic">The the the cat cat cat</em></li>
    </ul>
    <p class="normal">It is not hard for a human to see that candidate 1 is far better than candidate 2. Let’s calculate the precision:</p>
    <p class="center"><em class="italic">Precision for candidate 2 = 6/6 = 1</em></p>
    <p class="normal">As we can see, the precision<a id="_idIndexMarker941"/> score disagrees with the judgment we made. Therefore, precision alone cannot be trusted to be a good measure of the quality of a translation.</p>
    <h2 id="_idParaDest-245" class="heading-2">Modified precision</h2>
    <p class="normal">To address the precision<a id="_idIndexMarker942"/> limitation, we can use a modified 1-gram precision. The modified precision clips the number of occurrences of each unique word in the candidate by the number of times that word appeared in the reference: </p>
    <p class="center"><img src="../Images/B14070_09_061.png" alt="" style="height: 2.60em !important;"/></p>
    <p class="normal">Therefore, for candidates 1 and 2, the modified precision would be as follows:</p>
    <p class="center"><em class="italic">Mod-1-gram-Precision Candidate 1 = (1 + 1 + 1 + 1 + 1)/ 6 = 5/6</em></p>
    <p class="center"><em class="italic">Mod-1-gram-Precision Candidate 2 = (2 + 1) / 6 = 3/6</em></p>
    <p class="normal">We can already see that this is a good modification as the precision of candidate 2 is reduced. This can be extended to any n-gram by considering <em class="italic">n</em> words at a time instead of a single word.</p>
    <h2 id="_idParaDest-246" class="heading-2">Brevity penalty</h2>
    <p class="normal">Precision naturally prefers<a id="_idIndexMarker943"/> small sentences. This raises a question<a id="_idIndexMarker944"/> in evaluation, as the MT system might generate small sentences for longer references and still have higher precision. Therefore, a <strong class="keyWord">brevity penalty</strong> is introduced to avoid this. The brevity penalty is calculated by the following: </p>
    <p class="center"><img src="../Images/B14070_09_074.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">Here, <em class="italic">c</em> is the candidate sentence length and <em class="italic">r</em> is the reference<a id="_idIndexMarker945"/> sentence length. In our example, we calculate<a id="_idIndexMarker946"/> it as shown here:</p>
    <ul>
      <li class="bulletList">BP for candidate 1 = <img src="../Images/B14070_09_062.png" alt="" style="height: 1.15em !important; vertical-align: -0.05em !important;"/></li>
      <li class="bulletList">BP for candidate 2 = <img src="../Images/B14070_09_062.png" alt="" style="height: 1.15em !important; vertical-align: -0.05em !important;"/></li>
    </ul>
    <h2 id="_idParaDest-247" class="heading-2">The final BLEU score</h2>
    <p class="normal">Next, to calculate the BLEU score, we first calculate<a id="_idIndexMarker947"/> several different modified n-gram precisions for a bunch of different <em class="italic">n=1,2,…,N</em> values. We will then calculate the weighted geometric mean of the n-gram precisions: </p>
    <p class="center"><img src="../Images/B14070_09_065.png" alt="" style="height: 3.33em !important;"/></p>
    <p class="normal">Here, <em class="italic">w</em><sub class="subscript">n</sub> is the weight for the modified n-gram precision <em class="italic">p</em><sub class="subscript">n</sub>. By default, equal weights are used for all n-gram values. In conclusion, BLEU calculates a modified n-gram precision and penalizes the modified-n-gram precision with a brevity penalty. The modified n-gram precision avoids potential high precision values given to meaningless sentences (for example, candidate 2).</p>
    <h1 id="_idParaDest-248" class="heading-1">Visualizing Attention patterns</h1>
    <p class="normal">Remember that we specifically<a id="_idIndexMarker948"/> defined a model called <code class="inlineCode">attention_visualizer</code> to generate attention matrices? With the model trained, we can now look at these attention patterns by feeding data to the model. Here’s how the model was defined:</p>
    <pre class="programlisting code"><code class="hljs-code">attention_visualizer = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out])
</code></pre>
    <p class="normal">We’ll also define a function to get the processed attention matrix along with label data that we can use directly for visualization purposes:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_attention_matrix_for_sampled_data</span>(<span class="hljs-params">attention_model, target_lookup_layer, test_xy, n_samples=</span><span class="hljs-number">5</span>):
    
    test_x, test_y = test_xy
    
    rand_ids = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(test_xy[<span class="hljs-number">0</span>]), 
    size=(n_samples,))
    results = []
    
    <span class="hljs-keyword">for</span> rid <span class="hljs-keyword">in</span> rand_ids:
        en_input = test_x[rid:rid+<span class="hljs-number">1</span>]
        de_input = test_y[rid:rid+<span class="hljs-number">1</span>,:-<span class="hljs-number">1</span>]
                        
        attn_weights, predictions = attention_model.predict([en_input, 
        de_input])
        predicted_word_ids = np.argmax(predictions, axis=-<span class="hljs-number">1</span>).ravel()
        predicted_words = [target_lookup_layer.get_vocabulary()[wid] 
        <span class="hljs-keyword">for</span> wid <span class="hljs-keyword">in</span> predicted_word_ids]
        
        clean_en_input = []
        en_start_i = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> i, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(en_input.ravel()):
            <span class="hljs-keyword">if</span> w==<span class="hljs-string">'&lt;pad&gt;'</span>: 
                en_start_i = i+<span class="hljs-number">1</span>
                <span class="hljs-keyword">continue</span>
                
            clean_en_input.append(w)
            <span class="hljs-keyword">if</span> w==<span class="hljs-string">'&lt;/s&gt;'</span>: <span class="hljs-keyword">break</span>
        clean_predicted_words = []
        <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> predicted_words:
            clean_predicted_words.append(w)
            <span class="hljs-keyword">if</span> w==<span class="hljs-string">'&lt;/s&gt;'</span>: <span class="hljs-keyword">break</span>
        
        results.append(
            {
                <span class="hljs-string">"</span><span class="hljs-string">attention_weights"</span>: attn_weights[
<span class="hljs-number">                0</span>,:<span class="hljs-built_in">len</span>(clean_predicted_words),en_start_i:en_start_
                i+<span class="hljs-built_in">len</span>(clean_en_input)
                ], 
                <span class="hljs-string">"input_words"</span>: clean_en_input,  
                <span class="hljs-string">"predicted_words"</span>: clean_predicted_words
            }
        )
        
    <span class="hljs-keyword">return</span> results
</code></pre>
    <p class="normal">This function<a id="_idIndexMarker949"/> does the following:</p>
    <ul>
      <li class="bulletList">Randomly samples <code class="inlineCode">n_samples</code> indices from the test data.</li>
      <li class="bulletList">For each random index:<ul>
          <li class="bulletList">Gets the inputs of the data point at that index (<code class="inlineCode">en_input</code> and <code class="inlineCode">de_input</code>)</li>
          <li class="bulletList">Gets the predicted words by feeding <code class="inlineCode">en_input</code> and <code class="inlineCode">de_input</code> to the <code class="inlineCode">attention_visualizer</code> (stored in <code class="inlineCode">predicted_words</code>)</li>
          <li class="bulletList">Cleans <code class="inlineCode">en_input</code> by removing any uninformative tokens (e.g. <code class="inlineCode">&lt;pad&gt;</code>) and assigns to <code class="inlineCode">clean_en_input</code></li>
          <li class="bulletList">Cleans <code class="inlineCode">predicted_words</code> by removing tokens after the <code class="inlineCode">&lt;/s&gt;</code> token (stored in <code class="inlineCode">clean_predicted_words</code>)</li>
          <li class="bulletList">Gets the attention weights only corresponding to the words left in the clean inputs and predicted words from <code class="inlineCode">attn_weights</code></li>
          <li class="bulletList">Appends the <code class="inlineCode">clean_en_input</code>, <code class="inlineCode">clean_predicted_words</code>, and attention weights matrix to results</li>
        </ul>
      </li>
    </ul>
    <p class="normal">The results contain all the information we need to visualize attention patterns. You can see the actual code used to create the following visualizations in the notebook <code class="inlineCode">Ch09-Seq2seq-Models/ch09_seq2seq.ipynb</code>. </p>
    <p class="normal">Let’s take a few samples from our test dataset and visualize attention patterns exhibited by the model (<em class="italic">Figure 9.16</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_16.png" alt=""/></figure>
    <figure class="mediaobject"><img src="../Images/B14070_09_16.1.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.16: Visualizing attention patterns for a few test inputs</p>
    <p class="normal">Overall, we’d like to see a heat map that has a roughly diagonal activation of energy. This is because both languages have a similar construct in terms of the direction of the language. And we can clearly see that in both examples. </p>
    <p class="normal">Looking at specific words<a id="_idIndexMarker950"/> in the first example, you can see the model focuses heavily on <em class="italic">evening</em> to predict <em class="italic">Abends</em>, <em class="italic">atmosphere</em> to predict <em class="italic">Ambiente</em>, and so on. In the second, you see that the model is focusing on the word <em class="italic">free</em> to predict <em class="italic">kostenlosen</em>, which is German for <em class="italic">free</em>.</p>
    <p class="normal">Next, we discuss how to infer translations from the trained model.</p>
    <h1 id="_idParaDest-249" class="heading-1">Inference with NMT</h1>
    <p class="normal">Inferencing<a id="_idIndexMarker951"/> is slightly different from the training process for NMT (<em class="italic">Figure 9.17</em>). As we do not have a target sentence at the inference time, we need a way to trigger the decoder at the end of the encoding phase. It’s not difficult as we have already done the groundwork for this in the data we have. We simply kick off the decoder by using <code class="inlineCode">&lt;s&gt;</code> as the first input to the decoder. Then we recursively call the decoder using the predicted word as the input for the next timestep. We continue this way until the model:</p>
    <ul>
      <li class="bulletList">Outputs <code class="inlineCode">&lt;/s&gt;</code> as the predicted token or</li>
      <li class="bulletList">Reaches a pre-defined sentence length</li>
    </ul>
    <p class="normal">To do this, we have to define<a id="_idIndexMarker952"/> a new model using the existing weights of the training model. This is because our trained model is designed to consume a sequence of decoder inputs at once. We need a mechanism to recursively call the decoder. Here’s how we can define the inference model:</p>
    <ul>
      <li class="bulletList">Define an encoder model that outputs the encoder’s hidden state sequence and the last encoder state.</li>
      <li class="bulletList">Define a new decoder that takes a decoder input having a time dimension of 1 and a new input, to which we will input the previous hidden state value of the decoder (initialized with the encoder’s last state).</li>
    </ul>
    <p class="normal">With that, we can start feeding data to generate predictions as follows:</p>
    <ul>
      <li class="bulletList">Preprocess <em class="italic">x</em><sub class="subscript">s</sub> as in data processing</li>
      <li class="bulletList">Feed <em class="italic">x</em><sub class="subscript">s</sub> into <img src="../Images/B14070_09_066.png" alt="" style="height: 1.15em !important; vertical-align: -0.20em !important;"/> and calculate the encoder’s state sequence and the last state <em class="italic">h</em> conditioned on <em class="italic">x</em><sub class="subscript">s</sub></li>
      <li class="bulletList">Initialize <img src="../Images/B14070_09_067.png" alt="" style="height: 1.15em !important; vertical-align: -0.22em !important;"/> with <em class="italic">h</em></li>
      <li class="bulletList">For the initial prediction step, predict <img src="../Images/B14070_09_068.png" alt="" style="height: 1.45em !important; vertical-align: -0.43em !important;"/> by conditioning the prediction on <img src="../Images/B14070_09_069.png" alt="" style="height: 1.45em !important; vertical-align: -0.13em !important;"/> as the first word and <em class="italic">h</em></li>
      <li class="bulletList">For subsequent time steps, while <img src="../Images/B14070_09_070.png" alt="" style="height: 1.57em !important; vertical-align: -0.19em !important;"/> and predictions haven’t reached a pre-defined length threshold, predict <img src="../Images/B14070_09_071.png" alt="" style="height: 1.45em !important; vertical-align: -0.14em !important;"/> by conditioning the prediction on <img src="../Images/B14070_09_072.png" alt="" style="height: 1.88em !important;"/> and <em class="italic">h</em></li>
    </ul>
    <p class="normal">This produces the translation given an input sequence of text:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.17: Inferring from an NMT</p>
    <p class="normal">The actual code can be found in the notebook <code class="inlineCode">Ch09-Seq2seq-Models/ch09_seq2seq.ipynb</code>. We will leave<a id="_idIndexMarker953"/> it for the reader to study the code and understand the implementation. We conclude our discussion about machine translation here. Now, let’s briefly examine another application of sequence-to-sequence learning.</p>
    <h1 id="_idParaDest-250" class="heading-1">Other applications of Seq2Seq models – chatbots</h1>
    <p class="normal">One other popular application<a id="_idIndexMarker954"/> of sequence-to-sequence models is in creating chatbots. A chatbot is a computer program that is able to have a realistic conversation with a human. Such applications are very useful for companies with a huge customer base. Responding to customers asking basic questions for which answers are obvious accounts for a significant portion of customer support requests. A chatbot <a id="_idIndexMarker955"/>can serve customers with basic concerns when it is able to find an answer. Also, if the chatbot is unable to answer a question, the request gets redirected to a human operator. Chatbots can save a lot of the time that human operators spend answering basic concerns and let them attend to more difficult tasks.</p>
    <h2 id="_idParaDest-251" class="heading-2">Training a chatbot</h2>
    <p class="normal">So, how can we use a sequence-to-sequence <a id="_idIndexMarker956"/>model to train a chatbot? The answer is quite straightforward as we have already learned about the machine translation model. The only difference would be how the source and target sentence pairs are formed.</p>
    <p class="normal">In the NMT system, the sentence<a id="_idIndexMarker957"/> pairs consist of a source sentence and the corresponding translation in a target language for that sentence. However, in training a chatbot, the data is extracted from the dialogue between two people. The source sentences would be the sentences/phrases uttered by person A, and the target sentences would be the replies to person A made by person B. One dataset that can be used for this purpose consists of movie dialogues between people and is found at <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html"><span class="url">https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html</span></a>.</p>
    <p class="normal">Here are links to several other datasets for training conversational chatbots:</p>
    <ul>
      <li class="bulletList">Reddit comments dataset: <a href="https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/"><span class="url">https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/</span></a></li>
      <li class="bulletList">Maluuba dialogue dataset: <a href="https://datasets.maluuba.com/Frames"><span class="url">https://datasets.maluuba.com/Frames</span></a></li>
      <li class="bulletList">Ubuntu dialogue corpus: <a href="http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/"><span class="url">http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/</span></a></li>
      <li class="bulletList">NIPS conversational intelligence challenge: <a href="http://convai.io/"><span class="url">http://convai.io/</span></a></li>
      <li class="bulletList">Microsoft Research social media text corpus: <a href="https://tinyurl.com/y7ha9rc5"><span class="url">https://tinyurl.com/y7ha9rc5</span></a></li>
    </ul>
    <p class="normal"><em class="italic">Figure 9.18</em> shows the similarity of a chatbot system to an NMT system. For example, we train a chatbot with a dataset consisting of dialogues between two people. The encoder takes in the sentences/phrases spoken by one person, where the decoder is trained to predict the other person’s response. After training in such a way, we can use the chatbot to provide a response to a given question:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_18.png" alt="Training a chatbot"/></figure>
    <p class="packt_figref">Figure 9.18: Illustration of a chatbot</p>
    <h2 id="_idParaDest-252" class="heading-2">Evaluating chatbots – the Turing test</h2>
    <p class="normal">After building<a id="_idIndexMarker958"/> a chatbot, one way to evaluate<a id="_idIndexMarker959"/> its effectiveness is using the Turing test. The Turing test was invented by Alan Turing in the 1950s as a way of measuring the intelligence of a machine. The experiment settings are well suited for evaluating chatbots. The experiment is set up as follows.</p>
    <p class="normal">There are three parties involved: an evaluator (that is, a human) (<strong class="keyWord">A</strong>), another human (<strong class="keyWord">B</strong>), and a machine (<strong class="keyWord">C</strong>). The three of them sit in three different rooms so that none of them can see the others. The only communication medium is text, which is typed into a computer by one party, and the receiver sees the text on a computer on their side. The evaluator communicates with both the human and the machine. And at the end of the conversation, the evaluator is to distinguish the machine from the human. If the evaluator cannot make the distinction, the machine is said to have passed the Turing test. This setup is illustrated in <em class="italic">Figure 9.19</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_09_19.png" alt="Evaluating chatbots – Turing test"/></figure>
    <p class="packt_figref">Figure 9.19: The Turing test</p>
    <p class="normal">This concludes the section on other applications of Seq2Seq models. We briefly discussed the application<a id="_idIndexMarker960"/> of creating chatbots, which is a popular use for sequential <a id="_idIndexMarker961"/>models.</p>
    <h1 id="_idParaDest-253" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we talked in detail about NMT systems. Machine translation is the task of translating a given text corpus from a source language to a target language. First, we talked about the history of machine translation briefly to build a sense of appreciation for what has gone into machine translation for it to become what it is today. We saw that today, the highest-performing machine translation systems are actually NMT systems. Next, we solved the NMT task of generating English to German translations. We talked about the dataset preprocessing that needs to be done, and extracting important statistics about the data (e.g. sequence lengths). We then talked about the fundamental concept of these systems and decomposed the model into the embedding layer, the encoder, the context vector, and the decoder. We also introduced techniques like teacher forcing and Bahdanau attention, which are aimed at improving model performance. Then we discussed how training and inference work in NMT systems. We also discussed a new metric called BLEU and how it is used to measure performance on sequence-to-sequence problems like machine translation.</p>
    <p class="normal">Finally, we briefly talked about another popular application of sequence-to-sequence learning: chatbots. Chatbots are machine learning applications that are able to have realistic conversations with a human and even answer questions. We saw that NMT systems and chatbots work similarly, and only the training data is different. We also discussed the Turing test, which is a qualitative test that can be used to evaluate chatbots.</p>
    <p class="normal">In the next chapter, we will look at a new type of model that came out in 2016 and is leading both the NLP and computer vision worlds: the Transformer.</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="https://packt.link/nlpgithub"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="https://packt.link/nlp"><span class="url">https://packt.link/nlp</span></a></p>
    <figure class="mediaobject"> <img src="../Images/QR_Code5143653472357468031.png" alt=""/></figure>
  </div>
</body></html>