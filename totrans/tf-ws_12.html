<html><head></head><body>
		<div id="_idContainer406" class="Content">
			<h1 id="_idParaDest-224"><a id="_idTextAnchor248"/>Appendix</h1>
		</div>
		<div id="_idContainer475" class="Content">
			<h1 id="_idParaDest-225"><a id="_idTextAnchor249"/>1. Introduction to Machine Learning with TensorFlow</h1>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor250"/>Activity 1.01: Performing Tensor Addition in TensorFlow</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li>Import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create two tensors with a rank <strong class="source-inline">0</strong> using TensorFlow's <strong class="source-inline">Variable</strong> class:<p class="source-code">var1 = tf.Variable(2706, tf.int32)</p><p class="source-code">var2 = tf.Variable(2386, tf.int32)</p></li>
				<li>Create a new variable to add the two scalars created and print the result:<p class="source-code">var_sum = var1 + var2</p><p class="source-code">var_sum.numpy()</p><p>This will result in the following output:</p><p class="source-code">5092</p><p>This output shows the total revenue for <strong class="source-inline">Product A</strong> at <strong class="source-inline">Location X</strong>.</p></li>
				<li>Create two tensors, a scalar of rank <strong class="source-inline">0</strong> and a vector of rank <strong class="source-inline">1</strong>, using TensorFlow's <strong class="source-inline">Variable</strong> class:<p class="source-code">scalar1 = tf.Variable(95, tf.int32)</p><p class="source-code">vector1 = tf.Variable([2706, 2799, 5102], \</p><p class="source-code">                      tf.int32)</p></li>
				<li>Create a new variable as the sum of the scalar and vector created and print the result:<p class="source-code">vector_scalar_sum = scalar1 + vector1</p><p class="source-code">vector_scalar_sum.numpy()</p><p>This will result in the following output:</p><p class="source-code">array([2801, 2894, 5197])</p><p>The result is the new sales goal for <strong class="source-inline">Salesperson 1</strong> at <strong class="source-inline">Location X</strong>.</p></li>
				<li>Now create three tensors with a rank of 2, representing the revenue for each product, salesperson, and location, using TensorFlow's <strong class="source-inline">Variable</strong> class:<p class="source-code">matrix1 = tf.Variable([[2706, 2799, 5102], \</p><p class="source-code">                       [2386, 4089, 5932]], tf.int32)</p><p class="source-code">matrix2 = tf.Variable([[5901, 1208, 645], \</p><p class="source-code">                       [6235, 1098, 948]], tf.int32)</p><p class="source-code">matrix3 = tf.Variable([[3908, 2339, 5520], \</p><p class="source-code">                       [4544, 1978, 4729]], tf.int32)</p></li>
				<li>Create a new variable as the sum of the three tensors created and print the result:<p class="source-code"><a id="_idTextAnchor251"/>matrix_sum = matrix1 + matrix2 + matrix3</p><p class="source-code">matrix_sum.numpy()</p><p>This will result in the following output:</p><div id="_idContainer407" class="IMG---Figure"><img src="image/B16341_01_42.jpg" alt="Figure 1.42: The output of the matrix summation as a NumPy variable&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.42: The output of the matrix summation as a NumPy variable</p>
			<p>The result represents the total revenue for each product at each location.</p>
			<p>In this activity, you performed addition on tensors with ranks <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, and <strong class="source-inline">2</strong>, and showed that scalars (tensors of rank 0) can be added to tensors of other ranks, known as scalar addition. </p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor252"/>Activity 1.02: Performing Tensor Reshaping and Transposition in TensorFlow</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a one-dimensional array with 24 elements using TensorFlow's <strong class="source-inline">Variable</strong> class. Verify the shape of the matrix:<p class="source-code">array1 = tf.Variable([*range(24)])</p><p class="source-code">array1.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[24]</p></li>
				<li>Reshape the matrix so that it has 12 rows and 2 columns using TensorFlow's <strong class="source-inline">reshape</strong> function. Verify the shape of the new matrix:<p class="source-code">reshape1 = tf.reshape(array1, shape=[12, 2])</p><p class="source-code">reshape1.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[12, 2]</p></li>
				<li>Reshape the matrix so that it has a shape of <strong class="source-inline">3x4x2</strong> using TensorFlow's <strong class="source-inline">reshape</strong> function. Verify the shape of the new matrix:<p class="source-code">reshape2 = tf.reshape(array1, shape=[3, 4, 2])</p><p class="source-code">reshape2.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[3, 4, 2]</p></li>
				<li>Verify that the rank of this new tensor is of rank <strong class="source-inline">3</strong> by using TensorFlow's <strong class="source-inline">rank</strong> function:<p class="source-code">tf.rank(reshape2).numpy()</p><p>This will result in the following output:</p><p class="source-code">3</p></li>
				<li>Transpose the tensor created in <em class="italic">step 3</em>. Verify the shape of the new tensor:<p class="source-code">transpose1 = tf.transpose(reshape1)</p><p class="source-code">transpose1.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[2, 12]</p></li>
			</ol>
			<p>In this activity, you have practiced performing tensor reshaping and transposition on tensors of various ranks and learned how to change the rank of a tensor by reshaping it. You simulated the grouping of 24 school children into class projects of varying sizes using TensorFlow's <strong class="source-inline">reshape</strong> and <strong class="source-inline">transpose</strong> functions.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor253"/>Activity 1.03: Applying Activation Functions </h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a <strong class="source-inline">3x4</strong> tensor as an input in which the rows represent the sales from various sales representatives, the columns represent various vehicles available at the dealership, and values represent the average percentage difference from the MSRP. The values can be positive or negative depending on whether the salesperson was able to sell for more or less than the MSRP:<p class="source-code">input1 = tf.Variable([[-0.013, 0.024, 0.06, 0.022], \</p><p class="source-code">                      [0.001, -0.047, 0.039, 0.016], \</p><p class="source-code">                      [0.018, 0.030, -0.021, -0.028]], \</p><p class="source-code">                     tf.float32)</p></li>
				<li>Create a <strong class="source-inline">4x1</strong> <strong class="source-inline">weights</strong> tensor with a shape of <strong class="source-inline">4x1</strong> representing the MSRP of the cars:<p class="source-code">weights = tf.Variable([[19995.95], [24995.50], \</p><p class="source-code">                       [36745.50], [29995.95]], \</p><p class="source-code">                      tf.float32)</p></li>
				<li>Create a bias tensor of size <strong class="source-inline">3x1</strong> representing the fixed costs associated with each salesperson:<p class="source-code">bias = tf.Variable([[-2500.0],[-2500.0],[-2500.0]], \</p><p class="source-code">                   tf.float32)</p></li>
				<li>Matrix multiply the input by the weight to show the average deviation from the MSRP on all cars and add the bias to subtract the fixed costs of the salesperson:<p class="source-code">output = tf.matmul(input1,weights) + bias</p><p class="source-code">output</p><p>The following is the output:</p><div id="_idContainer408" class="IMG---Figure"><img src="image/B16341_01_43.jpg" alt="Figure 1.43: The output of the matrix multiplication&#13;&#10;"/></div><p class="figure-caption">Figure 1.43: The output of the matrix multiplication</p></li>
				<li>Apply a ReLU activation function to highlight the net-positive salespeople:<p class="source-code">output = tf.keras.activations.relu(output)</p><p class="source-code">output </p><p>This will result in the following output:</p><div id="_idContainer409" class="IMG---Figure"><img src="image/B16341_01_44.jpg" alt="Figure 1.44: The output after applying the activation function&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.44: The output after applying the activation function</p>
			<p>This result shows the result of salespeople that had net-positive sales; those with net-negative sales are zeroed.</p>
			<p>In this activity, you performed tensor multiplication on tensors of various sizes, tensor addition, and also applied an activation function. You began by defining the tensors, followed by matrix multiplying two of them, then adding a bias tensor, and finally applying an activation function to the result.</p>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor254"/>2. Loading and Processing Data</h1>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor255"/>Activity 2.01: Loading Tabular Data and Rescaling Numerical Fields with a MinMax Scaler</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this activity. Save the file as <strong class="source-inline">Activity2-01.ipnyb</strong>.</li>
				<li>In a new Jupyter Notebook cell, import the pandas library, as follows: <p class="source-code">import pandas as pd</p></li>
				<li>Create a new pandas DataFrame named <strong class="source-inline">df</strong> and read the <strong class="source-inline">Bias_correction_ucl.csv</strong> file into it. Examine whether your data is properly loaded by printing the resultant DataFrame:<p class="source-code">df = pd.read_csv(<strong class="bold">'Bias_correction_ucl.csv'</strong>)</p><p class="callout-heading">Note </p><p class="callout">Make sure you change the path (highlighted) to the CSV file based on its location on your system. If you're running the Jupyter notebook from the same directory where the CSV file is stored, you can run the preceding code without any modification.</p></li>
				<li>Drop the <strong class="source-inline">date</strong> column using the <strong class="source-inline">drop</strong> method. Since you're dropping the columns, pass <strong class="source-inline">1</strong> to the <strong class="source-inline">axis</strong> argument and <strong class="source-inline">True</strong> to the <strong class="source-inline">inplace</strong> argument:<p class="source-code">df.drop('Date', inplace=True, axis=1)</p></li>
				<li>Plot a histogram of the <strong class="source-inline">Present_Tmax</strong> column that represents the maximum temperature across dates and weather stations across the dataset:<p class="source-code">ax = df['Present_Tmax'].hist(color='gray')</p><p class="source-code">ax.set_xlabel("Normalized Temperature")</p><p class="source-code">ax.set_ylabel("Frequency")</p><p>The output will be as follows:</p><div id="_idContainer410" class="IMG---Figure"><img src="image/B16341_02_20.jpg" alt="Figure 2.20: A Temperature versus Frequency histogram of the Present_Tmax column&#13;&#10;"/></div><p class="figure-caption">Figure 2.20: A Temperature versus Frequency histogram of the Present_Tmax column</p><p>The resultant histogram shows the distribution of values for the <strong class="source-inline">Present_Tmax</strong> column.</p></li>
				<li>Import <strong class="source-inline">MinMaxScaler</strong> and use it to fit and transform the feature DataFrame:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">scaler = MinMaxScaler()</p><p class="source-code">df2 = scaler.fit_transform(df)</p><p class="source-code">df2 = pd.DataFrame(df2, columns=df.columns)</p></li>
				<li>Plot a histogram of the transformed <strong class="source-inline">Present_Tmax</strong> column:<p class="source-code">ax = df2['Present_Tmax'].hist(color='gray')</p><p class="source-code">ax.set_xlabel("Normalized Temperature")</p><p class="source-code">ax.set_ylabel("Frequency")</p><p>The output will be as follows:</p><div id="_idContainer411" class="IMG---Figure"><img src="image/B16341_02_21.jpg" alt="Figure 2.21: A histogram of the rescaled Present_Tmax column&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 2.21: A histogram of the rescaled Present_Tmax column</p>
			<p>The resultant histogram shows that the temperature values range from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, as evidenced by the range on the <em class="italic">x</em> axis of the histogram. By using <strong class="source-inline">MinMaxScaler</strong>, the values will always have a minimum value of <strong class="source-inline">0</strong> and a maximum value of <strong class="source-inline">1</strong>.</p>
			<p>In this activity, you have performed some further preprocessing of the numerical fields. Here, you scaled the numerical fields so that they have a minimum value of <strong class="source-inline">0</strong> and a maximum value of <strong class="source-inline">1</strong>. This could be beneficial over the standard scaler if the numerical fields are not normally distributed. It also ensures the resulting fields are bound between a minimum and maximum value.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor256"/>Activity 2.02: Loading Image Data for Batch Processing</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this activity. Save the file as <strong class="source-inline">Activity2-02.ipnyb</strong>.</li>
				<li>In a new Jupyter Notebook cell, import the <strong class="source-inline">ImageDataGenerator</strong> class from Keras' preprocessing package:<p class="source-code">from tensorflow.keras.preprocessing.image \</p><p class="source-code">    import ImageDataGenerator</p></li>
				<li>Instantiate the <strong class="source-inline">ImageDataGenerator</strong> class and pass the <strong class="source-inline">rescale</strong> argument with a value of <strong class="source-inline">1/255</strong> to convert image values so that they're between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>:<p class="source-code">train_datagen = ImageDataGenerator(rescale = 1./255,\</p><p class="source-code">                                   shear_range = 0.2,\</p><p class="source-code">                                   rotation_range= 180,\</p><p class="source-code">                                   zoom_range = 0.2,\</p><p class="source-code">                                   horizontal_flip = True)</p></li>
				<li>Use the data generator's <strong class="source-inline">flow_from_directory</strong> method to direct the data generator to the image data. Pass in the arguments of the target size, the batch size, and the class mode:<p class="source-code">training_set = train_datagen.flow_from_directory\</p><p class="source-code">               ('image_data',\</p><p class="source-code">                target_size = (64, 64),\</p><p class="source-code">                batch_size = 25,\</p><p class="source-code">                class_mode = 'binary')</p></li>
				<li>Create a function to display the images in the batch:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">def show_batch(image_batch, label_batch):\</p><p class="source-code">    lookup = {v: k for k, v in </p><p class="source-code">        training_set.class_indices.items()}</p><p class="source-code">    label_batch = [lookup[label] for label in \</p><p class="source-code">                  label_batch]</p><p class="source-code">    plt.figure(figsize=(10,10))</p><p class="source-code">    for n in range(25):</p><p class="source-code">        ax = plt.subplot(5,5,n+1)</p><p class="source-code">        plt.imshow(image_batch[n])</p><p class="source-code">        plt.title(label_batch[n].title())</p><p class="source-code">        plt.axis('off')</p></li>
				<li>Take a batch from the data generator and pass it to the function to display the images and their labels:<p class="source-code">image_batch, label_batch = next(training_set)</p><p class="source-code">show_batch(image_batch, label_batch)</p><p>The output will be as follows:</p><div id="_idContainer412" class="IMG---Figure"><img src="image/B16341_02_22.jpg" alt="Figure 2.22: Augmented images from a batch&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 2.22: Augmented images from a batch</p>
			<p>he output shows a batch of 25 images and their respective labels that have been augmented by rotation, zooming, and shearing. The augmented images show the same objects but with different pixel values, which helps create more robust models.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor257"/>Activity 2.03: Loading Audio Data for Batch Processing</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this activity. Save the file as <strong class="source-inline">Activity2-03.ipnyb</strong>.</li>
				<li>In a new Jupyter Notebook cell, import the TensorFlow and <strong class="source-inline">os</strong> libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import os</p></li>
				<li>Create a function that will load and then return an audio file using TensorFlow's <strong class="source-inline">read_file</strong> function followed by the <strong class="source-inline">decode_wav</strong> function, respectively. Return the transpose of the resultant tensor:<p class="source-code">def load_audio(file_path, sample_rate=44100):</p><p class="source-code">    # Load audio at 44.1kHz sample-rate</p><p class="source-code">    audio = tf.io.read_file(file_path)</p><p class="source-code">    audio, sample_rate = tf.audio.decode_wav\</p><p class="source-code">                         (audio,\</p><p class="source-code">                          desired_channels=-1,\</p><p class="source-code">                          desired_samples=sample_rate)</p><p class="source-code">    return tf.transpose(audio)</p></li>
				<li>Load in the paths to the audio data as a list using <strong class="source-inline">os.list_dir</strong>:<p class="source-code">prefix = " ../Datasets/data_speech_commands_v0.02"\</p><p class="source-code">        "/zero/"</p><p class="source-code">paths = [os.path.join(prefix, path) for path in \</p><p class="source-code">         os.listdir(prefix)]</p></li>
				<li>Create a function that will take a dataset object, shuffle it, and load the audio using the function you created in <em class="italic">Step 2</em>. Then, apply the absolute value and the <strong class="source-inline">log1p</strong> function to the dataset. This function adds <strong class="source-inline">1</strong> to each value then takes the logarithm. Next, repeat the dataset object, batch it, and prefetch it with a buffer size equal to the batch size:<p class="source-code">def prep_ds(ds, shuffle_buffer_size=1024, \</p><p class="source-code">            batch_size=16):</p><p class="source-code">    # Randomly shuffle (file_path, label) dataset</p><p class="source-code">    ds = ds.shuffle(buffer_size=shuffle_buffer_size)</p><p class="source-code">    # Load and decode audio from file paths</p><p class="source-code">    ds = ds.map(load_audio)</p><p class="source-code">    # Take the absolute value</p><p class="source-code">    ds = ds.map(tf.abs)</p><p class="source-code">    # Apply log1p function</p><p class="source-code">    ds = ds.map(tf.math.log1p)</p><p class="source-code">    # Repeat dataset forever</p><p class="source-code">    ds = ds.repeat()</p><p class="source-code">    # Prepare batches</p><p class="source-code">    ds = ds.batch(batch_size)</p><p class="source-code">    # Prefetch</p><p class="source-code">    ds = ds.prefetch(buffer_size=batch_size)</p><p class="source-code">    return ds</p></li>
				<li>Create a dataset object using TensorFlow's <strong class="source-inline">from_tensor_slices</strong> function and pass in the paths to the audio files. Then, apply the function you created in <em class="italic">Step 5</em> to the dataset object:<p class="source-code">ds = tf.data.Dataset.from_tensor_slices(paths)</p><p class="source-code">train_ds = prep_ds(ds)</p></li>
				<li>Take the first batch of the dataset and print it out:<p class="source-code">for x in train_ds.take(1):\</p><p class="source-code">     print(x)</p><p>The output will look as follows:</p><div id="_idContainer413" class="IMG---Figure"><img src="image/B16341_02_23.jpg" alt="Figure 2.23: A batch of the audio data&#13;&#10;"/></div><p class="figure-caption">Figure 2.23: A batch of the audio data</p><p>The output shows the first batch of MFCC spectrum values in tensor form.</p></li>
				<li>Plot the first audio file from the batch:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">plt.plot(x[0,:,:].numpy().T, color = 'gray')</p><p class="source-code">plt.xlabel('Sample')</p><p class="source-code">plt.ylabel('Value'))</p><p>The output will look as follows:</p><div id="_idContainer414" class="IMG---Figure"><img src="image/B16341_02_24.jpg" alt="Figure 2.24: A visual representation of the batch of the preprocessed audio data&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 2.24: A visual representation of the batch of the preprocessed audio data</p>
			<p>The preceding plot shows the preprocessed audio data. You can see that the values are non-negative, with a minimum value of <strong class="source-inline">0</strong>, and that the data is logarithmically scaled.</p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor258"/>3. TensorFlow Development</h1>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor259"/>Activity 3.01: Using TensorBoard to Visualize Tensor Transformations</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import the TensorFlow library and set a seed:<p class="source-code">import tensorflow as tf</p><p class="source-code">tf.random.set_seed(42)</p></li>
				<li>Set the log directory and initialize a file writer object to write the trace:<p class="source-code">logdir = 'logs/'</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>Create a TensorFlow function to multiply two tensors and add a value of <strong class="source-inline">1</strong> to all elements in the resulting tensor using the <strong class="source-inline">ones_like</strong> function to create a tensor of the same shape as the result of the matrix multiplication. Then, apply a sigmoid function to each value of the tensor:<p class="source-code">@tf.function</p><p class="source-code">def my_func(x, y):</p><p class="source-code">    r1 = tf.matmul(x, y)</p><p class="source-code">    r2 = r1 + tf.ones_like(r1)</p><p class="source-code">    r3 = tf.keras.activations.sigmoid(r2)</p><p class="source-code">    return r3</p></li>
				<li>Create two tensors with the shape <strong class="source-inline">5x5x5</strong>:<p class="source-code">x = tf.random.uniform((5, 5, 5))</p><p class="source-code">y = tf.random.uniform((5, 5, 5))</p></li>
				<li>Turn on graph tracing:<p class="source-code">tf.summary.trace_on(graph=True, profiler=True)</p></li>
				<li>Apply the function to the two tensors and export the trace to the log directory:<p class="source-code">z = my_func(x, y)</p><p class="source-code">with writer.as_default():</p><p class="source-code">    tf.summary.trace_export(name="my_func_trace",\</p><p class="source-code">                            step=0,\</p><p class="source-code">                            profiler_outdir=logdir)</p></li>
				<li>Launch TensorBoard in the command line and view the graph in a browser:<p class="source-code">tensorboard --logdir=./logs</p><p>You should get something like the following image:</p><div id="_idContainer415" class="IMG---Figure"><img src="image/B16341_03_06.jpg" alt="Figure 3.19: A visual representation of tensor transformation in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.19: A visual representation of tensor transformation in TensorBoard</p>
			<p>The result represents the graph created for the tensor transformation. You can see in the bottom left at the beginning of the graph that a matrix multiplication is performed on the tensors named <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> on the node named <strong class="source-inline">MatMul</strong>. In the bottom right is the creation of the tensor using the <strong class="source-inline">ones_like</strong> function. The input nodes represent the shape of the tensor and the value, which is a constant value. Upon the creation of the two tensors, they are input into a node representing the addition function, after which the output is input to a node representing the application of the sigmoid function. The final nodes represent the creation of the output tensor.</p>
			<p>In this activity, you created functions for tensor transformation, and then presented a visual representation of the transformation in TensorBoard.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor260"/>Activity 3.02: Performing Word Embedding from a Pre-Trained Model from TensorFlow Hub</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import TensorFlow and TensorFlow Hub and print the version of the library:<p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">print('TF version: ', tf.__version__)</p><p class="source-code">print('HUB version: ', hub.__version__)</p><p>You should get the versions of TensorFlow and TensorFlow Hub.</p><div id="_idContainer416" class="IMG---Figure"><img src="image/B16341_03_20.jpg" alt="Figure 3.20: The output of the versions of TensorFlow and TensorFlow Hub in Google Colab&#13;&#10;"/></div><p class="figure-caption">Figure 3.20: The output of the versions of TensorFlow and TensorFlow Hub in Google Colab</p></li>
				<li>Set the handle for the module for the universal sentence encoder:<p class="source-code">module_handle ="<a href="https://tfhub.dev/google">https://tfhub.dev/google</a>"\</p><p class="source-code">               "/universal-sentence-encoder/4"</p></li>
				<li>Use the TensorFlow Hub <strong class="source-inline">KerasLayer</strong> class to create a hub layer, passing in the following arguments: <strong class="source-inline">module_handle</strong>, <strong class="source-inline">input_shape</strong>, and <strong class="source-inline">dtype</strong>:<p class="source-code">hub_layer = hub.KerasLayer(module_handle, input_shape=[],\ </p><p class="source-code">                           dtype=tf.string)</p></li>
				<li>Create a list containing a string to encode with the encoder:<p class="source-code">text = ['The TensorFlow Workshop']</p></li>
				<li>Apply <strong class="source-inline">hub_layer</strong> to the text to embed the sentence as a vector:<p class="source-code">hub_layer(text)</p><p>You should get the following output:</p><div id="_idContainer417" class="IMG---Figure"><img src="image/B16341_03_18.jpg" alt="Figure 3.21: The output of the embedding vector&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.21: The output of the embedding vector</p>
			<p>Here, you can see that the text has been converted to a 512-dimensional embedding vector. The embedding vector is a one-dimensional tensor that maps the text into a vector of continuous variables as shown in the preceding figure.</p>
			<p>In this activity, you used the Google Colab environment to download a model from TensorFlow Hub. You used a universal sentence encoder to embed a sentence into a 512-dimensional vector. This activity has shown that with a few short lines of code on powerful remote servers, you can access state-of-the-art machine learning models for any application.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor261"/>4. Regression and Classification Models</h1>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor262"/>Activity 4.01: Creating a Multi-Layer ANN with TensorFlow</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this activity.</li>
				<li>Import the TensorFlow and pandas libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p></li>
				<li>Load in the dataset using the pandas <strong class="source-inline">read_csv</strong> function:<p class="source-code">df = pd.read_csv(<strong class="bold">'superconductivity.csv'</strong>)</p><p class="callout-heading">Note </p><p class="callout">Make sure you change the path (highlighted) to the CSV file based on its location on your system. If you're running the Jupyter notebook from the same directory where the CSV file is stored, you can run the preceding code without any modification.   </p></li>
				<li>Drop the <strong class="source-inline">date</strong> column and drop any rows that have null values:<p class="source-code">df.dropna(inplace=True)</p></li>
				<li>Create target and feature datasets:<p class="source-code">target = df['critical_temp']</p><p class="source-code">features = df.drop('critical_temp', axis=1)</p></li>
				<li>Rescale the feature dataset:<p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">feature_array = scaler.fit_transform(features)</p><p class="source-code">features = pd.DataFrame(feature_array, columns=features.columns)</p></li>
				<li>Initialize a Keras model of the <strong class="source-inline">Sequential</strong> class:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Add an input layer to the model using the model's <strong class="source-inline">add</strong> method, and set <strong class="source-inline">input_shape</strong> to be the number of columns in the feature dataset. Add four hidden layers of sizes <strong class="source-inline">64</strong>, <strong class="source-inline">32</strong>, <strong class="source-inline">16</strong>, and <strong class="source-inline">8</strong> to the model with the first having a ReLU activation function, then add an output layer with one unit:<p class="source-code">model.add(tf.keras.layers.InputLayer\</p><p class="source-code">         (input_shape=features.shape[1],), \</p><p class="source-code">          name='Input_layer'))</p><p class="source-code">model.add(tf.keras.layers.Dense(64, activation='relu', \</p><p class="source-code">                                name='Dense_layer_1'))</p><p class="source-code">model.add(tf.keras.layers.Dense(32, name='Dense_layer_2'))</p><p class="source-code">model.add(tf.keras.layers.Dense(16, name='Dense_layer_3'))</p><p class="source-code">model.add(tf.keras.layers.Dense(8, name='Dense_layer_4'))</p><p class="source-code">model.add(tf.keras.layers.Dense(1, name='Output_layer'))</p></li>
				<li>Compile the model with an RMSprop optimizer with a learning rate equal to <strong class="source-inline">0.001</strong> and the mean squared error for the loss:<p class="source-code">model.compile(tf.optimizers.RMSprop(0.001), loss='mse')</p></li>
				<li>Create a TensorBoard callback:<p class="source-code">tensorboard_callback = tf.keras.callbacks\</p><p class="source-code">                         .TensorBoard(log_dir="./logs")</p></li>
				<li>Fit the model to the training data for <strong class="source-inline">100</strong> epochs, with a batch size equal to <strong class="source-inline">32</strong> and a validation split equal to 20%:<p class="source-code">model.fit(x=features.to_numpy(), y=target.to_numpy(), \</p><p class="source-code">          epochs=100, callbacks=[tensorboard_callback], \</p><p class="source-code">          batch_size=32, validation_split=0.2)</p><p>You should get the following output:</p><div id="_idContainer418" class="IMG---Figure"><img src="image/B16341_04_16.jpg" alt="Figure 4.16: The output of the fitting process showing the epoch, &#13;&#10;training time per sample, and loss after each epoch&#13;&#10;"/></div><p class="figure-caption">Figure 4.16: The output of the fitting process showing the epoch, training time per sample, and loss after each epoch</p></li>
				<li>Evaluate the model on the training data:<p class="source-code">loss = model.evaluate(features.to_numpy(), target.to_numpy())</p><p class="source-code">print('loss:', loss)</p><p>This will result in the following output:</p><p class="source-code">loss: 165.735601268987</p></li>
				<li>Visualize the model architecture and model-fitting process in TensorBoard by calling the following on the command line:<p class="source-code">tensorboard –-logdir=logs/</p><p>The model architecture should look like the following:</p><div id="_idContainer419" class="IMG---Figure"><img src="image/B16341_04_17.jpg" alt="Figure 4.17: A visual representation of the model architecture in TensorBoard&#13;&#10;"/></div><p class="figure-caption">Figure 4.17: A visual representation of the model architecture in TensorBoard</p></li>
				<li>Visualize the model-fitting process in TensorBoard. You should get the following output:<div id="_idContainer420" class="IMG---Figure"><img src="image/B16341_04_18.jpg" alt="Figure 4.18: A visual representation of the loss as a function of an epoch &#13;&#10;on the training and validation split in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.18: A visual representation of the loss as a function of an epoch on the training and validation split in TensorBoard</p>
			<p>During the model-fitting process, the loss on the training and validation sets is calculated after each epoch and displayed in TensorBoard in the <strong class="source-inline">SCALARS</strong> tab. From TensorBoard, you can see that the mean squared error reduces after each epoch consistently on the training set but plateaus on the validation set.</p>
			<p>In this activity, you have further practiced building models in TensorFlow and viewing its architecture and training process in TensorBoard. During this section, you have learned how to build, train, and evaluate ANNs using TensorFlow for regression tasks. You used Keras layers of the <strong class="source-inline">Dense</strong> class as an easy way to create fully connected layers that include activation functions on the output of the layers. The layers can be created simply by passing in the number of units desired in the layer. Keras configures the initialization of the weights and biases, as well as any other additional parameters that are common in a machine learning workflow.</p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor263"/>Activity 4.02: Creating a Multi-Layer Classification ANN with TensorFlow</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook to implement this activity.</li>
				<li>Import the TensorFlow and pandas libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p></li>
				<li>Load in the dataset using the pandas <strong class="source-inline">read_csv</strong> function:<p class="source-code">df = pd.read_csv(<strong class="bold">'superconductivity.csv'</strong>)</p><p class="callout-heading">Note </p><p class="callout">Make sure you change the path (highlighted) to the CSV file based on its location on your system. If you're running the Jupyter notebook from the same directory where the CSV file is stored, you can run the preceding code without any modification.   </p></li>
				<li>Drop any rows that have null values:<p class="source-code">df.dropna(inplace=True)</p></li>
				<li>Set the target values to <strong class="source-inline">true</strong> when values of the <strong class="source-inline">critical_temp</strong> column are above <strong class="source-inline">77.36</strong> and <strong class="source-inline">false</strong> when below. The feature dataset is the remaining columns in the dataset:<p class="source-code">target = df['critical_temp'].apply(lambda x: 1 if x&gt;77.36 else 0)</p><p class="source-code">features = df.drop('critical_temp', axis=1)</p></li>
				<li>Rescale the feature dataset:<p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">scaler = StandardScaler()</p><p class="source-code">feature_array = scaler.fit_transform(features)</p><p class="source-code">features = pd.DataFrame(feature_array, columns=features.columns)</p></li>
				<li>Initialize a Keras model of the <strong class="source-inline">Sequential</strong> class:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Add an input layer to the model using the model's <strong class="source-inline">add</strong> method and set <strong class="source-inline">input_shape</strong> to the number of columns in the feature dataset. Add three hidden layers of sizes <strong class="source-inline">32</strong>, <strong class="source-inline">16</strong>, and <strong class="source-inline">8</strong> to the model, then add an output layer with <strong class="source-inline">1</strong> unit and a sigmoid activation function:<p class="source-code">model.add(tf.keras.layers.InputLayer\</p><p class="source-code">         (input_shape=features.shape[1], \</p><p class="source-code">          name='Input_layer'))</p><p class="source-code">model.add(tf.keras.layers.Dense(32, name='Hidden_layer_1'))</p><p class="source-code">model.add(tf.keras.layers.Dense(16, name='Hidden_layer_2'))</p><p class="source-code">model.add(tf.keras.layers.Dense(8, name='Hidden_layer_3'))</p><p class="source-code">model.add(tf.keras.layers.Dense(1, name='Output_layer', \</p><p class="source-code">                                activation='sigmoid'))</p></li>
				<li>Compile the model with an RMSprop optimizer with a learning rate equal to <strong class="source-inline">0.0001</strong> and binary cross-entropy for the loss and compute the accuracy metric:<p class="source-code">model.compile(tf.optimizers.RMSprop(0.0001), \</p><p class="source-code">              loss= 'binary_crossentropy', metrics=['accuracy'])</p></li>
				<li>Create a TensorBoard callback:<p class="source-code">tensorboard_callback = tf.keras.callbacks.TensorBoard\</p><p class="source-code">                       (log_dir="./logs")</p></li>
				<li>Fit the model to the training data for <strong class="source-inline">50</strong> epochs and a validation split equal to 20%:<p class="source-code">model.fit(x=features.to_numpy(), y=target.to_numpy(),\</p><p class="source-code">          epochs=50, callbacks=[tensorboard_callback],\</p><p class="source-code">          validation_split=0.2)</p><p>You should get the following output:</p><div id="_idContainer421" class="IMG---Figure"><img src="image/B16341_04_19.jpg" alt="Figure 4.19: The output of the fitting process showing the epoch, training time per sample, loss, and accuracy after each epoch, and evaluated on the validation split&#13;&#10;"/></div><p class="figure-caption">Figure 4.19: The output of the fitting process showing the epoch, training time per sample, loss, and accuracy after each epoch, and evaluated on the validation split</p></li>
				<li>Evaluate the model on the training data:<p class="source-code">loss, accuracy = model.evaluate(features.to_numpy(), \</p><p class="source-code">                                target.to_numpy())</p><p class="source-code">print(f'loss: {loss}, accuracy: {accuracy}')</p><p>This will display the following output:</p><p class="source-code">loss: 0.21984571637242145, accuracy: 0.8893383145332336</p></li>
				<li>Visualize the model architecture and model-fitting process in TensorBoard by calling the following on the command line:<p class="source-code">tensorboard –-logdir=logs/</p><p>You should get a screen similar to the following in the browser:</p><div id="_idContainer422" class="IMG---Figure"><img src="image/B16341_04_20.jpg" alt="Figure 4.20: A visual representation of the model architecture in TensorBoard&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.20: A visual representation of the model architecture in TensorBoard</p>
			<p>The loss function can be visualized as follows:</p>
			<div>
				<div id="_idContainer423" class="IMG---Figure">
					<img src="image/B16341_04_21.jpg" alt="Figure 4.21: A visual representation of the accuracy and loss as a function of an epoch on the training and validation split in TensorBoard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.21: A visual representation of the accuracy and loss as a function of an epoch on the training and validation split in TensorBoard</p>
			<p>During the model-fitting process, the accuracy and loss on the training and validation sets are calculated after each epoch and displayed in TensorBoard in the <strong class="source-inline">SCALARS</strong> tab. From TensorBoard, you can see that the loss metric (binary cross-entropy) reduces after each epoch consistently on the training set but plateaus on the validation set.</p>
			<p>In this activity, you have practiced building classification models in TensorFlow by building a multi-layer ANN to determine whether a material will exhibit superconductivity above or below the boiling point of nitrogen. Moreover, you used TensorBoard to view the models' architecture and monitor key metrics during the training process, including the loss and the accuracy of the models.</p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor264"/>5. Classification Models</h1>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor265"/>Activity 5.01: Bu<a id="_idTextAnchor266"/>ilding a Character Recognition Model with TensorFlow</h2>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> that contains the URL to the dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">          '/The-TensorFlow-Workshop/master/Chapter05'\</p><p class="source-code">          '/dataset/letter-recognition.data'</p></li>
				<li>Load the dataset into a <strong class="source-inline">DataFrame()</strong> function called <strong class="source-inline">data</strong> using <strong class="source-inline">read_csv()</strong> method, provide the URL to the CSV file, and set <strong class="source-inline">header=None</strong> as the dataset doesn't provide column names. Print the first five rows using <strong class="source-inline">head()</strong> method.<p class="source-code">data = pd.read_csv(file_url, header=None)</p><p class="source-code">data.head()</p><p>The expected output will be as follows:</p><div id="_idContainer424" class="IMG---Figure"><img src="image/B16341_05_42.jpg" alt="Figure 5.42: First five rows of the data&#13;&#10;"/></div><p class="figure-caption">Figure 5.42: First five rows of the data</p><p>You can see that the dataset contains <strong class="source-inline">17</strong> columns and they are all numeric. Column <strong class="source-inline">0</strong> is the <strong class="source-inline">target</strong> variable, and each value corresponds to a letter of the alphabet.</p></li>
				<li>Extract the target variable (column <strong class="source-inline">0</strong>) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">target</strong>:<p class="source-code">target = data.pop(0)</p></li>
				<li>Split <strong class="source-inline">data</strong> into a training set by keeping the first 15,000 observations and save it in a variable called <strong class="source-inline">X_train</strong>. Perform the same split on <strong class="source-inline">target</strong> and save the first 15,000 cases in a variable called <strong class="source-inline">y_train</strong>:<p class="source-code">X_train = data[:15000]</p><p class="source-code">y_train = target[:15000]</p></li>
				<li>Split <strong class="source-inline">data</strong> into a test set by keeping the last 5,000 observations and save it in a variable called <strong class="source-inline">X_test</strong>. Perform the same split on <strong class="source-inline">target</strong> and save the last 5,000 cases in a variable called <strong class="source-inline">y_test</strong>:<p class="source-code">X_test = data[15000:]</p><p class="source-code">y_test = target[15000:]</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias:<p class="source-code">import tensorflow as tf</p></li>
				<li>Set the seed as <strong class="source-inline">8</strong> using <strong class="source-inline">tf.random.set_seed()</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong> and store it in a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Import the <strong class="source-inline">Dense()</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function and the input shape as <strong class="source-inline">(16,)</strong>, which corresponds to the number of features from the dataset. Save it in a variable called <strong class="source-inline">fc1</strong>:<p class="source-code">fc1 = Dense(512, input_shape=(16,), activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc2</strong>:<p class="source-code">fc2 = Dense(512, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc3</strong>:<p class="source-code">fc3 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc4</strong>:<p class="source-code">fc4 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">26</strong> units with <strong class="source-inline">Dense()</strong> and specify softmax as the activation function. Save it in a variable called <strong class="source-inline">fc5</strong>:<p class="source-code">fc5 = Dense(26, activation='softmax')</p></li>
				<li>Sequentially add all five fully connected layers to the model using <strong class="source-inline">add()</strong> method.<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(fc2)</p><p class="source-code">model.add(fc3)</p><p class="source-code">model.add(fc4)</p><p class="source-code">model.add(fc5)</p></li>
				<li>Print the summary of the model using <strong class="source-inline">summary()</strong> method.<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer425" class="IMG---Figure"><img src="image/B16341_05_43.jpg" alt="Figure 5.43: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 5.43: Summary of the model architecture</p><p>The preceding output shows that there are five layers in your model (as expected) and also tells you the number of parameters at each layer. </p></li>
				<li>Instantiate <strong class="source-inline">SparseCategoricalCrossentropy()</strong> from <strong class="source-inline">tf.keras.losses</strong> and save it in a variable called <strong class="source-inline">loss</strong>:<p class="source-code">loss = tf.keras.losses.SparseCategoricalCrossentropy()</p></li>
				<li>Instantiate <strong class="source-inline">Adam()</strong> from <strong class="source-inline">tf.keras.optimizers</strong> with <strong class="source-inline">0.001</strong> as the learning rate and save it in a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the model using <strong class="source-inline">compile()</strong> method, specify the optimizer and loss parameters you just created, and use accuracy as the metric to be reported:<p class="source-code">model.compile(optimizer=optimizer, loss=loss, \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Start the model training process using <strong class="source-inline">fit()</strong> method on the training set for five epochs:<p class="source-code">model.fit(X_train, y_train, epochs=5)</p><p>The expected output will be as follows:</p><div id="_idContainer426" class="IMG---Figure"><img src="image/B16341_05_44.jpg" alt="Figure 5.44: Logs of the training process&#13;&#10;"/></div><p class="figure-caption">Figure 5.44: Logs of the training process</p><p>The preceding output shows the logs of each epoch during the training of the model. Note that it took around 2 seconds to process a single epoch, and the accuracy score increased from <strong class="source-inline">0.6229</strong> (first epoch) to <strong class="source-inline">0.9011</strong> (fifth epoch).</p></li>
				<li>Evaluate the performance of the model on the test set using <strong class="source-inline">evaluate()</strong> method.<p class="source-code">model.evaluate(X_test, y_test)</p><p>The expected output will be as follows:</p><div id="_idContainer427" class="IMG---Figure"><img src="image/B16341_05_45.jpg" alt="Figure 5.45: Performance of the model on the test set&#13;&#10;"/></div><p class="figure-caption">Figure 5.45: Performance of the model on the test set</p></li>
				<li>Predict the probabilities for each class on the test set using <strong class="source-inline">predict()</strong> method. Save it in a variable called <strong class="source-inline">preds_proba</strong>:<p class="source-code">preds_proba = model.predict(X_test)</p></li>
				<li>Convert the class probabilities into a single predicted value using <strong class="source-inline">argmax()</strong> method with <strong class="source-inline">axis=1</strong>:<p class="source-code">preds = preds_proba.argmax(axis=1)</p></li>
				<li>Import <strong class="source-inline">confusion_matrix</strong> from <strong class="source-inline">tensorflow.math</strong>:<p class="source-code">from tensorflow.math import confusion_matrix</p></li>
				<li>Print the confusion matrix on the test set:<p class="source-code">confusion_matrix(y_test, preds)</p><p>The expected output will be as follows:</p><div id="_idContainer428" class="IMG---Figure"><img src="image/B16341_05_39.jpg" alt="Figure 5.46: Confusion matrix of the test set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.46: Confusion matrix of the test set</p>
			<p>The preceding output shows the model is correctly predicting the 26 letters of the alphabet most of the time (most of the values are located on the diagonal). It achieved an accuracy score of around 0.89 for both the training and test sets. This activity concludes the section on multi-class classification. In the section ahead, you will look at another type of classification called multi-label.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor267"/>Activity 5.02: Building a Movie Genre Tagging a Model with TensorFlow</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable called <strong class="source-inline">feature_url</strong> that contains the URL to the dataset:<p class="source-code">feature_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">              '/PacktWorkshops'/The-TensorFlow-Workshop'\</p><p class="source-code">              '/master/Chapter05'/dataset/IMDB-F-features.csv'</p></li>
				<li>Load the dataset into a DataFrame called <strong class="source-inline">feature</strong> using <strong class="source-inline">read_csv()</strong> method and provide the URL to the CSV file. Print the first five rows using the <strong class="source-inline">head()</strong> method:<p class="source-code">feature = pd.read_csv(feature_url)</p><p class="source-code">feature.head()</p><p>The expected output will be as follows:</p><div id="_idContainer429" class="IMG---Figure"><img src="image/B16341_05_47.jpg" alt="Figure 5.47: The first five rows of the features&#13;&#10;"/></div><p class="figure-caption">Figure 5.47: The first five rows of the features</p></li>
				<li>Create a variable called <strong class="source-inline">target_url</strong> that contains the URL to the dataset:<p class="source-code">target_url = 'https://raw.githubusercontent.com'\</p><p class="source-code">             '/PacktWorkshops/The-TensorFlow-Workshop'\</p><p class="source-code">             '/master/Chapter05'/dataset/IMDB-F-targets.csv'</p></li>
				<li>Load the dataset into a DataFrame called <strong class="source-inline">target</strong> using <strong class="source-inline">read_csv()</strong> method and provide the URL to the CSV file. Print the first five rows using the <strong class="source-inline">head()</strong> method:<p class="source-code">target = pd.read_csv(target_url)</p><p class="source-code">target.head()</p><p>The expected output will be as follows:</p><div id="_idContainer430" class="IMG---Figure"><img src="image/B16341_05_48.jpg" alt="Figure 5.48: The first five rows of the targets&#13;&#10;"/></div><p class="figure-caption">Figure 5.48: The first five rows of the targets</p></li>
				<li>Split the data into a training set by keeping the first 15,000 observations and save it in a variable called <strong class="source-inline">X_train</strong>. Perform the same split on <strong class="source-inline">target</strong> and save the first 15,000 cases in a variable called <strong class="source-inline">y_train</strong>:<p class="source-code">X_train = feature[:15000]</p><p class="source-code">y_train = target[:15000]</p></li>
				<li>Split the data into a test set by keeping the last 5,000 observations and save it in a variable called <strong class="source-inline">X_test</strong>. Perform the same split on <strong class="source-inline">target</strong> and save the last 5,000 cases in a variable called <strong class="source-inline">y_test</strong>:<p class="source-code">X_test = feature[15000:]</p><p class="source-code">y_test = target[15000:]</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias:<p class="source-code">import tensorflow as tf</p></li>
				<li>Set the seed for <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">8</strong> using <strong class="source-inline">tf.random.set_seed()</strong>. This will help to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong> and store it in a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Import the <strong class="source-inline">Dense()</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function and the input shape as <strong class="source-inline">(1001,)</strong> which corresponds to the number of features from the dataset. Save it in a variable called <strong class="source-inline">fc1</strong>:<p class="source-code">fc1 = Dense(512, input_shape=(1001,), activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc2</strong>:<p class="source-code">fc2 = Dense(512, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc3</strong>:<p class="source-code">fc3 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save it in a variable called <strong class="source-inline">fc4</strong>:<p class="source-code">fc4 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">28</strong> units with <strong class="source-inline">Dense()</strong> and specify sigmoid as the activation function. Save it in a variable called <strong class="source-inline">fc5</strong>:<p class="source-code">fc5 = Dense(28, activation='sigmoid')</p></li>
				<li>Sequentially add all five fully connected layers to the model using <strong class="source-inline">add()</strong> method.<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(fc2)</p><p class="source-code">model.add(fc3)</p><p class="source-code">model.add(fc4)</p><p class="source-code">model.add(fc5)</p></li>
				<li>Print the summary of the model using <strong class="source-inline">summary()</strong> method.<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer431" class="IMG---Figure"><img src="image/B16341_05_49.jpg" alt="Figure 5.49: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 5.49: Summary of the model architecture</p></li>
				<li>Instantiate <strong class="source-inline">BinaryCrossentropy()</strong> from <strong class="source-inline">tf.keras.losses</strong> and save it in a variable called <strong class="source-inline">loss</strong>:<p class="source-code">loss = tf.keras.losses.BinaryCrossentropy()</p></li>
				<li>Instantiate <strong class="source-inline">Adam()</strong> from <strong class="source-inline">tf.keras.optimizers</strong> with <strong class="source-inline">0.001</strong> as the learning rate and save it in a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the model using <strong class="source-inline">compile()</strong> method and specify the optimizer and loss parameters that were just created, with accuracy as the metric to be reported:<p class="source-code">model.compile(optimizer=optimizer, loss=loss, \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method on the training set for <strong class="source-inline">20</strong> epochs:<p class="source-code">model.fit(X_train, y_train, epochs=20)</p><p>The expected output will be as follows:</p><div id="_idContainer432" class="IMG---Figure"><img src="image/B16341_05_50.jpg" alt="Figure 5.50: Logs of the training process&#13;&#10;"/></div><p class="figure-caption">Figure 5.50: Logs of the training process</p><p>You can observe that the model is trained for 20 epochs and that accuracy is improving, achieving <strong class="source-inline">61.67%</strong> after the ninth epoch.</p></li>
				<li>Evaluate the performance of the model on the test set using the <strong class="source-inline">evaluate()</strong> method:<p class="source-code">model.evaluate(X_test, y_test)</p><p>The expected output will be as follows:</p><div id="_idContainer433" class="IMG---Figure"><img src="image/B16341_05_51.jpg" alt="Figure 5.51: Performance of the model on the test set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.51: Performance of the model on the test set</p>
			<p>The preceding output shows the model achieved an accuracy score of <strong class="source-inline">0.13</strong> on the test set, which is extremely low, while it got an accuracy of <strong class="source-inline">0.62</strong> on the training set. This model is struggling to learn the relevant pattern to correctly predict the different genres of movies. You could try different architectures with different numbers of hidden layers and units on your own. You can also try different learning rates and optimizers. As the scores are very different on the training and test sets, the model is overfitting and has simply learned patterns relevant to just the training set. </p>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor268"/>6. Regularization and Hyperparameter Tuning</h1>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor269"/>Activity 6.01: Predicting Income with L1 and L2 Regularizers</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a list called <strong class="source-inline">usecols</strong> containing the column names <strong class="source-inline">AAGE</strong>, <strong class="source-inline">ADTIND</strong>, <strong class="source-inline">ADTOCC</strong>, <strong class="source-inline">SEOTR</strong>, <strong class="source-inline">WKSWORK</strong>, and <strong class="source-inline">PTOTVAL</strong>:<p class="source-code">usecols = ['AAGE','ADTIND','ADTOCC','SEOTR','WKSWORK', 'PTOTVAL']</p></li>
				<li>Create a variable called <strong class="source-inline">train_url</strong> that contains the URL to the training set:<p class="source-code">train_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">            '/The-TensorFlow-Workshop/master/Chapter06'\</p><p class="source-code">            '/dataset/census-income-train.csv'</p></li>
				<li>Load the training dataset into a DataFrame, <strong class="source-inline">train_data</strong>, using the <strong class="source-inline">read_csv()</strong> method. Provide the URL to the CSV file and the <strong class="source-inline">usecols</strong> list to the <strong class="source-inline">usecols</strong> parameter. Print the first five rows using the <strong class="source-inline">head()</strong> method:<p class="source-code">train_data = pd.read_csv(train_url, usecols=usecols)</p><p class="source-code">train_data.head()</p><p>The expected output will be as follows:</p><div id="_idContainer434" class="IMG---Figure"><img src="image/B16341_06_23.jpg" alt="Figure 6.23: First five rows of the training set&#13;&#10;"/></div><p class="figure-caption">Figure 6.23: First five rows of the training set</p></li>
				<li>Extract the target variable (<strong class="source-inline">PTOTVAL</strong>) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">train_target</strong>:<p class="source-code">train_target = train_data.pop('PTOTVAL')</p></li>
				<li>Create a variable called <strong class="source-inline">test_url</strong> that contains the URL to the test set:<p class="source-code">test_url = 'https://github.com/PacktWorkshops'\</p><p class="source-code">           '/The-TensorFlow-Workshop/blob/master/Chapter06'\</p><p class="source-code">           '/dataset/census-income-test.csv?raw=true'</p></li>
				<li>Load the test dataset into a DataFrame, <strong class="source-inline">X_test</strong>, using the <strong class="source-inline">read_csv()</strong> method. Provide the URL to the CSV file and the <strong class="source-inline">usecols</strong> list to the <strong class="source-inline">usecols</strong> parameter. Print the first five rows using the <strong class="source-inline">head()</strong> method:<p class="source-code">test_data = pd.read_csv(test_url, usecols=usecols)</p><p class="source-code">test_data.head()</p><p>The expected output will be as follows:</p><div id="_idContainer435" class="IMG---Figure"><img src="image/B16341_06_24.jpg" alt="Figure 6.24: First five rows of the test set&#13;&#10;"/></div><p class="figure-caption">Figure 6.24: First five rows of the test set</p></li>
				<li>Extract the target variable (<strong class="source-inline">PTOTVAL</strong>) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">test_target</strong>:<p class="source-code">test_target = test_data.pop('PTOTVAL')</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias. Then, import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Set the seed as <strong class="source-inline">8</strong> using <strong class="source-inline">tf.random.set_seed()</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong> and store it in a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">1048</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function and the input shape as <strong class="source-inline">(5,)</strong>, which corresponds to the number of features from the dataset. Save it in a variable called <strong class="source-inline">fc1</strong>:<p class="source-code">fc1 = Dense(1048, input_shape=(5,), activation='relu')</p></li>
				<li>Create three fully connected layers of <strong class="source-inline">512</strong>, <strong class="source-inline">128</strong>, and <strong class="source-inline">64</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save them in three variables, called <strong class="source-inline">fc2</strong>, <strong class="source-inline">fc3</strong>, and <strong class="source-inline">fc4</strong>, respectively:<p class="source-code">fc2 = Dense(512, activation='relu')</p><p class="source-code">fc3 = Dense(128, activation='relu')</p><p class="source-code">fc4 = Dense(64, activation='relu')</p></li>
				<li>Create a fully connected layer of three units (corresponding to the number of classes) with <strong class="source-inline">Dense()</strong> and specify softmax as the activation function. Save it in a variable called <strong class="source-inline">fc5</strong>:<p class="source-code">fc5 = Dense(3, activation='softmax')</p></li>
				<li>Create a fully connected layer of a single unit with <strong class="source-inline">Dense()</strong>. Save it in a variable called <strong class="source-inline">fc5</strong>:<p class="source-code">fc5 = Dense(1)</p></li>
				<li>Sequentially add all five fully connected layers to the model using the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(fc2)</p><p class="source-code">model.add(fc3)</p><p class="source-code">model.add(fc4)</p><p class="source-code">model.add(fc5)</p></li>
				<li>Print the summary of the model:<p class="source-code">model.summary()</p><p>You will get the following output:</p><div id="_idContainer436" class="IMG---Figure"><img src="image/B16341_06_25.jpg" alt="Figure 6.25: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 6.25: Summary of the model architecture</p></li>
				<li>Instantiate <strong class="source-inline">Adam()</strong> from <strong class="source-inline">tf.keras.optimizers</strong> with <strong class="source-inline">0.05</strong> as the learning rate and save it in a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.05)</p></li>
				<li>Compile the model, specify the optimizer, and set <strong class="source-inline">mse</strong> as the loss and metric to be displayed:<p class="source-code">model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method for five epochs and split the data into a validation set with 20% of the data:<p class="source-code">model.fit(train_data, train_target, epochs=5, \</p><p class="source-code">          validation_split=0.2)</p><p>The expected output will be as follows:</p><div id="_idContainer437" class="IMG---Figure"><img src="image/B16341_06_26.jpg" alt="Figure 6.26: Logs of the training process&#13;&#10;"/></div><p class="figure-caption">Figure 6.26: Logs of the training process</p><p>The preceding output shows the model is overfitting. It achieved an MSE score of <strong class="source-inline">1005740</strong> on the training set and only <strong class="source-inline">1070237</strong> on the validation set. Now, train another model with L1 and L2 regularization.</p></li>
				<li>Create five fully connected layers similar to the previous models and specify both L1 and L2 regularizers for the <strong class="source-inline">kernel_regularizer</strong> parameters. Use the value <strong class="source-inline">0.001</strong> for the regularizer factor. Save them into five variables, called <strong class="source-inline">reg_fc1</strong>, <strong class="source-inline">reg_fc2</strong>, <strong class="source-inline">reg_fc3</strong>, <strong class="source-inline">reg_fc4</strong>, and <strong class="source-inline">reg_fc5</strong>:<p class="source-code">reg_fc1 = Dense(1048, input_shape=(5,), activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l1_l2(l1=0.001, l2=0.001))</p><p class="source-code">reg_fc2 = Dense(512, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l1_l2(l1=0.001, l2=0.001))</p><p class="source-code">reg_fc3 = Dense(128, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l1_l2(l1=0.001, l2=0.001))</p><p class="source-code">reg_fc4 = Dense(64, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l1_l2(l1=0.001, l2=0.001))</p><p class="source-code">reg_fc5 = Dense(1, activation='relu')</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong>, store it in a variable called <strong class="source-inline">model2</strong>, and add all five fully connected layers sequentially to the model using the <strong class="source-inline">add()</strong> method:<p class="source-code">model2 = tf.keras.Sequential()</p><p class="source-code">model2.add(reg_fc1)</p><p class="source-code">model2.add(reg_fc2)</p><p class="source-code">model2.add(reg_fc3)</p><p class="source-code">model2.add(reg_fc4)</p><p class="source-code">model2.add(reg_fc5)</p></li>
				<li>Print the summary of the model:<p class="source-code">model2.summary()</p><p>The output will be as follows:</p><div id="_idContainer438" class="IMG---Figure"><img src="image/B16341_06_27.jpg" alt="Figure 6.27: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 6.27: Summary of the model architecture</p></li>
				<li>Compile the model using the <strong class="source-inline">compile()</strong> method, specify the optimizer, and set <strong class="source-inline">mse</strong> as the loss and metric to be displayed:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.1)</p><p class="source-code">model2.compile(optimizer=optimizer, loss='mse', metrics=['mse'])</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method for five epochs and split the data into a validation set with 20% of the data:<p class="source-code">model2.fit(train_data, train_target, epochs=5, \</p><p class="source-code">           validation_split=0.2)</p><p>The output will be as follows:</p><div id="_idContainer439" class="IMG---Figure"><img src="image/B16341_06_28.jpg" alt="Figure 6.28: Logs of the training process&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.28: Logs of the training process</p>
			<p>With the addition of L1 and L2 regularization, the model has similar accuracy scores between the training (<strong class="source-inline">4028182</strong>) and test (<strong class="source-inline">3970020</strong>) sets. Therefore, the model is not overfitting much.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor270"/>Activity 6.02: Predicting Income with Bayesian Optimization from Keras Tuner</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a list called <strong class="source-inline">usecols</strong> containing the following column names: <strong class="source-inline">AAGE</strong>, <strong class="source-inline">ADTIND</strong>, <strong class="source-inline">ADTOCC</strong>, <strong class="source-inline">SEOTR</strong>, <strong class="source-inline">WKSWORK</strong>, and <strong class="source-inline">PTOTVAL</strong>:<p class="source-code">usecols = ['AAGE','ADTIND','ADTOCC','SEOTR','WKSWORK', 'PTOTVAL']</p></li>
				<li>Create a variable called <strong class="source-inline">train_url</strong> that contains the URL to the training set:<p class="source-code">train_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">            '/The-TensorFlow-Workshop/master/Chapter06'\</p><p class="source-code">            '/dataset/census-income-train.csv'</p></li>
				<li>Load the training dataset into a DataFrame called <strong class="source-inline">train_data</strong> using the <strong class="source-inline">read_csv()</strong> method, and provide the URL to the CSV file and the <strong class="source-inline">usecols</strong> list to the <strong class="source-inline">usecols</strong> parameter. Print the first five rows using the <strong class="source-inline">head()</strong> method:<p class="source-code">train_data = pd.read_csv(train_url, usecols=usecols)</p><p class="source-code">train_data.head()</p></li>
				<li>You will get the following output:<div id="_idContainer440" class="IMG---Figure"><img src="image/B16341_06_29.jpg" alt="Figure 6.29: First five rows of the training set&#13;&#10;"/></div><p class="figure-caption">Figure 6.29: First five rows of the training set</p></li>
				<li>Extract the target variable (<strong class="source-inline">PTOTVAL</strong>) using the <strong class="source-inline">pop()</strong> method, and save it in a variable called <strong class="source-inline">train_target</strong>:<p class="source-code">train_target = train_data.pop('PTOTVAL')</p></li>
				<li>Create a variable called <strong class="source-inline">test_url</strong> that contains the URL to the test set:<p class="source-code">test_url = 'https://github.com/PacktWorkshops'\</p><p class="source-code">           '/The-TensorFlow-Workshop/blob/master/Chapter06'\</p><p class="source-code">           '/dataset/census-income-test.csv?raw=true'</p></li>
				<li>Load the test dataset into a DataFrame called <strong class="source-inline">X_test</strong> using the <strong class="source-inline">read_csv()</strong> method and provide the URL to the CSV file and the <strong class="source-inline">usecols</strong> list to the <strong class="source-inline">usecols</strong> parameter. Print the first five rows using the <strong class="source-inline">head()</strong> method:<p class="source-code">test_data = pd.read_csv(test_url, usecols=usecols)</p><p class="source-code">test_data.head()</p><p>The output will be the following:</p><div id="_idContainer441" class="IMG---Figure"><img src="image/B16341_06_30.jpg" alt="Figure 6.30: First five rows of the test set&#13;&#10;"/></div><p class="figure-caption">Figure 6.30: First five rows of the test set</p></li>
				<li>Extract the target variable (<strong class="source-inline">PTOTVAL</strong>) using the <strong class="source-inline">pop()</strong> method, and save it in a variable called <strong class="source-inline">test_target</strong>:<p class="source-code">test_target = test_data.pop('PTOTVAL')</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias. Then, import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Set the seed as <strong class="source-inline">8</strong> using <strong class="source-inline">tf.random.set_seed()</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Define a function called <strong class="source-inline">model_builder</strong> to create a sequential model with the same architecture as <em class="italic">Activity 6.01</em>, <em class="italic">Predicting Income with L1 and L2 Regularizers</em>. But this time, provide a hyperparameter, <strong class="source-inline">hp.Choice</strong>, for the learning rate, <strong class="source-inline">hp.Int</strong> for the number of units for the input layer, and <strong class="source-inline">hp.Choice</strong> for L2 regularization:<p class="source-code">def model_builder(hp):</p><p class="source-code">model = tf.keras.Sequential()</p><p class="source-code">hp_l2 = hp.Choice('l2', values = [0.1, 0.01, 0.001])</p><p class="source-code">hp_units = hp.Int('units', min_value=128, max_value=512, step=64)</p><p class="source-code">reg_fc1 = Dense(hp_units, input_shape=(5,), activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l2(l=hp_l2))</p><p class="source-code">reg_fc2 = Dense(512, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l2(l=hp_l2))</p><p class="source-code">reg_fc3 = Dense(128, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l2(l=hp_l2))</p><p class="source-code">reg_fc4 = Dense(128, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l2(l=hp_l2))</p><p class="source-code">reg_fc5 = Dense(1)</p><p class="source-code">model.add(reg_fc1)</p><p class="source-code">model.add(reg_fc2)</p><p class="source-code">model.add(reg_fc3)</p><p class="source-code">model.add(reg_fc4)</p><p class="source-code">model.add(reg_fc5)</p><p class="source-code">hp_learning_rate = hp.Choice('learning_rate', \</p><p class="source-code">                             values = [0.01, 0.001])</p><p class="source-code">optimizer = tf.keras.optimizers.Adam(hp_learning_rate)</p><p class="source-code">model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])</p><p class="source-code">return model</p></li>
				<li>Install the <strong class="source-inline">keras-tuner</strong> package and then import it and assign it the <strong class="source-inline">kt</strong> alias: <p class="source-code">!pip install keras-tuner</p><p class="source-code">import kerastuner as kt</p></li>
				<li>Instantiate a <strong class="source-inline">BayesianOptimization</strong> tuner, and assign <strong class="source-inline">val_mse</strong> to <strong class="source-inline">objective</strong> and <strong class="source-inline">10</strong> to <strong class="source-inline">max_trials</strong>:<p class="source-code">tuner = kt.BayesianOptimization(model_builder, \</p><p class="source-code">                                objective = 'val_mse', \</p><p class="source-code">                                max_trials = 10)</p></li>
				<li>Launch the hyperparameter search with <strong class="source-inline">search()</strong> on the training and test sets:<p class="source-code">tuner.search(train_data, train_target, \</p><p class="source-code">             validation_data=(test_data, test_target))</p></li>
				<li>Extract the best hyperparameter combination (index <strong class="source-inline">0</strong>) with <strong class="source-inline">get_best_hyperparameters()</strong> and save it in a variable called <strong class="source-inline">best_hps</strong>:<p class="source-code">best_hps = tuner.get_best_hyperparameters()[0]</p></li>
				<li>Extract the best value for the number of units for the input layer, save it in a variable called <strong class="source-inline">best_units</strong>, and print its value:<p class="source-code">best_units = best_hps.get('units')</p><p class="source-code">best_units</p><p>You will get the following output:</p><p class="source-code">128</p><p>The best value for the number of units of the input layer found by Hyperband is <strong class="source-inline">128</strong>.</p></li>
				<li>Extract the best value for the learning rate, save it in a variable called <strong class="source-inline">best_lr</strong>, and print its value:<p class="source-code">best_lr = best_hps.get('learning_rate')</p><p class="source-code">best_lr</p><p>The best value for the learning rate hyperparameter found by Hyperband is <strong class="source-inline">0.001</strong>:</p><p class="source-code">0.001</p></li>
				<li>Extract the best value for the L2 regularization, save it in a variable called <strong class="source-inline">best_l2</strong>, and print its value:<p class="source-code">best_l2 = best_hps.get('l2')</p><p class="source-code">best_l2</p></li>
				<li>The best value for the learning rate hyperparameter found by Hyperband is <strong class="source-inline">0.001</strong>:<p class="source-code">0.001</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method for five epochs and use the test set for <strong class="source-inline">validation_data</strong>:<p class="source-code">model = tuner.hypermodel.build(best_hps)</p><p class="source-code">model.fit(X_train, y_train, epochs=5, \</p><p class="source-code">          validation_data=(X_test, y_test))</p><p>You should get an output similar to the following:</p><div id="_idContainer442" class="IMG---Figure"><img src="image/B16341_06_22.jpg" alt="Figure 6.31: Logs of the training process&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.31: Logs of the training process</p>
			<p>With Bayesian optimization, you found the best combination of hyperparameters for the number of units for the input layer (<strong class="source-inline">128</strong>), learning rate (<strong class="source-inline">0.001</strong>), and L2 regularization (<strong class="source-inline">0.001</strong>). With these hyperparameters, the final model achieved an MSE score of <strong class="source-inline">994174</strong> on the training set and <strong class="source-inline">989335</strong> on the test set. This is a great improvement from <em class="italic">Activity 6.01</em>, <em class="italic">Predicting Income with L1 and L2 Regularizers</em>, and the model is not overfitting much.</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor271"/>7. Convolutional Neural Networks</h1>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor272"/>Activity 7.01: Building a CNN with More ANN Layers</h2>
			<p><strong class="bold">Solution:</strong></p>
			<p>There are several possible ways to arrive at a solution for this activity. The following steps describe one of these methods and are similar to those used on the <strong class="source-inline">CIFAR-10</strong> dataset earlier in the chapter:</p>
			<ol>
				<li value="1">Start a new Jupyter notebook.</li>
				<li>Import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Import the additional libraries needed:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, \</p><p class="source-code">    Dropout, Activation, Rescaling</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay</p></li>
				<li>Load the <strong class="source-inline">CIFAR-100</strong> dataset directly from <strong class="source-inline">tensorflow_datasets</strong> and view its properties:<p class="source-code">(c100_train_dataset, c100_test_dataset), \</p><p class="source-code">dataset_info = tfds.load('cifar100',\</p><p class="source-code">                         split = ['train', 'test'],\</p><p class="source-code">                         data_dir = 'content/Cifar100/',\</p><p class="source-code">                         shuffle_files = True,\</p><p class="source-code">                         as_supervised = True,\</p><p class="source-code">                         with_info = True)</p><p class="source-code">assert isinstance(c100_train_dataset, tf.data.Dataset)</p><p class="source-code">image_shape = dataset_info.features["image"].shape</p><p class="source-code">print(f'Shape of Images in the Dataset: \t{image_shape}')</p><p class="source-code">num_classes = dataset_info.features["label"].num_classes</p><p class="source-code">print(f'Number of Classes in the Dataset: \t{num_classes}')</p><p class="source-code">names_of_classes = dataset_info.features["label"].names</p><p class="source-code">print(f'Names of Classes in the Dataset: \t{names_of_classes}\n')</p><p class="source-code">print(f'Total examples in Train Dataset: \</p><p class="source-code">      \t{len(c100_train_dataset)}')</p><p class="source-code">print(f'Total examples in Test Dataset: \</p><p class="source-code">      \t{len(c100_test_dataset)}')</p><p>This will give the following output:</p><div id="_idContainer443" class="IMG---Figure"><img src="image/B16341_07_42.jpg" alt="Figure 7.42: Properties of the CIFAR-100 dataset&#13;&#10;"/></div><p class="figure-caption">Figure 7.42: Properties of the CIFAR-100 dataset</p></li>
				<li>Use a rescaling layer to rescale images. Then, build a test and train data pipeline by rescaling, caching, shuffling, batching, and prefetching the images:<p class="source-code">normalization_layer = Rescaling(1./255)</p><p class="source-code">c100_train_dataset = c100_train_dataset.map\</p><p class="source-code">                     (lambda x, y: (normalization_layer(x), y), \</p><p class="source-code">                      num_parallel_calls = \</p><p class="source-code">                      tf.data.experimental.AUTOTUNE)</p><p class="source-code">c100_train_dataset = c100_train_dataset.cache()</p><p class="source-code">c100_train_dataset = c100_train_dataset.shuffle\</p><p class="source-code">                     (len(c100_train_dataset))</p><p class="source-code">c100_train_dataset = c100_train_dataset.batch(32)</p><p class="source-code">c100_train_dataset = c100_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)</p><p class="source-code">c100_test_dataset = c100_test_dataset.map\</p><p class="source-code">                    (lambda x, y: (normalization_layer(x), y), \</p><p class="source-code">                     num_parallel_calls = \</p><p class="source-code">                     tf.data.experimental.AUTOTUNE)</p><p class="source-code">c100_test_dataset = c100_test_dataset.cache()</p><p class="source-code">c100_test_dataset = c100_test_dataset.batch(128)</p><p class="source-code">c100_test_dataset = \</p><p class="source-code">c100_test_dataset.prefetch(tf.data.experimental.AUTOTUNE)</p></li>
				<li>Build the model using the functional API:<p class="source-code">input_layer = Input(shape=image_shape)</p><p class="source-code">x = Conv2D(filters = 32, kernel_size = \</p><p class="source-code">           (3, 3), strides=2)(input_layer)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Conv2D(filters = 64, kernel_size = (3, 3), strides=2)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Conv2D(filters = 128, kernel_size = (3, 3), strides=2)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Flatten()(x)</p><p class="source-code">x = Dropout(rate = 0.5)(x)</p><p class="source-code">x = Dense(units = 1024)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Dropout(rate = 0.2)(x)</p><p class="source-code">x = Dense(units = num_classes)(x)</p><p class="source-code">output = Activation('softmax')(x)</p><p class="source-code">c100_classification_model = Model(input_layer, output)</p></li>
				<li>Compile and fit the model:<p class="source-code">c100_classification_model.compile(\</p><p class="source-code">    optimizer='adam', \</p><p class="source-code">    loss='sparse_categorical_crossentropy', \</p><p class="source-code">    metrics = ['accuracy'], loss_weights = None, \</p><p class="source-code">    weighted_metrics = None, run_eagerly = None, \</p><p class="source-code">    steps_per_execution = None</p><p class="source-code">)</p><p class="source-code">history = c100_classification_model.fit\</p><p class="source-code">          (c100_train_dataset, \</p><p class="source-code">           validation_data=c100_test_dataset, \</p><p class="source-code">           epochs=15)</p><p>The output will look like the following image:</p><div id="_idContainer444" class="IMG---Figure"><img src="image/B16341_07_43.jpg" alt="Figure 7.43: Model fit&#13;&#10;"/></div><p class="figure-caption">Figure 7.43: Model fit</p></li>
				<li>Plot the loss and accuracy by using the following code:<p class="source-code">def plot_trend_by_epoch(tr_values, val_values, title):</p><p class="source-code">    epoch_number = range(len(tr_values))</p><p class="source-code">    plt.plot(epoch_number, tr_values, 'r')</p><p class="source-code">    plt.plot(epoch_number, val_values, 'b')</p><p class="source-code">    plt.title(title)</p><p class="source-code">    plt.xlabel('epochs')</p><p class="source-code">    plt.legend(['Training '+title, 'Validation '+title])</p><p class="source-code">    plt.figure()</p><p class="source-code">hist_dict = history.history</p><p class="source-code">tr_loss, val_loss = hist_dict['loss'], \</p><p class="source-code">                    hist_dict['val_loss']</p><p class="source-code">plot_trend_by_epoch(tr_loss, val_loss, "Loss")</p><p class="source-code">tr_accuracy, val_accuracy = hist_dict['accuracy'], \</p><p class="source-code">                            hist_dict['val_accuracy']</p><p class="source-code">plot_trend_by_epoch(tr_accuracy, val_accuracy, "Accuracy")</p><p>Loss plot would look like the following:</p><div id="_idContainer445" class="IMG---Figure"><img src="image/B16341_07_44.jpg" alt="Figure 7.44: Loss plot&#13;&#10;"/></div><p> </p><p class="figure-caption">Figure 7.44: Loss plot</p><p>Accuracy plot would look like the following:</p><div id="_idContainer446" class="IMG---Figure"><img src="image/B16341_07_45.jpg" alt="Figure 7.45: Accuracy plot&#13;&#10;"/></div><p class="figure-caption">Figure 7.45: Accuracy plot</p></li>
				<li>Display a misclassified example. Use the following code:<p class="source-code">test_labels = []</p><p class="source-code">test_images = []</p><p class="source-code">for image, label in tfds.as_numpy(c100_test_dataset.unbatch()):</p><p class="source-code">    test_images.append(image)</p><p class="source-code">    test_labels.append(label)</p><p class="source-code">test_labels = np.array(test_labels)</p><p class="source-code">predictions = c100_classification_model.predict\</p><p class="source-code">              (c100_test_dataset).argmax(axis=1)</p><p class="source-code">incorrect_predictions = np.where(predictions != test_labels)[0]</p><p class="source-code">index = np.random.choice(incorrect_predictions)</p><p class="source-code">plt.imshow(test_images[index])</p><p class="source-code">print(f'True label: {names_of_classes[test_labels[index]]}')</p><p class="source-code">print(f'Predicted label: {names_of_classes[predictions[index]]}')</p><p>This will produce the following output:</p><div id="_idContainer447" class="IMG---Figure"><img src="image/B16341_07_46.jpg" alt="Figure 7.46: Wrong classification example&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.46: Wrong classification example</p>
			<p>The output shows an example of a wrong classification: the prediction was lion, and the true value was mouse. In this activity, the number of classes was 100, which makes it significantly more difficult than in <em class="italic">Exercise 7.05</em>, <em class="italic">Building a CNN</em>, in which there were only 10 classes. Nevertheless, you can see that after 15 epochs, the accuracy continued to increase, and loss continued to decrease even on the validation dataset. You could then expect better model performance if you were to let the model train for more epochs.</p>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor273"/>8. Pre-Trained Networks</h1>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor274"/>Activity 8.01: Fruit Classification with <a id="_idTextAnchor275"/>Fine-Tuning</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the TensorFlow library as <strong class="source-inline">tf</strong>:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> containing a link to the dataset:<p class="source-code">file_url = 'https://github.com/PacktWorks<a id="_idTextAnchor276"/>hops/'\</p><p class="source-code">          'The-TensorFlow-Workshop/blob/master'\</p><p class="source-code">          '/Chapter08/dataset/fruits360.zip'</p></li>
				<li>Download the dataset using <strong class="source-inline">tf.keras.get_file</strong> with <strong class="source-inline">'fruits360.zip'</strong>, <strong class="source-inline">origin=file_url</strong>, and <strong class="source-inline">extract=True</strong> as parameters, and save the result to a variable called <strong class="source-inline">zip_dir</strong>:<p class="source-code">zip_dir = tf.keras.utils.get_file('fruits360.zip', \</p><p class="source-code">                                  origin=file_url, extract=True)</p></li>
				<li>Import the <strong class="source-inline">pathlib</strong> library:<p class="source-code">import pathlib</p></li>
				<li>Create a variable called <strong class="source-inline">path</strong> containing the full path to the <strong class="source-inline">fruits360_filtered</strong> directory using <strong class="source-inline">pathlib.Path(zip_dir).parent</strong>:<p class="source-code">path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'</p></li>
				<li>Create two variables called <strong class="source-inline">train_dir</strong> and <strong class="source-inline">validation_dir</strong> that take the full path to the train (<strong class="source-inline">Training</strong>) and validation (<strong class="source-inline">Test</strong>) folders, respectively:<p class="source-code">train_dir = path / 'Training'</p><p class="source-code">validation_dir = path / 'Test'</p></li>
				<li>Create two variables called <strong class="source-inline">total_train</strong> and <strong class="source-inline">total_val</strong> that get the number of images for the training and validation sets: <p class="source-code">total_train = 11398</p><p class="source-code">total_val = 4752</p></li>
				<li>Import <strong class="source-inline">ImageDataGenerator</strong> from <strong class="source-inline">tensorflow.keras.preprocessing</strong>:<p class="source-code">from tensorflow.keras.preprocessing.image</p><p class="source-code">    import ImageDataGenerator</p></li>
				<li>Create an <strong class="source-inline">ImageDataGenerator</strong> model called <strong class="source-inline">train_img_gen</strong> with data augmentation:<p class="source-code">train_img_gen = ImageDataGenerator(rescale=1./255, \</p><p class="source-code">                                   rotation_range=40, \</p><p class="source-code">                                   width_shift_range=0.1, \</p><p class="source-code">                                   height_shift_range=0.1, \</p><p class="source-code">                                   shear_range=0.2, \</p><p class="source-code">                                   zoom_range=0.2, \</p><p class="source-code">                                   horizontal_flip=True, \</p><p class="source-code">                                   fill_mode='nearest'))</p></li>
				<li>Create an <strong class="source-inline">ImageDataGenerator</strong> mode called <strong class="source-inline">val_img_gen</strong> with rescaling by dividing by <strong class="source-inline">255</strong>:<p class="source-code">val_img_gen = ImageDataGenerator(rescale=1./255)</p></li>
				<li>Create four variables called <strong class="source-inline">batch_size</strong>, <strong class="source-inline">img_height</strong>, <strong class="source-inline">img_width</strong>, and <strong class="source-inline">channel</strong> that take the values <strong class="source-inline">32</strong>, <strong class="source-inline">224</strong>, <strong class="source-inline">224</strong>, and <strong class="source-inline">3</strong>, respectively:<p class="source-code">Batch_size = 32</p><p class="source-code">img_height = 224</p><p class="source-code">img_width = 224</p><p class="source-code">channel = 3</p></li>
				<li>Create a data generator called <strong class="source-inline">train_data_gen</strong> using <strong class="source-inline">flow_from_directory()</strong> and specify the batch size, training folder, and target size:<p class="source-code">train_data_gen = train_image_generator.flow_from_directory\</p><p class="source-code">                 (batch_size=batch_size, directory=train_dir, \</p><p class="source-code">                  target_size=(img_height, img_width))</p></li>
				<li>Create a data generator called <strong class="source-inline">val_data_gen</strong> using <strong class="source-inline">flow_from_directory()</strong> and specify the batch size, validation folder, and target size:<p class="source-code">val_data_gen = validation_image_generator.flow_from_directory\</p><p class="source-code">               (batch_size=batch_size, directory=validation_dir,\</p><p class="source-code">                target_size=(img_height, img_width))</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> as the seed for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong>:<p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Import <strong class="source-inline">NASNetMobile</strong> from <strong class="source-inline">tensorflow.keras.applications</strong>:<p class="source-code">from tensorflow.keras.applications</p><p class="source-code">import NASNetMobile</p></li>
				<li>Instantiate a <strong class="source-inline">NASNetMobile</strong> model into a variable called <strong class="source-inline">base_model</strong>:<p class="source-code">base_model = NASNetMobile(input_shape=(img_height, img_width, \</p><p class="source-code">                                       channel), \</p><p class="source-code">                          weights='imagenet', include_top=False)</p></li>
				<li>Print a summary of this <strong class="source-inline">NASNetMobile</strong> model:<p class="source-code">base_model.summary()</p><p>The expected output is as follows:</p><div id="_idContainer448" class="IMG---Figure"><img src="image/B16341_08_08.jpg" alt="Figure 8.8: Summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 8.8: Summary of the model</p></li>
				<li>Create a new model using <strong class="source-inline">tf.keras.Sequential()</strong> by adding the base model to the <strong class="source-inline">Flatten</strong> and <strong class="source-inline">Dense</strong> layers. Save this model to a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential([base_model,\</p><p class="source-code">                             layers.Flatten(),\</p><p class="source-code">                             layers.Dense(500, \</p><p class="source-code">                                          activation='relu'), \</p><p class="source-code">                             layers.Dense(120, \</p><p class="source-code">                                          activation='softmax')])</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.optimizers.Adam()</strong> class with <strong class="source-inline">0.001</strong> as the learning rate and save it to a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the neural network using the <strong class="source-inline">compile()</strong> method with <strong class="source-inline">categorical_crossentropy</strong> as the loss function, an Adam optimizer with a learning rate of <strong class="source-inline">0.001</strong>, and <strong class="source-inline">accuracy</strong> as the metric to be displayed:<p class="source-code">model.compile(loss='categorical_crossentropy', \</p><p class="source-code">              optimizer=optimizer, metrics=['accuracy'])</p></li>
				<li>Fit the neural networks with <strong class="source-inline">fit()</strong> method. This model may take a few minutes to train:<p class="source-code">model.fit(train_data_gen,</p><p class="source-code">          steps_per_epoch=len(features_train) // batch_size,\</p><p class="source-code">          epochs=5,\</p><p class="source-code">          validation_data=val_data_gen,\</p><p class="source-code">          validation_steps=len(features_test) // batch_size\</p><p class="source-code">)</p><p>The expected output is as follows:</p><div id="_idContainer449" class="IMG---Figure"><img src="image/B16341_08_06.jpg" alt="Figure 8.9: Epochs of the trained model&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.9: Epochs of the trained model</p>
			<p>In this activity, you used fine-tuning to customize a <strong class="source-inline">NASNetMobile</strong> model pre-trained on ImageNet on a dataset containing images of fruit. You froze the first 700 layers of this model and trained only the last few on five epochs. You achieved an accuracy score of <strong class="source-inline">0.9549</strong> for the training set and <strong class="source-inline">0.8264</strong> for the test set.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor277"/>Activity 8.02: Transfer Learning with Tenso<a id="_idTextAnchor278"/>rFlow Hub</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> containing a link to the dataset:<p class="source-code">file_url = 'https://storage.googleapis.com'\</p><p class="source-code">           '/mledu-datasets/cats_and_dogs_filtered.zip'</p></li>
				<li>Download the dataset using <strong class="source-inline">tf.keras.get_file</strong> with <strong class="source-inline">cats_and_dogs.zip</strong>, <strong class="source-inline">origin=file_url</strong>, and <strong class="source-inline">extract=True</strong> as parameters and save the result to a variable called <strong class="source-inline">zip_dir</strong>:<p class="source-code">zip_dir = tf.keras.utils.get_file('cats_and_dogs.zip', \</p><p class="source-code">                                  origin=file_url, extract=True)</p></li>
				<li>Import the <strong class="source-inline">pathlib</strong> library:<p class="source-code">import pathlib</p></li>
				<li>Create a variable called <strong class="source-inline">path</strong> containing the full path to the <strong class="source-inline">cats_and_dogs_filtered</strong> directory using <strong class="source-inline">pathlib.Path(zip_dir).parent</strong>:<p class="source-code">path = pathlib.Path(zip_dir).parent / 'cats_and_dogs_filtered'</p></li>
				<li>Create two variables called <strong class="source-inline">train_dir</strong> and <strong class="source-inline">validation_dir</strong> that take the full path to the <strong class="source-inline">train</strong> and <strong class="source-inline">validation</strong> folders:<p class="source-code">train_dir = path / 'train'</p><p class="source-code">validation_dir = path / 'validation'</p></li>
				<li>Create two variables called <strong class="source-inline">total_train</strong> and <strong class="source-inline">total_val</strong> that will get the number of images for the training and validation sets (<strong class="source-inline">2000</strong> and <strong class="source-inline">1000</strong>, respectively):<p class="source-code">total_train = 2000</p><p class="source-code">total_val = 1000</p></li>
				<li>Import <strong class="source-inline">ImageDataGenerator</strong> from <strong class="source-inline">tensorflow.keras.preprocessing</strong>:<p class="source-code">from tensorflow.keras.preprocessing.image </p><p class="source-code">import ImageDataGenerator</p></li>
				<li>Instantiate two <strong class="source-inline">ImageDataGenerator</strong> classes and call them <strong class="source-inline">train_image_generator</strong> and <strong class="source-inline">validation_image_generator</strong>. These will rescale images by dividing by <strong class="source-inline">255</strong>:<p class="source-code">train_image_generator = ImageDataGenerator(rescale=1./255)</p><p class="source-code">validation_image_generator = ImageDataGenerator(rescale=1./255)</p></li>
				<li>Create three variables called <strong class="source-inline">batch_size</strong>, <strong class="source-inline">img_height</strong>, and <strong class="source-inline">img_width</strong> that take the values <strong class="source-inline">32</strong>, <strong class="source-inline">224</strong>, and <strong class="source-inline">224</strong>, respectively:<p class="source-code">batch_size = 32</p><p class="source-code">img_height = 224</p><p class="source-code">img_width = 224</p></li>
				<li>Create a data generator called <strong class="source-inline">train_data_gen</strong> using <strong class="source-inline">flow_from_directory()</strong> and specify the batch size, the path to the training folder, target size, and mode of the class: <p class="source-code">train_data_gen = train_image_generator.flow_from_directory\</p><p class="source-code">                 (batch_size=batch_size, directory=train_dir, \</p><p class="source-code">                  shuffle=True, target_size=(img_height, \</p><p class="source-code">                                             img_width), \</p><p class="source-code">                  class_mode='binary')</p></li>
				<li>Create a data generator called <strong class="source-inline">val_data_gen</strong> using <strong class="source-inline">flow_from_directory()</strong> and specify the batch size, paths to the validation folder, target size, and mode of the class:<p class="source-code">val_data_gen = validation_image_generator.flow_from_directory\</p><p class="source-code">               (batch_size=batch_size, \</p><p class="source-code">                directory=validation_dir, \</p><p class="source-code">                target_size=(img_height, img_width), \</p><p class="source-code">                class_mode='binary')</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> (this is totally arbitrary) as <strong class="source-inline">seed</strong> for numpy and tensorflow:<p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Import <strong class="source-inline">tensorflow_hub</strong>, as shown here:<p class="source-code">import tensorflow_hub as hub</p></li>
				<li>Load the EfficientNet B0 feature vector from TensorFlow Hub:<p class="source-code">MODULE_HANDLE = 'https://tfhub.dev/google/efficientnet/b0'\</p><p class="source-code">                '/feature-vector/1'</p><p class="source-code">module = hub.load(MODULE_HANDLE)</p></li>
				<li>Create a new model that combines the EfficientNet B0 module with two new top layers, with <strong class="source-inline">500</strong> and <strong class="source-inline">1</strong> as units, and ReLu and sigmoid as the activation functions:<p class="source-code">model = tf.keras.Sequential\</p><p class="source-code">        ([hub.KerasLayer(MODULE_HANDLE,\</p><p class="source-code">                         input_shape=(224, 224, 3)),</p><p class="source-code">          layers.Dense(500, activation='relu'),</p><p class="source-code">          layers.Dense(1, activation='sigmoid')])</p></li>
				<li>Compile this model by providing <strong class="source-inline">binary_crossentropy</strong> as the <strong class="source-inline">loss</strong> function, an Adam optimizer with a learning rate of <strong class="source-inline">0.001</strong>, and <strong class="source-inline">accuracy</strong> as the metric to be displayed:<p class="source-code">model.compile(loss='binary_crossentropy', \</p><p class="source-code">              optimizer=tf.keras.optimizers.Adam(0.001), \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Fit the model and provide the train and validation data generators. Run it for five epochs:<p class="source-code">model.fit(train_data_gen, \</p><p class="source-code">          steps_per_epoch = total_train // batch_size, \</p><p class="source-code">          epochs=5, \</p><p class="source-code">          validation_data=val_data_gen, \</p><p class="source-code">          validation_steps=total_val // batch_size)</p><p>The expected output will be as follows:</p><div id="_idContainer450" class="IMG---Figure"><img src="image/B16341_08_07.jpg" alt="Figure 8.10: Model training output&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.10: Model training output</p>
			<p>In this activity, you achieved a very high accuracy score (with <strong class="source-inline">1</strong> and <strong class="source-inline">0.99</strong> for the training and test sets, respectively), using transfer learning from TensorFlow Hub. You used the <strong class="bold">EfficientNet B0</strong> feature vector combined with two custom final layers, and your final model is almost perfectly predicting images of cats and dogs.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor279"/>9. Recurrent Neural Networks</h1>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor280"/>Activity 9.01: Building an RNN with Multiple LSTM Layers to Predict Power Consumption</h2>
			<p><strong class="bold">Solution:</strong></p>
			<p>Perform the following steps to complete this activity.</p>
			<ol>
				<li value="1">Open a new Jupyter or Colab notebook.</li>
				<li>Import the libraries needed. Use <strong class="source-inline">numpy</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">datetime</strong>, and <strong class="source-inline">MinMaxScaler</strong> to scale the dataset between zero and one:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import datetime</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler</p></li>
				<li>Use the <strong class="source-inline">read_csv()</strong> function to read in your CSV file and store your dataset in a pandas DataFrame, <strong class="source-inline">data</strong>:<p class="source-code">data = pd.read_csv("household_power_consumption.csv")</p></li>
				<li>Create a new column, <strong class="source-inline">Datetime</strong>, by combining <strong class="source-inline">Date</strong> and <strong class="source-inline">Time</strong> columns using the following code:<p class="source-code">data['Date'] = pd.to_datetime(data['Date'], format="%d/%m/%Y")</p><p class="source-code">data['Datetime'] = data['Date'].dt.strftime('%Y-%m-%d') + ' ' \</p><p class="source-code">                   +  data['Time']</p><p class="source-code">data['Datetime'] = pd.to_datetime(data['Datetime'])</p></li>
				<li>Sort the DataFrame in ascending order using the <strong class="source-inline">Datetime</strong> column:<p class="source-code">data = data.sort_values(['Datetime'])</p></li>
				<li>Create a list called <strong class="source-inline">num_cols</strong> containing the columns that have numeric values – <strong class="source-inline">Global_active_power</strong>, <strong class="source-inline">Global_reactive_power</strong>, <strong class="source-inline">Voltage</strong>, <strong class="source-inline">Global_intensity</strong>, <strong class="source-inline">Sub_metering_1</strong>, <strong class="source-inline">Sub_metering_2</strong>, and <strong class="source-inline">Sub_metering_3</strong>:<p class="source-code">num_cols = ['Global_active_power', 'Global_reactive_power', \</p><p class="source-code">            'Voltage', 'Global_intensity', 'Sub_metering_1', \</p><p class="source-code">            'Sub_metering_2', 'Sub_metering_3']</p></li>
				<li>Convert all columns listed in <strong class="source-inline">num_cols</strong> to a numeric datatype:<p class="source-code">for col in num_cols:</p><p class="source-code">    data[col] = pd.to_numeric(data[col], errors='coerce')</p></li>
				<li>Call the <strong class="source-inline">head()</strong> function on your data to take a look at the first five rows of your DataFrame:<p class="source-code">data.head()</p><p>You should get the following output:</p><div id="_idContainer451" class="IMG---Figure"><img src="image/B16341_09_40.jpg" alt="Figure 9.40: First five rows of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 9.40: First five rows of the DataFrame</p></li>
				<li>Call <strong class="source-inline">tail()</strong> on your data to take a look at the last five rows of your DataFrame:<p class="source-code">data.tail()</p><p>You should get the following output:</p><div id="_idContainer452" class="IMG---Figure"><img src="image/B16341_09_41.jpg" alt="Figure 9.41: Last five rows of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 9.41: Last five rows of the DataFrame</p></li>
				<li>Iterate through columns in <strong class="source-inline">num_cols</strong> and fill in missing values with the average using the following code:<p class="source-code">for col in num_cols:</p><p class="source-code">    data[col].fillna(data[col].mean(), inplace=True)</p></li>
				<li>Use <strong class="source-inline">drop()</strong> to remove <strong class="source-inline">Date</strong>, <strong class="source-inline">Time</strong>, <strong class="source-inline">Global_reactive_power</strong>, and <strong class="source-inline">Datetime</strong> columns from your DataFrame and save the results in a variable called <strong class="source-inline">df</strong>:<p class="source-code">df = data.drop(['Date', 'Time', 'Global_reactive_power', 'Datetime'], \</p><p class="source-code">               axis = 1)</p></li>
				<li>Create a scaler from <strong class="source-inline">MinMaxScaler</strong> to your DataFrame to numbers between zero and one. Use <strong class="source-inline">fit_transform</strong> to fit the model to the data and then transform the data according to the fitted model:<p class="source-code">scaler = MinMaxScaler()</p><p class="source-code">scaled_data = scaler.fit_transform(df)</p><p class="source-code">scaled_data </p><p>You should get the following output:</p><div id="_idContainer453" class="IMG---Figure"><img src="image/B16341_09_42.jpg" alt="Figure 9.42: Standardized training data&#13;&#10;"/></div><p class="figure-caption">Figure 9.42: Standardized training data</p><p>The preceding screenshot shows the data has been standardized. Values sit between 0 and 1 now.</p></li>
				<li>Create two empty lists called <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> that will be used to store features and target variables:<p class="source-code">X = []</p><p class="source-code">y = []</p></li>
				<li>Create a training dataset that has the previous 60 minutes' power consumption so that you can predict the value for the next minute. Use a <strong class="source-inline">for</strong> loop to create data in 60 time steps:<p class="source-code">for i in range(60, scaled_data.shape[0]):</p><p class="source-code">    X.append(scaled_data [i-60:i])</p><p class="source-code">    y.append(scaled_data [i, 0])</p></li>
				<li>Convert <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> into NumPy arrays in preparation for training your model:<p class="source-code">X, y = np.array(X), np.array(y)</p></li>
				<li>Split the dataset into training and testing sets with data before and after the index <strong class="source-inline">217440</strong>, respectively:<p class="source-code">X_train = X[:217440]</p><p class="source-code">y_train = y[:217440]</p><p class="source-code">X_test = X[217440:]</p><p class="source-code">y_test = y[217440:]</p></li>
				<li>You will need some additional libraries for building LSTM. Use <strong class="source-inline">Sequential</strong> to initialize the neural net, <strong class="source-inline">Dense</strong> to add a dense layer, <strong class="source-inline">LSTM</strong> to add an LSTM layer, and <strong class="source-inline">Dropout</strong> to help prevent overfitting:<p class="source-code">from tensorflow.keras import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense, LSTM, Dropout</p></li>
				<li>Initialize your neural network. Add LSTM layers with <strong class="source-inline">20</strong>, <strong class="source-inline">40</strong>, and <strong class="source-inline">80</strong> units. Use a ReLU activation function and set <strong class="source-inline">return_sequences</strong> to <strong class="source-inline">True</strong>. The <strong class="source-inline">input_shape</strong> should be the dimensions of your training set (the number of features and days). Finally, add your dropout layer:<p class="source-code">regressor = Sequential()</p><p class="source-code">regressor.add(LSTM(units= 20, activation = 'relu',\</p><p class="source-code">                   return_sequences = True,\</p><p class="source-code">                   input_shape = (X_train.shape[1], X_train.shape[2])))</p><p class="source-code">regressor.add(Dropout(0.5))</p><p class="source-code">regressor.add(LSTM(units= 40, \</p><p class="source-code">                   activation = 'relu', \</p><p class="source-code">                   return_sequences = True))</p><p class="source-code">regressor.add(Dropout(0.5))</p><p class="source-code">regressor.add(LSTM(units= 80, \</p><p class="source-code">                   activation = 'relu'))</p><p class="source-code">regressor.add(Dropout(0.5))</p><p class="source-code">regressor.add(Dense(units = 1))  </p></li>
				<li>Print the architecture of the model using the <strong class="source-inline">summary()</strong> function:<p class="source-code">regressor.summary()</p><p>The preceding command gives valuable information about the model, layers, and parameters:</p><div id="_idContainer454" class="IMG---Figure"><img src="image/B16341_09_43.jpg" alt="Figure 9.43: Model summary&#13;&#10;"/></div><p class="figure-caption">Figure 9.43: Model summary</p></li>
				<li>Use the <strong class="source-inline">compile()</strong> method to configure your model for training. Select Adam as your optimizer and mean squared error to measure your loss function:<p class="source-code">regressor.compile(optimizer='adam', loss = 'mean_squared_error')</p></li>
				<li>Fit your model and set it to run on two epochs. Set your batch size to <strong class="source-inline">32</strong>:<p class="source-code">regressor.fit(X_train, y_train, epochs=2, batch_size=32)</p></li>
				<li>Save the predictions on the test set in a variable called <strong class="source-inline">y_pred</strong> using <strong class="source-inline">regressor.predict(X_test)</strong>:<p class="source-code">y_pred = regressor.predict(X_test)</p></li>
				<li>Take a look at the real household power consumption and your predictions for the last hour of data from your test set:<p class="source-code">plt.figure(figsize=(14,5))</p><p class="source-code">plt.plot(y_test[-60:], color = 'black', \</p><p class="source-code">         label = "Real Power Consumption")</p><p class="source-code">plt.plot(y_pred[-60:], color = 'gray', \</p><p class="source-code">         label = 'Predicted Power Consumption')</p><p class="source-code">plt.title('Power Consumption Prediction')</p><p class="source-code">plt.xlabel('time')</p><p class="source-code">plt.ylabel('Power Consumption')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>You should get the following output:</p><div id="_idContainer455" class="IMG---Figure"><img src="image/B16341_09_44.jpg" alt="Figure 9.44: Household power consumption prediction visualization&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.44: Household power consumption prediction visualization</p>
			<p>As you can see in <em class="italic">Figure 9.44</em>, your results are pretty good. You can observe that for the most part, your predictions are close to the actual values.</p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor281"/>Activity 9.02: Building an RNN for Predicting Tweets' Sentiment</h2>
			<p><strong class="bold">Solution:</strong></p>
			<p>Perform the following steps to complete this activity:</p>
			<ol>
				<li value="1">Open a new Jupyter or Colab notebook.</li>
				<li>Import the libraries needed. Use <strong class="source-inline">numpy</strong> for computation and <strong class="source-inline">pandas</strong> to work with your dataset:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p></li>
				<li>Use the <strong class="source-inline">read_csv</strong> method to read in your CSV file and store your dataset in a pandas DataFrame, <strong class="source-inline">data</strong>:<p class="source-code">data = pd.read_csv("<a href="">https://raw.githubusercontent.com"\</a></p><p class="source-code">                   "/PacktWorkshops/The-TensorFlow-Workshop"\</p><p class="source-code">                   "/master/Chapter09/Datasets/tweets.csv")</p></li>
				<li>Call the <strong class="source-inline">head()</strong> method on your data to take a look at the first five rows of your DataFrame: <p class="source-code">data.head()</p><p>You should get the following output:</p><div id="_idContainer456" class="IMG---Figure"><img src="image/B16341_09_45.jpg" alt="Figure 9.45: First five rows of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 9.45: First five rows of the DataFrame</p><p>In the preceding screenshot, you can see the different sentiments stored in the <strong class="source-inline">airline_sentiment</strong> column.</p></li>
				<li>Call <strong class="source-inline">tail()</strong> on your data to take a look at the last five rows of your DataFrame: <p class="source-code"><strong class="source-inline">data.tail()</strong></p><p>You should get the following output:</p><div id="_idContainer457" class="IMG---Figure"><img src="image/B16341_09_46.jpg" alt="Figure 9.46: Last five rows of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 9.46: Last five rows of the DataFrame</p></li>
				<li>Create a new DataFrame called <strong class="source-inline">df</strong> that will have only <strong class="source-inline">text</strong> as features and <strong class="source-inline">airline_sentiment</strong> as the target variable:<p class="source-code">df = data[['text','airline_sentiment']]</p></li>
				<li>Subset <strong class="source-inline">df</strong> by removing all rows where <strong class="source-inline">airline_sentiment</strong> is equal to <strong class="source-inline">neutral</strong> by using the following command:<p class="source-code">df = df[df['airline_sentiment'] != 'neutral']</p></li>
				<li>Transform the <strong class="source-inline">airline_sentiment</strong> column to a numeric type by replacing <strong class="source-inline">negative</strong> with <strong class="source-inline">0</strong> and <strong class="source-inline">positive</strong> with <strong class="source-inline">1</strong>. Save the result to a variable, <strong class="source-inline">y</strong>:<p class="source-code">y = df['airline_sentiment'].map({'negative':0, 'positive':1}).values</p></li>
				<li>Create a variable, <strong class="source-inline">X</strong>, that will contain the data from the text column in <strong class="source-inline">df</strong>:<p class="source-code">X = df['text']</p></li>
				<li>Import <strong class="source-inline">Tokenizer</strong> from <strong class="source-inline">tensorflow.keras.preprocessing.text</strong> and <strong class="source-inline">pad_sequences</strong> from <strong class="source-inline">tensorflow.keras.preprocessing.sequence</strong>:<p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p><p class="source-code">from tensorflow.keras.preprocessing.sequence \</p><p class="source-code">    import pad_sequences</p></li>
				<li>Instantiate a <strong class="source-inline">Tokenizer()</strong> class with <strong class="source-inline">num_words</strong> equal to <strong class="source-inline">10000</strong>. This will keep only the first 10,000 most frequent words. Save it into a variable, <strong class="source-inline">tokenizer</strong>: <p class="source-code">tokenizer = Tokenizer(num_words=10000)</p></li>
				<li>Fit <strong class="source-inline">tokenizer</strong> on the data <strong class="source-inline">X</strong>:<p class="source-code">tokenizer.fit_on_texts(X)</p></li>
				<li>Print the vocabulary from <strong class="source-inline">tokenizer</strong>:<p class="source-code">tokenizer.word_index</p><p>You should get output like the following:</p><div id="_idContainer458" class="IMG---Figure"><img src="image/B16341_09_47.jpg" alt="Figure 9.47: Vocabulary defined by tokenizer&#13;&#10;"/></div><p class="figure-caption">Figure 9.47: Vocabulary defined by tokenizer</p><p>From the output vocabulary, you can see the word <strong class="source-inline">to</strong> has been assigned the index <strong class="source-inline">1</strong>, <strong class="source-inline">the</strong> is assigned <strong class="source-inline">2</strong>, and so on. You can use it to map the raw text into a numerical version of it.</p></li>
				<li>Create the <strong class="source-inline">vocab_size</strong> variable, to contain the length of the tokenizer vocabulary plus an additional character that will be used for unknown words:<p class="source-code">vocab_size = len(tokenizer.word_index) + 1</p></li>
				<li>Transform the raw text from <strong class="source-inline">X</strong> to an encoded version using the vocabulary from <strong class="source-inline">tokenizer</strong>. Save the result in a variable called <strong class="source-inline">encoded_tweets</strong>:<p class="source-code">encoded_tweets = tokenizer.texts_to_sequences(X)</p></li>
				<li>Pad <strong class="source-inline">encoded_tweets</strong> with <strong class="source-inline">0</strong> at the end for a maximum of 280 characters. Save the result in a variable called <strong class="source-inline">padded_tweets</strong>:<p class="source-code">padded_tweets = pad_sequences(encoded_tweets, maxlen=280, padding='post')</p></li>
				<li>Print the shape of <strong class="source-inline">padded_tweets</strong>:<p class="source-code">padded_tweets.shape</p><p>You should get the following result:</p><p class="source-code">(11541, 280)</p></li>
				<li>As you can see, prepared tweets now all have the same length, that is, 280 characters. </li>
				<li>Randomly permute the indices of <strong class="source-inline">padded_tweets</strong>. Save the result in the <strong class="source-inline">indices</strong> variable:<p class="source-code">indices = np.random.permutation(padded_tweets.shape[0])</p></li>
				<li>Create two variables, <strong class="source-inline">train_idx</strong> and <strong class="source-inline">test_idx</strong>, to contain the first 10,000 indices and the remaining ones respectively:<p class="source-code">train_idx = indices[:10000]</p><p class="source-code">test_idx = indices[10000:]</p></li>
				<li>Using <strong class="source-inline">padded_tweets</strong> and <strong class="source-inline">y</strong>, split the data into training and testing sets. Save them into four different variables called <strong class="source-inline">X_train</strong>, <strong class="source-inline">X_test</strong>, <strong class="source-inline">y_train</strong>, and <strong class="source-inline">y_test</strong>:<p class="source-code">X_train = padded_tweets[train_idx,]</p><p class="source-code">X_test = padded_tweets[test_idx,]</p><p class="source-code">y_train = y[train_idx,]</p><p class="source-code">y_test = y[test_idx,]</p></li>
				<li>You will need some additional libraries to build your model. Import <strong class="source-inline">Sequential</strong>, <strong class="source-inline">Dense</strong>, <strong class="source-inline">LSTM</strong>, <strong class="source-inline">Dropout</strong>, and <strong class="source-inline">Embedding</strong> using the following code:<p class="source-code">from tensorflow.keras import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding</p></li>
				<li>Initialize your neural network. Add an embedding layer by providing the length of the vocabulary, the length of the embedding layer, and the input length. Add two LSTM layers with <strong class="source-inline">50</strong> and <strong class="source-inline">100</strong> units. Use a ReLU activation function and set <strong class="source-inline">return_sequences</strong> to <strong class="source-inline">True</strong>. Then, add a dropout layer for each LSTM with a dropout of 20%. Finally, add a fully-connected layer with sigmoid as the final activation function:<p class="source-code">model = Sequential()</p><p class="source-code">  </p><p class="source-code">model.add(Embedding(vocab_size, embedding_vector_length, input_length=280))</p><p class="source-code">model.add(LSTM(units= 50, activation = 'relu', return_sequences = True))</p><p class="source-code">model.add(Dropout(0.2))</p><p class="source-code">model.add(LSTM(100, activation = 'relu'))</p><p class="source-code">model.add(Dropout(0.2))</p><p class="source-code">model.add(Dense(1, activation='sigmoid'))</p></li>
				<li>Check the summary of the model using the <strong class="source-inline">summary()</strong> function: <p class="source-code">model.summary()</p><p>You should get the following output:</p><div id="_idContainer459" class="IMG---Figure"><img src="image/B16341_09_48.jpg" alt="Figure 9.48: Model summary&#13;&#10;"/></div><p class="figure-caption">Figure 9.48: Model summary</p></li>
				<li>Use the <strong class="source-inline">compile()</strong> method to configure your model for training. Select <strong class="source-inline">adam</strong> as your optimizer, <strong class="source-inline">binary_crossentropy</strong> to measure your loss function, and <strong class="source-inline">accuracy</strong> as the metric to be displayed:<p class="source-code">model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])</p></li>
				<li>Fit your model and set it to run on two epochs. Set your batch size to <strong class="source-inline">32</strong>:<p class="source-code">model.fit(X_train, y_train, epochs=2, batch_size=32)</p><p>You should get the following output:</p><div id="_idContainer460" class="IMG---Figure"><img src="image/B16341_09_49.jpg" alt="Figure 9. 49: Training the model&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9. 49: Training the model</p>
			<p>As you can see in <em class="italic">Figure 9.49</em>, your model achieved an accuracy of <strong class="source-inline">0.7978</strong> on the training set with minimal data preparation. You can try to improve this by removing stop words or extremely frequent words such as <strong class="source-inline">the</strong> and <strong class="source-inline">a</strong> that don't really help to assess the sentiment of a tweet and see if you can achieve the same performance on the testing set. You can deduce that the model can correctly predict almost 80% of the sentiments for the tweets in the training data.</p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor282"/>10. Custom TensorFlow Components</h1>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor283"/>Activity 10.01: Building a Model with Custom Layers and a Custom Loss Function</h2>
			<p><strong class="bold">Solution:</strong></p>
			<p>To get started, open a new Colab or Jupyter Notebook. If you are using Google Colab, you will need to download the dataset into your Google Drive first:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook or Google Colab notebook.</li>
				<li>If you are using Google Colab, you can upload your dataset locally with the following code. Otherwise, go to <em class="italic">step 4</em>. Click on <strong class="source-inline">Choose Files</strong> to navigate to the CSV file and click <strong class="source-inline">Open</strong>. Save the file as <strong class="source-inline">uploaded</strong>. Then, go to the folder where you saved the dataset:<p class="source-code">from google.colab import files</p><p class="source-code">uploaded = files.upload()</p></li>
				<li>Unzip the dataset in the current folder:<p class="source-code">!unzip \*.zip</p></li>
				<li>Create a variable, <strong class="source-inline">directory</strong>, that contains the path to the dataset:<p class="source-code">directory = "/content/gdrive/My Drive/Datasets/pneumonia-or-healthy/"</p></li>
				<li>Import all the required libraries:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import pathlib</p><p class="source-code">import os</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras import optimizers</p><p class="source-code">from tensorflow.keras.preprocessing.image import ImageDataGenerator</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import Input, Conv2D, ReLU, \</p><p class="source-code">    BatchNormalization,Add, AveragePooling2D, Flatten, Dense</p><p class="source-code">from tensorflow.keras.models import Model  </p></li>
				<li>Create a variable, <strong class="source-inline">path</strong>, that contains the full path to the data using <strong class="source-inline">pathlib.Path</strong>:<p class="source-code">path = pathlib.Path(directory)</p></li>
				<li>Create two variables, called <strong class="source-inline">train_dir</strong> and <strong class="source-inline">validation_dir</strong>, that take the full paths to the train and validation folders, respectively:<p class="source-code">train_dir = path / 'training_set'</p><p class="source-code">validation_dir = path / 'test_set'</p></li>
				<li>Create four variables, called <strong class="source-inline">train_table_dir</strong>, <strong class="source-inline">train_glass_dir</strong>, <strong class="source-inline">validation_table_dir</strong>, and <strong class="source-inline">validation_glass_dir</strong>, that take the full paths to the glass and table folders for the train and validation sets, respectively:<p class="source-code">train_table_dir = train_dir / 'table'</p><p class="source-code">train_glass_dir = train_dir /'glass'</p><p class="source-code">validation_table_dir = validation_dir / 'table'</p><p class="source-code">validation_glass_dir = validation_dir / 'glass'</p></li>
				<li>Create four variables that will contain the number of images of glasses and tables for the training and validation sets:<p class="source-code">num_train_table = len([f for f in os.listdir(train_table_dir)if \</p><p class="source-code">                       os.path.isfile(os.path.join\</p><p class="source-code">                                      (train_table_dir, f))])</p><p class="source-code">num_train_glass = len([f for f in os.listdir(train_glass_dir)if \</p><p class="source-code">                       os.path.isfile(os.path.join\</p><p class="source-code">                                      (train_glass_dir, f))])</p><p class="source-code">num_validation_table = len([f for f in os.listdir\</p><p class="source-code">                            (validation_table_dir)if</p><p class="source-code">os.path.isfile(os.path.join(validation_table_dir, f))])</p><p class="source-code">num_validation_glass = len([f for f in os.listdir\</p><p class="source-code">                            (validation_glass_dir)if \</p><p class="source-code">                            os.path.isfile\</p><p class="source-code">                            (os.path.join\</p><p class="source-code">                            (validation_glass_dir, f))])</p></li>
				<li>Display a bar chart with the total number of images of glasses and tables:<p class="source-code">plt.bar(['table', 'glass'], \</p><p class="source-code">        [num_train_table + num_validation_table, \</p><p class="source-code">         num_train_glass + num_validation_glass], \</p><p class="source-code">        align='center', \</p><p class="source-code">        alpha=0.5)</p><p class="source-code">plt.show()</p><p>You should get the following output:</p><div id="_idContainer461" class="IMG---Figure"><img src="image/B16341_10_12.jpg" alt="Figure 10.12: Number of images of glasses and tables&#13;&#10;"/></div><p class="figure-caption">Figure 10.12: Number of images of glasses and tables</p><p>The preceding chart shows you the dataset is well balanced. There are almost as many images of glasses as tables, around 3,500 images each.</p></li>
				<li>Create two variables, called <strong class="source-inline">total_train</strong> and <strong class="source-inline">total_val</strong>, that will get the number of images for the training and validation sets, respectively:<p class="source-code">total_train = len(os.listdir(train_table_dir)) + \</p><p class="source-code">              len(os.listdir(validation_table_dir))</p><p class="source-code">total_val = len(os.listdir(train_glass_dir)) + \</p><p class="source-code">            len(os.listdir(validation_glass_dir))</p></li>
				<li>Import the <strong class="source-inline">ImageDataGenerator</strong> class:<p class="source-code">from tensorflow.keras.preprocessing.image \</p><p class="source-code">    import ImageDataGenerator</p></li>
				<li>Instantiate two <strong class="source-inline">ImageDataGenerator</strong> classes, <strong class="source-inline">train_image_generator</strong> and <strong class="source-inline">validation_image_generator</strong>, that will rescale the images by dividing by 255:<p class="source-code">train_image_generator = ImageDataGenerator(rescale=1./255)</p><p class="source-code">validation_image_generator = ImageDataGenerator(rescale=1./255)</p></li>
				<li>Create three variables, called <strong class="source-inline">batch_size</strong>, <strong class="source-inline">img_height</strong>, and <strong class="source-inline">img_width</strong>, that take the values <strong class="source-inline">32</strong>, <strong class="source-inline">100</strong>, and <strong class="source-inline">100</strong>, respectively:<p class="source-code">batch_size = 32</p><p class="source-code">img_height = 100</p><p class="source-code">img_width = 100</p></li>
				<li>Create a data generator called <strong class="source-inline">train_data_gen</strong> using <strong class="source-inline">flow_from_directory()</strong> method and specify the batch size, the path to the training folder, the value of the <strong class="source-inline">shuffle</strong> parameter, the size of the target, and the class mode:<p class="source-code">train_data_gen = train_image_generator.flow_from_directory\</p><p class="source-code">                 (batch_size=batch_size, directory=train_dir, \</p><p class="source-code">                  shuffle=True, \</p><p class="source-code">                  target_size=(img_height, img_width), \</p><p class="source-code">                  class_mode='binary')</p></li>
				<li>Create a data generator called <strong class="source-inline">val_data_gen</strong> using <strong class="source-inline">flow_from_directory()</strong> method and specify the batch size, the path to the validation folder, the size of the target, and the class mode:<p class="source-code">val_data_gen = validation_image_generator.flow_from_directory\</p><p class="source-code">               (batch_size=batch_size, directory=validation_dir,\</p><p class="source-code">                target_size=(img_height, img_width), \</p><p class="source-code">                class_mode='binary')</p></li>
				<li>Create your custom loss function. Use <strong class="source-inline">def</strong> and choose a name for your custom loss, <strong class="source-inline">custom_loss_function</strong>, in this case. Then, add your two arguments, <strong class="source-inline">y_true</strong> and <strong class="source-inline">y_pred</strong>. Now, create a variable, <strong class="source-inline">squared_difference</strong>, to store the square of <strong class="source-inline">y_true</strong> minus <strong class="source-inline">y_pred</strong>. Finally, return the calculated loss using your <strong class="source-inline">tf.reduce_mean</strong> from <strong class="source-inline">squared_difference</strong>:<p class="source-code">def custom_loss_function(y_true, y_pred):</p><p class="source-code">    squared_difference = tf.square(float(y_true) - float(y_pred))</p><p class="source-code">    return tf.reduce_mean(squared_difference, axis=-1)</p></li>
				<li>Build a function that takes your input as a tensor and adds ReLU and batch normalization to it:<p class="source-code">def relu_batchnorm_layer(input):</p><p class="source-code">    return BatchNormalization()(ReLU()(input))</p></li>
				<li>Create a function to build the residual block. You will need to take a tensor as your input and pass it to two Conv2D layers. Next, add the input to the output, followed by ReLU and batch normalization.<p>Since you used an <strong class="source-inline">Add</strong> layer for the skip connection in your <strong class="source-inline">residual_block</strong>, you need to make sure that its inputs are always of the same shape. The <strong class="source-inline">downsample</strong> parameter is used to specify the strides of the first Conv2D layer. It specifies <strong class="source-inline">strides=2</strong> if <strong class="source-inline">True</strong> and <strong class="source-inline">strides=1</strong> if <strong class="source-inline">False</strong>. When <strong class="source-inline">strides=1</strong>, the output (<strong class="source-inline">int_output</strong>) is the same size as the input. But when <strong class="source-inline">strides=2</strong>, the dimensions of <strong class="source-inline">int_ouput</strong> are halved. To take this into account, add a Conv2D layer with <strong class="source-inline">kernel_size=1</strong> to the skip connection:</p><p class="source-code">def residual_block(input, downsample: bool, filters: int, \</p><p class="source-code">                   kernel_size: int = 3):</p><p class="source-code">    int_output = Conv2D(filters=filters, kernel_size=kernel_size, </p><p class="source-code">                        strides= (1 if not downsample else 2), </p><p class="source-code">                        padding="same")(input)</p><p class="source-code">    int_output = relu_batchnorm_layer(int_output)</p><p class="source-code">    int_output = Conv2D(filters=filters, kernel_size=kernel_size, </p><p class="source-code">                        padding="same")(int_output)</p><p class="source-code">    if downsample:</p><p class="source-code">        int_output2 = Conv2D(filters=filters, kernel_size=1, strides=2,</p><p class="source-code">                             padding="same")(input)</p><p class="source-code">        output = Add()([int_output2, int_output]) </p><p class="source-code">    else:</p><p class="source-code">        output = Add()([input, int_output])</p><p class="source-code">    output = relu_batchnorm_layer(output)</p><p class="source-code">    return output</p></li>
				<li>Now, use the <strong class="source-inline">keras.layers.Input()</strong> layer to define the input layer of your model. Here, your shape is 100 pixels by 100 pixels and has three colors (RGB). Then, create your model with your custom architecture. Finally, reference your input and output tensors with <strong class="source-inline">model = Model (inputs, outputs)</strong>:<p class="source-code">inputs = Input(shape=(100, 100, 3))</p><p class="source-code">num_filters = 32</p><p class="source-code">    </p><p class="source-code">t = BatchNormalization()(inputs)</p><p class="source-code">t = Conv2D(kernel_size=3,</p><p class="source-code">           strides=1,</p><p class="source-code">           filters=32,</p><p class="source-code">           padding="same")(t)</p><p class="source-code">t = relu_batchnorm_layer(t)</p><p class="source-code">    </p><p class="source-code">num_blocks_list = [1, 3, 5, 6, 1]</p><p class="source-code">for i in range(len(num_blocks_list)):</p><p class="source-code">    num_blocks = num_blocks_list[i]</p><p class="source-code">    for j in range(num_blocks):</p><p class="source-code">        t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)</p><p class="source-code">    num_filters *= 2</p><p class="source-code">    </p><p class="source-code">t = AveragePooling2D(4)(t)</p><p class="source-code">t = Flatten()(t)</p><p class="source-code">outputs = Dense(1, activation='sigmoid')(t)</p><p class="source-code">    </p><p class="source-code">model = Model(inputs, outputs)</p></li>
				<li>Get a summary of your model:<p class="source-code">model.summary()</p><p>The summary will be shown on running the preceding command:</p><div id="_idContainer462" class="IMG---Figure"><img src="image/B16341_10_13.jpg" alt="Figure 10.13: Model summary&#13;&#10;"/></div><p> </p><p class="figure-caption">Figure 10.13: Model summary</p></li>
				<li>Compile this model by providing your custom loss function, using Adam as the optimizer and accuracy as the metric to be displayed:<p class="source-code">model.compile(</p><p class="source-code">       optimizer='adam',</p><p class="source-code">       loss=custom_loss_function,</p><p class="source-code">       metrics=['accuracy']</p><p class="source-code">)</p></li>
				<li>Fit the model and provide the train and validation data generators, the number of epochs, the steps per epoch, and the validation steps:<p class="source-code">history = model.fit(</p><p class="source-code">    Train_data_gen,</p><p class="source-code">    steps_per_epoch=total_train // batch_size,</p><p class="source-code">    epochs=5,</p><p class="source-code">    validation_data=val_data_gen,</p><p class="source-code">    validation_steps=total_val // batch_size</p><p class="source-code">)</p><p>You should get the following output:</p><div id="_idContainer463" class="IMG---Figure"><img src="image/B16341_10_14.jpg" alt="Figure 10.14: Screenshot of the training progress&#13;&#10;"/></div><p class="figure-caption">Figure 10.14: Screenshot of the training progress</p><p>The preceding screenshot shows the information displayed by TensorFlow during the training of your model. You can see the accuracy achieved on the training and validation sets for each epoch. On the fifth epoch, the model is <strong class="source-inline">85.9%</strong> accurate on the training set and <strong class="source-inline">88.5%</strong> on the validation set.</p></li>
				<li>Plot your training and validation accuracy:<p class="source-code">plt.plot(history.history['accuracy'])</p><p class="source-code">plt.plot(history.history['val_accuracy'])</p><p class="source-code">plt.title('Training Accuracy vs Validation Accuracy')</p><p class="source-code">plt.ylabel('Accuracy')</p><p class="source-code">plt.xlabel('Epoch')</p><p class="source-code">plt.legend(['Train', 'Validation'], loc='upper left')</p><p class="source-code">plt.show()</p><p>You should get the following output:</p><div id="_idContainer464" class="IMG---Figure"><img src="image/B16341_10_15.jpg" alt="Figure 10.15: Training and validation accuracy&#13;&#10;"/></div><p class="figure-caption">Figure 10.15: Training and validation accuracy</p><p>The preceding chart shows the accuracy scores for the training and validation sets for each epoch.</p></li>
				<li>Plot your training and validation loss:<p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.title('Training Loss vs Validation Loss')</p><p class="source-code">plt.ylabel('Loss')</p><p class="source-code">plt.xlabel('Epoch')</p><p class="source-code">plt.legend(['Train', 'Validation'], loc='upper left')</p><p class="source-code">plt.show()</p><p>You should get the following output:</p><div id="_idContainer465" class="IMG---Figure"><img src="image/B16341_10_16.jpg" alt="Figure 10.16: Training and validation loss&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 10.16: Training and validation loss</p>
			<p>The preceding chart shows the loss scores for the training and validation sets for each epoch.</p>
			<p>With this activity, you have successfully built a custom MSE loss function and a custom residual block layer and trained this custom deep learning model on the glass versus table dataset. You now know how to go beyond the default classes offered by TensorFlow and build your own custom deep learning models.</p>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor284"/>11. Generative Models</h1>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor285"/>Activity 11.01: Generating Images Using GANs</h2>
			<p><strong class="bold">Solution</strong>:</p>
			<p>Perform the following steps to complete this activity:</p>
			<ol>
				<li value="1">Load Google Colab and Google Drive:<p class="source-code">try:</p><p class="source-code">    from google.colab import drive</p><p class="source-code">    drive.mount('/content/drive', force_remount=True)</p><p class="source-code">    COLAB = True</p><p class="source-code">    print("Note: using Google CoLab")</p><p class="source-code">    %tensorflow_version 2.x</p><p class="source-code">except:</p><p class="source-code">    print("Note: not using Google CoLab")</p><p class="source-code">    COLAB = False</p><p>Your output should look something like this:</p><p class="source-code">Mounted at /content/drive</p><p class="source-code">Note: using Google CoLab</p></li>
				<li>Import the libraries that you will be using:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.models import Sequential, Model, load_model</p><p class="source-code">from tensorflow.keras.layers import InputLayer, Reshape, Dropout, Dense </p><p class="source-code">from tensorflow.keras.layers import Flatten, BatchNormalization</p><p class="source-code">from tensorflow.keras.layers import UpSampling2D, Conv2D</p><p class="source-code">from tensorflow.keras.layers import Activation, ZeroPadding2D</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">from tensorflow.keras.layers import LeakyReLU</p><p class="source-code">import zipfile</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">from PIL import Image</p><p class="source-code">from tqdm import tqdm</p><p class="source-code">import os </p><p class="source-code">import time</p><p class="source-code">from skimage.io import imread</p></li>
				<li>Create a function to format a time string to track your time usage:<p class="source-code">def time_string(sec_elapsed):</p><p class="source-code">    hour = int(sec_elapsed / (60 * 60))</p><p class="source-code">    minute = int((sec_elapsed % (60 * 60)) / 60)</p><p class="source-code">    second = sec_elapsed % 60</p><p class="source-code">    return "{}:{:&gt;02}:{:&gt;05.2f}".format(hour, minute, second)</p></li>
				<li>Set the generation resolution to <strong class="source-inline">3</strong>. Also, set <strong class="source-inline">img_rows</strong> and <strong class="source-inline">img_cols</strong> to <strong class="source-inline">5</strong> and <strong class="source-inline">img_margin</strong> to <strong class="source-inline">16</strong> so that your preview images will be a <strong class="source-inline">5x5</strong> array (25 images) with a 16-pixel margin. Set <strong class="source-inline">seed_vector</strong> equal to <strong class="source-inline">200</strong>, <strong class="source-inline">data_path</strong> to where you stored your image dataset, and <strong class="source-inline">epochs</strong> to <strong class="source-inline">500</strong>. Finally, print the parameters:<p class="source-code">gen_res = 3 </p><p class="source-code">gen_square = 32 * gen_res</p><p class="source-code">img_chan = 3</p><p class="source-code">img_rows = 5</p><p class="source-code">img_cols = 5</p><p class="source-code">img_margin = 16</p><p class="source-code">seed_vector = 200</p><p class="source-code">data_path = 'banana-or-orange/training_set/'</p><p class="source-code">epochs = 500</p><p class="source-code">num_batch = 32</p><p class="source-code">num_buffer = 60000</p><p class="source-code">print(f"Will generate a resolution of {gen_res}.")</p><p class="source-code">print(f"Will generate {gen_square}px square images.")</p><p class="source-code">print(f"Will generate {img_chan} image channels.")</p><p class="source-code">print(f"Will generate {img_rows} preview rows.")</p><p class="source-code">print(f"Will generate {img_cols} preview columns.")</p><p class="source-code">print(f"Our preview margin equals {img_margin}.")</p><p class="source-code">print(f"Our data path is: {data_path}.")</p><p class="source-code">print(f"Our number of epochs are: {epochs}.")</p><p class="source-code">print(f"Will generate a batch size of {num_batch}.")</p><p class="source-code">print(f"Will generate a buffer size of {num_buffer}.")</p><p>Your output should look something like this:</p><div id="_idContainer466" class="IMG---Figure"><img src="image/B16341_11_30.jpg" alt="Figure 11.30: Output showing the parameters&#13;&#10;"/></div><p class="figure-caption">Figure 11.30: Output showing the parameters</p></li>
				<li>If a NumPy preprocessed file exists from prior execution, then load it into memory; otherwise, preprocess the data and save the image binary:<p class="source-code">training_binary_path = os.path.join(data_path,</p><p class="source-code">        f'training_data_{gen_square}_{gen_square}.npy')</p><p class="source-code">print(f"Looking for file: {training_binary_path}")</p><p class="source-code">if not os.path.isfile(training_binary_path):</p><p class="source-code">    start = time.time()</p><p class="source-code">    print("Loading training images…")</p><p class="source-code">    train_data = []</p><p class="source-code">    images_path = os.path.join(data_path,'banana')</p><p class="source-code">    for filename in tqdm(os.listdir(images_path)):</p><p class="source-code">        path = os.path.join(images_path,filename)</p><p class="source-code">        images = Image.open(path).resize((gen_square,</p><p class="source-code">                                          gen_square),\</p><p class="source-code">                                         Image.ANTIALIAS)</p><p class="source-code">        train_data.append(np.asarray(images))</p><p class="source-code">    train_data = np.reshape(train_data,(-1,gen_square,</p><p class="source-code">              gen_square,img_chan))</p><p class="source-code">    train_data = train_data.astype(np.float32)</p><p class="source-code">    train_data = train_data / 127–5 - 1.</p><p class="source-code">    print("Saving training image binary...")</p><p class="source-code">    np.save(training_binary_path,train_data)</p><p class="source-code">    elapsed = time.time()-start</p><p class="source-code">    print (f'Image preprocess time: {time_string(elapsed)}')</p><p class="source-code">else:</p><p class="source-code">    print("Loading training data...")</p><p class="source-code">    train_data = np.load(training_binary_path)</p></li>
				<li>Batch and shuffle the data. Use the <strong class="source-inline">tensorflow.data.Dataset</strong> object library to use its functions to shuffle the dataset and create batches:<p class="source-code">train_dataset = tf.data.Dataset.from_tensor_slices(train_data) \</p><p class="source-code">                       .shuffle(num_buffer).batch(num_batch)</p></li>
				<li>Build the generator for the DCGAN:<p class="source-code">def create_dc_generator(seed_size, channels):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(4*4*256,activation="relu",input_dim=seed_size))</p><p class="source-code">    model.add(Reshape((4,4,256)))</p><p class="source-code">    model.add(UpSampling2D())</p><p class="source-code">    model.add(Conv2D(256,kernel_size=3,padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(Activation("relu"))</p><p class="source-code">    model.add(UpSampling2D())</p><p class="source-code">    model.add(Conv2D(256,kernel_size=3,padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(Activation("relu"))</p><p class="source-code">   </p><p class="source-code">    # Output resolution, additional upsampling</p><p class="source-code">    model.add(UpSampling2D())</p><p class="source-code">    model.add(Conv2D(128,kernel_size=3,padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(Activation("relu"))</p><p class="source-code">    if gen_res&gt;1:</p><p class="source-code">        model.add(UpSampling2D(size=(gen_res,gen_res)))</p><p class="source-code">        model.add(Conv2D(128,kernel_size=3,padding="same"))</p><p class="source-code">        model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">        model.add(Activation("relu"))</p><p class="source-code">    # Final CNN layer</p><p class="source-code">    model.add(Conv2D(channels,kernel_size=3,padding="same"))</p><p class="source-code">    model.add(Activation("tanh"))</p><p class="source-code">    return model</p></li>
				<li>Build the discriminator for the DCGAN:<p class="source-code">def create_dc_discriminator(image_shape):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Conv2D(32, kernel_size=3, strides=2, \</p><p class="source-code">                     input_shape=image_shape, </p><p class="source-code">                     padding="same"))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))</p><p class="source-code">    model.add(ZeroPadding2D(padding=((0,1),(0,1))))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Conv2D(256, kernel_size=3, strides=1, padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Conv2D(512, kernel_size=3, strides=1, padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Flatten())</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    return model</p></li>
				<li>Build the generator for the vanilla GAN:<p class="source-code">def create_generator(seed_size, channels):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(96*96*3,activation="tanh",input_dim=seed_size))</p><p class="source-code">    model.add(Reshape((96,96,3)))</p><p class="source-code">    return model</p></li>
				<li>Build the discriminator for the vanilla GAN:<p class="source-code">def create_discriminator(img_size):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(InputLayer(input_shape=img_size))</p><p class="source-code">    model.add(Dense(1024, activation="tanh"))</p><p class="source-code">    model.add(Flatten())</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    return model</p></li>
				<li>Create a function to generate and save images that can be used to view progress during the model's training:<p class="source-code">def save_images(generator, cnt, noise, prefix=None):</p><p class="source-code">    img_array = np.full(( </p><p class="source-code">        img_margin + (img_rows * (gen_square+img_margin)), </p><p class="source-code">        img_margin + (img_cols * (gen_square+img_margin)), 3), </p><p class="source-code">        255, dtype=np.uint8)</p><p class="source-code">  </p><p class="source-code">    gen_imgs = generator.predict(noise)</p><p class="source-code">    gen_imgs = 0.5 * gen_imgs + 0.5</p><p class="source-code">    img_count = 0</p><p class="source-code">    for row in range(img_rows):</p><p class="source-code">        for col in range(img_cols):</p><p class="source-code">            r = row * (gen_square+16) + img_margin</p><p class="source-code">            c = col * (gen_square+16) + img_margin</p><p class="source-code">            img_array[r:r+gen_square,c:c+gen_square] \</p><p class="source-code">                = gen_imgs[img_count] * 255</p><p class="source-code">            img_count += 1</p><p class="source-code">          </p><p class="source-code">    output_path = os.path.join(data_path,'output')</p><p class="source-code">    if not os.path.exists(output_path):</p><p class="source-code">        os.makedirs(output_path)</p><p class="source-code">  </p><p class="source-code">    filename = os.path.join(output_path,f"train{prefix}-{cnt}.png")</p><p class="source-code">    im = Image.fromarray(img_array)</p><p class="source-code">    im.save(filename)</p></li>
				<li>Initialize the generator for the DCGAN and view the output:<p class="source-code">dc_generator = create_dc_generator(seed_vector, img_chan)</p><p class="source-code">noise = tf.random.normal([1, seed_vector])</p><p class="source-code">gen_img = dc_generator(noise, training=False)</p><p class="source-code">plt.imshow(gen_img[0, :, :, 0])</p><p>Your output should look something like this:</p><div id="_idContainer467" class="IMG---Figure"><img src="image/B16341_11_31.jpg" alt="Figure 11.31: Output showing noise from the DCGAN generator&#13;&#10;"/></div><p class="figure-caption">Figure 11.31: Output showing noise from the DCGAN generator</p></li>
				<li>Initialize the generator for the vanilla GAN and view the output:<p class="source-code">generator = create_generator(seed_vector, img_chan)</p><p class="source-code">gen_van_img = generator(noise, training=False)</p><p class="source-code">plt.imshow(gen_van_img[0, :, :, 0])</p><p>You should get the following output:</p><div id="_idContainer468" class="IMG---Figure"><img src="image/B16341_11_32.jpg" alt="Figure 11.31: Output showing noise from the DCGAN generator&#13;&#10;"/></div><p class="figure-caption">Figure 11.32: Output showing noise from the vanilla GAN generator</p></li>
				<li>Print the decision of the DCGAN discriminator evaluated on the seed image:<p class="source-code">img_shape = (gen_square,gen_square,img_chan)</p><p class="source-code">discriminator = create_discriminator(img_shape)</p><p class="source-code">decision = discriminator(gen_img)</p><p class="source-code">print (decision)</p><p>Your output should look something like this:</p><p class="source-code">tf.Tensor([[0.4994658]], shape=(1,1), dtype=float32)</p></li>
				<li>Print the decision of the vanilla GAN evaluated on the seed image:<p class="source-code">discriminator = create_discriminator(img_shape)</p><p class="source-code">decision = discriminator(gen_img)</p><p class="source-code">print(decision)</p><p>Your output should look something like this:</p><p class="source-code">tf.Tensor([[0.5055983]], shape=(1,1), dtype=float32)</p></li>
				<li>Create your loss functions. Since the output of both the discriminator and generator networks is different, you can define two separate loss functions for them. Moreover, they need to be trained separately in independent passes through the networks. Both GANs can utilize the same loss functions for their discriminators and generators. You can use <strong class="source-inline">tf.keras.losses.BinaryCrossentropy</strong> for <strong class="source-inline">cross_entropy</strong>. This calculates the loss between true and predicted labels. Then, define the <strong class="source-inline">discrim_loss</strong> function from <strong class="source-inline">real_output</strong> and <strong class="source-inline">fake_output</strong> using <strong class="source-inline">tf.ones</strong> and <strong class="source-inline">tf.zeros</strong> to calculate <strong class="source-inline">total_loss</strong>:<p class="source-code">cross_entropy = tf.keras.losses.BinaryCrossentropy()</p><p class="source-code">def discrim_loss(real_output, fake_output):</p><p class="source-code">    real_loss = cross_entropy(tf.ones_like(real_output), real_output)</p><p class="source-code">    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)</p><p class="source-code">    total_loss = real_loss + fake_loss</p><p class="source-code">    return total_loss</p><p class="source-code">def gen_loss(fake_output):</p><p class="source-code">    return cross_entropy(tf.ones_like(fake_output), fake_output)</p></li>
				<li>Create two Adam optimizers, one for the generator and one for the discriminator. Use the same learning rate and momentum for each:<p class="source-code">gen_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)</p><p class="source-code">disc_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)</p><p>Here, you have your individual training step. It's very important that you only modify one network's weights at a time. With <strong class="source-inline">tf.GradientTape()</strong>, you can train the discriminator and generator at the same time, but separately from one another. This is how TensorFlow does automatic differentiation. It calculates the derivatives. You'll see that it creates two "tapes" – <strong class="source-inline">gen_tape</strong> and <strong class="source-inline">disc_tape</strong>. Think of these as recordings of the calculations for each. </p></li>
				<li>Create <strong class="source-inline">real_output</strong> and <strong class="source-inline">fake_output</strong> for the discriminator. Use this for the generator loss (<strong class="source-inline">g_loss</strong>). Then, calculate the discriminator loss (<strong class="source-inline">d_loss</strong>) and the gradients of both the generator and discriminator with <strong class="source-inline">gradients_of_generator</strong> and <strong class="source-inline">gradients_of_discriminator</strong> and apply them. Encapsulate these steps within a function, passing in the generator, discriminator, and images, and returning the generator loss (<strong class="source-inline">g_loss</strong>) and discriminator loss (<strong class="source-inline">d_loss</strong>):<p class="source-code">@tf.function</p><p class="source-code">def train_step(generator, discriminator, images):</p><p class="source-code">    seed = tf.random.normal([num_batch, seed_vector])</p><p class="source-code">    with tf.GradientTape() as gen_tape, \</p><p class="source-code">         tf.GradientTape() as disc_tape:</p><p class="source-code">         gen_imgs = generator(seed, training=True)</p><p class="source-code">        real_output = discriminator(images, training=True)</p><p class="source-code">        fake_output = discriminator(gen_imgs, training=True)</p><p class="source-code">        g_loss = gen_loss(fake_output)</p><p class="source-code">        d_loss = discrim_loss(real_output, fake_output)</p><p class="source-code">    </p><p class="source-code">        gradients_of_generator = gen_tape.gradient(\</p><p class="source-code">            g_loss, generator.trainable_variables)</p><p class="source-code">        gradients_of_discriminator = disc_tape.gradient(\</p><p class="source-code">            d_loss, discriminator.trainable_variables)</p><p class="source-code">        gen_optimizer.apply_gradients(zip(</p><p class="source-code">            gradients_of_generator, generator.trainable_variables))</p><p class="source-code">        disc_optimizer.apply_gradients(zip(</p><p class="source-code">            gradients_of_discriminator, </p><p class="source-code">            discriminator.trainable_variables))</p><p class="source-code">    return g_loss,d_loss</p></li>
				<li>Create a number of fixed seeds with <strong class="source-inline">fixed_seeds</strong> equal to the number of images to display so that you can track the same images. This allows you to see how individual seeds evolve over time, tracking your time with <strong class="source-inline">for epoch in range</strong>. Now, loop through each batch with <strong class="source-inline">for image_batch in dataset</strong>. Continue to track your loss for both the generator and discriminator with <strong class="source-inline">generator_loss</strong> and <strong class="source-inline">discriminator_loss</strong>. Now, you have a nice display of all this information as it trains:<p class="source-code">def train(generator, discriminator, dataset, epochs, prefix=None):</p><p class="source-code">    fixed_seed = np.random.normal(0, 1, (img_rows * img_cols, </p><p class="source-code">                                         seed_vector))</p><p class="source-code">    start = time.time()</p><p class="source-code">    for epoch in range(epochs):</p><p class="source-code">         epoch_start = time.time()</p><p class="source-code">        g_loss_list = []</p><p class="source-code">        d_loss_list = []</p><p class="source-code">        for image_batch in dataset:</p><p class="source-code">            t = train_step(image_batch)</p><p class="source-code">            g_loss_list.append(t[0])</p><p class="source-code">            d_loss_list.append(t[1])</p><p class="source-code">        generator_loss = sum(g_loss_list) / len(g_loss_list)</p><p class="source-code">        discriminator_loss = sum(d_loss_list) / len(d_loss_list)</p><p class="source-code">        epoch_elapsed = time.time() - epoch_start</p><p class="source-code">        if (epoch + 1) % 100 == 0:</p><p class="source-code">            print (f'Epoch {epoch+1}, gen loss={generator_loss},</p><p class="source-code">        disc loss={discriminator_loss},'\</p><p class="source-code">                   f' {time_string(epoch_elapsed)}')</p><p class="source-code">        save_images(epoch,fixed_seed)</p><p class="source-code">    elapsed = time.time()-start</p><p class="source-code">    print (f'Training time: {time_string(elapsed)}')</p></li>
				<li>Train the DCGAN model on your training dataset:<p class="source-code">train(dc_generator, dc_discriminator, train_dataset, \</p><p class="source-code">      epochs, prefix='-dc-gan')</p><p>Your output should look something like this:</p><div id="_idContainer469" class="IMG---Figure"><img src="image/B16341_11_33.jpg" alt="Figure 11.33: Output during training of the DCGAN model&#13;&#10;"/></div><p class="figure-caption">Figure 11.33: Output during training of the DCGAN model</p><p>The output shows the loss for the generator and discriminator at each epoch.</p></li>
				<li>Train the vanilla model on your training dataset:<p class="source-code">train(generator, discriminator, train_dataset, epochs, \</p><p class="source-code">      prefix='-vanilla')</p><p>Your output should look something like this:</p><div id="_idContainer470" class="IMG---Figure"><img src="image/B16341_11_34.jpg" alt="Figure 11.34: Output during training of the vanilla GAN model&#13;&#10;"/></div><p class="figure-caption">Figure 11.34: Output during training of the vanilla GAN model</p></li>
				<li>View your images generated by the DCGAN model after the 100<span class="superscript">th</span> epoch:<p class="source-code">a = imread('banana-or-orange/training_set/output'\</p><p class="source-code">           '/train-dc-gan-99.png')</p><p class="source-code">plt.imshow(a)</p><p>You will get output like the following:</p><div id="_idContainer471" class="IMG---Figure"><img src="image/B16341_11_35.jpg" alt="Figure 11.35: Output images from the DCGAN model after 100 epochs&#13;&#10;"/></div><p class="figure-caption">Figure 11.35: Output images from the DCGAN model after 100 epochs</p></li>
				<li>View your images generated by the DCGAN model after the 500<span class="superscript">th</span> epoch:<p class="source-code">a = imread('/ banana-or-orange/training_set'\</p><p class="source-code">           '/output/train-dc-gan-499.png')</p><p class="source-code">plt.imshow(a)</p><p>You will get output like the following:</p><div id="_idContainer472" class="IMG---Figure"><img src="image/B16341_11_36.jpg" alt="Figure 11.36: Output images from the DCGAN model after 500 epochs&#13;&#10;"/></div><p class="figure-caption">Figure 11.36: Output images from the DCGAN model after 500 epochs</p></li>
				<li>View your images generated by the vanilla GAN model after the 100<span class="superscript">th</span> epoch: <p class="source-code">a = imread('banana-or-orange/training_set'\</p><p class="source-code">           '/output/train-vanilla-99.png')</p><p class="source-code">plt.imshow(a)</p><p>You will get output like the following:</p><div id="_idContainer473" class="IMG---Figure"><img src="image/B16341_11_37.jpg" alt="Figure 11.37: Output images from the vanilla GAN model after 100 epochs&#13;&#10;"/></div><p class="figure-caption">Figure 11.37: Output images from the vanilla GAN model after 100 epochs</p></li>
				<li>View your images generated by the vanilla GAN model after the 500<span class="superscript">th</span> epoch:<p class="source-code">a = imread('/ banana-or-orange/training_set'\</p><p class="source-code">           '/output/train-vanilla-499.png')</p><p class="source-code">plt.imshow(a)</p><p>You will get output like the following:</p><div id="_idContainer474" class="IMG---Figure"><img src="image/B16341_11_38.jpg" alt="Figure 11.38: Output images from the vanilla GAN model after 500 epochs&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.38: Output images from the vanilla GAN model after 500 epochs</p>
			<p>The output shows the images generated by the vanilla GAN after 500 epochs. You can see that they are very different from those generated by the DCGAN.</p>
			<p>You've just completed the last activity of the book. You created your own images with a DCGAN and compared them to a vanilla GAN model. As you can see from <em class="italic">Figure 11.36</em> and <em class="italic">Figure 11.38</em>, the results are very different from those of the DCGAN model, which were clearly recognizable as banana-like with different variations and orientations. With that model, though some images were more banana-like than others, all still exhibit at least some identifiable characteristics of bananas, such as color, shape, and presence of the black tip. The results from the vanilla GAN model, however, look more like pixel averages of the training dataset, which is overall not a good representation of real-life bananas. All images seem to have the same orientation, which may be another indicator that the results are more of a pixel average of the training data.</p>
		</div>
	

		<div>
			<div id="_idContainer476" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer479">
				<div id="_idContainer477">
					<img src="image/Matthew_Moocarme.png" alt="Rayon"/>
				</div>
				<div id="_idContainer478" class="Basic-Text-Frame">
					<p class="Paragraph-Style-1"><strong class="bold">Matthew Moocarme</strong></p>
				</div>
			</div>
		</div>
		<div>
			<div id="_idContainer482">
				<div id="_idContainer480">
					<img src="image/Anthony_So.png" alt="Rayon"/>
				</div>
				<div id="_idContainer481" class="Basic-Text-Frame">
					<p class="Paragraph-Style-1"><strong class="bold">Anthony So</strong></p>
				</div>
			</div>
		</div>
		<div>
			<div id="_idContainer485">
				<div id="_idContainer483">
					<img src="image/Anthony_Maddalone.png" alt="Rayon"/>
				</div>
				<div id="_idContainer484" class="Basic-Text-Frame">
					<p class="Paragraph-Style-1"><strong class="bold">Anthony Maddalone</strong></p>
				</div>
			</div>
		</div>
		<div>
			<div id="_idContainer486">
			</div>
		</div>
		<div>
			<div id="_idContainer487" class="Content">
			</div>
		</div>
		<div id="_idContainer489" class="Content">
			<h2 id="_idParaDest-257"><a id="_idTextAnchor286"/>Hey!</h2>
			<p>We're Matthew Moocarme, Anthony So, and Anthony Maddalone, the authors of this book. We really hope you enjoyed reading our book and found it useful for learning TensorFlow.</p>
			<p>It would really help us (and other potential readers!) if you could leave a review on Amazon sharing your thoughts on <em class="italic">The TensorFlow Workshop</em>.</p>
			<p>Go to the link <a href="https://packt.link/r/1800205252">https://packt.link/r/1800205252</a>.</p>
			<p>OR</p>
			<p>Scan the QR code to leave your review.</p>
			<div>
				<div id="_idContainer488" class="IMG---Figure">
					<img src="image/qr-code-https___packt.link_r_1800205252.jpg" alt="Barcode"/>
				</div>
			</div>
			<p>Your review will help us to understand what's worked well in this book and what could be improved upon for future editions, so it really is appreciated.</p>
			<p>Best wishes,</p>
			<p>Matthew Moocarme, Anthony So, and Anthony Maddalone</p>
		</div>
		<div>
			<div id="_idContainer490">
				<img src="image/Packt_Logo.png" alt="Packt Logo"/>
			</div>
		</div>
	</body></html>