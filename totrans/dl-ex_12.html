<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Neural Sentiment Analysis</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we are going to address one of the hot and trendy applications in natural language processing, which is called <strong class="calibre13">sentiment analysis</strong>. Most people nowadays express their opinions about something through social media platforms, and making use of this vast amount of text to keep track of customer satisfaction about something is very crucial for companies or even governments.<br class="calibre20"/></p>
<p class="calibre2">In this chapter, we are going to use recurrent-type neural networks to build a sentiment analysis solution. The following topics will be addressed in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">General sentiment analysis architecture</li>
<li class="calibre8">Sentiment analysis—model implementation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General sentiment analysis architecture</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to focus on the general deep learning architectures that can be used for sentiment analysis. The following figure shows the processing steps that are required for building the sentiment analysis model.</p>
<p class="calibre2">So, first off, we are going to deal with natural human language:</p>
<div class="CDPAlignCenter"><img src="assets/93bdac3a-0ed3-447d-84f2-8fae6e0ce5b0.png" class="calibre149"/></div>
<div class="CDPAlignCenter1">Figure 1: A general pipeline for sentiment analysis solutions or even sequence-based natural language solutions</div>
<p class="calibre2">We are going to use movie reviews to build this sentiment analysis application. The goal of this application is to produce positive and negative reviews based on the input raw text. For example, if the raw text is something like, <strong class="calibre13">This movie is good</strong>, then we need the model to produce a positive sentiment for it.</p>
<p class="calibre2">A sentiment analysis application will take us through a lot of processing steps that are needed to work with natural human languages inside a neural network such as embeddings.</p>
<p class="calibre2">So in this case, we have a raw text, for example, <strong class="calibre13">This is not a good movie!</strong> What we want to end up with is whether this is a negative or positive sentiment.</p>
<p class="calibre2">There are several difficulties in this type of application:</p>
<ul class="calibre7">
<li class="calibre8">One of them is that the sequences may have <strong class="calibre1">different lengths</strong>. This is a very short one, but we will see examples of text that have more than 500 words.</li>
<li class="calibre8">Another problem is that if we just look at individual words (for example, good), that indicates a positive sentiment. However, it is preceded by the word <strong class="calibre1">not</strong>, so now it's a negative sentiment. This can get a lot more complicated, and we will see an example of it later.</li>
</ul>
<p class="calibre2">As we learned in the previous chapter, a neural network cannot work on raw text, so we need to first convert it into what are called <strong class="calibre13">tokens</strong>. These are basically just integer values, so we go through our entire dataset and we count the number of times each word is being used. Then, we make a vocabulary and each word gets an index in this vocabulary. So the word <strong class="calibre13">this</strong> has an integer ID or token <strong class="calibre13">11</strong>, the word <strong class="calibre13">is</strong> has a token <strong class="calibre13">6</strong>, <strong class="calibre13">not</strong> has a token <strong class="calibre13">21</strong>, and so forth. So now, we have converted the raw text into a list of integers called tokens.</p>
<p class="calibre2">A neural network still cannot operate on this data, because if we have a vocabulary of 10,000 words, the tokens can take values between 0 and 9,999, and they may not be related at all. So, word number 998 may have a completely different semantic meaning than word number 999. </p>
<p class="calibre2">Therefore, we will use the idea of representation learning or embeddings that we learned about in the previous chapter. This embedding layer converts integer tokens into real-valued vectors, so token <strong class="calibre13">11</strong> becomes the vector [0.67,0.36,...,0.39], as shown in <em class="calibre19">Figure 1</em>. The same applies to the next token number 6.</p>
<p class="calibre2">A quick recap of what we studied in the previous chapter: this embedding layer in the preceding figure learns the mapping between tokens and their corresponding real-valued vector. Also, the embedding layer learns the semantic meanings of the words so that words that have similar meanings are somehow close to each other in this embedding space. </p>
<p class="calibre2">Out of the input raw text, we get a two-dimensional matrix, or tensor, which can now be inputted to the <strong class="calibre13">recurrent neural network</strong> <span class="calibre10">(</span><strong class="calibre13">RNN</strong><span class="calibre10">)</span>. This can process sequences of arbitrary length and the output of this network is then fed into a fully connected or dense layer with a sigmoid activation function. So, the output is between 0 and 1, where a value of 0 is taken to mean a negative sentiment. But what if the value of the sigmoid function is neither 0 nor 1? Then we need to introduce a cut-off or a threshold value in the middle so that if the value is below 0.5, then the corresponding input is taken to be a negative sentiment, and a value above this threshold is taken to be a positive sentiment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RNNs – sentiment analysis context</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, let's recap the basic concepts of RNNs and also talk about them in the context of the sentiment analysis application. As we mentioned in the RNN chapter, the basic building block of a RNN is a recurrent unit, as shown in this figure:</p>
<div class="CDPAlignCenter"><img src="assets/9b9d9701-39be-4d45-938e-abb4c10c7bc3.png" class="calibre150"/></div>
<div class="CDPAlignCenter1">Figure 2: An abstract idea of an RNN unit</div>
<p class="calibre2">This figure is an abstraction of what goes on inside the recurrent unit. What we have here is the input, so this would be a word, for example, <strong class="calibre13">good</strong>. Of course, it has to be converted to embedding vectors. However, we will ignore that for now. Also, this unit has a kind of memory state, and depending on the contents of this <strong class="calibre13">State</strong> and the <strong class="calibre13">Input</strong>, we will update this state and write new data into the state. For example, imagine that we have previously seen the word <strong class="calibre13">not</strong> in the input; we write that to the state so that when we see the word <strong class="calibre13">good</strong> on one of the following inputs, we know from the state that we have just seen the word <strong class="calibre13">not</strong>. Now, we see the word <strong class="calibre13">good</strong>. Thus, we have to write into the state that we have seen the words <strong class="calibre13">not good</strong> together so that this might indicate that the whole input text probably has a negative sentiment.</p>
<p class="calibre2">The mapping from the old state and the input to the new contents of the state is done through a so-called <strong class="calibre13">Gate</strong>, and the way these are implemented differs across different versions of recurrent units. It is basically a matrix operation with an activation function, but as we will see in a moment, there is a problem with backpropagating gradients. So, the RNN has to be designed in a special way so that the gradients are not distorted too much.</p>
<p class="calibre2">In a recurrent unit, we have a similar gate for producing the output, and once again the output of the recurrent unit depends on the current contents of the state and the input that we are seeing. So what we can try and do is unroll the processing that takes place with a recurrent unit:</p>
<div class="CDPAlignCenter"><img src="assets/5cb1e335-63bd-4471-b9c1-cff84f65718f.png" class="calibre151"/></div>
<div class="CDPAlignCenter1">Figure 3: Unrolled version of the recurrent neural net</div>
<p class="calibre2">Now, what we have here is just one recurrent unit, but the flow chart shows what happens at different time steps. So:</p>
<ul class="calibre7">
<li class="calibre8">In time step 1, we input the word <strong class="calibre1">this</strong> to the recurrent unit and it has its internal memory state first initialized to zero. This is done by TensorFlow whenever we start processing a new sequence of data. So, we see the word <strong class="calibre1">this</strong> and the recurrent unit state is 0. Hence, we use the internal gate to update the memory state and <strong class="calibre1">this</strong> is then used in time step number two where we input the word <strong class="calibre1">is</strong>; now, the memory state has some contents. There's not a whole lot of meaning in the word <strong class="calibre1">this</strong>, so the state might still be around 0.</li>
<li class="calibre8">And there's also not a lot of meaning in <strong class="calibre1">is</strong>, so perhaps the state is still somewhat 0.</li>
<li class="calibre8">In the next time step, we see the word <strong class="calibre1">not</strong>, and this has meaning we ultimately want to predict, which is the sentiment of the whole input text. This one is what we need to store in the memory so that the gate inside the recurrent unit sees that the state already probably contains near-zero values. But now it wants to store what we have just seen the word <strong class="calibre1">not</strong>, so it saves some nonzero value in this state.</li>
<li class="calibre8">Then, we move on to the next time step, where we have the word <strong class="calibre1">a</strong>; this also doesn't have much information, so it's probably just ignored. It just copies over the state.</li>
<li class="calibre8">Now, we have the word <strong class="calibre1">very</strong>, and this indicates that whatever sentiment exists might be a strong sentiment, so the recurrent unit now knows that we have seen <strong class="calibre1">not </strong>and <strong class="calibre1">very</strong>. It stores this somehow in its memory state.</li>
<li class="calibre8">In the next time step, we see the word <strong class="calibre1">good</strong>, so now the network knows <strong class="calibre1">not very good</strong> and it thinks, <em class="calibre25">Oh, this is probably a negative sentiment!</em> Hence, it stores that value in the internal state.</li>
<li class="calibre8">Then, in the final time step, we see <strong class="calibre1">movie</strong>, and this is not really relevant, so it's probably just ignored.</li>
<li class="calibre8">Next, we use the other gate inside the recurrent unit to output the contents of the memory state, and then it is processed with the sigmoid function (which we don't show here). We get an output value between 0 and 1.</li>
</ul>
<p class="calibre2">The idea <span class="calibre10">then</span><span class="calibre10"> </span><span class="calibre10">is that we want to train this network on many many thousands of examples of movie reviews from the Internet Movie database, where, for each input text, we give it the true sentiment value of either positive or negative. Then, we want TensorFlow to find out what the gates inside the recurrent unit should be so that they accurately map this input text to the correct sentiment:</span></p>
<div class="CDPAlignCenter"><img src="assets/adf86d1b-4eba-43b5-b736-15177296aad2.png" class="calibre152"/></div>
<div class="CDPAlignCenter1">Figure 4: Used architecture for this chapter's implementation</div>
<p class="calibre2">The architecture for the RNN we will be using in this implementation is an RNN-type architecture with three layers. In the first layer, what we've just explained <span class="calibre10">happens,</span><span class="calibre10"> </span><span class="calibre10">except that now we need to output the value from the recurrent unit at each time step. Then, we gather a new sequence of data, which is the output of the first recurrent layer. Next, we can input it to the second recurrent layer because recurrent units need sequences of input data (and the output that we got from the first layer and the one that we want to feed into the second recurrent layer are some floating-point values whose <span class="calibre10">meanings </span>we don't really understand). This has a meaning inside the RNN, but it's not something we as humans will understand. Then, we do similar processing in the second recurrent layer.</span></p>
<p class="calibre2">So, first, we initialize the internal memory state of this recurrent unit to 0; then, we take the first output from the first recurrent layer and input it. We process it with the gates inside this recurrent unit, update the state, take the output of the first layer's recurrent unit for the second word <strong class="calibre13">is</strong>, and use that as input as well as the internal memory state. We continue doing this until we have processed the whole sequence, and then we gather up all the outputs of the second recurrent layer. We use them as inputs in the third recurrent layer, where we do a similar processing. But here, we only want the output for the last time step, which is a kind of summary for everything that has been fed so far. We then output that to a fully connected layer that we don't show here. Finally, we have the sigmoid activation function, so we get a value between zero and one, which represents negative and positive sentiments, respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploding and vanishing gradients - recap</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we mentioned in the previous chapter, there's a phenomenon called <strong class="calibre13">exploding</strong> and <strong class="calibre13">vanishing</strong> of gradients values, which is very important in RNNs. Let's go back and look at <em class="calibre19">Figure 1</em>; that flowchart explains what this phenomenon is.</p>
<p class="calibre2">Imagine we have a text with 500 words in this dataset that we will be using to implement our sentiment analysis classifier. At every time step, we apply the internal gates in the recurrent unit in a recursive manner; so if there are 500 words, we will apply these gates 500 times to update the internal memory state of the recurrent unit.</p>
<p class="calibre2">As we know, the way neural networks are trained is by using so-called backpropagation of gradients, so we have some loss function that gets the output of the neural network and then the true output that we desire for the given input text. Then, we want to minimize this loss value so that the actual output of the neural network corresponds to the desired output for this particular input text. So, we need to take the gradient of this loss function with respect to the weights inside these recurrent units, and these weights are for the gates that are updating the internal state and outputting the value in the end. </p>
<p class="calibre2">Now, the gate is applied maybe 500 times, and if this has a multiplication in it, what we essentially get is an exponential function. So, if you multiply a value with itself 500 times and if this value is slightly less than 1, then it will very quickly vanish or get lost. Similarly, if a value slightly more than 1 is multiplied with itself 500 times, it'll explode.</p>
<p class="calibre2">The only values that can survive 500 multiplications are 0 and 1. They will remain the same, so the recurrent unit is actually much more complicated than what you see here. This is the abstract idea—that we want to somehow map the internal memory state and the input to update the internal memory state and to output some value—but in reality, we need to be very careful about propagating the gradients backwards through these gates so that we don't have this exponential multiplication over many many time steps. We also encourage you to see some tutorials on the mathematical definition of recurrent units.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis – model implementation</h1>
                </header>
            
            <article>
                
<p class="calibre2">We have seen all the bits and pieces of how to implement a stacked version of the LSTM variation of RNNs. To make things a bit exciting, we are going to use a higher level API called <kbd class="calibre12">Keras</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keras</h1>
                </header>
            
            <article>
                
<div class="packtquote">"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research." – Keras website</div>
<p class="calibre2">So, Keras is just a wrapper around TensorFlow and other deep learning frameworks. It's really good for prototyping and getting things built very quickly, but on the other hand, it gives you less control over your code. We'll take a chance to implement this sentiment analysis model in Keras so that you get a hands-on implementation in both TensorFlow and Keras. You can use Keras for fast prototyping and TensorFlow for a production-ready system.</p>
<p class="calibre2">More interesting news for you is that you don't have to switch to a totally different environment. You can now access Keras as a module in TensorFlow and import packages just like the following:</p>
<pre class="calibre21">from tensorflow.python.keras.models <br class="title-page-name"/>import Sequential<br class="title-page-name"/>from tensorflow.python.keras.layers <br class="title-page-name"/>import Dense, GRU, Embedding<br class="title-page-name"/>from tensorflow.python.keras.optimizers <br class="title-page-name"/>import Adam<br class="title-page-name"/>from tensorflow.python.keras.preprocessing.text <br class="title-page-name"/>import Tokenizer<br class="title-page-name"/>from tensorflow.python.keras.preprocessing.sequence <br class="title-page-name"/>import pad_sequences</pre>
<p class="calibre2">So, let's go ahead and use what we can <span class="calibre10">now</span><span class="calibre10"> </span><span class="calibre10">call a more abstracted module inside TensorFlow that will help us to prototype deep learning solutions very fast. This is because we will get to write full deep learning solutions in just a few lines of code.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis and preprocessing</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, let's move on to the actual implementation where we need to load the data. Keras actually has a functionality that can be used to load this sentiment dataset from IMDb, but the problem is that it has already mapped all the words to integer tokens. This is such an essential part of working with natural human language insight neural networks that I really want to show you how to do it.</p>
<p class="calibre2">Also, if you want to use this code for sentiment analysis of whatever data you might have in some other language, you will need to do this yourself, so we have just quickly implemented some functions for downloading this dataset.</p>
<p class="calibre2">Let's start off by importing a bunch of required packages:</p>
<pre class="calibre21">%matplotlib inline<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from scipy.spatial.distance import cdist<br class="title-page-name"/>from tensorflow.python.keras.models import Sequential<br class="title-page-name"/>from tensorflow.python.keras.layers import Dense, GRU, Embedding<br class="title-page-name"/>from tensorflow.python.keras.optimizers import Adam<br class="title-page-name"/>from tensorflow.python.keras.preprocessing.text import Tokenizer<br class="title-page-name"/>from tensorflow.python.keras.preprocessing.sequence import pad_sequences</pre>
<p class="calibre2">And then we load the dataset:</p>
<pre class="calibre21">import imdb<br class="title-page-name"/>imdb.maybe_download_and_extract()<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>- Download progress: 100.0%<br class="title-page-name"/>Download finished. Extracting files.<br class="title-page-name"/>Done.</pre>
<pre class="calibre21">input_text_train, target_train = imdb.load_data(train=True)<br class="title-page-name"/>input_text_test, target_test = imdb.load_data(train=False)</pre>
<pre class="calibre21">print("Size of the trainig set: ", len(input_text_train))<br class="title-page-name"/>print("Size of the testing set:  ", len(input_text_test))<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>Size of the trainig set: 25000<br class="title-page-name"/>Size of the testing set: 25000</pre>
<p class="calibre2">As you can see, it has 25,000 texts in the training set and in the testing set.</p>
<p class="calibre2">Let's just see one example from the training set and how it looks:</p>
<pre class="calibre21">#combine dataset<br class="title-page-name"/>text_data = input_text_train + input_text_test<br class="title-page-name"/>input_text_train[1]<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>'This is a really heart-warming family movie. It has absolutely brilliant animal training and "acting" (if you can call it like that) as well (just think about the dog in "How the Grinch stole Christmas"... it was plain bad training). The Paulie story is extremely well done, well reproduced and in general the characters are really elaborated too. Not more to say except that this is a GREAT MOVIE!&lt;br /&gt;&lt;br /&gt;My ratings: story 8.5/10, acting 7.5/10, animals+fx 8.5/10, cinematography 8/10.&lt;br /&gt;&lt;br /&gt;My overall rating: 8/10 - BIG FAMILY MOVIE AND VERY WORTH WATCHING!'<br class="title-page-name"/><br class="title-page-name"/>target_train[1]<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>1.0</pre>
<p class="calibre2">This is a fairly short one and the sentiment value is <kbd class="calibre12">1.0</kbd>, which means it is a positive sentiment, so this is a positive review of whatever movie this was about.</p>
<p class="calibre2">Now, we get to the tokenizer, and this is the first step of processing this raw data because the neural network cannot work on text data. Keras has implemented what is called a <strong class="calibre13">tokenizer</strong> for building a vocabulary and mapping from words to an integer.</p>
<p class="calibre2">Also, we can say that we want a maximum of 10,000 words, so it will use <span class="calibre10">only </span>the 10,000 most popular words from the dataset:</p>
<pre class="calibre21">num_top_words = 10000<br class="title-page-name"/>tokenizer_obj = Tokenizer(num_words=num_top_words)</pre>
<p class="calibre2">Now, we take all the text from the dataset and we call this function <kbd class="calibre12">fit</kbd> on texts:</p>
<pre class="calibre21">tokenizer_obj.fit_on_texts(text_data)</pre>
<p class="calibre2">The tokenizer takes about 10 seconds, and then it will have built the vocabulary. It looks like this:</p>
<pre class="calibre21">tokenizer_obj.word_index<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>{'britains': 33206,<br class="title-page-name"/> 'labcoats': 121364,<br class="title-page-name"/> 'steeled': 102939,<br class="title-page-name"/> 'geddon': 67551,<br class="title-page-name"/> "rossilini's": 91757,<br class="title-page-name"/> 'recreational': 27654,<br class="title-page-name"/> 'suffices': 43205,<br class="title-page-name"/> 'hallelujah': 30337,<br class="title-page-name"/> 'mallika': 30343,<br class="title-page-name"/> 'kilogram': 122493,<br class="title-page-name"/> 'elphic': 104809,<br class="title-page-name"/> 'feebly': 32818,<br class="title-page-name"/> 'unskillful': 91728,<br class="title-page-name"/> "'mistress'": 122218,<br class="title-page-name"/> "yesterday's": 25908,<br class="title-page-name"/> 'busco': 85664,<br class="title-page-name"/> 'goobacks': 85670,<br class="title-page-name"/> 'mcfeast': 71175,<br class="title-page-name"/> 'tamsin': 77763,<br class="title-page-name"/> "petron's": 72628,<br class="title-page-name"/> "'lion": 87485,<br class="title-page-name"/> 'sams': 58341,<br class="title-page-name"/> 'unbidden': 60042,<br class="title-page-name"/> "principal's": 44902,<br class="title-page-name"/> 'minutiae': 31453,<br class="title-page-name"/> 'smelled': 35009,<br class="title-page-name"/> 'history\x97but': 75538,<br class="title-page-name"/> 'vehemently': 28626,<br class="title-page-name"/> 'leering': 14905,<br class="title-page-name"/> 'kýnay': 107654,<br class="title-page-name"/> 'intendend': 101260,<br class="title-page-name"/> 'chomping': 21885,<br class="title-page-name"/> 'nietsze': 76308,<br class="title-page-name"/> 'browned': 83646,<br class="title-page-name"/> 'grosse': 17645,<br class="title-page-name"/> "''gaslight''": 74713,<br class="title-page-name"/> 'forseeing': 103637,<br class="title-page-name"/> 'asteroids': 30997,<br class="title-page-name"/> 'peevish': 49633,<br class="title-page-name"/> "attic'": 120936,<br class="title-page-name"/> 'genres': 4026,<br class="title-page-name"/> 'breckinridge': 17499,<br class="title-page-name"/> 'wrist': 13996,<br class="title-page-name"/> "sopranos'": 50345,<br class="title-page-name"/> 'embarasing': 92679,<br class="title-page-name"/> "wednesday's": 118413,<br class="title-page-name"/> 'cervi': 39092,<br class="title-page-name"/> 'felicity': 21570,<br class="title-page-name"/> "''horror''": 56254,<br class="title-page-name"/> 'alarms': 17764,<br class="title-page-name"/> "'ol": 29410,<br class="title-page-name"/> 'leper': 27793,<br class="title-page-name"/> 'once\x85': 100641,<br class="title-page-name"/> 'iverson': 66834,<br class="title-page-name"/> 'triply': 117589,<br class="title-page-name"/> 'industries': 19176,<br class="title-page-name"/> 'brite': 16733,<br class="title-page-name"/> 'amateur': 2459,<br class="title-page-name"/> "libby's": 46942,<br class="title-page-name"/> 'eeeeevil': 120413,<br class="title-page-name"/> 'jbc33': 51111,<br class="title-page-name"/> 'wyoming': 12030,<br class="title-page-name"/> 'waned': 30059,<br class="title-page-name"/> 'uchida': 63203,<br class="title-page-name"/> 'uttter': 93299,<br class="title-page-name"/> 'irector': 123847,<br class="title-page-name"/> 'outriders': 95156,<br class="title-page-name"/> 'perd': 118465,<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.}</pre>
<p class="calibre2">So, each word is now associated with an integer; therefore, the word <kbd class="calibre12">the</kbd> has number <kbd class="calibre12">1</kbd>:</p>
<pre class="calibre21">tokenizer_obj.word_index['the']<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>1</pre>
<p class="calibre2">Here, <kbd class="calibre12">and</kbd> has number <kbd class="calibre12">2</kbd>:</p>
<pre class="calibre21">tokenizer_obj.word_index['and']<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>2</pre>
<p class="calibre2">The word <kbd class="calibre12">a</kbd> has <kbd class="calibre12">3</kbd>:</p>
<pre class="calibre21">tokenizer_obj.word_index['a']<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>3</pre>
<p class="calibre2">And so on. We see that <kbd class="calibre12">movie</kbd> has number <kbd class="calibre12">17</kbd>:</p>
<pre class="calibre21">tokenizer_obj.word_index['movie']<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>17</pre>
<p class="calibre2">And <kbd class="calibre12">film</kbd> has number <kbd class="calibre12">19</kbd>:</p>
<pre class="calibre21">tokenizer_obj.word_index['film']<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>19</pre>
<p class="calibre2">What all <span class="calibre10">this </span>means is that <kbd class="calibre12">the</kbd> was the most used word in the dataset and <kbd class="calibre12">and</kbd> was the second most used in the dataset. So, whenever we want to map words to integer tokens, we will get these numbers.</p>
<p class="calibre2">Let's try and take the word number <kbd class="calibre12">743</kbd> for example, and this was the word <kbd class="calibre12">romantic</kbd>:</p>
<pre class="calibre21">tokenizer_obj.word_index['romantic']<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>743</pre>
<p class="calibre2">So, whenever we see the word <kbd class="calibre12">romantic</kbd> in the input text, we map it to the token integer <kbd class="calibre12">743</kbd>. We use the tokenizer again to convert all the words in the text in the training set into integer tokens:</p>
<pre class="calibre21">input_text_train[1]<br class="title-page-name"/>Output:<br class="title-page-name"/>'This is a really heart-warming family movie. It has absolutely brilliant animal training and "acting" (if you can call it like that) as well (just think about the dog in "How the Grinch stole Christmas"... it was plain bad training). The Paulie story is extremely well done, well reproduced and in general the characters are really elaborated too. Not more to say except that this is a GREAT MOVIE!&lt;br /&gt;&lt;br /&gt;My ratings: story 8.5/10, acting 7.5/10, animals+fx 8.5/10, cinematography 8/10.&lt;br /&gt;&lt;br /&gt;My overall rating: 8/10 - BIG FAMILY MOVIE AND VERY WORTH WATCHING!</pre>
<p class="calibre2">When we convert that text to integer tokens, it becomes an array of integers:</p>
<pre class="calibre21">np.array(input_train_tokens[1])<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>array([ 11, 6, 3, 62, 488, 4679, 236, 17, 9, 45, 419,<br class="title-page-name"/>        513, 1717, 2425, 2, 113, 43, 22, 67, 654, 9, 37,<br class="title-page-name"/>         12, 14, 69, 39, 101, 42, 1, 826, 8, 85, 1,<br class="title-page-name"/>       6418, 3492, 1156, 9, 13, 1042, 74, 2425, 1, 6419, 64,<br class="title-page-name"/>          6, 568, 69, 221, 69, 2, 8, 825, 1, 102, 23,<br class="title-page-name"/>         62, 96, 21, 51, 5, 131, 556, 12, 11, 6, 3,<br class="title-page-name"/>         78, 17, 7, 7, 56, 2818, 64, 723, 447, 156, 113,<br class="title-page-name"/>        702, 447, 156, 1598, 3611, 723, 447, 156, 633, 723, 156,<br class="title-page-name"/>          7, 7, 56, 437, 670, 723, 156, 191, 236, 17, 2,<br class="title-page-name"/>         52, 278, 147])</pre>
<p class="calibre2">So, the word <kbd class="calibre12">this</kbd> becomes the number 11, the word <kbd class="calibre12">is</kbd> becomes the number 59, and so forth.</p>
<p class="calibre2">We also need to convert the rest of the text:</p>
<pre class="calibre21">input_test_tokens = tokenizer_obj.texts_to_sequences(input_text_test)</pre>
<p class="calibre2">Now, there's another problem because the sequences of tokens have different lengths depending on the length of the original text, even though the recurrent units can work with sequences of arbitrary length. But the way that TensorFlow works is that all of the data in a batch needs to have the same length.</p>
<p class="calibre2">So, we can either ensure that all sequences in the entire dataset have the same length, or write a custom data generator that ensures that the sequences in a single batch have the same length. Now, it is a lot simpler to ensure that all the sequences in the dataset have the same length, but the problem is that there are some outliers. We have some sentences that, I think, are more than 2,200 words long. It will hurt our memory very much if we have all the <em class="calibre19">short</em> sentences with more than 2,200 words. So what we will do instead is make a compromise; first, we need to count all the words, or the number of tokens in each of these input sequences. What we see is that the average number of words in a sequence is about 221:</p>
<pre class="calibre21">total_num_tokens = [len(tokens) for tokens in input_train_tokens + input_test_tokens]<br class="title-page-name"/>total_num_tokens = np.array(total_num_tokens)<br class="title-page-name"/><br class="title-page-name"/>#Get the average number of tokens<br class="title-page-name"/>np.mean(total_num_tokens)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>221.27716</pre>
<p class="calibre2">And we see that the maximum number of words is more than 2,200:</p>
<pre class="calibre21">np.max(total_num_tokens)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>2208</pre>
<p class="calibre2">Now, there's a huge difference between the average and the max, and again we would be wasting a lot of memory if we just padded all the sentences in the dataset so that they would all have <kbd class="calibre12">2208</kbd> tokens. This would especially be a problem if you have a dataset with millions of sequences of text. </p>
<p class="calibre2">So what we will do is make a compromise where we will pad all sequences and truncate the ones that are too long so that they have <kbd class="calibre12">544</kbd> words. The way we calculated this was like this—we took the average number of words in all the sequences in the dataset and we added two standard deviations:</p>
<pre class="calibre21">max_num_tokens = np.mean(total_num_tokens) + 2 * np.std(total_num_tokens)<br class="title-page-name"/>max_num_tokens = int(max_num_tokens)<br class="title-page-name"/>max_num_tokens<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>544</pre>
<p class="calibre2">What do we get out of this is? We cover about 95% of the text in the dataset, so only about 5% are longer than <kbd class="calibre12">544</kbd> words:</p>
<pre class="calibre21">np.sum(total_num_tokens &lt; max_num_tokens) / len(total_num_tokens)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>0.94532</pre>
<p class="calibre2">Now, we call these functions in Keras. They will either pad the sequences that are too short (so they will just add zeros) or truncate the sequences that are too long (basically just cut off some of the words if the text is too long).</p>
<p class="calibre2">Now, there's an important thing here: we can do this padding and truncating in pre or post mode. So imagine we have a sequence of integer tokens and we want to pad this because it's too short. We can:</p>
<ul class="calibre7">
<li class="calibre8">Either pad all of these zeros at the beginning so that we have the actual integer tokens down at the end.</li>
<li class="calibre8">Or do it in the opposite way so that we have all this data at the beginning and then all the zeros at the end. But if we just go back and look at the preceding RNN flowchart, remember that it is processing the sequence one step at a time so if we start processing zeros, it will probably not mean anything and the internal state would have probably just remain zero. So, whenever it finally sees an integer token for a specific word, it will know okay now we start processing the data.</li>
</ul>
<p class="calibre2">However, if all the zeros were at the end, we would have started processing all the data; then we'd have some internal state inside the recurrent unit. Right now, we see a whole lot of zeros, so that might actually destroy the internal state that we have just calculated. This is why it might be a good idea to have the zeros padded at the beginning.</p>
<p class="calibre2">But the other problem is when we truncate a text, so if the text is very long, we will truncate it to get it to fit to <kbd class="calibre12">544</kbd> words, or whatever the number was. Now, imagine we've caught this sentence here in the middle somewhere and it says <strong class="calibre13">this very good movie</strong> or <strong class="calibre13">this is not</strong>. You know, of course, <span class="calibre10">that </span>we do this <span class="calibre10">only </span>for very long sequences, but it is possible that we lose <span class="calibre10">essential information</span> for properly classifying this text. So it is a compromise that we're making when we are truncating input text. A better way would be to create a batch and just pad text inside that batch. So, when we see a very very long sequence, we pad the other sequences to have the same length. But we don't need to store all of this data in memory because most of it is wasted.</p>
<p class="calibre2">Let's go back and convert the entire dataset so that it is truncated and padded; thus, it's one big matrix of data:</p>
<pre class="calibre21">seq_pad = 'pre'<br class="title-page-name"/><br class="title-page-name"/>input_train_pad = pad_sequences(input_train_tokens, maxlen=max_num_tokens,<br class="title-page-name"/> padding=seq_pad, truncating=seq_pad)<br class="title-page-name"/><br class="title-page-name"/>input_test_pad = pad_sequences(input_test_tokens, maxlen=max_num_tokens,<br class="title-page-name"/> padding=seq_pad, truncating=seq_pad)</pre>
<p class="calibre2">We check the shape of this matrix:</p>
<pre class="calibre21">input_train_pad.shape<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>(25000, 544)<br class="title-page-name"/><br class="title-page-name"/>input_test_pad.shape<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>(25000, 544)</pre>
<p class="calibre2">So, let's have a look at specific sample tokens before and after padding:</p>
<pre class="calibre21">np.array(input_train_tokens[1])<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>array([ 11, 6, 3, 62, 488, 4679, 236, 17, 9, 45, 419,<br class="title-page-name"/>        513, 1717, 2425, 2, 113, 43, 22, 67, 654, 9, 37,<br class="title-page-name"/>         12, 14, 69, 39, 101, 42, 1, 826, 8, 85, 1,<br class="title-page-name"/>       6418, 3492, 1156, 9, 13, 1042, 74, 2425, 1, 6419, 64,<br class="title-page-name"/>          6, 568, 69, 221, 69, 2, 8, 825, 1, 102, 23,<br class="title-page-name"/>         62, 96, 21, 51, 5, 131, 556, 12, 11, 6, 3,<br class="title-page-name"/>         78, 17, 7, 7, 56, 2818, 64, 723, 447, 156, 113,<br class="title-page-name"/>        702, 447, 156, 1598, 3611, 723, 447, 156, 633, 723, 156,<br class="title-page-name"/>          7, 7, 56, 437, 670, 723, 156, 191, 236, 17, 2,<br class="title-page-name"/>         52, 278, 147])</pre>
<p class="calibre2">And after padding, this sample will look like the following:</p>
<pre class="calibre21">input_train_pad[1]<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>array([ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br class="title-page-name"/>          0, 0, 11, 6, 3, 62, 488, 4679, 236, 17, 9,<br class="title-page-name"/>         45, 419, 513, 1717, 2425, 2, 113, 43, 22, 67, 654,<br class="title-page-name"/>          9, 37, 12, 14, 69, 39, 101, 42, 1, 826, 8,<br class="title-page-name"/>         85, 1, 6418, 3492, 1156, 9, 13, 1042, 74, 2425, 1,<br class="title-page-name"/>       6419, 64, 6, 568, 69, 221, 69, 2, 8, 825, 1,<br class="title-page-name"/>        102, 23, 62, 96, 21, 51, 5, 131, 556, 12, 11,<br class="title-page-name"/>          6, 3, 78, 17, 7, 7, 56, 2818, 64, 723, 447,<br class="title-page-name"/>        156, 113, 702, 447, 156, 1598, 3611, 723, 447, 156, 633,<br class="title-page-name"/>        723, 156, 7, 7, 56, 437, 670, 723, 156, 191, 236,<br class="title-page-name"/>         17, 2, 52, 278, 147], dtype=int32)</pre>
<p class="calibre2">Also, we need a functionality to map backwards so that it maps from integer tokens back to text words; we just need that here. It's a very simple helper function, so let's go ahead and implement it:</p>
<pre class="calibre21">index = tokenizer_obj.word_index<br class="title-page-name"/>index_inverse_map = dict(zip(index.values(), index.keys()))</pre>
<pre class="calibre21">def convert_tokens_to_string(input_tokens):<br class="title-page-name"/> <br class="title-page-name"/> # Convert the tokens back to words<br class="title-page-name"/> input_words = [index_inverse_map[token] for token in input_tokens if token != 0]<br class="title-page-name"/> <br class="title-page-name"/> # join them all words.<br class="title-page-name"/> combined_text = " ".join(input_words)<br class="title-page-name"/><br class="title-page-name"/>return combined_text</pre>
<p class="calibre2">Now, for example, the original text in the dataset is like this:</p>
<pre class="calibre21">input_text_train[1]<br class="title-page-name"/>Output:<br class="title-page-name"/><br class="title-page-name"/>input_text_train[1]<br class="title-page-name"/><br class="title-page-name"/>'This is a really heart-warming family movie. It has absolutely brilliant animal training and "acting" (if you can call it like that) as well (just think about the dog in "How the Grinch stole Christmas"... it was plain bad training). The Paulie story is extremely well done, well reproduced and in general the characters are really elaborated too. Not more to say except that this is a GREAT MOVIE!&lt;br /&gt;&lt;br /&gt;My ratings: story 8.5/10, acting 7.5/10, animals+fx 8.5/10, cinematography 8/10.&lt;br /&gt;&lt;br /&gt;My overall rating: 8/10 - BIG FAMILY MOVIE AND VERY WORTH WATCHING!'</pre>
<p class="calibre2">If we use a helper function to convert the tokens back to text words, we get this text:</p>
<pre class="calibre21">convert_tokens_to_string(input_train_tokens[1])<br class="title-page-name"/><br class="title-page-name"/>'this is a really heart warming family movie it has absolutely brilliant animal training and acting if you can call it like that as well just think about the dog in how the grinch stole christmas it was plain bad training the paulie story is extremely well done well and in general the characters are really too not more to say except that this is a great movie br br my ratings story 8 5 10 acting 7 5 10 animals fx 8 5 10 cinematography 8 10 br br my overall rating 8 10 big family movie and very worth watching'</pre>
<p class="calibre2">It's basically the same except for punctuation and other symbols.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, we need to create the RNN, and we will do this in Keras because it's very simple. We do that with the so-called <kbd class="calibre12">sequential</kbd> model. </p>
<p class="calibre2">The first layer of this architecture will be what is called an <strong class="calibre13">embedding</strong>. If we look back at the flowchart in <em class="calibre19">Figure 1</em>, what we just did was we converted the raw input text to integer tokens. But we still cannot input this to a RNN, so we have to convert that into embedding vectors, which are values that are somewhere between -1 and 1. They can exceed to some extent but are generally somewhere between -1 and 1, and this is data that we can then work on in the neural network.</p>
<p class="calibre2">It's somewhat magical because this embedding layer trains simultaneously with the RNN and it doesn't see the raw words. It sees integer tokens but learns to recognize that there is some pattern in how words are being used together. So it can, sort of, deduce that some words or some integer tokens have similar meaning, and then it encodes this in embedding vectors that look somewhat the same.</p>
<p class="calibre2">Therefore, what we need to decide is the length of each vector so that, for example, the token "11" gets converted into a real-valued vector. In this example, we will use a length of 8, which is actually extremely short (normally, it is somewhere between 100 and 300). Try and change this number of elements in the embedding vectors and rerun this code to see what you get as a result.</p>
<p class="calibre2">So, we set the embedding size to 8 and then use Keras to add this embedding layer to the RNN. This has to be the first layer in the network:</p>
<pre class="calibre21">embedding_layer_size = 8<br class="title-page-name"/><br class="title-page-name"/>rnn_type_model.add(Embedding(input_dim=num_top_words,<br class="title-page-name"/>                    output_dim=embedding_layer_size,<br class="title-page-name"/>                    input_length=max_num_tokens,<br class="title-page-name"/>                    name='embedding_layer'))</pre>
<p class="calibre2">Then, we can add the first recurrent layer, and we will use what is called a <strong class="calibre13">Gated Recurrent Unit</strong> (<strong class="calibre13">GRU</strong>). Often, you will see that people use what is called <strong class="calibre13">LSTM</strong>, but others seem to suggest that the GRU is better because there are gates inside LSTM that are redundant. And indeed the simpler code works just as well with fewer gates. You could add a thousand more gates to LSTM and that still doesn't mean it gets better.</p>
<p class="calibre2">So, let's define our GRU architectures; we say that we want an output dimensionality of 16 and we need to return sequences:</p>
<pre class="calibre21">rnn_type_model.add(GRU(units=16, return_sequences=True))</pre>
<p class="calibre2">If we look at the flowchart in <em class="calibre19">Figure 4</em>, we want to add a second recurrent layer:</p>
<pre class="calibre21">rnn_type_model.add(GRU(units=8, return_sequences=True))</pre>
<p class="calibre2">Then, we have the third and final recurrent layer, which will not output a sequence because it will be followed by a dense layer; it should only give the final output of the GRU and not a whole sequence of outputs:</p>
<pre class="calibre21">rnn_type_model.add(GRU(units=4))</pre>
<p class="calibre2">Then, the output here will be fed into a fully connected or dense layer, which is just supposed to output one value for each input sequence. This is processed with the sigmoid activation function so it outputs a value between 0 and 1:</p>
<pre class="calibre21">rnn_type_model.add(Dense(1, activation='sigmoid'))</pre>
<p class="calibre2">Then, we say we want to use the Adam optimizer with this learning rate here, and the loss function should be the binary cross-entropy between the output from the RNN and the actual class value from the training set, which will be a value of either 0 or 1:</p>
<pre class="calibre21">model_optimizer = Adam(lr=1e-3)<br class="title-page-name"/><br class="title-page-name"/>rnn_type_model.compile(loss='binary_crossentropy',<br class="title-page-name"/>              optimizer=model_optimizer,<br class="title-page-name"/>              metrics=['accuracy'])</pre>
<p class="calibre2">And now, we can just print a summary of what the model looks like:</p>
<pre class="calibre21">rnn_type_model.summary()<br class="title-page-name"/><br class="title-page-name"/>_________________________________________________________________<br class="title-page-name"/>Layer (type) Output Shape Param # <br class="title-page-name"/>=================================================================<br class="title-page-name"/>embedding_layer (Embedding) (None, 544, 8) 80000 <br class="title-page-name"/>_________________________________________________________________<br class="title-page-name"/>gru_1 (GRU) (None, None, 16) 1200 <br class="title-page-name"/>_________________________________________________________________<br class="title-page-name"/>gru_2 (GRU) (None, None, 8) 600 <br class="title-page-name"/>_________________________________________________________________<br class="title-page-name"/>gru_3 (GRU) (None, 4) 156 <br class="title-page-name"/>_________________________________________________________________<br class="title-page-name"/>dense_1 (Dense) (None, 1) 5 <br class="title-page-name"/>=================================================================<br class="title-page-name"/>Total params: 81,961<br class="title-page-name"/>Trainable params: 81,961<br class="title-page-name"/>Non-trainable params: 0<br class="title-page-name"/>_________________________</pre>
<p class="calibre2">So, as you can see, we have the embedding layer, the first recurrent unit, the second, third, and dense layer. Note that this doesn't have a lot of parameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training and results analysis</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, it's time to kick off the training process, which is very easy here:</p>
<pre class="calibre21">Output:<br class="title-page-name"/>rnn_type_model.fit(input_train_pad, target_train,<br class="title-page-name"/>          validation_split=0.05, epochs=3, batch_size=64)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>Train on 23750 samples, validate on 1250 samples<br class="title-page-name"/>Epoch 1/3<br class="title-page-name"/>23750/23750 [==============================]23750/23750 [==============================] - 176s 7ms/step - loss: 0.6698 - acc: 0.5758 - val_loss: 0.5039 - val_acc: 0.7784<br class="title-page-name"/><br class="title-page-name"/>Epoch 2/3<br class="title-page-name"/>23750/23750 [==============================]23750/23750 [==============================] - 175s 7ms/step - loss: 0.4631 - acc: 0.7834 - val_loss: 0.2571 - val_acc: 0.8960<br class="title-page-name"/><br class="title-page-name"/>Epoch 3/3<br class="title-page-name"/>23750/23750 [==============================]23750/23750 [==============================] - 174s 7ms/step - loss: 0.3256 - acc: 0.8673 - val_loss: 0.3266 - val_acc: 0.8600</pre>
<p class="calibre2">Let's test the trained model against the test set:</p>
<pre class="calibre21">model_result = rnn_type_model.evaluate(input_test_pad, target_test)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>25000/25000 [==============================]25000/25000 [==============================] - 60s 2ms/step<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>print("Accuracy: {0:.2%}".format(model_result[1]))<br class="title-page-name"/>Output:<br class="title-page-name"/>Accuracy: 85.26%</pre>
<p class="calibre2">Now, let's see an example of some misclassified texts.</p>
<p class="calibre2">So first, we calculate the predicted classes for the first 1,000 sequences in the test set and then we take the actual class values. We compare them and get a list of indices where this mismatch <span class="calibre10">exists:</span></p>
<pre class="calibre21">target_predicted = rnn_type_model.predict(x=input_test_pad[0:1000])<br class="title-page-name"/>target_predicted = target_predicted.T[0]</pre>
<p class="calibre2">Use the cut-off threshold to indicate that all values above <kbd class="calibre12">0.5</kbd> will be considered positive and the others will be considered negative:</p>
<pre class="calibre21">class_predicted = np.array([1.0 if prob&gt;0.5 else 0.0 for prob in target_predicted])</pre>
<p class="calibre2">Now, let's get the actual class for these 1,000 sequences:</p>
<pre class="calibre21">class_actual = np.array(target_test[0:1000])</pre>
<p class="calibre2">Let's get the incorrect samples from the output:</p>
<pre class="calibre21">incorrect_samples = np.where(class_predicted != class_actual)<br class="title-page-name"/>incorrect_samples = incorrect_samples[0]<br class="title-page-name"/>len(incorrect_samples)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>122</pre>
<p class="calibre2">So, we see that there are 122 of these texts that were incorrectly classified; that's 12.1% of the 1,000 texts we calculated here. Let's look at the first misclassified text:</p>
<pre class="calibre21">index = incorrect_samples[0]<br class="title-page-name"/>index</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>9<br class="title-page-name"/><br class="title-page-name"/>incorrectly_predicted_text = input_text_test[index]<br class="title-page-name"/>incorrectly_predicted_text</pre>
<pre class="calibre21">Output:<br class="title-page-name"/><br class="title-page-name"/>'I am not a big music video fan. I think music videos take away personal feelings about a particular song.. Any song. In other words, creative thinking goes out the window. Likewise, Personal feelings aside about MJ, toss aside. This was the best music video of alltime. Simply wonderful. It was a movie. Yes folks it was. Brilliant! You had awesome acting, awesome choreography, and awesome singing. This was spectacular. Simply a plot line of a beautiful young lady dating a man, but was he a man or something sinister. Vincent Price did his thing adding to the song and video. MJ was MJ, enough said about that. This song was to video, what Jaguars are for cars. Top of the line, PERFECTO. What was even better about this was, that we got the real MJ without the thousand facelifts. Though ironically enough, there was more than enough makeup and costumes to go around. Folks go to Youtube. Take 14 mins. out of your life and see for yourself what a wonderful work of art this particular video really is.'</pre>
<p class="calibre2">Let's have a look at the model output for this sample as well as the actual class:</p>
<pre class="calibre21">target_predicted[index]</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>0.1529513<br class="title-page-name"/><br class="title-page-name"/>class_actual[index]<br class="title-page-name"/>Output:<br class="title-page-name"/>1.0</pre>
<p class="calibre2">Now, let's test our trained model against a set of new data samples and see its results:</p>
<pre class="calibre21">test_sample_1 = "This movie is fantastic! I really like it because it is so good!"<br class="title-page-name"/>test_sample_2 = "Good movie!"<br class="title-page-name"/>test_sample_3 = "Maybe I like this movie."<br class="title-page-name"/>test_sample_4 = "Meh ..."<br class="title-page-name"/>test_sample_5 = "If I were a drunk teenager then this movie might be good."<br class="title-page-name"/>test_sample_6 = "Bad movie!"<br class="title-page-name"/>test_sample_7 = "Not a good movie!"<br class="title-page-name"/>test_sample_8 = "This movie really sucks! Can I get my money back please?"<br class="title-page-name"/>test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]</pre>
<p class="calibre2">Now, let's convert them to integer tokens:</p>
<pre class="calibre21">test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)</pre>
<p class="calibre2">And then pad them:</p>
<pre class="calibre21">test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_num_tokens,<br class="title-page-name"/>                           padding=seq_pad, truncating=seq_pad)<br class="title-page-name"/>test_samples_tokens_pad.shape<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>(8, 544)</pre>
<p class="calibre2">Finally, let's run the model against them:</p>
<pre class="calibre21">rnn_type_model.predict(test_samples_tokens_pad)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>array([[0.9496784 ],<br class="title-page-name"/> [0.9552593 ],<br class="title-page-name"/> [0.9115685 ],<br class="title-page-name"/> [0.9464672 ],<br class="title-page-name"/> [0.87672734],<br class="title-page-name"/> [0.81883633],<br class="title-page-name"/> [0.33248223],<br class="title-page-name"/> [0.15345531 ]], dtype=float32)</pre>
<p class="calibre2">So, a value close to zero means a negative sentiment and a value that's close to 1 means a positive sentiment; finally, these numbers will vary every time you train the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we covered an interesting application, which is sentiment analysis. Sentiment analysis is used by different companies to track customer's satisfaction with their products. Even governments use sentiment analysis solutions to track citizen satisfaction about something that they want to do in the future.</p>
<p class="calibre2">Next up, we are going to focus on some advanced deep learning architectures that can be used for semi-supervised and unsupervised applications.</p>


            </article>

            
        </section>
    </body></html>