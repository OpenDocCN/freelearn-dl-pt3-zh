["```\n    def resnet_block(\n        input_tensor, size, kernel_size, filters, stage, \\\n         conv_strides=(2, 2), training=None\n    ):\n        x = conv_building_block(\n            input_tensor,\n            kernel_size,\n            filters,\n            stage=stage,\n            strides=conv_strides,\n            block=\"block_0\",\n            training=training,\n        )\n        for i in range(size - 1):\n            x = identity_building_block(\n                x,\n                kernel_size,\n                filters,\n                stage=stage,\n                block=\"block_%d\" % (i + 1),\n                training=training,\n            )\n        return x\n    ```", "```\n    def resnet(num_blocks, img_input=None, classes=10, training=None):\n        \"\"\"Builds the ResNet architecture using provided \n           config\"\"\"\n    ```", "```\n        if backend.image_data_format() == \"channels_first\":\n            x = layers.Lambda(\n                lambda x: backend.permute_dimensions(x, \\\n                    (0, 3, 1, 2)), name=\"transpose\"\n            )(img_input)\n            bn_axis = 1\n        else:  # channel_last\n            x = img_input\n            bn_axis = 3\n    ```", "```\n        x = tf.keras.layers.ZeroPadding2D(padding=(1, 1), \\\n                                         name=\"conv1_pad\")(x)\n        x = tf.keras.layers.Conv2D(16,(3, 3),strides=(1, 1),\n                             padding=\"valid\",\n                             kernel_initializer=\"he_normal\",\n                             kernel_regularizer= \\\n                                tf.keras.regularizers.l2(\n                                     L2_WEIGHT_DECAY), \n                             bias_regularizer= \\\n                                 tf.keras.regularizers.l2(\n                                     L2_WEIGHT_DECAY), \n                                                            name=\"conv1\",)(x)\n        x = tf.keras.layers.BatchNormalization(axis=bn_axis,\n                 name=\"bn_conv1\", momentum=BATCH_NORM_DECAY,\n                 epsilon=BATCH_NORM_EPSILON,)\\\n                      (x, training=training)\n        x = tf.keras.layers.Activation(\"relu\")(x)\n    ```", "```\n        x = resnet_block(x, size=num_blocks, kernel_size=3,\n            filters=[16, 16], stage=2, conv_strides=(1, 1),\n            training=training,)\n        x = resnet_block(x, size=num_blocks, kernel_size=3,\n            filters=[32, 32], stage=3, conv_strides=(2, 2),\n            training=training)\n        x = resnet_block(x, size=num_blocks, kernel_size=3,\n            filters=[64, 64], stage=4, conv_strides=(2, 2),\n            training=training,)\n    ```", "```\n    x = tf.keras.layers.GlobalAveragePooling2D(\n                                         name=\"avg_pool\")(x)\n        x = tf.keras.layers.Dense(classes,\n            activation=\"softmax\",\n            kernel_initializer=\"he_normal\",\n            kernel_regularizer=tf.keras.regularizers.l2(\n                 L2_WEIGHT_DECAY), \n            bias_regularizer=tf.keras.regularizers.l2(\n                 L2_WEIGHT_DECAY), \n            name=\"fc10\",)(x)\n    ```", "```\n        inputs = img_input\n        # Create model.\n        model = tf.keras.models.Model(inputs, x, name=f\"resnet{6 * num_blocks + 2}\")\n        return model\n    ```", "```\n    resnet_mini = functools.partial(resnet, num_blocks=1)\n    resnet20 = functools.partial(resnet, num_blocks=3)\n    resnet32 = functools.partial(resnet, num_blocks=5)\n    resnet44 = functools.partial(resnet, num_blocks=7)\n    resnet56 = functools.partial(resnet, num_blocks=9)\n    ```", "```\n    import os\n    import sys\n    import tensorflow as tf\n    import tensorflow_datasets as tfds\n    if \".\" not in sys.path:\n        sys.path.insert(0, \".\")\n    import resnet\n    ```", "```\n    dataset_name = \"dmlab\"  # \"cifar10\" or \"cifar100\"; See tensorflow.org/datasets/catalog for complete list\n    # NOTE: dmlab is large in size; Download bandwidth and # GPU memory to be considered\n    datasets, info = tfds.load(name=\"dmlab\", with_info=True,\n                               as_supervised=True)\n    dataset_train, dataset_test = datasets[\"train\"], \\\n                                  datasets[\"test\"]\n    input_shape = info.features[\"image\"].shape\n    num_classes = info.features[\"label\"].num_classes\n    ```", "```\n    strategy = tf.distribute.MirroredStrategy()\n    print(f\"Number of devices: {\n               strategy.num_replicas_in_sync}\")\n    ```", "```\n    num_train_examples = info.splits[\"train\"].num_examples\n    num_test_examples = info.splits[\"test\"].num_examples\n    BUFFER_SIZE = 1000  # Increase as per available memory\n    BATCH_SIZE_PER_REPLICA = 64\n    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * \\\n                      strategy.num_replicas_in_sync\n    ```", "```\n    def preprocess(image, label):\n        image = tf.cast(image, tf.float32)\n        image /= 255\n        return image, label\n    ```", "```\n    train_dataset = (\n        dataset_train.map(preprocess).cache().\\\n            shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n    )\n    eval_dataset = dataset_test.map(preprocess).batch(\n                                                 BATCH_SIZE)\n    ```", "```\n    with strategy.scope():\n        # model = create_model()\n        model = create_model(\"resnet_mini\")\n        tf.keras.utils.plot_model(model, \n                                 to_file=\"./slim_resnet.png\", \n                                 show_shapes=True)\n        model.compile(\n            loss=\\\n              tf.keras.losses.SparseCategoricalCrossentropy(\n                  from_logits=True),\n            optimizer=tf.keras.optimizers.Adam(),\n            metrics=[\"accuracy\"],\n        )\n    ```", "```\n    checkpoint_dir = \"./training_checkpoints\"\n    checkpoint_prefix = os.path.join(checkpoint_dir, \n                                     \"ckpt_{epoch}\")\n    callbacks = [\n        tf.keras.callbacks.TensorBoard(\n            log_dir=\"./logs\", write_images=True, \\\n            update_freq=\"batch\"\n        ),\n        tf.keras.callbacks.ModelCheckpoint(\n            filepath=checkpoint_prefix, \\\n            save_weights_only=True\n        ),\n    ]\n    ```", "```\n    model.fit(train_dataset, epochs=12, callbacks=callbacks)\n    ```", "```\n    path = \"saved_model/\"\n    model.save(path, save_format=\"tf\")\n    ```", "```\n    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n    eval_loss, eval_acc = model.evaluate(eval_dataset)\n    print(\"Eval loss: {}, Eval Accuracy: {}\".format(eval_loss, eval_acc))\n    ```", "```\n    unreplicated_model = tf.keras.models.load_model(path)\n    unreplicated_model.compile(\n        loss=tf.keras.losses.\\\n             SparseCategoricalCrossentropy(from_logits=True),\n        optimizer=tf.keras.optimizers.Adam(),\n        metrics=[\"accuracy\"],\n    )\n    eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n    print(\"Eval loss: {}, Eval Accuracy: {}\".format(eval_loss, eval_acc))\n    ```", "```\n    with strategy.scope():\n        replicated_model = tf.keras.models.load_model(path)\n        replicated_model.compile(\n            loss=tf.keras.losses.\\\n             SparseCategoricalCrossentropy(from_logits=True),\n            optimizer=tf.keras.optimizers.Adam(),\n            metrics=[\"accuracy\"],\n        )\n        eval_loss, eval_acc = \\\n            replicated_model.evaluate(eval_dataset)\n        print(\"Eval loss: {}, \\\n              Eval Accuracy: {}\".format(eval_loss, eval_acc))\n    ```", "```\n    # Uncomment the following lines and fill worker details \n    # based on your cluster configuration\n    # tf_config = {\n    #    \"cluster\": {\"worker\": [\"1.2.3.4:1111\", \n                     \"localhost:2222\"]},\n    #    \"task\": {\"index\": 0, \"type\": \"worker\"},\n    # }\n    # os.environ[\"TF_CONFIG\"] = json.dumps(tf_config)\n    ```", "```\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    ```", "```\n    NUM_GPUS = 2\n    BS_PER_GPU = 128\n    NUM_EPOCHS = 60\n    HEIGHT = 32\n    WIDTH = 32\n    NUM_CHANNELS = 3\n    NUM_CLASSES = 10\n    NUM_TRAIN_SAMPLES = 50000\n    BASE_LEARNING_RATE = 0.1\n    ```", "```\n    def normalize(x, y):\n        x = tf.image.per_image_standardization(x)\n        return x, y\n    def augmentation(x, y):\n        x = tf.image.resize_with_crop_or_pad(x, HEIGHT + 8, \n                                             WIDTH + 8)\n        x = tf.image.random_crop(x, [HEIGHT, WIDTH, \n                                     NUM_CHANNELS])\n        x = tf.image.random_flip_left_right(x)\n        return x, y\n    ```", "```\n    (x, y), (x_test, y_test) = \\\n          keras.datasets.cifar10.load_data()\n    train_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n    test_dataset = \\\n        tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    ```", "```\n    tf.random.set_seed(22)\n    ```", "```\n    train_dataset = (\n        train_dataset.map(augmentation)\n        .map(normalize)\n        .shuffle(NUM_TRAIN_SAMPLES)\n        .batch(BS_PER_GPU * NUM_GPUS, drop_remainder=True)\n    )\n    ```", "```\n    test_dataset = test_dataset.map(normalize).batch(\n        BS_PER_GPU * NUM_GPUS, drop_remainder=True\n    )\n    ```", "```\n    opt = keras.optimizers.SGD(learning_rate=0.1, \n                               momentum=0.9)\n    input_shape = (HEIGHT, WIDTH, NUM_CHANNELS)\n    img_input = tf.keras.layers.Input(shape=input_shape)\n    ```", "```\n    with strategy.scope():\n        model = resnet.resnet56(img_input=img_input, \n                                classes=NUM_CLASSES)\n        model.compile(\n            optimizer=opt,\n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"sparse_categorical_accuracy\"],\n        )\n    ```", "```\n    model.fit(train_dataset, epochs=NUM_EPOCHS)\n    ```", "```\n    model.save(path, save_format=\"tf\")\n    # 12.2 Load\n    loaded_model = tf.keras.models.load_model(path)\n    loaded_model.compile(\n        loss=tf.keras.losses.\\\n            SparseCategoricalCrossentropy(from_logits=True),\n        optimizer=tf.keras.optimizers.Adam(),\n        metrics=[\"accuracy\"],\n    )\n    # 12.3 Evaluate\n    eval_loss, eval_acc = loaded_model.evaluate(eval_dataset)\n    ```", "```\n    import argparse\n    import os\n    from datetime import datetime\n    import gym\n    import gym.wrappers\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras.layers import (\n        Conv2D,\n        Dense,\n        Dropout,\n        Flatten,\n        Input,\n        MaxPool2D,\n    )\n    ```", "```\n    import procgen  # Import & register procgen Gym envs\n    ```", "```\n    parser = argparse.ArgumentParser(prog=\"TFRL-Cookbook-Ch9-Distributed-RL-Agent\")\n    parser.add_argument(\"--env\", default=\"procgen:procgen-coinrun-v0\")\n    parser.add_argument(\"--update-freq\", type=int, default=16)\n    parser.add_argument(\"--epochs\", type=int, default=3)\n    parser.add_argument(\"--actor-lr\", type=float, default=1e-4)\n    parser.add_argument(\"--critic-lr\", type=float, default=1e-4)\n    parser.add_argument(\"--clip-ratio\", type=float, default=0.1)\n    parser.add_argument(\"--gae-lambda\", type=float, default=0.95)\n    parser.add_argument(\"--gamma\", type=float, default=0.99)\n    parser.add_argument(\"--logdir\", default=\"logs\")\n    args = parser.parse_args()\n    ```", "```\n    logdir = os.path.join(\n        args.logdir, parser.prog, args.env, \\\n        datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    )\n    print(f\"Saving training logs to:{logdir}\")\n    writer = tf.summary.create_file_writer(logdir)\n    ```", "```\n    class Actor:\n        def __init__(self, state_dim, action_dim, \n        execution_strategy):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.execution_strategy = execution_strategy\n            with self.execution_strategy.scope():\n                self.weight_initializer = \\\n                    tf.keras.initializers.he_normal()\n                self.model = self.nn_model()\n                self.model.summary()  # Print a summary of\n                # the Actor model\n                self.opt = \\\n                    tf.keras.optimizers.Nadam(args.actor_lr)\n    ```", "```\n        def nn_model(self):\n            obs_input = Input(self.state_dim)\n            conv1 = Conv2D(\n                filters=64,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"same\",\n                input_shape=self.state_dim,\n                data_format=\"channels_last\",\n                activation=\"relu\",\n            )(obs_input)\n            pool1 = MaxPool2D(pool_size=(3, 3), \\\n                              strides=1)(conv1)\n    ```", "```\n           conv2 = Conv2D(\n                filters=32,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool1)\n            pool2 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                        (conv2)\n            conv3 = Conv2D(\n                filters=16,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool2)\n            pool3 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                        (conv3)\n            conv4 = Conv2D(\n                filters=8,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool3)\n            pool4 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                        (conv4)\n    ```", "```\n           flat = Flatten()(pool4)\n            dense1 = Dense(\n                16, activation=\"relu\", \\\n                   kernel_initializer=self.weight_initializer\n            )(flat)\n            dropout1 = Dropout(0.3)(dense1)\n            dense2 = Dense(\n                8, activation=\"relu\", \\\n                   kernel_initializer=self.weight_initializer\n            )(dropout1)\n            dropout2 = Dropout(0.3)(dense2)\n    ```", "```\n            output_discrete_action = Dense(\n                self.action_dim,\n                activation=\"softmax\",\n                kernel_initializer=self.weight_initializer,\n            )(dropout2)\n            return tf.keras.models.Model(\n                inputs=obs_input, \n                outputs = output_discrete_action, \n                name=\"Actor\")\n    ```", "```\n        def get_action(self, state):\n            # Convert [Image] to np.array(np.adarray)\n            state_np = np.array([np.array(s) for s in state])\n            if len(state_np.shape) == 3:\n                # Convert (w, h, c) to (1, w, h, c)\n                state_np = np.expand_dims(state_np, 0)\n            logits = self.model.predict(state_np)  \n            # shape: (batch_size, self.action_dim)\n            action = np.random.choice(self.action_dim, \n                                      p=logits[0])\n            # 1 Action per instance of env; Env expects:\n            # (num_instances, actions)\n            # action = (action,)\n            return logits, action\n    ```", "```\n        def compute_loss(self, old_policy, new_policy, \n        actions, gaes):\n            log_old_policy = tf.math.log(tf.reduce_sum(\n                                       old_policy * actions))\n            log_old_policy = tf.stop_gradient(log_old_policy)\n            log_new_policy = tf.math.log(tf.reduce_sum(\n                                       new_policy * actions))\n            # Avoid INF in exp by setting 80 as the upper \n            # bound since,\n            # tf.exp(x) for x>88 yeilds NaN (float32)\n            ratio = tf.exp(\n                tf.minimum(log_new_policy - \\\n                           tf.stop_gradient(log_old_policy),\\\n                           80)\n            )\n            clipped_ratio = tf.clip_by_value(\n                ratio, 1.0 - args.clip_ratio, 1.0 + \\\n                args.clip_ratio\n            )\n            gaes = tf.stop_gradient(gaes)\n            surrogate = -tf.minimum(ratio * gaes, \\\n                                    clipped_ratio * gaes)\n            return tf.reduce_mean(surrogate)\n    ```", "```\n        def train(self, old_policy, states, actions, gaes):\n            actions = tf.one_hot(actions, self.action_dim)  \n            # One-hot encoding\n            actions = tf.reshape(actions, [-1, \\\n                                 self.action_dim])  \n            # Add batch dimension\n            actions = tf.cast(actions, tf.float64)\n            with tf.GradientTape() as tape:\n                logits = self.model(states, training=True)\n                loss = self.compute_loss(old_policy, logits, \n                                         actions, gaes)\n            grads = tape.gradient(loss, \n                              self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \n                             self.model.trainable_variables))\n            return loss\n    ```", "```\n        @tf.function\n        def train_distributed(self, old_policy, states,\n                              actions, gaes):\n            per_replica_losses = self.execution_strategy.run(\n                self.train, args=(old_policy, states, \n                                  actions, gaes))\n            return self.execution_strategy.reduce(\n                tf.distribute.ReduceOp.SUM, \\\n                    per_replica_losses, axis=None)\n    ```", "```\n    class Critic:\n        def __init__(self, state_dim, execution_strategy):\n            self.state_dim = state_dim\n            self.execution_strategy = execution_strategy\n            with self.execution_strategy.scope():\n                self.weight_initializer = \\\n                    tf.keras.initializers.he_normal()\n                self.model = self.nn_model()\n                self.model.summary()  \n                # Print a summary of the Critic model\n                self.opt = \\\n                    tf.keras.optimizers.Nadam(args.critic_lr)\n    ```", "```\n        def nn_model(self):\n            obs_input = Input(self.state_dim)\n            conv1 = Conv2D(\n                filters=64,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"same\",\n                input_shape=self.state_dim,\n                data_format=\"channels_last\",\n                activation=\"relu\",\n            )(obs_input)\n            pool1 = MaxPool2D(pool_size=(3, 3), strides=2)\\\n                        (conv1)\n    ```", "```\n            conv2 = Conv2D(filters=32, kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\", activation=\"relu\",)(pool1)\n            pool2 = MaxPool2D(pool_size=(3, 3), strides=2)\\\n                        (conv2)\n            conv3 = Conv2D(filters=16,\n                kernel_size=(3, 3), strides=(1, 1),\n                padding=\"valid\", activation=\"relu\",)(pool2)\n            pool3 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                        (conv3)\n            conv4 = Conv2D(filters=8, kernel_size=(3, 3),\n                strides=(1, 1), padding=\"valid\",\n                activation=\"relu\",)(pool3)\n            pool4 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                        (conv4)\n            flat = Flatten()(pool4)\n            dense1 = Dense(16, activation=\"relu\", \n                           kernel_initializer =\\\n                               self.weight_initializer)\\\n                           (flat)\n            dropout1 = Dropout(0.3)(dense1)\n            dense2 = Dense(8, activation=\"relu\", \n                           kernel_initializer = \\\n                               self.weight_initializer)\\\n                           (dropout1)\n            dropout2 = Dropout(0.3)(dense2)\n    ```", "```\n            value = Dense(\n                1, activation=\"linear\", \n                kernel_initializer=self.weight_initializer)\\\n                (dropout2)\n            return tf.keras.models.Model(inputs=obs_input, \\\n                                         outputs=value, \\\n                                         name=\"Critic\")\n    ```", "```\n        def compute_loss(self, v_pred, td_targets):\n            mse = tf.keras.losses.MeanSquaredError(\n                     reduction=tf.keras.losses.Reduction.SUM)\n            return mse(td_targets, v_pred)\n    ```", "```\n        def train(self, states, td_targets):\n            with tf.GradientTape() as tape:\n                v_pred = self.model(states, training=True)\n                # assert v_pred.shape == td_targets.shape\n                loss = self.compute_loss(v_pred, \\\n                               tf.stop_gradient(td_targets))\n            grads = tape.gradient(loss, \\\n                           self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \\\n                           self.model.trainable_variables))\n            return loss\n    ```", "```\n        @tf.function\n        def train_distributed(self, states, td_targets):\n            per_replica_losses = self.execution_strategy.run(\n                self.train, args=(states, td_targets)\n            )\n            return self.execution_strategy.reduce(\n                tf.distribute.ReduceOp.SUM, \\\n                per_replica_losses, axis=None\n            )\n    ```", "```\n    class PPOAgent:\n        def __init__(self, env):\n            \"\"\"Distributed PPO Agent for image observations \n            and discrete action-space Gym envs\n            Args:\n                env (gym.Env): OpenAI Gym I/O compatible RL \n                environment with discrete action space\n            \"\"\"\n            self.env = env\n            self.state_dim = self.env.observation_space.shape\n            self.action_dim = self.env.action_space.n\n            # Create a Distributed execution strategy\n            self.distributed_execution_strategy = \\\n                         tf.distribute.MirroredStrategy()\n            print(f\"Number of devices: {self.\\\n                    distributed_execution_strategy.\\\n                    num_replicas_in_sync}\")\n            # Create Actor & Critic networks under the \n            # distributed execution strategy scope\n            with self.distributed_execution_strategy.scope():\n                self.actor = Actor(self.state_dim, \n                                self.action_dim, \n                                tf.distribute.get_strategy())\n                self.critic = Critic(self.state_dim, \n                                tf.distribute.get_strategy())\n    ```", "```\n        def gae_target(self, rewards, v_values, next_v_value,\n        done):\n            n_step_targets = np.zeros_like(rewards)\n            gae = np.zeros_like(rewards)\n            gae_cumulative = 0\n            forward_val = 0\n            if not done:\n                forward_val = next_v_value\n            for k in reversed(range(0, len(rewards))):\n                delta = rewards[k] + args.gamma * \\\n                  forward_val - v_values[k]\n                gae_cumulative = args.gamma * \\\n                  args.gae_lambda * gae_cumulative + delta\n                gae[k] = gae_cumulative\n                forward_val = v_values[k]\n                n_step_targets[k] = gae[k] + v_values[k]\n            return gae, n_step_targets\n    ```", "```\n        def train(self, max_episodes=1000):\n            with self.distributed_execution_strategy.scope():\n                with writer.as_default():\n                    for ep in range(max_episodes):\n                        state_batch = []\n                        action_batch = []\n                        reward_batch = []\n                        old_policy_batch = []\n                        episode_reward, done = 0, False\n                        state = self.env.reset()\n                        prev_state = state\n                        step_num = 0\n    ```", "```\n                          while not done:\n                            self.env.render()\n                            logits, action = \\\n                                 self.actor.get_action(state)\n                            next_state, reward, dones, _ = \\\n                                        self.env.step(action)\n                            step_num += 1\n                            print(f\"ep#:{ep} step#:{step_num} \n                                    step_rew:{reward} \\\n                                    action:{action} \\\n                                    dones:{dones}\",end=\"\\r\",)\n                            done = np.all(dones)\n                            if done:\n                                next_state = prev_state\n                            else:\n                                prev_state = next_state\n                            state_batch.append(state)\n                            action_batch.append(action)\n                            reward_batch.append(\n                                            (reward + 8) / 8)\n                            old_policy_batch.append(logits)  \n    ```", "```\n                             if len(state_batch) >= \\\n                             args.update_freq or done:\n                                states = np.array(\n                                    [state.squeeze() for \\\n                                     state in state_batch])\n                                actions = \\\n                                    np.array(action_batch)\n                                rewards = \\\n                                    np.array(reward_batch)\n                                old_policies = np.array(\n                                    [old_pi.squeeze() for \\\n                                 old_pi in old_policy_batch])\n                                v_values = self.critic.\\\n                                        model.predict(states)\n                                next_v_value = self.critic.\\\n                                   model.predict(\n                                       np.expand_dims(\n                                           next_state, 0))\n                                gaes, td_targets = \\\n                                     self.gae_target(\n                                         rewards, v_values,\n                                         next_v_value, done)\n                                actor_losses, critic_losses=\\\n                                                       [], []   \n    ```", "```\n                                   for epoch in range(args.\\\n                                   epochs):\n                                    actor_loss = self.actor.\\\n                                      train_distributed(\n                                         old_policies,\n                                         states, actions,\n                                         gaes)\n                                    actor_losses.\\\n                                      append(actor_loss)\n                                    critic_loss = self.\\\n                                    critic.train_distributed(\n                                       states, td_targets)\n                                    critic_losses.\\\n                                       append(critic_loss)\n                                # Plot mean actor & critic \n                                # losses on every update\n                                tf.summary.scalar(\n                                    \"actor_loss\", \n                                     np.mean(actor_losses), \n                                     step=ep)\n                                tf.summary.scalar(\n                                     \"critic_loss\", \n                                      np.mean(critic_losses), \n                                      step=ep) \n    ```", "```\n\n                                state_batch = []\n                                action_batch = []\n                                reward_batch = []\n                                old_policy_batch = []\n                            episode_reward += reward\n                            state = next_state \n    ```", "```\n    if __name__ == \"__main__\":\n        env_name = \"procgen:procgen-coinrun-v0\"\n        env = gym.make(env_name, render_mode=\"rgb_array\")\n        env = gym.wrappers.Monitor(env=env, \n                            directory=\"./videos\", force=True)\n        agent = PPOAgent(env)\n        agent.train()\n    ```", "```\nimport pickle\nimport sys\nimport fire\nimport gym\nimport numpy as np\nimport ray\nif \".\" not in sys.path:\n    sys.path.insert(0, \".\")\nfrom sac_agent_base import SAC\n```", "```\n    @ray.remote\n    class ParameterServer(object):\n        def __init__(self, weights):\n            values = [value.copy() for value in weights]\n            self.weights = values\n        def push(self, weights):\n            values = [value.copy() for value in weights]\n            self.weights = values\n        def pull(self):\n            return self.weights\n        def get_weights(self):\n            return self.weights\n    ```", "```\n        # save weights to disk\n        def save_weights(self, name):\n            with open(name + \"weights.pkl\", \"wb\") as pkl:\n                pickle.dump(self.weights, pkl)\n            print(f\"Weights saved to {name + \n                                      ‘weights.pkl’}.\")\n    ```", "```\n    @ray.remote\n    class ReplayBuffer:\n        \"\"\"\n        A simple FIFO experience replay buffer for RL Agents\n        \"\"\"\n        def __init__(self, obs_shape, action_shape, size):\n            self.cur_states = np.zeros([size, obs_shape[0]],\n                                        dtype=np.float32)\n            self.actions = np.zeros([size, action_shape[0]],\n                                     dtype=np.float32)\n            self.rewards = np.zeros(size, dtype=np.float32)\n            self.next_states = np.zeros([size, obs_shape[0]],\n                                         dtype=np.float32)\n            self.dones = np.zeros(size, dtype=np.float32)\n            self.idx, self.size, self.max_size = 0, 0, size\n            self.rollout_steps = 0\n    ```", "```\n        def store(self, obs, act, rew, next_obs, done):\n            self.cur_states[self.idx] = np.squeeze(obs)\n            self.actions[self.idx] = np.squeeze(act)\n            self.rewards[self.idx] = np.squeeze(rew)\n            self.next_states[self.idx] = np.squeeze(next_obs)\n            self.dones[self.idx] = done\n            self.idx = (self.idx + 1) % self.max_size\n            self.size = min(self.size + 1, self.max_size)\n            self.rollout_steps += 1\n    ```", "```\n        def sample_batch(self, batch_size=32):\n            idxs = np.random.randint(0, self.size, \n                                     size=batch_size)\n            return dict(\n                cur_states=self.cur_states[idxs],\n                actions=self.actions[idxs],\n                rewards=self.rewards[idxs],\n                next_states=self.next_states[idxs],\n                dones=self.dones[idxs])\n    ```", "```\n    @ray.remote\n    def rollout(ps, replay_buffer, config):\n        \"\"\"Collect experience using an exploration policy\"\"\"\n        env = gym.make(config[\"env\"])\n        obs, reward, done, ep_ret, ep_len = env.reset(), 0, \\\n                                              False, 0, 0\n        total_steps = config[\"steps_per_epoch\"] * \\\n                       config[\"epochs\"]\n        agent = SAC(env.observation_space.shape, \\\n                    env.action_space)\n        weights = ray.get(ps.pull.remote())\n        target_weights = agent.actor.get_weights()\n        for i in range(len(target_weights)):  \n        # set tau% of target model to be new weights\n            target_weights[i] = weights[i]\n        agent.actor.set_weights(target_weights)\n    ```", "```\n        for step in range(total_steps):\n            if step > config[\"random_exploration_steps\"]:\n                # Use Agent’s policy for exploration after \n                `random_exploration_steps`\n                a = agent.act(obs)\n            else:  # Use a uniform random exploration policy\n                a = env.action_space.sample()\n            next_obs, reward, done, _ = env.step(a)\n            print(f\"Step#:{step} reward:{reward} \\\n                    done:{done}\")\n            ep_ret += reward\n            ep_len += 1\n    ```", "```\n            done = False if ep_len == config[\"max_ep_len\"]\\\n                     else done\n            # Store experience to replay buffer\n            replay_buffer.store.remote(obs, a, reward, \n                                       next_obs, done)\n    ```", "```\n            obs = next_obs\n            if done or (ep_len == config[\"max_ep_len\"]):\n                \"\"\"\n                Perform parameter sync at the end of the \n                trajectory.\n                \"\"\"\n                obs, reward, done, ep_ret, ep_len = \\\n                                 env.reset(), 0, False, 0, 0\n                weights = ray.get(ps.pull.remote())\n                agent.actor.set_weights(weights)\n    ```", "```\n    @ray.remote(num_gpus=1, max_calls=1)\n    def train(ps, replay_buffer, config):\n        agent = SAC(config[\"obs_shape\"], \\\n                    config[\"action_space\"])\n        weights = ray.get(ps.pull.remote())\n        agent.actor.set_weights(weights)\n        train_step = 1\n        while True:\n            agent.train_with_distributed_replay_memory(\n                ray.get(replay_buffer.sample_batch.remote())\n            )\n            if train_step % config[\"worker_update_freq\"]== 0:\n                weights = agent.actor.get_weights()\n                ps.push.remote(weights)\n            train_step += 1\n    ```", "```\n    def main(\n        env=\"MountainCarContinuous-v0\",\n        epochs=1000,\n        steps_per_epoch=5000,\n        replay_size=100000,\n        random_exploration_steps=1000,\n        max_ep_len=1000,\n        num_workers=4,\n        num_learners=1,\n        worker_update_freq=500,\n    ):\n        config = {\n            \"env\": env,\n            \"epochs\": epochs,\n            \"steps_per_epoch\": steps_per_epoch,\n            \"max_ep_len\": max_ep_len,\n            \"replay_size\": replay_size,\n            \"random_exploration_steps\": \\\n                 random_exploration_steps,\n            \"num_workers\": num_workers,\n            \"num_learners\": num_learners,\n            \"worker_update_freq\": worker_update_freq,\n        }\n    ```", "```\n        env = gym.make(config[\"env\"])\n        config[\"obs_shape\"] = env.observation_space.shape\n        config[\"action_space\"] = env.action_space\n        ray.init()\n        agent = SAC(config[\"obs_shape\"], \\\n                    config[\"action_space\"])\n    ```", "```\n        params_server = \\\n            ParameterServer.remote(agent.actor.get_weights())\n        replay_buffer = ReplayBuffer.remote(\n            config[\"obs_shape\"], \\\n            config[\"action_space\"].shape, \\\n            config[\"replay_size\"]\n        )\n    ```", "```\n        task_rollout = [\n            rollout.remote(params_server, replay_buffer, \n                           config)\n            for i in range(config[\"num_workers\"])\n        ]\n    ```", "```\n        task_train = [\n            train.remote(params_server, replay_buffer, \n                         config)\n            for i in range(config[\"num_learners\"])\n        ]\n    ```", "```\n    We will wait for the tasks to complete on the main thread before exiting:\n        ray.wait(task_rollout)\n        ray.wait(task_train)\n    ```", "```\n    if __name__ == \"__main__\":\n        fire.Fire(main)\n    ```", "```\n    (tfrl-cookbook)praveen@dev-cluster:~/tfrl-cookbook$python 4_building_blocks_for_distributed_rl_using_ray.py main --env=\"MountaincarContinuous-v0\" --num_workers=8 --num_learners=3\n    ```", "```\n pip install ray[tune,rllib]\n```", "```\n    --eager flag is also specified, which forces RLLib to use eager execution (the default mode of execution in TensorFlow 2.x).\n    ```", "```\n    (tfrl-cookbook) praveen@dev-cluster:~/tfrl-cookbook$rllib train --run PPO --env \"procgen:procgen-coinrun-v0\" --eager\n    ```", "```\n        ValueError: No default configuration for obs shape [64, 64, 3], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.\n    ```", "```\n    from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n    import tensorflow as tf\n    def conv_layer(depth, name):\n        return tf.keras.layers.Conv2D(\n            filters=depth, kernel_size=3, strides=1, \\\n            padding=\"same\", name=name\n        )\n    ```", "```\n    def residual_block(x, depth, prefix):\n        inputs = x\n        assert inputs.get_shape()[-1].value == depth\n        x = tf.keras.layers.ReLU()(x)\n        x = conv_layer(depth, name=prefix + \"_conv0\")(x)\n        x = tf.keras.layers.ReLU()(x)\n        x = conv_layer(depth, name=prefix + \"_conv1\")(x)\n        return x + inputs\n    ```", "```\n    def conv_sequence(x, depth, prefix):\n        x = conv_layer(depth, prefix + \"_conv\")(x)\n        x = tf.keras.layers.MaxPool2D(pool_size=3, \\\n                                      strides=2,\\\n                                      padding=\"same\")(x)\n        x = residual_block(x, depth, prefix=prefix + \\\n                           \"_block0\")\n        x = residual_block(x, depth, prefix=prefix + \\\n                           \"_block1\")\n        return x\n    ```", "```\n    class CustomModel(TFModelV2):\n        \"\"\"Deep residual network that produces logits for \n           policy and value for value-function;\n        Based on architecture used in IMPALA paper:https://\n           arxiv.org/abs/1802.01561\"\"\"\n        def __init__(self, obs_space, action_space, \n        num_outputs, model_config, name):\n            super().__init__(obs_space, action_space, \\\n                             num_outputs, model_config, name)\n            depths = [16, 32, 32]\n            inputs = tf.keras.layers.Input(\n                            shape=obs_space.shape,\n                            name=\"observations\")\n            scaled_inputs = tf.cast(inputs, \n                                    tf.float32) / 255.0\n            x = scaled_inputs\n            for i, depth in enumerate(depths):\n                x = conv_sequence(x, depth, prefix=f\"seq{i}\")\n            x = tf.keras.layers.Flatten()(x)\n            x = tf.keras.layers.ReLU()(x)\n            x = tf.keras.layers.Dense(units=256,\n                                      activation=\"relu\", \n                                      name=\"hidden\")(x)\n            logits = tf.keras.layers.Dense(units=num_outputs,\n                                           name=\"pi\")(x)\n            value = tf.keras.layers.Dense(units=1, \n                                          name=\"vf\")(x)\n            self.base_model = tf.keras.Model(inputs, \n                                            [logits, value])\n            self.register_variables(\n                                   self.base_model.variables)\n    ```", "```\n        def forward(self, input_dict, state, seq_lens):\n            # explicit cast to float32 needed in eager\n            obs = tf.cast(input_dict[\"obs\"], tf.float32)\n            logits, self._value = self.base_model(obs)\n            return logits, state\n    ```", "```\n        def value_function(self):\n            return tf.reshape(self._value, [-1])\n    ```", "```\n    import ray\n    import sys\n    from ray import tune\n    from ray.rllib.models import ModelCatalog\n    if not \".\" in sys.path:\n        sys.path.insert(0, \".\")\n    from custom_model import CustomModel\n    ray.init()  # Can also initialize a cluster with multiple \n    #nodes here using the cluster head node’s IP\n    ```", "```\n    # Register custom-model in ModelCatalog\n    ModelCatalog.register_custom_model(\"CustomCNN\", \n                                        CustomModel)\n    experiment_analysis = tune.run(\n        \"PPO\",\n        config={\n            \"env\": \"procgen:procgen-coinrun-v0\",\n            \"num_gpus\": 0,\n            \"num_workers\": 2,\n            \"model\": {\"custom_model\": \"CustomCNN\"},\n            \"framework\": \"tf2\",\n            \"log_level\": \"INFO\",\n        },\n        local_dir=\"ray_results\",  # store experiment results\n        #  in this dir\n    )\n    ray.shutdown()\n    ```", "```\n    import sys\n    import ray\n    import ray.rllib.agents.impala as impala\n    from ray.tune.logger import pretty_print\n    from ray.rllib.models import ModelCatalog\n    if not \".\" in sys.path:\n        sys.path.insert(0, \".\")\n    from custom_model import CustomModel\n    ray.init()  # You can also initialize a multi-node ray \n    # cluster here\n    ```", "```\n    # Register custom-model in ModelCatalog\n    ModelCatalog.register_custom_model(\"CustomCNN\", \n                                        CustomModel)\n    config = impala.DEFAULT_CONFIG.copy()\n    config[\"num_gpus\"] = 0\n    config[\"num_workers\"] = 1\n    config[\"model\"][\"custom_model\"] = \"CustomCNN\"\n    config[\"log_level\"] = \"INFO\"\n    config[\"framework\"] = \"tf2\"\n    trainer = impala.ImpalaTrainer(config=config,\n                            env=\"procgen:procgen-coinrun-v0\")\n    ```", "```\n    for step in range(1000):\n        # Custom training loop\n        result = trainer.train()\n        print(pretty_print(result))\n        if step % 100 == 0:\n            checkpoint = trainer.save()\n            print(\"checkpoint saved at\", checkpoint\n    ```", "```\n    # Restore agent from a checkpoint and start a new \n    # training run with a different config\n    config[\"lr\"] =  ray.tune.grid_search([0.01, 0.001])\"]\n    ray.tune.run(trainer, config=config, restore=checkpoint)\n    ```", "```\n    ray.shutdown()\n    ```"]