<html><head></head><body><div id="sbo-rt-content"><div>&#13;
			<div id="_idContainer086" class="Basic-Graphics-Frame">&#13;
			</div>&#13;
		</div>&#13;
		<div id="_idContainer087" class="Content">&#13;
			<h1 id="_idParaDest-76"><a id="_idTextAnchor144"/>Section 3 â€“ Scaling and Tuning ML Works</h1>&#13;
			<p>Having covered how to set up a training job through various means of TensorFlow Enterprise model development, now is the time to scale the training process by using a cluster of GPUs or TPUs. You will learn how to leverage distributed training strategies and implement hyperparameter tuning to scale and improve your model training experiment.</p>&#13;
			<p>In this part, you will learn about how to set up GPUs and TPUs in a GCP environment for submitting a model training job in GCP. You also will learn about the latest hyperparameter tuning API and run it at scale using GCP resources.</p>&#13;
			<p>This section comprises the following chapters:</p>&#13;
			<ul>&#13;
				<li><a href="B16070_05_Final_JM_ePub.xhtml#_idTextAnchor145"><em class="italic">Chapter 5</em></a>, <em class="italic">Training at Scale</em></li>&#13;
				<li><a href="B16070_06_Final_JM_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 6</em></a>, <em class="italic">Hyperparameter Tuning</em></li>&#13;
			</ul>&#13;
		</div>&#13;
	</div></body></html>