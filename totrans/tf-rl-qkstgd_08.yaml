- en: Deep RL Applied to Autonomous Driving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autonomous driving is one of the hottest technological revolutions in development
    as of the time of writing this. It will dramatically alter how humanity looks
    at transportation in general, and will drastically reduce travel costs as well
    as increase safety. Several state-of-the-art algorithms are used by the autonomous
    vehicle development community to this end. These include, but are not limited
    to, perception, localization, path planning, and control. Perception deals with
    the identification of the environment around an autonomous vehicle—pedestrians,
    cars, bicycles, and so on. Localization involves the identification of the exact
    location—or pose to be more precise—of the vehicle in a precomputed map of the
    environment. Path planning, as the name implies, is the process of planning the
    path of the autonomous vehicle, both in the long term (say, from point *A* to
    point *B*) as well as the shorter term (say, the next 5 seconds). Control is the
    actual execution of the desired path, including evasive maneuvers. In particular,
    **reinforcement learning** (**RL**) is widely used in the path planning and control
    of the autonomous vehicle, both for urban as well as highway driving.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use the **The Open Racing Car Simulator** (**TORCS**)
    simulator to train an RL agent to learn to successfully drive on a racetrack.
    While the CARLA simulator is more robust and has realistic rendering, TORCS is
    easier to use and so is a good first option. The interested reader is encouraged
    to try out training RL agents on the CARLA simulator after completing this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning to use TORCS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a **Deep Deterministic Policy Gradient** (**DDPG**) agent to learn
    to drive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a **Proximal Policy Optimization** (**PPO**) agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete this chapter, we will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Python (version 2 or 3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (version 1.4 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TORCS racing car simulator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Car driving simulators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Applying RL in autonomous driving necessitates the use of robust car-driving
    simulators, as the RL agent cannot be trained on the road directly. To this end,
    several open source car-driving simulators have been developed by the research
    community, with each having its own pros and cons. Some of the open source car
    driving simulators are:'
  prefs: []
  type: TYPE_NORMAL
- en: CARLA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://vladlen.info/papers/carla.pdf](http://vladlen.info/papers/carla.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Developed at Intel labs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Suited to urban driving
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TORCS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Racing car
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepTraffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://selfdrivingcars.mit.edu/deeptraffic/](https://selfdrivingcars.mit.edu/deeptraffic/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Developed at MIT
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Suited to highway driving
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to use TORCS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will first learn how to use the TORCS racing car simulator, which is an
    open source simulator. You can obtain the download instructions from [http://torcs.sourceforge.net/index.php?name=Sections&op=viewarticle&artid=3](http://torcs.sourceforge.net/index.php?name=Sections&op=viewarticle&artid=3)
    but the salient steps are summarized as follows for Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the `torcs-1.3.7.tar.bz2` file from [https://sourceforge.net/projects/torcs/files/all-in-one/1.3.7/torcs-1.3.7.tar.bz2/download](https://sourceforge.net/projects/torcs/files/all-in-one/1.3.7/torcs-1.3.7.tar.bz2/download)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unpack the package with `tar xfvj torcs-1.3.7.tar.bz2`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cd torcs-1.3.7`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`./configure`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`make`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`make install`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`make datainstall`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The default installation directories are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`/usr/local/bin`: TORCS command (directory should be in your `PATH`)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/usr/local/lib/torcs`: TORCS dynamic `libs` (directory MUST be in your `LD_LIBRARY_PATH`
    if you don''t use the TORCS shell)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/usr/local/share/games/torcs`: TORCS data files'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By running the `torcs` command (the default location is `/usr/local/bin/torcs`),
    you can now see the TORCS simulator open. The desired settings can then be chosen,
    including the choice of the car, racetrack, and so on. The simulator can also
    be played as a video game, but we are interested in using it to train an RL agent.
  prefs: []
  type: TYPE_NORMAL
- en: State space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will next define the state space for TORCS. *Table 2: Description of the
    available sensors (part II). Ranges are reported with their unit of measure (where
    defined)* of the *Simulated Car Racing Championship: Competition Software Manual*
    document at [https://arxiv.org/pdf/1304.1672.pdf](https://arxiv.org/pdf/1304.1672.pdf)
    provides a summary of the state parameters that are available for the simulator.
    We will use the following entries as our state space; the number in brackets identifies
    the size of the entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '`angle`: Angle between the car direction and the track (1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`track`: This will give us the end of the track measured every 10 degrees from
    -90 to +90 degrees; it has 19 real values, counting the end values (19)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trackPos`: Distance between the car and the track axis (1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speedX`: Speed of the car in the longitudinal direction (1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speedY`: Speed of the car in the transverse direction (1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speedZ`: Speed of the car in the *Z*-direction; we don''t need this actually,
    but we retain it for now (1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wheelSpinVel`: The rotational speed of the four wheels of the car (4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rpm`: The car engine''s rpm (1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See the previously mentioned document for a better understanding of the preceding
    variables, including their permissible ranges. Summing up the number of real valued
    entries, we note that our state space is a real valued vector of size 1+19+1+1+1+1+4+1
    = 29\. Our action space is of size *3*: the steering, acceleration, and brake.
    Steering is in the range [*-1,1*], and acceleration is in the range [*0,1*], as
    is the brake.'
  prefs: []
  type: TYPE_NORMAL
- en: Support files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The open source community has also developed two Python files to interface
    TORCS with Python so that we can call TORCS from Python commands. In addition,
    to automatically start TORCS, we need another `sh` file. These three files are
    summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gym_torcs.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`snakeoil3_gym.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`autostart.sh`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These files are included in the code files for the chapter ([https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide](https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide)),
    but can also be obtained from a Google search. In lines ~130-160 of `gym_torcs.py`,
    the reward function is set. You can see the following lines, which convert the
    raw simulator states to NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The reward function is then set as follows. Note that we give rewards for higher
    longitudinal speed along the track (the cosine of the angle term), and penalize
    lateral speed (the sine of the angle term). Track position is also penalized.
    Ideally, if this were zero, we would be at the center of the track, and values
    of *+1* or *-1* imply that we are at the edges of the track, which is not desired
    and hence penalized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We terminate the episode if the car is out of the track and/or the progress
    of the agent is stuck using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to train an RL agent to drive a car in TORCS successfully.
    We will use a DDPG agent first.
  prefs: []
  type: TYPE_NORMAL
- en: Training a DDPG agent to learn to drive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the DDPG code is the same as we saw earlier in [Chapter 5](c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml), *Deep
    Deterministic Policy Gradients (DDPG)*; only the differences will be summarized
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Coding ddpg.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our state dimension for TORCS is `29` and the action dimension is `3`; these
    are set in `ddpg.py` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Coding AandC.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The actor and critic file, `AandC.py`, also needs to be modified. In particular,
    the `create_actor_network` in the `ActorNetwork` class is edited to have two hidden
    layers with `400` and `300` neurons, respectively. Also, the output consists of
    three actions: `steering`, `acceleration`, and `brake`. Since steering is in the
    [*-1,1*] range, the `tanh` activation function is used; `acceleration` and `brake`
    are in the [*0,1*] range, and so the `sigmoid` activation function is used. We
    then `concat` them along axis dimension `1`, and this is the output of our actor''s
    policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, the `CriticNetwork` class `create_critic_network()` function is edited
    to have two hidden layers for the neural network, with `400` and `300` neurons,
    respectively. This is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The other changes to be made are in `TrainOrTest.py`, which we will look into next.
  prefs: []
  type: TYPE_NORMAL
- en: Coding TrainOrTest.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the TORCS environment from `gym_torcs` so that we can train the RL agent
    on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Import TORCS**: Import the TORCS environment from `gym_torcs` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**The** `env` **variable**: Create a TORCS environment variable using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Relaunch TORCS**: Since TORCS is known to have a memory leak error, reset
    the environment every `100` episodes using `relaunch=True`; otherwise reset without
    any arguments as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Stack up state space**: Use the following command to stack up the 29-dimension
    space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Number of time steps per episode**: Choose the number of time steps `msteps` to
    run per episode. For the first `100` episodes, the agent has not learned much,
    and so you can choose `100` time steps per episode; we gradually increase this
    linearly for later episodes up to the upper limit of `max_steps`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step is not critical and the agent's learning is not dependent on the number
    of steps we choose per episode. Feel free to experiment with how `msteps` is set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose the number of time steps as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Full throttle**: For the first `10` episodes, we apply full throttle to warm
    up the neural network parameters. Only after that, do we start using the actor''s
    policy. Note that TORCS typically learns in about ~1,500–2,000 episodes, so the
    first `10` episodes will not really have much influence later on in the learning.
    Apply full throttle to warm up the neural network parameters as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it for the changes that need to be made to the code for the DDPG to
    play TORCS. The rest of the code is the same as that covered in [Chapter 5](c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml),
    *Deep Deterministic Policy Gradients (DDPG)*. We can train the agent using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter `1` for training; `0` is for testing a pretrained agent. Training can
    take about 2–5 days depending on the speed of the computer used. But this is a
    fun problem and is worth the effort. The number of steps experienced per episode,
    as well as the rewards, are stored in `analysis_file.txt`, which we can plot.
    The number of time steps per episode is plotted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a0210b4-3bb8-4c79-9db0-22b783f78002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Number of time steps per episode of TORCS (training mode)'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the car has learned to drive reasonably well after ~600 episodes,
    with more efficient driving after ~1,500 episodes. Approximately ~300 time steps
    correspond to one lap of the racetrack. Thus, the agent is able to drive more
    than seven to eight laps without terminating toward the end of the training. For
    a cool video of the DDPG agent driving, see the following YouTube link: [https://www.youtube.com/watch?v=ajomz08hSIE](https://www.youtube.com/watch?v=ajomz08hSIE).
  prefs: []
  type: TYPE_NORMAL
- en: Training a PPO agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw previously how to train a DDPG agent to drive a car on TORCS. How to
    use a PPO agent is left as an exercise for the interested reader. This is a nice
    challenge to complete. The PPO code from [Chapter 7](7f55a061-06a5-4f69-ab05-4eff75c2dacd.xhtml),
    *Trust Region Policy Optimization and Proximal Policy Optimization*, can be reused,
    with the necessary changes made to the TORCS environment. The PPO code for TORCS
    is also supplied in the code repository ([https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide](https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide)),
    and the interested reader can peruse it. A cool video of a PPO agent driving a
    car in TORCS is in the following YouTube video at: [https://youtu.be/uE8QaJQ7zDI](https://youtu.be/uE8QaJQ7zDI)'
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge for the interested reader is to use **Trust Region Policy
    Optimization** (**TRPO**) for the TORCS racing car problem. Try this too, if interested!
    This is one way to master RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to apply RL algorithms to train an agent to learn
    to drive a car autonomously. We installed the TORCS racing-car simulator and also
    learned how to interface it with Python, so that we can train RL agents. We also
    did a deep dive into the state space for TORCS and the meaning of each of these
    terms. The DDPG algorithm was then used to train an agent to learn to drive successfully
    in TORCS. The video rendering in TORCS is really cool! The trained agent was able
    to drive more than seven to eight laps around the racetrack successfully. Finally,
    the use of PPO for the same problem of driving a car autonomously was also explored
    and left as an exercise for the interested reader; code for this is supplied in
    the book's repository.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes this chapter as well as the book. Feel free to read upon more
    material online on the application of RL for autonomous driving and robotics.
    This is now a very hot area of both academic and industry research, and is well
    funded, with several job openings in these areas. Wishing you the best!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why can you not use DQN for the TORCS problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We used the Xavier weights initializer for the neural network weights. What
    other weight initializers are you aware of, and how well will the trained agent
    perform with them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the `abs()` function used in the reward function, and why is it used
    for the last two terms but not for the first term?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you ensure smoother driving than what was observed in the video?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is a replay buffer used in DDPG but not in PPO?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Continuous control with deep reinforcement learning*, Timothy P. Lillicrap,
    Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
    Silver, Daan Wierstra, arXiv:1509.02971 (DDPG paper): [https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Proximal Policy Optimization Algorithms*, John Schulman, Filip Wolski, Prafulla
    Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347 (PPO paper): [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TORCS: [http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Reinforcement Learning Hands-On*, by *Maxim Lapan*, *Packt Publishing*:
    [https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands](https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
