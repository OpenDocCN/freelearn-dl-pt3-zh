<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generative Adversarial Networks</h1>
                </header>
            
            <article>
                
<p class="calibre2"><strong class="calibre13">Generative Adversarial Networks</strong> (<strong class="calibre13">GANs</strong>) are deep neural net architectures that consist of two networks pitted against each other (hence the name <strong class="calibre13">adversarial</strong>).</p>
<p class="calibre2">GANs were introduced in a paper (<a href="https://arxiv.org/abs/1406.2661" target="_blank" class="calibre11">https://arxiv.org/abs/1406.2661</a>) by Ian Goodfellow and other researchers<span class="calibre10">, including Yoshua Bengio,</span> at the University of Montreal in 2014. Referring to GANs, Facebook's AI research director, Yann LeCun, called <strong class="calibre13">adversarial training</strong> the most interesting idea in the last 10 years in machine learning.</p>
<p class="calibre2">The potential of GANs is huge, because they can learn to mimic any distribution of data. That is, GANs can be taught to create worlds eerily similar to our own in any domain: images, music, speech, or prose. They are robot artists in a sense, and their output is impressive (<a href="https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html" target="_blank" class="calibre11">https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html</a>)—and poignant too.</p>
<p class="calibre2">The following topics will be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">An intuitive introduction</li>
<li class="calibre8">Simple implementation of GANs</li>
<li class="calibre8">Deep convolutional GANs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An intuitive introduction</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to introduce GANs in a very intuitive way. To get an idea of how GANs work, we will adopt a fake scenario of getting a ticket for a party.</p>
<p class="calibre2">The story starts with a very interesting party or event being held somewhere, and you are very interested in attending it. You hear about this event very late and all the tickets are sold out, but you will do anything to get into the party. So you come up with an idea! You will try to fake a ticket that needs to be exactly the same as the original one, or very, very similar to it. But because life is not easy, there's another challenge: you don't know what the original ticket looks like. So from your experience of going to such parties, you start to imagine what the ticket might look like and start to design the ticket based on your imagination.</p>
<p class="calibre2">You will try to design the ticket and then go to the event and show the ticket to the security guys. Hopefully, they will be convinced and will let you in. But you don't want to show your face multiple times to the security guards, so you decide to get help from your friend, who will take your initial guess about the original ticket and show it to the security guards. If they don't let him in, he will get some information for you about what the ticket might look like, based on seeing some people getting in with the actual ticket. You will refine the ticket based on your friend's comments until the security guards let him in. At this point<span class="calibre10">—</span>and at this point only—you will design another one that has exactly the same look and get <span class="calibre10">yourself </span>in.</p>
<p class="calibre2">Do, think too much about how unrealistic this story <span class="calibre10">is, </span>but the way GANs work is very similar to this story. GANs are very trendy nowadays, and people are using them for many applications in the field of computer vision.</p>
<p class="calibre2">There are many interesting applications that you can use GANs for, and we will implement and mention some of them.</p>
<p class="calibre2">In GANs, there are two main components that have made a breakthrough in many computer vision fields. The first component is called <strong class="calibre13">Generator</strong> and the second one is called <strong class="calibre13">Discriminator</strong>:</p>
<ul class="calibre7">
<li class="calibre8">The Generator will try to generate data samples out of a specific probability distribution, which is very similar to the guy who was trying to replicate a ticket for the event</li>
<li class="calibre8">The Discriminator will judge (like the security guys who are trying to find flaws in the ticket to decide whether it's original or fake) whether its input is coming from the original training set (an original ticket) or from the generator part (designed by the guy who's trying to replicate the original ticket):</li>
</ul>
<div class="CDPAlignCenter"><img src="assets/4961129f-2166-4bfd-9304-70a05d5d46c8.png" class="calibre167"/></div>
<div class="title-page-name">
<div class="CDPAlignCenter1">Figure 1: GANs – general architecture</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simple implementation of GANs</h1>
                </header>
            
            <article>
                
<p class="calibre2">From the story of faking a ticket to an event, the idea of GANs seems to be very intuitive. So to get a clear understanding of how GANs work and how to implement them, we are going to demonstrate a simple implementation of a GAN on the MNIST dataset.</p>
<p class="calibre2">First, we need to build the core of the GAN network, which is comprised of two major components: the generator and the discriminator. As we said, the generator will try to imagine or fake data samples from a specific probability distribution; the discriminator, which has access to and sees the actual data samples, will judge whether the generator's output has any flaws in the design or it's very close to the original data samples. Similar to the scenario of the event, the whole purpose of the generator is to try to convince the discriminator that the generated image is from the real dataset and hence try to fool him.</p>
<p class="calibre2">The training process has a similar end to the event story; the generator will finally manage to generate images that look very similar to the original data samples:</p>
<div class="CDPAlignCenter"><img src="assets/45302309-65dd-4e06-96f8-c1c5ca5d1569.png" class="calibre168"/></div>
<div class="CDPAlignCenter1">Figure 2: GAN general architecture for the MNIST dataset<br class="title-page-name"/></div>
<div class="title-page-name">
<p class="calibre2">The typical structure of any GAN is shown in <em class="calibre19">Figure 2</em>, which will be trained on the MNIST dataset. The <kbd class="calibre12">Latent sample</kbd> part in this figure is a random thought or vector that the generator uses to replicate the real images with fake ones.</p>
<p class="calibre2">As we mentioned, the discriminator works as a judge and it will try to separate the real images from the fake ones that were designed by the generator. So the output of this network will be binary, which can be represented by a sigmoid function with 0 (meaning the input is a fake image) and 1 (meaning that the input is a real image).</p>
</div>
<p class="calibre2">Let's go ahead and start implementing this architecture to see how it performs on the MNIST dataset.</p>
<p class="calibre2">Let's start of by importing the required libraries for this implementation:</p>
<pre class="calibre21">%matplotlib inline<br class="title-page-name"/><br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>import pickle as pkl<br class="title-page-name"/><br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import tensorflow as tf</pre>
<p class="calibre2">We will be using the MNIST dataset, so we are going to use TensorFlow helpers to get the dataset and store it somewhere:</p>
<pre class="calibre21">from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/>mnist_dataset = input_data.read_data_sets('MNIST_data')</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>Extracting MNIST_data/train-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/train-labels-idx1-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model inputs</h1>
                </header>
            
            <article>
                
<p class="calibre2">Before diving into building the core of the GAN, which is represented by the generator and discriminator, we are going to define the inputs of our computational graph. As shown in <em class="calibre19">Figure 2</em>, we need two inputs. The first one will be the real images, which will be fed to the discriminator. The other input is called <strong class="calibre13">latent space</strong>, which will be fed to the generator and used to generate its fake images:</p>
<pre class="calibre21"># Defining the model input for the generator and discrimator<br class="title-page-name"/>def inputs_placeholders(discrimator_real_dim, gen_z_dim):<br class="title-page-name"/>    real_discrminator_input = tf.placeholder(tf.float32, (None, discrimator_real_dim), name="real_discrminator_input")<br class="title-page-name"/>    generator_inputs_z = tf.placeholder(tf.float32, (None, gen_z_dim), name="generator_input_z")<br class="title-page-name"/>    <br class="title-page-name"/>    return real_discrminator_input, generator_inputs_z</pre>
<div class="CDPAlignCenter"><img src="assets/c738df95-3bb7-40dd-9862-b5d6f2d3292a.png" class="calibre169"/></div>
<div class="CDPAlignCenter1">Figure 3: Architecture of the MNIST GAN implementation</div>
<p class="calibre2">Now it's time to dive into building the <span class="calibre10">two </span>core components of our architecture. We will start by building the generator part. As shown in <em class="calibre19">Figure 3</em>, the generator will consist of at least one hidden layer, which will work as an approximator. Also, instead of using the normal ReLU activation function, we will use something called a leaky ReLU. This will allow the gradient values to flow through the layer without any constraints (more in the next section about leaky RelU).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variable scope</h1>
                </header>
            
            <article>
                
<p class="calibre2">Variable scope is a feature of TensorFlow that helps us to do the following:</p>
<ul class="calibre7">
<li class="calibre8">Make sure that we have some naming conventions to retrieve them later, for example, by making them start with the word generator or discriminator, which will help us during the training of the network. We could have used the name scope feature, but this feature won't help us for the second purpose.</li>
<li class="calibre8">To be able to reuse or retrain the same network but with different inputs. For example, we are going to sample fake images from the generator to see how good the generator is for replicating the original ones. Also, the discriminator will have access to the real and fake images, which will make it easy for us to reuse the variables instead of creating new ones while building the computational graph.</li>
</ul>
<p class="calibre2">The following statement will show how to use the variable scope feature of TensorFlow:</p>
<pre class="calibre21">with tf.variable_scope('scopeName', reuse=False):<br class="title-page-name"/>    # Write your code here</pre>
<p class="calibre2">You can read more about the benefits of using the variable scope feature at <a href="https://www.tensorflow.org/programmers_guide/variable_scope#the_problem" target="_blank" class="calibre11">https://www.tensorflow.org/programmers_guide/variable_scope#the_problem</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Leaky ReLU</h1>
                </header>
            
            <article>
                
<p class="calibre2">We mentioned that we will be using a different version than the ReLU activation function, which is called leaky ReLU. The traditional version of the ReLU activation function will just take the maximum between the input value and zero, by other means truncating negative values to zero. Leaky ReLU, which is the version that we will be using, allows some negative values to exist, hence the name <strong class="calibre13">leaky ReLU</strong>.</p>
<p class="calibre2">Sometimes, if we use the traditional ReLU activation function, the network gets stuck in a popular state called the dying state, and that's because the network produces nothing but zeros for all the outputs.</p>
<p class="calibre2">The idea of using leaky ReLU is to prevent this dying state by allowing some negative values to pass through.</p>
<p class="calibre2">The whole idea behind making the generator work is to receive gradient values from the discriminator, and if the network is stuck in a dying situation, the learning process won't happen.</p>
<p class="calibre2">The following figures illustrate the difference between traditional ReLU and its leaky version:</p>
<div class="CDPAlignCenter"><img src="assets/282a88ea-0437-4ea8-b383-342103b56ce6.png" class="calibre170"/></div>
<div class="CDPAlignCenter1">Figure 4: ReLU function</div>
<div class="CDPAlignCenter"><img src="assets/1138c8e8-e7cd-47a0-b5d0-7a899bc7e5e9.png" class="calibre171"/></div>
<div class="CDPAlignCenter1">Figure 5: Leaky ReLU activation functions</div>
<p class="calibre2">The leaky ReLU activation function is not implemented in TensorFlow, so we need to implement it ourselves. The output of this activation function will be positive if the input is positive, and will be a controlled negative value if the input is negative. We will control the negative value by a parameter called <strong class="calibre13">alpha</strong>, which will introduce tolerance of the network by allowing some negative values to pass.</p>
<p class="calibre2">The following equation represents the leaky ReLU that we will be implementing:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation33" src="assets/87b4a5a5-f789-4a60-b7a6-30053c8fb0cd.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generator</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">The MNIST images are normalized between 0 and 1, where the <kbd class="calibre12">sigmoid</kbd> activation function can work best. But in practice, the <kbd class="calibre12">tanh</kbd> activation function is found to give better performance than any other function. So, in order to use the <kbd class="calibre12">tanh</kbd> activation function, we will need to re-scale the range of the pixel values of these images to be between -1 and 1:</p>
<pre class="calibre21">def generator(gen_z, gen_out_dim, num_hiddern_units=128, reuse_vars=False, leaky_relu_alpha=0.01):<br class="title-page-name"/>    <br class="title-page-name"/>    ''' Building the generator part of the network<br class="title-page-name"/>    <br class="title-page-name"/>        Function arguments<br class="title-page-name"/>        ---------<br class="title-page-name"/>        gen_z : the generator input tensor<br class="title-page-name"/>        gen_out_dim : the output shape of the generator<br class="title-page-name"/>        num_hiddern_units : Number of neurons/units in the hidden layer<br class="title-page-name"/>        reuse_vars : Reuse variables with tf.variable_scope<br class="title-page-name"/>        leaky_relu_alpha : leaky ReLU parameter<br class="title-page-name"/>        <br class="title-page-name"/>        Function Returns<br class="title-page-name"/>        -------<br class="title-page-name"/>        tanh_output, logits_layer: <br class="title-page-name"/>    '''<br class="title-page-name"/>    with tf.variable_scope('generator', reuse=reuse_vars):<br class="title-page-name"/>        <br class="title-page-name"/>        # Defining the generator hidden layer<br class="title-page-name"/>        hidden_layer_1 = tf.layers.dense(gen_z, num_hiddern_units, activation=None)<br class="title-page-name"/>        <br class="title-page-name"/>        # Feeding the output of hidden_layer_1 to leaky relu<br class="title-page-name"/>        hidden_layer_1 = tf.maximum(hidden_layer_1, leaky_relu_alpha*hidden_layer_1)<br class="title-page-name"/>        <br class="title-page-name"/>        # Getting the logits and tanh layer output<br class="title-page-name"/>        logits_layer = tf.layers.dense(hidden_layer_1, gen_out_dim, activation=None)<br class="title-page-name"/>        tanh_output = tf.nn.tanh(logits_layer)<br class="title-page-name"/>        <br class="title-page-name"/>        return tanh_output, logits_layer</pre></div>
<p class="calibre2">Now we have the generator part ready. Let's go ahead and define the second component of the network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discriminator</h1>
                </header>
            
            <article>
                
<p class="calibre2">Next up, we will build the second main component in the generative adversarial network, which is the discriminator. The discriminator is pretty much the same as the generator, but instead of using the <kbd class="calibre12">tanh</kbd> activation function, we will be using the <kbd class="calibre12">sigmoid</kbd> activation function; it will produce a binary output that will represent the judgment of the discriminator on the input image:</p>
<pre class="calibre21">def discriminator(disc_input, num_hiddern_units=128, reuse_vars=False, leaky_relu_alpha=0.01):<br class="title-page-name"/>    ''' Building the discriminator part of the network<br class="title-page-name"/>    <br class="title-page-name"/>        Function Arguments<br class="title-page-name"/>        ---------<br class="title-page-name"/>        disc_input : discrminator input tensor<br class="title-page-name"/>        num_hiddern_units : Number of neurons/units in the hidden layer<br class="title-page-name"/>        reuse_vars : Reuse variables with tf.variable_scope<br class="title-page-name"/>        leaky_relu_alpha : leaky ReLU parameter<br class="title-page-name"/>        <br class="title-page-name"/>        Function Returns<br class="title-page-name"/>        -------<br class="title-page-name"/>        sigmoid_out, logits_layer: <br class="title-page-name"/>    '''<br class="title-page-name"/>    with tf.variable_scope('discriminator', reuse=reuse_vars):<br class="title-page-name"/>        <br class="title-page-name"/>        # Defining the generator hidden layer<br class="title-page-name"/>        hidden_layer_1 = tf.layers.dense(disc_input, num_hiddern_units, activation=None)<br class="title-page-name"/>        <br class="title-page-name"/>        # Feeding the output of hidden_layer_1 to leaky relu<br class="title-page-name"/>        hidden_layer_1 = tf.maximum(hidden_layer_1, leaky_relu_alpha*hidden_layer_1)<br class="title-page-name"/>        <br class="title-page-name"/>        logits_layer = tf.layers.dense(hidden_layer_1, 1, activation=None)<br class="title-page-name"/>        sigmoid_out = tf.nn.sigmoid(logits_layer)<br class="title-page-name"/>        <br class="title-page-name"/>        return sigmoid_out, logits_layer</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the GAN network</h1>
                </header>
            
            <article>
                
<p class="calibre2">After defining the main functions that will build the generator and discriminator parts, it's time to stack them up and define the model losses and optimizers for this implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model hyperparameters</h1>
                </header>
            
            <article>
                
<p class="calibre2">We can fine-tune the GANs by changing the following set of hyperparameters:</p>
<pre class="calibre21"># size of discriminator input image<br class="title-page-name"/>#28 by 28 will flattened to be 784<br class="title-page-name"/>input_img_size = 784 <br class="title-page-name"/><br class="title-page-name"/># size of the generator latent vector<br class="title-page-name"/>gen_z_size = 100<br class="title-page-name"/><br class="title-page-name"/># number of hidden units for the generator and discriminator hidden layers<br class="title-page-name"/>gen_hidden_size = 128<br class="title-page-name"/>disc_hidden_size = 128<br class="title-page-name"/><br class="title-page-name"/>#leaky ReLU alpha parameter which controls the leak of the function<br class="title-page-name"/>leaky_relu_alpha = 0.01<br class="title-page-name"/><br class="title-page-name"/># smoothness of the label <br class="title-page-name"/>label_smooth = 0.1</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the generator and discriminator</h1>
                </header>
            
            <article>
                
<p class="calibre2">After defining the two main parts of our architecture that will be used to generate fake MNIST images (which will look exactly the same as the real ones), it's time to build the network using the functions that we have defined so far. In order to build the network, we are going to follow these steps:</p>
<ol class="calibre16">
<li class="calibre8">Defining the inputs for our model, which will consist of two variables. One of these variables will be the real images, which will be fed to the discriminator, and the second will be the latent space to be used by the generator to replicate the original images.</li>
<li class="calibre8">Calling the defined generator function to build the generator part of the network.</li>
<li class="calibre8">Calling the defined discriminator function to build the discriminator part of the network, but we are going to call this function twice. One call will be for the real data and the second call will be for the fake data from the generator.</li>
<li class="calibre8">Keeping the weights of real and fake images the same by reusing the variables:</li>
</ol>
<pre class="calibre172">tf.reset_default_graph()<br class="title-page-name"/><br class="title-page-name"/># creating the input placeholders for the discrminator and generator<br class="title-page-name"/>real_discrminator_input, generator_input_z = inputs_placeholders(input_img_size, gen_z_size)<br class="title-page-name"/><br class="title-page-name"/>#Create the generator network<br class="title-page-name"/>gen_model, gen_logits = generator(generator_input_z, input_img_size, gen_hidden_size, reuse_vars=False, leaky_relu_alpha=leaky_relu_alpha)<br class="title-page-name"/><br class="title-page-name"/># gen_model is the output of the generator<br class="title-page-name"/>#Create the generator network<br class="title-page-name"/>disc_model_real, disc_logits_real = discriminator(real_discrminator_input, disc_hidden_size, reuse_vars=False, leaky_relu_alpha=leaky_relu_alpha)<br class="title-page-name"/>disc_model_fake, disc_logits_fake = discriminator(gen_model, disc_hidden_size, reuse_vars=True, leaky_relu_alpha=leaky_relu_alpha)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discriminator and generator losses</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this part, we need to define the discriminator and generator losses, and this can be considered to be the most tricky part of this implementation.</p>
<p class="calibre2">We know that the generator tries to replicate the original images and that the discriminator works as a judge, receiving both images from the generator and the original input images. So while designing our loss for each part, we need to target two things.</p>
<p class="calibre2">First, we need the discriminator part of the network to be able to distinguish between the fake images generated by the generator and the real images coming from the original training examples. During training time, we will feed the discriminator part with a batch that is divided into two categories. The first category will be images from the original input and the second category will be images from the fake ones that got generated by the generator.</p>
<p class="calibre2">So the final general loss of the discriminator will be the sum of its ability to accept the real ones as real and detect the fake ones as fake; then the final total loss will be:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation34" src="assets/85448d3e-1c11-4804-a615-f1b6463e6b26.png"/></div>
<pre class="calibre21">tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_layer, labels=labels))</pre>
<p class="calibre2">So we need to calculate two losses to come up with the final discriminator loss.</p>
<p class="calibre2">The first loss, <kbd class="calibre12">disc_loss_real</kbd>, will be calculated based on the <kbd class="calibre12">logits</kbd> values that we will get from the discriminator and the <kbd class="calibre12">labels</kbd>, which will be all ones in this case since we know that all the images in this mini-batch are all coming from the real input images of the MNIST dataset. To enhance the ability of the model to generalize on the test set and give better results, people have found that practically changing the value of <span class="calibre10">1</span><span class="calibre10"> </span><span class="calibre10">to 0.9 is better. This kind of change to the label introduces something called <strong class="calibre13">label smooth</strong>:</span></p>
<pre class="calibre21"> labels = tf.ones_like(tensor) * (1 - smooth)</pre>
<p class="calibre2">For the second part of the discriminator loss, which is the ability of the discriminator to detect fake images, the loss will be between the logits values that we will get from the discriminator and labels; all of these are zeros since we know that all the images in this mini-batch are coming from the generator, and not from the original input.</p>
<p class="calibre2">Now that we have discussed the discriminator loss, we need to calculate the generator loss as well. The generator loss will be called <kbd class="calibre12">gen_loss</kbd>, which will be the loss between <kbd class="calibre12">disc_logits_fake</kbd> (the output of the discriminator for the fake images) and the labels (which will be all ones since the generator is trying to convince the discriminator with its design of the fake image):</p>
<pre class="calibre21"><br class="title-page-name"/># calculating the losses of the discrimnator and generator<br class="title-page-name"/>disc_labels_real = tf.ones_like(disc_logits_real) * (1 - label_smooth)<br class="title-page-name"/>disc_labels_fake = tf.zeros_like(disc_logits_fake)<br class="title-page-name"/><br class="title-page-name"/>disc_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(labels=disc_labels_real, logits=disc_logits_real)<br class="title-page-name"/>disc_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(labels=disc_labels_fake, logits=disc_logits_fake)<br class="title-page-name"/><br class="title-page-name"/>#averaging the disc loss<br class="title-page-name"/>disc_loss = tf.reduce_mean(disc_loss_real + disc_loss_fake)<br class="title-page-name"/><br class="title-page-name"/>#averaging the gen loss<br class="title-page-name"/>gen_loss = tf.reduce_mean(<br class="title-page-name"/>    tf.nn.sigmoid_cross_entropy_with_logits(<br class="title-page-name"/>        labels=tf.ones_like(disc_logits_fake), <br class="title-page-name"/>        logits=disc_logits_fake))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizers</h1>
                </header>
            
            <article>
                
<p class="calibre2">Finally, the optimizers part! In this section, we will define the optimization criteria that will be used during the training process. First off, we are going to update the variables of the generator and discriminator separately, so we need to be able to retrieve the variables of each part.</p>
<p class="calibre2">For the first optimizer, the generator one, we will retrieve all the variables that start with the name <kbd class="calibre12">generator</kbd> from the trainable variables of the computational graph; then we can check which variable is which by referring to its name.</p>
<p class="calibre2">We'll do the same for the discriminator variables as well, by letting in all variables that start with <kbd class="calibre12">discriminator</kbd>. After that, we can pass the list of variables that we want to be optimized to the optimizer.</p>
<p class="calibre2">So the variable scope feature of TensorFlow gave us the ability to retrieve variables that start with a certain string, and then we can have two different lists of variables, one for the generator and another one for the discriminator:</p>
<pre class="calibre21"><br class="title-page-name"/># building the model optimizer<br class="title-page-name"/><br class="title-page-name"/>learning_rate = 0.002<br class="title-page-name"/><br class="title-page-name"/># Getting the trainable_variables of the computational graph, split into Generator and Discrimnator parts<br class="title-page-name"/>trainable_vars = tf.trainable_variables()<br class="title-page-name"/>gen_vars = [var for var in trainable_vars if var.name.startswith("generator")]<br class="title-page-name"/>disc_vars = [var for var in trainable_vars if var.name.startswith("discriminator")]<br class="title-page-name"/><br class="title-page-name"/>disc_train_optimizer = tf.train.AdamOptimizer().minimize(disc_loss, var_list=disc_vars)<br class="title-page-name"/>gen_train_optimizer = tf.train.AdamOptimizer().minimize(gen_loss, var_list=gen_vars)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now let's kick off the training process and see how GANs will manage to generate images similar to the MNIST ones:</p>
<pre class="calibre21">train_batch_size = 100<br class="title-page-name"/>num_epochs = 100<br class="title-page-name"/>generated_samples = []<br class="title-page-name"/>model_losses = []<br class="title-page-name"/><br class="title-page-name"/>saver = tf.train.Saver(var_list = gen_vars)<br class="title-page-name"/><br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/>    sess.run(tf.global_variables_initializer())<br class="title-page-name"/>    <br class="title-page-name"/>    for e in range(num_epochs):<br class="title-page-name"/>        for ii in range(mnist_dataset.train.num_examples//train_batch_size):<br class="title-page-name"/>            input_batch = mnist_dataset.train.next_batch(train_batch_size)<br class="title-page-name"/>            <br class="title-page-name"/>            # Get images, reshape and rescale to pass to D<br class="title-page-name"/>            input_batch_images = input_batch[0].reshape((train_batch_size, 784))<br class="title-page-name"/>            input_batch_images = input_batch_images*2 - 1<br class="title-page-name"/>            <br class="title-page-name"/>            # Sample random noise for G<br class="title-page-name"/>            gen_batch_z = np.random.uniform(-1, 1, size=(train_batch_size, gen_z_size))<br class="title-page-name"/>            <br class="title-page-name"/>            # Run optimizers<br class="title-page-name"/>            _ = sess.run(disc_train_optimizer, feed_dict={real_discrminator_input: input_batch_images, generator_input_z: gen_batch_z})<br class="title-page-name"/>            _ = sess.run(gen_train_optimizer, feed_dict={generator_input_z: gen_batch_z})<br class="title-page-name"/>        <br class="title-page-name"/>        # At the end of each epoch, get the losses and print them out<br class="title-page-name"/>        train_loss_disc = sess.run(disc_loss, {generator_input_z: gen_batch_z, real_discrminator_input: input_batch_images})<br class="title-page-name"/>        train_loss_gen = gen_loss.eval({generator_input_z: gen_batch_z})<br class="title-page-name"/>            <br class="title-page-name"/>        print("Epoch {}/{}...".format(e+1, num_epochs),<br class="title-page-name"/>              "Disc Loss: {:.3f}...".format(train_loss_disc),<br class="title-page-name"/>              "Gen Loss: {:.3f}".format(train_loss_gen)) <br class="title-page-name"/>        <br class="title-page-name"/>        # Save losses to view after training<br class="title-page-name"/>        model_losses.append((train_loss_disc, train_loss_gen))<br class="title-page-name"/>        <br class="title-page-name"/>        # Sample from generator as we're training for viegenerator_inputs_zwing afterwards<br class="title-page-name"/>        gen_sample_z = np.random.uniform(-1, 1, size=(16, gen_z_size))<br class="title-page-name"/>        generator_samples = sess.run(<br class="title-page-name"/>                       generator(generator_input_z, input_img_size, reuse_vars=True),<br class="title-page-name"/>                       feed_dict={generator_input_z: gen_sample_z})<br class="title-page-name"/>        <br class="title-page-name"/>        generated_samples.append(generator_samples)<br class="title-page-name"/>        saver.save(sess, './checkpoints/generator_ck.ckpt')<br class="title-page-name"/><br class="title-page-name"/># Save training generator samples<br class="title-page-name"/>with open('train_generator_samples.pkl', 'wb') as f:<br class="title-page-name"/>    pkl.dump(generated_samples, f)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch 71/100... Disc Loss: 1.078... Gen Loss: 1.361
Epoch 72/100... Disc Loss: 1.037... Gen Loss: 1.555
Epoch 73/100... Disc Loss: 1.194... Gen Loss: 1.297
Epoch 74/100... Disc Loss: 1.120... Gen Loss: 1.730
Epoch 75/100... Disc Loss: 1.184... Gen Loss: 1.425
Epoch 76/100... Disc Loss: 1.054... Gen Loss: 1.534
Epoch 77/100... Disc Loss: 1.457... Gen Loss: 0.971
Epoch 78/100... Disc Loss: 0.973... Gen Loss: 1.688
Epoch 79/100... Disc Loss: 1.324... Gen Loss: 1.370
Epoch 80/100... Disc Loss: 1.178... Gen Loss: 1.710
Epoch 81/100... Disc Loss: 1.070... Gen Loss: 1.649
Epoch 82/100... Disc Loss: 1.070... Gen Loss: 1.530
Epoch 83/100... Disc Loss: 1.117... Gen Loss: 1.705
Epoch 84/100... Disc Loss: 1.042... Gen Loss: 2.210
Epoch 85/100... Disc Loss: 1.152... Gen Loss: 1.260
Epoch 86/100... Disc Loss: 1.327... Gen Loss: 1.312
Epoch 87/100... Disc Loss: 1.069... Gen Loss: 1.759
Epoch 88/100... Disc Loss: 1.001... Gen Loss: 1.400
Epoch 89/100... Disc Loss: 1.215... Gen Loss: 1.448
Epoch 90/100... Disc Loss: 1.108... Gen Loss: 1.342
Epoch 91/100... Disc Loss: 1.227... Gen Loss: 1.468
Epoch 92/100... Disc Loss: 1.190... Gen Loss: 1.328
Epoch 93/100... Disc Loss: 0.869... Gen Loss: 1.857
Epoch 94/100... Disc Loss: 0.946... Gen Loss: 1.740
Epoch 95/100... Disc Loss: 0.925... Gen Loss: 1.708
Epoch 96/100... Disc Loss: 1.067... Gen Loss: 1.427
Epoch 97/100... Disc Loss: 1.099... Gen Loss: 1.573
Epoch 98/100... Disc Loss: 0.972... Gen Loss: 1.884
Epoch 99/100... Disc Loss: 1.292... Gen Loss: 1.610
Epoch 100/100... Disc Loss: 1.103... Gen Loss: 1.736</pre>
<p class="calibre2">After running the model for 100 epochs, we have a trained model that will be able to generate images similar to the original input images that we fed to the discriminator:</p>
<pre class="calibre21">fig, ax = plt.subplots()<br class="title-page-name"/>model_losses = np.array(model_losses)<br class="title-page-name"/>plt.plot(model_losses.T[0], label='Disc loss')<br class="title-page-name"/>plt.plot(model_losses.T[1], label='Gen loss')<br class="title-page-name"/>plt.title("Model Losses")<br class="title-page-name"/>plt.legend()</pre>
<p class="calibre2">Output:</p>
<div class="CDPAlignCenter"><img src="assets/f56c476d-c084-4820-81e9-d1571c8a5033.png" class="calibre173"/></div>
<div class="CDPAlignCenter1">Figure 6: Discriminator and Generator Losses</div>
<p class="calibre2">As shown in the preceding figure, you can see that the model losses, which are represented by the Discriminator and Generator lines, are converging.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generator samples from training</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's test the performance of the model and even see how the generation skills (designing tickets for the event) of the generator got enhanced while approaching the end of the training process:</p>
<pre class="calibre21">def view_generated_samples(epoch_num, g_samples):<br class="title-page-name"/>    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)<br class="title-page-name"/>    <br class="title-page-name"/>    print(gen_samples[epoch_num][1].shape)<br class="title-page-name"/>    <br class="title-page-name"/>    for ax, gen_image in zip(axes.flatten(), g_samples[0][epoch_num]):<br class="title-page-name"/>        ax.xaxis.set_visible(False)<br class="title-page-name"/>        ax.yaxis.set_visible(False)<br class="title-page-name"/>        img = ax.imshow(gen_image.reshape((28,28)), cmap='Greys_r')<br class="title-page-name"/>    <br class="title-page-name"/>    return fig, axes<br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">Before plotting some generated images from the last epoch in the training process, we need to load the persisted file that contains generated samples at each epoch during the training process:</p>
<pre class="calibre21"># Load samples from generator taken while training<br class="title-page-name"/>with open('train_generator_samples.pkl', 'rb') as f:<br class="title-page-name"/>    gen_samples = pkl.load(f)</pre>
<p class="calibre2">Now, let's plot the 16 generated images from the last epoch of the training process and see how the generator was able to generate meaningful numbers such as 3, 7, and 2:</p>
<pre class="calibre21">_ = view_generated_samples(-1, gen_samples)</pre>
<div class="CDPAlignCenter"><img src="assets/2e23c0b1-d92c-4a30-8967-03d5a3da995e.png" class="calibre174"/></div>
<div class="CDPAlignCenter1">Figure 7: Samples from the final training epoch</div>
<p class="calibre2">We can even see the designing skills of the generator over different epochs. So let's visualize the images that are generated by it in every 10 epochs:</p>
<pre class="calibre21">rows, cols = 10, 6<br class="title-page-name"/>fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)<br class="title-page-name"/><br class="title-page-name"/>for gen_sample, ax_row in zip(gen_samples[::int(len(gen_samples)/rows)], axes):<br class="title-page-name"/>    for image, ax in zip(gen_sample[::int(len(gen_sample)/cols)], ax_row):<br class="title-page-name"/>        ax.imshow(image.reshape((28,28)), cmap='Greys_r')<br class="title-page-name"/>        ax.xaxis.set_visible(False)<br class="title-page-name"/>        ax.yaxis.set_visible(False)</pre>
<div class="CDPAlignCenter"><img src="assets/0f84e24e-868e-4667-a52c-52afb81d2dc9.png" class="calibre175"/></div>
<div class="CDPAlignCenter1">Figure 8: Images g<span>enerated </span>as the network was training, for every 10 epochs</div>
<p class="calibre2">As you can see, the designing skills of the generator and its ability to generate fake images were very limited at first, and then it was enhanced towards the end of the training process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling from the generator</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous section, we went through some examples that were generated during the training process of this GAN architecture. We can also generate completely new images from the generator by loading the checkpoints that we have saved and feeding the generator with a new latent space that it can use to generate new images:</p>
<pre class="calibre21"># Sampling from the generator<br class="title-page-name"/>saver = tf.train.Saver(var_list=g_vars)<br class="title-page-name"/><br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/>    <br class="title-page-name"/>    #restoring the saved checkpints<br class="title-page-name"/>    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))<br class="title-page-name"/>    gen_sample_z = np.random.uniform(-1, 1, size=(16, z_size))<br class="title-page-name"/>    generated_samples = sess.run(<br class="title-page-name"/>                   generator(generator_input_z, input_img_size, reuse_vars=True),<br class="title-page-name"/>                   feed_dict={generator_input_z: gen_sample_z})<br class="title-page-name"/>view_generated_samples(0, [generated_samples])</pre>
<div class="CDPAlignCenter"><img src="assets/a01f9682-b0f7-4eff-8906-4baaacce3ec9.png" class="calibre176"/></div>
<div class="CDPAlignCenter1">Figure 9: Samples from the generator</div>
<div class="title-page-name">
<p class="calibre2">There are some observations that you can come up with while implementing this example. During the first epochs of the training process, the generator doesn't have any skills to produce similar images to the real one because it doesn't know what they look like. Even the discriminator doesn't know how to distinguish between fake images made by the generator and the. At the beginning of training, two interesting situations occur. First, the generator does not know how to create images like the real ones that we fed originally to the network. Second, the discriminator doesn't know the difference between the real and fake images.</p>
<p class="calibre2">Later on, the generator starts to fake images that make sense to some extent, and that's because the generator will learn the data distribution that the original input images are coming from. In parallel, the discriminator will be able to distinguish between fake and real images and it will be fooled by the end of the training process.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">GANs are being used nowadays for lots of interesting applications. GANs can be used for different setups such as semi-supervised and unsupervised tasks. Also, because of the huge number of researchers working on GANs, these models are progressing day by day and their ability to generate images or videos is getting better and better.</p>
<p class="calibre2">These kinds of models can be used for many interesting commercial applications, such as adding a plugin to Photoshop that can take commands like <kbd class="calibre12">make my smile more appealing</kbd>. They can also be used for image denoising.</p>


            </article>

            
        </section>
    </body></html>