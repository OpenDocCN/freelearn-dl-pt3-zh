["```\nwhile not done:\n    > collect transitions  from the real environment using a policy \n    > add the transitions to the buffer \n    > learn a model  that minimizes  in a supervised way using data in \n    > (optionally learn )\n\n    repeat K times: \n        > sample an initial state \n        > simulate transitions  from the model using a policy \n        > update the policy  using a model-free RL\n```", "```\nInitialize randomly policy  and models \nInitialize empty buffer \n\nwhile not done:\n    > populate buffer  with transitions  from the real environment using policy  (or random)\n    > learn models  that minimize  in a supervised way using data in \n\n    until convergence: \n        > sample an initial state \n        > simulate transitions  using models  and the policy \n        > take a TRPO update to optimize policy \n```", "```\n...\nif ep == 0:\n    act = env.action_space.sample()\nelse:\n    act = sess.run(a_sampl, feed_dict={obs_ph:[env.n_obs], log_std:init_log_std})\n...\n```", "```\n...\nmodel_buffer.generate_random_dataset()\ntrain_obs, train_act, _, train_nxt_obs, _ = model_buffer.get_training_batch()\nvalid_obs, valid_act, _, valid_nxt_obs, _ = model_buffer.get_valid_batch()\nprint('Log Std policy:', sess.run(log_std))\n\nfor i in range(num_ensemble_models):\ntrain_model(train_obs, train_act, train_nxt_obs, valid_obs, valid_act, valid_nxt_obs, step_count, i)\n```", "```\n        best_sim_test = np.zeros(num_ensemble_models)\n        for it in range(80):\n            obs_batch, act_batch, adv_batch, rtg_batch = simulate_environment(sim_env, action_op_noise, simulated_steps)\n\n            policy_update(obs_batch, act_batch, adv_batch, rtg_batch)\n```", "```\n            if (it+1) % 5 == 0:\n                sim_rewards = []\n\n                for i in range(num_ensemble_models):\n                    sim_m_env = NetworkEnv(gym.make(env_name), model_op, pendulum_reward, pendulum_done, i+1)\n                    mn_sim_rew, _ = test_agent(sim_m_env, action_op, num_games=5)\n                    sim_rewards.append(mn_sim_rew)\n\n                sim_rewards = np.array(sim_rewards)\n                if (np.sum(best_sim_test >= sim_rewards) > int(num_ensemble_models*0.7)) \\\n                    or (len(sim_rewards[sim_rewards >= 990]) > int(num_ensemble_models*0.7)):\n                    break\n                else:\n                  best_sim_test = sim_rewards\n```", "```\n    def train_model(tr_obs, tr_act, tr_nxt_obs, v_obs, v_act, v_nxt_obs, step_count, model_idx):\n        mb_valid_loss1 = run_model_loss(model_idx, v_obs, v_act, v_nxt_obs)\n\n        model_assign(model_idx, initial_variables_models[model_idx])\n\n        mb_valid_loss = run_model_loss(model_idx, v_obs, v_act, v_nxt_obs)\n```", "```\n        acc_m_losses = []\n        last_m_losses = []\n        md_params = sess.run(models_variables[model_idx])\n        best_mb = {'iter':0, 'loss':mb_valid_loss, 'params':md_params}\n        it = 0\n\n        lb = len(tr_obs)\n        shuffled_batch = np.arange(lb)\n        np.random.shuffle(shuffled_batch)\n\n        while best_mb['iter'] > it - model_iter:\n\n            # update the model on each mini-batch\n            last_m_losses = []\n            for idx in range(0, lb, model_batch_size):\n                minib = shuffled_batch[idx:min(idx+minibatch_size,lb)]\n\n                if len(minib) != minibatch_size:\n                  _, ml = run_model_opt_loss(model_idx, tr_obs[minib], tr_act[minib], tr_nxt_obs[minib])\n                  acc_m_losses.append(ml)\n                  last_m_losses.append(ml)\n\n            # Check if the loss on the validation set has improved\n            mb_valid_loss = run_model_loss(model_idx, v_obs, v_act, v_nxt_obs)\n            if mb_valid_loss < best_mb['loss']:\n                best_mb['loss'] = mb_valid_loss\n                best_mb['iter'] = it\n                best_mb['params'] = sess.run(models_variables[model_idx])\n\n            it += 1\n\n        # Restore the model with the lower validation loss\n        model_assign(model_idx, best_mb['params'])\n\n        print('Model:{}, iter:{} -- Old Val loss:{:.6f} New Val loss:{:.6f} -- New Train loss:{:.6f}'.format(model_idx, it, mb_valid_loss1, best_mb['loss'], np.mean(last_m_losses)))\n```", "```\ndef pendulum_reward(ob, ac):\n    return 1\n```", "```\ndef pendulum_done(ob):\n    return np.abs(np.arcsin(np.squeeze(ob[3]))) > .2\n```"]