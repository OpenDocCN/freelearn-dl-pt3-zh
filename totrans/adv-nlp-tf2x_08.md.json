["```\nif movie.review has \"amazing acting\" in it:\nthen sentiment is positive \n```", "```\n(tf24nlp) $ wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n(tf24nlp) $ tar xvzf aclImdb_v1.tar.gz \n```", "```\ndef load_reviews(path, columns=[\"filename\", 'review']):\n    assert len(columns) == 2\n    l = list()\n    for filename in glob.glob(path):\n        # print(filename)\n        with open(filename, 'r') as f:\n            review = f.read()\n            l.append((filename, review))\n    return pd.DataFrame(l, columns=columns) \n```", "```\nunsup_df = load_reviews(\"./aclImdb/train/unsup/*.txt\")\nunsup_df.describe() \n```", "```\nfilename \n```", "```\nreview \n```", "```\ncount \n```", "```\n50000 \n```", "```\n50000 \n```", "```\nunique \n```", "```\n50000 \n```", "```\n49507 \n```", "```\ntop \n```", "```\n./aclImdb/train/unsup/24211_0.txt \n```", "```\nAm not from America, I usually watch this show... \n```", "```\nfreq \n```", "```\n1 \n```", "```\n5 \n```", "```\ndef load_labelled_data(path, neg='/neg/', \n                       pos='/pos/', shuffle=True):\n    neg_df = load_reviews(path + neg + \"*.txt\")\n    pos_df = load_reviews(path + pos + \"*.txt\")\n    neg_df['sentiment'] = 0\n    pos_df['sentiment'] = 1\n    df = pd.concat([neg_df, pos_df], axis=0)\n    if shuffle:\n        df = df.sample(frac=1, random_state=42)\n    return df \n```", "```\ntrain_df = load_labelled_data(\"./aclImdb/train/\")\ntrain_df.head() \n```", "```\nfilename \n```", "```\nreview \n```", "```\nsentiment \n```", "```\n6868 \n```", "```\n./aclImdb/train//neg/6326_4.txt \n```", "```\nIf you're in the mood for some dopey light ent... \n```", "```\n0 \n```", "```\n11516 \n```", "```\n./aclImdb/train//pos/11177_8.txt \n```", "```\n*****Spoilers herein*****<br /><br />What real... \n```", "```\n1 \n```", "```\n9668 \n```", "```\n./aclImdb/train//neg/2172_2.txt \n```", "```\nBottom of the barrel, unimaginative, and pract... \n```", "```\n0 \n```", "```\n1140 \n```", "```\n./aclImdb/train//pos/2065_7.txt \n```", "```\nFearful Symmetry is a pleasant episode with a ... \n```", "```\n1 \n```", "```\n1518 \n```", "```\n./aclImdb/train//pos/7147_10.txt \n```", "```\nI found the storyline in this movie to be very... \n```", "```\n1 \n```", "```\ndef fn_to_score(f):\n    scr = f.split(\"/\")[-1]  # get file name\n    scr = scr.split(\".\")[0] # remove extension\n    scr = int (scr.split(\"_\")[-1]) #the score\n    return scr\ntrain_df['score'] = train_df.filename.apply(fn_to_score) \n```", "```\ntest_df = load_labelled_data(\"./aclImdb/test/\") \n```", "```\ntext = unsup_df.review.to_list() + train_df.review.to_list() \n```", "```\ntxt = [ BeautifulSoup(x).text for x in text ] \n```", "```\nencoder = tfds.features.text.SubwordTextEncoder.\\\n                build_from_corpus(txt, target_vocab_size=2**13)\nencoder.save_to_file(\"imdb\") \n```", "```\nfrom sklearn.model_selection import train_test_split\n# Randomly split training into 2k / 23k sets\ntrain_2k, train_23k = train_test_split(train_df, test_size=23000, \n                                      random_state=42, \n                                      stratify=train_df.sentiment)\ntrain_2k.to_pickle(\"train_2k.df\") \n```", "```\n# we need a sample of 2000 reviews for training\nnum_recs = 2000\ntrain_small = pd.read_pickle(\"train_2k.df\")\n# we dont need the snorkel column\ntrain_small = train_small.drop(columns=['snorkel'])\n# remove markup\ncleaned_reviews = train_small.review.apply(lambda x: BeautifulSoup(x).text)\n# convert pandas DF in to tf.Dataset\ntrain = tf.data.Dataset.from_tensor_slices(\n                             (cleaned_reviews.values,\n                             train_small.sentiment.values)) \n```", "```\n# transformation functions to be used with the dataset\nfrom tensorflow.keras. pre-processing import sequence\ndef encode_pad_transform(sample):\n    encoded = imdb_encoder.encode(sample.numpy())\n    pad = sequence.pad_sequences([encoded], padding='post', maxlen=150)\n    return np.array(pad[0], dtype=np.int64)  \ndef encode_tf_fn(sample, label):\n    encoded = tf.py_function(encode_pad_transform, \n                                       inp=[sample], \n                                       Tout=(tf.int64))\n    encoded.set_shape([None])\n    label.set_shape([])\n    return encoded, label\nencoded_train = train.map(encode_tf_fn,\n                 num_parallel_calls=tf.data.experimental.AUTOTUNE) \n```", "```\n# remove markup\ncleaned_reviews = test_df.review.apply(\nlambda x: BeautifulSoup(x).text)\n# convert pandas DF in to tf.Dataset\ntest = tf.data.Dataset.from_tensor_slices((cleaned_reviews.values, \n                                        test_df.sentiment.values))\nencoded_test = test.map(encode_tf_fn,\n                 num_parallel_calls=tf.data.experimental.AUTOTUNE) \n```", "```\n# Length of the vocabulary \nvocab_size = imdb_encoder.vocab_size \n# Number of RNN units\nrnn_units = 64\n# Embedding size\nembedding_dim = 64\n#batch size\nBATCH_SIZE=100 \n```", "```\nfrom tensorflow.keras.layers import Embedding, LSTM, \\\n                                    Bidirectional, Dense,\\\n                                    Dropout\n\ndropout=0.5\ndef build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, dropout=0.):\n    model = tf.keras.Sequential([\n        Embedding(vocab_size, embedding_dim, mask_zero=True,\n                  batch_input_shape=[batch_size, None]),\n        Bidirectional(LSTM(rnn_units, return_sequences=True)),\n        Bidirectional(tf.keras.layers.LSTM(rnn_units)),\n        Dense(rnn_units, activation='relu'),\n        Dropout(dropout),\n        Dense(1, activation='sigmoid')\n      ])\n    return model \n```", "```\nbilstm = build_model_bilstm(\n  vocab_size = vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)\nbilstm.summary() \n```", "```\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (100, None, 64)           529024    \n_________________________________________________________________\nbidirectional_8 (Bidirection (100, None, 128)          66048     \n_________________________________________________________________\nbidirectional_9 (Bidirection (100, 128)                98816     \n_________________________________________________________________\ndense_6 (Dense)              (100, 64)                 8256      \n_________________________________________________________________\ndropout_6 (Dropout)          (100, 64)                 0         \n_________________________________________________________________\ndense_7 (Dense)              (100, 1)                  65        \n=================================================================\nTotal params: 702,209\nTrainable params: 702,209\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\nbilstm.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy', 'Precision', 'Recall'])\nencoded_train_batched = encoded_train.shuffle(num_recs, seed=42).\\\n                                    batch(BATCH_SIZE)\nbilstm.fit(encoded_train_batched, epochs=15) \n```", "```\nTrain for 15 steps\nEpoch 1/15\n20/20 [==============================] - 16s 793ms/step - loss: 0.6943 - accuracy: 0.4795 - Precision: 0.4833 - Recall: 0.5940\nâ€¦\nEpoch 15/15\n20/20 [==============================] - 4s 206ms/step - loss: 0.0044 - accuracy: 0.9995 - Precision: 0.9990 - Recall: 1.0000 \n```", "```\nbilstm.evaluate(encoded_test.batch(BATCH_SIZE)) \n```", "```\n250/250 [==============================] - 33s 134ms/step - loss: 2.1440 - accuracy: 0.7591 - precision: 0.7455 - recall: 0.7866 \n```", "```\nPOSITIVE = 1\nNEGATIVE = 0\nABSTAIN = -1\nfrom snorkel.labeling.lf import labeling_function\n@labeling_function()\ndef time_waste(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = \"time waste\"\n    ex2 = \"waste of time\"\n    if ex1 in x.review.lower() or ex2 in x.review.lower():\n        return NEGATIVE\n    return ABSTAIN \n```", "```\n(tf24nlp) $ pip install snorkel==0.9.5 \n```", "```\nneg = train_df[train_df.sentiment==0].sample(n=5, random_state=42)\nfor x in neg.review.tolist():\n    print(x) \n```", "```\n@labeling_function()\ndef cheesy_dull(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = \"cheesy\"\n    ex2 = \"dull\"\n    if ex1 in x.review.lower() or ex2 in x.review.lower():\n        return NEGATIVE\n    return ABSTAIN \n```", "```\n@labeling_function()\ndef garbage(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = \"garbage\"\n    if ex1 in x.review.lower():\n        return NEGATIVE\n    return ABSTAIN\n@labeling_function()\ndef terrible(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = \"terrible\"\n    if ex1 in x.review.lower():\n        return NEGATIVE\n    return ABSTAIN\n@labeling_function()\ndef unsatisfied(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = \"unsatisf\"  # unsatisfactory, unsatisfied\n    if ex1 in x.review.lower():\n        return NEGATIVE\n    return ABSTAIN \n```", "```\nneg_lfs = [atrocious, terrible, piece_of, woefully_miscast, \n           bad_acting, cheesy_dull, disappoint, crap, garbage, \n           unsatisfied, ridiculous] \n```", "```\nimport re\n@labeling_function()\ndef classic(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = \"a classic\"\n    if ex1 in x.review.lower():\n        return POSITIVE\n    return ABSTAIN\n@labeling_function()\ndef great_direction(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = \"(great|awesome|amazing|fantastic|excellent) direction\"\n    if re.search(ex1, x.review.lower()):\n        return POSITIVE\n    return ABSTAIN\n@labeling_function()\ndef great_story(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = \"(great|awesome|amazing|fantastic|excellent|dramatic) (script|story)\"\n    if re.search(ex1, x.review.lower()):\n        return POSITIVE\n    return ABSTAIN \n```", "```\npos_lfs = [classic, must_watch, oscar, love, great_entertainment,\n           very_entertaining, amazing, brilliant, fantastic, \n           awesome, great_acting, great_direction, great_story,\n           favourite]\n# set of labeling functions\nlfs = neg_lfs + pos_lfs \n```", "```\n# let's take a sample of 100 records from training set\nlf_train = train_df.sample(n=1000, random_state=42)\nfrom snorkel.labeling.model import LabelModel\nfrom snorkel.labeling import PandasLFApplier\n# Apply the LFs to the unlabeled training data\napplier = PandasLFApplier(lfs)\nL_train = applier.apply(lf_train) \n```", "```\n# Train the label model and compute the training labels\nlabel_model = LabelModel(cardinality=2, verbose=True)\nlabel_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\nlf_train[\"snorkel\"] = label_model.predict(L=L_train, \n                                      **tie_break_policy=****\"abstain\"**) \n```", "```\npred_lfs = lf_train[lf_train.snorkel > -1]\npred_lfs.describe() \n```", "```\nsentiment \n```", "```\nscore \n```", "```\nsnorkel \n```", "```\ncount \n```", "```\n598.000000 \n```", "```\n598.000000 \n```", "```\n598.000000 \n```", "```\npred_mistake = pred_lfs[pred_lfs.sentiment != pred_lfs.snorkel]\npred_mistake.describe() \n```", "```\nsentiment \n```", "```\nscore \n```", "```\nsnorkel \n```", "```\ncount \n```", "```\n164.000000 \n```", "```\n164.000000 \n```", "```\n164.000000 \n```", "```\nen_stopw = set(stopwords.words(\"english\"))\ndef get_words(review, words, stopw=en_stopw):\n    review = BeautifulSoup(review).text        # remove HTML tags\n    review = re.sub('[^A-Za-z]', ' ', review)  # remove non letters\n    review = review.lower()\n    tok_rev = wt(review)\n    rev_word = [word for word in tok_rev if word not in stopw]\n    words += rev_word \n```", "```\npos_rev = train_df[train_df.sentiment == 1]\npos_words = []\npos_rev.review.apply(get_words, args=(pos_words,))\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\npos_words_sen = \" \".join(pos_words)\npos_wc = WordCloud(width = 600,height = 512).generate(pos_words_sen)\nplt.figure(figsize = (12, 8), facecolor = 'k')\nplt.imshow(pos_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show() \n```", "```\nfrom collections import Counter\npos = Counter(pos_words)\nneg = Counter(neg_words)\n# let's try to build a naive bayes model for sentiment classification\ntot_words = pos + neg\ntot_words.most_common(10) \n```", "```\n[('movie', 44031),\n ('film', 40147),\n ('one', 26788),\n ('like', 20274),\n ('good', 15140),\n ('time', 12724),\n ('even', 12646),\n ('would', 12436),\n ('story', 11983),\n ('really', 11736)] \n```", "```\ntop2k = [x for (x, y) in tot_words.most_common(2000)] \n```", "```\ndef featurize(review, topk=top2k, stopw=en_stopw):\n    review = BeautifulSoup(review).text        # remove HTML tags\n    review = re.sub('[^A-Za-z]', ' ', review)  # remove nonletters\n    review = review.lower()\n    tok_rev = wt(review)\n    rev_word = [word for word in tok_rev if word not in stopw]\n    features = {}\n    for word in top2k:\n        features['contains({})'.format(word)] = (word in rev_word)\n    return features\ntrain = [(featurize(rev), senti) for (rev, senti) in \n                        zip(train_df.review, train_df.sentiment)] \n```", "```\nclassifier = nltk.NaiveBayesClassifier.train(train)\n# 0: negative sentiment, 1: positive sentiment\nclassifier.show_most_informative_features(20) \n```", "```\nMost Informative Features\n       contains(unfunny) = True      0 : 1      =     14.1 : 1.0\n         contains(waste) = True      0 : 1      =     12.7 : 1.0\n     contains(pointless) = True      0 : 1      =     10.4 : 1.0\n     contains(redeeming) = True      0 : 1      =     10.1 : 1.0\n     contains(laughable) = True      0 : 1      =      9.3 : 1.0\n         contains(worst) = True      0 : 1      =      9.0 : 1.0\n         contains(awful) = True      0 : 1      =      8.4 : 1.0\n        contains(poorly) = True      0 : 1      =      8.2 : 1.0\n   contains(wonderfully) = True      1 : 0      =      7.6 : 1.0\n         contains(sucks) = True      0 : 1      =      7.0 : 1.0\n          contains(lame) = True      0 : 1      =      6.9 : 1.0\n      contains(pathetic) = True      0 : 1      =      6.4 : 1.0\n    contains(delightful) = True      1 : 0      =      6.0 : 1.0\n        contains(wasted) = True      0 : 1      =      6.0 : 1.0\n          contains(crap) = True      0 : 1      =      5.9 : 1.0\n   contains(beautifully) = True      1 : 0      =      5.8 : 1.0\n      contains(dreadful) = True      0 : 1      =      5.7 : 1.0\n          contains(mess) = True      0 : 1      =      5.6 : 1.0\n      contains(horrible) = True      0 : 1      =      5.5 : 1.0\n        contains(superb) = True      1 : 0      =      5.4 : 1.0\n       contains(garbage) = True      0 : 1      =      5.3 : 1.0\n         contains(badly) = True      0 : 1      =      5.3 : 1.0\n        contains(wooden) = True      0 : 1      =      5.2 : 1.0\n      contains(touching) = True      1 : 0      =      5.1 : 1.0\n      contains(terrible) = True      0 : 1      =      5.1 : 1.0 \n```", "```\n# Some positive high prob words - arbitrary cutoff of 4.5x\n'''\n   contains(wonderfully) = True       1 : 0      =      7.6 : 1.0\n    contains(delightful) = True       1 : 0      =      6.0 : 1.0\n   contains(beautifully) = True       1 : 0      =      5.8 : 1.0\n        contains(superb) = True       1 : 0      =      5.4 : 1.0\n      contains(touching) = True       1 : 0      =      5.1 : 1.0\n   contains(brilliantly) = True       1 : 0      =      4.7 : 1.0\n    contains(friendship) = True       1 : 0      =      4.6 : 1.0\n        contains(finest) = True       1 : 0      =      4.5 : 1.0\n      contains(terrific) = True       1 : 0      =      4.5 : 1.0\n           contains(gem) = True       1 : 0      =      4.5 : 1.0\n   contains(magnificent) = True       1 : 0      =      4.5 : 1.0\n'''\nwonderfully_kw = make_keyword_lf(keywords=[\"wonderfully\"], \nlabel=POSITIVE)\ndelightful_kw = make_keyword_lf(keywords=[\"delightful\"], \nlabel=POSITIVE)\nsuperb_kw = make_keyword_lf(keywords=[\"superb\"], label=POSITIVE)\npos_words = [\"beautifully\", \"touching\", \"brilliantly\", \n\"friendship\", \"finest\", \"terrific\", \"magnificent\"]\npos_nb_kw = make_keyword_lf(keywords=pos_words, label=POSITIVE)\n@labeling_function()\ndef superlatives(x):\n    if not isinstance(x.review, str):\n        return ABSTAIN\n    ex1 = [\"best\", \"super\", \"great\",\"awesome\",\"amaz\", \"fantastic\", \n           \"excellent\", \"favorite\"]\n    pos_words = [\"beautifully\", \"touching\", \"brilliantly\", \n                 \"friendship\", \"finest\", \"terrific\", \"magnificent\", \n                 \"wonderfully\", \"delightful\"]\n    ex1 += pos_words\n    rv = x.review.lower()\n    counts = [rv.count(x) for x in ex1]\n    if sum(counts) >= 3:\n        return POSITIVE\n    return ABSTAIN \n```", "```\n# Utilities for defining keywords based functions\ndef keyword_lookup(x, keywords, label):\n    if any(word in x.review.lower() for word in keywords):\n        return label\n    return ABSTAIN\ndef make_keyword_lf(keywords, label):\n    return LabelingFunction(\n        name=f\"keyword_{keywords[0]}\",\n        f=keyword_lookup,\n        resources=dict(keywords=keywords, label=label),\n    ) \n```", "```\nL_train_full = applier.apply(train_df)\nlabel_model = LabelModel(cardinality=2, verbose=True)\nlabel_model.fit(L_train_full, n_epochs=500, log_freq=50, seed=123)\nmetrics = label_model.score(L=L_train_full, Y=train_df.sentiment, \n                            tie_break_policy=\"abstain\",\n                            metrics=[\"accuracy\", \"coverage\", \n                                     \"precision\", \n                                     \"recall\", \"f1\"])\nprint(\"All Metrics: \", metrics) \n```", "```\nLabel Model Accuracy:     78.5%\nAll Metrics:  {**'accuracy': 0.7854110013835218, 'coverage': 0.83844**, 'precision': 0.8564883605745418, 'recall': 0.6744344773790951, 'f1': 0.7546367008509709} \n```", "```\nfrom snorkel.labeling import LFAnalysis\nLFAnalysis(L=L_train_full, lfs=lfs).lf_summary() \n```", "```\n j Polarity  Coverage  Overlaps  Conflicts\natrocious             0      [0]   0.00816   0.00768    0.00328\nterrible              1      [0]   0.05356   0.05356    0.02696\npiece_of              2      [0]   0.00084   0.00080    0.00048\nwoefully_miscast      3      [0]   0.00848   0.00764    0.00504\n**bad_acting            4      [0]   0.08748   0.08348    0.04304**\ncheesy_dull           5      [0]   0.05136   0.04932    0.02760\nbad                  11      [0]   0.03624   0.03624    0.01744\nkeyword_waste        12      [0]   0.07336   0.06848    0.03232\nkeyword_pointless    13      [0]   0.01956   0.01836    0.00972\nkeyword_redeeming    14      [0]   0.01264   0.01192    0.00556\nkeyword_laughable    15      [0]   0.41036   0.37368    0.20884\nnegatives            16      [0]   0.35300   0.34720    0.17396\nclassic              17      [1]   0.01684   0.01476    0.00856\nmust_watch           18      [1]   0.00176   0.00140    0.00060\noscar                19      [1]   0.00064   0.00060    0.00016\nlove                 20      [1]   0.08660   0.07536    0.04568\ngreat_entertainment  21      [1]   0.00488   0.00488    0.00292\nvery_entertaining    22      [1]   0.00544   0.00460    0.00244\n**amazing              23      [1]   0.05028   0.04516    0.02340**\ngreat                31      [1]   0.27728   0.23568    0.13800\nkeyword_wonderfully  32      [1]   0.01248   0.01248    0.00564\nkeyword_delightful   33      [1]   0.01188   0.01100    0.00500\nkeyword_superb       34      [1]   0.02948   0.02636    0.01220\nkeyword_beautifully  35      [1]   0.08284   0.07428    0.03528\nsuperlatives         36      [1]   0.14656   0.14464    0.07064\nkeyword_remarkable   37      [1]   0.32052   0.26004    0.14748 \n```", "```\n# Grid Search\nfrom itertools import product\nlrs = [1e-1, 1e-2, 1e-3]\nl2s = [0, 1e-1, 1e-2]\nn_epochs = [100, 200, 500]\noptimizer = [\"sgd\", \"adam\"]\nthresh = [0.8, 0.9]\nlma_best = 0\nparams_best = []\nfor params in product(lrs, l2s, n_epochs, optimizer, thresh):\n    # do the initial pass to access the accuracies\n    label_model.fit(L_train_full, n_epochs=params[2], log_freq=50, \n                    seed=123, optimizer=params[3], lr=params[0], \n                    l2=params[1])\n\n    # accuracies\n    weights = label_model.get_weights()\n\n    # LFs above our threshold \n    vals = weights > params[4]\n\n    # the LM requires at least 3 LFs to train\n    if sum(vals) >= 3:\n        L_filtered = L_train_full[:, vals]\n        label_model.fit(L_filtered, n_epochs=params[2], \n                        log_freq=50, seed=123, \n                        optimizer=params[3], lr=params[0], \n                        l2=params[1])\n        label_model_acc = label_model.score(L=L_filtered, \n                          Y=train_df.sentiment, \n                          tie_break_policy=\"abstain\")[\"accuracy\"]\n        if label_model_acc > lma_best:\n            lma_best = label_model_acc\n            params_best = params\n\nprint(\"best = \", lma_best, \" params \", params_best) \n```", "```\nbest =  0.8399649430324277  params  (0.001, 0.1, 200, 'adam', 0.9) \n```", "```\ntrain_df[\"snorkel\"] = label_model.predict(L=L_filtered, \n                               tie_break_policy=\"abstain\")\nfrom sklearn.model_selection import train_test_split\n# Randomly split training into 2k / 23k sets\ntrain_2k, train_23k = train_test_split(train_df, test_size=23000, \n                                       random_state=42, \n                                       stratify=train_df.sentiment)\ntrain_23k.snorkel.hist()\ntrain_23k.sentiment.hist() \n```", "```\nlbl_train = train_23k[train_23k.snorkel > -1]\nlbl_train = lbl_train.drop(columns=[\"sentiment\"])\np_sup = lbl_train.rename(columns={\"snorkel\": \"sentiment\"})\np_sup.to_pickle(\"snorkel_train_labeled.df\") \n```", "```\n# Now apply this to all the unsupervised reviews\n# Apply the LFs to the unlabeled training data\napplier = PandasLFApplier(lfs)\n# now let's apply on the unsupervised dataset\nL_train_unsup = applier.apply(unsup_df)\nlabel_model = LabelModel(cardinality=2, verbose=True)\nlabel_model.fit(L_train_unsup[:, vals], n_epochs=params_best[2], \n                optimizer=params_best[3], \n                lr=params_best[0], l2=params_best[1], \n                log_freq=100, seed=42)\nunsup_df[\"snorkel\"] = label_model.predict(L=L_train_unsup[:, vals], \n                                   tie_break_policy=\"abstain\")\n# rename snorkel to sentiment & concat to the training dataset\npred_unsup_lfs = unsup_df[unsup_df.snorkel > -1]\np2 = pred_unsup_lfs.rename(columns={\"snorkel\": \"sentiment\"})\nprint(p2.info())\np2.to_pickle(\"snorkel-unsup-nbs.df\") \n```", "```\np3 = pred_unsup_lfs2.rename(columns={\"snorkel2\": \"sentiment\"})\nprint(p3.info())\np3.to_pickle(\"snorkel-unsup-nbs-v2.df\") \n```", "```\n# labelled version of training data split\np1 = pd.read_pickle(\"snorkel_train_labeled.df\")\np2 = pd.read_pickle(\"snorkel-unsup-nbs-v2.df\")\np2 = p2.drop(columns=['snorkel']) # so that everything aligns\n# now concatenate the three DFs\np2 = pd.concat([train_small, p1, p2]) # training plus snorkel labelled data\nprint(\"showing hist of additional data\")\n# now balance the labels\npos = p2[p2.sentiment == 1]\nneg = p2[p2.sentiment == 0]\nrecs = min(pos.shape[0], neg.shape[0])\npos = pos.sample(n=recs, random_state=42)\nneg = neg.sample(n=recs, random_state=42)\np3 = pd.concat((pos,neg))\np3.sentiment.hist() \n```", "```\n# remove markup\ncleaned_unsup_reviews = p3.review.apply(\n                             lambda x: BeautifulSoup(x).text)\nsnorkel_reviews = pd.concat((cleaned_reviews, cleaned_unsup_reviews))\nsnorkel_labels = pd.concat((train_small.sentiment, p3.sentiment)) \n```", "```\n# convert pandas DF in to tf.Dataset\nsnorkel_train = tf.data.Dataset.from_tensor_slices((\n                                   snorkel_reviews.values,\n                                   snorkel_labels.values))\nencoded_snorkel_train = snorkel_train.map(encode_tf_fn,\n                 num_parallel_calls=tf.data.experimental.AUTOTUNE) \n```", "```\nshuffle_size = snorkel_reviews.shape[0] // BATCH_SIZE * BATCH_SIZE\nencoded_snorkel_batched = encoded_snorkel_train.shuffle( \n                                  buffer_size=shuffle_size,\n                                  seed=42).batch(BATCH_SIZE,\n                                  drop_remainder=True) \n```", "```\nbilstm2.fit(encoded_snorkel_batched, epochs=20) \n```", "```\nTrain for 359 steps\nEpoch 1/20\n359/359 [==============================] - 92s 257ms/step - loss: 0.4399 - accuracy: 0.7860 - Precision: 0.7900 - Recall: 0.7793\nâ€¦\nEpoch 20/20\n359/359 [==============================] - 82s 227ms/step - loss: 0.0339 - accuracy: 0.9886 - Precision: 0.9879 - Recall: 0.9893 \n```", "```\nbilstm2.evaluate(encoded_test.batch(BATCH_SIZE)) \n```", "```\n250/250 [==============================] - 35s 139ms/step - loss: 1.9134 - accuracy: 0.7658 - precision: 0.7812 - recall: 0.7386 \n```"]