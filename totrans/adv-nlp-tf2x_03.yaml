- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the fundamental building blocks of NLU is **Named Entity Recognition**
    (**NER**). The names of people, companies, products, and quantities can be tagged
    in a piece of text with NER, which is very useful in chatbot applications and
    many other use cases in information retrieval and extraction. NER will be the
    main focus of this chapter. Building and training a model capable of doing NER
    requires several techniques, such as **Conditional Random Fields** (**CRFs**)
    and **Bi-directional LSTMs** (**BiLSTMs**). Advanced TensorFlow techniques like
    custom layers, losses, and training loops are also used. We will build on the
    knowledge of BiLSTMs gained from the previous chapter. Specifically, the following
    will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of NER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an NER tagging model with BiLSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRFs and Viterbi algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a custom Keras layer for CRFs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a custom loss function in Keras and TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a model with a custom training loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It all starts with understanding NER, which is the focus of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a sentence or a piece of text, the objective of an NER model is to locate
    and classify text tokens as named entities in categories such as people''s names,
    organizations and companies, physical locations, quantities, monetary quantities,
    times, dates, and even protein or DNA sequences. NER should tag the following
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ashish paid Uber $80 to go to the Twitter offices in San Francisco.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[Ashish]*[PER] *paid [Uber]*[ORG] *[$80]*[MONEY] *to go the [Twitter]*[ORG]
    *offices in [San Francisco]*[LOC]*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example from the Google Cloud Natural Language API, with several
    additional classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: An NER example from the Google Cloud Natural Language API'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common tags are listed in the table below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Example Tag | Example |'
  prefs: []
  type: TYPE_TB
- en: '| Person | PER | *Gregory* went to the castle. |'
  prefs: []
  type: TYPE_TB
- en: '| Organization | ORG | *WHO* just issued an epidemic advisory. |'
  prefs: []
  type: TYPE_TB
- en: '| Location | LOC | She lives in *Seattle*. |'
  prefs: []
  type: TYPE_TB
- en: '| Money | MONEY | You owe me *twenty dollars*. |'
  prefs: []
  type: TYPE_TB
- en: '| Percentage | PERCENT | Stocks have risen *10%* today. |'
  prefs: []
  type: TYPE_TB
- en: '| Date | DATE | Let''s meet on *Wednesday*. |'
  prefs: []
  type: TYPE_TB
- en: '| Time | TIME | Is it *5 pm* already? |'
  prefs: []
  type: TYPE_TB
- en: 'There are different data sets and tagging schemes that can be used to train
    NER models. Different data sets will have different subsets of the tags listed
    above. In other domains, there may be additional tags specific to the domain.
    The Defence Science Technology Laboratory in the UK created a data set called
    **re3d** ([https://github.com/dstl/re3d](https://github.com/dstl/re3d)), which
    has entity types such as vehicle (Boeing 777), weapon (rifle), and military platform
    (tank). The availability of adequately sized labeled data sets in various languages
    is a significant challenge. Here is a link to a good collection of NER data sets:
    [https://github.com/juand-r/entity-recognition-datasets](https://github.com/juand-r/entity-recognition-datasets).
    In many use cases, you will need to spend a lot of time collecting and annotating
    data. For example, if you are building a chatbot for ordering pizza, the entities
    could be bases, sauces, sizes, and toppings.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a few different ways to build an NER model. If the sentence is considered
    a sequence, then this task can be modeled as a word-by-word labeling task. Hence,
    models similar to the models used for **Part of Speech** (**POS**) tagging are
    applicable. Features can be added to a model to improve labeling. The POS of a
    word and its neighboring words are the most straightforward features to add. Word
    shape features that model lowercase letters can add a lot of information, principally
    because a lot of the entity types deal with proper nouns, such as those for people
    and organizations. Organization names can be abbreviated. For example, the World
    Health Organization can be represented as WHO. Note that this feature will only
    work in languages that distinguish between lowercase and uppercase letters.
  prefs: []
  type: TYPE_NORMAL
- en: Another vital feature involves checking a word in a **gazetteer**. A gazetteer
    is like a database of important geographical entities. See [geonames.org](http://geonames.org)
    for an example of a data set licensed under Creative Commons. A set of people's
    names in the USA can be sourced from the US Social Security Administration at
    [https://www.ssa.gov/oact/babynames/state/namesbystate.zip](https://www.ssa.gov/oact/babynames/state/namesbystate.zip).
    The linked ZIP file has the names of people born in the United States since 1910,
    grouped by state. Similarly, Dunn and Bradstreet, popularly known as D&B, offers
    a data set of companies with over 200 million businesses across the world that
    can be licensed. The biggest challenge with this approach is the complexity of
    maintaining these lists over time.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on a model that does not rely on additional external
    data on top of labelled data for training, like a gazetteer, and also has no dependence
    on hand-crafted features. We will try to get to as high a level of accuracy as
    possible using deep neural networks and some additional techniques. The model
    we will use will be a combination of BiLSTM and a CRF on top. This model is based
    on the paper titled *Neural Architectures for Named Entity Recognition*, written
    by Guillaume Lample et al. and presented at the NAACL-HTL conference in 2016\.
    This paper was state of the art in 2016 with an F1 score of 90.94\. Currently,
    the SOTA has an F1-score of 93.5, where the model uses extra training data. These
    numbers are measured on the CoNLL 2003 English data set. The GMB data set will
    be used in this chapter. The next section describes this data set.
  prefs: []
  type: TYPE_NORMAL
- en: The GMB data set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With all the basics in the bag, we are ready to build a model that classifies
    NERs. For this task, the **Groningen Meaning Bank** (**GMB**) data set will be
    used. This dataset is not considered a gold standard. This means that this data
    set is built using automatic tagging software, followed by human raters updating
    subsets of the data. However, this is a very large and rich data set. This data
    has a lot of useful annotations that make it quite suitable for training models.
    It is also constructed from public domain text, making it easy to use for training.
    The following named entities are tagged in this corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: geo = Geographical entity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: org = Organization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: per = Person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gpe = Geopolitical entity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tim = Time indicator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: art = Artifact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eve = Event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: nat = Natural phenomenon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each of these categories, there can be subcategories. For example, *tim*
    may be further sub-divided and represented as *tim-dow* representing a time entity
    corresponding to a day of the week, or *tim-dat*, which represents a date. For
    this exercise, these sub-entities are going to be aggregated into the eight top-level
    entities listed above. The number of examples varies widely between the sub-entities.
    Consequently, the accuracy varies widely due to the lack of enough training data
    for some of these subcategories.
  prefs: []
  type: TYPE_NORMAL
- en: The data set also provides the NER entity for each word. In many cases, an entity
    may comprise multiple words. If *Hyde Park* is a geographical entity, both words
    will be tagged as a *geo* entity. In terms of training models for NER, there is
    another way to represent this data that can have a significant impact on the accuracy
    of the model. This requires the usage of the BIO tagging scheme. In this scheme,
    the first word of an entity, single word or multi-word, is tagged with *B-{entity
    tag}*. If the entity is multi-word, each successive word would be tagged as *I-{entity
    tag}*. In the example above, *Hyde Park* would be tagged as *B-geo I-geo*. All
    these are steps of pre-processing that are required for a data set. All the code
    for this example can be found in the `NER with BiLSTM and CRF.ipynb` notebook
    in the `chapter3-ner-with-lstm-crf` folder of the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started by loading and processing the data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data can be downloaded from the University of Groningen website as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that the data is quite large – over 800MB. If `wget` is not available
    on your system, you may use any other tool such as, `curl` or a browser to download
    the data set. This step may take some time to complete. If you have a challenge
    accessing the data set from the University server, you may download a copy from
    Kaggle: [https://www.kaggle.com/bradbolliger/gmb-v220](https://www.kaggle.com/bradbolliger/gmb-v220).
    Also note that since we are going to be working on large data sets, some of the
    following steps may take some time to execute. In the world of **Natural Language
    Processing** (**NLP**), more training data and training time is key to great results.'
  prefs: []
  type: TYPE_NORMAL
- en: All the code for this example can be found in the `NER with BiLSTM and CRF.ipynb`
    notebook in the `chapter3-ner-with-lstm-crf` folder of the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: The data unzips into the `gmb-2.2.0` folder. The `data` subfolder has a number
    of subfolders with different files. `README` supplied with the data set provides
    details about the various files and their contents. For this example, we will
    be using only files named `en.tags` in various subdirectories. These files are
    tab-separated files with each word of a sentence in a row.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are ten columns of information:'
  prefs: []
  type: TYPE_NORMAL
- en: The token itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A POS tag as used in the Penn Treebank ([ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz](ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lemma
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A named-entity tag, or 0 if none
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A WordNet word sense number for the respective lemma-POS combinations, or 0
    if not applicable ([http://wordnet.princeton.edu](http://wordnet.princeton.edu))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For verbs and prepositions, a list of the VerbNet roles of the arguments in
    order of combination in the **Combinatory Categorial Grammar** (**CCG**) derivation,
    or `[]` if not applicable ([http://verbs.colorado.edu/~mpalmer/projects/verbnet.html](http://verbs.colorado.edu/~mpalmer/projects/verbnet.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic relation in noun-noun compounds, possessive apostrophes, temporal modifiers,
    and so on. Indicated using a preposition, or 0 if not applicable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An animacy tag as proposed by Zaenen et al. (2004), or 0 if not applicable ([http://dl.acm.org/citation.cfm?id=1608954](http://dl.acm.org/citation.cfm?id=1608954))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A supertag (lexical category of CCG)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lambda-DRS representing the semantics of the token in Boxer's Prolog format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Out of these fields, we are going to use only the token and the named entity
    tag. However, we will work through loading the POS tag for a future exercise.
    The following code gets all the paths for these tags files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A few processing steps need to happen. Each file has a number of sentences,
    with each words in a row. The entire sentence as a sequence and the corresponding
    sequence of NER tags need to be fed in as inputs while training the model. As
    mentioned above, the NER tags also need to be simplified to the top-level entities
    only. Secondly, the NER tags need to be converted to the IOB format. **IOB** stands
    for **In-Other-Begin**. These letters are used as a prefix to the NER tag. The
    sentence fragment in the table below shows how this scheme works:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reverend | Terry | Jones | arrived | in | New | York |'
  prefs: []
  type: TYPE_TB
- en: '| B-per | I-per | I-per | O | O | B-geo | I-geo |'
  prefs: []
  type: TYPE_TB
- en: The table above shows this tagging scheme after processing. Note that New York
    is one location. As soon as *New* is encountered, it marks the start of the geo
    NER tag, hence it is assigned B-geo. The next word is *York*, which is a continuation
    of the same geographical entity. For any network, classifying the word *New* as
    the start of the geographical entity is going to be very challenging. However,
    a BiLSTM network would be able to see the succeeding words, which helps quite
    a bit with disambiguation. Furthermore, the advantage of IOB tags is that the
    accuracy of the model improves considerably in terms of detection. This happens
    because once the beginning of an NER tag is detected, the choices for the next
    tag become quite limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get to the code. First, create a directory to store all the processed
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to process the tags so that we strip the subcategories of the NER tags
    out. It would also be nice to collect some stats on the types of tags in the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The NER tag and IOB tag counters are set up above. A method for stripping the
    subcategory out of the NER tags is defined. The next method takes a sequence of
    tags and converts them into IOB format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once these two convenience functions are ready, all the tags files need to
    be read and processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'First, a counter is set for the number of sentences. A list of files written
    with paths are also initialized. As processed files are written out, their paths
    are added to the `outfiles` variable. This list will be used later to load all
    the data and to train the model. Files are read and split into two empty newline
    characters. That is the marker for the end of a sentence in the file. Only the
    actual words, POS tokens, and NER tokens are used from the file. Once these are
    collected, a new CSV file is written with three columns: the sentence, a sequence
    of POS tags, and a sequence of NER tags. This step may take a little while to
    execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm the distribution of the NER tags before and after processing, we
    can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As is evident, some tags were very infrequent, like *tim-dom*. It would be next
    to impossible for a network to learn them. Aggregating up one level helps increase
    the signal for these tags. To check if the entire process completed properly,
    check that the `ner` folder has 10,000 files. Now, let us load the processed data
    to normalize, tokenize, and vectorize it.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing and vectorizing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this section, `pandas` and `numpy` methods will be used. The first step
    is to load the contents of the processed files into one `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This step may take a while given that it is processing 10,000 files. Once the
    content is loaded, we can check the structure of the `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Both the text and NER tags need to be tokenized and encoded into numbers for
    use in training. We are going to be using core methods provided by the `keras.preprocessing`
    package. First, the tokenizer will be used to tokenize the text. In this example,
    the text only needs to be tokenized by white spaces, as it has been broken up
    already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The default values for the tokenizer are quite reasonable. However, in this
    particular case, it is important to only tokenize on spaces and not clean the
    special characters out. Otherwise the data will become mis-formatted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Even though we do not use the POS tags, the processing for them is included.
    Use of the POS tags can have an impact on the accuracy of an NER model. Many NER
    entities are nouns, for example. However, we will see how to process POS tags
    but not use them in the model as features. This is left as an exercise to the
    reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tokenizer has some useful features. It provides a way to restrict the
    size of the vocabulary by word counts, TF-IDF, and so on. If the `num_words` parameter
    is passed with a numeric value, the tokenizer will limit the number of tokens
    by word frequencies to that number. The `fit_on_texts` method takes in all the
    texts, tokenizes them, and constructs dictionaries with tokens that will be used
    later to tokenize and encode in one go. A convenience function, `get_config()`,
    can be called after the tokenizer has been fit on texts to provide information
    about the tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `index_word` dictionary property in the config provides a mapping between
    IDs and tokens. There is a considerable amount of information in the config. The
    vocabularies can be obtained from the config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Tokenizing and encoding text and named entity labels is quite easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Since sequences are of different sizes, they will all be padded or truncated
    to a size of 50 tokens. A helper function is used for this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The last step above is to ensure that shapes are correct before moving to the
    next step. Verifying shapes is a very important part of developing code in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an additional step that needs to be performed on the labels. Since
    there are multiple labels, each label token needs to be one-hot encoded like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to build and train a model.
  prefs: []
  type: TYPE_NORMAL
- en: A BiLSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first model we will try is a BiLSTM model. First, the basic constants need
    to be set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, a convenience function for instantiating models is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We are going to train our own embeddings. The next chapter will talk about using
    pre-trained embeddings and using them in models. After the embedding layer, there
    is a BiLSTM layer, followed by a `TimeDistributed` dense layer. This last layer
    is different from the sentiment analysis model, where there was only a single
    unit for binary output. In this problem, for each word in the input sequence,
    an NER token needs to be predicted. So, the output has as many tokens as the input
    sequence. Consequently, output tokens correspond 1-to-1 with input tokens and
    are classified as one of the NER classes. The `TimeDistributed` layer provides
    this capability. The other thing to note in this model is the use of regularization.
    It is important that the model does not overfit the training data. Since LSTMs
    have high model capacity, using regularization is very important. Feel free to
    play with some of these hyperparameters to get a feel for how the model will react.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the model can be compiled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This simplistic model has over 2.6 million parameters!
  prefs: []
  type: TYPE_NORMAL
- en: If you notice, the bulk of the parameters are coming from the size of the vocabulary.
    The vocabulary has 39,422 words. This increases the model training time and computational
    capacity required. One way to reduce this is to make the vocabulary size smaller.
    The easiest way to do this would be to only consider words that have more than
    a certain frequency of occurrence or to remove words smaller than a certain number
    of characters. The vocabulary can also be reduced by converting all characters
    to lower case. However, in NER, case is a very important feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model is ready for training. The last thing that is needed is to split
    the data into train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the model is ready for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Over 15 epochs of training, the model is doing quite well with over 99% accuracy.
    Let''s see how the model performs on the test set and whether the regularization
    helped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The model performs well on the test data set, with over 96.5% accuracy. The
    difference between the train and test accuracies is still there, implying that
    the model could use some additional regularization. You can play with the dropout
    variable or add additional dropout layers between the embedding and BiLSTM layers,
    and between the `TimeDistributed` layer and the final Dense layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a sentence fragment tagged by this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Faure | Gnassingbe | said | in | a | speech | carried | by | state | media
    | Friday |'
  prefs: []
  type: TYPE_TB
- en: '| Actual | B-per | I-per | O | O | O | O | O | O | O | O | B-tim |'
  prefs: []
  type: TYPE_TB
- en: '| Model | B-per | I-per | O | O | O | O | O | O | O | O | B-tim |'
  prefs: []
  type: TYPE_TB
- en: This model is not doing poorly at all. It was able to identify the person and
    time entities in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: As good as this model is, it does not use an important characteristic of named
    entity tags – a given tag is highly correlated with the tag coming after it. CRFs
    can take advantage of this information and further improve the accuracy of NER
    tasks. Let's understand how CRFs work and add them to the network above next.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional random fields (CRFs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'BiLSTM models look at a sequence of input words and predict the label for the
    current word. In making this determination, only the information of previous inputs
    is considered. Previous predictions play no role in making this decision. However,
    there is information encoded in the sequence of labels that is being discounted.
    To illustrate this point, consider a subset of NER tags: **O**, **B-Per**, **I-Per**,
    **B-Geo**, and **I-Geo**. This represents two domains of person and geographical
    entities and an *Other* category for everything else. Based on the structure of
    IOB tags, we know that any **I**- tag must be preceded by a **B-I** from the same
    domain. This also implies that an **I**- tag cannot be preceded by an **O** tag.
    The following diagram shows the possible state transitions between these tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Possible NER tag transitions'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.2* color codes similar types of transitions with the same color.
    An **O** tag can transition only to a **B** tag. A **B** tag can go to its corresponding
    **I** tag or back to the **O** tag. An **I** tag can transition back to itself,
    an **O** tag, or a **B** tag of a different domain (not represented in the diagram
    for simplicity). For a set of **N** tags, these transitions can be represented
    by a matrix of dimension *N x N*. *P*[i,j] denotes the possibility of tag *j*
    coming after tag *i*. Note that these transition weights can be learned based
    on the data. Such a learned transition weights matrix could be used during prediction
    to consider the entire sequence of predicted labels and make updates to the probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an illustrative matrix with indicative transition weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '| From > To | O | B-Geo | I-Geo | B-Org | I-Org |'
  prefs: []
  type: TYPE_TB
- en: '| O | 3.28 | 2.20 | 0.00 | 3.66 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| B-Geo | -0.25 | -0.10 | 4.06 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| I-Geo | -0.17 | -0.61 | 3.51 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| B-Org | -0.10 | -0.23 | 0.00 | -1.02 | 4.81 |'
  prefs: []
  type: TYPE_TB
- en: '| I-Org | -0.33 | -1.75 | 0.00 | -1.38 | 5.10 |'
  prefs: []
  type: TYPE_TB
- en: As per the table above, the weight of the edge connecting I-Org to B-Org has
    a weight of -1.38, implying that this transition is extremely unlikely to happen.
    Practically, implementing a CRF has three main steps. The first step is modifying
    the score generated by the BiLSTM layer and accounting for the transition weights,
    as shown above. A sequence of predictions
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'generated by the BiLSTM layer above for a sequence of *n* tags in the space
    of *k* unique tags is available, which operates on an input sequence *X*. *P*
    represents a matrix of dimensions *n × k*, where the element *P*[i,j] represents
    the probability of *j*^(th) tag for output at the position *y*[i]. Let *A* be
    a square matrix of transition probabilities as shown above, with a dimension of
    *(k + 2) × (k + 2)* where two additional tokens are added for start- and end-of-sentence
    markers. Element *A*[i,j] represents the transition probability from *i* to tag
    *j*. Using these values, a new score can be calculated like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A softmax can be calculated over all possible tag sequences to get the probability
    for a given sequence *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Y*[X] represents all possible tag sequences, including those that may not
    conform to the IOB tag format. To train using this softmax, a log-likelihood can
    be calculated over this. Through clever use of dynamic programming, a combinatorial
    explosion can be avoided, and the denominator can be computed quite efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: Only simplistic math is shown to help build an intuition of how this method
    works. The actual computations will become clear in the custom layer implementation
    below.
  prefs: []
  type: TYPE_NORMAL
- en: While decoding, the output sequence is the one that has the maximum score among
    these possible sequences, calculated conceptually using an `argmax` style function.
    The Viterbi algorithm is commonly used to implement a dynamic programming solution
    for decoding. First, let us code the model and the training for it before getting
    into decoding.
  prefs: []
  type: TYPE_NORMAL
- en: NER with BiLSTM and CRFs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementing a BiLSTM network with CRFs requires adding a CRF layer on top
    of the BiLSTM network developed above. However, a CRF is not a core part of the
    TensorFlow or Keras layers. It is available through the `tensorflow_addons` or
    `tfa` package. The first step is to install this package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many sub-packages, but the convenience functions for the CRF are
    in the `tfa.text` subpackage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: tfa.text methods'
  prefs: []
  type: TYPE_NORMAL
- en: While low-level methods for implementing the CRF layer are provided, a high-level
    layer-like construct is not provided. The implementation of a CRF requires a custom
    layer, a loss function, and a training loop. Post training, we will look at how
    to implement a customized inference function that will use Viterbi decoding.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the custom CRF layer, loss, and model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the flow above, there will be an embedding layer and a BiLSTM layer.
    The output of the BiLSTM needs to be evaluated with the CRF log-likelihood loss
    described above. This is the loss that needs to be used to train the model. The
    first step in implementation is creating a custom layer. Implementing a custom
    layer in Keras requires subclassing `keras.layers.Layer`. The main method to be
    implemented is `call()`, which takes inputs to the layer, transforms them, and
    returns the result. Additionally, the constructor to the layer can also set up
    any parameters that are needed. Let''s start with the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The main parameters that are needed are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The number of labels and the transition matrix**: As described in the section
    above, a transition matrix needs to be learned. The dimension of that square matrix
    is the number of labels. The transition matrix is initialized using the parameters.
    This transition parameters matrix is not trainable through gradient descent. It
    is calculated as a consequence of computing the log-likelihoods. The transition
    parameters matrix can also be passed into this layer if it has been learned in
    the past.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The mask id**: Since the sequences are padded, it is important to recover
    the original sequence lengths for computing transition scores. By convention,
    a value of 0 is used for the mask, and that is the default. This parameter is
    set up for future configurability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second method is to compute the result of applying this layer. Note that
    as a layer, the CRF layer merely regurgitates the outputs during training time.
    The CRF layer is useful only during inference. At inference time, it uses the
    transition matrix and logic to correct the sequences'' output by the BiLSTM layers
    before returning them. For now, this method is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This method takes the inputs as well as a parameter that helps specify if this
    method is called during training or during inference. If this variable is not
    passed, it is pulled from the Keras backend. When models are trained with the
    `fit()` method, `learning_phase()` returns `True`. When the `.predict()` method
    is called on a model, this flag is set to `false`.
  prefs: []
  type: TYPE_NORMAL
- en: As sequences being passed are masked, this layer needs to know the real sequence
    lengths during inference time for decoding. A variable is passed for it but is
    unused at this time. Now that the basic CRF layer is ready, let's build the model.
  prefs: []
  type: TYPE_NORMAL
- en: A custom CRF model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the model builds on a number of preexisting layers in addition to the
    custom CRF layer above, explicit imports help the readability of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to define a constructor that will create the various layers
    and store the appropriate dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This constructor takes in the number of hidden units for the BiLSTM later, the
    size of words in the vocabulary, the number of NER labels, and the size of the
    embeddings. Additionally, a default name is set by the constructor, which can
    be overridden at the time of instantiation. Any additional parameters supplied
    are passed along as keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'During training and prediction, the following method will be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: So, in a few lines of code, we have implemented a customer model using the custom
    CRF layer developed above. The only thing that we need now to train this model
    is a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: A custom loss function for NER using a CRF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s implement the loss function as part of the CRF layer, encapsulated in
    a function of the same name. Note that when this function is called, it is usually
    passed the labels and predicted values. We will model our loss function on the
    custom loss functions in TensorFlow. Add this code to the CRF layer class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes the true labels and predicted labels. Both of these tensors
    are usually of the shape (batch size, max sequence length, number of NER labels).
    However, the log-likelihood function in the `tfa` package expects the labels to
    be in a (batch size, max sequence length)-shaped tensor. So a convenience function,
    implemented as part of the CRF layer and shown below, is used to perform the conversion
    of label shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The log-likelihood function also requires the actual sequence lengths for each
    example. These sequence lengths can be computed from the labels and the mask identifier
    that was set up in the constructor of this layer (see above). This process is
    encapsulated in another convenience function, also part of the CRF layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: First, a Boolean mask is generated from the labels by comparing the value of
    the label to the mask ID. Then, through casting the Boolean as an integer and
    summing across the row, the length of the sequence is regenerated. Now, the `tfa.text.crf_log_likelihood()`
    function is called to calculate and return the log-likelihoods and the transition
    matrix. The CRF layer's transition matrix is updated with the transition matrix
    returned from the function call. Finally, the loss is computed by summing up all
    the log-likelihoods returned.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, our coded custom model is ready to start training. We will need
    to set up the data and create a custom training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing custom training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model needs to be instantiated and initialized for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'As in past examples, an Adam optimizer will be used. Next, we will construct
    `tf.data.DataSet` from the DataFrames loaded in the BiLSTM section above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Roughly 20% of the data is reserved for testing. The rest is used for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement a custom training loop, TensorFlow 2.0 exposes a gradient tape.
    This allows low-level management of the main steps required for training any model
    with gradient descent. These steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Computing the forward pass predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing the loss when these predictions are compared with the labels
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing the gradients for the trainable parameters based on the loss and then
    using the optimizer to adjust the weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let us train this model for 5 epochs and watch the loss as training progresses.
    Compare this to the 15 epochs of training for the previous model. The custom training
    loop is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'A metric is created to keep track of the average loss over time. For 5 epochs,
    inputs and labels are pulled from the training data set, one batch at a time.
    Using `tf.GradientTape()` to keep track of the operations, the steps outlined
    in the bullets above are implemented. Note that we pass the trainable variable
    manually as this is a custom training loop. Finally, the loss metric is printed
    every 50^(th) step to show training progress. This yields the results below, which
    have been abbreviated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Given we implemented a custom training loop, without requiring a compilation
    of the model, we could not obtain a summary of the model parameters before. To
    get an idea of the size of the model, a summary can be obtained now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: It is comparable in size to the previous model but has some untrainable parameters.
    These are coming from the transition matrix. The transition matrix is not learned
    through gradient descent. Thus, they are classified as non-trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: However, training loss is hard to interpret. To compute accuracy, we need to
    implement decoding, which is the focus of the next section. For the moment, let's
    assume that decoding is available and examine the results of training for 5 epochs.
    For illustration purposes, here is a sentence from the test set with the results
    pulled at the end of the first epoch and at the end of five epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example sentence is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding true label is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a difficult example for NER with *The Washington Post* as a three-word
    organization, where the first word is very common and used in multiple contexts,
    and the second word is also the name of a geographical location. Also note the
    imperfect labels of the GMB data set, where the second tag of the name *Ushakov*
    is tagged as an organization. At the end of the first epoch of training, the model
    predicts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'It gets confused by the organization not being where it expects it to be. It
    also shows that it hasn''t learned the transition probabilities by putting an
    I-org tag after a B-geo tag. However, it does not make a mistake in the person
    portion. Unfortunately for the model, it will not get credit for this great prediction
    of the person tag, and due to imperfect labels, it will still count as a miss.
    The result after five epochs of training is better than the original:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This is a great result, given the limited amount of training we have done. Now,
    let's see how we can decode the sentence in the CRF layer to get these sequences.
    The algorithm used for decoding is called the Viterbi decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Viterbi decoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A straightforward way to predict the sequence of labels is to output the label
    that has the highest activation from the previous layers of the network. However,
    this could be sub-optimal as it assumes that each label prediction is independent
    of the previous or successive predictions. The Viterbi algorithm is used to take
    the predictions for each word in the sequence and apply a maximization algorithm
    so that the output sequence has the highest likelihood. In future chapters, we
    will see another way of accomplishing the same objective through beam search.
    Viterbi decoding involves maximizing over the entire sequence as opposed to optimizing
    at each word of the sequence. To illustrate this algorithm and way of thinking,
    let's take an example of a sentence of 5 words, and a set of 3 labels. These labels
    could be O, B-geo, and I-geo as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm needs the transition matrix values between labels. Recall that
    this was generated and stored in the custom CRF layer above. Let''s say that the
    matrix looks like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '| From > To | Mask | O | B-geo | I-geo |'
  prefs: []
  type: TYPE_TB
- en: '| Mask | 0.6 | 0.3 | 0.2 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| O | 0.8 | 0.5 | 0.6 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| B-geo | 0.2 | 0.4 | 0.01 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| I-geo | 0.3 | 0.4 | 0.01 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: 'To explain how the algorithm works, the figure shown below will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Steps in the Viterbi decoder'
  prefs: []
  type: TYPE_NORMAL
- en: The sentence starts from the left. Arrows from the start of the word to the
    first token represent the probability of the transition between the two tokens.
    The numbers on the arrows should match the values in the transition matrix above.
    Within the circles denoting labels, scores generated by the neural network, the
    BiLSTM model, in our case, are shown for the first word. These scores need to
    be added together to give the final score of the words. Note that we switched
    the terminology from probabilities to scores as normalization is not being performed
    for this particular example.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the first word label
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Score of *O*: 0.3 (transition score) + 0.2 (activation score) = 0.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Score of *B-geo*: 0.2 (transition score) + 0.3 (activation score) = 0.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Score of *I-geo*: 0.01 (transition score) + 0.01 (activation score) = 0.02'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, it is equally likely that an *O* or *B-geo* tag will be the
    starting tag. Let''s consider the next tag and calculate the scores using the
    same approach for the following sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (*O*, *B-geo*) = 0.6 + 0.3 = 0.9 | (*B-geo*, *O*) = 0.4 + 0.3 = 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| (*O*, *I-geo*) = 0.01+ 0.25 = 0.26 | (*B-geo*, *B-geo*) = 0.01 + 0.3 = 0.31
    |'
  prefs: []
  type: TYPE_TB
- en: '| (*O*, *O*) = 0.5 + 0.3 = 0.8 | (*B-geo*, *I-geo*) = 0.7 + 0.25 = 0.95 |'
  prefs: []
  type: TYPE_TB
- en: 'This process is called the forward pass. It should also be noted, even though
    this is a contrived example, that activations at a given input may not be the
    best predictor of the right label for that word once the previous labels have
    been considered. If the sentence was only two words, then the scores for various
    sequences could be calculated by summing by each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (*Start*, *O*, *B-Geo*) = 0.5 + 0.9 = 1.4 | (*Start*, *B-Geo*, *O*) = 0.5
    + 0.7 = 1.2 |'
  prefs: []
  type: TYPE_TB
- en: '| (*Start*, *O*, *O*) = 0.5 + 0.8 = 1.3 | (*Start*, *B-geo*, *B-geo*) = 0.5
    + 0.31 = 0.81 |'
  prefs: []
  type: TYPE_TB
- en: '| (*Start*, *O*, *I-Geo*) = 0.5 + 0.26 = 0.76 | (*Start*, *B-geo*, *I-geo*)
    = 0.5 + 0.95) = 1.45 |'
  prefs: []
  type: TYPE_TB
- en: If only the activation scores were considered, the most probable sequences would
    be either (*Start*, *B-geo*, *O*) or (*Start*, *B-geo*, *B-geo*). However, using
    the transition scores along with the activations means that the sequence with
    the highest probability is (*Start*, *B-geo*, *I-geo*) in this example. While
    the forward pass gives the highest score of the entire sequence given the last
    token, the backward pass process would reconstruct the sequence that resulted
    in this highest score. This is essentially the Viterbi algorithm, which uses dynamic
    programming to perform these steps in an efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing this algorithm is aided by the fact the core computation is provided
    as a method in the `tfa` package. This decoding step will be implemented in the
    `call()` method of the CRF layer implemented above. Modify this method to look
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The new lines added have been highlighted. The `viterbi_decode()` method takes
    the activations from the previous layers and the transition matrix along with
    the maximum sequence length to compute the path with the highest score. This score
    is also returned, but we ignore it for our purposes of inference. This process
    needs to be performed for each sequence in the batch. Note that this method returns
    sequences on different lengths. This makes it harder to convert into tensors,
    so a utility function is used to pad the returned sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: A dropout layer works completely opposite to the way this CRF layer works. A
    dropout layer modifies the inputs only during training time. During inference,
    it merely passes all the inputs through.
  prefs: []
  type: TYPE_NORMAL
- en: Our CRF layer works in the exact opposite fashion. It passes the inputs through
    during training, but it transforms inputs using the Viterbi decoder during inference
    time. Note the use of the `training` parameter to control the behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the layer is modified and ready, the model needs to be re-instantiated
    and trained. Post-training, inference can be performed like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run inference on a small batch of testing data. Let''s check the
    result for the example sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the highlighted output, the results are better than the actual
    data!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a sense of the accuracy of the training, a custom method needs to be
    implemented. This is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `numpy`''s `MaskedArray` feature, the predictions and labels are compared
    and converted to an integer array, and the mean is calculated to compute the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: This is a pretty accurate model, just after 5 epochs of training and with very
    simple architecture, all while using embeddings that are trained from scratch.
    A recall metric can also be implemented in a similar fashion. A BiLSTM-only model,
    shown earlier, took 15 epochs of training to get to a similar accuracy!
  prefs: []
  type: TYPE_NORMAL
- en: This completes the implementation of an NER model using BiLSTMs and CRFs. If
    this is interesting and you would like to continue working on this, look for the
    CoNLL 2003 data set for NER. Even today, papers are being published that aim to
    improve the accuracy of the models based on that data set.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered quite a lot of ground in this chapter. NER and its importance
    in the industry were explained. To build NER models, BiLSTMs and CRFs are needed.
    Using BiLSTMs, which we learned about in the previous chapter while building a
    sentiment classification model, we built a first version of a model that can label
    named entities. This model was further improved using CRFs. In the process of
    building these models, we covered the use of the TensorFlow DataSet API. We also
    built advanced models for CRF mode by building a custom Keras layer, a custom
    model, custom loss function, and a custom training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Thus far, we have trained embeddings for tokens in the models. A considerable
    amount of lift can be achieved by using pre-trained embeddings. In the next chapter,
    we'll focus on the concept of transfer learning and the use of pre-trained embeddings
    like BERT.
  prefs: []
  type: TYPE_NORMAL
