- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于BiLSTM、CRF和Viterbi解码的命名实体识别（NER）
- en: 'One of the fundamental building blocks of NLU is **Named Entity Recognition**
    (**NER**). The names of people, companies, products, and quantities can be tagged
    in a piece of text with NER, which is very useful in chatbot applications and
    many other use cases in information retrieval and extraction. NER will be the
    main focus of this chapter. Building and training a model capable of doing NER
    requires several techniques, such as **Conditional Random Fields** (**CRFs**)
    and **Bi-directional LSTMs** (**BiLSTMs**). Advanced TensorFlow techniques like
    custom layers, losses, and training loops are also used. We will build on the
    knowledge of BiLSTMs gained from the previous chapter. Specifically, the following
    will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言理解（NLU）的一个基础构建模块是**命名实体识别**（**NER**）。通过NER，可以在文本中标记出人名、公司名、产品名和数量等实体，这在聊天机器人应用以及信息检索和提取的许多其他用例中都非常有用。NER将在本章中作为主要内容。构建和训练一个能够进行NER的模型需要几种技术，例如**条件随机场**（**CRFs**）和**双向LSTM**（**BiLSTMs**）。还会使用一些高级TensorFlow技术，如自定义层、损失函数和训练循环。我们将基于上一章获得的BiLSTMs知识进行拓展。具体来说，将涵盖以下内容：
- en: Overview of NER
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NER概述
- en: Building an NER tagging model with BiLSTM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个基于BiLSTM的NER标注模型
- en: CRFs and Viterbi algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CRFs和Viterbi算法
- en: Building a custom Keras layer for CRFs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为CRFs构建自定义Keras层
- en: Building a custom loss function in Keras and TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras和TensorFlow中构建自定义损失函数
- en: Training a model with a custom training loop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义训练循环训练模型
- en: It all starts with understanding NER, which is the focus of the next section.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都始于理解NER，这是下一节的重点。
- en: Named Entity Recognition
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'Given a sentence or a piece of text, the objective of an NER model is to locate
    and classify text tokens as named entities in categories such as people''s names,
    organizations and companies, physical locations, quantities, monetary quantities,
    times, dates, and even protein or DNA sequences. NER should tag the following
    sentence:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一句话或一段文本，NER模型的目标是定位并将文本中的词语分类为命名实体，类别包括人名、组织与公司、地理位置、数量、货币数量、时间、日期，甚至蛋白质或DNA序列。NER应该标记以下句子：
- en: '*Ashish paid Uber $80 to go to the Twitter offices in San Francisco.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*阿什什支付了80美元，乘坐Uber去Twitter位于旧金山的办公室。*'
- en: 'as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示：
- en: '*[Ashish]*[PER] *paid [Uber]*[ORG] *[$80]*[MONEY] *to go the [Twitter]*[ORG]
    *offices in [San Francisco]*[LOC]*.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*[阿什什]*[PER] *支付了[Uber]*[ORG] *[$80]*[MONEY] *去[Twitter]*[ORG] *位于[旧金山]*[LOC]的办公室。*'
- en: 'Here is an example from the Google Cloud Natural Language API, with several
    additional classes:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自Google Cloud自然语言API的一个示例，包含多个附加类别：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_03_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  描述自动生成](img/B16252_03_01.png)'
- en: 'Figure 3.1: An NER example from the Google Cloud Natural Language API'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：来自Google Cloud自然语言API的NER示例
- en: 'The most common tags are listed in the table below:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的标签列在下表中：
- en: '| Type | Example Tag | Example |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 示例标签 | 示例 |'
- en: '| Person | PER | *Gregory* went to the castle. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 人物 | PER | *格雷戈里*去了城堡。 |'
- en: '| Organization | ORG | *WHO* just issued an epidemic advisory. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 组织 | ORG | *世界卫生组织*刚刚发布了疫情警告。 |'
- en: '| Location | LOC | She lives in *Seattle*. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | LOC | 她住在*西雅图*。 |'
- en: '| Money | MONEY | You owe me *twenty dollars*. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 金钱 | MONEY | 你欠我*二十美元*。 |'
- en: '| Percentage | PERCENT | Stocks have risen *10%* today. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 百分比 | PERCENT | 股票今天上涨了*10%*。 |'
- en: '| Date | DATE | Let''s meet on *Wednesday*. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | DATE | 我们周三见。 |'
- en: '| Time | TIME | Is it *5 pm* already? |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | TIME | *已经是下午五点了吗？* |'
- en: 'There are different data sets and tagging schemes that can be used to train
    NER models. Different data sets will have different subsets of the tags listed
    above. In other domains, there may be additional tags specific to the domain.
    The Defence Science Technology Laboratory in the UK created a data set called
    **re3d** ([https://github.com/dstl/re3d](https://github.com/dstl/re3d)), which
    has entity types such as vehicle (Boeing 777), weapon (rifle), and military platform
    (tank). The availability of adequately sized labeled data sets in various languages
    is a significant challenge. Here is a link to a good collection of NER data sets:
    [https://github.com/juand-r/entity-recognition-datasets](https://github.com/juand-r/entity-recognition-datasets).
    In many use cases, you will need to spend a lot of time collecting and annotating
    data. For example, if you are building a chatbot for ordering pizza, the entities
    could be bases, sauces, sizes, and toppings.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的数据集和标注方案可以用来训练 NER 模型。不同的数据集会包含上述标注的不同子集。在其他领域，可能会有针对该领域的额外标签。英国的国防科学技术实验室（Defence
    Science Technology Laboratory）创建了一个名为**re3d**的数据集（[https://github.com/dstl/re3d](https://github.com/dstl/re3d)），其中包含诸如车辆（如波音
    777）、武器（如步枪）和军事平台（如坦克）等实体类型。各种语言中适当大小的标注数据集的可用性是一个重大挑战。以下是一个很好的 NER 数据集集合链接：[https://github.com/juand-r/entity-recognition-datasets](https://github.com/juand-r/entity-recognition-datasets)。在许多使用案例中，你需要花费大量时间收集和标注数据。例如，如果你正在为订购披萨构建一个聊天机器人，实体可能包括基础、酱料、尺寸和配料。
- en: There are a few different ways to build an NER model. If the sentence is considered
    a sequence, then this task can be modeled as a word-by-word labeling task. Hence,
    models similar to the models used for **Part of Speech** (**POS**) tagging are
    applicable. Features can be added to a model to improve labeling. The POS of a
    word and its neighboring words are the most straightforward features to add. Word
    shape features that model lowercase letters can add a lot of information, principally
    because a lot of the entity types deal with proper nouns, such as those for people
    and organizations. Organization names can be abbreviated. For example, the World
    Health Organization can be represented as WHO. Note that this feature will only
    work in languages that distinguish between lowercase and uppercase letters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 NER 模型有几种不同的方法。如果将句子视为一个序列，那么这个任务可以建模为逐字标注任务。因此，类似于**词性标注**（**POS**）的模型是适用的。可以向模型中添加特征以改进标注。一个词的词性及其相邻词汇是最直接可以添加的特征。用于建模小写字母的词形特征可以提供大量信息，主要是因为许多实体类型涉及专有名词，比如人名和组织名。组织名称可能会被缩写。例如，世界卫生组织可以表示为
    WHO。请注意，这一特征仅适用于区分大小写字母的语言。
- en: Another vital feature involves checking a word in a **gazetteer**. A gazetteer
    is like a database of important geographical entities. See [geonames.org](http://geonames.org)
    for an example of a data set licensed under Creative Commons. A set of people's
    names in the USA can be sourced from the US Social Security Administration at
    [https://www.ssa.gov/oact/babynames/state/namesbystate.zip](https://www.ssa.gov/oact/babynames/state/namesbystate.zip).
    The linked ZIP file has the names of people born in the United States since 1910,
    grouped by state. Similarly, Dunn and Bradstreet, popularly known as D&B, offers
    a data set of companies with over 200 million businesses across the world that
    can be licensed. The biggest challenge with this approach is the complexity of
    maintaining these lists over time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的特征是检查一个词是否在**地名辞典**中。地名辞典就像一个重要地理实体的数据库。可以参考[geonames.org](http://geonames.org)上的数据集，该数据集获得了创意共享（Creative
    Commons）许可。美国社会保障管理局（US Social Security Administration）提供了一份美国人名数据集，地址为[https://www.ssa.gov/oact/babynames/state/namesbystate.zip](https://www.ssa.gov/oact/babynames/state/namesbystate.zip)。该压缩文件包含自1910年以来出生在美国的人的名字，按州分组。类似地，知名的邓白氏公司（Dunn
    and Bradstreet，简称 D&B）提供了一份全球超过2亿家企业的数据集，用户可以申请许可。使用这种方法的最大挑战是随着时间的推移维护这些列表的复杂性。
- en: In this chapter, we will focus on a model that does not rely on additional external
    data on top of labelled data for training, like a gazetteer, and also has no dependence
    on hand-crafted features. We will try to get to as high a level of accuracy as
    possible using deep neural networks and some additional techniques. The model
    we will use will be a combination of BiLSTM and a CRF on top. This model is based
    on the paper titled *Neural Architectures for Named Entity Recognition*, written
    by Guillaume Lample et al. and presented at the NAACL-HTL conference in 2016\.
    This paper was state of the art in 2016 with an F1 score of 90.94\. Currently,
    the SOTA has an F1-score of 93.5, where the model uses extra training data. These
    numbers are measured on the CoNLL 2003 English data set. The GMB data set will
    be used in this chapter. The next section describes this data set.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注一个不依赖额外外部数据（如地名词典）进行训练的模型，也不依赖人工特征。我们将尽可能使用深度神经网络和一些额外技术来提高准确度。我们将使用的模型将是
    BiLSTM 和 CRF 的组合。该模型基于 Guillaume Lample 等人撰写的论文《命名实体识别的神经网络架构》，并在2016年NAACL-HTL会议上发表。这篇论文在2016年处于前沿水平，F1
    分数为90.94。目前，SOTA（最先进技术）的F1分数为93.5，其中模型使用了额外的训练数据。这些数据是在 CoNLL 2003 英文数据集上测量的。本章将使用
    GMB 数据集。下一节将描述该数据集。
- en: The GMB data set
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GMB 数据集
- en: 'With all the basics in the bag, we are ready to build a model that classifies
    NERs. For this task, the **Groningen Meaning Bank** (**GMB**) data set will be
    used. This dataset is not considered a gold standard. This means that this data
    set is built using automatic tagging software, followed by human raters updating
    subsets of the data. However, this is a very large and rich data set. This data
    has a lot of useful annotations that make it quite suitable for training models.
    It is also constructed from public domain text, making it easy to use for training.
    The following named entities are tagged in this corpus:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基础知识掌握后，我们准备构建一个用于分类命名实体识别（NER）的模型。对于这一任务，将使用**格罗宁根语义库**（**GMB**）数据集。这个数据集并不被认为是黄金标准。也就是说，该数据集是通过自动标注软件构建的，随后由人工评分员更新数据子集。然而，这是一个非常大且丰富的数据集，包含了大量有用的注释，非常适合用于模型训练。它也来源于公共领域的文本，因此很容易用于训练。这个语料库中标注了以下命名实体：
- en: geo = Geographical entity
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: geo = 地理实体
- en: org = Organization
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: org = 组织
- en: per = Person
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: per = 人物
- en: gpe = Geopolitical entity
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gpe = 地缘政治实体
- en: tim = Time indicator
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tim = 时间指示符
- en: art = Artifact
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: art = 人造物
- en: eve = Event
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: eve = 事件
- en: nat = Natural phenomenon
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nat = 自然现象
- en: In each of these categories, there can be subcategories. For example, *tim*
    may be further sub-divided and represented as *tim-dow* representing a time entity
    corresponding to a day of the week, or *tim-dat*, which represents a date. For
    this exercise, these sub-entities are going to be aggregated into the eight top-level
    entities listed above. The number of examples varies widely between the sub-entities.
    Consequently, the accuracy varies widely due to the lack of enough training data
    for some of these subcategories.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些类别中，可能存在子类别。例如，*tim* 可能进一步细分为 *tim-dow*，表示一周中的某一天，或者 *tim-dat*，表示一个日期。对于本次练习，这些子实体将被汇总成上述的八个顶级实体。这些子实体的样本数量差异很大，因此，由于某些子类别缺乏足够的训练数据，准确度差异也很大。
- en: The data set also provides the NER entity for each word. In many cases, an entity
    may comprise multiple words. If *Hyde Park* is a geographical entity, both words
    will be tagged as a *geo* entity. In terms of training models for NER, there is
    another way to represent this data that can have a significant impact on the accuracy
    of the model. This requires the usage of the BIO tagging scheme. In this scheme,
    the first word of an entity, single word or multi-word, is tagged with *B-{entity
    tag}*. If the entity is multi-word, each successive word would be tagged as *I-{entity
    tag}*. In the example above, *Hyde Park* would be tagged as *B-geo I-geo*. All
    these are steps of pre-processing that are required for a data set. All the code
    for this example can be found in the `NER with BiLSTM and CRF.ipynb` notebook
    in the `chapter3-ner-with-lstm-crf` folder of the GitHub repository.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started by loading and processing the data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data can be downloaded from the University of Groningen website as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Please note that the data is quite large – over 800MB. If `wget` is not available
    on your system, you may use any other tool such as, `curl` or a browser to download
    the data set. This step may take some time to complete. If you have a challenge
    accessing the data set from the University server, you may download a copy from
    Kaggle: [https://www.kaggle.com/bradbolliger/gmb-v220](https://www.kaggle.com/bradbolliger/gmb-v220).
    Also note that since we are going to be working on large data sets, some of the
    following steps may take some time to execute. In the world of **Natural Language
    Processing** (**NLP**), more training data and training time is key to great results.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: All the code for this example can be found in the `NER with BiLSTM and CRF.ipynb`
    notebook in the `chapter3-ner-with-lstm-crf` folder of the GitHub repository.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: The data unzips into the `gmb-2.2.0` folder. The `data` subfolder has a number
    of subfolders with different files. `README` supplied with the data set provides
    details about the various files and their contents. For this example, we will
    be using only files named `en.tags` in various subdirectories. These files are
    tab-separated files with each word of a sentence in a row.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'There are ten columns of information:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The token itself
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A POS tag as used in the Penn Treebank ([ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz](ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz))
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lemma
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A named-entity tag, or 0 if none
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A WordNet word sense number for the respective lemma-POS combinations, or 0
    if not applicable ([http://wordnet.princeton.edu](http://wordnet.princeton.edu))
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For verbs and prepositions, a list of the VerbNet roles of the arguments in
    order of combination in the **Combinatory Categorial Grammar** (**CCG**) derivation,
    or `[]` if not applicable ([http://verbs.colorado.edu/~mpalmer/projects/verbnet.html](http://verbs.colorado.edu/~mpalmer/projects/verbnet.html))
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic relation in noun-noun compounds, possessive apostrophes, temporal modifiers,
    and so on. Indicated using a preposition, or 0 if not applicable
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名词-名词复合词中的语义关系、所有格撇号、时间修饰语等。通过介词表示，若不适用则为0
- en: An animacy tag as proposed by Zaenen et al. (2004), or 0 if not applicable ([http://dl.acm.org/citation.cfm?id=1608954](http://dl.acm.org/citation.cfm?id=1608954))
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据Zaenen等人（2004年）提出的建议，提供一个生动性标签，若不适用则为0（[http://dl.acm.org/citation.cfm?id=1608954](http://dl.acm.org/citation.cfm?id=1608954)）
- en: A supertag (lexical category of CCG)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个超级标签（CCG的词汇类别）
- en: The lambda-DRS representing the semantics of the token in Boxer's Prolog format
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用Boxer的Prolog格式表示的lambda-DRS，表示标记的语义
- en: 'Out of these fields, we are going to use only the token and the named entity
    tag. However, we will work through loading the POS tag for a future exercise.
    The following code gets all the paths for these tags files:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些字段中，我们只使用标记和命名实体标签。然而，我们将在未来的练习中加载POS标签。以下代码获取这些标签文件的所有路径：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A few processing steps need to happen. Each file has a number of sentences,
    with each words in a row. The entire sentence as a sequence and the corresponding
    sequence of NER tags need to be fed in as inputs while training the model. As
    mentioned above, the NER tags also need to be simplified to the top-level entities
    only. Secondly, the NER tags need to be converted to the IOB format. **IOB** stands
    for **In-Other-Begin**. These letters are used as a prefix to the NER tag. The
    sentence fragment in the table below shows how this scheme works:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进行一些处理步骤。每个文件包含多个句子，每个句子中的单词排列成行。整个句子作为一个序列，与之对应的NER标签序列在训练模型时需要一起输入。如前所述，NER标签也需要简化为仅包含顶级实体。其次，NER标签需要转换为IOB格式。**IOB**代表**In-Other-Begin**。这些字母作为前缀附加到NER标签上。下表中的句子片段展示了该方案的工作方式：
- en: '| Reverend | Terry | Jones | arrived | in | New | York |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Reverend | Terry | Jones | arrived | in | New | York |'
- en: '| B-per | I-per | I-per | O | O | B-geo | I-geo |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| B-per | I-per | I-per | O | O | B-geo | I-geo |'
- en: The table above shows this tagging scheme after processing. Note that New York
    is one location. As soon as *New* is encountered, it marks the start of the geo
    NER tag, hence it is assigned B-geo. The next word is *York*, which is a continuation
    of the same geographical entity. For any network, classifying the word *New* as
    the start of the geographical entity is going to be very challenging. However,
    a BiLSTM network would be able to see the succeeding words, which helps quite
    a bit with disambiguation. Furthermore, the advantage of IOB tags is that the
    accuracy of the model improves considerably in terms of detection. This happens
    because once the beginning of an NER tag is detected, the choices for the next
    tag become quite limited.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上表展示了处理后的标签方案。请注意，New York是一个地点。一旦遇到*New*，它标志着地理位置NER标签的开始，因此被标记为B-geo。下一个单词是*York*，它是同一地理实体的延续。对于任何网络来说，将单词*New*分类为地理实体的开始将是非常具有挑战性的。然而，BiLSTM网络能够看到随后的单词，这对消除歧义非常有帮助。此外，IOB标签的优势在于，在检测方面，模型的准确性显著提高。这是因为一旦检测到NER标签的开始，下一标签的选择就会大大受限。
- en: 'Let''s get to the code. First, create a directory to store all the processed
    files:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入代码部分。首先，创建一个目录来存储所有处理后的文件：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We want to process the tags so that we strip the subcategories of the NER tags
    out. It would also be nice to collect some stats on the types of tags in the documents:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望处理这些标签，以便去除NER标签中的子类别。还希望收集文档中标签类型的一些统计数据：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The NER tag and IOB tag counters are set up above. A method for stripping the
    subcategory out of the NER tags is defined. The next method takes a sequence of
    tags and converts them into IOB format:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上面已设置了NER标签和IOB标签计数器。定义了一个方法来去除NER标签中的子类别。下一个方法接受一系列标签并将其转换为IOB格式：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once these two convenience functions are ready, all the tags files need to
    be read and processed:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这两个便捷函数准备好后，所有的标签文件都需要被读取并处理：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'First, a counter is set for the number of sentences. A list of files written
    with paths are also initialized. As processed files are written out, their paths
    are added to the `outfiles` variable. This list will be used later to load all
    the data and to train the model. Files are read and split into two empty newline
    characters. That is the marker for the end of a sentence in the file. Only the
    actual words, POS tokens, and NER tokens are used from the file. Once these are
    collected, a new CSV file is written with three columns: the sentence, a sequence
    of POS tags, and a sequence of NER tags. This step may take a little while to
    execute:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，设置一个计数器来统计句子的数量。还初始化了一个包含路径的文件列表。随着处理文件的写入，它们的路径会被添加到`outfiles`变量中。这个列表将在稍后用于加载所有数据并训练模型。文件被读取并根据两个空行符进行分割。该符号表示文件中句子的结束。文件中仅使用实际的单词、POS标记和NER标记。收集完这些后，将写入一个新的CSV文件，包含三列：句子、POS标签序列和NER标签序列。这个步骤可能需要一点时间来执行：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To confirm the distribution of the NER tags before and after processing, we
    can use the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认NER标签在处理前后的分布，我们可以使用以下代码：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As is evident, some tags were very infrequent, like *tim-dom*. It would be next
    to impossible for a network to learn them. Aggregating up one level helps increase
    the signal for these tags. To check if the entire process completed properly,
    check that the `ner` folder has 10,000 files. Now, let us load the processed data
    to normalize, tokenize, and vectorize it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，有些标签非常不常见，比如*tim-dom*。网络几乎不可能学习到它们。将其聚合到一个层级有助于增加这些标签的信号。为了检查整个过程是否完成，可以检查`ner`文件夹是否有10,000个文件。现在，让我们加载处理后的数据以进行标准化、分词和向量化。
- en: Normalizing and vectorizing data
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化和向量化数据
- en: 'For this section, `pandas` and `numpy` methods will be used. The first step
    is to load the contents of the processed files into one `DataFrame`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，将使用`pandas`和`numpy`方法。第一步是将处理过的文件内容加载到一个`DataFrame`中：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This step may take a while given that it is processing 10,000 files. Once the
    content is loaded, we can check the structure of the `DataFrame`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步可能需要一些时间，因为它正在处理10,000个文件。一旦内容加载完成，我们可以检查`DataFrame`的结构：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Both the text and NER tags need to be tokenized and encoded into numbers for
    use in training. We are going to be using core methods provided by the `keras.preprocessing`
    package. First, the tokenizer will be used to tokenize the text. In this example,
    the text only needs to be tokenized by white spaces, as it has been broken up
    already:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和NER标签都需要被分词并编码成数字，以便用于训练。我们将使用`keras.preprocessing`包提供的核心方法。首先，将使用分词器来分词文本。在这个例子中，由于文本已经被空格分割开，所以只需要通过空格进行分词：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The default values for the tokenizer are quite reasonable. However, in this
    particular case, it is important to only tokenize on spaces and not clean the
    special characters out. Otherwise the data will become mis-formatted:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器的默认值相当合理。然而，在这种特殊情况下，重要的是只按空格进行分词，而不是清理特殊字符。否则，数据会变得格式错误：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Even though we do not use the POS tags, the processing for them is included.
    Use of the POS tags can have an impact on the accuracy of an NER model. Many NER
    entities are nouns, for example. However, we will see how to process POS tags
    but not use them in the model as features. This is left as an exercise to the
    reader.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们不使用POS标签，处理它们的步骤仍然包括在内。POS标签的使用会对NER模型的准确性产生影响。例如，许多NER实体是名词。然而，我们将看到如何处理POS标签但不将其作为特征用于模型。这部分留给读者作为练习。
- en: 'This tokenizer has some useful features. It provides a way to restrict the
    size of the vocabulary by word counts, TF-IDF, and so on. If the `num_words` parameter
    is passed with a numeric value, the tokenizer will limit the number of tokens
    by word frequencies to that number. The `fit_on_texts` method takes in all the
    texts, tokenizes them, and constructs dictionaries with tokens that will be used
    later to tokenize and encode in one go. A convenience function, `get_config()`,
    can be called after the tokenizer has been fit on texts to provide information
    about the tokens:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器有一些有用的功能。它提供了一种通过词频、TF-IDF等方式限制词汇表大小的方法。如果传入`num_words`参数并指定一个数字，分词器将根据词频限制令牌的数量为该数字。`fit_on_texts`方法接受所有文本，将其分词，并构建一个字典，稍后将在一次操作中用于分词和编码。可以在分词器适配完文本后调用方便的`get_config()`函数，以提供有关令牌的信息：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `index_word` dictionary property in the config provides a mapping between
    IDs and tokens. There is a considerable amount of information in the config. The
    vocabularies can be obtained from the config:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 配置中的`index_word`字典属性提供了ID与标记之间的映射。配置中包含了大量信息。词汇表可以从配置中获取：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Tokenizing and encoding text and named entity labels is quite easy:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对文本和命名实体标签进行分词和编码是非常简单的：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Since sequences are of different sizes, they will all be padded or truncated
    to a size of 50 tokens. A helper function is used for this task:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于序列的大小不同，它们将被填充或截断为50个标记的大小。为此任务使用了一个辅助函数：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The last step above is to ensure that shapes are correct before moving to the
    next step. Verifying shapes is a very important part of developing code in TensorFlow.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上面最后一步是确保在进入下一步之前，形状是正确的。验证形状是开发TensorFlow代码中非常重要的一部分。
- en: 'There is an additional step that needs to be performed on the labels. Since
    there are multiple labels, each label token needs to be one-hot encoded like so:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 需要对标签执行额外的步骤。由于有多个标签，每个标签标记需要进行独热编码，如下所示：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, we are ready to build and train a model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好构建和训练模型了。
- en: A BiLSTM model
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个BiLSTM模型
- en: 'The first model we will try is a BiLSTM model. First, the basic constants need
    to be set up:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试的第一个模型是BiLSTM模型。首先，需要设置基本常量：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, a convenience function for instantiating models is defined:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个便捷函数来实例化模型：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We are going to train our own embeddings. The next chapter will talk about using
    pre-trained embeddings and using them in models. After the embedding layer, there
    is a BiLSTM layer, followed by a `TimeDistributed` dense layer. This last layer
    is different from the sentiment analysis model, where there was only a single
    unit for binary output. In this problem, for each word in the input sequence,
    an NER token needs to be predicted. So, the output has as many tokens as the input
    sequence. Consequently, output tokens correspond 1-to-1 with input tokens and
    are classified as one of the NER classes. The `TimeDistributed` layer provides
    this capability. The other thing to note in this model is the use of regularization.
    It is important that the model does not overfit the training data. Since LSTMs
    have high model capacity, using regularization is very important. Feel free to
    play with some of these hyperparameters to get a feel for how the model will react.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练我们自己的嵌入层。下一章将讨论如何使用预训练的嵌入层并将其应用于模型。在嵌入层之后是一个BiLSTM层，接着是一个`TimeDistributed`全连接层。这个最后的层与情感分析模型有所不同，情感分析模型只有一个用于二分类输出的单元。而在这个问题中，对于输入序列中的每个单词，都需要预测一个NER标记。因此，输出的标记与输入的标记一一对应，并被分类为其中一个NER类别。`TimeDistributed`层提供了这种能力。这个模型的另一个需要注意的地方是正则化的使用。确保模型不会过拟合训练数据非常重要。由于LSTM具有较高的模型容量，因此使用正则化非常重要。可以随意调整这些超参数，看看模型的反应。
- en: 'Now the model can be compiled:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以编译模型了：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This simplistic model has over 2.6 million parameters!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的模型有超过260万个参数！
- en: If you notice, the bulk of the parameters are coming from the size of the vocabulary.
    The vocabulary has 39,422 words. This increases the model training time and computational
    capacity required. One way to reduce this is to make the vocabulary size smaller.
    The easiest way to do this would be to only consider words that have more than
    a certain frequency of occurrence or to remove words smaller than a certain number
    of characters. The vocabulary can also be reduced by converting all characters
    to lower case. However, in NER, case is a very important feature.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到，大部分参数来自词汇表的大小。词汇表包含39,422个单词。这增加了模型的训练时间和所需的计算能力。减少这个问题的一种方法是将词汇表的大小减小。最简单的方法是只考虑出现频率超过某个阈值的单词，或者去除小于某个字符数的单词。还可以通过将所有字符转换为小写来减少词汇表的大小。然而，在NER任务中，大小写是一个非常重要的特征。
- en: 'This model is ready for training. The last thing that is needed is to split
    the data into train and test sets:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型已经准备好进行训练了。最后需要做的事情是将数据拆分为训练集和测试集：
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, the model is ready for training:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型已准备好进行训练：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Over 15 epochs of training, the model is doing quite well with over 99% accuracy.
    Let''s see how the model performs on the test set and whether the regularization
    helped:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 经过15个epochs的训练，模型表现得相当不错，准确率超过99%。让我们看看模型在测试集上的表现，正则化是否有所帮助：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The model performs well on the test data set, with over 96.5% accuracy. The
    difference between the train and test accuracies is still there, implying that
    the model could use some additional regularization. You can play with the dropout
    variable or add additional dropout layers between the embedding and BiLSTM layers,
    and between the `TimeDistributed` layer and the final Dense layer.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a sentence fragment tagged by this model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Faure | Gnassingbe | said | in | a | speech | carried | by | state | media
    | Friday |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| Actual | B-per | I-per | O | O | O | O | O | O | O | O | B-tim |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| Model | B-per | I-per | O | O | O | O | O | O | O | O | B-tim |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: This model is not doing poorly at all. It was able to identify the person and
    time entities in the sentence.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: As good as this model is, it does not use an important characteristic of named
    entity tags – a given tag is highly correlated with the tag coming after it. CRFs
    can take advantage of this information and further improve the accuracy of NER
    tasks. Let's understand how CRFs work and add them to the network above next.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Conditional random fields (CRFs)
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'BiLSTM models look at a sequence of input words and predict the label for the
    current word. In making this determination, only the information of previous inputs
    is considered. Previous predictions play no role in making this decision. However,
    there is information encoded in the sequence of labels that is being discounted.
    To illustrate this point, consider a subset of NER tags: **O**, **B-Per**, **I-Per**,
    **B-Geo**, and **I-Geo**. This represents two domains of person and geographical
    entities and an *Other* category for everything else. Based on the structure of
    IOB tags, we know that any **I**- tag must be preceded by a **B-I** from the same
    domain. This also implies that an **I**- tag cannot be preceded by an **O** tag.
    The following diagram shows the possible state transitions between these tags:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_02.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Possible NER tag transitions'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.2* color codes similar types of transitions with the same color.
    An **O** tag can transition only to a **B** tag. A **B** tag can go to its corresponding
    **I** tag or back to the **O** tag. An **I** tag can transition back to itself,
    an **O** tag, or a **B** tag of a different domain (not represented in the diagram
    for simplicity). For a set of **N** tags, these transitions can be represented
    by a matrix of dimension *N x N*. *P*[i,j] denotes the possibility of tag *j*
    coming after tag *i*. Note that these transition weights can be learned based
    on the data. Such a learned transition weights matrix could be used during prediction
    to consider the entire sequence of predicted labels and make updates to the probabilities.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an illustrative matrix with indicative transition weights:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '| From > To | O | B-Geo | I-Geo | B-Org | I-Org |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| O | 3.28 | 2.20 | 0.00 | 3.66 | 0.00 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| B-Geo | -0.25 | -0.10 | 4.06 | 0.00 | 0.00 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| I-Geo | -0.17 | -0.61 | 3.51 | 0.00 | 0.00 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| B-Org | -0.10 | -0.23 | 0.00 | -1.02 | 4.81 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| I-Org | -0.33 | -1.75 | 0.00 | -1.38 | 5.10 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: As per the table above, the weight of the edge connecting I-Org to B-Org has
    a weight of -1.38, implying that this transition is extremely unlikely to happen.
    Practically, implementing a CRF has three main steps. The first step is modifying
    the score generated by the BiLSTM layer and accounting for the transition weights,
    as shown above. A sequence of predictions
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_001.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'generated by the BiLSTM layer above for a sequence of *n* tags in the space
    of *k* unique tags is available, which operates on an input sequence *X*. *P*
    represents a matrix of dimensions *n × k*, where the element *P*[i,j] represents
    the probability of *j*^(th) tag for output at the position *y*[i]. Let *A* be
    a square matrix of transition probabilities as shown above, with a dimension of
    *(k + 2) × (k + 2)* where two additional tokens are added for start- and end-of-sentence
    markers. Element *A*[i,j] represents the transition probability from *i* to tag
    *j*. Using these values, a new score can be calculated like so:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_002.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'A softmax can be calculated over all possible tag sequences to get the probability
    for a given sequence *y*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_003.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: '*Y*[X] represents all possible tag sequences, including those that may not
    conform to the IOB tag format. To train using this softmax, a log-likelihood can
    be calculated over this. Through clever use of dynamic programming, a combinatorial
    explosion can be avoided, and the denominator can be computed quite efficiently.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Only simplistic math is shown to help build an intuition of how this method
    works. The actual computations will become clear in the custom layer implementation
    below.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: While decoding, the output sequence is the one that has the maximum score among
    these possible sequences, calculated conceptually using an `argmax` style function.
    The Viterbi algorithm is commonly used to implement a dynamic programming solution
    for decoding. First, let us code the model and the training for it before getting
    into decoding.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: NER with BiLSTM and CRFs
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementing a BiLSTM network with CRFs requires adding a CRF layer on top
    of the BiLSTM network developed above. However, a CRF is not a core part of the
    TensorFlow or Keras layers. It is available through the `tensorflow_addons` or
    `tfa` package. The first step is to install this package:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'There are many sub-packages, but the convenience functions for the CRF are
    in the `tfa.text` subpackage:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_03_03.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: tfa.text methods'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: While low-level methods for implementing the CRF layer are provided, a high-level
    layer-like construct is not provided. The implementation of a CRF requires a custom
    layer, a loss function, and a training loop. Post training, we will look at how
    to implement a customized inference function that will use Viterbi decoding.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the custom CRF layer, loss, and model
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the flow above, there will be an embedding layer and a BiLSTM layer.
    The output of the BiLSTM needs to be evaluated with the CRF log-likelihood loss
    described above. This is the loss that needs to be used to train the model. The
    first step in implementation is creating a custom layer. Implementing a custom
    layer in Keras requires subclassing `keras.layers.Layer`. The main method to be
    implemented is `call()`, which takes inputs to the layer, transforms them, and
    returns the result. Additionally, the constructor to the layer can also set up
    any parameters that are needed. Let''s start with the constructor:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The main parameters that are needed are:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '**The number of labels and the transition matrix**: As described in the section
    above, a transition matrix needs to be learned. The dimension of that square matrix
    is the number of labels. The transition matrix is initialized using the parameters.
    This transition parameters matrix is not trainable through gradient descent. It
    is calculated as a consequence of computing the log-likelihoods. The transition
    parameters matrix can also be passed into this layer if it has been learned in
    the past.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The mask id**: Since the sequences are padded, it is important to recover
    the original sequence lengths for computing transition scores. By convention,
    a value of 0 is used for the mask, and that is the default. This parameter is
    set up for future configurability.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second method is to compute the result of applying this layer. Note that
    as a layer, the CRF layer merely regurgitates the outputs during training time.
    The CRF layer is useful only during inference. At inference time, it uses the
    transition matrix and logic to correct the sequences'' output by the BiLSTM layers
    before returning them. For now, this method is quite simple:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This method takes the inputs as well as a parameter that helps specify if this
    method is called during training or during inference. If this variable is not
    passed, it is pulled from the Keras backend. When models are trained with the
    `fit()` method, `learning_phase()` returns `True`. When the `.predict()` method
    is called on a model, this flag is set to `false`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: As sequences being passed are masked, this layer needs to know the real sequence
    lengths during inference time for decoding. A variable is passed for it but is
    unused at this time. Now that the basic CRF layer is ready, let's build the model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: A custom CRF model
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the model builds on a number of preexisting layers in addition to the
    custom CRF layer above, explicit imports help the readability of the code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The first step is to define a constructor that will create the various layers
    and store the appropriate dimensions:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This constructor takes in the number of hidden units for the BiLSTM later, the
    size of words in the vocabulary, the number of NER labels, and the size of the
    embeddings. Additionally, a default name is set by the constructor, which can
    be overridden at the time of instantiation. Any additional parameters supplied
    are passed along as keyword arguments.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'During training and prediction, the following method will be called:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: So, in a few lines of code, we have implemented a customer model using the custom
    CRF layer developed above. The only thing that we need now to train this model
    is a loss function.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: A custom loss function for NER using a CRF
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s implement the loss function as part of the CRF layer, encapsulated in
    a function of the same name. Note that when this function is called, it is usually
    passed the labels and predicted values. We will model our loss function on the
    custom loss functions in TensorFlow. Add this code to the CRF layer class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This function takes the true labels and predicted labels. Both of these tensors
    are usually of the shape (batch size, max sequence length, number of NER labels).
    However, the log-likelihood function in the `tfa` package expects the labels to
    be in a (batch size, max sequence length)-shaped tensor. So a convenience function,
    implemented as part of the CRF layer and shown below, is used to perform the conversion
    of label shapes:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The log-likelihood function also requires the actual sequence lengths for each
    example. These sequence lengths can be computed from the labels and the mask identifier
    that was set up in the constructor of this layer (see above). This process is
    encapsulated in another convenience function, also part of the CRF layer:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: First, a Boolean mask is generated from the labels by comparing the value of
    the label to the mask ID. Then, through casting the Boolean as an integer and
    summing across the row, the length of the sequence is regenerated. Now, the `tfa.text.crf_log_likelihood()`
    function is called to calculate and return the log-likelihoods and the transition
    matrix. The CRF layer's transition matrix is updated with the transition matrix
    returned from the function call. Finally, the loss is computed by summing up all
    the log-likelihoods returned.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: At this point, our coded custom model is ready to start training. We will need
    to set up the data and create a custom training loop.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Implementing custom training
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model needs to be instantiated and initialized for training:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As in past examples, an Adam optimizer will be used. Next, we will construct
    `tf.data.DataSet` from the DataFrames loaded in the BiLSTM section above:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Roughly 20% of the data is reserved for testing. The rest is used for training.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement a custom training loop, TensorFlow 2.0 exposes a gradient tape.
    This allows low-level management of the main steps required for training any model
    with gradient descent. These steps are:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Computing the forward pass predictions
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing the loss when these predictions are compared with the labels
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing the gradients for the trainable parameters based on the loss and then
    using the optimizer to adjust the weights
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let us train this model for 5 epochs and watch the loss as training progresses.
    Compare this to the 15 epochs of training for the previous model. The custom training
    loop is shown below:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'A metric is created to keep track of the average loss over time. For 5 epochs,
    inputs and labels are pulled from the training data set, one batch at a time.
    Using `tf.GradientTape()` to keep track of the operations, the steps outlined
    in the bullets above are implemented. Note that we pass the trainable variable
    manually as this is a custom training loop. Finally, the loss metric is printed
    every 50^(th) step to show training progress. This yields the results below, which
    have been abbreviated:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Given we implemented a custom training loop, without requiring a compilation
    of the model, we could not obtain a summary of the model parameters before. To
    get an idea of the size of the model, a summary can be obtained now:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: It is comparable in size to the previous model but has some untrainable parameters.
    These are coming from the transition matrix. The transition matrix is not learned
    through gradient descent. Thus, they are classified as non-trainable parameters.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: However, training loss is hard to interpret. To compute accuracy, we need to
    implement decoding, which is the focus of the next section. For the moment, let's
    assume that decoding is available and examine the results of training for 5 epochs.
    For illustration purposes, here is a sentence from the test set with the results
    pulled at the end of the first epoch and at the end of five epochs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The example sentence is:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The corresponding true label is:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This is a difficult example for NER with *The Washington Post* as a three-word
    organization, where the first word is very common and used in multiple contexts,
    and the second word is also the name of a geographical location. Also note the
    imperfect labels of the GMB data set, where the second tag of the name *Ushakov*
    is tagged as an organization. At the end of the first epoch of training, the model
    predicts:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'It gets confused by the organization not being where it expects it to be. It
    also shows that it hasn''t learned the transition probabilities by putting an
    I-org tag after a B-geo tag. However, it does not make a mistake in the person
    portion. Unfortunately for the model, it will not get credit for this great prediction
    of the person tag, and due to imperfect labels, it will still count as a miss.
    The result after five epochs of training is better than the original:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This is a great result, given the limited amount of training we have done. Now,
    let's see how we can decode the sentence in the CRF layer to get these sequences.
    The algorithm used for decoding is called the Viterbi decoder.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Viterbi decoding
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A straightforward way to predict the sequence of labels is to output the label
    that has the highest activation from the previous layers of the network. However,
    this could be sub-optimal as it assumes that each label prediction is independent
    of the previous or successive predictions. The Viterbi algorithm is used to take
    the predictions for each word in the sequence and apply a maximization algorithm
    so that the output sequence has the highest likelihood. In future chapters, we
    will see another way of accomplishing the same objective through beam search.
    Viterbi decoding involves maximizing over the entire sequence as opposed to optimizing
    at each word of the sequence. To illustrate this algorithm and way of thinking,
    let's take an example of a sentence of 5 words, and a set of 3 labels. These labels
    could be O, B-geo, and I-geo as an example.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm needs the transition matrix values between labels. Recall that
    this was generated and stored in the custom CRF layer above. Let''s say that the
    matrix looks like so:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '| From > To | Mask | O | B-geo | I-geo |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| Mask | 0.6 | 0.3 | 0.2 | 0.01 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| O | 0.8 | 0.5 | 0.6 | 0.01 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| B-geo | 0.2 | 0.4 | 0.01 | 0.7 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| I-geo | 0.3 | 0.4 | 0.01 | 0.5 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: 'To explain how the algorithm works, the figure shown below will be used:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_03_04.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Steps in the Viterbi decoder'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The sentence starts from the left. Arrows from the start of the word to the
    first token represent the probability of the transition between the two tokens.
    The numbers on the arrows should match the values in the transition matrix above.
    Within the circles denoting labels, scores generated by the neural network, the
    BiLSTM model, in our case, are shown for the first word. These scores need to
    be added together to give the final score of the words. Note that we switched
    the terminology from probabilities to scores as normalization is not being performed
    for this particular example.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the first word label
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Score of *O*: 0.3 (transition score) + 0.2 (activation score) = 0.5'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Score of *B-geo*: 0.2 (transition score) + 0.3 (activation score) = 0.5'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Score of *I-geo*: 0.01 (transition score) + 0.01 (activation score) = 0.02'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, it is equally likely that an *O* or *B-geo* tag will be the
    starting tag. Let''s consider the next tag and calculate the scores using the
    same approach for the following sequences:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '| (*O*, *B-geo*) = 0.6 + 0.3 = 0.9 | (*B-geo*, *O*) = 0.4 + 0.3 = 0.7 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| (*O*, *I-geo*) = 0.01+ 0.25 = 0.26 | (*B-geo*, *B-geo*) = 0.01 + 0.3 = 0.31
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| (*O*, *O*) = 0.5 + 0.3 = 0.8 | (*B-geo*, *I-geo*) = 0.7 + 0.25 = 0.95 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: 'This process is called the forward pass. It should also be noted, even though
    this is a contrived example, that activations at a given input may not be the
    best predictor of the right label for that word once the previous labels have
    been considered. If the sentence was only two words, then the scores for various
    sequences could be calculated by summing by each step:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '| (*Start*, *O*, *B-Geo*) = 0.5 + 0.9 = 1.4 | (*Start*, *B-Geo*, *O*) = 0.5
    + 0.7 = 1.2 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| (*Start*, *O*, *O*) = 0.5 + 0.8 = 1.3 | (*Start*, *B-geo*, *B-geo*) = 0.5
    + 0.31 = 0.81 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| (*Start*, *O*, *I-Geo*) = 0.5 + 0.26 = 0.76 | (*Start*, *B-geo*, *I-geo*)
    = 0.5 + 0.95) = 1.45 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: If only the activation scores were considered, the most probable sequences would
    be either (*Start*, *B-geo*, *O*) or (*Start*, *B-geo*, *B-geo*). However, using
    the transition scores along with the activations means that the sequence with
    the highest probability is (*Start*, *B-geo*, *I-geo*) in this example. While
    the forward pass gives the highest score of the entire sequence given the last
    token, the backward pass process would reconstruct the sequence that resulted
    in this highest score. This is essentially the Viterbi algorithm, which uses dynamic
    programming to perform these steps in an efficient manner.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing this algorithm is aided by the fact the core computation is provided
    as a method in the `tfa` package. This decoding step will be implemented in the
    `call()` method of the CRF layer implemented above. Modify this method to look
    like so:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The new lines added have been highlighted. The `viterbi_decode()` method takes
    the activations from the previous layers and the transition matrix along with
    the maximum sequence length to compute the path with the highest score. This score
    is also returned, but we ignore it for our purposes of inference. This process
    needs to be performed for each sequence in the batch. Note that this method returns
    sequences on different lengths. This makes it harder to convert into tensors,
    so a utility function is used to pad the returned sequences:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: A dropout layer works completely opposite to the way this CRF layer works. A
    dropout layer modifies the inputs only during training time. During inference,
    it merely passes all the inputs through.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Our CRF layer works in the exact opposite fashion. It passes the inputs through
    during training, but it transforms inputs using the Viterbi decoder during inference
    time. Note the use of the `training` parameter to control the behavior.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the layer is modified and ready, the model needs to be re-instantiated
    and trained. Post-training, inference can be performed like so:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This will run inference on a small batch of testing data. Let''s check the
    result for the example sentence:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As we can see in the highlighted output, the results are better than the actual
    data!
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'To get a sense of the accuracy of the training, a custom method needs to be
    implemented. This is shown below:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Using `numpy`''s `MaskedArray` feature, the predictions and labels are compared
    and converted to an integer array, and the mean is calculated to compute the accuracy:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: This is a pretty accurate model, just after 5 epochs of training and with very
    simple architecture, all while using embeddings that are trained from scratch.
    A recall metric can also be implemented in a similar fashion. A BiLSTM-only model,
    shown earlier, took 15 epochs of training to get to a similar accuracy!
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: This completes the implementation of an NER model using BiLSTMs and CRFs. If
    this is interesting and you would like to continue working on this, look for the
    CoNLL 2003 data set for NER. Even today, papers are being published that aim to
    improve the accuracy of the models based on that data set.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered quite a lot of ground in this chapter. NER and its importance
    in the industry were explained. To build NER models, BiLSTMs and CRFs are needed.
    Using BiLSTMs, which we learned about in the previous chapter while building a
    sentiment classification model, we built a first version of a model that can label
    named entities. This model was further improved using CRFs. In the process of
    building these models, we covered the use of the TensorFlow DataSet API. We also
    built advanced models for CRF mode by building a custom Keras layer, a custom
    model, custom loss function, and a custom training loop.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Thus far, we have trained embeddings for tokens in the models. A considerable
    amount of lift can be achieved by using pre-trained embeddings. In the next chapter,
    we'll focus on the concept of transfer learning and the use of pre-trained embeddings
    like BERT.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
