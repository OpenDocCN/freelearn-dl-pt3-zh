- en: Autoencoders – Feature Extraction and Denoising
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An autoencoder network is nowadays one of the widely used deep learning architectures.
    It's mainly used for unsupervised learning of efficient decoding tasks. It can
    also be used for dimensionality reduction by learning an encoding or a representation
    for a specific dataset. Using autoencoders in this chapter, we'll show how to
    denoise your dataset by constructing another dataset with the same dimensions
    but less noise. To use this concept in practice, we will extract the important
    features from the MNIST dataset and try to see how the performance will be significantly
    enhanced by this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoder architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing the MNIST dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An autoencoder is yet another deep learning architecture that can be used for
    many interesting tasks, but it can also be considered as a variation of the vanilla
    feed-forward neural network, where the output has the same dimensions as the input.
    As shown in *Figure 1*, the way autoencoders work is by feeding data samples *(x[1],...,x[6]) *to
    the network. It will try to learn a lower representation of this data in layer
    *L2*, which you might call a way of encoding your dataset in a lower representation.
    Then, the second part of the network, which you might call a decoder, is responsible
    for constructing an output from this representation ![](img/f1bff437-7017-445b-93d4-7886dd3f1623.png).
    You can think of the intermediate lower representation that the network learns
    from the input data as a compressed version of it.
  prefs: []
  type: TYPE_NORMAL
- en: Not very different from all the other deep learning architectures that we have
    seen so far, autoencoders use backpropagation algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'An autoencoder neural network is an unsupervised learning algorithm that applies
    backpropagation, setting the target values to be equal to the inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e447a3cc-4dd7-4ae1-bba0-fd6c845ebbe5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: General autoencoder architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will demonstrate some examples of different variations of
    autoencoders using the MNIST dataset. As a concrete example, suppose the inputs *x*
    are the pixel intensity values from a 28 x 28 image (784 pixels); so the number
    of input data samples is *n=784*. There are *s2=392* hidden units in layer *L2*.
    And since the output will be of the same dimensions as the input data samples,
    *y ∈ R784*. The number of neurons in the input layer will be *784*, followed by
    *392* neurons in the middle layer *L2*; so the network will be a lower representation,
    which is a compressed version of the output. The network will then feed this compressed
    lower representation of the input *a(L2) ∈ R392* to the second part of the network,
    which will try hard to reconstruct the input pixels *784* from this compressed
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders rely on the fact that the input samples represented by the image
    pixels will be somehow correlated and then it will use this fact to reconstruct
    them. So autoencoders are a bit similar to dimensionality reduction techniques,
    because they learn a lower representation of the input data as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, a typical autoencoder will consist of three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder part, which is responsible for compressing the input into a lower
    representation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code, which is the intermediate result of the encoder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder, which is responsible for reconstructing the the original input
    using this code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure shows the three main components of a typical autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f678096-4330-4dfb-b907-c4f22b8c3971.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: How encoders function over an image'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, autoencoders part learn a compressed representation of the
    input that are then fed to the third part, which tries to reconstruct the input.
    The reconstructed input will be similar to the output but it won't be exactly
    the same as the original output, so autoencoders can't be used for compression
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we mentioned, a typical autoencoder consists of three parts. Let''s explore
    these three parts in more detail. To motivate you, we are not going to reinvent
    the wheel here in this chapter. The encoder-decoder part is nothing but a fully
    connected neural network, and the code part is another neural network but it''s
    not fully connected. The dimensionality of this code part is controllable and
    we can treat it as a hyperparameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/552fb5f6-0459-4e13-887b-d55d288e0f72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: General encoder-decoder architecture of autoencoders'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into using autoencoders for compressing the MNIST dataset, we
    are going to list the set of hyperparameters that we can use to fine-tune the
    autoencoder model. There are mainly four hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code part size**: This is the number of units in the middle layer. The lower
    the number of units we have in this layer, the more compressed the representation
    of the input we get.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Number of layers in the encoder and decoder**: As we mentioned, the encoder
    and decoder are nothing but a fully connected neural network that we can make
    as deep as we can by adding more layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Number of units per layer**: We can also use a different number of units
    in each layer. The shape of the encoder and decoder is very similar to DeconvNets,
    where the number of layers in the encoders decreases as we approach the code part
    and then starts to increase as we approach the final layer of the decoder.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model loss function**: We can use different loss functions as well, such
    as MSE or cross-entropy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After defining these hyperparameters and giving them initial values, we can
    train the network using a backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Compressing the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we'll build a simple autoencoder that can be used to compress
    the MNIST dataset. So we will feed the images of this dataset to the encoder part,
    which will try to learn a lower compressed representation for them; then we will
    try to construct the input images again in the decoder part.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start the implementation by getting the MNIST dataset, using the helper
    functions of TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the necessary packages for this implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start off by plotting some examples from the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9ca739a8-c53b-48ee-b8b3-18f455abca36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Example image from the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/25c86e3f-00cd-4eda-a9de-ed311292e709.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Example image from the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build the encoder, we need to figure out how many pixels each MNIST
    image will have so that we can figure out the size of the input layer of the encoder.
    Each image from the MNIST dataset is 28 by 28 pixels, so we will reshape this
    matrix to a vector of 28 x 28 = 784 pixel values. We don't have to normalize the
    images of MNIST because they are already normalized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start off building our three components of the model. In this implementation,
    we will use a very simple architecture of a single hidden layer followed by ReLU
    activation, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a887c40b-4430-4ce7-95c3-8ebf56ad2b69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Encoder-decoder architecture for MNIST implementation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and implement this simple encoder-decoder architecture according
    to the preceding explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we have defined our model and also used a binary cross-entropy since the
    images, pixels are already normalized.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll kick off the training process. We'll use the helper function
    of the `mnist_dataset` object in order to get a random batch from the dataset
    with a specific size; then we'll run the optimizer on this batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start this section by creating the session variable, which will be responsible
    for executing the computational graph that we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, let''s kick off the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After running the preceding code snippet for 20 epochs, we will get a trained
    model that is able to generate or reconstruct images from the test set of the
    MNIST data. Bear in mind that if we feed images that are not similar to the ones
    that the model was trained on, then the reconstruction process just won't work
    because autoencoders are data-specific.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test the trained model by feeding some images from the test set and
    see how the model is able to reconstruct them in the decoder part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7167052e-0b8f-4049-9738-4cca4e2c0e0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Examples of the original test images (first row) and their constructions
    (second row)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the reconstructed images are very close to the input ones, but
    we can probably get better images using convolution layers in the encoder-decoder
    part.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous simple implementation did a good job while trying to reconstruct
    input images from the MNIST dataset, but we can get a better performance through
    a convolution layer in the encoder and the decoder parts of the autoencoder. The
    resulting network of this replacement is called **convolutional autoencoder**
    (**CAE**). This flexibility of being able to replace layers is a great advantage
    of autoencoders and makes them applicable to different domains.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture that we'll be using for the CAE will contain upsampling layers
    in the decoder part of the network to get the reconstructed version of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this implementation, we can use any kind of imaging dataset and see how
    the convolutional version of the autoencoder will make a difference. We will still
    be using the MNIST dataset for this, so let''s start off by getting the dataset
    using the TensorFlow helpers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s show one digit from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2105032e-0770-41d3-af6a-85e6f825f43f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Example image from the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this implementation, we will be using convolution layers with stride 1, and
    the padding parameter is set to be the same. By this, we won't change the height
    or width of the image. Also, we are using a set of max pooling layers to reduce
    the width and height of the image and hence building a compressed lower representation
    of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s go ahead and build the core of our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now we are good to go. We've built the decoder-decoder part of the convolutional
    neural network while showing how the dimensions of the input image will be reconstructed
    in the decoder part.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the model built, we can kick off the learning process by generating
    random batches form the MNIST dataset and feed them to the optimizer defined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start off by creating the session variable; it will be responsible for
    executing the computational graph that we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code snippet for 20 epochs, we''ll get a trained
    CAE, so let''s go ahead and test this model by feeding similar images from the
    MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/38895773-80a4-4f5d-b572-2f728dac20dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Examples of the original test images (first row) and their constructions
    (second row) using the convolution autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can take the autoencoder architecture further by forcing it to learn more
    important features about the input data. By adding noise to the input images and
    having the original ones as the target, the model will try to remove this noise
    and learn important features about them in order to come up with meaningful reconstructed
    images in the output. This kind of CAE architecture can be used to remove noise
    from input images. This specific variation of autoencoders is called **denoising
    autoencoder**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5100d26b-63c5-4c69-93cd-4fb8df5ddcb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Examples of original images and the same images after adding a bit
    of Gaussian noise'
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s start off by implementing the architecture in the following figure.
    The only extra thing that we have added to this denoising autoencoder architecture
    is some noise in the original input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a04b580-1641-457a-8609-cf2392f35c28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: General denoising architecture of autoencoders'
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this implementation, we will be using more layers in the encoder and decoder
    part, and the reason for this is the new complexity that we have added to the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: The next model is exactly the same as the previous CAE but with extra layers
    that will help us to reconstruct a noise-free image from a noisy one.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s go ahead and build this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a more complex or deeper version of the convolutional model.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's time to start training this deeper network, which in turn will take more
    time to converge by reconstructing noise-free images from the noisy input.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s start off by creating the session variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we will kick off the training process but for more number of epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now we have trained the model to be able to produce noise-free images, which
    makes autoencoders applicable to many domains.
  prefs: []
  type: TYPE_NORMAL
- en: In the next snippet of code, we will not feed the row images of the MNIST test
    set to the model as we need to add noise to these images first to see how the
    trained model will be able to produce noise-free images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here I''m adding noise to the test images and passing them through the autoencoder.
    It does a surprisingly great job of removing the noise, even though it''s sometimes
    difficult to tell what the original number is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/14c7f119-bf36-4b37-aa0a-777b02b5703a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Examples of original test images with some Gaussian noise (top row)
    and their construction based on the trained denoising autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: Applications of autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example of constructing images from a lower representation,
    we saw it was very similar to the original input, and also we saw the benefits
    of CANs while denoising the noisy dataset. This kind of example we have implemented
    above is really useful for the image construction applications and dataset denoising.
    So you can generalize the above implementation to any other example of interest
    to you.
  prefs: []
  type: TYPE_NORMAL
- en: Also, throughout this chapter, we have seen how flexible the autoencoder architecture is and
    how we can make different changes to it. We have even tested it to solve harder
    problems of removing noise from input images. This kind of flexibility opens the
    door to many more applications that auoencoders will be a great fit for.
  prefs: []
  type: TYPE_NORMAL
- en: Image colorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoders—especially the convolutional version—can be used for harder tasks
    such as image colorization. In the following example, we feed the model with an
    input image without any colors, and the reconstructed version of this image will
    be colorized by the autoencoder model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d095b47-f875-4eed-89ec-378f9c56067d.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The CAE is trained to colorize the image'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e56cfc3-2c87-4af8-b844-081f5549e6ff.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Colorization paper architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Now that our autoencoder is trained, we can use it to colorize pictures we have
    never seen before!
  prefs: []
  type: TYPE_NORMAL
- en: This kind of application can be used to color very old images that were taken
    in the early days of the camera.
  prefs: []
  type: TYPE_NORMAL
- en: More applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another interesting application can be producing images with higher resolution,
    or neural image enhancement, like the following figures show.
  prefs: []
  type: TYPE_NORMAL
- en: 'These figures show more realistic versions of image colorization by Richard
    Zhang:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/737b6da4-0bd7-437c-8a6b-e79adbe19b63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Colorful image colorization by Richard Zhang, Phillip Isola, and
    Alexei A. Efros'
  prefs: []
  type: TYPE_NORMAL
- en: 'This figure shows another application of autoencoders to make image enhancements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1eae3e42-4da8-4cea-b961-30bbe5229382.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Neural enhancement by Alexjc (https://github.com/alexjc/neural-enhance)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a totally new architecture that can be used for
    many interesting applications. Autoencoders are very flexible, so feel free to
    come up with your own problem in the area of image enhancement, colorization,
    or construction. Also, there are more variations of autoencoders, called **variational
    autoencoders**. They are also used for very interesting applications, such as
    image generation.
  prefs: []
  type: TYPE_NORMAL
