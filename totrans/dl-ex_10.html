<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Recurrent-Type Neural Networks - Language Modeling</h1>
                </header>
            
            <article>
                
<p class="calibre2"><strong class="calibre13"> Recurrent neural networks</strong> (<strong class="calibre13">RNNs</strong>) are a class of deep learning architectures that are widely used for natural language processing. This set of architectures enables us to provide contextual information for current predictions and also have specific architecture that deals with long-term dependencies in any input sequence. In this chapter, we'll demonstrate how to make a sequence-to-sequence model, which will be useful in many applications in NLP. We will demonstrate these concepts by building a character-level language model and see how our model generates sentences similar to original input sequences.</p>
<p class="calibre2">The following topics will be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">The intuition behind RNNs</li>
<li class="calibre8">LSTM networks</li>
<li class="calibre8">Implementation of the language model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The intuition behind RNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">All the deep learning architectures that we have dealt with so far have no mechanism to memorize the input that they have received previously. For instance, if you feed a <strong class="calibre13">feed-forward neural network</strong> (<strong class="calibre13">FNN</strong>) with a sequence of characters such as <strong class="calibre13">HELLO</strong>, when the network gets to <strong class="calibre13">E</strong>, you will find that it didn't preserve any information/forgotten that it just read <strong class="calibre13">H</strong>. This is a serious problem for sequence-based learning. And since it has no memory of any previous characters it read, this kind of network will be very difficult to train to predict the next character. This doesn't make sense for lots of applications such as language modeling, machine translation, speech recognition, and so on.</p>
<p class="calibre2">For this specific reason, we are going to introduce RNNs, a set of deep learning architectures that do preserve information and memorize what they have just encountered.</p>
<p class="calibre2">Let's demonstrate how RNNs should work on the same input sequence of characters, <strong class="calibre13">HELLO</strong>. When the RNN cell/unit receives <strong class="calibre13">E</strong> as an input, it also receives that character <strong class="calibre13">H</strong>, which it received earlier. This feeding of the present character along with the past one as an input to the RNN cell gives a great advantage to these architectures, which is short-term memory; it also makes these architectures usable for predicting/guessing the most likely character after <strong class="calibre13">H</strong>, which is <strong class="calibre13">L</strong>, in this specific sequence of characters.</p>
<p class="calibre2">We have seen that previous architectures assign weights to their inputs; RNNs follow the same optimization process of assigning weights to their multiple inputs, which is the present and past. So in this case, the network will assign two different matrices of weights to each one of them. In order to do that, we will be using <strong class="calibre13">gradient descent</strong> and a heavier version of backpropagation, which is called <strong class="calibre13">backpropagation through time</strong> (<strong class="calibre13">BPTT</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks architectures</h1>
                </header>
            
            <article>
                
<p class="calibre2">Depending on our background of using previous deep learning architectures, you will find out why RNNs are special. The previous architectures that we have learned about are not flexible in terms of their input or training. They accept a fixed-size sequence/vector/image as an input and produce another fixed-size one as an output. RNN architectures are somehow different, because they enable you to feed a sequence as input and get another sequence as output, or to have sequences in the input only/output only as shown in <em class="calibre19">Figure 1</em>. This kind of flexibility is very useful for multiple applications such as language modeling and sentiment analysis:</p>
<div class="CDPAlignCenter"><img src="assets/567e14af-e7cf-4439-bdf0-7a72a595a1f9.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 1: Flexibility of RNNs in terms of shape of input or output (http://karpathy.github.io/2015/05/21/rnn-effectiveness/)</div>
<p class="calibre2">The intuition behind these set of architectures is to mimic the way humans process information. In any typical conversation your understanding of someone's words is totally dependent on what he said previously and you might even be able to predict what he's going to say next based on what he just said. </p>
<p class="calibre2">The exact same process should be followed in the case of RNNs. For example, imagine you want translate a specific word in a sentence. You can't use traditional FNNs for that, because they won't be able to use the translation of previous words as an input with the current word that we want to translate, and this may result in an incorrect translation because of the lack of contextual information around this word.</p>
<p class="calibre2">RNNs do preserves information about the past and they have some kind of loops to allow the previously learned information to be used for the current prediction at any given point:</p>
<div class="CDPAlignCenter"><img src="assets/5975b720-dd6b-44f2-b227-732b2aa481d8.png" class="calibre125"/></div>
<div class="CDPAlignCenter1">Figure 2: RNNs architecture which has loop to persist information for past steps (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</div>
<p class="calibre2">In <em class="calibre19">Figure 2</em>, we have some neural networks called <em class="calibre19">A</em> which receives an input <em class="calibre19">X<sub class="calibre28">t</sub></em> and produces and output <em class="calibre19">h<sub class="calibre28">t</sub></em>. Also, it receives information from past steps with the help of this loop.</p>
<p class="calibre2">This loop seems to unclear, but if we used the unrolled version of <em class="calibre19">Figure 2</em>, you will find out that it's very simple and intuitive, and that the RNN is nothing but a repeated version of the same network (which could be normal FNN), as shown in <em class="calibre19">Figure 3</em>:</p>
<div class="CDPAlignCenter"><img src="assets/8fdae554-aebd-49fb-a8ef-24071579e7fe.png" class="calibre126"/></div>
<div class="CDPAlignCenter1">Figure 3: An unrolled version of the recurrent neural network architecture (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</div>
<div class="title-page-name">
<p class="calibre2">This intuitive architecture of RNNs and its flexibility in terms of input/output shape make them a good fit for interesting sequence-based learning tasks such as machine translation, language modeling, sentiment analysis, image captioning, and more.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examples of RNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, we have an intuitive understanding of how RNNs work and how it's going to be useful in different interesting sequence-based examples. Let's have a closer look of some of these interesting examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Character-level language models</h1>
                </header>
            
            <article>
                
<p class="calibre2">Language modeling is an essential task for many applications such as speech recognition, machine translation and more. In this section, we'll try to mimic the training process of RNNs and get a deeper understanding of how these networks work. We'll build a language model that operate over characters. So, we will feed our network with a chunk of text with the purpose of trying to build a probability distribution of the next character given the previous ones which will allow us to generate text similar to the one we feed as an input in the training process.</p>
<p class="calibre2">For example, suppose we have a language with only four letters as its vocabulary, <strong class="calibre13">helo</strong>.</p>
<p class="calibre2">The task is to train a recurrent neural network on a specific input sequence of characters such as <strong class="calibre13">hello</strong>. In this specific example, we have four training samples:</p>
<ol class="calibre16">
<li class="calibre8">The probability of the character <strong class="calibre1">e</strong> should be calculated given the context of the first input character <strong class="calibre1">h</strong>,</li>
<li class="calibre8">The probability of the character <strong class="calibre1">l</strong> should be calculated given the context of <strong class="calibre1">he</strong>,</li>
<li class="calibre8">The probability of the character <strong class="calibre1">l</strong> should be calculated given the context of <strong class="calibre1">hel</strong>, and</li>
<li class="calibre8">Finally the probability of the character <strong class="calibre1">o</strong> should be calculated given the context of <strong class="calibre1">hell</strong></li>
</ol>
<p class="calibre2">As we learned in previous chapters, machine learning techniques in general which deep learning is a part of, only accept real-value numbers as input. So, wee need somehow convert or encode or input character to a numerical form. To do this, we will use one-hot-vector encoding which is a way to encode text by have a vector of zeros except for a single entry in the vector, which is the index of the character in the vocabulary of this language that we are trying to model (in this case <strong class="calibre13">helo</strong>). After encoding our training samples, we will provide them to the RNN-type model one at a time. At each given character, the output of the RNN-type model will be a 4-dimensional vector (the size of the vector corresponds to the size of the vocab) which represents the probability of each character in the vocabulary being the next one after the given input character. <em class="calibre19">Figure 4</em> clarifies this process:</p>
<div class="CDPAlignCenter"><img src="assets/ed9df466-611d-4550-aa61-c2ac49e16102.jpeg" class="calibre127"/></div>
<div class="CDPAlignCenter1">Figure 4: Example of RNN-type network with one-hot-vector encoded characters as an input and the output will be distribution over the vocab representing the most likely character after the current one (source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/)</div>
<p class="calibre2">As shown in <em class="calibre19">Figure 4</em>, you can see that we fed the first character in our input sequence <strong class="calibre13">h</strong> to the model and the output was 4-dimensional vector representing the confidence about the next character. So it has a confidence of <strong class="calibre13">1.0</strong> of <strong class="calibre13">h</strong> being the next character after the input <strong class="calibre13">h</strong>, a confidence of <strong class="calibre13">2.2</strong> of <strong class="calibre13">e</strong> being the next character, a confidence of <strong class="calibre13">-3.0</strong> to <strong class="calibre13">l</strong> being the next character, <span class="calibre10">and finally a confidence of <strong class="calibre13">4.1</strong> to <strong class="calibre13">o</strong> being the next character. In this specific example, we know the correct next character will be <strong class="calibre13">e</strong>,<strong class="calibre13"> </strong>based on our training sequence <strong class="calibre13">hello</strong>. So our primary goal while training this RNN-type network is increase the confidence of <strong class="calibre13">e</strong> being the next character and decrease the confidence of other characters. To do this kind of optimization we will be using gradient descent and backpropagation algorithms to update the weights and influence the network to produce a higher confidence for our correct next character, <strong class="calibre13">e</strong>, and so on, for the other 3 training examples.</span></p>
<p class="calibre2">As you can see the output of the RNN-type network produces a confidence distribution over all the characters of the vocab being the next one. We can turn this confidence distribution into a probability distribution such that the increase of one characters probability being the next one will result in decreasing the others probabilities because the probability needs to sum up to 1. For this specific modification we can use a standard softmax layer to every output vector.</p>
<p class="calibre2">For generating text from these kind of networks, we can feed an initial character to the model and get a probability distribution over the characters that are likely to be next, and then we can sample from these characters and feed it back as an input to the model. We'll be able to get a sequence of characters by repeating this process over and over again as many times as we want to generate a text with a desired length.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Language model using Shakespeare data</h1>
                </header>
            
            <article>
                
<p class="calibre2">From the preceding example, we can get the model to generate text. But the network will surprise us, as it's not only going to generate text but also it's going to learn the style and the structure in training data. We can demonstrate this interesting process by training an RNN-type model on specific kind of text that has structure and style in it, such as the following Shakespeare work. </p>
<p class="calibre2">Let's have a look at a generated output from the trained network:</p>
<div class="packtquote">Second Senator:<br class="title-page-name"/>
They are away this miseries, produced upon my soul,<br class="title-page-name"/>
Breaking and strongly should be buried, when I perish<br class="title-page-name"/>
The earth and thoughts of many states.</div>
<p class="calibre2">In spite of the fact that the network only knows how to produce one single character at a time, it was able to generate a meaningful text and names that actually have the structure and style of Shakespeare work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The vanishing gradient problem</h1>
                </header>
            
            <article>
                
<p class="calibre2">While training these sets of RNN-type architectures, we use gradient descent and backprogagation through time, which introduced some successes for lots of sequence-based learning tasks. But because of the nature of the gradient and due to using fast training strategies, it could be shown that the gradient values will tend to be too small and vanish. This process introduced the vanishing gradient problem that many practitioners fall into. Later on in this chapter, we will discuss how researchers approached these kind of problems and produced variations of the vanilla RNNs to overcome this problem:</p>
<div class="CDPAlignCenter"><img src="assets/79e4dcaf-8767-4631-94d6-2f24a58024d1.png" class="calibre128"/></div>
<div class="CDPAlignCenter1">Figure 5: Vanishing gradient problem</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The problem of long-term dependencies</h1>
                </header>
            
            <article>
                
<p class="calibre2">Another challenging problem faced by researchers is the long-term dependencies that one can find in text. For example, if someone feeds a sequence like <em class="calibre19">I used to live in France and I learned how to speak...</em> the next obvious word in the sequence is the word French.</p>
<p class="calibre2">In these kind of situation vanilla RNNs will be able to handle it because it has short-term dependencies, as shown in <em class="calibre19">Figure 6</em>:</p>
<div class="CDPAlignCenter"><img src="assets/568df409-a768-4696-ae1a-d6e371d6cb6e.png" class="calibre129"/></div>
<div class="CDPAlignCenter1">Figure 6: Showing short-term dependencies in the text (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</div>
<div class="title-page-name">
<p class="calibre2"><span class="calibre10">Another example, if someone started the sequence by saying that <em class="calibre19">I used to live in France..</em> and then he/she start to describe the beauty of living there and finally he ended the sequence by <em class="calibre19">I learned to speak French</em>. So, for the model to predict the language that he/she learned at the end of the sequence, the model needs to have some information about the early words <em class="calibre19">live</em> and <em class="calibre19">France</em>. The model won't be able to handle these kind of situation, if it doesn't manage to keep track of long term dependencies in the text:</span></p>
</div>
<div class="CDPAlignCenter"><img src="assets/12e35c64-5d9d-4ecc-b969-4e627147b77d.png" class="calibre130"/></div>
<div class="title-page-name">
<div class="CDPAlignCenter1">Figure 7: The challenge of long-term dependencies in text (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</div>
<p class="calibre2">To handle vanishing gradients and long-term dependencies in the text, researchers introduced a variation of the vanilla RNN network called <strong class="calibre13">Long Short Term Networks </strong>(<strong class="calibre13">LSTM</strong>).</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">LSTM, a variation of an RNN that is used to help learning long term dependencies in the text. LSTMs were initially introduced by Hochreiter &amp; Schmidhuber (1997) (link: <a href="http://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" class="calibre11">http://www.bioinf.jku.at/publications/older/2604.pdf</a>), and many researchers worked on it and produced interesting results in many domains.</p>
<p class="calibre2">These kind of architectures will be able to handle the problem of long-term dependencies in the text because of its inner architecture.</p>
<p class="calibre2">LSTMs are similar to the vanilla RNN as it has a repeating module over time, but the inner architecture of this repeated module is different from the vanilla RNNs. It includes more layers for forgetting and updating information:</p>
<div class="CDPAlignCenter"><img src="assets/9ae7087a-fced-4f5c-86c8-84855ad8301a.png" class="calibre131"/></div>
<div class="CDPAlignCenter1">Figure 8: The repeating module in a standard RNN containing a single layer (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</div>
<p class="calibre2">As mentioned previously, the vanilla RNNs have a single NN layer, but the LSTMs have four different layers interacting in a special way. This special kind of interaction is what makes LSTM, work very well for many domains, which we'll see while building our language model example:</p>
<div class="CDPAlignCenter"><img src="assets/a385710b-f1db-4c91-af43-244a22b863b8.png" class="calibre132"/></div>
<div class="CDPAlignCenter1">Figure 9: The repeating module in an LSTM containing four interacting layers (source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</div>
<p class="calibre2">For more details about the mathematical details and how the four layers are actually interacting with each other, you can have a look at this interesting tutorial: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" class="calibre11">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why does LSTM work?</h1>
                </header>
            
            <article>
                
<p class="calibre2">The first step in our vanilla LSTM architecture it to decide which information is not necessary and it will work by throwing it away to leave more room for more important information. For this, we have a layer called <strong class="calibre13">forget gate layer</strong>, which looks at the previous output <em class="calibre19">h<sub class="calibre28">t-1</sub></em> and the current input <em class="calibre19">x<sub class="calibre28">t</sub></em> and decides which information we are going to throw away.</p>
<p class="calibre2">The next step in the LSTM architecture is to decide which information is worth keeping/persisting and storing in the cell. This is done in two steps:</p>
<ol class="calibre16">
<li class="calibre8">A layer called <strong class="calibre1">input gate layer</strong>, which decides which values of the previous state of the cell needs to be updated</li>
<li class="calibre8">The second step is to generate a set of new candidate values that will be added to the cell</li>
</ol>
<p class="calibre2">Finally, we need to decide what the LSTM cell is going to output. This output will be based on our cell state, but will be a filtered version.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation of the language model</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we'll build a language model that operates over characters. For this implementation, we will use an Anna Karenina novel and see how the network will learn to implement the structure and style of the text:</p>
<div class="CDPAlignCenter"><img src="assets/29c6dfef-8eee-49f1-a36c-b6826af3c065.jpeg" class="calibre133"/></div>
<div class="CDPAlignCenter1">Figure 10: General architecture for the character-level RNN (source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/)</div>
<div class="packtinfobox">
<p class="calibre9">This network is based off of Andrej Karpathy's post on RNNs (link: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" class="calibre11">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>) and implementation in Torch (link: <a href="https://github.com/karpathy/char-rnn" target="_blank" class="calibre11">https://github.com/karpathy/char-rnn</a>). Also, there's some information here at r2rt (link: <a href="http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html" target="_blank" class="calibre11">http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html</a>) and from Sherjil Ozairp (link: <a href="https://github.com/sherjilozair/char-rnn-tensorflow" target="_blank" class="calibre11">https://github.com/sherjilozair/char-rnn-tensorflow</a>) on GitHub. The following is the general architecture of the character-wise RNN.</p>
</div>
<div class="title-page-name">
<p class="calibre2">We'll build a character-level RNN trained on the Anna Karenina novel (link: <a href="https://en.wikipedia.org/wiki/Anna_Karenina" target="_blank" class="calibre11">https://en.wikipedia.org/wiki/Anna_Karenina</a>)<a href="https://en.wikipedia.org/wiki/Anna_Karenina" target="_blank" class="calibre11">.</a> It'll be able to generate new text based on the text from the book. You will find the <kbd class="calibre12">.txt</kbd> file included with the assets of this implementation.</p>
<p class="calibre2">Let’s start by importing the necessary libraries for this character-level implementation:</p>
<pre class="calibre21">import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/><br class="title-page-name"/>from collections import namedtuple</pre>
<p class="calibre2">To start off, we need to prepare the dataset by loading it and converting it in to integers. So, we will convert the characters into integers and then encode them as integers which makes it straightforward and easy to use as input variables for the model:</p>
</div>
<pre class="calibre21">#reading the Anna Karenina novel text file<br class="title-page-name"/>with open('Anna_Karenina.txt', 'r') as f:<br class="title-page-name"/>    textlines=f.read()<br class="title-page-name"/><br class="title-page-name"/>#Building the vocan and encoding the characters as integers<br class="title-page-name"/>language_vocab = set(textlines)<br class="title-page-name"/>vocab_to_integer = {char: j for j, char in enumerate(language_vocab)}<br class="title-page-name"/>integer_to_vocab = dict(enumerate(language_vocab))<br class="title-page-name"/>encoded_vocab = np.array([vocab_to_integer[char] for char in textlines], dtype=np.int32)</pre>
<p class="calibre2">So, let's have look at the first 200 characters from the Anna Karenina text:</p>
<pre class="calibre21">textlines[:200]<br class="title-page-name"/>Output:<br class="title-page-name"/>"Chapter 1\n\n\nHappy families are all alike; every unhappy family is unhappy in its own\nway.\n\nEverything was in confusion in the Oblonskys' house. The wife had\ndiscovered that the husband was carrying on"</pre>
<p class="calibre2">We have also converted the characters to a convenient form for the network, which is integers. So, let's have a look at the encoded version of the characters:</p>
<pre class="calibre21">encoded_vocab[:200]<br class="title-page-name"/>Output:<br class="title-page-name"/>array([70, 34, 54, 29, 24, 19, 76, 45, 2, 79, 79, 79, 69, 54, 29, 29, 49,<br class="title-page-name"/>       45, 66, 54, 39, 15, 44, 15, 19, 12, 45, 54, 76, 19, 45, 54, 44, 44,<br class="title-page-name"/>      45, 54, 44, 15, 27, 19, 58, 45, 19, 30, 19, 76, 49, 45, 59, 56, 34,<br class="title-page-name"/>       54, 29, 29, 49, 45, 66, 54, 39, 15, 44, 49, 45, 15, 12, 45, 59, 56,<br class="title-page-name"/>       34, 54, 29, 29, 49, 45, 15, 56, 45, 15, 24, 12, 45, 11, 35, 56, 79,<br class="title-page-name"/>       35, 54, 49, 53, 79, 79, 36, 30, 19, 76, 49, 24, 34, 15, 56, 16, 45,<br class="title-page-name"/>       35, 54, 12, 45, 15, 56, 45, 31, 11, 56, 66, 59, 12, 15, 11, 56, 45,<br class="title-page-name"/>       15, 56, 45, 24, 34, 19, 45, 1, 82, 44, 11, 56, 12, 27, 49, 12, 37,<br class="title-page-name"/>       45, 34, 11, 59, 12, 19, 53, 45, 21, 34, 19, 45, 35, 15, 66, 19, 45,<br class="title-page-name"/>       34, 54, 64, 79, 64, 15, 12, 31, 11, 30, 19, 76, 19, 64, 45, 24, 34,<br class="title-page-name"/>       54, 24, 45, 24, 34, 19, 45, 34, 59, 12, 82, 54, 56, 64, 45, 35, 54,<br class="title-page-name"/>       12, 45, 31, 54, 76, 76, 49, 15, 56, 16, 45, 11, 56], dtype=int32)</pre>
<p class="calibre2">Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text. Here's how many classes our network has to pick from.</p>
<p class="calibre2">So, we will be feeding the model a character at a time, and the model will predict the next character by producing a probability distribution over the possible number of characters that could come next (vocab), which is equivalent to a number of classes the network needs to pick from:</p>
<pre class="calibre21">len(language_vocab)<br class="title-page-name"/>Output:<br class="title-page-name"/>83</pre>
<p class="calibre2">Since we'll be using stochastic gradient descent to train our model, we need to convert our data into training batches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mini-batch generation for training</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we will divide our data into small batches to be used for training. So, the batches will consist of many sequences of desired number of sequence steps. So, let's look at a visual example in <em class="calibre19">Figure 11</em>:</p>
<div class="CDPAlignCenter"><img src="assets/77f38b3c-ced1-4784-af60-897d92f968ad.png" class="calibre134"/></div>
<div class="CDPAlignCenter1">Figure 11: Illustration of how batches and sequences would look like (source: http://oscarmore2.github.io/Anna_KaRNNa_files/charseq.jpeg)</div>
<p class="calibre2">So, now we need to define a function that will iterate through the encoded text and generate the batches. In this function we will be using a very nice mechanism of Python called <strong class="calibre13">yield</strong> (link: <a href="https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/" target="_blank" class="calibre11">https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/</a>).</p>
<p class="calibre2">A typical batch will have <em class="calibre19">N × M</em> characters, where <em class="calibre19">N</em> is the number of sequences and <em class="calibre19">M</em> is, number of sequence steps. For getting the number of possible batches in our dataset, we can simply divide the length of the data by the desired batch size and after getting this number of possible batches, we can drive how many characters should be in each batch.</p>
<p class="calibre2">After that, we need to split the dataset we have into a desired number of sequences (<em class="calibre19">N</em>). We can use <kbd class="calibre12">arr.reshape(size)</kbd>. We know we want <em class="calibre19">N</em> sequences (<kbd class="calibre12">num_seqs</kbd> is used in, following code), let's make that the size of the first dimension. For the second dimension, you can use -1 as a placeholder in the size; it'll fill up the array with the appropriate data for you. After this, you should have an array that is <em class="calibre19">N × (M * K)</em>, where <em class="calibre19">K</em> is the number of batches.</p>
<p class="calibre2">Now that we have this array, we can iterate through it to get the training batches, where each batch has <em class="calibre19">N × M</em> characters. For each subsequent batch, the window moves over by <kbd class="calibre12">num_steps</kbd>. Finally, we also want to create both the input and output arrays for ours to be used as the model input. This step of creating the output values is very easy; remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation22" src="assets/2b300aec-cfbe-4d7f-a835-c7ef2c9944a3.png"/></div>
<p class="calibre2">Where <em class="calibre19">x</em> is the input batch and <em class="calibre19">y</em> is the target batch.</p>
<p class="calibre2">The way I like to do this window is to use range to take steps of size <kbd class="calibre12">num_steps</kbd>, starting from 0 to <kbd class="calibre12">arr.shape[1]</kbd>, the total number of steps in each sequence. That way, the integers you get from the range always point to the start of a batch, and each window is <kbd class="calibre12">num_steps</kbd> wide:</p>
<pre class="calibre21">def generate_character_batches(data, num_seq, num_steps):<br class="title-page-name"/>    '''Create a function that returns batches of size<br class="title-page-name"/>       num_seq x num_steps from data.<br class="title-page-name"/>    '''<br class="title-page-name"/>    # Get the number of characters per batch and number of batches<br class="title-page-name"/>    num_char_per_batch = num_seq * num_steps<br class="title-page-name"/>    num_batches = len(data)//num_char_per_batch<br class="title-page-name"/>    <br class="title-page-name"/>    # Keep only enough characters to make full batches<br class="title-page-name"/>    data = data[:num_batches * num_char_per_batch]<br class="title-page-name"/>    <br class="title-page-name"/>    # Reshape the array into n_seqs rows<br class="title-page-name"/>    data = data.reshape((num_seq, -1))<br class="title-page-name"/>    <br class="title-page-name"/>    for i in range(0, data.shape[1], num_steps):<br class="title-page-name"/>        # The input variables<br class="title-page-name"/>        input_x = data[:, i:i+num_steps]<br class="title-page-name"/>        <br class="title-page-name"/>        # The output variables which are shifted by one<br class="title-page-name"/>        output_y = np.zeros_like(input_x)<br class="title-page-name"/>        <br class="title-page-name"/>        output_y[:, :-1], output_y[:, -1] = input_x[:, 1:], input_x[:, 0]<br class="title-page-name"/>        yield input_x, output_y</pre>
<p class="calibre2">So, let's demonstrate this using this function by generating a batch of 15 sequences and 50 sequence steps:</p>
<pre class="calibre21">generated_batches = generate_character_batches(encoded_vocab, 15, 50)<br class="title-page-name"/>input_x, output_y = next(generated_batches)<br class="title-page-name"/>print('input\n', input_x[:10, :10])<br class="title-page-name"/>print('\ntarget\n', output_y[:10, :10])<br class="title-page-name"/>Output:<br class="title-page-name"/><br class="title-page-name"/>input<br class="title-page-name"/> [[70 34 54 29 24 19 76 45 2 79]<br class="title-page-name"/> [45 19 44 15 16 15 82 44 19 45]<br class="title-page-name"/> [11 45 44 15 16 34 24 38 34 19]<br class="title-page-name"/> [45 34 54 64 45 82 19 19 56 45]<br class="title-page-name"/> [45 11 56 19 45 27 56 19 35 79]<br class="title-page-name"/> [49 19 54 76 12 45 44 54 12 24]<br class="title-page-name"/> [45 41 19 45 16 11 45 15 56 24]<br class="title-page-name"/> [11 35 45 24 11 45 39 54 27 19]<br class="title-page-name"/> [82 19 66 11 76 19 45 81 19 56]<br class="title-page-name"/> [12 54 16 19 45 44 15 27 19 45]]<br class="title-page-name"/><br class="title-page-name"/>target<br class="title-page-name"/> [[34 54 29 24 19 76 45 2 79 79]<br class="title-page-name"/> [19 44 15 16 15 82 44 19 45 16]<br class="title-page-name"/> [45 44 15 16 34 24 38 34 19 54]<br class="title-page-name"/> [34 54 64 45 82 19 19 56 45 82]<br class="title-page-name"/> [11 56 19 45 27 56 19 35 79 35]<br class="title-page-name"/> [19 54 76 12 45 44 54 12 24 45]<br class="title-page-name"/> [41 19 45 16 11 45 15 56 24 11]<br class="title-page-name"/> [35 45 24 11 45 39 54 27 19 33]<br class="title-page-name"/> [19 66 11 76 19 45 81 19 56 24]<br class="title-page-name"/> [54 16 19 45 44 15 27 19 45 24]]</pre>
<p class="calibre2">Next up, we'll be looking forward to building the core of this example, which is the LSTM model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Before diving into building the character-level model using LSTMs, it is worth mentioning something called <strong class="calibre13">Stacked LSTM</strong>.</p>
<p class="calibre2">Stacked LSTMs are useful for looking at your information at different time scales.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacked LSTMs</h1>
                </header>
            
            <article>
                
<div class="packtquote">"Building a deep RNN by stacking multiple recurrent hidden states on top of each other. This approach potentially allows the hidden state at each level to operate at different timescale" How to Construct Deep Recurrent Neural Networks (link: https://arxiv.org/abs/1312.6026), 2013</div>
<div class="packtquote">"RNNs are inherently deep in time, since their hidden state is a function of all previous hidden states. The question that inspired this paper was whether RNNs could also benefit from depth in space; that is from stacking multiple recurrent hidden layers on top of each other, just as feedforward layers are stacked in conventional deep networks". Speech Recognition With Deep RNNs (link: <a href="https://arxiv.org/abs/1303.5778" target="_blank" class="calibre11">https://arxiv.org/abs/1303.5778</a>), 2013</div>
<p class="calibre2">Most researchers are using stacked LSTMs for challenging sequence prediction problems. A stacked LSTM architecture can be defined as an LSTM model comprised of multiple LSTM layers. The preceding LSTM layer provides a sequence output rather than a single-value output to the LSTM layer as follows.</p>
<p class="calibre2">Specifically, it's one output per input time step, rather than one output time step for all input time steps:</p>
<div class="CDPAlignCenter"><img src="assets/fcf09a5d-b4e6-4c54-826b-58aed18b75a7.png" class="calibre135"/></div>
<div class="CDPAlignCenter1">Figure 12: Stacked LSTMs</div>
<p class="calibre2">So in this example, we will be using this kind of stacked LSTM architecture, which gives better performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model architecture</h1>
                </header>
            
            <article>
                
<p class="calibre2">This is where you'll build the network. We'll break it into parts so that it's easier to reason about each bit. Then, we can connect them with the whole network:</p>
<div class="CDPAlignCenter"><img src="assets/9b4a2032-77d5-4c59-a4de-0a4cf19bccfd.png" class="calibre136"/></div>
<div class="CDPAlignCenter1">Figure 13: Character-level model architecture</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inputs</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, let's start by defining the model inputs as placeholders. The inputs of the model will be training data and the targets. We will also use a parameter called <kbd class="calibre12">keep_probability</kbd> for the dropout layer, which helps the model avoid overfitting:</p>
<pre class="calibre21">def build_model_inputs(batch_size, num_steps):<br class="title-page-name"/>    <br class="title-page-name"/>    # Declare placeholders for the input and output variables<br class="title-page-name"/>    inputs_x = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')<br class="title-page-name"/>    targets_y = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')<br class="title-page-name"/>    <br class="title-page-name"/>    # define the keep_probability for the dropout layer<br class="title-page-name"/>    keep_probability = tf.placeholder(tf.float32, name='keep_prob')<br class="title-page-name"/>    <br class="title-page-name"/>    return inputs_x, targets_y, keep_probability</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an LSTM cell</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we will write a function for creating the LSTM cell, which will be used in the hidden layer. This cell will be the building block for our model. So, we will create this cell using TensorFlow. Let's have a look at how we can use TensorFlow to build a basic LSTM cell.</p>
<p class="calibre2">We call the following line of code to create an LSTM cell with the parameter <kbd class="calibre12">num_units</kbd> representing the number of units in the hidden layer:</p>
<pre class="calibre21">lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units)</pre>
<p class="calibre2">To prevent overfitting, we can use something called <strong class="calibre13">dropout</strong>, which is a mechanism for preventing the model from overfitting the data by decreasing the model's complexity:</p>
<pre class="calibre21">tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_probability)</pre>
<p class="calibre2">As we mentioned before, we will be using the stacked LSTM architecture; it will help us to look at the data from different angles and has been <span class="calibre10">practically </span>found to perform better. In order to define a stacked LSTM in TensorFlow, we can use the <kbd class="calibre12">tf.contrib.rnn.MultiRNNCell</kbd> function (link: <a href="https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell" target="_blank" class="calibre11">https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell</a>):</p>
<pre class="calibre21">tf.contrib.rnn.MultiRNNCell([cell]*num_layers)</pre>
<p class="calibre2">Initially for the first cell, there will be no previous information, so we need to initialize the cell state to be zeros. We can use the following function to do that:</p>
<pre class="calibre21">initial_state = cell.zero_state(batch_size, tf.float32)</pre>
<p class="calibre2">So, let's put it all together and create our LSTM cell:</p>
<pre class="calibre21">def build_lstm_cell(size, num_layers, batch_size, keep_probability):<br class="title-page-name"/><br class="title-page-name"/>    ### Building the LSTM Cell using the tensorflow function<br class="title-page-name"/>    lstm_cell = tf.contrib.rnn.BasicLSTMCell(size)<br class="title-page-name"/>    <br class="title-page-name"/>    # Adding dropout to the layer to prevent overfitting<br class="title-page-name"/>    drop_layer = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_probability)<br class="title-page-name"/>    <br class="title-page-name"/>    # Add muliple cells together and stack them up to oprovide a level of more understanding<br class="title-page-name"/>    stakced_cell = tf.contrib.rnn.MultiRNNCell([drop_layer] * num_layers)<br class="title-page-name"/>    initial_cell_state = lstm_cell.zero_state(batch_size, tf.float32)<br class="title-page-name"/>    <br class="title-page-name"/>    return lstm_cell, initial_cell_state</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RNN output</h1>
                </header>
            
            <article>
                
<p class="calibre2">Next up, we need to create the output layer, which is responsible for reading the output of the individual LSTM cells and passing them through a fully connected layer. This layer has a softmax output for producing a probability distribution over the likely character to be next after the input one.</p>
<p class="calibre2">As you know, we have generated input batches for the network with size <em class="calibre19">N × M</em> characters, where <em class="calibre19">N</em> is the number of sequences in this batch and <em class="calibre19">M</em> is the number of sequence steps. We have also used <em class="calibre19">L</em> hidden units in the hidden layer while creating the model. Based on the batch size and number of hidden units, the output of the network will be a <em class="calibre19">3D</em> Tensor with size <em class="calibre19">N × M × L</em>, and that's because we call the LSTM cell <em class="calibre19">M</em> times, one for each sequence step. Each call to LSTM cell produces an output of size <em class="calibre19">L</em>. Finally, we need to do this as many as number of sequences <em class="calibre19">N</em> as the we have.</p>
<p class="calibre2">So we pass this <em class="calibre19">N<span class="calibre10"> </span>× M<span class="calibre10"> </span>× L</em> output to a fully connected layer (which is the same for all outputs with the same weights), but before doing this, we reshape the output to a <em class="calibre19">2D</em> tensor, which has a shape of <em class="calibre19">(M * N) × L</em>. This reshaping will make things easier for us when operating on the output, because the new shape will be more convenient; the values of each row represents the <em class="calibre19">L</em> outputs of the LSTM cell, and hence it's one row for each sequence and step.</p>
<p class="calibre2">After getting the new shape, we can connect it to the fully connected layer with the softmax by doing matrix multiplication with the weights. The weights created in the LSTM cells and the weight that we are about to create here have the same name by default, and TensorFlow will raise an error in such a case. To avoid this error, we can wrap the weight and bias variables created here in a variable scope using the TensorFlow function <kbd class="calibre12">tf.variable_scope()</kbd>.</p>
<p class="calibre2">After explaining the shape of the output and how we are going to reshape it, to make things easier, let's go ahead and code this <kbd class="calibre12">build_model_output</kbd> function:</p>
<pre class="calibre21">def build_model_output(output, input_size, output_size):<br class="title-page-name"/><br class="title-page-name"/>    # Reshaping output of the model to become a bunch of rows, where each row correspond for each step in the seq<br class="title-page-name"/>    sequence_output = tf.concat(output, axis=1)<br class="title-page-name"/>    reshaped_output = tf.reshape(sequence_output, [-1, input_size])<br class="title-page-name"/>    <br class="title-page-name"/>    # Connect the RNN outputs to a softmax layer<br class="title-page-name"/>    with tf.variable_scope('softmax'):<br class="title-page-name"/>        softmax_w = tf.Variable(tf.truncated_normal((input_size, output_size), stddev=0.1))<br class="title-page-name"/>        softmax_b = tf.Variable(tf.zeros(output_size))<br class="title-page-name"/>    <br class="title-page-name"/>    # the output is a set of rows of LSTM cell outputs, so the logits will be a set<br class="title-page-name"/>    # of rows of logit outputs, one for each step and sequence<br class="title-page-name"/>    logits = tf.matmul(reshaped_output, softmax_w) + softmax_b<br class="title-page-name"/>    <br class="title-page-name"/>    # Use softmax to get the probabilities for predicted characters<br class="title-page-name"/>    model_out = tf.nn.softmax(logits, name='predictions')<br class="title-page-name"/>    <br class="title-page-name"/>    return model_out, logits</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training loss</h1>
                </header>
            
            <article>
                
<p class="calibre2">Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First, we need to one-hot encode the targets; we're getting them as encoded characters. Then, we reshape the one-hot targets, so it's a <em class="calibre19">2D</em> tensor with size <em class="calibre19">(M * N) × C</em>, where <em class="calibre19">C</em> is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with <em class="calibre19">C</em> units. So, our logits will also have size <em class="calibre19">(M * N) ×<span class="calibre10"> </span>C</em>.</p>
<p class="calibre2">Then, we run the <kbd class="calibre12">logits</kbd> and <kbd class="calibre12">targets</kbd> through <kbd class="calibre12">tf.nn.softmax_cross_entropy_with_logits</kbd> and find the mean to get the loss:</p>
<pre class="calibre21">def model_loss(logits, targets, lstm_size, num_classes):<br class="title-page-name"/>    <br class="title-page-name"/>    # convert the targets to one-hot encoded and reshape them to match the logits, one row per batch_size per step<br class="title-page-name"/>    output_y_one_hot = tf.one_hot(targets, num_classes)<br class="title-page-name"/>    output_y_reshaped = tf.reshape(output_y_one_hot, logits.get_shape())<br class="title-page-name"/>    <br class="title-page-name"/>    #Use the cross entropy loss<br class="title-page-name"/>    model_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=output_y_reshaped)<br class="title-page-name"/>    model_loss = tf.reduce_mean(model_loss)<br class="title-page-name"/>    return model_loss</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizer</h1>
                </header>
            
            <article>
                
<p class="calibre2">Finally, we need to use an optimization method that will help us learn something from the dataset. As we know, vanilla RNNs have exploding and vanishing gradient issues. LSTMs fix only one issue, which is the vanishing of the gradient values, but even after using LSTM, some gradient values explode and grow without bounds. In order to fix this problem, we can use something called <strong class="calibre13">gradient clipping</strong>, which is a technique to clip the gradients that explode to a specific threshold.</p>
<p class="calibre2">So, let's define our optimizer by using the Adam optimizer for the learning process:</p>
<pre class="calibre21">def build_model_optimizer(model_loss, learning_rate, grad_clip):<br class="title-page-name"/> <br class="title-page-name"/>    # define optimizer for training, using gradient clipping to avoid the exploding of the gradients<br class="title-page-name"/>    trainable_variables = tf.trainable_variables()<br class="title-page-name"/>    gradients, _ = tf.clip_by_global_norm(tf.gradients(model_loss, trainable_variables), grad_clip)<br class="title-page-name"/>    <br class="title-page-name"/>    #Use Adam Optimizer<br class="title-page-name"/>    train_operation = tf.train.AdamOptimizer(learning_rate)<br class="title-page-name"/>    model_optimizer = train_operation.apply_gradients(zip(gradients, trainable_variables))<br class="title-page-name"/>    <br class="title-page-name"/>    return model_optimizer</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the network</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use <kbd class="calibre12">tf.nn.dynamic_rnn</kbd> (link: <a href="https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn" target="_blank" class="calibre11">https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn</a>). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as <kbd class="calibre12">final_state</kbd>, so we can pass it to the first LSTM cell in the the next mini-batch run. For <kbd class="calibre12">tf.nn.dynamic_rnn</kbd>, we pass in the cell and initial state we get from <kbd class="calibre12">build_lstm</kbd>, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN:</p>
<pre class="calibre21">class CharLSTM:<br class="title-page-name"/>    <br class="title-page-name"/>    def __init__(self, num_classes, batch_size=64, num_steps=50, <br class="title-page-name"/>                       lstm_size=128, num_layers=2, learning_rate=0.001, <br class="title-page-name"/>                       grad_clip=5, sampling=False):<br class="title-page-name"/>    <br class="title-page-name"/>        # When we're using this network for generating text by sampling, we'll be providing the network with<br class="title-page-name"/>        # one character at a time, so providing an option for it.<br class="title-page-name"/>        if sampling == True:<br class="title-page-name"/>            batch_size, num_steps = 1, 1<br class="title-page-name"/>        else:<br class="title-page-name"/>            batch_size, num_steps = batch_size, num_steps<br class="title-page-name"/><br class="title-page-name"/>        tf.reset_default_graph()<br class="title-page-name"/>        <br class="title-page-name"/>        # Build the model inputs placeholders of the input and target variables<br class="title-page-name"/>        self.inputs, self.targets, self.keep_prob = build_model_inputs(batch_size, num_steps)<br class="title-page-name"/><br class="title-page-name"/>        # Building the LSTM cell<br class="title-page-name"/>        lstm_cell, self.initial_state = build_lstm_cell(lstm_size, num_layers, batch_size, self.keep_prob)<br class="title-page-name"/><br class="title-page-name"/>        ### Run the data through the LSTM layers<br class="title-page-name"/>        # one_hot encode the input<br class="title-page-name"/>        input_x_one_hot = tf.one_hot(self.inputs, num_classes)<br class="title-page-name"/>        <br class="title-page-name"/>        # Runing each sequence step through the LSTM architecture and finally collecting the outputs<br class="title-page-name"/>        outputs, state = tf.nn.dynamic_rnn(lstm_cell, input_x_one_hot, initial_state=self.initial_state)<br class="title-page-name"/>        self.final_state = state<br class="title-page-name"/>        <br class="title-page-name"/>        # Get softmax predictions and logits<br class="title-page-name"/>        self.prediction, self.logits = build_model_output(outputs, lstm_size, num_classes)<br class="title-page-name"/>        <br class="title-page-name"/>        # Loss and optimizer (with gradient clipping)<br class="title-page-name"/>        self.loss = model_loss(self.logits, self.targets, lstm_size, num_classes)<br class="title-page-name"/>        self.optimizer = build_model_optimizer(self.loss, learning_rate, grad_clip)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model hyperparameters</h1>
                </header>
            
            <article>
                
<p class="calibre2">As with any deep learning architecture, there are a few hyperparameters that someone can use to control the model and fine-tune it. The following is the set of hyperparameters that we are using for this architecture:</p>
<ul class="calibre7">
<li class="calibre8">Batch size is the number of sequences running through the network in one pass.</li>
<li class="calibre8">The number of steps is the number of characters in the sequence the network is trained on. Larger is better typically; the network will learn more long-range dependencies, but will take longer to train. 100 is typically a good number here.</li>
<li class="calibre8">The LSTM size is the number of units in the hidden layers.</li>
<li class="calibre8">Architecture number layers is the number of hidden LSTM layers to use.</li>
<li class="calibre8">Learning rate is the typical learning rate for training.</li>
<li class="calibre8">And finally, the new thing that we call keep probability is used by the dropout layer; it helps the network to avoid overfitting. So if your network is overfitting, try decreasing this.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, let's kick off the training process by providing the inputs and outputs to the built model and then use the optimizer to train the network. Don't forget that we need to use the previous state while making predictions for the current state. Thus, we need to pass the output state back to the network so that it can be used during the prediction of the next input.</p>
<p class="calibre2">Let's provide initial values for our hyperparameters (you can tune them afterwards depending on the dataset you are using to train this architecture):</p>
<pre class="calibre21"><br class="title-page-name"/>batch_size = 100        # Sequences per batch<br class="title-page-name"/>num_steps = 100         # Number of sequence steps per batch<br class="title-page-name"/>lstm_size = 512         # Size of hidden layers in LSTMs<br class="title-page-name"/>num_layers = 2          # Number of LSTM layers<br class="title-page-name"/>learning_rate = 0.001   # Learning rate<br class="title-page-name"/>keep_probability = 0.5  # Dropout keep probability</pre>
<pre class="calibre21">epochs = 5<br class="title-page-name"/><br class="title-page-name"/># Save a checkpoint N iterations<br class="title-page-name"/>save_every_n = 100<br class="title-page-name"/><br class="title-page-name"/>LSTM_model = CharLSTM(len(language_vocab), batch_size=batch_size, num_steps=num_steps,<br class="title-page-name"/>                lstm_size=lstm_size, num_layers=num_layers, <br class="title-page-name"/>                learning_rate=learning_rate)<br class="title-page-name"/><br class="title-page-name"/>saver = tf.train.Saver(max_to_keep=100)<br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/>    sess.run(tf.global_variables_initializer())<br class="title-page-name"/>    <br class="title-page-name"/>    # Use the line below to load a checkpoint and resume training<br class="title-page-name"/>    #saver.restore(sess, 'checkpoints/______.ckpt')<br class="title-page-name"/>    counter = 0<br class="title-page-name"/>    for e in range(epochs):<br class="title-page-name"/>        # Train network<br class="title-page-name"/>        new_state = sess.run(LSTM_model.initial_state)<br class="title-page-name"/>        loss = 0<br class="title-page-name"/>        for x, y in generate_character_batches(encoded_vocab, batch_size, num_steps):<br class="title-page-name"/>            counter += 1<br class="title-page-name"/>            start = time.time()<br class="title-page-name"/>            feed = {LSTM_model.inputs: x,<br class="title-page-name"/>                    LSTM_model.targets: y,<br class="title-page-name"/>                    LSTM_model.keep_prob: keep_probability,<br class="title-page-name"/>                    LSTM_model.initial_state: new_state}<br class="title-page-name"/>            batch_loss, new_state, _ = sess.run([LSTM_model.loss, <br class="title-page-name"/>                                                 LSTM_model.final_state, <br class="title-page-name"/>                                                 LSTM_model.optimizer], <br class="title-page-name"/>                                                 feed_dict=feed)<br class="title-page-name"/>            <br class="title-page-name"/>            end = time.time()<br class="title-page-name"/>            print('Epoch number: {}/{}... '.format(e+1, epochs),<br class="title-page-name"/>                  'Step: {}... '.format(counter),<br class="title-page-name"/>                  'loss: {:.4f}... '.format(batch_loss),<br class="title-page-name"/>                  '{:.3f} sec/batch'.format((end-start)))<br class="title-page-name"/>        <br class="title-page-name"/>            if (counter % save_every_n == 0):<br class="title-page-name"/>                saver.save(sess, "checkpoints/i{}_l{}.ckpt".format(counter, lstm_size))<br class="title-page-name"/>    <br class="title-page-name"/>    saver.save(sess, "checkpoints/i{}_l{}.ckpt".format(counter, lstm_size))</pre>
<p class="calibre2">At the end of the training process, you should get an error close to this:</p>
<pre class="calibre21">.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch number: 5/5...  Step: 978...  loss: 1.7151...  0.050 sec/batch
Epoch number: 5/5...  Step: 979...  loss: 1.7428...  0.051 sec/batch
Epoch number: 5/5...  Step: 980...  loss: 1.7151...  0.050 sec/batch
Epoch number: 5/5...  Step: 981...  loss: 1.7236...  0.050 sec/batch
Epoch number: 5/5...  Step: 982...  loss: 1.7314...  0.051 sec/batch
Epoch number: 5/5...  Step: 983...  loss: 1.7369...  0.051 sec/batch
Epoch number: 5/5...  Step: 984...  loss: 1.7075...  0.065 sec/batch
Epoch number: 5/5...  Step: 985...  loss: 1.7304...  0.051 sec/batch
Epoch number: 5/5...  Step: 986...  loss: 1.7128...  0.049 sec/batch
Epoch number: 5/5...  Step: 987...  loss: 1.7107...  0.051 sec/batch
Epoch number: 5/5...  Step: 988...  loss: 1.7351...  0.051 sec/batch
Epoch number: 5/5...  Step: 989...  loss: 1.7260...  0.049 sec/batch
Epoch number: 5/5...  Step: 990...  loss: 1.7144...  0.051 sec/batch</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving checkpoints</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, let's load the checkpoints. For more about saving and loading checkpoints, you can check out the TensorFlow documentation (<a href="https://www.tensorflow.org/programmers_guide/variables" target="_blank" class="calibre11">https://www.tensorflow.org/programmers_guide/variables</a>):</p>
<pre class="calibre21">tf.train.get_checkpoint_state('checkpoints')</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>model_checkpoint_path: "checkpoints/i990_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i100_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i200_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i300_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i400_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i500_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i600_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i700_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i800_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i900_l512.ckpt"
all_model_checkpoint_paths: "checkpoints/i990_l512.ckpt"</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating text</h1>
                </header>
            
            <article>
                
<p class="calibre2">We have a trained model based on our input dataset. The next step is to use this trained model to generate text and see how this model learned the style and structure of the input data. To do this, we can start with some initial characters and then feed the new, predicted one as an input in the next step. We will repeat this process until we get a text with a specific length.</p>
<p class="calibre2">In the following code, we have also added extra statements to the function to prime the network with some initial text and start from there.</p>
<p class="calibre2">The network gives us predictions or probabilities for each character in the vocab. To reduce noise and only use the ones that the network is more confident about, we're going to only choose a new character from the top <em class="calibre19">N</em> most probable characters in the output:</p>
<pre class="calibre21">def choose_top_n_characters(preds, vocab_size, top_n_chars=4):<br class="title-page-name"/>    p = np.squeeze(preds)<br class="title-page-name"/>    p[np.argsort(p)[:-top_n_chars]] = 0<br class="title-page-name"/>    p = p / np.sum(p)<br class="title-page-name"/>    c = np.random.choice(vocab_size, 1, p=p)[0]<br class="title-page-name"/>    return c</pre>
<pre class="calibre21">def sample_from_LSTM_output(checkpoint, n_samples, lstm_size, vocab_size, prime="The "):<br class="title-page-name"/>    samples = [char for char in prime]<br class="title-page-name"/>    LSTM_model = CharLSTM(len(language_vocab), lstm_size=lstm_size, sampling=True)<br class="title-page-name"/>    saver = tf.train.Saver()<br class="title-page-name"/>    with tf.Session() as sess:<br class="title-page-name"/>        saver.restore(sess, checkpoint)<br class="title-page-name"/>        new_state = sess.run(LSTM_model.initial_state)<br class="title-page-name"/>        for char in prime:<br class="title-page-name"/>            x = np.zeros((1, 1))<br class="title-page-name"/>            x[0,0] = vocab_to_integer[char]<br class="title-page-name"/>            feed = {LSTM_model.inputs: x,<br class="title-page-name"/>                    LSTM_model.keep_prob: 1.,<br class="title-page-name"/>                    LSTM_model.initial_state: new_state}<br class="title-page-name"/>            preds, new_state = sess.run([LSTM_model.prediction, LSTM_model.final_state], <br class="title-page-name"/>                                         feed_dict=feed)<br class="title-page-name"/><br class="title-page-name"/>        c = choose_top_n_characters(preds, len(language_vocab))<br class="title-page-name"/>        samples.append(integer_to_vocab[c])<br class="title-page-name"/><br class="title-page-name"/>        for i in range(n_samples):<br class="title-page-name"/>            x[0,0] = c<br class="title-page-name"/>            feed = {LSTM_model.inputs: x,<br class="title-page-name"/>                    LSTM_model.keep_prob: 1.,<br class="title-page-name"/>                    LSTM_model.initial_state: new_state}<br class="title-page-name"/>            preds, new_state = sess.run([LSTM_model.prediction, LSTM_model.final_state], <br class="title-page-name"/>                                         feed_dict=feed)<br class="title-page-name"/><br class="title-page-name"/>            c = choose_top_n_characters(preds, len(language_vocab))<br class="title-page-name"/>            samples.append(integer_to_vocab[c])<br class="title-page-name"/>        <br class="title-page-name"/>    return ''.join(samples)</pre>
<p class="calibre2">Let's start the sampling process using the latest checkpoint saved:</p>
<pre class="calibre21">tf.train.latest_checkpoint('checkpoints')</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>'checkpoints/i990_l512.ckpt'</pre>
<p class="calibre2">Now, it's time to sample using this latest checkpoint:</p>
<pre class="calibre21">checkpoint = tf.train.latest_checkpoint('checkpoints')<br class="title-page-name"/>sampled_text = sample_from_LSTM_output(checkpoint, 1000, lstm_size, len(language_vocab), prime="Far")<br class="title-page-name"/>print(sampled_text)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>INFO:tensorflow:Restoring parameters from checkpoints/i990_l512.ckpt</pre>
<pre class="calibre21">Farcial the
confiring to the mone of the correm and thinds. She
she saw the
streads of herself hand only astended of the carres to her his some of the princess of which he came him of
all that his white the dreasing of
thisking the princess and with she was she had
bettee a still and he was happined, with the pood on the mush to the peaters and seet it.

"The possess a streatich, the may were notine at his mate a misted
and the
man of the mother at the same of the seem her
felt. He had not here.

"I conest only be alw you thinking that the partion
of their said."

"A much then you make all her
somether. Hower their centing
about
this, and I won't give it in
himself.
I had not come at any see it will that there she chile no one that him.

"The distiction with you all.... It was
a mone of the mind were starding to the simple to a mone. It to be to ser in the place," said Vronsky.
"And a plais in
his face, has alled in the consess on at they to gan in the sint
at as that
he would not be and t</pre>
<p class="calibre2">You can see that we were able to generate some meaningful words and some meaningless words. In order to get more results, you can run the model for more epochs and try to play with the hyperparameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">We learned about RNNs, how they work, and why they have become a big deal. We trained an RNN character-level language model on fun novel datasets and saw where RNNs are going. You can confidently expect a large amount of innovation in the space of RNNs, and I believe they will become a pervasive and critical component of intelligent systems.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    </body></html>