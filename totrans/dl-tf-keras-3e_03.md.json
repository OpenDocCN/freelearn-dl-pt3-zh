["```\n#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784) \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n```", "```\nmodel.add(layers.MaxPooling2D((2, 2))) \n```", "```\nlayers.Convolution2D(20, (5, 5), activation='relu', input_shape=input_shape) \n```", "```\nlayers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)) \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models, optimizers\n# network and training\nEPOCHS = 5\nBATCH_SIZE = 128\nVERBOSE = 1\nOPTIMIZER = tf.keras.optimizers.Adam()\nVALIDATION_SPLIT=0.90\nIMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\nINPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\nNB_CLASSES = 10  # number of outputs = number of digits \n```", "```\n#define the convnet \ndef build(input_shape, classes):\n    model = models.Sequential() \n```", "```\n# CONV => RELU => POOL\nmodel.add(layers.Convolution2D(20, (5, 5), activation='relu',\n            input_shape=input_shape))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) \n```", "```\n# CONV => RELU => POOL\nmodel.add(layers.Convolution2D(50, (5, 5), activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) \n```", "```\n# Flatten => RELU layers\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(500, activation='relu'))\n# a softmax classifier\nmodel.add(layers.Dense(classes, activation=\"softmax\"))\nreturn model \n```", "```\n# data: shuffled and split between train and test sets\n(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n# reshape\nX_train = X_train.reshape((60000, 28, 28, 1))\nX_test = X_test.reshape((10000, 28, 28, 1))\n# normalize\nX_train, X_test = X_train / 255.0, X_test / 255.0\n# cast\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n# convert class vectors to binary class matrices\ny_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\ny_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)\n# initialize the optimizer and model\nmodel = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n    metrics=[\"accuracy\"])\nmodel.summary()\n# use TensorBoard, princess Aurora!\ncallbacks = [\n  # Write TensorBoard logs to './logs' directory\n  tf.keras.callbacks.TensorBoard(log_dir='./logs')\n]\n# fit \nhistory = model.fit(X_train, y_train, \n        batch_size=BATCH_SIZE, epochs=EPOCHS, \n        verbose=VERBOSE, validation_split=VALIDATION_SPLIT,\n        callbacks=callbacks)\nscore = model.evaluate(X_test, y_test, verbose=VERBOSE)\nprint(\"\\nTest score:\", score[0])\nprint('Test accuracy:', score[1]) \n```", "```\nModel: \"sequential_1\"\n_____________________________________________________________________\nLayer (type)                    Output Shape              Param #    \n=====================================================================\nconv2d_2 (Conv2D)               (None, 24, 24, 20)        520        \n\nmax_pooling2d_2 (MaxPooling  2D) (None, 12, 12, 20)       0          \n\nconv2d_3 (Conv2D)               (None, 8, 8, 50)          25050      \n\nmax_pooling2d_3 (MaxPooling  2D) (None, 4, 4, 50)         0          \n\nflatten   (Flatten)             (None, 800)               0          \n\ndense   (Dense)                 (None, 500)               400500     \n\ndense_1 (Dense)                 (None, 10)                5010    \n\n=====================================================================\nTotal params: 431,080\nTrainable params: 431,080\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/20\n[2019-04-04 14:18:28.546158: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profile Session started.\n48000/48000 [==============================] - 28s 594us/sample - loss: 0.2035 - accuracy: 0.9398 - val_loss: 0.0739 - val_accuracy: 0.9783\nEpoch 2/20\n48000/48000 [==============================] - 26s 534us/sample - loss: 0.0520 - accuracy: 0.9839 - val_loss: 0.0435 - val_accuracy: 0.9868\nEpoch 3/20\n48000/48000 [==============================] - 27s 564us/sample - loss: 0.0343 - accuracy: 0.9893 - val_loss: 0.0365 - val_accuracy: 0.9895\nEpoch 4/20\n48000/48000 [==============================] - 27s 562us/sample - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.0452 - val_accuracy: 0.9868\nEpoch 5/20\n48000/48000 [==============================] - 27s 562us/sample - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.0428 - val_accuracy: 0.9873\nEpoch 6/20\n48000/48000 [==============================] - 28s 548us/sample - loss: 0.0585 - accuracy: 0.9820 - val_loss: 0.1038 - val_accuracy: 0.9685\nEpoch 7/20\n48000/48000 [==============================] - 26s 537us/sample - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0388 - val_accuracy: 0.9896\nEpoch 8/20\n48000/48000 [==============================] - 29s 589us/sample - loss: 0.0097 - accuracy: 0.9966 - val_loss: 0.0347 - val_accuracy: 0.9899\nEpoch 9/20\n48000/48000 [==============================] - 29s 607us/sample - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0515 - val_accuracy: 0.9859\nEpoch 10/20\n48000/48000 [==============================] - 27s 565us/sample - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0376 - val_accuracy: 0.9904\nEpoch 11/20\n48000/48000 [==============================] - 30s 627us/sample - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.0366 - val_accuracy: 0.9911\nEpoch 12/20\n48000/48000 [==============================] - 24s 505us/sample - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0389 - val_accuracy: 0.9910\nEpoch 13/20\n48000/48000 [==============================] - 28s 584us/sample - loss: 0.0057 - accuracy: 0.9978 - val_loss: 0.0531 - val_accuracy: 0.9890\nEpoch 14/20\n48000/48000 [==============================] - 28s 580us/sample - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.0409 - val_accuracy: 0.9911\nEpoch 15/20\n48000/48000 [==============================] - 26s 537us/sample - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0436 - val_accuracy: 0.9911\nEpoch 16/20\n48000/48000 [==============================] - 25s 513us/sample - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.0480 - val_accuracy: 0.9890\nEpoch 17/20\n48000/48000 [==============================] - 24s 499us/sample - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0535 - val_accuracy: 0.9888\nEpoch 18/20\n48000/48000 [==============================] - 24s 505us/sample - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0349 - val_accuracy: 0.9926\nEpoch 19/20\n48000/48000 [==============================] - 29s 599us/sample - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0377 - val_accuracy: 0.9920\nEpoch 20/20\n48000/48000 [==============================] - 25s 524us/sample - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0477 - val_accuracy: 0.9917\n10000/10000 [==============================] - 2s 248us/sample - loss: 0.0383 - accuracy: 0.9915\nTest score: 0.03832608199457617\nTest accuracy: 0.9915 \n```", "```\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/10\n[2019-04-04 15:57:17.848186: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profile Session started.\n48000/48000 [==============================] - 26s 544us/sample - loss: 0.2134 - accuracy: 0.9361 - val_loss: 0.0688 - val_accuracy: 0.9783\nEpoch 2/10\n48000/48000 [==============================] - 30s 631us/sample - loss: 0.0550 - accuracy: 0.9831 - val_loss: 0.0533 - val_accuracy: 0.9843\nEpoch 3/10\n48000/48000 [==============================] - 30s 621us/sample - loss: 0.0353 - accuracy: 0.9884 - val_loss: 0.0410 - val_accuracy: 0.9874\nEpoch 4/10\n48000/48000 [==============================] - 37s 767us/sample - loss: 0.0276 - accuracy: 0.9910 - val_loss: 0.0381 - val_accuracy: 0.9887\nEpoch 5/10\n48000/48000 [==============================] - 24s 509us/sample - loss: 0.0200 - accuracy: 0.9932 - val_loss: 0.0406 - val_accuracy: 0.9881\nEpoch 6/10\n48000/48000 [==============================] - 31s 641us/sample - loss: 0.0161 - accuracy: 0.9950 - val_loss: 0.0423 - val_accuracy: 0.9881\nEpoch 7/10\n48000/48000 [==============================] - 29s 613us/sample - loss: 0.0129 - accuracy: 0.9955 - val_loss: 0.0396 - val_accuracy: 0.9894\nEpoch 8/10\n48000/48000 [==============================] - 27s 554us/sample - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.0454 - val_accuracy: 0.9871\nEpoch 9/10\n48000/48000 [==============================] - 24s 510us/sample - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0388 - val_accuracy: 0.9902\nEpoch 10/10\n48000/48000 [==============================] - 26s 542us/sample - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.0440 - val_accuracy: 0.99892\n10000/10000 [==============================] - 2s 196us/sample - loss: 0.0327 - accuracy: 0.9910\nTest score: 0.03265062951518773\nTest accuracy: 0.991 \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models, optimizers\n# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\nIMG_CHANNELS = 3\nIMG_ROWS = 32\nIMG_COLS = 32\n#constant\nBATCH_SIZE = 128\nEPOCHS = 20\nCLASSES = 10\nVERBOSE = 1\nVALIDATION_SPLIT = 0.2\nOPTIM = tf.keras.optimizers.RMSprop() \n```", "```\n#define the convnet \ndef build(input_shape, classes):\n    model = models.Sequential() \n    model.add(layers.Convolution2D(32, (3, 3), activation='relu',\n                        input_shape=input_shape))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(layers.Dropout(0.25)) \n```", "```\n model.add(layers.Flatten())\n    model.add(layers.Dense(512, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(classes, activation='softmax'))\n    return model \n```", "```\n# use TensorBoard, princess Aurora!\ncallbacks = [\n  # Write TensorBoard logs to './logs' directory\n  tf.keras.callbacks.TensorBoard(log_dir='./logs')\n]\n# train\nmodel.compile(loss='categorical_crossentropy', optimizer=OPTIM,\n    metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, batch_size=BATCH_SIZE,\n    epochs=EPOCHS, validation_split=VALIDATION_SPLIT, \n    verbose=VERBOSE, callbacks=callbacks) \nscore = model.evaluate(X_test, y_test,\n                     batch_size=BATCH_SIZE, verbose=VERBOSE)\nprint(\"\\nTest score:\", score[0])\nprint('Test accuracy:', score[1]) \n```", "```\nEpoch 17/20\n40000/40000 [==============================] - 112s 3ms/sample - loss: 0.6282 - accuracy: 0.7841 - val_loss: 1.0296 - val_accuracy: 0.6734\nEpoch 18/20\n40000/40000 [==============================] - 76s 2ms/sample - loss: 0.6140 - accuracy: 0.7879 - val_loss: 1.0789 - val_accuracy: 0.6489\nEpoch 19/20\n40000/40000 [==============================] - 74s 2ms/sample - loss: 0.5931 - accuracy: 0.7958 - val_loss: 1.0461 - val_accuracy: 0.6811\nEpoch 20/20\n40000/40000 [==============================] - 71s 2ms/sample - loss: 0.5724 - accuracy: 0.8042 - val_loss: 0.1.0527 - val_accuracy: 0.6773\n10000/10000 [==============================] - 5s 472us/sample - loss: 1.0423 - accuracy: 0.6686\nTest score: 1.0423416819572449\nTest accuracy: 0.6686 \n```", "```\ndef build_model(): \n    model = models.Sequential()\n\n    #1st block\n    model.add(layers.Conv2D(32, (3,3), padding='same', \n        input_shape=x_train.shape[1:], activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    model.add(layers.Dropout(0.2))\n    #2nd block\n    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    model.add(layers.Dropout(0.3))\n    #3d block \n    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    model.add(layers.Dropout(0.4))\n    #dense  \n    model.add(layers.Flatten())\n    model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n    return model\n    model.summary() \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models, regularizers, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\n\nEPOCHS=50\nNUM_CLASSES = 10\ndef load_data():\n    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n    x_train = x_train.astype('float32')\n    x_test = x_test.astype('float32')\n\n    #normalize \n    mean = np.mean(x_train,axis=(0,1,2,3))\n    std = np.std(x_train,axis=(0,1,2,3))\n    x_train = (x_train-mean)/(std+1e-7)\n    x_test = (x_test-mean)/(std+1e-7)\n\n    y_train =  tf.keras.utils.to_categorical(y_train,NUM_CLASSES)\n    y_test =  tf.keras.utils.to_categorical(y_test,NUM_CLASSES)\n    return x_train, y_train, x_test, y_test \n```", "```\n(x_train, y_train, x_test, y_test) = load_data()\nmodel = build_model()\nmodel.compile(loss='categorical_crossentropy', \n            optimizer='RMSprop', \n            metrics=['accuracy'])\n#train\nbatch_size = 64\nmodel.fit(x_train, y_train, batch_size=batch_size,\n    epochs=EPOCHS, validation_data=(x_test,y_test)) \nscore = model.evaluate(x_test, y_test,\n                     batch_size=batch_size)\nprint(\"\\nTest score:\", score[0])\nprint('Test accuracy:', score[1]) \n```", "```\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n#image augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    )\ndatagen.fit(x_train) \n```", "```\n#train\nbatch_size = 64\nmodel.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n                    epochs=EPOCHS,\n                    verbose=1,validation_data=(x_test,y_test))\n#save to disk\nmodel_json = model.to_json()\nwith open('model.json', 'w') as json_file:\n    json_file.write(model_json)\nmodel.save_weights('model.h5') \n#test\nscores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\nprint('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0])) \n```", "```\nEpoch 46/50\n50000/50000 [==============================] - 36s 722us/sample - loss: 0.2440 - accuracy: 0.9183 - val_loss: 0.4918 - val_accuracy: 0.8546\nEpoch 47/50\n50000/50000 [==============================] - 34s 685us/sample - loss: 0.2338 - accuracy: 0.9208 - val_loss: 0.4884 - val_accuracy: 0.8574\nEpoch 48/50\n50000/50000 [==============================] - 32s 643us/sample - loss: 0.2383 - accuracy: 0.9189 - val_loss: 0.5106 - val_accuracy: 0.8556\nEpoch 49/50\n50000/50000 [==============================] - 37s 734us/sample - loss: 0.2285 - accuracy: 0.9212 - val_loss: 0.5017 - val_accuracy: 0.8581\nEpoch 49/50\n50000/50000 [==============================] - 36s 712us/sample - loss: 0.2263 - accuracy: 0.9228 - val_loss: 0.4911 - val_accuracy: 0.8591\n10000/10000 [==============================] - 2s 160us/sample - loss: 0.4911 - accuracy: 0.8591\nTest score: 0.4911323667049408\nTest accuracy: 0.8591 \n```", "```\nimport numpy as np\nimport scipy.misc\nfrom tensorflow.keras.models import model_from_json\nfrom tensorflow.keras.optimizers import SGD\n#load model\nmodel_architecture = 'cifar10_architecture.json'\nmodel_weights = 'cifar10_weights.h5'\nmodel = model_from_json(open(model_architecture).read())\nmodel.load_weights(model_weights)\n#load images\nimg_names = ['cat-standing.jpg', 'dog.jpg']\nimgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), (32, 32)),\n                     (2, 0, 1)).astype('float32')\n           for img_name in img_names]\nimgs = np.array(imgs) / 255\n# train\noptim = SGD()\nmodel.compile(loss='categorical_crossentropy', optimizer=optim,\n    metrics=['accuracy'])\n# predict \npredictions = model.predict_classes(imgs)\nprint(predictions) \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n# define a VGG16 network\ndef VGG_16(weights_path=None):\n    model = models.Sequential()\n    model.add(layers.ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(128, (3, 3), activation='relu'))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(128, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(256, (3, 3), activation='relu'))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(256, (3, 3), activation='relu'))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(256, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\n    model.add(layers.ZeroPadding2D((1,1)))\n    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\n    model.add(layers.Flatten())\n    #top layer of the VGG net\n    model.add(layers.Dense(4096, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(4096, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1000, activation='softmax'))\n    if weights_path:\n        model.load_weights(weights_path)\n    return model \n```", "```\nimport cv2\nim = cv2.resize(cv2.imread('cat.jpg'), (224, 224)).astype(np.float32)\n#im = im.transpose((2,0,1))\nim = np.expand_dims(im, axis=0)\n# Test pretrained model\nmodel = VGG_16('/Users/antonio/.keras/models/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\nmodel.summary()\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\nout = model.predict(im)\nprint(np.argmax(out)) \n```", "```\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n---------------------------------------------------------------\n285 \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n# pre built model with pre-trained weights on imagenet\nmodel = VGG16(weights='imagenet', include_top=True)\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy')\n# resize into VGG16 trained images' format\nim = cv2.resize(cv2.imread('steam-locomotive.jpg'), (224, 224))\nim = np.expand_dims(im, axis=0)\n# predict\nout = model.predict(im)\nindex = np.argmax(out)\nprint(index)\nplt.plot(out.ravel())\nplt.show()\n#this should print 820 for steaming train \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16 \nfrom tensorflow.keras import models\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nimport numpy as np\nimport cv2\n# prebuild model with pre-trained weights on imagenet\nbase_model = VGG16(weights='imagenet', include_top=True)\nprint (base_model)\nfor i, layer in enumerate(base_model.layers):\n    print (i, layer.name, layer.output_shape)\n# extract features from block4_pool block\nmodel = models.Model(inputs=base_model.input, \n    outputs=base_model.get_layer('block4_pool').output)\nimg_path = 'cat.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n# get the features from this block\nfeatures = model.predict(x)\nprint(features) \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import layers, models\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False) \n```", "```\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n87916544/87910968 [===========================] â€“ 26s 0us/step \n```", "```\n# layer.name, layer.input_shape, layer.output_shape\n('mixed10', [(None, 8, 8, 320), (None, 8, 8, 768), (None, 8, 8, 768), (None, 8, 8, 192)], (None, 8, 8, 2048))\n('avg_pool', (None, 8, 8, 2048), (None, 1, 1, 2048))\n('flatten', (None, 1, 1, 2048), (None, 2048))\n('predictions', (None, 2048), (None, 1000)) \n```", "```\nx = base_model.output\n# let's add a fully-connected layer as first layer\nx = layers.Dense(1024, activation='relu')(x)\n# and a logistic layer with 200 classes as last layer\npredictions = layers.Dense(200, activation='softmax')(x)\n# model to train\nmodel = models.Model(inputs=base_model.input, outputs=predictions) \n```", "```\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False \n```", "```\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n# train the model on the new data for a few epochs\nmodel.fit_generator(...) \n```", "```\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 172 layers and unfreeze the rest:\nfor layer in model.layers[:172]:\n   layer.trainable = False\nfor layer in model.layers[172:]:\n   layer.trainable = True \n```", "```\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom tensorflow.keras.optimizers import SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel.fit_generator(...) \n```", "```\n#\n#content distance\n#\ndef get_content_loss(base_content, target):\n  return tf.reduce_mean(tf.square(base_content - target)) \n```", "```\n#style distance\n#\ndef gram_matrix(input_tensor):\n  # image channels first \n  channels = int(input_tensor.shape[-1])\n  a = tf.reshape(input_tensor, [-1, channels])\n  n = tf.shape(a)[0]\n  gram = tf.matmul(a, a, transpose_a=True)\n  return gram / tf.cast(n, tf.float32)\n\ndef get_style_loss(base_style, gram_target):\n  # height, width, num filters of each layer\n  height, width, channels = base_style.get_shape().as_list()\n  gram_style = gram_matrix(base_style)\n\n  return tf.reduce_mean(tf.square(gram_style - gram_target)) \n```"]