- en: Neural Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to address one of the hot and trendy applications
    in natural language processing, which is called **sentiment analysis**. Most people
    nowadays express their opinions about something through social media platforms,
    and making use of this vast amount of text to keep track of customer satisfaction
    about something is very crucial for companies or even governments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to use recurrent-type neural networks to build
    a sentiment analysis solution. The following topics will be addressed in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: General sentiment analysis architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis—model implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General sentiment analysis architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to focus on the general deep learning architectures
    that can be used for sentiment analysis. The following figure shows the processing
    steps that are required for building the sentiment analysis model.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, first off, we are going to deal with natural human language:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93bdac3a-0ed3-447d-84f2-8fae6e0ce5b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A general pipeline for sentiment analysis solutions or even sequence-based
    natural language solutions'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use movie reviews to build this sentiment analysis application.
    The goal of this application is to produce positive and negative reviews based
    on the input raw text. For example, if the raw text is something like, **This
    movie is good**, then we need the model to produce a positive sentiment for it.
  prefs: []
  type: TYPE_NORMAL
- en: A sentiment analysis application will take us through a lot of processing steps
    that are needed to work with natural human languages inside a neural network such
    as embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: So in this case, we have a raw text, for example, **This is not a good movie!** What
    we want to end up with is whether this is a negative or positive sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several difficulties in this type of application:'
  prefs: []
  type: TYPE_NORMAL
- en: One of them is that the sequences may have **different lengths**. This is a
    very short one, but we will see examples of text that have more than 500 words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another problem is that if we just look at individual words (for example, good),
    that indicates a positive sentiment. However, it is preceded by the word **not**,
    so now it's a negative sentiment. This can get a lot more complicated, and we
    will see an example of it later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we learned in the previous chapter, a neural network cannot work on raw text,
    so we need to first convert it into what are called **tokens**. These are basically
    just integer values, so we go through our entire dataset and we count the number
    of times each word is being used. Then, we make a vocabulary and each word gets
    an index in this vocabulary. So the word **this** has an integer ID or token **11**,
    the word **is** has a token **6**, **not** has a token **21**, and so forth. So
    now, we have converted the raw text into a list of integers called tokens.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network still cannot operate on this data, because if we have a vocabulary
    of 10,000 words, the tokens can take values between 0 and 9,999, and they may
    not be related at all. So, word number 998 may have a completely different semantic
    meaning than word number 999.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we will use the idea of representation learning or embeddings that
    we learned about in the previous chapter. This embedding layer converts integer
    tokens into real-valued vectors, so token **11** becomes the vector [0.67,0.36,...,0.39],
    as shown in *Figure 1*. The same applies to the next token number 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick recap of what we studied in the previous chapter: this embedding layer
    in the preceding figure learns the mapping between tokens and their corresponding
    real-valued vector. Also, the embedding layer learns the semantic meanings of
    the words so that words that have similar meanings are somehow close to each other
    in this embedding space.'
  prefs: []
  type: TYPE_NORMAL
- en: Out of the input raw text, we get a two-dimensional matrix, or tensor, which
    can now be inputted to the **recurrent neural network** (**RNN**). This can process
    sequences of arbitrary length and the output of this network is then fed into
    a fully connected or dense layer with a sigmoid activation function. So, the output
    is between 0 and 1, where a value of 0 is taken to mean a negative sentiment.
    But what if the value of the sigmoid function is neither 0 nor 1? Then we need
    to introduce a cut-off or a threshold value in the middle so that if the value
    is below 0.5, then the corresponding input is taken to be a negative sentiment,
    and a value above this threshold is taken to be a positive sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs – sentiment analysis context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s recap the basic concepts of RNNs and also talk about them in the
    context of the sentiment analysis application. As we mentioned in the RNN chapter,
    the basic building block of a RNN is a recurrent unit, as shown in this figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b9d9701-39be-4d45-938e-abb4c10c7bc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An abstract idea of an RNN unit'
  prefs: []
  type: TYPE_NORMAL
- en: This figure is an abstraction of what goes on inside the recurrent unit. What
    we have here is the input, so this would be a word, for example, **good**. Of
    course, it has to be converted to embedding vectors. However, we will ignore that
    for now. Also, this unit has a kind of memory state, and depending on the contents
    of this **State** and the **Input**, we will update this state and write new data
    into the state. For example, imagine that we have previously seen the word **not** in
    the input; we write that to the state so that when we see the word **good** on
    one of the following inputs, we know from the state that we have just seen the
    word **not**. Now, we see the word **good**. Thus, we have to write into the state
    that we have seen the words **not good** together so that this might indicate
    that the whole input text probably has a negative sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: The mapping from the old state and the input to the new contents of the state
    is done through a so-called **Gate**, and the way these are implemented differs
    across different versions of recurrent units. It is basically a matrix operation
    with an activation function, but as we will see in a moment, there is a problem
    with backpropagating gradients. So, the RNN has to be designed in a special way
    so that the gradients are not distorted too much.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a recurrent unit, we have a similar gate for producing the output, and once
    again the output of the recurrent unit depends on the current contents of the
    state and the input that we are seeing. So what we can try and do is unroll the
    processing that takes place with a recurrent unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cb1e335-63bd-4471-b9c1-cff84f65718f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Unrolled version of the recurrent neural net'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what we have here is just one recurrent unit, but the flow chart shows
    what happens at different time steps. So:'
  prefs: []
  type: TYPE_NORMAL
- en: In time step 1, we input the word **this** to the recurrent unit and it has
    its internal memory state first initialized to zero. This is done by TensorFlow
    whenever we start processing a new sequence of data. So, we see the word **this** and
    the recurrent unit state is 0\. Hence, we use the internal gate to update the
    memory state and **this** is then used in time step number two where we input
    the word **is**; now, the memory state has some contents. There's not a whole
    lot of meaning in the word **this**, so the state might still be around 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And there's also not a lot of meaning in **is**, so perhaps the state is still
    somewhat 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next time step, we see the word **not**, and this has meaning we ultimately
    want to predict, which is the sentiment of the whole input text. This one is what
    we need to store in the memory so that the gate inside the recurrent unit sees
    that the state already probably contains near-zero values. But now it wants to
    store what we have just seen the word **not**, so it saves some nonzero value
    in this state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we move on to the next time step, where we have the word **a**; this also
    doesn't have much information, so it's probably just ignored. It just copies over
    the state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we have the word **very**, and this indicates that whatever sentiment exists
    might be a strong sentiment, so the recurrent unit now knows that we have seen
    **not **and **very**. It stores this somehow in its memory state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next time step, we see the word **good**, so now the network knows **not
    very good** and it thinks, *Oh, this is probably a negative sentiment!* Hence,
    it stores that value in the internal state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, in the final time step, we see **movie**, and this is not really relevant,
    so it's probably just ignored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we use the other gate inside the recurrent unit to output the contents
    of the memory state, and then it is processed with the sigmoid function (which
    we don't show here). We get an output value between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The idea then is that we want to train this network on many many thousands
    of examples of movie reviews from the Internet Movie database, where, for each
    input text, we give it the true sentiment value of either positive or negative.
    Then, we want TensorFlow to find out what the gates inside the recurrent unit
    should be so that they accurately map this input text to the correct sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adf86d1b-4eba-43b5-b736-15177296aad2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Used architecture for this chapter''s implementation'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture for the RNN we will be using in this implementation is an RNN-type
    architecture with three layers. In the first layer, what we've just explained happens, except
    that now we need to output the value from the recurrent unit at each time step.
    Then, we gather a new sequence of data, which is the output of the first recurrent
    layer. Next, we can input it to the second recurrent layer because recurrent units
    need sequences of input data (and the output that we got from the first layer
    and the one that we want to feed into the second recurrent layer are some floating-point
    values whose meanings we don't really understand). This has a meaning inside the
    RNN, but it's not something we as humans will understand. Then, we do similar
    processing in the second recurrent layer.
  prefs: []
  type: TYPE_NORMAL
- en: So, first, we initialize the internal memory state of this recurrent unit to
    0; then, we take the first output from the first recurrent layer and input it.
    We process it with the gates inside this recurrent unit, update the state, take
    the output of the first layer's recurrent unit for the second word **is**, and
    use that as input as well as the internal memory state. We continue doing this
    until we have processed the whole sequence, and then we gather up all the outputs
    of the second recurrent layer. We use them as inputs in the third recurrent layer,
    where we do a similar processing. But here, we only want the output for the last
    time step, which is a kind of summary for everything that has been fed so far.
    We then output that to a fully connected layer that we don't show here. Finally,
    we have the sigmoid activation function, so we get a value between zero and one,
    which represents negative and positive sentiments, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Exploding and vanishing gradients - recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in the previous chapter, there's a phenomenon called **exploding**
    and **vanishing** of gradients values, which is very important in RNNs. Let's
    go back and look at *Figure 1*; that flowchart explains what this phenomenon is.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have a text with 500 words in this dataset that we will be using
    to implement our sentiment analysis classifier. At every time step, we apply the
    internal gates in the recurrent unit in a recursive manner; so if there are 500
    words, we will apply these gates 500 times to update the internal memory state
    of the recurrent unit.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, the way neural networks are trained is by using so-called backpropagation
    of gradients, so we have some loss function that gets the output of the neural
    network and then the true output that we desire for the given input text. Then,
    we want to minimize this loss value so that the actual output of the neural network
    corresponds to the desired output for this particular input text. So, we need
    to take the gradient of this loss function with respect to the weights inside
    these recurrent units, and these weights are for the gates that are updating the
    internal state and outputting the value in the end.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the gate is applied maybe 500 times, and if this has a multiplication in
    it, what we essentially get is an exponential function. So, if you multiply a
    value with itself 500 times and if this value is slightly less than 1, then it
    will very quickly vanish or get lost. Similarly, if a value slightly more than
    1 is multiplied with itself 500 times, it'll explode.
  prefs: []
  type: TYPE_NORMAL
- en: The only values that can survive 500 multiplications are 0 and 1\. They will
    remain the same, so the recurrent unit is actually much more complicated than
    what you see here. This is the abstract idea—that we want to somehow map the internal
    memory state and the input to update the internal memory state and to output some
    value—but in reality, we need to be very careful about propagating the gradients
    backwards through these gates so that we don't have this exponential multiplication
    over many many time steps. We also encourage you to see some tutorials on the
    mathematical definition of recurrent units.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis – model implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen all the bits and pieces of how to implement a stacked version of
    the LSTM variation of RNNs. To make things a bit exciting, we are going to use
    a higher level API called `Keras`.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Keras is a high-level neural networks API, written in Python and capable of
    running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on
    enabling fast experimentation. Being able to go from idea to result with the least
    possible delay is key to doing good research." – Keras website'
  prefs: []
  type: TYPE_NORMAL
- en: So, Keras is just a wrapper around TensorFlow and other deep learning frameworks.
    It's really good for prototyping and getting things built very quickly, but on
    the other hand, it gives you less control over your code. We'll take a chance
    to implement this sentiment analysis model in Keras so that you get a hands-on
    implementation in both TensorFlow and Keras. You can use Keras for fast prototyping
    and TensorFlow for a production-ready system.
  prefs: []
  type: TYPE_NORMAL
- en: 'More interesting news for you is that you don''t have to switch to a totally
    different environment. You can now access Keras as a module in TensorFlow and
    import packages just like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: So, let's go ahead and use what we can now call a more abstracted module inside
    TensorFlow that will help us to prototype deep learning solutions very fast. This
    is because we will get to write full deep learning solutions in just a few lines
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's move on to the actual implementation where we need to load the data.
    Keras actually has a functionality that can be used to load this sentiment dataset
    from IMDb, but the problem is that it has already mapped all the words to integer
    tokens. This is such an essential part of working with natural human language
    insight neural networks that I really want to show you how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if you want to use this code for sentiment analysis of whatever data you
    might have in some other language, you will need to do this yourself, so we have
    just quickly implemented some functions for downloading this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start off by importing a bunch of required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we load the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it has 25,000 texts in the training set and in the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s just see one example from the training set and how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is a fairly short one and the sentiment value is `1.0`, which means it
    is a positive sentiment, so this is a positive review of whatever movie this was
    about.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we get to the tokenizer, and this is the first step of processing this
    raw data because the neural network cannot work on text data. Keras has implemented
    what is called a **tokenizer** for building a vocabulary and mapping from words
    to an integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can say that we want a maximum of 10,000 words, so it will use only the
    10,000 most popular words from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we take all the text from the dataset and we call this function `fit`
    on texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The tokenizer takes about 10 seconds, and then it will have built the vocabulary.
    It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So, each word is now associated with an integer; therefore, the word `the` has
    number `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `and` has number `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The word `a` has `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And so on. We see that `movie` has number `17`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And `film` has number `19`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: What all this means is that `the` was the most used word in the dataset and
    `and` was the second most used in the dataset. So, whenever we want to map words
    to integer tokens, we will get these numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try and take the word number `743` for example, and this was the word
    `romantic`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'So, whenever we see the word `romantic` in the input text, we map it to the
    token integer `743`. We use the tokenizer again to convert all the words in the
    text in the training set into integer tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When we convert that text to integer tokens, it becomes an array of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: So, the word `this` becomes the number 11, the word `is` becomes the number
    59, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to convert the rest of the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, there's another problem because the sequences of tokens have different
    lengths depending on the length of the original text, even though the recurrent
    units can work with sequences of arbitrary length. But the way that TensorFlow
    works is that all of the data in a batch needs to have the same length.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can either ensure that all sequences in the entire dataset have the
    same length, or write a custom data generator that ensures that the sequences
    in a single batch have the same length. Now, it is a lot simpler to ensure that
    all the sequences in the dataset have the same length, but the problem is that
    there are some outliers. We have some sentences that, I think, are more than 2,200
    words long. It will hurt our memory very much if we have all the *short* sentences
    with more than 2,200 words. So what we will do instead is make a compromise; first,
    we need to count all the words, or the number of tokens in each of these input
    sequences. What we see is that the average number of words in a sequence is about
    221:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And we see that the maximum number of words is more than 2,200:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now, there's a huge difference between the average and the max, and again we
    would be wasting a lot of memory if we just padded all the sentences in the dataset
    so that they would all have `2208` tokens. This would especially be a problem
    if you have a dataset with millions of sequences of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what we will do is make a compromise where we will pad all sequences and
    truncate the ones that are too long so that they have `544` words. The way we
    calculated this was like this—we took the average number of words in all the sequences
    in the dataset and we added two standard deviations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'What do we get out of this is? We cover about 95% of the text in the dataset,
    so only about 5% are longer than `544` words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, we call these functions in Keras. They will either pad the sequences that
    are too short (so they will just add zeros) or truncate the sequences that are
    too long (basically just cut off some of the words if the text is too long).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, there''s an important thing here: we can do this padding and truncating
    in pre or post mode. So imagine we have a sequence of integer tokens and we want
    to pad this because it''s too short. We can:'
  prefs: []
  type: TYPE_NORMAL
- en: Either pad all of these zeros at the beginning so that we have the actual integer
    tokens down at the end.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or do it in the opposite way so that we have all this data at the beginning
    and then all the zeros at the end. But if we just go back and look at the preceding
    RNN flowchart, remember that it is processing the sequence one step at a time
    so if we start processing zeros, it will probably not mean anything and the internal
    state would have probably just remain zero. So, whenever it finally sees an integer
    token for a specific word, it will know okay now we start processing the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, if all the zeros were at the end, we would have started processing
    all the data; then we'd have some internal state inside the recurrent unit. Right
    now, we see a whole lot of zeros, so that might actually destroy the internal
    state that we have just calculated. This is why it might be a good idea to have
    the zeros padded at the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: But the other problem is when we truncate a text, so if the text is very long,
    we will truncate it to get it to fit to `544` words, or whatever the number was.
    Now, imagine we've caught this sentence here in the middle somewhere and it says
    **this very good movie** or **this is not**. You know, of course, that we do this only for
    very long sequences, but it is possible that we lose essential information for
    properly classifying this text. So it is a compromise that we're making when we
    are truncating input text. A better way would be to create a batch and just pad
    text inside that batch. So, when we see a very very long sequence, we pad the
    other sequences to have the same length. But we don't need to store all of this
    data in memory because most of it is wasted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back and convert the entire dataset so that it is truncated and padded;
    thus, it''s one big matrix of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the shape of this matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s have a look at specific sample tokens before and after padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And after padding, this sample will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we need a functionality to map backwards so that it maps from integer
    tokens back to text words; we just need that here. It''s a very simple helper
    function, so let''s go ahead and implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, for example, the original text in the dataset is like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use a helper function to convert the tokens back to text words, we get
    this text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: It's basically the same except for punctuation and other symbols.
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we need to create the RNN, and we will do this in Keras because it's very
    simple. We do that with the so-called `sequential` model.
  prefs: []
  type: TYPE_NORMAL
- en: The first layer of this architecture will be what is called an **embedding**.
    If we look back at the flowchart in *Figure 1*, what we just did was we converted
    the raw input text to integer tokens. But we still cannot input this to a RNN,
    so we have to convert that into embedding vectors, which are values that are somewhere
    between -1 and 1\. They can exceed to some extent but are generally somewhere
    between -1 and 1, and this is data that we can then work on in the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: It's somewhat magical because this embedding layer trains simultaneously with
    the RNN and it doesn't see the raw words. It sees integer tokens but learns to
    recognize that there is some pattern in how words are being used together. So
    it can, sort of, deduce that some words or some integer tokens have similar meaning,
    and then it encodes this in embedding vectors that look somewhat the same.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, what we need to decide is the length of each vector so that, for
    example, the token "11" gets converted into a real-valued vector. In this example,
    we will use a length of 8, which is actually extremely short (normally, it is
    somewhere between 100 and 300). Try and change this number of elements in the
    embedding vectors and rerun this code to see what you get as a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we set the embedding size to 8 and then use Keras to add this embedding
    layer to the RNN. This has to be the first layer in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can add the first recurrent layer, and we will use what is called a
    **Gated Recurrent Unit** (**GRU**). Often, you will see that people use what is
    called **LSTM**, but others seem to suggest that the GRU is better because there
    are gates inside LSTM that are redundant. And indeed the simpler code works just
    as well with fewer gates. You could add a thousand more gates to LSTM and that
    still doesn't mean it gets better.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s define our GRU architectures; we say that we want an output dimensionality
    of 16 and we need to return sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the flowchart in *Figure 4*, we want to add a second recurrent
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have the third and final recurrent layer, which will not output a
    sequence because it will be followed by a dense layer; it should only give the
    final output of the GRU and not a whole sequence of outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the output here will be fed into a fully connected or dense layer, which
    is just supposed to output one value for each input sequence. This is processed
    with the sigmoid activation function so it outputs a value between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we say we want to use the Adam optimizer with this learning rate here, and
    the loss function should be the binary cross-entropy between the output from the
    RNN and the actual class value from the training set, which will be a value of
    either 0 or 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, we can just print a summary of what the model looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: So, as you can see, we have the embedding layer, the first recurrent unit, the
    second, third, and dense layer. Note that this doesn't have a lot of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and results analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it''s time to kick off the training process, which is very easy here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s test the trained model against the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's see an example of some misclassified texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'So first, we calculate the predicted classes for the first 1,000 sequences
    in the test set and then we take the actual class values. We compare them and
    get a list of indices where this mismatch exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the cut-off threshold to indicate that all values above `0.5` will be considered
    positive and the others will be considered negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s get the actual class for these 1,000 sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get the incorrect samples from the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we see that there are 122 of these texts that were incorrectly classified;
    that''s 12.1% of the 1,000 texts we calculated here. Let''s look at the first
    misclassified text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s have a look at the model output for this sample as well as the actual
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s test our trained model against a set of new data samples and see
    its results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s convert them to integer tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'And then pad them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s run the model against them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: So, a value close to zero means a negative sentiment and a value that's close
    to 1 means a positive sentiment; finally, these numbers will vary every time you
    train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered an interesting application, which is sentiment analysis.
    Sentiment analysis is used by different companies to track customer's satisfaction
    with their products. Even governments use sentiment analysis solutions to track
    citizen satisfaction about something that they want to do in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we are going to focus on some advanced deep learning architectures
    that can be used for semi-supervised and unsupervised applications.
  prefs: []
  type: TYPE_NORMAL
