["```\n%tensorflow_version 2.x\nimport tensorflow as tf\nimport os\nimport io\ntf.__version__ \n```", "```\nTensorFlow 2.x is selected.\n'2.4.0' \n```", "```\n# Download the zip file\npath_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\norigin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",\n                  extract=True)\n# Unzip the file into a folder\n!unzip $path_to_zip -d data \n```", "```\nArchive:  /root/.keras/datasets/smsspamcollection.zip\n  inflating: data/SMSSpamCollection  \n  inflating: data/readme \n```", "```\n# Let's see if we read the data correctly\nlines = io.open('data/SMSSpamCollection').read().strip().split('\\n')\nlines[0] \n```", "```\n'ham\\tGo until jurong point, crazy.. Available only in bugis n great world' \n```", "```\nspam_dataset = []\nfor line in lines:\n  label, text = line.split('\\t')\n  if label.strip() == 'spam':\n    spam_dataset.append((1, text.strip()))\n  else:\n    spam_dataset.append(((0, text.strip())))\nprint(spam_dataset[0]) \n```", "```\n(0, 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...') \n```", "```\nimport pandas as pd\ndf = pd.DataFrame(spam_dataset, columns=['Spam', 'Message']) \n```", "```\nimport re\ndef message_length(x):\n  # returns total number of characters\n  return len(x)\ndef num_capitals(x):\n **_, count = re.subn(****r'[A-Z]'****,** **''****, x)** **# only works in english**\n  return count\ndef num_punctuation(x):\n  _, count = re.subn(r'\\W', '', x)\n  return count \n```", "```\ndf['Capitals'] = df['Message'].apply(num_capitals)\ndf['Punctuation'] = df['Message'].apply(num_punctuation)\ndf['Length'] = df['Message'].apply(message_length)\ndf.describe() \n```", "```\ntrain=df.sample(frac=0.8,random_state=42)\ntest=df.drop(train.index)\nx_train = train[['Length', 'Capitals', 'Punctuation']]\ny_train = train[['Spam']]\nx_test = test[['Length', 'Capitals', 'Punctuation']]\ny_test = test[['Spam']] \n```", "```\n# Basic 1-layer neural network model for evaluation\ndef make_model(input_dims=3, num_units=12):\n  model = tf.keras.Sequential()\n  # Adds a densely-connected layer with 12 units to the model:\n  model.add(tf.keras.layers.Dense(num_units, \n                                  input_dim=input_dims,\n                                  activation='relu'))\n  # Add a sigmoid layer with a binary output unit:\n  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n  model.compile(loss='binary_crossentropy', optimizer='adam', \n                metrics=['accuracy'])\n  return model \n```", "```\nmodel = make_model()\nmodel.fit(x_train, y_train, epochs=10, batch_size=10) \n```", "```\nTrain on 4459 samples\nEpoch 1/10\n4459/4459 [==============================] - 1s 281us/sample - loss: 0.6062 - accuracy: 0.8141\nEpoch 2/10\nâ€¦\nEpoch 10/10\n4459/4459 [==============================] - 1s 145us/sample - loss: 0.1976 - accuracy: 0.9305 \n```", "```\nmodel.evaluate(x_test, y_test) \n```", "```\n1115/1115 [==============================] - 0s 94us/sample - loss: 0.1949 - accuracy: 0.9336\n[0.19485870356516988, 0.9336323] \n```", "```\ny_train_pred = model.predict_classes(x_train)\n# confusion matrix\ntf.math.confusion_matrix(tf.constant(y_train.Spam), \n                         y_train_pred)\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[3771,   96],\n      [ 186,  406]], dtype=int32)> \n```", "```\nSentence = 'Go until Jurong point, crazy.. Available only in bugis n great world'\nsentence.split() \n```", "```\n['Go',\n 'until',\n 'jurong',\n **'point,',\n 'crazy..',**\n 'Available',\n 'only',\n 'in',\n 'bugis',\n 'n',\n 'great',\n 'world'] \n```", "```\n!pip install stanfordnlp \n```", "```\nImport stanfordnlp as snlp\nen = snlp.download('en') \n```", "```\nen = snlp.Pipeline(lang='en', processors='tokenize') \n```", "```\ntokenized = en(sentence)\nlen(tokenized.sentences) \n```", "```\n2 \n```", "```\nfor snt in tokenized.sentences:\n  for word in snt.tokens:\n    print(word.text)\n  print(\"<End of Sentence>\") \n```", "```\nGo\nuntil\njurong\n**point\n,\ncrazy**\n..\n<End of Sentence>\nAvailable\nonly\nin\nbugis\nn\ngreat\nworld\n<End of Sentence> \n```", "```\njp = snlp.download('ja') \n```", "```\njp = snlp.download('ja')\njp_line = jp(\"![](img/B16252_01_001.png)\") \n```", "```\nfor snt in jp_line.sentences:\n  for word in snt.tokens:\n    print(word.text) \n```", "```\n![](img/B16252_01_002.png) \n```", "```\nen = snlp.Pipeline(lang='en')\ndef word_counts(x, pipeline=en):\n  doc = pipeline(x)\n  count = sum([len(sentence.tokens) for sentence in doc.sentences])\n  return count \n```", "```\ntrain['Words'] = train['Message'].apply(word_counts)\ntest['Words'] = test['Message'].apply(word_counts)\nx_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\ny_train = train[['Spam']]\nx_test = test[['Length', 'Punctuation', 'Capitals' , 'Words']]\ny_test = test[['Spam']]\nmodel = make_model(input_dims=4) \n```", "```\n`/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.` \n```", "```\nmodel.fit(x_train, y_train, epochs=10, batch_size=10)\nTrain on 4459 samples\nEpoch 1/10\n4459/4459 [==============================] - 1s 202us/sample - loss: 2.4261 - accuracy: 0.6961\n...\nEpoch 10/10\n4459/4459 [==============================] - 1s 142us/sample - loss: 0.2061 - accuracy: 0.9312 \n```", "```\ntrain.loc[train.Spam == 1].describe() \n```", "```\ntrain.loc[train.Spam == 0].describe() \n```", "```\n!pip install stopwordsiso \n```", "```\nimport stopwordsiso as stopwords\nstopwords.langs() \n```", "```\nsorted(stopwords.stopwords('en')) \n```", "```\n[\"'ll\",\n \"'tis\",\n \"'twas\",\n \"'ve\",\n '10',\n '39',\n 'a',\n \"a's\",\n 'able',\n 'ableabout',\n 'about',\n 'above',\n 'abroad',\n 'abst',\n 'accordance',\n 'according',\n 'accordingly',\n 'across',\n 'act',\n 'actually',\n 'ad',\n 'added',\n... \n```", "```\nen_sw = stopwords.stopwords('en')\ndef word_counts(x, pipeline=en):\n  doc = pipeline(x)\n  count = 0\n  for sentence in doc.sentences:\n    for token in sentence.tokens:\n        if token.text.lower() not in en_sw:\n          count += 1\n  return count \n```", "```\ntrain['Words'] = train['Message'].apply(word_counts)\ntest['Words'] = test['Message'].apply(word_counts)\nx_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\ny_train = train[['Spam']]\nx_test = test[['Length', 'Punctuation', 'Capitals', 'Words']]\ny_test = test[['Spam']]\nmodel = make_model(input_dims=4)\nmodel.fit(x_train, y_train, epochs=10, batch_size=10) \n```", "```\nEpoch 1/10\n4459/4459 [==============================] - 2s 361us/sample - loss: 0.5186 - accuracy: 0.8652\nEpoch 2/10\n...\nEpoch 9/10\n4459/4459 [==============================] - 2s 355us/sample - loss: 0.1790 - accuracy: 0.9417\nEpoch 10/10\n4459/4459 [==============================] - 2s 361us/sample - loss: 0.1802 - accuracy: 0.9421 \n```", "```\nmodel.evaluate(x_test, y_test) \n```", "```\n1115/1115 [==============================] - 0s 74us/sample - loss: 0.1954 - accuracy: 0.9372\n [0.19537461110027382, 0.93721974] \n```", "```\nen = snlp.Pipeline(lang='en')\ntxt = \"Yo you around? A friend of mine's lookin.\"\npos = en(txt) \n```", "```\ndef print_pos(doc):\n    text = \"\"\n    for sentence in doc.sentences:\n         for token in sentence.tokens:\n            text += token.words[0].text + \"/\" + \\\n                    token.words[0].upos + \" \"\n         text += \"\\n\"\n    return text \n```", "```\nprint(print_pos(pos)) \n```", "```\nYo/PRON you/PRON around/ADV ?/PUNCT \nA/DET friend/NOUN of/ADP mine/PRON 's/PART lookin/NOUN ./PUNCT \n```", "```\nen_sw = stopwords.stopwords('en')\ndef word_counts_v3(x, pipeline=en):\n  doc = pipeline(x)\n  totals = 0.\n  count = 0.\n  non_word = 0.\n  for sentence in doc.sentences:\n    totals += len(sentence.tokens)  # (1)\n    for token in sentence.tokens:\n        if token.text.lower() not in en_sw:\n          if token.words[0].upos not in ['PUNCT', 'SYM']:\n            count += 1.\n          else:\n            non_word += 1.\n  non_word = non_word / totals\n  return pd.Series([count, non_word], index=['Words_NoPunct', 'Punct']) \n```", "```\ntrain_tmp = train['Message'].apply(word_counts_v3)\ntrain = pd.concat([train, train_tmp], axis=1) \n```", "```\ntest_tmp = test['Message'].apply(word_counts_v3)\ntest = pd.concat([test, test_tmp], axis=1) \n```", "```\ntrain.loc[train['Spam']==0].describe() \n```", "```\ntrain.loc[train['Spam']==1].describe() \n```", "```\nx_train = train[['Length', 'Punctuation', 'Capitals', 'Words_NoPunct', 'Punct']]\ny_train = train[['Spam']]\nx_test = test[['Length', 'Punctuation', 'Capitals' , 'Words_NoPunct', 'Punct']]\ny_test = test[['Spam']]\nmodel = make_model(input_dims=5)\n# model = make_model(input_dims=3)\nmodel.fit(x_train, y_train, epochs=10, batch_size=10) \n```", "```\nTrain on 4459 samples\nEpoch 1/10\n4459/4459 [==============================] - 1s 236us/sample - loss: 3.1958 - accuracy: 0.6028\nEpoch 2/10\n...\nEpoch 10/10\n4459/4459 [==============================] - 1s 139us/sample - loss: 0.1788 - **accuracy: 0.9466** \n```", "```\nmodel.evaluate(x_test, y_test) \n```", "```\n1115/1115 [==============================] - 0s 91us/sample - loss: 0.2076 - accuracy: **0.9426**\n[0.20764057086989485, 0.9426009] \n```", "```\ntext = \"Stemming is aimed at reducing vocabulary and aid understanding of morphological processes. This helps people understand the morphology of words and reduce size of corpus.\"\nlemma = en(text) \n```", "```\nlemmas = \"\"\nfor sentence in lemma.sentences:\n        for token in sentence.tokens:\n            lemmas += token.words[0].lemma +\"/\" + \\\n                    token.words[0].upos + \" \"\n        lemmas += \"\\n\"\nprint(lemmas) \n```", "```\nstem/NOUN be/AUX aim/VERB at/SCONJ **reduce/VERB** vocabulary/NOUN and/CCONJ aid/NOUN **understanding/NOUN** of/ADP **morphological/ADJ** process/NOUN ./PUNCT \nthis/PRON help/VERB people/NOUN **understand/VERB** the/DET **morphology/NOUN** of/ADP word/NOUN and/CCONJ **reduce/VERB** size/NOUN of/ADP corpus/ADJ ./PUNCT \n```", "```\ncorpus = [\n          \"I like fruits. Fruits like bananas\",\n          \"I love bananas but eat an apple\",\n          \"An apple a day keeps the doctor away\"\n] \n```", "```\n!pip install sklearn \n```", "```\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names() \n```", "```\n['an',\n 'apple',\n 'away',\n 'bananas',\n 'but',\n 'day',\n 'doctor',\n 'eat',\n 'fruits',\n 'keeps',\n 'like',\n 'love',\n 'the'] \n```", "```\nX.toarray() \n```", "```\narray([[0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0],\n       [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n       [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]]) \n```", "```\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity(X.toarray())\narray([[1\\.        , 0.13608276, 0\\.        ],\n       [0.13608276, 1\\.        , 0.3086067 ],\n       [0\\.        , 0.3086067 , 1\\.        ]]) \n```", "```\nquery = vectorizer.transform([\"apple and bananas\"])\ncosine_similarity(X, query)\narray([[0.23570226],\n       [0.57735027],\n       [0.26726124]]) \n```", "```\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntransformer = TfidfTransformer(smooth_idf=False)\ntfidf = transformer.fit_transform(X.toarray())\npd.DataFrame(tfidf.toarray(), \n             columns=vectorizer.get_feature_names()) \n```", "```\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn. pre-processing import LabelEncoder\ntfidf = TfidfVectorizer(binary=True)\nX = tfidf.fit_transform(train['Message']).astype('float32')\nX_test = tfidf.transform(test['Message']).astype('float32')\nX.shape \n```", "```\n(4459, 7741) \n```", "```\n_, cols = X.shape\nmodel2 = make_model(cols)  # to match tf-idf dimensions\ny_train = train[['Spam']]\ny_test = test[['Spam']]\nmodel2.fit(X.toarray(), y_train, epochs=10, batch_size=10) \n```", "```\nTrain on 4459 samples\nEpoch 1/10\n4459/4459 [==============================] - 2s 380us/sample - loss: 0.3505 - accuracy: 0.8903\n...\nEpoch 10/10\n4459/4459 [==============================] - 1s 323us/sample - loss: 0.0027 - accuracy: 1.0000 \n```", "```\nmodel2.evaluate(X_test.toarray(), y_test) \n```", "```\n1115/1115 [==============================] - 0s 134us/sample - loss: 0.0581 - accuracy: 0.9839\n[0.05813191874545786, 0.9838565] \n```", "```\ny_test_pred = model2.predict_classes(X_test.toarray())\ntf.math.confusion_matrix(tf.constant(y_test.Spam), \n                         y_test_pred)\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[958,   2],\n       [ 16, 139]], dtype=int32)> \n```", "```\n!pip install gensim \n```", "```\nfrom gensim.models.word2vec import Word2Vec\nimport gensim.downloader as api\nmodel_w2v = api.load(\"word2vec-google-news-300\") \n```", "```\nmodel_w2v.most_similar(\"cookies\",topn=10) \n```", "```\n[('cookie', 0.745154082775116),\n ('oatmeal_raisin_cookies', 0.6887780427932739),\n ('oatmeal_cookies', 0.662139892578125),\n ('cookie_dough_ice_cream', 0.6520504951477051),\n ('brownies', 0.6479344964027405),\n ('homemade_cookies', 0.6476464867591858),\n ('gingerbread_cookies', 0.6461867690086365),\n ('Cookies', 0.6341644525527954),\n ('cookies_cupcakes', 0.6275068521499634),\n ('cupcakes', 0.6258294582366943)] \n```", "```\nmodel_w2v.doesnt_match([\"USA\",\"Canada\",\"India\",\"Tokyo\"]) \n```", "```\n'Tokyo' \n```", "```\nking = model_w2v['king']\nman = model_w2v['man']\nwoman = model_w2v['woman']\nqueen = king - man + woman  \nmodel_w2v.similar_by_vector(queen) \n```", "```\n[('king', 0.8449392318725586),\n ('queen', 0.7300517559051514),\n ('monarch', 0.6454660892486572),\n ('princess', 0.6156251430511475),\n ('crown_prince', 0.5818676948547363),\n ('prince', 0.5777117609977722),\n ('kings', 0.5613663792610168),\n ('sultan', 0.5376776456832886),\n ('Queen_Consort', 0.5344247817993164),\n ('queens', 0.5289887189865112)] \n```"]