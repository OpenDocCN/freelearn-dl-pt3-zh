["```\nimport gym \n```", "```\nenv = gym.make('Blackjack-v0') \n```", "```\nprint(env.reset()) \n```", "```\n(15, 9, True) \n```", "```\nprint(env.action_space) \n```", "```\nDiscrete(2) \n```", "```\nimport gym\nimport pandas as pd\nfrom collections import defaultdict \n```", "```\nenv = gym.make('Blackjack-v0') \n```", "```\ndef policy(state):\n    return 0 if state[0] > 19 else 1 \n```", "```\nstate = env.reset()\nprint(state) \n```", "```\n(20, 5, False) \n```", "```\nprint(policy(state)) \n```", "```\n0 \n```", "```\nnum_timesteps = 100 \n```", "```\ndef generate_episode(policy): \n```", "```\n episode = [] \n```", "```\n state = env.reset() \n```", "```\n for t in range(num_timesteps): \n```", "```\n action = policy(state) \n```", "```\n next_state, reward, done, info = env.step(action) \n```", "```\n episode.append((state, action, reward)) \n```", "```\n if done:\n            break\n\n        state = next_state\n    return episode \n```", "```\nprint(generate_episode(policy)) \n```", "```\n[((10, 2, False), 1, 0), ((20, 2, False), 0, 1.0)] \n```", "```\ntotal_return = defaultdict(float)\nN = defaultdict(int) \n```", "```\nnum_iterations = 500000 \n```", "```\nfor i in range(num_iterations): \n```", "```\n episode = generate_episode(policy) \n```", "```\n states, actions, rewards = zip(*episode) \n```", "```\n for t, state in enumerate(states): \n```", "```\n R = (sum(rewards[t:])) \n```", "```\n total_return[state] =  total_return[state] + R \n```", "```\n N[state] =  N[state] + 1 \n```", "```\ntotal_return = pd.DataFrame(total_return.items(),columns=['state', 'total_return']) \n```", "```\nN = pd.DataFrame(N.items(),columns=['state', 'N']) \n```", "```\ndf = pd.merge(total_return, N, on=\"state\") \n```", "```\ndf.head(10) \n```", "```\ndf['value'] = df['total_return']/df['N'] \n```", "```\ndf.head(10) \n```", "```\ndf[df['state']==(21,9,False)]['value'].values \n```", "```\narray([1.0]) \n```", "```\ndf[df['state']==(5,8,False)]['value'].values \n```", "```\narray([-1.0]) \n```", "```\nfor i in range(num_iterations):\n\n    episode = generate_episode(env,policy)\n    states, actions, rewards = zip(*episode)\n    for t, state in enumerate(states):\n **if** **state** **not****in** **states[****0****:t]:**\n            R = (sum(rewards[t:]))\n            total_return[state] = total_return[state] + R\n            N[state] = N[state] + 1 \n```", "```\ndef epsilon_greedy_policy(state, epsilon):\n    if random.uniform(0,1) < epsilon:\n         return env.action_space.sample()\n    else:\n         return max(list(range(env.action_space.n)), key = lambda x: q[(state,x)]) \n```", "```\nimport gym\nimport pandas as pd\nimport random\nfrom collections import defaultdict \n```", "```\nenv = gym.make('Blackjack-v0') \n```", "```\nQ = defaultdict(float) \n```", "```\ntotal_return = defaultdict(float) \n```", "```\nN = defaultdict(int) \n```", "```\ndef epsilon_greedy_policy(state,Q): \n```", "```\n epsilon = 0.5 \n```", "```\n if random.uniform(0,1) < epsilon:\n        return env.action_space.sample()\n    else:\n        return max(list(range(env.action_space.n)), key = lambda x: Q[(state,x)]) \n```", "```\nnum_timesteps = 100 \n```", "```\ndef generate_episode(Q): \n```", "```\n episode = [] \n```", "```\n state = env.reset() \n```", "```\n for t in range(num_timesteps): \n```", "```\n action = epsilon_greedy_policy(state,Q) \n```", "```\n next_state, reward, done, info = env.step(action) \n```", "```\n episode.append((state, action, reward)) \n```", "```\n if done:\n            break\n\n        state = next_state\n    return episode \n```", "```\nnum_iterations = 500000 \n```", "```\nfor i in range(num_iterations): \n```", "```\n episode = generate_episode(Q) \n```", "```\n all_state_action_pairs = [(s, a) for (s,a,r) in episode] \n```", "```\n rewards = [r for (s,a,r) in episode] \n```", "```\n for t, (state, action,_) in enumerate(episode): \n```", "```\n if not (state, action) in all_state_action_pairs[0:t]: \n```", "```\n R = sum(rewards[t:]) \n```", "```\n total_return[(state,action)] = total_return[(state,action)] + R \n```", "```\n N[(state, action)] += 1 \n```", "```\n Q[(state,action)] = total_return[(state, action)] / N[(state, action)] \n```", "```\ndf = pd.DataFrame(Q.items(),columns=['state_action pair','value']) \n```", "```\ndf.head(11) \n```", "```\ndf[124:126] \n```"]