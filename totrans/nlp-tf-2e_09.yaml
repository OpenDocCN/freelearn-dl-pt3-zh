- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequence-to-Sequence Learning – Neural Machine Translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequence-to-sequence learning is the term used for tasks that require mapping
    an arbitrary-length sequence to another arbitrary-length sequence. This is one
    of the most sophisticated tasks in NLP, which involves learning many-to-many mappings.
    Examples of this task include **Neural Machine Translation** (**NMT**) and creating
    chatbots. NMT is where we translate a sentence from one language (source language)
    to another (target language). Google Translate is an example of an NMT system.
    Chatbots (that is, software that can communicate with/answer a person) are able
    to converse with humans in a realistic manner. This is especially useful for various
    service providers, as chatbots can be used to find answers to easily solvable
    questions that customers might have, instead of redirecting them to human operators.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to implement an NMT system. However, before
    diving directly into such recent advances, we will first briefly visit some **Statistical
    Machine Translation** (**SMT**) methods, which preceded NMT and were the state-of-the-art
    systems until NMT caught up. Next, we will walk through the steps required for
    building an NMT. Finally, we will learn how to implement a real NMT system that
    translates from German to English, step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this chapter will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief historical tour of machine translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding neural machine translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for the NMT system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the NMT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The BLEU score – evaluating the machine translation systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing attention patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference with NMT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other applications of Seq2Seq models – chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans often communicate with each other by means of a language, compared to
    other communication methods (for example, gesturing). Currently, more than 6,000
    languages are spoken worldwide. Furthermore, learning a language to a level where
    it is easily understandable to a native speaker of that language is a difficult
    task to master. However, communication is essential for sharing knowledge, socializing,
    and expanding your network. Therefore, language acts as a barrier to communicating
    with people in different parts of the world. This is where **Machine Translation**
    (**MT**) comes in. MT systems allow the user to input a sentence in their own
    tongue (known as the source language) and output a sentence in a desired target
    language.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with MT can be formulated as follows. Say we are given a sentence
    (or a sequence of words) *W*[s] belonging to a source language *S*, defined by
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_09_002.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The source language would be translated to a sentence ![](img/B14070_09_003.png),
    where *T* is the target language and is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_09_005.png).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_006.png) is obtained through the MT system, which outputs
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B14070_09_008.png) is the pool of possible translation candidates
    found by the algorithm for the source sentence. Also, the best candidate from
    the pool of candidates is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_009.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_09_073.png) is the model parameters. During training, we
    optimize the model to maximize the probability of some known target translations
    for a set of corresponding source translations (that is, training data).
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed the formal setup of the language translation problem
    that we’re interested in solving. Next, we will walk through the history of MT
    to get a feel of how people tried solving this in the early days.
  prefs: []
  type: TYPE_NORMAL
- en: A brief historical tour of machine translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will discuss the history of MT. The inception of MT involved rule-based
    systems. Then, more statistically sound MT systems emerged. **Statistical Machine
    Translation** (**SMT**) used various measures of statistics of a language to produce
    translations to another language. Then came the era of NMT. NMT currently holds
    state-of-the-art performance in most machine learning tasks compared with other
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NMT came long after statistical machine learning, and statistical machine learning
    has been around for more than half a century now. The inception of SMT methods
    dates back to 1950-60, when during one of the first recorded projects, the *Georgetown-IBM
    experiment*, more than 60 Russian sentences were translated to English. o give
    some perspective, this attempt is almost as old as the invention of the transistor.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the initial techniques for MT was word-based machine translation. This
    system performed word-to-word translations using bilingual dictionaries. However,
    as you can imagine, this method has serious limitations. The obvious limitation
    is that word-to-word translation is not a one-to-one mapping between different
    languages. In addition, word-to-word translation may lead to incorrect results
    as it does not consider the context of a given word. The translation of a given
    word in the source language can change depending on the context in which it is
    used. To understand this with a concrete example, let’s look at the translation
    example from English to French in *Figure 9.1*. You can see that in the given
    two English sentences, a single word changes. However, this creates drastic changes
    in the translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rule-based translation](img/B14070_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Translations (English to French) between languages are not one-to-one
    mappings between words'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the 1960s, the **Automatic Language Processing Advisory Committee** (**ALPAC**)
    released a report, *Languages and machines: computers in translation and linguistics,*
    *National Academy of the Sciences (1966)*, on MT’s prospects. The conclusion was
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*There is no immediate or predictable prospect of useful machine translation.*'
  prefs: []
  type: TYPE_NORMAL
- en: This was because MT was slower, less accurate, and more expensive than human
    translation at the time. This delivered a huge blow to MT advancements, and almost
    a decade passed in silence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next came corpora-based MT, where an algorithm was trained using tuples of
    source sentences, and the corresponding target sentence was obtained through a
    parallel corpus, that is, the parallel corpus would be of the format `[(<source_sentence_1>,
    <target_sentence_1>), (<source_sentence_2>, <target_sentence_2>), …]`. The parallel
    corpus is a large text corpus formed as tuples, consisting of text from the source
    language and the corresponding translation of that text. An illustration of this
    is shown in *Table 9.1*. It should be noted that building a parallel corpus is
    much easier than building bilingual dictionaries and they are more accurate because
    the training data is richer than word-to-word training data. Furthermore, instead
    of directly relying on manually created bilingual dictionaries, a bilingual dictionary
    (that is, the transition models) of two languages can be built using the parallel
    corpus. A transition model shows how likely a target word/phrase is to be the
    correct translation, given the current source word/phrase. In addition to learning
    the transition model, corpora-based MT also learns the word alignment models.
    A word alignment model can represent how words in a phrase from the source language
    correspond to the translation of that phrase. An example of parallel corpora and
    a word alignment model is depicted in *Figure 9.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rule-based translation](img/B14070_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Word alignment between two different languages'
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustration of an example parallel corpora is shown in *Table 9.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Source language sentences (English)** | **Target language sentences (French)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| I went home | Je suis allé à la maison |'
  prefs: []
  type: TYPE_TB
- en: '| John likes to play guitar | John aime jouer de la guitare |'
  prefs: []
  type: TYPE_TB
- en: '| He is from England | Il est d’Angleterre |'
  prefs: []
  type: TYPE_TB
- en: '| … | …. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9.1: Parallel corpora for English and French sentences'
  prefs: []
  type: TYPE_NORMAL
- en: Another approach was interlingual machine translation, which involved translating
    the source sentence into a language-neutral **interlingua** (that is, a metalanguage),
    and then generating the translated sentence out of the interlingua. More specifically,
    an interlingual machine translation system consists of two important components,
    an analyzer and a synthesizer. The analyzer will take the source sentence and
    identify agents (for example, nouns), actions (for example, verbs), and so on,
    and also how they interact with each other. Next, these identified elements are
    represented by means of an interlingual lexicon. An example of an interlingual
    lexicon can be made with the synsets (that is, the group of synonyms sharing a
    common meaning) available in WordNet. Then, from this interlingual representation,
    the synthesizer will create the translation. Since the synthesizer knows the nouns,
    verbs, and so on through the interlingual representation, it can generate the
    translation in the target language by incorporating language-specific grammar
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Machine Translation (SMT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, more statistically sound systems started emerging. One of the pioneering
    models of this era was IBM Models 1-5, which did word-based translation. However,
    as we discussed earlier, word translations do not match one to one from the source
    language to a target language (for example, compound words and morphology). Eventually,
    researchers started experimenting with phrase-based translation systems, which
    made some notable advances in machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Phrase-based translation works in a similar way to word-based translation, except
    that it uses phrases of a language as the atomic units of translation instead
    of individual words. This is a more sensible approach as it makes modeling the
    one-to-many, many-to-one, or many-to-many relationships between words easier.
    The main goal of phrase-based translation is to learn a phrase-translation model
    that contains a probability distribution of different candidate target phrases
    for a given source phrase. As you can imagine, this method involves maintaining
    huge databases of various phrases in two languages. A reordering step for phrases
    is also performed as there is no monotonic ordering of words between a sentence
    from one language and one in another.
  prefs: []
  type: TYPE_NORMAL
- en: An example of this is shown in *Figure 9.2*; if the words were monotonically
    ordered between languages, there would not be crosses between word mappings.
  prefs: []
  type: TYPE_NORMAL
- en: One of the limitations of this approach is that the decoding process (finding
    the best target phrase for a given source phrase) is expensive. This is due to
    the size of the phrase database, as well as the fact that a source phrase often
    contains multiple target language phrases. To alleviate the burden, syntax-based
    translations arose.
  prefs: []
  type: TYPE_NORMAL
- en: 'In syntax-based translation, the source sentence is represented by a syntax
    tree. In *Figure 9.3*, **NP** represents a noun phrase, **VP** a verb phrase,
    and **S** a sentence. Then a **reordering phase** takes place, where the tree
    nodes are reordered to change the order of subject, verb, and object, depending
    on the target language. This is because the sentence structure can change depending
    on the language (for example, in English it is *subject-verb-object*, whereas
    in Japanese it is *subject-object-verb*). The reordering is decided according
    to something known as the **r-table**. The r-table contains the likelihood probabilities
    for the tree nodes to be changed to some other order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical Machine Translation (SMT)](img/B14070_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Syntax tree for a sentence'
  prefs: []
  type: TYPE_NORMAL
- en: An **insertion phase** then takes place. In the insertion phase, we stochastically
    insert a word into each node of the tree. This is due to the assumption that there
    is an invisible `NULL` word, and it generates target words at random positions
    of the tree. Also, the probability of inserting a word is determined by something
    called the **n-table**, which is a table that contains probabilities of inserting
    a particular word into the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the **translation phase** occurs, where each leaf node is translated to
    the target word in a word-by-word manner. Finally, the translated sentence is
    read off the syntax tree, to construct the target sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Machine Translation (NMT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, around the year 2014, NMT systems were introduced. NMT is an end-to-end
    system that takes a full sentence as an input, performs certain transformations,
    and then outputs the translated sentence for the corresponding source sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, NMT eliminates the need for the feature engineering required for
    machine translation, such as building phrase translation models and building syntax
    trees, which is a big win for the NLP community. Also, NMT has outperformed all
    the other popular MT techniques in a very short period, just two to three years.
    In *Figure 9.4*, we depict the results of various MT systems reported in the MT
    literature. For example, 2016 results are obtained from Sennrich and others in
    their paper *Edinburgh Neural Machine Translation Systems for WMT 16, Association
    for Computational Linguistics, Proceedings of the First Conference on Machine
    Translation, August 2016: 371-376*, and from Williams and others in their paper
    *Edinburgh’s Statistical Machine Translation Systems for WMT16, Association for
    Computational Linguistics, Proceedings of the First Conference on Machine Translation,
    August 2016: 399-410*. All the MT systems are evaluated with the BLEU score. The
    BLEU score denotes the number of n-grams (for example, unigrams and bigrams) of
    candidate translation that matched in the reference translation. So the higher
    the BLEU score, the better the MT system. We’ll discuss the BLEU metric in detail
    later in the chapter. There is no need to highlight that NMT is a clear-cut winner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural Machine Translation (NMT)](img/B14070_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Comparison of statistical machine translation system to NMT systems.
    Courtesy of Rico Sennrich'
  prefs: []
  type: TYPE_NORMAL
- en: A case study assessing the potential of NMT systems is available in *Is Neural
    Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions,
    Junczys-Dowmunt, Hoang and Dwojak, Proceedings of the Ninth International Workshop
    on Spoken Language Translation*, *Seattle (2016)*.
  prefs: []
  type: TYPE_NORMAL
- en: The study looks at the performance of different systems on several translation
    tasks between various languages (English, Arabic, French, Russian, and Chinese).
    The results also support that NMT systems (NMT 1.2M and NMT 2.4M) perform better
    than SMT systems (PB-SMT and Hiero).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.5* shows several statistics for a set from a 2017 state-of-the-art
    machine translator. This is from a presentation, *State of the Machine Translation,
    Intento, Inc, 2017*, produced by Konstantin Savenkov, cofounder and CEO of Intento.
    We can see that the performance of the MT produced by DeepL ([https://www.deepl.com](https://www.deepl.com))
    appears to be competing closely with other MT giants, including Google. The comparison
    includes MT systems such as DeepL (NMT), Google (NMT), Yandex (NMT-SMT hybrid),
    Microsoft (has both SMT and NMT), IBM (SMT), Prompt (rule-based), and SYSTRAN
    (rule-based/SMT hybrid). The graph clearly shows that NMT systems are leading
    the current MT advancements. The LEPOR score is used to assess different systems.
    LEPOR is a more advanced metric than BLEU, and it attempts to solve the **language
    bias problem**. The language bias problem refers to the phenomenon that some evaluation
    metrics (such as BLEU) perform well for certain languages, but perform poorly
    for others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it should also be noted that the results do contain some bias due
    to the averaging mechanism used in this comparison. For example, Google Translate
    has been averaged over a larger set of languages (including difficult translation
    tasks), whereas DeepL has been averaged over a smaller and relatively easier subset
    of languages. Therefore, we should not conclude that the DeepL MT system is always
    better than the Google MT system. Nevertheless, the overall results provide a
    general comparison of the performance of the current NMT and SMT systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural Machine Translation (NMT)](img/B14070_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Performance of various MT systems. Courtesy of Intento, Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: We saw that NMT has already outperformed SMT systems in very few years, and
    is the current state of the art. We will now move on to discussing the details
    and architecture of an NMT system. Finally, we will be implementing an NMT system
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding neural machine translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an appreciation for how machine translation has evolved over
    time, let’s try to understand how state-of-the-art NMT works. First, we will take
    a look at the model architecture used by neural machine translators and then move
    on to understanding the actual training algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition behind NMT systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s understand the intuition underlying an NMT system’s design. Say
    you are a fluent English and German speaker and were asked to translate the following
    sentence into German:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I went home*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This sentence translates to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ich ging nach Hause*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it might not have taken more than a few seconds for a fluent person
    to translate this, there is a certain process that produces the translation. First,
    you read the English sentence, and then you create a thought or concept about
    what this sentence represents or implies, in your mind. And finally, you translate
    the sentence into German. The same idea is used for building NMT systems (see
    *Figure 9.6*). The encoder reads the source sentence (that is, similar to you
    reading the English sentence). Then the encoder outputs a context vector (the
    context vector corresponds to the thought/concept you imagined after reading the
    sentence). Finally, the decoder takes in the context vectors and outputs the translation
    in German:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Intuition behind NMT](img/B14070_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Conceptual architecture of an NMT system'
  prefs: []
  type: TYPE_NORMAL
- en: NMT architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will look at the architecture in more detail. The sequence-to-sequence
    approach was originally proposed by Sutskever, Vinyals, and Le in their paper
    *Sequence to Sequence Learning with Neural Networks, Proceedings of the 27th International
    Conference on Neural Information Processing Systems - Volume 2: 3104-3112.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the diagram in *Figure 9.6*, we can see that there are two major components
    in the NMT architecture. These are called the encoder and decoder. In other words,
    NMT can be seen as an encoder-decoder architecture. The **encoder** converts a
    sentence from a given source language into a thought vector (i.e. a contextualized
    representation), and the **decoder** decodes or translates the thought into a
    target language. As you can see, this shares some features with the interlingual
    machine translation method we briefly talked about. This explanation is illustrated
    in *Figure 9.7*. The left-hand side of the context vector denotes the encoder
    (which takes a source sentence in word by word to train a time-series model).
    The right-hand side denotes the decoder, which outputs word by word (while using
    the previous word as the current input) the corresponding translation of the source
    sentence. We will also use embedding layers (for both the source and target languages)
    where the semantics of the individual tokens will be learned and fed as inputs
    to the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NMT architecture](img/B14070_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Unrolling the source and target sentences over time'
  prefs: []
  type: TYPE_NORMAL
- en: 'With a basic understanding of what NMT looks like, let’s formally define the
    objective of the NMT. The ultimate objective of an NMT system is to maximize the
    log likelihood, given a source sentence *x*[s] and its corresponding *y*[t]. That
    is, to maximize the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_010.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N* refers to the number of source and target sentence inputs we have
    as training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, during inference, for a given source sentence, ![](img/B14070_09_011.png),
    we will find the ![](img/B14070_09_012.png) translation using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_013.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_09_014.png) is the predicted token at the *i*^(th) time
    step and ![](img/B14070_09_015.png) is the set of possible candidate sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Before we examine each part of the NMT architecture, let’s define the mathematical
    notation to understand the system more concretely. As our sequential model, we
    will choose a **Gated Recurrent Unit** (**GRU**), as it is simpler than an LSTM
    and performs comparatively well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define the encoder GRU as ![](img/B14070_09_016.png) and the decoder
    GRU as ![](img/B14070_09_017.png). At the time step ![](img/B14070_09_018.png),
    let’s define the output state of a general GRU as *h*[t]. That is, feeding the
    input *x*[t] into the GRU produces *h*[t]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we will talk about the embedding layer, the encoder, the context vector,
    and finally, the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: The embedding layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already seen the power of word embeddings. Here, we can also leverage
    embeddings to improve model performance. We will be using two-word embedding layers,
    ![](img/B14070_09_021.png), for the source language and ![](img/B14070_09_022.png)
    for the target language. So, instead of feeding *x*[t] directly into the GRU,
    we will be getting ![](img/B14070_09_023.png). However, to avoid excessive notation,
    we will assume ![](img/B14070_09_024.png).
  prefs: []
  type: TYPE_NORMAL
- en: The encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the encoder is responsible for generating a thought vector
    or a context vector that represents what is meant by the source language. For
    this, we will use a GRU-based network (see *Figure 9.8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: A GRU cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder is initialized with *h* at time step 0 (*h*[0]) with a zero vector
    by default. The encoder takes a sequence of words, ![](img/B14070_09_025.png),
    as the input and calculates a context vector, ![](img/B14070_09_026.png), where
    *v* is the final external hidden state obtained after processing the final element
    ![](img/B14070_09_027.png), of the sequence *x*[s]. We represent this as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_028.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_09_029.png)'
  prefs: []
  type: TYPE_IMG
- en: The context vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of the context vector (*v*) is to represent a sentence of a source
    language concisely. Also, in contrast to how the encoder’s state is initialized
    (that is, it is initialized with zeros), the context vector becomes the initial
    state for the decoder GRU. In other words, the decoder GRU doesn’t start with
    an initial state of zeros, but with the context vector as its initial state. This
    creates a linkage between the encoder and the decoder and makes the whole model
    end-to-end differentiable. We will talk about this in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decoder is responsible for decoding the context vector into the desired
    translation. Our decoder is an RNN as well. Though it is possible for the encoder
    and decoder to share the same set of weights, it is usually better to use two
    different networks for the encoder and the decoder. This increases the number
    of parameters in our model, allowing us to learn the translations more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the decoder’s states are initialized with the context vector, i.e. ![](img/B14070_09_030.png),
    as shown here: ![](img/B14070_09_031.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![](img/B14070_09_032.png) is the initial state vector of the decoder
    (![](img/B14070_09_033.png)).
  prefs: []
  type: TYPE_NORMAL
- en: This (*v*) is the crucial link that connects the encoder with the decoder to
    form an end-to-end computational chain (see in *Figure 9.6* that the only thing
    shared by the encoder and decoder is *v*). Also, this is the only piece of information
    that is available to the decoder about the source sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we will compute the *m*^(th) prediction of the translated sentence with
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_034.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_09_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The full NMT system with the details of how the GRU cell in the encoder connects
    to the GRU cell in the decoder, and how the softmax layer is used to output predictions,
    is shown in *Figure 9.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: The encoder-decoder architecture with the GRUs. Both the encoder
    and the decoder have a separate GRU component. Additionally, the decoder has a
    fully-connected (dense) layer and a softmax layer that produce the final predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will go through the steps required to prepare data for
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data for the NMT system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will understand the data and learn about the process for
    preparing data for training and predicting from the NMT system. First, we will
    talk about how to prepare training data (that is, the source sentence and target
    sentence pairs) to train the NMT system, followed by inputting a given source
    sentence to produce the translation of the source sentence.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset we’ll be using for this chapter is the WMT-14 English-German translation
    data from [https://nlp.stanford.edu/projects/nmt/](https://nlp.stanford.edu/projects/nmt/).
    There are ~4.5 million sentence pairs available. However, we will use only 250,000
    sentence pairs due to computational feasibility. The vocabulary consists of the
    50,000 most common English words and the 50,000 most common German words, and
    words not found in the vocabulary will be replaced with a special token, `<unk>`.
    You will need to download the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train.de` – File containing German sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train.en` – File containing English sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab.50K.de` – File containing German vocabulary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab.50K.en` – File containing English vocabulary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train.de` and `train.en` contain parallel sentences in German and English,
    respectively. Once downloaded we will load the sentences as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you print the data you just loaded, for the two languages, you would have
    sentences like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Adding special tokens
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next step is to add a few special tokens to the start and end of our sentences.
    We will add `<s>` to mark the start of a sentence and `</s>` to mark the end of
    a sentence. We can easily achieve this using the following list comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a very important step for Seq2Seq models. `<s>` and `</s>` tokens serve
    an extremely important role during model inference. As you will see, at inference
    time, we will be using the decoder to predict one word at a time, by using the
    output of the previous time step as an input. This way we can predict for an arbitrary
    number of time steps. Using `<s>` as the starting token gives us a way to signal
    to the decoder that it should start predicting tokens from the target language.
    Next, if we do not use the `</s>` token to mark the end of a sentence, we cannot
    signal the decoder to end a sentence. This can lead the model to enter an infinite
    loop of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting training, validation, and testing datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to split our dataset into three parts: a training set, a validation
    set, and a testing set. Specifically, let’s use 80% of sentences to train the
    model, 10% as validation data, and the remaining 10% as testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Defining sequence lengths for the two languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A key statistic we have to understand at this point is how long, generally,
    the sentences in our corpus are. It is quite likely that the two languages will
    have different sentence lengths. To learn the statistics of this, we’ll be using
    the pandas library in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are first converting the `train_en_sentences` to a `pd.Series` object.
    `pd.Series` is an indexed series of values (an array). Here, each value is a list
    of tokens belonging to each sentence. Calling `.str.len()` will give us the length
    of each list of tokens. Finally, the `describe` method will give important statistics
    such as mean, standard deviation, and percentiles. Here. we are specifically asking
    for 5%, 50%, and 95% percentiles.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are only using the training data for this calculation. If you include
    validation or test datasets in these calculations, we may be leaking data about
    validation and test data. Therefore, it’s best to only use the training dataset
    for these calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result from the previous code gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can get the same for the German sentences the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here we can see that 95% of English sentences have 53 tokens, where 95% of German
    sentences have 47 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Padding the sentences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we need to pad our sentences. For this, we will use the `pad_sequences()`
    function provided in Keras. This function takes in values for the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` – A list of strings/IDs representing the text corpus. Each document
    can either be a list of strings or a list of integers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxlen` – The maximum length to pad for (defaults to `None`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` – The type of data (defaults to `''int32''`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` – The side to pad short sequences (defaults to `''pre''`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncating` – The side to truncate long sequences from (defaults to `''pre''`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value` – The values to pad with (defaults to `0.0`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use this function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We are padding all of the training, validation, and test sentences in both English
    and German. We will use the recently found sequence lengths as the padding/truncating
    length.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reversing the source sentence**'
  prefs: []
  type: TYPE_NORMAL
- en: We can also perform a special trick on the source sentences. Say we have the
    sentence *ABC* in the source language, which we want to translate to ![](img/B14070_09_036.png)
    in the target language. We will first reverse the source sentences so that the
    sentence *ABC* is read as *CBA*. This means that in order to translate *ABC* to
    ![](img/B14070_09_036.png), we need to feed in *CBA*. This improves the performance
    of our model significantly, especially when the source and target languages share
    the same sentence structure (for example, *subject-verb-object*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to understand why this helps. Mainly, it helps to build good *communication*
    between the encoder and the decoder. Let’s start from the previous example. We
    will concatenate the source and target sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you calculate the distance (that is, the number of words separating two
    words) from *A* to ![](img/B14070_09_039.png) or *B* to ![](img/B14070_09_040.png),
    they will be the same. However, consider this when you reverse the source sentence,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_041.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *A* is very close to ![](img/B14070_09_039.png) and so on. Also, to build
    good translations, building good communications at the very start is important.
    This simple trick can possibly help NMT systems to improve their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the source sentence reversing step is a subjective preprocessing step.
    This might not be necessary for some translational tasks. For example, if your
    translation task is to translate from Japanese (which is often written in *subject-object-verb*
    format) to Filipino (often written *verb-subject-object*), then reversing the
    source sentence might actually cause harm rather than helping. This is because
    by reversing the text in Japanese, you are increasing the distance between the
    starting element of the target sentence (that is, the verb (Japanese)) and the
    corresponding source language entity (that is, the verb (Filipino)).
  prefs: []
  type: TYPE_NORMAL
- en: Next let’s define our encoder-decoder model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will define the model from end to end.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to implement an encoder-decoder based NMT model equipped with additional
    techniques to boost performance. Let’s start off by converting our string tokens
    to IDs.
  prefs: []
  type: TYPE_NORMAL
- en: Converting tokens to IDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we jump to the model, we have one more text processing operation remaining,
    that is, converting the processed text tokens into numerical IDs. We are going
    to use a `tf.keras.layers.Layer` to do this. Particularly, we’ll be using the
    `StringLookup` layer to create a layer in our model that converts each token into
    a numerical ID. As the first step, let us load the vocabulary files provided in
    the data. Before doing so, we will define the variable `n_vocab` to denote the
    size of the vocabulary for each language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Originally, each vocabulary contains 50,000 tokens. However, we’ll take only
    half of this to reduce the memory requirement. Note that we allow one extra token
    as there’s a special token `<unk>` to denote **out-of-vocabulary** (**OOV**) words.
    With a 50,000-token vocabulary, it is quite easy to run out of memory due to the
    size of the final prediction layer we’ll build. While cutting back on the size
    of the vocabulary, we have to make sure that we preserve the most common 25,000
    words. Fortunately, each vocabulary file is organized such that words are ordered
    by their frequency of occurrence (high to low). Therefore, we just need to read
    the first 25,001 lines of text from the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we do the same for the German vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of the vocabularies contain the special OOV token `<unk>` as the first
    line. We’ll pop that out of the `en_vocabulary` and `de_vocabulary` lists as we
    need this for the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s how we can define our English `StringLookup` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s understand the arguments provided to this layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vocabulary` – Contains a list of words that are found in the corpus (except
    certain special tokens that will be discussed below)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oov_token` – A special out-of-vocabulary token that will be used to replace
    tokens not listed in the vocabulary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_token` – A special token that will be used to mask inputs (e.g. uninformative
    padded tokens)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_max_tokens` – If padding should occur to bring arbitrary-length sequences
    in a batch of data to the same length'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarly, we define a lookup layer for the German language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: With the groundwork laid out, we can start building the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start the encoder with an input layer. The input layer will take in a batch
    of sequences of tokens. Each sequence of tokens is `n_en_seq_length` elements
    long. Remember that we padded or truncated the sentences to make sure all of them
    have a fixed length of `n_en_seq_length`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we use the previously defined `StringLookup` layer to convert the string
    tokens into word IDs. As we saw, the `StringLookup` layer can take a list of unique
    words (i.e. a vocabulary) and create a lookup operation to convert a given token
    into a numerical ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'With the tokens converted into IDs, we route the generated word IDs to a token
    embedding layer. We pass in the size of the vocabulary (derived from the `en_lookup_layer`''s
    `get_vocabulary()` method) and the embedding size (128) and finally we ask the
    layer to mask any zero-valued inputs as they don’t contain any information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the embedding layer is stored in `encoder_emb_out`. Next we define
    a GRU layer to process the sequence of English token embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note how we are setting both the `return_sequences` and `return_state` arguments
    to `True`. To recap, `return_sequences` returns the full sequence of hidden states
    as the output (instead of returning only the last), where `return_state` returns
    the last state of the model as an additional output. We need both these outputs
    to build the rest of our model. For example, we need to pass the last state of
    the encoder to the decoder as the initial state. For that, we need the last state
    of the encoder (stored in `encoder_gru_last_state`). We will discuss the purpose
    of this in more detail as we go. We now have everything to define the encoder
    part of our model. It takes in a batch of sequences of string tokens and returns
    the full sequence of GRU hidden states as the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: With the encoder defined, let’s build the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our decoder will be more complex than the encoder. The objective of the decoder
    is, given the last encoder state and the previous token the decoder predicted,
    predict the next token. For example, for the German sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*<s> ich ging zum Laden </s>*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input** | <s> | ich | ging | zum | Laden |'
  prefs: []
  type: TYPE_TB
- en: '| **Output** | ich | ging | zum | Laden | </s> |'
  prefs: []
  type: TYPE_TB
- en: 'This technique is known as **teacher forcing**. In other words, the decoder
    is leveraging previous tokens of the target itself to predict the next token.
    This makes the translation task easier for the model. We can understand this phenomenon
    as follows. Say a teacher asks a kindergarten student to complete the following
    sentence, given just the first word:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I ___ ____ ___ ___ ____ ____*'
  prefs: []
  type: TYPE_NORMAL
- en: This means that the child needs to pick a subject, verb, and object; know the
    syntax of the language; understand the grammar rules of the language; and so on.
    Therefore, the likelihood of the child producing an incorrect sentence is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we ask the child to produce it word by word, they might do a better
    job at coming up with a sentence. In other words, we ask the child to produce
    the next word given the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I ____*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we ask them to fill in the blank given:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I like ____*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And continue in the same fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I like to ___, I like to fly ____, I like to fly kites ____*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This way, the child can do a better job at producing a correct and meaningful
    sentence. We can adopt the same approach to alleviate the difficulty of the translation
    task, as shown in *Figure 9.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: The teacher forcing mechanism. The darker arrows in the inputs
    depict newly introduced input connections to the decoder. The right-hand side
    figure shows how the decoder GRU cell changes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To feed in previous tokens predicted by the decoder, we need an input layer
    for the decoder. When formulating the decoder inputs and outputs this way, for
    a sequence of tokens with length *n*, the input and output are *n-1* tokens long:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `de_lookup_layer` defined earlier to convert tokens to IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the encoder, let’s define an embedding layer for the German language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a GRU layer in the decoder that will take the token embeddings and
    produce hidden outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are passing the encoder’s last state to a special argument called
    `initial_state` in the GRU’s `call()` method. This ensures that the decoder uses
    the encoder’s last state to initialize its memory.
  prefs: []
  type: TYPE_NORMAL
- en: The next step of our journey takes us to one of the most important concepts
    in machine learning, ‘attention.’ So far, the decoder had to rely on the encoder’s
    last state as the ‘only’ input/signal about the source language. This is like
    asking to summarize a sentence using a single word. Generally, when doing so,
    you lose a lot of the meaning and message in this conversion. Attention alleviates
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention: Analyzing the encoder states'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of relying just on the encoder’s last state, attention enables the decoder
    to analyze the complete history of state outputs. The decoder does this at every
    step of the prediction and creates a weighted average of all the state outputs
    depending on what it needs to produce at that step. For example, in the translation
    *I went to the shop -> ich ging zum Laden*, when predicting the word *ging*, the
    decoder will pay more attention to the first part of the English sentence than
    the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There have been many different implementations of attention over the years.
    It’s important to properly emphasize the need for attention in NMT systems. As
    you have learned previously, the context, or thought vector, that resides between
    the encoder and the decoder is a performance bottleneck (see *Figure 9.11*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Breaking the context vector bottleneck](img/B14070_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: The encoder-decoder architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand why this is a bottleneck, let’s imagine translating the following
    English sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I went to the flower market to buy some flowers*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This translates to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ich ging zum Blumenmarkt, um Blumen zu kaufen*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are to compress this into a fixed-length vector, the resulting vector
    needs to contain these:'
  prefs: []
  type: TYPE_NORMAL
- en: Information about the subject (*I*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the verbs (*buy* and *went*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the objects (*flowers* and *flower market*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interaction of the subjects, verbs, and objects with each other in the sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, the context vector has a size of 128 or 256 elements. Reliance on
    the context vector to store all this information with a small-sized vector is
    very impractical and an extremely difficult requirement for the system. Therefore,
    most of the time, the context vector fails to provide the complete information
    required to make a good translation. This results in an underperforming decoder
    that suboptimally translates a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: To make the problem worse, during decoding the context vector is observed only
    in the beginning. Thereafter, the decoder GRU must memorize the context vector
    until the end of the translation. This becomes more and more difficult for long
    sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Attention sidesteps this issue. With attention, the decoder will have access
    to the full state history of the encoder for each decoding time step. This allows
    the decoder to access a very rich representation of the source sentence. Furthermore,
    the attention mechanism introduces a softmax layer that allows the decoder to
    calculate a weighted mean of the past observed encoder states, which will be used
    as the context vector for the decoder. This allows the decoder to pay different
    amounts of attention to different words at different decoding steps.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.12* shows a conceptual breakdown of the attention mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The attention mechanism in detail](img/B14070_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Conceptual attention mechanism in NMT'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at how we can compute attention.
  prefs: []
  type: TYPE_NORMAL
- en: Computing Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let’s investigate the actual implementation of the attention mechanism
    in detail. For this, we will use the Bahdanau attention mechanism introduced in
    the paper *Neural Machine Translation by Learning to Jointly Align and Translate*,
    by Bahdanau et al. We will discuss the original attention mechanism here. However,
    we’ll be implementing a slightly different version of it, due to the limitations
    of TensorFlow. For consistency with the paper, we will use the following notations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoder’s *j*^(th) hidden state: *h*[j]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*i*^(th) target token: *y*[i]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*i*^(th) decode hidden state in the *i*^(th) time step: *s*[i]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Context vector: *c*[i]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our decoder GRU is a function of an input *y*[i] and a previous step’s hidden
    state ![](img/B14070_09_043.png). This can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *f* represents the actual update rules used to calculate *y*[i] and *s*[i-1].
    With the attention mechanism, we are introducing a new time-dependent context
    vector *c*[i] for the *i*^(th) decoding step. The *c*[i] vector is a weighted
    mean of the hidden states of all the unrolled encoder steps. A higher weight will
    be given to the *j*^(th) hidden state of the encoder if the *j*^(th) word is more
    important for translating the *i*^(th) word in the target language. This means
    the model can learn which words are important at which time step, regardless of
    the directionality of the two languages or alignment mismatches. Now the decoder
    GRU becomes this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_045.png)'
  prefs: []
  type: TYPE_IMG
- en: Conceptually, the attention mechanism can be thought of as a separate layer
    and illustrated as in *Figure 9.13*. As shown, attention functions as a layer.
    The attention layer is responsible for producing *c*[i] for the *i*^(th) time
    step of the decoding process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now see how to calculate *c*[i]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_046.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *L* is the number of words in the source sentence, and ![](img/B14070_09_047.png)
    is a normalized weight representing the importance of the *j*^(th) encoder hidden
    state for calculating the *i*^(th) decoder prediction. This is calculated using
    what is known as an energy value. We represent *e*[ij] as the energy of the encoder’s
    *j*^(th) position for predicting the decoder’s *i*^(th) position. *e*[ij] is computed
    using a small fully connected network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_048.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, ![](img/B14070_09_049.png) is calculated with a multilayer
    perceptron whose weights are *v*[a], *W*[a], and *U*[a], and ![](img/B14070_09_050.png)
    (decoder’s previous hidden state from (*i-1*)^(th) time step) and *h*[j] (encoder’s
    *j*^(th) hidden output) are the inputs to the network. Finally, we compute the
    normalized energy values (i.e. weights) using softmax normalization over all encoder
    timesteps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The attention mechanism is shown in *Figure 9.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The attention mechanism in detail](img/B14070_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: The attention mechanism'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We said above that we’ll be implementing a slightly different variation of
    Bahdanau attention. This is because TensorFlow currently does not support an attention
    mechanism that can be iteratively computed for each time step, similar to how
    an RNN works. Therefore, we are going to decouple the attention mechanism from
    the GRU model and have it computed separately. We will concatenate the attention
    output with the hidden output of the GRU layer and feed it to the final prediction
    layer. In other words, we are not feeding attention output to the GRU model, but
    directly to the prediction layer. This is depicted in *Figure 9.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: The attention mechanism employed in this chapter'
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement attention, we are going to use the sub-classing API of Keras.
    We’ll define a class called `BahdanauAttention` (which inherits from the `Layer`
    class) and override two functions in that:'
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__()` – Defines the layer’s initialization logic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`call()` – Defines the computational logic of the layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our defined class would look like this. But don’t worry, we’ll be going through
    those two functions in detail below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: First, we’ll be looking at the `__init__()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can see that we are defining three layers: weight matrix `W_a`, weight
    matrix `U_a`, and finally the `AdditiveAttention` layer, which contains the attention
    computation logic we discussed above. The `AdditiveAttention` layer takes in a
    query, value and a key. The query is the decoder states, and the value and key
    are all of encoder states produced.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will discuss this layer in more detail soon. We’ll discuss the details of
    this layer below. Next let’s look at the computations defined in the `call()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The first thing to note is that this function takes a query, a key, and a value.
    These three elements will drive the attention computation. In Bahdanau attention,
    you can think of the key and value as being the same thing. The query will represent
    each decoder GRU’s hidden states for each time step, and the value (or key) will
    represent each encoder GRU’s hidden states for each time step. In other words,
    we are querying an output for each decoder position based on values provided by
    the encoder’s hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap the computations we have to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_048.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_09_051.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_09_046.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First we compute `wa_query` (represents ![](img/B14070_09_055.png)) and `ua_key`
    (represents ![](img/B14070_09_056.png)). Next, we propagate these values to the
    attention layer. The `AdditiveAttention` layer ([https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention))
    performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Reshapes `wa_query` from `[batch_size, Tq, dim]` to shape`[batch_size, Tq, 1,
    dim]` and `ua_key` from `[batch_size, Tv, dim]` shape to `[batch_size, 1, Tv,
    dim]`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculates scores with shape `[batch_size, Tq, Tv]` as: `scores = tf.reduce_sum(tf.tanh(query
    + key), axis=-1)`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Uses scores to calculate a distribution with shape `[batch_size, Tq, Tv]` using
    softmax activation: `distribution = tf.nn.softmax(scores)`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses `distribution` to create a linear combination of `value` with shape `[batch_size,
    Tq, dim]`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Returns `tf.matmul(distribution, value)`, which represents a weighted average
    of all encoder states (i.e. `value`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, you can see that *step 2* performs the first equation, *step 3* performs
    the second equation, and finally *step 4* performs the third equation. Another
    thing worth noting is that *step 2* does not mention ![](img/B14070_09_057.png)
    from the first equation. ![](img/B14070_09_058.png) is essentially a weight matrix
    with which we compute the dot product. We can introduce this weight matrix by
    setting `use_scale=True` when defining the `AdditiveAttention` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Another important argument is the `return_attention_scores` argument when calling
    the `AdditiveAttention` layer. This gives us the distribution weight matrix defined
    in *step 3*. We will use this to visualize where the model was paying attention
    when decoding the translation.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the final model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the attention mechanism understood and implemented, let’s continue our
    implementation of the decoder. We will get the attention output sequence, with
    one attended output for each time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we’ll get the attention weights distribution matrix, which we’ll
    use to visualize attention patterns against inputs and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'When defining attention, we’ll also pass a mask that denotes which tokens need
    to be ignored when computing outputs (e.g. padded tokens). Combine the attention
    output and the decoder’s GRU output to create a single concatenated input for
    the prediction layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the prediction layer takes the concatenated attention’s context vector
    and the GRU output to produce probability distributions over the German tokens
    for each timestep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'With the encoder and the decoder fully defined, let’s define the end-to-end
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also going to define a secondary model called the `attention_visualizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `attention_visualizer` can generate attention patterns for a given set of
    inputs. This is a handy way to know if the model is paying attention to the correct
    words during the decoding process. This visualizer model will be used once the
    full model is trained. We will now look at how we can train our model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the NMT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have defined the NMT architecture and preprocessed the training
    data, it is quite straightforward to train the model. Here, we will define and
    illustrate (see *Figure 9.15*) the exact process used for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15: The training procedure for NMT'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the model training, we’re going to define a custom training loop, as there
    is a special metric we’d like to track. Unfortunately, this metric is not a readily
    available TensorFlow metric. But before that, there are several utility functions
    we need to define:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `prepare_data()` function takes the source sentence and target sentence
    pairs and generates encoder and decoder inputs and decoder labels. Let’s understand
    the arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`de_lookup_layer` – The `StringLookup` layer of the German language'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_xy` – A tuple containing tokenized English sentences and tokenized German
    sentences in the training set, respectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`valid_xy` – Similar to `train_xy` but for validation data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_xy` – Similar to `train_xy` but for test data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each training, validation, and test dataset, this function generates the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_inputs` – Tokenized English sentences as in the preprocessed dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs` – All tokens except the last of each German sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_labels` – All token IDs except the first of each German sentence,
    where token IDs are generated by the `de_lookup_layer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, you can see that `decoder_labels` will be `decoder_inputs` shifted one
    token to the left. Next we define the `shuffle_data()` function, which will shuffle
    a provided set of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic here is quite straightforward. We take the `encoder_inputs`, `decoder_inputs`,
    and `decoder_labels` (generated by the `prepare_data()` step) with `shuffle_inds`.
    If `shuffle_inds` is `None`, we generate a random permutation of the indices.
    Otherwise, we generate a random permutation of the `shuffle_inds` provided. Finally,
    we index all of the data according to the shuffled index. We can then train the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'During model training, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare encoder and decoder inputs and decoder outputs using the `prepare_data()`
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each epoch:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuffle the data if the flag `shuffle` is set to `True`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each iteration:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a batch of data from prepared inputs and outputs
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate that batch using `model.evaluate` to get the loss and accuracy
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Check if any of the samples are giving `nan` values (useful as a debugging step)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Train on the batch of data
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the BLEU score if the flag `predict_bleu_at_training` is set to `True`
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the model on validation data to get validation loss and accuracy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the validation BLEU score
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the loss, accuracy, and BLEU score on test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see that we are computing a new metric called the BLEU metric. BLEU
    is a special metric used to measure performance in sequence-to-sequence problems.
    It tries to maximize the correctness of n-grams of tokens, rather than measuring
    it on individual tokens (e.g. accuracy). The higher the BLEU score, the better.
    You will learn more about how the BLEU score is calculated in the next section.
    You can see the logic defined in the `BLEUMetric` object in the code.
  prefs: []
  type: TYPE_NORMAL
- en: In this, we are mostly doing the preprocessing of text to remove uninformative
    tokens, so that the BLEU score is not overestimated. For example, if we include
    the `<pad>` token, you will see high BLEU scores, as there are long sequences
    of `<pad>` tokens for short sentences. To compute the BLEU score, we’ll be using
    a third-party implementation available at [https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py).
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a large batch size, you may see TensorFlow throwing an exception
    starting out as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In this case, you may need to restart the notebook kernel, reduce the batch
    size, and rerun the code.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing we do, but haven’t discussed, is check for `NaN` (i.e. not-a-number)
    values. It can be very frustrating to see your loss value being `NaN` at the end
    of a training cycle. This is done by using the `check_for_nan()` function. This
    function will print out any specific data points that caused `NaN` values, so
    you have a much better idea of what caused it. You can find the implementation
    of the `check_for_nan()` function in the code.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, the current state-of-the-art BLEU score for German to English translation
    is 35.14 ([https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german)).
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is fully trained, you should see a BLEU score of around 15 for
    validation and test data. This is quite good, given that we used a very small
    proportion of the data (i.e. 250,000 sentences from more than 4 million) and a
    relatively simpler model compared to the state-of-the-art models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Improving NMT performance with deep GRUs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'One obvious improvement we can do is to increase the number of layers by stacking
    GRUs on top of each other, thereby creating a deep GRUs. For example, the Google
    NMT system uses eight LSTM layers stacked upon each other (*Google’s Neural Machine
    Translation System: Bridging the Gap between Human and Machine Translation, Wu
    and others, Technical Report (2016)*). Though this hampers the computational efficiency,
    having more layers greatly improves the neural network’s ability to learn the
    syntax and other linguistic characteristics of the two languages.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s understand how the BLEU score is calculated in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The BLEU score – evaluating the machine translation systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**BLEU** stands for **Bilingual Evaluation Understudy** and is a way of automatically
    evaluating machine translation systems. This metric was first introduced in the
    paper *BLEU: A Method for Automatic Evaluation of Machine Translation, Papineni
    and others, Proceedings of the 40th Annual Meeting of the Association for Computational
    Linguistics (ACL), Philadelphia, July 2002: 311-318*. We will be using an implementation
    of the BLEU score found at [https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py).
    Let’s understand how this is calculated in the context of machine translation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an example to learn the calculations of the BLEU score. Say
    we have two candidate sentences (that is, a sentence predicted by our MT system)
    and a reference sentence (that is, the corresponding actual translation) for some
    given source sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference 1: *The cat sat on the mat*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Candidate 1: *The cat is on the mat*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To see how good the translation is, we can use one measure, **precision**.
    Precision is a measure of how many words in the candidate are actually present
    in the reference. In general, if you consider a classification problem with two
    classes (denoted by negative and positive), precision is given by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_059.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s now calculate the precision for candidate 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision = # of times each word of candidate appeared in reference/# of words
    in candidate*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this can be given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_060.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Precision for candidate 1 = 5/6*'
  prefs: []
  type: TYPE_NORMAL
- en: This is also known as 1-gram precision since we consider a single word at a
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s introduce a new candidate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate 2: *The the the cat cat cat*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is not hard for a human to see that candidate 1 is far better than candidate
    2\. Let’s calculate the precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision for candidate 2 = 6/6 = 1*'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the precision score disagrees with the judgment we made. Therefore,
    precision alone cannot be trusted to be a good measure of the quality of a translation.
  prefs: []
  type: TYPE_NORMAL
- en: Modified precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To address the precision limitation, we can use a modified 1-gram precision.
    The modified precision clips the number of occurrences of each unique word in
    the candidate by the number of times that word appeared in the reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_061.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, for candidates 1 and 2, the modified precision would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mod-1-gram-Precision Candidate 1 = (1 + 1 + 1 + 1 + 1)/ 6 = 5/6*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mod-1-gram-Precision Candidate 2 = (2 + 1) / 6 = 3/6*'
  prefs: []
  type: TYPE_NORMAL
- en: We can already see that this is a good modification as the precision of candidate
    2 is reduced. This can be extended to any n-gram by considering *n* words at a
    time instead of a single word.
  prefs: []
  type: TYPE_NORMAL
- en: Brevity penalty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Precision naturally prefers small sentences. This raises a question in evaluation,
    as the MT system might generate small sentences for longer references and still
    have higher precision. Therefore, a **brevity penalty** is introduced to avoid
    this. The brevity penalty is calculated by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_074.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *c* is the candidate sentence length and *r* is the reference sentence
    length. In our example, we calculate it as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: BP for candidate 1 = ![](img/B14070_09_062.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BP for candidate 2 = ![](img/B14070_09_062.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final BLEU score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, to calculate the BLEU score, we first calculate several different modified
    n-gram precisions for a bunch of different *n=1,2,…,N* values. We will then calculate
    the weighted geometric mean of the n-gram precisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_065.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w*[n] is the weight for the modified n-gram precision *p*[n]. By default,
    equal weights are used for all n-gram values. In conclusion, BLEU calculates a
    modified n-gram precision and penalizes the modified-n-gram precision with a brevity
    penalty. The modified n-gram precision avoids potential high precision values
    given to meaningless sentences (for example, candidate 2).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Attention patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember that we specifically defined a model called `attention_visualizer`
    to generate attention matrices? With the model trained, we can now look at these
    attention patterns by feeding data to the model. Here’s how the model was defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll also define a function to get the processed attention matrix along with
    label data that we can use directly for visualization purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This function does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly samples `n_samples` indices from the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each random index:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gets the inputs of the data point at that index (`en_input` and `de_input`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gets the predicted words by feeding `en_input` and `de_input` to the `attention_visualizer`
    (stored in `predicted_words`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleans `en_input` by removing any uninformative tokens (e.g. `<pad>`) and assigns
    to `clean_en_input`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleans `predicted_words` by removing tokens after the `</s>` token (stored in
    `clean_predicted_words`)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gets the attention weights only corresponding to the words left in the clean
    inputs and predicted words from `attn_weights`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Appends the `clean_en_input`, `clean_predicted_words`, and attention weights
    matrix to results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The results contain all the information we need to visualize attention patterns.
    You can see the actual code used to create the following visualizations in the
    notebook `Ch09-Seq2seq-Models/ch09_seq2seq.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a few samples from our test dataset and visualize attention patterns
    exhibited by the model (*Figure 9.16*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_16.png)![](img/B14070_09_16.1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16: Visualizing attention patterns for a few test inputs'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we’d like to see a heat map that has a roughly diagonal activation
    of energy. This is because both languages have a similar construct in terms of
    the direction of the language. And we can clearly see that in both examples.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at specific words in the first example, you can see the model focuses
    heavily on *evening* to predict *Abends*, *atmosphere* to predict *Ambiente*,
    and so on. In the second, you see that the model is focusing on the word *free*
    to predict *kostenlosen*, which is German for *free*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discuss how to infer translations from the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Inference with NMT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inferencing is slightly different from the training process for NMT (*Figure
    9.17*). As we do not have a target sentence at the inference time, we need a way
    to trigger the decoder at the end of the encoding phase. It’s not difficult as
    we have already done the groundwork for this in the data we have. We simply kick
    off the decoder by using `<s>` as the first input to the decoder. Then we recursively
    call the decoder using the predicted word as the input for the next timestep.
    We continue this way until the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Outputs `</s>` as the predicted token or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reaches a pre-defined sentence length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To do this, we have to define a new model using the existing weights of the
    training model. This is because our trained model is designed to consume a sequence
    of decoder inputs at once. We need a mechanism to recursively call the decoder.
    Here’s how we can define the inference model:'
  prefs: []
  type: TYPE_NORMAL
- en: Define an encoder model that outputs the encoder’s hidden state sequence and
    the last encoder state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a new decoder that takes a decoder input having a time dimension of 1
    and a new input, to which we will input the previous hidden state value of the
    decoder (initialized with the encoder’s last state).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With that, we can start feeding data to generate predictions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess *x*[s] as in data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed *x*[s] into ![](img/B14070_09_066.png) and calculate the encoder’s state
    sequence and the last state *h* conditioned on *x*[s]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize ![](img/B14070_09_067.png) with *h*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the initial prediction step, predict ![](img/B14070_09_068.png) by conditioning
    the prediction on ![](img/B14070_09_069.png) as the first word and *h*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For subsequent time steps, while ![](img/B14070_09_070.png) and predictions
    haven’t reached a pre-defined length threshold, predict ![](img/B14070_09_071.png)
    by conditioning the prediction on ![](img/B14070_09_072.png) and *h*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This produces the translation given an input sequence of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_09_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.17: Inferring from an NMT'
  prefs: []
  type: TYPE_NORMAL
- en: The actual code can be found in the notebook `Ch09-Seq2seq-Models/ch09_seq2seq.ipynb`.
    We will leave it for the reader to study the code and understand the implementation.
    We conclude our discussion about machine translation here. Now, let’s briefly
    examine another application of sequence-to-sequence learning.
  prefs: []
  type: TYPE_NORMAL
- en: Other applications of Seq2Seq models – chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One other popular application of sequence-to-sequence models is in creating
    chatbots. A chatbot is a computer program that is able to have a realistic conversation
    with a human. Such applications are very useful for companies with a huge customer
    base. Responding to customers asking basic questions for which answers are obvious
    accounts for a significant portion of customer support requests. A chatbot can
    serve customers with basic concerns when it is able to find an answer. Also, if
    the chatbot is unable to answer a question, the request gets redirected to a human
    operator. Chatbots can save a lot of the time that human operators spend answering
    basic concerns and let them attend to more difficult tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Training a chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, how can we use a sequence-to-sequence model to train a chatbot? The answer
    is quite straightforward as we have already learned about the machine translation
    model. The only difference would be how the source and target sentence pairs are
    formed.
  prefs: []
  type: TYPE_NORMAL
- en: In the NMT system, the sentence pairs consist of a source sentence and the corresponding
    translation in a target language for that sentence. However, in training a chatbot,
    the data is extracted from the dialogue between two people. The source sentences
    would be the sentences/phrases uttered by person A, and the target sentences would
    be the replies to person A made by person B. One dataset that can be used for
    this purpose consists of movie dialogues between people and is found at [https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are links to several other datasets for training conversational chatbots:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reddit comments dataset: [https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maluuba dialogue dataset: [https://datasets.maluuba.com/Frames](https://datasets.maluuba.com/Frames)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ubuntu dialogue corpus: [http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/](http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NIPS conversational intelligence challenge: [http://convai.io/](http://convai.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft Research social media text corpus: [https://tinyurl.com/y7ha9rc5](https://tinyurl.com/y7ha9rc5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 9.18* shows the similarity of a chatbot system to an NMT system. For
    example, we train a chatbot with a dataset consisting of dialogues between two
    people. The encoder takes in the sentences/phrases spoken by one person, where
    the decoder is trained to predict the other person’s response. After training
    in such a way, we can use the chatbot to provide a response to a given question:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training a chatbot](img/B14070_09_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.18: Illustration of a chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating chatbots – the Turing test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After building a chatbot, one way to evaluate its effectiveness is using the
    Turing test. The Turing test was invented by Alan Turing in the 1950s as a way
    of measuring the intelligence of a machine. The experiment settings are well suited
    for evaluating chatbots. The experiment is set up as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three parties involved: an evaluator (that is, a human) (**A**),
    another human (**B**), and a machine (**C**). The three of them sit in three different
    rooms so that none of them can see the others. The only communication medium is
    text, which is typed into a computer by one party, and the receiver sees the text
    on a computer on their side. The evaluator communicates with both the human and
    the machine. And at the end of the conversation, the evaluator is to distinguish
    the machine from the human. If the evaluator cannot make the distinction, the
    machine is said to have passed the Turing test. This setup is illustrated in *Figure
    9.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating chatbots – Turing test](img/B14070_09_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.19: The Turing test'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the section on other applications of Seq2Seq models. We briefly
    discussed the application of creating chatbots, which is a popular use for sequential
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked in detail about NMT systems. Machine translation
    is the task of translating a given text corpus from a source language to a target
    language. First, we talked about the history of machine translation briefly to
    build a sense of appreciation for what has gone into machine translation for it
    to become what it is today. We saw that today, the highest-performing machine
    translation systems are actually NMT systems. Next, we solved the NMT task of
    generating English to German translations. We talked about the dataset preprocessing
    that needs to be done, and extracting important statistics about the data (e.g.
    sequence lengths). We then talked about the fundamental concept of these systems
    and decomposed the model into the embedding layer, the encoder, the context vector,
    and the decoder. We also introduced techniques like teacher forcing and Bahdanau
    attention, which are aimed at improving model performance. Then we discussed how
    training and inference work in NMT systems. We also discussed a new metric called
    BLEU and how it is used to measure performance on sequence-to-sequence problems
    like machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we briefly talked about another popular application of sequence-to-sequence
    learning: chatbots. Chatbots are machine learning applications that are able to
    have realistic conversations with a human and even answer questions. We saw that
    NMT systems and chatbots work similarly, and only the training data is different.
    We also discussed the Turing test, which is a qualitative test that can be used
    to evaluate chatbots.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at a new type of model that came out in 2016
    and is leading both the NLP and computer vision worlds: the Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)'
  prefs: []
  type: TYPE_IMG
