<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Creating an E2E Web App Using DL APIs and Customer Support Chatbot</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will draw together several tools and methods that we have learned how to use in previous chapters of this book, as well as introducing some great new tools and techniques, as well. This chapter covers a very important facet of an enterprise—customer support. For a budding business, customer support can be exhausting and frustrating to keep up with. More often than not, the questions raised by customers are easily answerable by referring to documentation or a set of FAQ answers provided by the company on their website, but customers don't often read through them. So, it would be great to have a layer <span>of automation</span> <span>in place,</span> <span>where the most common queries will be answered by a chatbot that is always available and responsive throughout the day.</span></p>
<p>This chapter discusses how to create a chatbot using Dialogflow to resolve general customer support queries and how to integrate it into a Django-based website. Furthermore, the chatbot will draw its answers from a Django API, which will be <span>hosted</span> <span>separately. We'll explore ways of implementing bot personalities and introduce a method of implementing <strong>Text-to-Speech</strong> (<strong>TTS</strong>)- and <strong>Speech-to-Text</strong> (<strong>STT</strong>)-based user interfaces via the Web Speech API, which deploys neural networks right to the user's browser.</span></p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>An introduction to NLP</li>
<li>An introduction to chatbots</li>
</ul>
<ul>
<li>Creating a Dialogflow bot with the personality of a customer support representative</li>
<li>Using ngrok to facilitate HTTPS APIs on localhost</li>
<li>Creating a testing UI using Django for managing orders within a company</li>
<li><span>Speech recognition and speech synthesis on a web page using the Web Speech API</span></li>
</ul>
<p>We will be drawing insights fro<span>m what we have learned in previous chapters and building on them, while at the same time revising a few concepts and introducing new ones along the way.</span> Let's begin by understanding <strong>Natural Language Processing</strong> (<strong>NLP</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can access the code for this chapter at <a href="https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter12">https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter12</a>.<a href="https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter12"/></p>
<p>You'll need the following software to run the code used in this chapter:</p>
<ul>
<li>Python 3.6+</li>
<li>Django 2.x</li>
</ul>
<p>All other installations will be covered during the course of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An introduction to NLP</h1>
                </header>
            
            <article>
                
<p>A popular—and one of the most exciting—fields of machine learning and deep learning applications is NLP, which refers to a collection of techniques and methods developed to understand and generate human language. The goals of NLP begin with comprehending the meaning of human language text and extend to generating human language, such that the generated sentences are meaningful and make sense to humans who read that text. NLP has found major usage in building systems that are able to take instructions and requests directly from humans in the form of natural language, such as chatbots. However, chatbots also need to respond in natural language, which is another aspect of NLP.</p>
<p>Let's study some common terms related to NLP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Corpus</h1>
                </header>
            
            <article>
                
<p>You will often come across the term <strong>corpus</strong> while you are studying NLP. In layman's terms, a corpus is a collection of writings from any one author or from a genre of literature. In the study of NLP, the dictionary definition of corpus gets a bit modified and can be stated as a collection of written text documents, such that they can all be categorized together by any metric of choice. These metrics might be authors, publishers, genres, types of writing, ranges of time, and other features associated with written texts.</p>
<p>For example, a collection of Shakespeare's works or the threads on any forum for any given topic can both be considered a corpus.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parts of speech</h1>
                </header>
            
            <article>
                
<p>When we decompose a sentence into its constituent words and perform a qualitative analysis of what each of the words of the sentence contributes to the overall meaning of that sentence, we perform the act of determining parts of speech. So, parts of speech are notations provided to words in a sentence based on how those words contribute to the meaning of the sentence.</p>
<p>In the English language, we commonly have eight types of parts of speech—the verb, the noun, the pronoun, the adjective, the adverb, the preposition, the conjunction, and the interjection.</p>
<p>For example, in the sentence "Ram is reading a book", "Ram" is a noun and the subject, "reading" is a word and the action, and "book" is a noun and the object.</p>
<p>You can read more about parts of speech at <a href="http://partofspeech.org/">http://partofspeech.org/</a>. You can try finding out the parts of speech of your own sentences at <a href="https://linguakit.com/en/part-of-speech-tagging">https://linguakit.com/en/part-of-speech-tagging</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tokenization</h1>
                </header>
            
            <article>
                
<p>Tokenization is the process of breaking down documents into sentences and sentences into words. This is important because it would be a computational nightmare if any computer program attempted to process entire documents as single strings, due to the resource-intensiveness associated with processing strings.</p>
<p>Furthermore, it is very rare that all sentences need to be read at once to be able to understand the meaning of an entire document. Often, each sentence has its own discrete meaning that can be assimilated with other sentences in the document by statistical methods to determine the overall meaning and content of any document.</p>
<p>Again, we often need to break down sentences into words in order to better process the sentence, such that the meaning of the sentence can be generalized and derived from a dictionary, where each word is listed individually.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stemming and lemmatization</h1>
                </header>
            
            <article>
                
<p>Stemming and lemmatization are closely related terms in NLP, but with a slight but significant difference. The objective of both methods is to determine the root word that any given word originates from, such that any derivates of the root word can be matched to the root word in the dictionary.</p>
<p>Stemming is a rule-based process where the words are trimmed and sometimes appended with modifiers that indicate its root word. However, stemming might, at times, produce root words that don't exist in the human dictionary and so mean nothing to the human reader.</p>
<p>Lemmatization is the process of converting words to their lemma, or their root word, as given in the dictionary. So, the originally intended meaning of the word can be derived from a human dictionary, making lemmatized text easier to work with than stemmed text. Furthermore, lemmatization takes into consideration the part of speech that any word is in any given sentence before determining its correct lemma, which a stemming algorithm overlooks. This makes lemmatization more context-aware than stemming.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bag of words</h1>
                </header>
            
            <article>
                
<p>It is not possible for computers to directly process and work with text. Hence, all text must be converted into numbers before being fed into a machine learning model. The process of changing text to an array of numbers, such that it is possible to retrieve the most important pieces of the original text from the converted text at any point in time, is known as feature extraction or encoding. <strong>Bag of Words</strong> <span>(</span><strong>BoW</strong><span>)</span> is one popular and simple technique used to perform feature extraction on text.</p>
<p>The steps associated with a BoW implementation are as follows:</p>
<ol>
<li>Extract all the unique words from the document.</li>
<li>Create a single vector with all the unique words in the document.</li>
<li>Convert each document into a Boolean array based on whether any word in the word vector is present in that document or not.</li>
</ol>
<p>For example, consider the following three documents:</p>
<ol>
<li>Ram is a boy.</li>
<li>Ram is a good boy.</li>
<li>Ram is not a girl.</li>
</ol>
<p>The unique words present in these documents can be listed in a vector as ["Ram", "is", "a", "boy", "good", "not", "girl"].</p>
<p>So, each sentence can be converted as follows:</p>
<ol>
<li>[1, 1, 1, 1, 0, 0, 0]</li>
<li>[1, 1, 1, 1, 1, 0, 0]</li>
<li>[1, 1, 1, 0, 0, 1, 1]</li>
</ol>
<p>You will observe that BoW tends to lose the information of where each word appears in the sentence or what meaning it contributes to the sentence. So, BoW is a very basic method of feature extraction and may not be suitable for several applications that require context-awareness.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Similarity</h1>
                </header>
            
            <article>
                
<p>The similarity is the measure of how similar any two given sentences are. It is a very popular operation in the domain of computer science, and anywhere where records are maintained, for searching the right documents, searching words in any document, authentication, and other applications.</p>
<p>There are several ways of calculating the similarity between any two given documents. The Jaccard index is one of the most basic forms, which computes the similarity of two documents based on the percentage ratio of the number of tokens that are the same in both documents over the total unique tokens in the documents.</p>
<p>Cosine similarity is another very popular similarity index, which is computed by calculating the cosine formed between the vectors of two documents when converted into vectors using BoW or any other feature-extraction technique.</p>
<p>With these concepts in mind, let's move on to studying chatbots, which are one of the most popular forms of application of NLP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An introduction to chatbots</h1>
                </header>
            
            <article>
                
<p>Chatbots are a segment of application of NLP that deals specifically with conversational interfaces. These interfaces can also expand their work to handle rudimentary commands and actions and are, in these cases, termed voice-based virtual assistants. Voice-based virtual assistants have been on the rise recently with the introduction of dedicated devices such as Google Home and Alexa by Amazon.</p>
<p>Chatbots can exist in multiple forms. They don't need to only be present as virtual assistants. You could talk to a chatbot in a game, where it tries to draw a storyline in a certain direction, or you could interact with the social chatbots that some companies use to reply to their customers on social media platforms, such as Twitter or Facebook. Chatbots can be considered a move over <strong>Interactive Voice Response</strong> (<strong>IVR</strong>) systems, with their added intelligence and ability to respond to unknown input, sometimes merely with a fallback reply or sometimes with a calculated response that draws on the input provided.</p>
<p>A virtual assistant can also exist on a website, giving instructions and offering help to visitors. Assistants such as these are regularly found on websites, mostly offering instant support to consumer queries. You must have noticed the "Ask a question" or "May I help you" chatboxes, usually at the bottom-right side of the screen, on several websites that sell products or services. More often than not, they employ the use of automated chatbots instead of real people to answer queries. Only in cases where the query is too complex to be answered by the automated customer support chatbot is the query transferred to a real person.</p>
<div class="packt_infobox">Creating conversational UIs is an art in itself. You need to be able to use words that are clear yet natural to a spoken tongue. You can learn more about creating conversational UIs at <a href="https://designguidelines.withgoogle.com/conversation/">https://designguidelines.withgoogle.com/conversation</a>.</div>
<p>In the next section, we will work on creating a chatbot that acts as a customer support agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Dialogflow bot with the personality of a customer support representative</h1>
                </header>
            
            <article>
                
<p>Dialogflow is a very popular tool used to create chatbots. Similar to Wit.ai, Botpress, Microsoft Bot Framework, and several other ready-to-deploy services available for creating chatbots, Dialogflow comes with the added advantage of its tight integration with <strong>Google Cloud Platform</strong> (<strong>GCP</strong>) and the possibility of using Dialogflow agents as actions for the <span>Google Assistant, which runs natively on billions of Android devices.</span></p>
<p>Dialogflow was formerly known as Api.ai. After its acquisition by Google, it was renamed and has <span>since</span> <span>grown in its popularity and extensibility. The platform allows very easy integration with several platforms, such as Facebook Messenger, Telegram, Slack, Line, Viber, and several other major communication platforms.</span></p>
<p>The project we will develop in this chapter will follow the following architecture diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1246 image-border" src="assets/a41d094b-e97b-407c-917d-75e3604a9f83.png" style="width:57.83em;height:22.00em;"/></p>
<p>We will use several libraries and services that are not mentioned in the preceding diagram. We'll introduce them during the course of the project and discuss why it is interesting for us to know about them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with Dialogflow</h1>
                </header>
            
            <article>
                
<p>To get started with Dialogflow, you should head to the official website, at <a href="https://dialogflow.com">https://dialogflow.com</a>, to get to the home page, which displays the product information and links to the documentation. It is always a great idea to study the documentation of any product or service you're trying to learn because it includes the entirety of the software's workings and functionalities. We will refer to sections in the documentation in the upcoming sections of this chapter.</p>
<div class="packt_infobox">You can find the Dialogflow documentation at <a href="https://cloud.google.com/dialogflow/docs/">https://cloud.google.com/dialogflow/docs/</a>.</div>
<p>Dialogflow is closely integrated with GCP and so we must first create a Google account. To do so, create an account by going to <a href="https://account.google.com">https://account.google.com</a>. You might have to provide a number of permissions on your Google account if you are using your account for the first time with Dialogflow.</p>
<p>Let's move on to the steps to explore and understand the Dialogflow account creation process and the various parts of the UI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – Opening the Dialogflow console</h1>
                </header>
            
            <article>
                
<p>You need to click on the <span class="packt_screen">Go to console</span> button at the top-right corner of the page at <a href="https://dialogflow.com">https://dialogflow.com</a>. Alternatively, you can type <a href="https://dialogflow.cloud.google.com/">https://dialogflow.cloud.google.com/</a> <span>in your browser</span><span>. If you're a first-time user, you will see a screen as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1247 image-border" src="assets/3f63c63f-055e-418d-ab6b-1584c6ee3e17.png" style="width:42.67em;height:15.00em;"/></p>
<p>The dashboard prompts you to create a new agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – Creating a new agent</h1>
                </header>
            
            <article>
                
<p>We will now create a Dialogflow agent. In terms of Dialogflow, an agent is another name for a chatbot. It is the agent that receives, processes, and responds to all input provided by the user.</p>
<p>Click on the <span class="packt_screen">Create Agent</span> button and fill in the required information about the agent to your liking, which includes the agent's name, the default language, the timezone, and the Google project name.</p>
<p>If you haven't used GCP prior to this step, you'll have to create a project. We've discussed the creation of GCP projects in <a href="093890b6-051d-49f9-9330-bdd58b92a762.xhtml" target="_blank"/><a href="093890b6-051d-49f9-9330-bdd58b92a762.xhtml">Chapter 6</a>, <em>Deep Learning on Google Cloud Platform Using Python</em>. Alternatively, you can simply let GCP automatically create a new project for you when creating the agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 – Understanding the dashboard</h1>
                </header>
            
            <article>
                
<p>After the successful creation of a Dialogflow agent, you'll be presented with a dashboard like that in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1248 image-border" src="assets/9dc140c8-5a57-428c-8d95-fe121fb43532.png" style="width:38.42em;height:27.58em;"/></p>
<p>On the left, you can see a menu containing the various components that make up the chatbot. This menu is going to be very useful and you should take a good look at all its contents to make sure you understand what we're referring to in the menu items. When we use sentences such as "Click on <span class="packt_screen">Entities</span>", we mean we want you to click on the <span class="packt_screen">Entities</span> item in this menu.</p>
<p>The center section will hold different content depending upon which component in the menu has been clicked on. By default, when you open the Dialogflow console, it contains the list of intents of the chatbot. What are intents?</p>
<p>An intent is an action that a user wishes to perform by any utterance they make to the chatbot. For example, when the user says <kbd>Bring me a cup of coffee</kbd>, their intent is to ask the chatbot to "bring coffee":</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1249 image-border" src="assets/b455813e-a6a5-486d-af4d-83efcfcd4c80.png" style="width:37.58em;height:22.50em;"/></p>
<p>On the far right, a panel is provided to test the chatbot at any moment. You can write any input text you wish to test the chatbot's response against and you'll be presented with a slew of information, along with the response that the chatbot produces.</p>
<p>Consider the following testing input and response:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1250 image-border" src="assets/1c8a88f2-9d9a-4fbf-92cd-6ed636277b4e.png" style="width:22.50em;height:49.42em;"/></p>
<p>When the user inputs <kbd>What is my order status</kbd>, the chatbot replies asking for the order ID of the order in question. This is matched to the <kbd>CheckOrderStatus</kbd> intent and requires a parameter named <kbd>OrderId</kbd>. We will be using this console regularly through this project to debug the chatbot during development.</p>
<p>While in the previous screenshots we've shown you a pre-configured agent with intents, your newly created agent won't have any custom intents at this point. Let's create them!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 – Creating the intents</h1>
                </header>
            
            <article>
                
<p>Now, let's create two intents. One intent will offer help to the user and the other will carry out a check on the status of the order ID provided by the user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4.1 – Creating HelpIntent</h1>
                </header>
            
            <article>
                
<p>In this sub-step, click on the <span class="packt_screen">+</span> button that is to the right of the <span class="packt_screen">Intents</span> item in the left-hand side menu. You will be presented with a blank intent creation form.</p>
<p>You will be able to see the following headings in the intent creation form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1251 image-border" src="assets/e229400a-7d81-4313-80b8-0ed1a34a5b3f.png" style="width:23.08em;height:24.33em;"/></p>
<p>For this intent, fill <span class="packt_screen">Intent Name</span> in as <kbd>HelpIntent</kbd>.</p>
<p>Now, follow the next steps to complete this intent creation.</p>
<p><strong>Step 4.1.1 – Entering the training phrases for HelpIntent</strong></p>
<p>Now, we need to define phrases that are likely to invoke this intent to action. To do so, click on the <span class="packt_screen">Training Phrases</span> heading and enter a few sample training phrases, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1252 image-border" src="assets/7dfccd76-fb68-4aae-8911-22af5e77a48c.png" style="width:28.33em;height:20.83em;"/></p>
<p>Make sure you click on <span class="packt_screen">Save</span> whenever you make any changes to an intent.</p>
<p><strong>Step 4.1.2 – Adding a response</strong></p>
<p>In order to respond to the user query in this intent, we need to define the possible responses. Click on the <span class="packt_screen">Responses</span> heading in the intent creation form and add a sample response to the query, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1253 image-border" src="assets/64a15276-092b-46b6-9c1e-7a2cf89d5d7b.png" style="width:32.00em;height:18.33em;"/></p>
<p>Save the intent. Once we have finished building it, we can test the chatbot by entering an input similar to the training phrases we defined for this intent.</p>
<p><strong>Step 4.1.3 – Testing the intent</strong></p>
<p>Let's test <kbd>HelpIntent</kbd>. In the right-hand side testing panel, input <kbd>Can you help me?</kbd>. The agent produces the following response:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1254 image-border" src="assets/5addae4f-c070-4f96-83f6-b2e65a6bf5e9.png" style="width:24.33em;height:25.83em;"/></p>
<p>Notice the matched intent at the bottom of the preceding screenshot. Since <kbd>HelpIntent</kbd> has successfully matched to the input, which was not explicitly defined in the training phrases, we can conclude that the agent works well.</p>
<div class="packt_infobox">Why is it important for the agent to respond to an input it has not been trained on? This is because while testing the agent for a particular intent, we want to be assured that any utterances exactly or closely matching the training phrases are matched by that intent. If it does not match closely related queries to the intent that is expected, you need to provide more training phrases and check whether there are any conflicting trainings in any other intents of the agent.</div>
<p>Now that we have an intent telling the user what this chatbot can be expected to do—that is, to check the status of the order—let's create an intent that can actually check the order status.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4.2 – Creating the CheckOrderStatus intent</h1>
                </header>
            
            <article>
                
<p>Click on the <span class="packt_screen">Create Intent</span> button and enter the name of the intent as <kbd>CheckOrderStatus</kbd>.</p>
<p><strong>Step 4.2.1 – Entering the training phrases for the CheckOrderStatus intent</strong></p>
<p>For this intent, we enter the following training phrases:</p>
<ol>
<li><kbd><span>What is the status for order id</span> <span class="selection">12345</span><span>?</span></kbd></li>
<li><kbd><span>When will my product arrive?</span></kbd></li>
<li><kbd><span>What has happened to my order?</span></kbd></li>
<li><kbd><span>When will my order arrive?</span></kbd></li>
<li><kbd><span>What's my order status?</span></kbd></li>
</ol>
<p>Note that the first training phrase is different from the rest because it contains an order ID.</p>
<p>We need to be able to identify it as an order ID and use that to fetch the order status.</p>
<p><strong>Step 4.2.2 – Extracting and saving the order ID from the input</strong></p>
<p>In the first training phrase of the <kbd>CheckOrderStatus</kbd> intent, double-click on <span class="packt_screen">12345</span> and a menu pops up, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1256 image-border" src="assets/c98bee73-f7c7-4615-b3dc-d54da04384c7.png" style="width:15.67em;height:19.67em;"/></p>
<p>Choose <span class="packt_screen">@sys.number</span> and then enter the parameter name as <kbd>OrderId</kbd>. Your training phrases will look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1257 image-border" src="assets/76dcff96-1ac6-4b4d-a3f9-465c3f178ff2.png" style="width:50.75em;height:19.25em;"/></p>
<p>But sometimes, as in the rest of the training phrases, the user will not mention the order ID without a prompt. Let's add a prompt and a way to store the order ID whenever it is found.</p>
<p><strong>Step 4.2.3 – Storing the parameter and prompting if not found</strong></p>
<p>Scroll down to the <span class="packt_screen">Actions and parameters</span> heading in the intent creation form. Enter <kbd>OrderId</kbd> for <span class="packt_screen">PARAMETER NAME</span> <span>and</span> <span class="packt_screen">VALUE</span> <span>and check the <span class="packt_screen">REQUIRED</span> checkbox. The following screenshot should look similar to what is on your screen now:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1258 image-border" src="assets/b57ad591-d3d9-41d2-927b-357920901174.png" style="width:53.17em;height:21.67em;"/></p>
<p>On the right-hand side of the <span><kbd>OrderId</kbd> parameter, click on</span> <span class="packt_screen">Define prompts</span> <span>to add a prompt for this parameter. A sample prompt could be <kbd>Sure, could you please let me know the Order ID? It looks like 12345!</kbd>.</span></p>
<p>We expect that after this prompt, the user will definitely state the order ID, which will then match the first training phrase of this intent.</p>
<p>After this, we need to define the response for this intent.</p>
<p><strong>Step 4.2.4 – Turning on responses through Fulfillment for the CheckOrderStatus intent</strong></p>
<p>Remember that this intent would need to fetch the order status from the order ID obtained. In such a case, a constant set of responses will not serve the purpose. So, we'll take the help of the <span class="packt_screen">Fulfillment</span> heading in the intent creation form.</p>
<p>Scroll down and turn on the f<span>ulfillment</span> <span>method</span> <span>webhook</span> <span>for this intent. This section now should look as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1259 image-border" src="assets/9c9f4952-6bd9-4355-82d6-947bb003c29a.png" style="width:23.50em;height:8.67em;"/></p>
<p><span class="packt_screen">Fullfillment</span> allows your Dialogflow agent to query external APIs to generate the response the agent has to make. The metadata associated with the query received by the agent is sent to the external API, which then understands and decides on the response the query needs to be given. This is useful for having dynamic responses through the chatbot.</p>
<p>We must now define this webhook to handle the fetching of the order status using the order ID.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 – Creating a webhook</h1>
                </header>
            
            <article>
                
<p>We'll now create a webhook that will run on the Firebase cloud console and call an external API, which is present in our <span class="packt_screen">Order management</span> portal.</p>
<p>Click on the <span class="packt_screen">Fulfillment</span> item in the menu bar. You'll be presented with the option to switch on a webhook or to use a Firebase cloud function. Turn on the inline editor. Your screen will resemble the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1260 image-border" src="assets/a462f840-72e8-4bd3-adda-859232692693.png" style="width:51.75em;height:25.92em;"/></p>
<p>We'll customize the two files present in the inline editor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6 – Creating a Firebase cloud function</h1>
                </header>
            
            <article>
                
<p>A Firebase cloud function runs on the Firebase platform and is billed as the provisions on the GCP project that you chose or created during the creation of your Dialogflow agent. You can read more about Cloud Functions at <a href="https://dialogflow.com/docs/how-tos/getting-started-fulfillment">https://dialogflow.com/docs/how-tos/getting-started-fulfillment</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6.1 – Adding the required packages to package.json</h1>
                </header>
            
            <article>
                
<p>In the <kbd>package.json</kbd> file on the inline editor, we'll add the <kbd>request</kbd> and <kbd>request-promise-native</kbd> packages to the dependencies, as shown:</p>
<pre>"dependencies": {<br/>    "actions-on-google": "^2.2.0",<br/>    "firebase-admin": "^5.13.1",<br/>    "firebase-functions": "^2.0.2",<br/>    "dialogflow": "^0.6.0",<br/>    "dialogflow-fulfillment": "^0.5.0",<br/>    "request": "*",<br/>    "request-promise-native": "*"<br/>  }</pre>
<p>These packages will be automatically fetched during the build of the agent, so you do not need to execute any commands explicitly to install them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6.2 – Adding logic to index.js</h1>
                </header>
            
            <article>
                
<p>We'll be adding the code required to call the API of our order management system. Add the following function inside the <kbd>dialogflowFirebaseFulfillment</kbd> object definition:</p>
<pre>function checkOrderStatus(){<br/>    const request = require('request-promise-native');<br/>    var orderId = agent.parameters.OrderId;<br/>    var url = "https://example.com/api/checkOrderStatus/"+orderId;<br/>    return request.get(url)<br/>        .then(jsonBody =&gt; {<br/>            var body = JSON.parse(jsonBody);<br/>            agent.add("Your order is: " + body.order[0].order_status);<br/>            return Promise.resolve(agent);<br/>        })<br/>        .catch(err =&gt; {<br/>            agent.add('Unable to get result');<br/>            return Promise.resolve(agent);<br/>        });<br/>  }</pre>
<p>At the end of the file, just before ending the <kbd>dialogflowFirebaseFulfillment</kbd> object definition, add the mapping for the function you created previously to the intent that was matched in the Dialogflow agent before invoking the webhook call for generating a response:</p>
<pre>  let intentMap = new Map();<br/>  intentMap.set('Default Welcome Intent', welcome);<br/>  intentMap.set('Default Fallback Intent', fallback);<br/>  intentMap.set('CheckOrderStatus', checkOrderStatus);<br/>  agent.handleRequest(intentMap);</pre>
<p>Now, click on <span class="packt_screen">Deploy</span> to deploy this function. You will get notifications for the status of the deployment at the bottom <span>right</span> <span>of the screen. Wait for the deployment and build to complete.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 7 – Adding a personality to the bot</h1>
                </header>
            
            <article>
                
<p>Adding a personality to the bot is more about how you chose your responses to be and how you drive the conversation through the responses and prompts in the agent.</p>
<p>For example, while we chose a very standard response to the inputs of the user in the previous example, we could definitely make it more interesting by using real-world language or other decorative elements in the responses. It would appear very realistic if instead of directly showing the output from the response fetching API, we added conversational decorators, such as <kbd>Great, now let me see where your order is...</kbd> and during the fetching and loading of the response to the agent, we made the <span class="packt_screen">Fulfillment</span> function generate conversational fillers such as <kbd>almost there...</kbd>, <kbd>just getting there...</kbd>, <kbd>hmmm, let me see...</kbd>, and other fillers, depending on the requirements of the situation.</p>
<p>You can also set some interesting trivia to the chatbot using the <span class="packt_screen">Small Talk</span> module of Dialogflow. To use it, click on the <span class="packt_screen">Small Talk</span> menu item on the left and enable small talk. You can add several interesting responses that your bot will make if it gets a particular query, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1261 image-border" src="assets/175d0651-9d60-46d8-9d33-70fc6f938486.png" style="width:53.58em;height:28.00em;"/></p>
<p>Small talk is very useful for adding a very unique personality to your chatbot!</p>
<p>In the next step, we will be creating a UI to interact with this chatbot directly from the order management website. However, since we're talking about REST API-based interfaces, we'll most likely host this UI separately from the API that we created for the order management system.</p>
<p>This cloud function calls an HTTPS API that you will need to create. In the next section, we will learn how to create an API that can handle HTTPS requests on your local machine.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using ngrok to facilitate HTTPS APIs on localhost</h1>
                </header>
            
            <article>
                
<p><span>You will need to create your own order management system API for the cloud function script to work so that it can fetch the order status from the API. You can find a quick sample at</span> <a href="http://tiny.cc/omsapi">http://tiny.cc/omsapi</a><span>. Your API must run on an HTTPS URL. To achieve this, you can use services such as PythonAnywhere and ngrok. While PythonAnywhere hosts your code on their servers and provides a fixed URL, ngrok can be installed and run locally to provide a forwarding address to <kbd>localhost</kbd>.</span></p>
<p>Say you have to run your Django project for the order management API on port <kbd>8000</kbd> of your system and now wish to provide an HTTPS URL so that you can test it; you can do so easily with ngrok by following these steps:</p>
<ol>
<li>Download the ngrok tool.</li>
</ol>
<p style="padding-left: 60px">First, head over to <a href="https://ngrok.com">https://ngrok.com</a> and click on the <span class="packt_screen">Download</span> button in the top navigation menu. Choose the correct version of the tool according to your needs and download it to your system.</p>
<ol start="2">
<li>Create an account.</li>
</ol>
<p style="padding-left: 60px">Next, sign up for an account on the website and go to the dashboard. You can use GitHub or Google authentication to set up your account quickly.</p>
<p style="padding-left: 60px">You will see the following dashboard:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1262 image-border" src="assets/e6d34b11-f143-43a2-bc9b-834b34f9dabc.png" style="width:49.33em;height:31.58em;"/></p>
<p style="padding-left: 60px">Since you've already downloaded and installed the tool, you can skip directly to connecting your account.</p>
<ol start="3">
<li>Connect your ngrok account with your tool.</li>
</ol>
<p style="padding-left: 60px">Copy the command given on the ngrok dashboard under the <em>Connect your account</em> section—it contains the authtoken for your account and, on running, connects the ngrok tool on your system to your ngrok account on the website.</p>
<p style="padding-left: 60px">Then, we're ready to move on to the <kbd>localhost</kbd> port.</p>
<ol start="4">
<li>Set up the ngrok address to forward to <kbd>localhost</kbd>.</li>
</ol>
<p style="padding-left: 60px">Finally, use the following command to start forwarding all requests made to a randomly generated ngrok URL to <kbd>localhost</kbd>:</p>
<pre style="padding-left: 60px"><strong>ngrok http 8000</strong></pre>
<p>The ngrok service starts and remains active as long as you keep the terminal open. You should see an output similar to the following screenshot on your screen:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1263 image-border" src="assets/876ab489-46d8-4ced-92a3-32a95fbb08c4.png" style="width:50.67em;height:29.92em;"/></p>
<p>All requests made to your ngrok URL will be logged on the terminal. You can find your ngrok URL in the <kbd>Forwarding</kbd> row of the table just above the request logs. Notice that both the <kbd>http</kbd> and <kbd>https</kbd> ports are being forwarded. You can now use the API service running on your local machine to make calls from Firebase, which only allows HTTPS calls.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a testing UI using Django to manage orders</h1>
                </header>
            
            <article>
                
<p>We've previously used Django in this book, namely in <a href="3bf31fe1-d41c-4410-a83c-1651da439c70.xhtml">Chapter 8</a>, <em>Deep Learning on Microsoft Azure Using Python</em>, and <a href="6158dd33-fac9-4a1f-867d-d53c827d7a7f.xhtml">Chapter 10</a>, <em>Securing Web Apps with Deep Learning</em>. So, we will skip over the nitty-gritty details of how Django works and how you can get started with it. Let's dive straight into creating a UI that you can interact with using your voice!</p>
<div class="packt_infobox">If you have not installed Django on your system already, please follow the <em>A brief introduction to Django web development</em> section in <a href="3bf31fe1-d41c-4410-a83c-1651da439c70.xhtml">Chapter 8</a>, <em>Deep Learning on Microsoft Azure Using Python</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – Creating a Django project</h1>
                </header>
            
            <article>
                
<p>Every Django website is a project. To create one, use this command:</p>
<pre><strong>django-admin startproject ordersui</strong></pre>
<p>A directory named <kbd>ordersui</kbd> is created with the following directory structure:</p>
<pre><strong>ordersui/</strong><br/><strong>| -- ordersui/</strong><br/><strong>|         __init.py__<br/>|         settings.py</strong><br/><strong>|         urls.py</strong><br/><strong>|         wsgi.py</strong><br/><strong>| -- manage.py</strong></pre>
<p>Let's proceed with creating the modules for this project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – Creating an app that uses the API of the order management system</h1>
                </header>
            
            <article>
                
<p>Remember that each Django project is composed of several Django apps working together. We will now create a Django app in this project that will consume the order management system API and provide a UI to see the content contained in the API database. This is important for verifying that the Dialogflow agent is properly working.</p>
<p>Switch to the <kbd>ordersui</kbd> directory using the <kbd>cd</kbd> command in a new terminal or command prompt. Then, use the following command to create an app:</p>
<pre><strong>python manage.py startapp apiui</strong></pre>
<p>This will create a directory within the <kbd>ordersui</kbd> Django project app directory with the following structure:</p>
<pre><strong>apiui/    </strong><br/><strong>| -- __init__.py</strong><br/><strong>| -- admin.py</strong><br/><strong>| -- apps.py</strong><br/><strong>| -- migrations/</strong><br/><strong>|         __init__.py</strong><br/><strong>| -- models.py</strong><br/><strong>| -- tests.py</strong><br/><strong>| -- views.py</strong></pre>
<p>Before we begin the development of modules, let's define some project-level settings in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 – Setting up settings.py</h1>
                </header>
            
            <article>
                
<p>We'll now make some configurations <span>that are</span> <span>required</span> <span>in the</span> <kbd>ordersui/settings.py</kbd> <span>file.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3.1 – Adding the apiui app to the list of installed apps</h1>
                </header>
            
            <article>
                
<p>In the list of <kbd>INSTALLED_APPS</kbd>, add the <kbd>apiui</kbd> app, as shown:</p>
<pre><strong># Application definition</strong><br/><br/><strong>INSTALLED_APPS = [</strong><br/><strong>    'apiui',</strong><br/><strong>    'django.contrib.admin',</strong><br/><strong>    'django.contrib.auth',</strong><br/><strong>    'django.contrib.contenttypes',</strong><br/><strong>    'django.contrib.sessions',</strong><br/><strong>    'django.contrib.messages',</strong><br/><strong>    'django.contrib.staticfiles',</strong><br/><strong>]</strong></pre>
<p>The Django framework only includes apps during runtime that are listed in the <kbd>INSTALLED_APPS</kbd> directive, as in the preceding code. We will also need to define the database connectivity for the project, which is shown in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3.2 – Removing the database setting</h1>
                </header>
            
            <article>
                
<p>We'll remove the database connectivity setup configuration since we don't need a database connection in this UI.</p>
<p>Comment out the <kbd>DATABASES</kbd> dictionary, as shown:</p>
<pre><strong># Database</strong><br/><strong># https://docs.djangoproject.com/en/2.2/ref/settings/#databases</strong><br/><br/><strong># DATABASES = {</strong><br/><strong>#     'default': {</strong><br/><strong>#         'ENGINE': 'django.db.backends.sqlite3',</strong><br/><strong>#         'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),</strong><br/><strong>#     }</strong><br/><strong># }</strong></pre>
<p>Save the file. With this done, we'll set up a URL route to point to the <kbd>apiui</kbd> routes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 – Adding routes to apiui</h1>
                </header>
            
            <article>
                
<p>Change the code in <kbd>ordersui/urls.py</kbd> to add the path to include the route setting file inside the <kbd>apiui</kbd> app. Your file will contain the following code:</p>
<pre><strong>from django.contrib import admin</strong><br/><strong>from django.urls import path, include</strong><br/><br/><strong>urlpatterns = [</strong><br/><strong>    path('', include('apiui.urls')),</strong><br/><strong>]</strong></pre>
<p>Save the file. After setting the routes at the project level, we will need to set routes at the module level, as we'll do in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 – Adding routes within the apiui app</h1>
                </header>
            
            <article>
                
<p>Now that we've directed the project to use the <kbd>apiui</kbd> URL routes, we need to create the file <span>required</span> <span>for this app. Create a file named</span> <kbd>urls.py</kbd> <span>within the</span> <kbd>apiui</kbd> <span>directory with the following content:</span></p>
<pre><strong>from django.urls import path</strong><br/><br/><strong>from . import views</strong><br/><br/><strong>urlpatterns = [</strong><br/><strong> path('', views.indexView, name='indexView'),</strong><br/><strong> path('&lt;int:orderId&gt;', views.viewOrder, name='viewOrder'),</strong><br/><strong>]</strong></pre>
<p>Save the file. Now that we've specified the routes available in the application, we need to create views for each of these routes, as we'll do in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6 – Creating the views required</h1>
                </header>
            
            <article>
                
<p>In the routes we created, we mentioned two views—<kbd>indexView</kbd>, which does not take any parameters, and <kbd>viewOrder</kbd>, which takes a parameter called <kbd>orderId</kbd>. Create a new file called <kbd>views.py</kbd> in the <kbd>apiui</kbd> directory and follow the next steps to create the views <span>required</span><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6.1 – Creating indexView</h1>
                </header>
            
            <article>
                
<p>This route will simply show the orders placed on the order management system. We use the following code:</p>
<pre><strong>from django.shortcuts import render, redirect</strong><br/><strong>from django.contrib import messages</strong><br/><strong>import requests</strong><br/><br/><strong>def indexView(request):</strong><br/><strong>    URL = "https://example.com/api/"</strong><br/><strong>    r = requests.get(url=URL)</strong><br/><strong>    data = r.json()</strong><br/><strong>    return render(request, 'index.html', context={'orders': data['orders']})</strong></pre>
<p>We will create the <kbd>viewOrder</kbd> view in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6.2 – Creating viewOrder</h1>
                </header>
            
            <article>
                
<p>If we pass an order ID to the same <kbd>/</kbd> route in the form of <kbd>/orderId</kbd>, then we should return the status of the order. Use the following code:</p>
<pre><strong>def viewOrder(request, orderId):</strong><br/><strong>    URL = "https://example.com/api/" + str(orderId)</strong><br/><strong>    r = requests.get(url=URL)</strong><br/><strong>    data = r.json()</strong><br/><strong>    return render(request, 'view.html', {'order': data['order']})</strong></pre>
<p>We have finished creating the different views that we will need for this project; however, we're yet to create the templates they will be rendering. Let's create the templates required in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 7 – Creating the templates</h1>
                </header>
            
            <article>
                
<p>In the view we defined previously, we used two templates—<kbd>index.html</kbd> and <kbd>view.html</kbd>. But to make them appear in sync with the design, we'll also set up a <kbd>base.html</kbd> template, which will be the master template for the rest of the view templates in the UI.</p>
<p>Since the templates are mostly just HTML boilerplate with little consequence to the vital content of the website, we have provided the code for these files at <a href="http://tiny.cc/ordersui-templates">http://tiny.cc/ordersui-templates</a>. You'll have to save the template files in a folder named <kbd>templates</kbd> inside the <kbd>apiui</kbd> directory.</p>
<p>At this stage, you'll be able to start up the Django project server and check out the website on your browser by using the following command:</p>
<pre><strong>python manage.py runserver</strong></pre>
<p>Now that our server is running, we will create a voice interface around it in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech recognition and speech synthesis on a web page using the Web Speech API</h1>
                </header>
            
            <article>
                
<p>A recent and very exciting development in the domain of web development is the introduction of the Web Speech API. While Google has rolled out full support for the Web Speech API in Google Chrome browsers for both desktop and Android, Safari and Firefox only have partial implementations available. The Web Speech API consists primarily of two components:</p>
<ul>
<li><strong>Speech synthesis</strong>: More popularly known as <strong>TTS</strong>. It performs the action of generating voice narration for any given text.</li>
<li><strong>Speech recognition</strong>: Also known as <strong>STT</strong>. It performs the function of recognizing the words spoken by the user and converting them into corresponding text.</li>
</ul>
<p>You can go through the very detailed documentation of the Web Speech API, which is available at the Mozilla documentation page ( <a href="http://tiny.cc/webspeech-moz">http://tiny.cc/webspeech-moz</a> ). You can find a demonstration of the technology provided by Google at <a href="http://tiny.cc/webspeech-demo">http://tiny.cc/webspeech-demo</a>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1264 image-border" src="assets/a516e800-2a10-471f-84b2-c76d65e116c1.png" style="width:28.42em;height:14.17em;"/></p>
<p>In the following steps, we'll add a Web Speech API-based <span class="packt_screen">Ask a question</span> button to our website UI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – Creating the button element</h1>
                </header>
            
            <article>
                
<p>All the code in this section has to be put into the <kbd>base.html</kbd> template of the UI so that it is available on all of the pages of the website.</p>
<p>We use the following code to quickly create a button with the <span class="packt_screen">Ask a question</span> <span>text</span> <span>that will be at the bottom-right corner of the web page sitewide:</span></p>
<pre>&lt;div id="customerChatRoot" class="btn btn-warning" style="position: fixed; bottom: 32px; right: 32px;"&gt;Ask a question&lt;/div&gt;</pre>
<p>Now, we will need to initialize and configure the Web Speech API, as we will do in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – Initializing the Web Speech API and performing configuration</h1>
                </header>
            
            <article>
                
<p>When the web page has finished loading, we need to initialize the Web Speech API object and set the necessary configurations for it. To do so, use the following code:</p>
<pre>$(document).ready(function(){<br/>            window.SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;<br/>            var finalTranscript = '';<br/>            var recognition = new window.SpeechRecognition();<br/>            recognition.interimResults = false;<br/>            recognition.maxAlternatives = 10;<br/>            recognition.continuous = true;<br/>            recognition.onresult = (event) =&gt; {<br/>               // define success content here <br/>            }<br/>        <br/>            // click handler for button here<br/>        });</pre>
<p>You can see that we've initialized a web <kbd>SpeechRecognition</kbd> API object and then performed some configurations on it. Let's try to understand these configurations:</p>
<ul>
<li><kbd>recognition.interimResults</kbd> (Boolean) directs whether the API should attempt to recognize interim results or words that are yet to be spoken. This would add overhead to our use case and so is turned off. Having it turned on is more beneficial in situations where the speed of the transcription matters more than the accuracy of the transcription, such as when generating live transcriptions for a person speaking.</li>
<li><kbd>recognition.maxAlternatives</kbd> (number) tells the browser how many alternatives can be produced for the same speech segment. This is useful in cases where it is not very clear to the browser what was said and the user can be given an option to choose the correct recognition.</li>
<li><kbd>recognition.continuous</kbd> <span>(Boolean) tells the browser whether the audio has to be captured continuously or whether it should stop after recognizing the speech once.</span></li>
</ul>
<p>However, we've not yet defined the code that is executed when a result is received after performing STT. We do so by adding code to the <kbd>recognition.onresult</kbd> function, as shown:</p>
<pre>              let interimTranscript = '';<br/>              for (let i = event.resultIndex, len = event.results.length; i &lt; len; i++) {<br/>                let transcript = event.results[i][0].transcript;<br/>                if (event.results[i].isFinal) {<br/>                  finalTranscript += transcript;<br/>                } else {<br/>                  interimTranscript += transcript;<br/>                }<br/>              }<br/>              goDialogFlow(finalTranscript);<br/>              <br/>              finalTranscript = '';</pre>
<p>The preceding block of code creates an interim transcript while the user is speaking, which is continually updated as more words are spoken. When the user stops speaking, the interim transcript is appended to the final transcript and passed to the function handling the interaction with Dialogflow. After the response is received from the Dialogflow agent, the final transcript is reset for the next voice input from the user.</p>
<p>Notice that we've sent the final recognized transcript of the user's speech to a function named <kbd>goDialogFlow()</kbd>. Let's define this function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 – Making a call to the Dialogflow agent</h1>
                </header>
            
            <article>
                
<p>Once we have the text version of the user's speech-based query, we will send it to the Dialogflow agent, as shown:</p>
<pre>function goDialogFlow(text){<br/>            $.ajax({<br/>                type: "POST",<br/>                url: "https://XXXXXXXX.gateway.dialogflow.cloud.ushakov.co",<br/>                contentType: "application/json; charset=utf-8",<br/>                dataType: "json",<br/>                data: JSON.stringify({ <br/>                    "session": "test",<br/>                    "queryInput": {<br/>                    "text": {<br/>                        "text": text,<br/>                        "languageCode": "en"<br/>                        }<br/>                    } <br/>                }),<br/>                success: function(data) {<br/>                    var res = data.queryResult.fulfillmentText;<br/>                    speechSynthesis.speak(new SpeechSynthesisUtterance(res));<br/>                },<br/>                error: function() {<br/>                    console.log("Internal Server Error");<br/>                }<br/>            }); <br/>        }</pre>
<p>You'll observe that when the API call succeeds, we use the SpeechSynthesis API to speak out the result to the user. Its usage is much more simple than the SpeechRecognition API and so is the first of the two to appear on Firefox and Safari.</p>
<p>Notice the API URL used in the preceding function. It might look weird currently and you might wonder where we obtained this URL from. What we did was essentially skip the requirement of setting the Dialogflow agent service account configurations using the terminal, which is always local to the system the script is working on and is so difficult to transport.</p>
<p>To obtain a similar URL for your project, follow along with the following steps; otherwise, skip <em>step 4</em> and move directly on to <em>step 5</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 – Creating a Dialogflow API proxy on Dialogflow Gateway by Ushakov</h1>
                </header>
            
            <article>
                
<p>Head over to <a href="https://dialogflow.cloud.ushakov.co/">https://dialogflow.cloud.ushakov.co/</a>. You'll be presented with an interface, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1265 image-border" src="assets/2de7c84c-4fa8-4746-9387-f965726690e7.png" style="width:19.83em;height:24.33em;"/></p>
<p>Dialogflow Gateway facilitates the interactions between your voice UI and the Dialogflow agent. This is very useful in situations where our project is hosted as a static website. Dialogflow Gateway provides simplified API wrappers around the Dialogflow API and is very easy to use.</p>
<p>You'll have to create an account to get started with Dialogflow, shown in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4.1 – Creating an account on Dialogflow Gateway</h1>
                </header>
            
            <article>
                
<p>Click on <span class="packt_screen">Get Started</span> to begin the account creation process on the platform. You'll be asked to sign in with your Google account. Make sure you use the same account that you used to create the Dialogflow agent previously.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4.2 – Creating a service account for your Dialogflow agent project</h1>
                </header>
            
            <article>
                
<p>We previously discussed in detail how to create a service account for GCP projects in <a href="093890b6-051d-49f9-9330-bdd58b92a762.xhtml">Chapter 6</a>, <em>Deep Learning on Google Cloud Platform Using Python</em>. Create a new service key for the project linked to your Dialogflow agent, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1266 image-border" src="assets/ad8232d2-aff2-47bb-8654-343e8a131ef2.png" style="width:33.50em;height:19.25em;"/></p>
<p>Once the key has been created successfully, a dialog box will pop up, telling you that the key has been saved to your computer, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1267 image-border" src="assets/caea516d-c88e-49ce-9a31-be83ead57a5b.png" style="width:29.92em;height:10.00em;"/></p>
<p>The service account credentials are downloaded to your local system in the form of JSON, with the name as shown in the preceding screenshot.</p>
<p>Now, we will use this service account credentials file to connect Dialogflow Gateway to our Dialogflow agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4.3 – Uploading the service key file to Dialogflow Gateway</h1>
                </header>
            
            <article>
                
<p>On the Dialogflow Gateway console, you'll find the <span class="packt_screen">Upload Keys</span> button. Click on it to upload your generated service account key file. Once uploaded, the console will display your Dialogflow API proxy URLs, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1268 image-border" src="assets/0dc2a2ff-d5ff-43e8-9208-afb12d3facbb.png" style="width:43.58em;height:25.58em;"/></p>
<p>We'll use the Gateway URL in the function we defined previously.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 – Adding a click handler for the button</h1>
                </header>
            
            <article>
                
<p>Finally, we add a <kbd>click</kbd> handler to the <span class="packt_screen">Ask a question</span> button so that it can trigger the speech recognition of the user input and the synthesis of output from the Dialogflow agent.</p>
<p>Within the document <kbd>ready</kbd> function defined in <em>step 2</em>, add the following <kbd>click</kbd> handler code:</p>
<pre><strong>$('#customerChatRoot').click(function(){</strong><br/><strong>    recognition.start();</strong><br/><strong>    $(this).text('Speak!');</strong><br/><strong>});</strong></pre>
<p>Now, when the microphone starts listening for the user input, the button text changes to <span class="packt_screen">Speak!</span>, prompting the user to start speaking.</p>
<p>Try testing the website on your setup and see how accurately you can get it to work!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we combined several technologies to come up with an end-to-end project that demonstrates one of the most rapidly growing aspects of applying deep learning to websites. We covered tools such as Dialogflow, Dialogflow Gateway, GCP IAM, Firebase Cloud Functions, and ngrok. We also demonstrated how to build a REST API-based UI and how to make it accessible using the Web Speech API. The Web Speech API, although presently at a nascent stage, is a cutting-edge piece of technology used in web browsers and is expected to grow rapidly in the coming years.</p>
<p>It is safe to say that deep learning on the web has huge potential and will be a key factor in the success of many upcoming businesses. In the next chapter, we'll explore some of the hottest research areas in deep learning for web development and how we can plan to progress in the best way.</p>


            </article>

            
        </section>
    </body></html>