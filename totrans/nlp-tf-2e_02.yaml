- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding TensorFlow 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will get an in-depth understanding of TensorFlow. This
    is an open source distributed numerical computation framework, and it will be
    the main platform on which we will be implementing all our exercises. This chapter
    covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The building blocks of TensorFlow (for example, variables and operations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Keras for building models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing our first neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will get started with TensorFlow by defining a simple calculation and trying
    to compute it using TensorFlow. After we complete this, we will investigate how
    TensorFlow executes this computation. This will help us to understand how the
    framework creates a computational graph to compute the outputs and execute this
    graph to obtain the desired outputs. Then we will dive into the details of how
    TensorFlow architecture operates by looking at how TensorFlow executes things,
    with the help of an analogy of how a fancy café works. We will then see how TensorFlow
    1 used to work so that we can better appreciate the amazing features TensorFlow
    2 offers. Note that when we use the word “TensorFlow” by itself, we are referring
    to TensorFlow 2\. We will specifically mention TensorFlow 1 if we are referring
    to TensorFlow 1.
  prefs: []
  type: TYPE_NORMAL
- en: Having gained a good conceptual and technical understanding of how TensorFlow
    operates, we will look at some of the important computations the framework offers.
    First, we will look at defining various data structures in TensorFlow, such as
    variables and tensors, and we’ll also see how to read inputs through data pipelines.
    Then we will work through some neural network-related operations (for example,
    convolution operation, defining losses, and optimization).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will apply this knowledge in an exciting exercise, where we will
    implement a neural network that can recognize images of handwritten digits. You
    will also see that you can implement or prototype neural networks very quickly
    and easily by using a high-level submodule such as Keras.
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 1*, *Introduction to Natural Language Processing*, we briefly discussed
    what TensorFlow is. Now let’s take a closer look at it. TensorFlow is an open
    source, distributed numerical computation framework released by Google that is
    mainly intended to alleviate the painful details of implementing a neural network
    (for example, computing derivatives of the weights of the neural network). TensorFlow
    takes this a step further by providing efficient implementations of such numerical
    computations using **Compute Unified Device Architecture** (**CUDA**), which is
    a parallel computational platform introduced by NVIDIA (for more information on
    CUDA, visit [https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)).
    The **Application Programming Interface** (**API**) of TensorFlow at [https://www.tensorflow.org/api_docs/python/tf/all_symbols](https://www.tensorflow.org/api_docs/python/tf/all_symbols)
    shows that TensorFlow provides thousands of operations that make our lives easier.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow was not developed overnight. This is a result of the persistence
    of talented, good-hearted developers and scientists who wanted to make a difference
    by bringing deep learning to a wider audience. If you are interested, you can
    take a look at the TensorFlow code at [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow).
    Currently, TensorFlow has around 3,000 contributors, and it sits on top of more
    than 115,000 commits, evolving to be better and better every day.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with TensorFlow 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s learn about a few essential components in the TensorFlow framework
    by working through a code example. Let’s write an example to perform the following
    computation, which is very common for neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_02_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This computation encompasses what happens in a single layer of a fully connected
    neural network. Here `W` and `x` are matrices and `b` is a vector. Then, “`.`"
    denotes the dot product. sigmoid is a non-linear transformation given by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_02_001.png)'
  prefs: []
  type: TYPE_IMG
- en: We will discuss how to do this computation through TensorFlow step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will need to import TensorFlow and NumPy. NumPy is another scientific
    computation framework that provides various mathematical and other operations
    to manipulate data. Importing them is essential before you run any type of TensorFlow
    or NumPy-related operation in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we will write a function that can take the inputs `x`, `W`, and `b`
    and perform this computation for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add a Python decorator called `tf.function` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Put simply, a Python decorator is just another function. A Python decorator
    provides a clean way to call another function whenever you call the decorated
    function. In other words, every time the `layer()` function is called, `tf.function()`
    is called. This can be used for various purposes, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging the content and operations in a function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating the inputs and outputs of another function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the `layer()` function is passing through `tf.function()`, TensorFlow will
    trace the content (in other words, the operations and data) in the function and
    build a computational graph automatically.
  prefs: []
  type: TYPE_NORMAL
- en: The computational graph (also known as the dataflow graph) builds a DAG (a directed
    acyclic graph) that shows what kind of inputs are required, and what sort of computations
    need to be done in the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the `layer()` function produces `h` by using inputs `x`, `W`,
    and `b`, and some transformations or operations such as `+` and `tf.matmul()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: A computational graph of the client'
  prefs: []
  type: TYPE_NORMAL
- en: If we look at an analogy for a DAG, if you think of the output as a *cake*,
    then the *graph* would be the recipe to make that cake using *ingredients* (that
    is, inputs).
  prefs: []
  type: TYPE_NORMAL
- en: The feature that builds this computational graph automatically in TensorFlow
    is known as **AutoGraph**. AutoGraph is not just looking at the operations in
    the passed function; it also scrutinizes the flow of operations. This means that
    you can have `if` statements, or `for`/`while` loops in your function, and AutoGraph
    will take care of those when building the graph. You will see more on AutoGraph
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow 1.x, the user needed to implement the computational graph explicitly.
    This meant the user could not write typical Python code using `if-else` statements
    or `for` loops, but had to explicitly control the flow of operations using special
    bespoke TensorFlow operations such as `tf.cond()` and `tf.control_dependencies()`.
    This is because, unlike TensorFlow 2.x, TensorFlow 1.x did not immediately execute
    operations when you called them. Rather, after they were defined, they needed
    to be executed explicitly using the context of a TensorFlow `Session`. For example,
    when you run the following in TensorFlow 1,
  prefs: []
  type: TYPE_NORMAL
- en: '`h = tf.nn.sigmoid(tf.matmul(x,W) + b)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`h` will not have any value until `h` is executed in the context of a `Session`.
    Therefore, `h` could not be treated like any other Python variable. Don’t worry
    if you don’t understand how the `Session` works. It will be discussed in the coming
    sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can use this function right away, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `x` is a simple NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`W` and `b` are TensorFlow variables defined using the `tf.Variable` object.
    `W` and `b` hold tensors. A tensor is essentially an *n*-dimensional array. For
    example, a one-dimensional vector or a two-dimensional matrix are called **tensors**.
    A `tf.Variable` is a mutable structure, which means the values in the tensor stored
    in that variable can change over time. For example, variables are used to store
    neural network weights, which change during the model optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note that for `W` and `b`, we provide some important arguments, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These are called variable initializers and are the tensors that will be assigned
    to the `W` and `b` variables initially. A variable must have an initial value
    provided. Here, `tf.initializers.RandomUniform` means that we uniformly sample
    values between `minval` `(-0.1)` and `maxval` `(0.1)` to assign values to the
    tensors. There are many different initializers provided in TensorFlow ([https://www.tensorflow.org/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)).
    It is also very important to define the *shape* of your initializer when you are
    defining the initializer itself. The `shape` property defines the size of each
    dimension of the output tensor. For example, if `shape` is `[10, 5]`, this means
    that it will be a two-dimensional structure and will have `10` elements on axis
    0 (rows) and `5` elements on axis 1 (columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Finally, `h` is called a TensorFlow tensor in general. A TensorFlow tensor is
    an immutable structure. Once a value is assigned to a TensorFlow tensor, it cannot
    be changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the term “tensor” is used in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: To refer to an *n*-dimensional array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To refer to an immutable data structure in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For both, the underlying concept is the same as they hold an *n*-dimensional
    data structure, only differing in the context they are used. The term will be
    used interchangeably to refer to these structures in our discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can immediately see the value of `h` using,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: which will give,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `numpy()` function retrieves the NumPy array from the TensorFlow Tensor
    object. The full code is as below. All the code examples in this chapter will
    be available in the `tensorflow_introduction.ipynb` file in the `ch2` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For future reference, let’s call our example *the sigmoid example*.
  prefs: []
  type: TYPE_NORMAL
- en: As you can already see, defining a TensorFlow computational graph and executing
    that is very “Pythonic”. This is because TensorFlow executes its operations “eagerly”,
    or immediately after the `layer()` function is called. This is a special mode
    in TensorFlow known as *eager execution* mode. This was an optional mode for TensorFlow
    1, but has been made the default in TensorFlow 2\.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that the next two sections will be somewhat complex and technical.
    However, don’t worry if you don’t understand everything completely because the
    explanation will be supplemented with a more digestible, and thorough, real-world
    example that explains how an order is fulfilled in our new-and-improved restaurant,
    *Café Le TensorFlow 2*.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2 architecture – What happens during graph build?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now understand what TensorFlow does when you execute TensorFlow operations.
  prefs: []
  type: TYPE_NORMAL
- en: When you call a function decorated by `tf.function()`, such as the `layer()`
    function, there is quite a bit happening in the background. First, TensorFlow
    will trace all the TensorFlow operations taking place in the function and build
    the computational graph automatically.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, `tf.function()` will return a function that executes the built dataflow
    graph when invoked. Therefore, `tf.function()` is a multi-stage process, where
    it first builds the dataflow graph and then executes it. Additionally, since TensorFlow
    traces each line in the function, if something goes wrong, TensorFlow can point
    to the exact line that is causing the issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our sigmoid example, the computational, or dataflow, graph would look like
    *Figure 2.2*. A single element or vertex of the graph is called a **node.** There
    are two main types of objects in this graph: *operations* and *tensors*. In the
    preceding example, `tf.nn.sigmoid` is an operation and `h` is a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: A computational graph of the client'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding graph shows the order of operations as well as how inputs flow
    through them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that `tf.function()` or AutoGraph is not a silver bullet that
    turns any arbitrary Python function using TensorFlow operations into a computational
    graph; it has its limitations. For example, the current version cannot handle
    recursive calls. To see a full list of the eager mode capabilities, refer to the
    following link: [https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md](https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Now we know that TensorFlow is skilled at creating a nice computational graph,
    with all the dependencies and operations so that it knows exactly how, when, and
    where the data flows. However, we did not quite answer how this graph is executed.
    In fact, TensorFlow does quite a bit behind the scenes. For example, the graph
    might be divided into subgraphs, and subsequently into even finer pieces, to achieve
    parallelization. These subgraphs or pieces will then be assigned to workers that
    will perform the assigned task.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow architecture – what happens when you execute the graph?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The computational graph uses the `tf.GraphDef` protocol to canonicalize the
    dataflow graph and send it to the distributed master. The distributed master would
    perform the actual operation execution and parameter updates in a single-process
    setting. In a distributed setting, the master would delegate these tasks to worker
    processes/devices and manage these worker processes. `tf.GraphDef` is a standardized
    representation of the graph specific to TensorFlow. The distributed master sees
    all computations in the graph and divides the computations into different devices
    (for example, different GPUs and CPUs). TensorFlow operations have multiple kernels.
    A kernel is a device-specific implementation of a certain operation. For example,
    the `tf.matmul()` function will be implemented differently to run on the CPU or
    GPU since, on a GPU, you can achieve much better performance due to more parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the computational graph will be broken into subgraphs and pruned by the
    distributed master. Although decomposing the computational graph in *Figure 2.2*
    appears too trivial in our example, the computational graph can exponentially
    grow in real-world solutions with many hidden layers. Additionally, it becomes
    important to break the computational graph into multiple pieces and shave off
    any redundant computations in order to get results faster (for example, in a multi-device
    setting).
  prefs: []
  type: TYPE_NORMAL
- en: Executing the graph or a subgraph (if the graph is divided into subgraphs) is
    called a single *task*, where each task is allocated to a single worker (which
    could be a single process or an entire device). These workers can run as a single
    process in a multi-process device (for example, a multi-processing CPU), or run
    on different devices (for example, CPUs and GPUs). In a distributed setting, we
    would have multiple workers executing tasks (for example, multiple workers training
    the model on different batches of data). On the contrary, we have only one set
    of parameters. So how do multiple workers manage to update the same set of parameters?
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this, there is one worker that is considered the parameter server
    and will hold the main copy of the parameters. The workers will copy the parameters
    over, update them, and send them back to the parameter server. Typically, the
    parameter server will define some resolution strategy to resolve multiple updates
    coming from multiple workers (for example, taking the mean). These details were
    provided so you can understand the complexity that has gone into TensorFlow. However,
    our book will be based on using TensorFlow in a single-process/worker setting.
    In this setting, the organization of the distributed master, workers, and the
    parameter server is much more straightforward and is absorbed mostly by a special
    session implementation used by TensorFlow. This general workflow of a TensorFlow
    client is depicted in *Figure 2.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_02.jpg](img/B14070_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: The generic execution of a TensorFlow client. A TensorFlow client
    starts with a graph that gets sent to the distributed master. The master spins
    up worker processes to perform actual tasks and parameter updates'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the calculation is done, the session brings back the updated data to the
    client from the parameter server. The architecture of TensorFlow is shown in *Figure
    2.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_04.png](img/B14070_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: TensorFlow framework architecture. This explanation is based on
    the official TensorFlow documentation found at: [https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md)'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the changes introduced in TensorFlow 2 can be attributed to front-end
    changes. That is, how the dataflow graph is built and when the graph is executed.
    The way the graph is executed remains more or less the same in TensorFlow 1 and
    2.
  prefs: []
  type: TYPE_NORMAL
- en: Now we know what happens end-to-end from the moment you execute `tf.function()`,
    but this was a very technical explanation, and nothing explains something better
    than a good analogy. Therefore, we will try to understand TensorFlow 2 with an
    analogy to our new and improved Café Le TensorFlow 2.
  prefs: []
  type: TYPE_NORMAL
- en: Café Le TensorFlow 2 – understanding TensorFlow 2 with an analogy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say the owners renovated our previous Café Le TensorFlow (this is an analogy
    from the first edition) and reopened it as Café Le TensorFlow 2\. The word around
    the town is that it’s much more opulent than it used to be. Remembering the great
    experience you had before, you book a table instantly and go there to grab a seat.
  prefs: []
  type: TYPE_NORMAL
- en: You want to order a *chicken burger with extra cheese and no tomatoes*. And
    you realize the café is indeed fancy. There’re no waiters here, but a voice-enabled
    tablet for each table into which you say what you want. This will get converted
    to a standard format that the chefs will understand (for example, table number,
    menu item ID, quantity, and special requirements).
  prefs: []
  type: TYPE_NORMAL
- en: Here, you represent the TensorFlow 2 program. The ability of the voice-enabled
    tablet that converts your voice (or TensorFlow operations) to the standard format
    (or GraphDef format) is analogous to the AutoGraph feature.
  prefs: []
  type: TYPE_NORMAL
- en: Now comes the best part. As soon as you start speaking, a manager will be looking
    at your order and assigning various tasks to chefs. The manager is responsible
    for making sure things happen as quickly as possible. The kitchen manager makes
    decisions, such as how many chefs are required to make the dish and which chefs
    are the best candidates for the job. The kitchen manager represents the distributed
    master.
  prefs: []
  type: TYPE_NORMAL
- en: Each chef has a cook whose responsibility it is to provide the chef with the
    right ingredients, equipment, and so forth. So, the kitchen manager takes the
    order to a single chef and a cook (a burger is not that hard to prepare) and asks
    them to prepare the dish. The chef looks at the order and tells the cook what
    is needed. So, the cook first finds the things that will be required (for example,
    buns, patties, and onions) and keeps them close to fulfill the chef’s requests
    as soon as possible. Moreover, the chef might also ask to keep the intermediate
    results (for example, cut vegetables) of the dish temporarily until the chef needs
    it back again. In our example, the chef is the operation executor, and the cook
    is the parameter server.
  prefs: []
  type: TYPE_NORMAL
- en: This café is full of surprises. As you are speaking out your order (that is,
    invoking Python functions that have TensorFlow operations), you see it getting
    prepared in real time through the tablet on your table (that is, eager execution).
  prefs: []
  type: TYPE_NORMAL
- en: 'The best thing about this video feed is that, if you see that the chef did
    not put enough cheese, you know exactly why the burger wasn’t as good as expected.
    So, you can either order another one or provide specific feedback. This is a great
    improvement over how TensorFlow 1 did things, where they would take your order
    and you would not see anything until the full burger had been prepared. This process
    is shown in *Figure 2.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: The restaurant analogy illustrated'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now have a look back at how TensorFlow 1 used to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flashback: TensorFlow 1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We said numerous times that TensorFlow 2 is very different from TensorFlow 1\.
    But we still don’t know what it used to be like. Therefore, let’s now do a bit
    of time traveling to see how the same sigmoid computation could have been implemented
    in TensorFlow 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Warning**'
  prefs: []
  type: TYPE_NORMAL
- en: You will not be able to execute the following code in TensorFlow 2.x as it stands.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll define a `graph` object, which we will populate with operations
    and variables later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `graph` object contains the computational graph that connects the various
    inputs and outputs we define in our program to get the final desired output. This
    is the same graph we discussed earlier. Also, we’ll define a `session` object
    that takes the defined graph as the input, which executes the graph. In other
    words, compared to TensorFlow 2, the `graph` object and the `session` object do
    what happens when you invoke them decorated by `tf.function()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll define a few tensors, namely `x`, `W`, `b`, and `h`. There are several
    different ways that you can define tensors in TensorFlow 1\. Here, we will look
    at three such different approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: First, `x` is a placeholder. Placeholders, as the name suggests, are not initialized
    with any value. Rather, we will provide the value on the fly at the time of the
    graph execution. If you remember from the TensorFlow 2 sigmoid exercise, we fed
    `x` (which was a NumPy array) directly to the function `layer(x, w, b)`. Unlike
    in TensorFlow 2, you cannot feed NumPy arrays directly to TensorFlow 1 graphs
    or operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we have the variables `W` and `b`. Variables are defined similarly to
    TensorFlow 2 with some minor changes in the syntax.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we have `h`, which is an immutable tensor produced by performing some
    operations on `x`, `W`, and `b`. Note that you will not see the value of `h` immediately
    as you needed to manually execute the graph in TensorFlow 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These tensors are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The lifetime of variables in TensorFlow 1 was managed by the session object,
    meaning that variables lived in memory for as long as the session lived (even
    after losing references to them in the code). However, in TensorFlow 2, variables
    are removed soon after the variables are not referenced in the code, just like
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll run an initialization operation that initializes the variables
    in the graph, `W` and `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will execute the graph to obtain the final output we need, `h`. This
    is done by running `session.run(...)`, where we provide the value to the placeholder
    as an argument of the `session.run()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we close the session, releasing any resources held by the `session`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the full code of this TensorFlow 1 example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, before TensorFlow 2 the user had to:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the computational graph using various TensorFlow data structures (for
    example, `tf.placeholder`) and operations (for example, `tf.matmul()`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute the required part of the graph using `session.run()` to fetch the results
    by feeding the correct data into the session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In conclusion, TensorFlow 1.x had several limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Coding with TensorFlow 1 did not provide the same intuitive “Pythonic” feeling
    as you needed to define the computational graph first and then invoke the execution
    of it. This is known as declarative programming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The design in TensorFlow 1 made it very hard to break the code down into manageable
    functions as the user needed to define the graph fully, before doing any computations.
    This resulted in very large functions or pieces of code containing very large
    computational graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It was very difficult to do real-time debugging of the code as TensorFlow had
    its own runtime that used `session.run()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But also, it was not without some advantages, such as the efficiency brought
    about by declaring the full computational graph upfront. Knowing all the computations
    in advance meant TensorFlow 1 could perform all sorts of optimizations (for example,
    graph pruning) to run the graph efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this part of the chapter, we discussed our first example in TensorFlow2 and
    the architecture of TensorFlow. Finally, we compared and contrasted TensorFlow
    1 and 2\. Next, we will discuss the various building blocks of TensorFlow 2.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs, variables, outputs, and operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we are returning from our journey into TensorFlow 1 and stepping back to
    TensorFlow 2\. Let’s proceed to the most common elements that comprise a TensorFlow
    2 program. If you read any of the millions of TensorFlow clients available on
    the internet, the TensorFlow-related code all falls into one of these buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs**: Data used to train and test our algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variables**: Mutable tensors, mostly defining the parameters of our algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outputs**: Immutable tensors storing both terminal and intermediate outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations**: Various transformations for inputs to produce the desired outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our earlier sigmoid example, we can find instances of all these categories.
    We list the respective TensorFlow elements and the notation used in the sigmoid
    example in *Table 2.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **TensorFlow element** | **Value from example client** |'
  prefs: []
  type: TYPE_TB
- en: '| Inputs | `x` |'
  prefs: []
  type: TYPE_TB
- en: '| Variables | `W` and `b` |'
  prefs: []
  type: TYPE_TB
- en: '| Outputs | `h` |'
  prefs: []
  type: TYPE_TB
- en: '| Operations | `tf.matmul(...)`, `tf.nn.sigmoid(...)` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.1: The different types of TensorFlow primitives we have encountered
    so far'
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections explain each of these TensorFlow elements listed in
    the table in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Defining inputs in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three different ways you can feed data to a TensorFlow program:'
  prefs: []
  type: TYPE_NORMAL
- en: Feeding data as NumPy arrays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feeding data as TensorFlow tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `tf.data` API to create an input pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will discuss a few different ways you can feed data to TensorFlow operations.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding data as NumPy arrays
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the simplest way to feed data into a TensorFlow program. Here, you pass
    a NumPy array as an input to the TensorFlow operation and the result is executed
    immediately. This is exactly what we did in the sigmoid example. If you look at
    `x`, it is a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding data as tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second method is like the first one, but the type of data is different.
    Here, we are defining `x` as a TensorFlow tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this in action, let’s modify our sigmoid example. Remember that we defined
    `x` as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, let’s define this as a tensor that contains specific values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, the full code would become as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now discuss how we can define data pipelines in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Building a data pipeline using the tf.data API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`tf.data` provides you with a convenient way to build data pipelines in TensorFlow.
    Input pipelines are designed for more heavy-duty programs that need to process
    a lot of data. For example, if you have a small dataset (for example, the MNIST
    dataset) that fits into the memory, input pipelines would be excessive. However,
    when working with complex data or problems, where you might need to work with
    large datasets that do not fit in memory, augment the data (for example, for adjusting
    image contrast/brightness), numerically transform it (for example, standardize),
    and so on. The `tf.data` API provides convenient functions that can be used to
    easily load and transform your data. Furthermore, it streamlines your data ingestion
    code with the model training.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the `tf.data` API offers various options to enhance the performance
    of your data pipeline, such as multi-processing and pre-fetching data. Pre-fetching
    refers to bringing data into the memory before it’s required and keeping it ready.
    We will discuss these methods in more detail as they are used in the upcoming
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating an input pipeline, we intend to perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Source the data from a data source (for example, an in-memory NumPy array, CSV
    file on disk, or individual files such as images).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply various transformations to the data (for example, cropping/resizing image
    data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate the resulting dataset element/batch-wise. Batching is required as deep
    learning models are trained on randomly sampled batches of data. As the datasets
    these models are trained on are large, they typically do not fit in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s write an input pipeline using TensorFlow’s `tf.data` API. In this example,
    we have three text files (`iris.data.1`, `iris.data.2`, and `iris.data.3`) in
    CSV format, each file having 50 lines and each line having 4 floating-point numbers
    (in other words, various lengths associated with a flower) and a string label
    separated by commas (an example line would be `5.6,2.9,3.6,1.3,Iris-versicolor`).
    We will now use the `tf.data` API to read data from these files. We also know
    that some of this data is corrupted (as with any real-life machine learning project).
    In our case, some data points have negative lengths. So, let’s first write a pipeline
    to go through the data row by row and print the corrupted outputs.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, refer to the official TensorFlow page on importing data
    at [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import a few important libraries as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define a list containing the filenames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will use one of the dataset readers provided in TensorFlow. The dataset
    reader takes in a list of filenames and another list that specifies the data types
    of each column in the dataset. As we saw previously, we have four floating numbers
    and one string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will organize our data into inputs and labels as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We are using lambda functions to separate out `x1,x2,x3,x4` into one dataset
    and `y` to another dataset, along with the `dataset.map()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lambda functions are a special type of function that allow you to define some
    computations succinctly. With lambda functions, you don’t need to name your function,
    which can be quite handy if you are using a certain function only once in your
    code. The format of the lambda function looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lambda <arguments>: <result returned after the computation>`'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you need to write a function that adds two numbers, simply
    write:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lambda x, y: x+y`'
  prefs: []
  type: TYPE_NORMAL
- en: Here, `tf.stack()` stacks individual tensors (here, the individual feature)
    to a single tensor. When using the `map` function, you first need to visualize
    what needs to be done to a single item in the dataset (a single item in our case
    is a single row from the dataset), and write the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The map function is very simple but powerful. All it does is transform a set
    of given inputs into a new set of values. For example, if you have a list, `xx`,
    that contains a list of numbers and want to convert them to power 2 element-wise,
    you can write something like `xx_pow = map(lambda x: x**2, xx)`. And this can
    be very easily parallelized as there’s no dependency between items.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can iterate through this dataset, examining individual data points,
    as you would iterate through a normal Python list. Here, we are printing out all
    the corrupted items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Since you don’t want those corrupted inputs in your dataset, you can use the
    `dataset.filter()` function to filter out those corrupted entries as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here we are checking whether the minimum element in `x` is greater than zero;
    if not, those elements will be filtered out of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful function is `dataset.batch()`. When training deep neural networks,
    we often traverse the dataset in batches, not individual items. `dataset.batch()`
    provides a convenient way to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you print the shape of a single element in your dataset, you should
    get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have examined the three different methods you can use to define
    inputs in TensorFlow, let’s see how we can define variables in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Defining variables in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variables play an important role in TensorFlow. A variable is essentially a
    tensor with a specific shape defining how many dimensions the variable will have
    and the size of each dimension. However, unlike a regular TensorFlow tensor, variables
    are *mutable*; meaning that the value of the variables can change after they are
    defined. This is an ideal property to have to implement the parameters of a learning
    model (for example, neural network weights), where the weights change slightly
    after each step of learning. For example, if you define a variable with `x = tf.Variable(0,dtype=tf.int32)`,
    you can change the value of that variable using a TensorFlow operation such as
    `tf.assign(x,x+1)`. However, if you define a tensor such as `x = tf.constant(0,dtype=tf.int32)`,
    you cannot change the value of the tensor, as you could for a variable. It should
    stay `0` until the end of the program execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variable creation is quite simple. In our sigmoid example, we already created
    two variables, `W` and `b`. When creating a variable, a few things are extremely
    important. We will list them here and discuss each in detail in the following
    paragraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: Variable shape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variable shape is a list of the `[x,y,z,...]` format. Each value in the
    list indicates how large the corresponding dimension or axis is. For instance,
    if you require a 2D tensor with 50 rows and 10 columns as the variable, the shape
    would be equal to `[50,10]`.
  prefs: []
  type: TYPE_NORMAL
- en: The dimensionality of the variable (that is, the length of the `shape` vector)
    is recognized as the rank of the tensor in TensorFlow. Do not confuse this with
    the rank of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor rank in TensorFlow indicates the dimensionality of the tensor; for a
    two-dimensional matrix, *rank* = 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, a variable requires an *initial* value to be initialized with. TensorFlow
    provides several different initializers for our convenience, including constant
    initializers and normal distribution initializers. Here are a few popular TensorFlow
    initializers you can use to initialize variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.initializers.Zeros`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.initializers.Constant`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.initializers.RandomNormal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.initializers.GlorotUniform`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The shape of the variable can be provided as a part of the initializer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The data type plays an important role in determining the size of a variable.
    There are many different data types, including the commonly used `tf.bool`, `tf.uint8`,
    `tf.float32`, and `tf.int32`. Each data type has a number of bits required to
    represent a single value with that type. For example, `tf.uint8` requires 8 bits,
    whereas `tf.float32` requires 32 bits. It is common practice to use the same data
    types for computations, as doing otherwise can lead to data type mismatches. So,
    if you have two different data types for two tensors that you need to transform,
    you have to explicitly convert one tensor to the other tensor’s type using the
    `tf.cast(...)` operation.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.cast(...)` operation is designed to cope with such situations. For example,
    if you have an `x` variable with the `tf.int32` type, which needs to be converted
    to `tf.float32`, employ `tf.cast(x,dtype=tf.float32)` to convert `x` to `tf.float32`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the *name* of the variable will be used as an ID to identify that variable
    in the graph. If you ever visualize the computational graph, the variable will
    appear by the argument passed to the `name` keyword. If you do not specify a name,
    TensorFlow will use the default naming scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the Python variable `tf.Variable` is assigned to is not known by
    the computational graph, and is not a part of TensorFlow variable naming. Consider
    this example where you specify a TensorFlow variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, the TensorFlow graph will know this variable by the name `b` and not `a`.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, let’s talk about how to define TensorFlow outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Defining outputs in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TensorFlow outputs are usually tensors, and the result of a transformation
    to either an input, or a variable, or both. In our example, `h` is an output,
    where `h = tf.nn.sigmoid(tf.matmul(x,W) + b)`. It is also possible to give such
    outputs to other operations, forming a chained set of operations. Furthermore,
    they do not necessarily have to be TensorFlow operations. You also can use standard
    Python arithmetic with TensorFlow. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Below, we explain various operations available in TensorFlow and how to use
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Defining operations in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An operation in TensorFlow takes one or more inputs and produces one or more
    outputs. If you take a look at the TensorFlow API at [https://www.tensorflow.org/api_docs/python/tf](https://www.tensorflow.org/api_docs/python/tf),
    you will see that TensorFlow has a massive collection of operations available.
    Here, we will take a look at a selected few of the myriad TensorFlow operations.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Comparison operations are useful for comparing two tensors. The following code
    example includes a few useful comparison operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the working of these operations, let’s consider two example tensors,
    `x` and `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s look at some mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow allows you to perform math operations on tensors that range from
    the simple to the complex. We will discuss a few of the mathematical operations
    made available in TensorFlow. The complete set of operations is available at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will look at the scatter operation.
  prefs: []
  type: TYPE_NORMAL
- en: Updating (scattering) values in tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A scatter operation, which refers to changing the values at certain indices
    of a tensor, is very common in scientific computing problems. This functionality
    was originally provided through an intimidating `tf.scatter_nd()` function, which
    can be difficult to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in recent TensorFlow versions, you can perform scatter operations
    via array indexing and slicing using NumPy-like syntax. Let’s see a few examples.
    Say you have the TensorFlow variable v, which is a [3,2] matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You can change the 0^(th) row of this tensor with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'which results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You can change the value at index [1,1] with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'which results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You can perform row slicing with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'which results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: It is important to remember that the scatter operation (performed via the `assign()`
    operation) can only be performed on `tf.Variables`, which are mutable structures.
    Remember that `tf.Tensor`/`tf.EagerTensor` are immutable objects.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting (gathering) values from a tensor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A gather operation is very similar to a scatter operation. Remember that scattering
    is about assigning values to tensors, whereas gathering retrieves the values of
    a tensor. Let’s understand this through an example. Say you have a TensorFlow
    tensor, `t`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You can obtain the 0^(th) row of `t` with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'which will return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also perform row-slicing with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'which will return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Unlike the scatter operation, the gather operation works both on `tf.Variable`
    and `tf.Tensor` structures.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network-related operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s look at several useful neural network-related operations that we
    will use heavily in the following chapters. The operations we will discuss here
    range from simple element-wise transformations (that is, activations) to computing
    partial derivatives of a set of parameters with respect to another value. We will
    also implement a simple neural network as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear activations used by neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nonlinear activations enable neural networks to perform well at numerous tasks.
    Typically, there is a nonlinear activation transformation (that is, activation
    layer) after each layer output in a neural network (except for the last layer).
    A nonlinear transformation helps a neural network to learn various nonlinear patterns
    that are present in data. This is very useful for complex real-world problems,
    where data often has more complex nonlinear patterns, in contrast to linear patterns.
    If not for the nonlinear activations between layers, a deep neural network would
    be a bunch of linear layers stacked on top of each other. Also, a set of linear
    layers can essentially be compressed to a single bigger linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, if not for the nonlinear activations, we cannot create a neural
    network with more than one layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s observe the importance of nonlinear activation through an example. First,
    recall the computation for the neural networks we saw in the sigmoid example.
    If we disregard b, it will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`h = sigmoid(W*x)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume a three-layer neural network (having `W1`, `W2`, and `W3` as layer weights)
    where each layer does the preceding computation; we can summarize the full computation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`h = sigmoid(W3*sigmoid(W2*sigmoid(W1*x)))`'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we remove the nonlinear activation (that is, sigmoid), we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`h = (W3 * (W2 * (W1 *x))) = (W3*W2*W1)*x`'
  prefs: []
  type: TYPE_NORMAL
- en: So, without the nonlinear activations, the three layers can be brought down
    to a single linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll list two commonly used nonlinear activations in neural networks (in
    other words, sigmoid and ReLU) and how they can be implemented in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The functional form of these computations is visualized in *Figure 2.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: The functional forms of sigmoid (left) and ReLU (right) activations'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A convolution operation is a widely used signal-processing technique. For images,
    convolution is used to produce different effects (such as blurring), or extract
    features (such as edges) from an image. An example of edge detection using convolution
    is shown in *Figure 2.7*. This is achieved by shifting a convolution filter on
    top of an image to produce a different output at each location (see *Figure* *2.8*
    later in this section). Specifically, at each location, we do element-wise multiplication
    of the elements in the convolution filter with the image patch (the same size
    as the convolution filter) that overlaps with the convolution filter and takes
    the sum of the multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_06.png](img/B14070_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Using the convolution operation for edge detection in an image
    (Source: [https://en.wikipedia.org/wiki/Kernel_(image_processing)](https://en.wikipedia.org/wiki/Kernel_(image_processing)))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the implementation of the convolution operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the apparently excessive number of square brackets used might make you
    think that the example can be made easy to follow by getting rid of these redundant
    brackets. Unfortunately, that is not the case. For the `tf.nn.conv2d(...)` operation,
    TensorFlow requires `input`, `filters`, and `strides` to be of an exact format.
    We will now go through each argument in `tf.conv2d(input, filters, strides, padding)`
    in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input**: This is typically a 4D tensor where the dimensions should be ordered
    as `[batch_size, height, width, channels]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size**: This is the amount of data (for example, inputs such as images,
    and words) in a single batch of data. We normally process data in batches as large
    datasets are used for learning. At a given training step, we randomly sample a
    small batch of data that approximately represents the full dataset. And doing
    this for many steps allows us to approximate the full dataset quite well. This
    `batch_size` parameter is the same as the one we discussed in the TensorFlow input
    pipeline example.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height and width**: This is the height and the width of the input.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**channels**: This is the depth of an input (for example, for an RGB image,
    the number of channels will be 3—a channel for each color).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**filters**: This is a 4D tensor that represents the convolution window of
    the convolution operation. The filter dimensions should be `[height, width, in_channels,
    out_channels]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height and width**: This is the height and the width of the filter (often
    smaller than that of the input)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**in_channels**: This is the number of channels of the input to the layer'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**out_channels**: This is the number of channels to be produced in the output
    of the layer'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strides**: This is a list with four elements, where the elements are `[batch_stride,
    height_stride, width_stride, channels_stride]`. The `strides` argument denotes
    how many elements to skip during a single shift of the convolution window on the
    input. Usually, you don’t have to worry about `batch_stride` and `channels_stride`.
    If you do not completely understand what `strides` is, you can use the default
    value of `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding**: This can be one of `[''SAME'', ''VALID'']`. It decides how to
    handle the convolution operation near the boundaries of the input. The `VALID`
    operation performs the convolution without padding. If we were to convolve an
    input of *n* length with a convolution window of size *h*, this will result in
    an output of size (*n-h+1 < n*). The diminishing of the output size can severely
    limit the depth of neural networks. `SAME` pads zeros to the boundary such that
    the output will have the same height and width as the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To gain a better understanding of what filter size, stride, and padding are,
    refer to *Figure 2.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_07.png](img/B14070_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: The convolution operation. Note how the kernel is moved over the
    input to compute values at each position'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the pooling operation.
  prefs: []
  type: TYPE_NORMAL
- en: The pooling operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A pooling operation behaves similarly to the convolution operation, but the
    final output is different. Instead of outputting the sum of the element-wise multiplication
    of the filter and the image patch, we now take the maximum element of the image
    patch for that location (see *Figure 2.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_08.png](img/B14070_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: The max-pooling operation'
  prefs: []
  type: TYPE_NORMAL
- en: Defining loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that, for a neural network to learn something useful, a loss needs
    to be defined. The loss represents how close or far away the predictions are from
    actual targets. There are several functions for automatically calculating the
    loss in TensorFlow, two of which are shown in the following code. The `tf.nn.l2_loss`
    function is the mean squared error loss, and `tf.nn.softmax_cross_entropy_with_logits`
    is another type of loss that actually gives better performance in classification
    tasks. And by logits here, we mean the unnormalized output of the neural network
    (that is, the linear output of the last layer of the neural network):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Here, we discussed several important operations intertwined with neural networks,
    such as the convolution operation and the pooling operation. We will now discuss
    how a sub-library in TensorFlow known as Keras can be used to build models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras: The model building API of TensorFlow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras was developed as a separate library that provides high-level building
    blocks to build models conveniently. It was initially platform-agnostic and supported
    many softwares (for example, TensorFlow and Theano).
  prefs: []
  type: TYPE_NORMAL
- en: However, TensorFlow acquired Keras and now is an integral part of TensorFlow
    for building models effortlessly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras’s primary focus is model building. For that, Keras provides several different
    APIs with varying degrees of flexibility and complexity. Choosing the right API
    for the job will require sound knowledge of the limitations of each API as well
    as experience. The APIs provided by Keras are:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential API – The most easy-to-use API. In this API, you simply stack layers
    on top of each other to create a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional API – The functional API provides more flexibility by allowing you
    to define custom models that can have multiple input layers/multiple output layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-classing API – The sub-classing API enables you to define custom reusable
    layers/models as Python classes. This is the most flexible API, but it requires
    strong familiarity with the API and raw TensorFlow operations to use it correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not confuse the Keras TensorFlow sub-module ([https://www.tensorflow.org/api_docs/python/tf/keras](https://www.tensorflow.org/api_docs/python/tf/keras))
    with the external Keras library ([https://keras.io/](https://keras.io/)). They
    share roots in terms of where they’ve come from, but they are not the same. You
    will run into strange issues if you treat them as the same during your development.
    In this book, we exclusively use `tf.keras`.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most innate concepts in Keras is that a model is composed of one
    or more layers connected in a specific way. Here, we will briefly go through what
    the code looks like, using different APIs to develop models. You are not expected
    to fully understand the code below. Rather, focus on the code style to spot any
    differences between the three methods.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using the Sequential API, you simply define your model as a list of layers.
    Here, the first element in the list is the closest to the input, where the last
    is the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have three layers. The first layer has 500 output
    nodes and takes in a vector of 784 elements as the input. The second layer is
    automatically connected to the first one, whereas the last layer is connected
    to the second layer. All of these layers are fully-connected layers, where all
    input nodes are connected to all output nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Functional API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the Functional API, we do things differently. We first define one or more
    input layers, and other layers that carry computations. Then we connect the inputs
    to outputs ourselves, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code, we start with an input layer that accepts a 784 element-long vector.
    The input is passed to a Dense layer that has 500 nodes. The output of that layer
    is assigned to `out_1`. Then `out_1` is passed to another Dense layer, which outputs
    `out_2`. Next, a Dense layer with 10 nodes outputs the final output. Finally,
    the model is defined as a `tf.keras.models.Model` object that takes two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: inputs – One or more input layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: outputs – One or more outputs produced by any `tf.keras.layers` type object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is identical to what was defined in the previous section. One of the
    benefits of the Functional API is that you can create far more complex models
    as you’re not bounded to have layers as a list. Because of this freedom, you can
    have multiple inputs connecting to many layers in many different ways and potentially
    produce many outputs as well.
  prefs: []
  type: TYPE_NORMAL
- en: Sub-classing API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we will use the sub-classing API to define a model. With sub-classing,
    you define your model as a Python object that inherits from the base object, `tf.keras.Model`.
    When using sub-classing, you need to define two important functions: `__init__()`,
    which will specify any special parameters, layers, and so on required to successfully
    perform the computations, and `call()`, which defines the computations that need
    to happen in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can see that our model has three layers, just like all the previous
    models we defined. Next, the call function defines how these layers connect to
    produce the final output. The sub-classing API is considered the most difficult
    to master, mainly due to the freedom allowed by the method. However, the rewards
    are immense once you learn the API as it enables you to define very complex models/layers
    as unit computations that can be reused later. Now that you understand how each
    API works, let’s implement a neural network using Keras and train it on a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our first neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Great! Now that you’ve learned the architecture and foundations of TensorFlow,
    it’s high time that we move on and implement something slightly more complex.
    Let’s implement a neural network. Specifically, we will implement a fully connected
    neural network model (FCNN), which we discussed in *Chapter 1*, *Introduction
    to Natural Language Processing*.
  prefs: []
  type: TYPE_NORMAL
- en: One of the stepping stones to the introduction of neural networks is to implement
    a neural network that is able to classify digits. For this task, we will be using
    the famous MNIST dataset made available at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  prefs: []
  type: TYPE_NORMAL
- en: You might feel a bit skeptical regarding our using a computer vision task rather
    than an NLP task. However, vision tasks can be implemented with less preprocessing
    and are easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: As this is our first encounter with neural networks, we will see how to implement
    this model using Keras. Keras is the high-level submodule that provides a layer
    of abstraction over TensorFlow. Therefore, you can implement neural networks with
    much less effort with Keras than using TensorFlow’s raw operations. To run the
    examples end to end, you can find the full exercise in the `tensorflow_introduction.ipynb`
    file in the `Ch02-Understanding-TensorFlow` folder. The next step is to prepare
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to download the dataset. TensorFlow out of the box provides
    convenient functions to download data and MNIST is one of those supported datasets.
    We will be performing four important steps during the data preparation:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data and storing it as `numpy.ndarray` objects. We will create
    a folder named data within our `ch2` directory and store the data there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping the images so that 2D grayscale images in the dataset will be converted
    to 1D vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing the images to have a zero-mean and unit-variance (also known as
    **whitening**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding the integer class labels. One-hot encoding refers to the process
    of representing integer class labels as a vector. For example, if you have 10
    classes and a class label of 3 (where labels range from 0-9), your one-hot encoded
    vector will be `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code performs these functions for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that we are using the `tf.keras.datasets.mnist.load_data()` function
    provided by TensorFlow to download the training and testing data. It will be downloaded
    to a folder named `data` within the `Ch02-Understanding-TensorFlow` folder. This
    will provide four output tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x_train` – A 60000 x 28 x 28 sized tensor where each image is 28 x 28'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_train` – A 60000 sized vector, where each element is a class label between
    0-9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x_test` – A 10000 x 28 x 28 sized tensor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_test` – A 10000 sized vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the data is downloaded, we reshape the 28 x 28 sized images into a 1D vector.
    This is because we will be implementing a fully connected neural network. Fully
    connected neural networks take a 1D vector as the input. Therefore, all the pixels
    in the image will be arranged as a sequence of pixels in order to feed into the
    model. Finally, if you look at the range of values present in the `x_train` and
    `x_test` tensors, they will be in the range of 0-255 (typical grayscale range).
    We would bring these values to a zero mean unit-variance range by subtracting
    the mean of each image and dividing by the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the neural network with Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now examine how to implement the type of neural network we discussed in
    *Chapter 1, Introduction to Natural Language Processing*, with Keras. The network
    is a fully connected neural network with 3 layers having 500, 250, and 10 nodes,
    respectively. The first two layers will use ReLU activation, whereas the last
    layer uses softmax. To implement this, we are going to use the simplest of the
    Keras APIs available to us – the Sequential API.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the full exercise in the `tensorflow_introduction.ipynb` file
    in the `Ch02-Understanding-TensorFlow` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that all it takes is a single line in the Keras Sequential API
    to define the model we just defined. Keras provides various types of layers. You
    can see the full list of layers available to you at [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers).
    For a fully connected network, we only need Dense layers that mimic the computations
    of a hidden layer in a fully connected network. With the model defined, you need
    to compile this model with an appropriate loss function, an optimizer, and, optionally,
    performance metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: With the model defined and compiled, we can now train our model on the prepared
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training a model could not be easier in Keras. Once the data is prepared, all
    you need to do is call the `model.fit()` function with the required arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '`model.fit()` accepts several important arguments. We will go through them
    in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x` – An input tensor. In our case, this is a 60000 x 784 sized tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y` – The one-hot encoded label tensor. In our case, this is a 60000 x 10 sized
    tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` – Deep learning models are trained with batches of data (in other
    words, stochastically) as opposed to feeding the full dataset at once. The batch
    size defines how many examples are included in a single batch. The larger the
    batch size, the better the accuracy of your model would be generally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs` – Deep learning models iterate through the dataset in batches several
    times. The number of times iterated through the dataset is known as the number
    of epochs. In our example, this is set to 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_split` – When training deep learning models, a validation set is
    used to monitor performance, where the validation set acts as a proxy for real-world
    performance. `validation_split` defines how much of the full dataset is to be
    used as the validation subset. In our example, this is set to 20% of the total
    dataset size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s what the training loss and validation accuracy look like over the number
    of epochs we trained the model (*Figure 2.10*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Training loss and validation accuracy over 10 epochs as the model
    is trained'
  prefs: []
  type: TYPE_NORMAL
- en: Next up is testing our model on some unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Testing the model is also straightforward. During testing, we measure the loss
    and the accuracy of the model on the test dataset. In order to evaluate the model
    on a dataset, Keras models provide a convenient function called `evaluate()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments expected by the `model.evaluate()` function are already covered
    during our discussion of `model.fit()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x` – An input tensor. In our case, this is a 10000 x 784 sized tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y` – The one-hot encoded label tensor. In our case, this is a 10000 x 10 sized
    tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` – Batch size defines how many examples are included in a single
    batch. The larger the batch size, the better the accuracy of your model would
    be generally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will get a loss of 0.138 and an accuracy of 98%. You will not get the exact
    same values due to various randomness present in the model, as well as during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we went through an end-to-end example of training a neural
    network. We prepared the data, trained the model on that data, and finally tested
    it on some unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you took your first steps to solving NLP tasks by understanding
    the primary underlying platform (TensorFlow) on which we will be implementing
    our algorithms. First, we discussed the underlying details of TensorFlow architecture.
    Next, we discussed the essential ingredients of a meaningful TensorFlow program.
    We got to know some new features in TensorFlow 2, such as the AutoGraph feature,
    in depth. We then discussed more exciting elements in TensorFlow such as data
    pipelines and various TensorFlow operations.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we discussed the TensorFlow architecture by lining up the explanation
    with an example TensorFlow program; the sigmoid example. In this TensorFlow program,
    we used the AutoGraph feature to generate a TensorFlow graph; that is, using the
    `tf.function()` decorator over the function that performs the TensorFlow operations.
    Then, a `GraphDef` object was created representing the graph and sent to the distributed
    master. The distributed master looked at the graph, decided which components to
    use for the relevant computation, and divided it into several subgraphs to make
    the computations faster. Finally, workers executed subgraphs and returned the
    result immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we discussed the various elements that comprise a typical TensorFlow
    client: inputs, variables, outputs, and operations. Inputs are the data we feed
    to the algorithm for training and testing purposes. We discussed three different
    ways of feeding inputs: using NumPy arrays, preloading data as TensorFlow tensors,
    and using `tf.data` to define an input pipeline. Then we discussed TensorFlow
    variables, how they differ from other tensors, and how to create and initialize
    them. Following this, we discussed how variables can be used to create intermediate
    and terminal outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed several available TensorFlow operations, including mathematical
    operations, matrix operations, and neural network-related operations that will
    be used later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, we discussed Keras, a sub-module in TensorFlow that supports building
    models. We learned that there are three different APIs for building models: the
    Sequential API, the Functional API, and the Sub-classing API. We learned that
    the Sequential API is the easiest to use, whereas the Sub-classing API takes much
    more effort. However, the Sequential API is very restrictive in terms of the type
    of models that can be implemented with it.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we implemented a neural network using all the concepts learned previously.
    We used a three-layer neural network to classify a MNIST digit dataset, and we
    used Keras (a high-level sub-module in TensorFlow) to implement this model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to use the fully connected neural network
    we implemented in this chapter for learning the semantic, numerical word representation
    of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)'
  prefs: []
  type: TYPE_IMG
