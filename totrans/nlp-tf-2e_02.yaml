- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Understanding TensorFlow 2
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 TensorFlow 2
- en: 'In this chapter, you will get an in-depth understanding of TensorFlow. This
    is an open source distributed numerical computation framework, and it will be
    the main platform on which we will be implementing all our exercises. This chapter
    covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将让你深入理解 TensorFlow。它是一个开源的分布式数值计算框架，也是我们将实现所有练习的主要平台。本章涵盖以下主题：
- en: What is TensorFlow?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: The building blocks of TensorFlow (for example, variables and operations)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 的构建模块（例如，变量和操作）
- en: Using Keras for building models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 构建模型
- en: Implementing our first neural network
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现我们的第一个神经网络
- en: We will get started with TensorFlow by defining a simple calculation and trying
    to compute it using TensorFlow. After we complete this, we will investigate how
    TensorFlow executes this computation. This will help us to understand how the
    framework creates a computational graph to compute the outputs and execute this
    graph to obtain the desired outputs. Then we will dive into the details of how
    TensorFlow architecture operates by looking at how TensorFlow executes things,
    with the help of an analogy of how a fancy café works. We will then see how TensorFlow
    1 used to work so that we can better appreciate the amazing features TensorFlow
    2 offers. Note that when we use the word “TensorFlow” by itself, we are referring
    to TensorFlow 2\. We will specifically mention TensorFlow 1 if we are referring
    to TensorFlow 1.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过定义一个简单的计算并尝试使用 TensorFlow 来计算它，开始学习 TensorFlow。完成这一步后，我们将研究 TensorFlow
    如何执行这个计算。这将帮助我们理解框架是如何创建一个计算图来计算输出，并执行该图以获得期望的输出。接着，我们将通过使用一个类比——一个高级咖啡馆是如何运作的——来深入了解
    TensorFlow 架构如何运作，了解 TensorFlow 如何执行任务。然后，我们将回顾 TensorFlow 1 的工作方式，以便更好地理解 TensorFlow
    2 所提供的惊人功能。请注意，当我们单独使用“TensorFlow”这个词时，我们指的是 TensorFlow 2。如果我们提到 TensorFlow 1，则会特别说明。
- en: Having gained a good conceptual and technical understanding of how TensorFlow
    operates, we will look at some of the important computations the framework offers.
    First, we will look at defining various data structures in TensorFlow, such as
    variables and tensors, and we’ll also see how to read inputs through data pipelines.
    Then we will work through some neural network-related operations (for example,
    convolution operation, defining losses, and optimization).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 TensorFlow 的操作有了很好的概念性和技术性理解之后，我们将探讨框架提供的一些重要计算。首先，我们将了解如何在 TensorFlow 中定义各种数据结构，例如变量和张量，并且我们还会看到如何通过数据管道读取输入。接着，我们将学习一些与神经网络相关的操作（例如，卷积操作、定义损失和优化）。
- en: Finally, we will apply this knowledge in an exciting exercise, where we will
    implement a neural network that can recognize images of handwritten digits. You
    will also see that you can implement or prototype neural networks very quickly
    and easily by using a high-level submodule such as Keras.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在一个令人兴奋的练习中应用这些知识，实施一个可以识别手写数字图像的神经网络。你还将看到，通过使用像 Keras 这样的高级子模块，你可以非常快速和轻松地实现或原型化神经网络。
- en: What is TensorFlow?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: In *Chapter 1*, *Introduction to Natural Language Processing*, we briefly discussed
    what TensorFlow is. Now let’s take a closer look at it. TensorFlow is an open
    source, distributed numerical computation framework released by Google that is
    mainly intended to alleviate the painful details of implementing a neural network
    (for example, computing derivatives of the weights of the neural network). TensorFlow
    takes this a step further by providing efficient implementations of such numerical
    computations using **Compute Unified Device Architecture** (**CUDA**), which is
    a parallel computational platform introduced by NVIDIA (for more information on
    CUDA, visit [https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)).
    The **Application Programming Interface** (**API**) of TensorFlow at [https://www.tensorflow.org/api_docs/python/tf/all_symbols](https://www.tensorflow.org/api_docs/python/tf/all_symbols)
    shows that TensorFlow provides thousands of operations that make our lives easier.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 1 章*，*自然语言处理简介* 中，我们简要讨论了什么是 TensorFlow。现在让我们更仔细地了解它。TensorFlow 是由 Google
    发布的一个开源分布式数值计算框架，主要目的是缓解实现神经网络时的痛苦细节（例如，计算神经网络权重的导数）。TensorFlow 通过使用 **计算统一设备架构**（**CUDA**），进一步提供了高效的数值计算实现，CUDA
    是 NVIDIA 推出的并行计算平台（关于 CUDA 的更多信息，请访问 [https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)）。TensorFlow
    的 **应用程序编程接口**（**API**）可以在 [https://www.tensorflow.org/api_docs/python/tf/all_symbols](https://www.tensorflow.org/api_docs/python/tf/all_symbols)
    查到，显示了 TensorFlow 提供了成千上万的操作，让我们的生活更轻松。
- en: TensorFlow was not developed overnight. This is a result of the persistence
    of talented, good-hearted developers and scientists who wanted to make a difference
    by bringing deep learning to a wider audience. If you are interested, you can
    take a look at the TensorFlow code at [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow).
    Currently, TensorFlow has around 3,000 contributors, and it sits on top of more
    than 115,000 commits, evolving to be better and better every day.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 不是一夜之间开发出来的。这是由一群有才华、心地善良的开发者和科学家的坚持努力的结果，他们希望通过将深度学习带给更广泛的受众来有所改变。如果你感兴趣，可以查看
    TensorFlow 的代码，地址是 [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)。目前，TensorFlow
    拥有约 3,000 名贡献者，并且已经有超过 115,000 次提交，每天都在不断发展，变得越来越好。
- en: Getting started with TensorFlow 2
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 TensorFlow 2
- en: 'Now let’s learn about a few essential components in the TensorFlow framework
    by working through a code example. Let’s write an example to perform the following
    computation, which is very common for neural networks:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过一个代码示例来学习 TensorFlow 框架中的一些基本组件。我们来编写一个执行以下计算的示例，这是神经网络中非常常见的操作：
- en: '![](img/B14070_02_002.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_02_002.png)'
- en: 'This computation encompasses what happens in a single layer of a fully connected
    neural network. Here `W` and `x` are matrices and `b` is a vector. Then, “`.`"
    denotes the dot product. sigmoid is a non-linear transformation given by the following
    equation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算涵盖了全连接神经网络中单个层发生的操作。这里 `W` 和 `x` 是矩阵，`b` 是向量。然后，“`.`”表示点积。sigmoid 是一个非线性变换，给定以下方程：
- en: '![](img/B14070_02_001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_02_001.png)'
- en: We will discuss how to do this computation through TensorFlow step by step.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步讨论如何通过 TensorFlow 来进行此计算。
- en: 'First, we will need to import TensorFlow and NumPy. NumPy is another scientific
    computation framework that provides various mathematical and other operations
    to manipulate data. Importing them is essential before you run any type of TensorFlow
    or NumPy-related operation in Python:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入 TensorFlow 和 NumPy。NumPy 是另一个科学计算框架，提供了各种数学和其他操作来处理数据。在运行任何与 TensorFlow
    或 NumPy 相关的操作之前，导入它们是必不可少的：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, we will write a function that can take the inputs `x`, `W`, and `b`
    and perform this computation for us:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将编写一个函数，该函数可以接收 `x`、`W` 和 `b` 作为输入，并为我们执行这个计算：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we add a Python decorator called `tf.function` as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个名为 `tf.function` 的 Python 装饰器，如下所示：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Put simply, a Python decorator is just another function. A Python decorator
    provides a clean way to call another function whenever you call the decorated
    function. In other words, every time the `layer()` function is called, `tf.function()`
    is called. This can be used for various purposes, such as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Python 装饰器就是另一个函数。Python 装饰器提供了一种干净的方式来调用另一个函数，每次调用被装饰的函数时。换句话说，每次调用 `layer()`
    函数时，都会调用 `tf.function()`。这可以用于多种目的，例如：
- en: Logging the content and operations in a function
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating the inputs and outputs of another function
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the `layer()` function is passing through `tf.function()`, TensorFlow will
    trace the content (in other words, the operations and data) in the function and
    build a computational graph automatically.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The computational graph (also known as the dataflow graph) builds a DAG (a directed
    acyclic graph) that shows what kind of inputs are required, and what sort of computations
    need to be done in the program.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the `layer()` function produces `h` by using inputs `x`, `W`,
    and `b`, and some transformations or operations such as `+` and `tf.matmul()`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: A computational graph of the client'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: If we look at an analogy for a DAG, if you think of the output as a *cake*,
    then the *graph* would be the recipe to make that cake using *ingredients* (that
    is, inputs).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The feature that builds this computational graph automatically in TensorFlow
    is known as **AutoGraph**. AutoGraph is not just looking at the operations in
    the passed function; it also scrutinizes the flow of operations. This means that
    you can have `if` statements, or `for`/`while` loops in your function, and AutoGraph
    will take care of those when building the graph. You will see more on AutoGraph
    in the next section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow 1.x, the user needed to implement the computational graph explicitly.
    This meant the user could not write typical Python code using `if-else` statements
    or `for` loops, but had to explicitly control the flow of operations using special
    bespoke TensorFlow operations such as `tf.cond()` and `tf.control_dependencies()`.
    This is because, unlike TensorFlow 2.x, TensorFlow 1.x did not immediately execute
    operations when you called them. Rather, after they were defined, they needed
    to be executed explicitly using the context of a TensorFlow `Session`. For example,
    when you run the following in TensorFlow 1,
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '`h = tf.nn.sigmoid(tf.matmul(x,W) + b)`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '`h` will not have any value until `h` is executed in the context of a `Session`.
    Therefore, `h` could not be treated like any other Python variable. Don’t worry
    if you don’t understand how the `Session` works. It will be discussed in the coming
    sections.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can use this function right away, as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, `x` is a simple NumPy array:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`W` and `b` are TensorFlow variables defined using the `tf.Variable` object.
    `W` and `b` hold tensors. A tensor is essentially an *n*-dimensional array. For
    example, a one-dimensional vector or a two-dimensional matrix are called **tensors**.
    A `tf.Variable` is a mutable structure, which means the values in the tensor stored
    in that variable can change over time. For example, variables are used to store
    neural network weights, which change during the model optimization.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note that for `W` and `b`, we provide some important arguments, such
    as the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These are called variable initializers and are the tensors that will be assigned
    to the `W` and `b` variables initially. A variable must have an initial value
    provided. Here, `tf.initializers.RandomUniform` means that we uniformly sample
    values between `minval` `(-0.1)` and `maxval` `(0.1)` to assign values to the
    tensors. There are many different initializers provided in TensorFlow ([https://www.tensorflow.org/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)).
    It is also very important to define the *shape* of your initializer when you are
    defining the initializer itself. The `shape` property defines the size of each
    dimension of the output tensor. For example, if `shape` is `[10, 5]`, this means
    that it will be a two-dimensional structure and will have `10` elements on axis
    0 (rows) and `5` elements on axis 1 (columns):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些被称为变量初始化器，是会被初始赋值给 `W` 和 `b` 变量的张量。变量必须提供一个初始值。在这里，`tf.initializers.RandomUniform`
    表示我们在 `minval` `(-0.1)` 和 `maxval` `(0.1)` 之间均匀地抽取值并赋给张量。TensorFlow 提供了许多不同的初始化器（[https://www.tensorflow.org/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)）。在定义初始化器时，定义初始化器的
    *shape*（形状）属性也非常重要。`shape` 属性定义了输出张量的每个维度的大小。例如，如果 `shape` 是 `[10, 5]`，这意味着它将是一个二维结构，在轴
    0（行）上有 `10` 个元素，在轴 1（列）上有 `5` 个元素：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, `h` is called a TensorFlow tensor in general. A TensorFlow tensor is
    an immutable structure. Once a value is assigned to a TensorFlow tensor, it cannot
    be changed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`h` 通常被称为 TensorFlow 张量。TensorFlow 张量是一个不可变结构。一旦一个值被赋给 TensorFlow 张量，它就不能再被更改。
- en: 'As you can see, the term “tensor” is used in two ways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，“张量”（tensor）这个术语有两种使用方式：
- en: To refer to an *n*-dimensional array
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要引用一个 *n* 维数组
- en: To refer to an immutable data structure in TensorFlow
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要引用 TensorFlow 中的不可变数据结构
- en: For both, the underlying concept is the same as they hold an *n*-dimensional
    data structure, only differing in the context they are used. The term will be
    used interchangeably to refer to these structures in our discussion.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两者，底层的概念是相同的，因为它们都持有一个 *n* 维的数据结构，只是在使用的上下文上有所不同。我们将在讨论中交替使用这个术语来指代这些结构。
- en: Finally, you can immediately see the value of `h` using,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以立即看到 `h` 的值，通过以下代码：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: which will give,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `numpy()` function retrieves the NumPy array from the TensorFlow Tensor
    object. The full code is as below. All the code examples in this chapter will
    be available in the `tensorflow_introduction.ipynb` file in the `ch2` folder:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy()` 函数从 TensorFlow 张量对象中获取 NumPy 数组。完整的代码如下。章节中的所有代码示例都可以在 `ch2` 文件夹中的
    `tensorflow_introduction.ipynb` 文件中找到：'
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For future reference, let’s call our example *the sigmoid example*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 供以后参考，我们称这个示例为 *sigmoid 示例*。
- en: As you can already see, defining a TensorFlow computational graph and executing
    that is very “Pythonic”. This is because TensorFlow executes its operations “eagerly”,
    or immediately after the `layer()` function is called. This is a special mode
    in TensorFlow known as *eager execution* mode. This was an optional mode for TensorFlow
    1, but has been made the default in TensorFlow 2\.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，定义 TensorFlow 计算图并执行它是非常“Pythonic”的。这是因为 TensorFlow 执行其操作是“急切的”（eager），即在调用`layer()`函数后立即执行。这是
    TensorFlow 中一种特殊模式，称为 *急切执行* 模式。在 TensorFlow 1 中这是一个可选模式，但在 TensorFlow 2 中已经成为默认模式。
- en: Also note that the next two sections will be somewhat complex and technical.
    However, don’t worry if you don’t understand everything completely because the
    explanation will be supplemented with a more digestible, and thorough, real-world
    example that explains how an order is fulfilled in our new-and-improved restaurant,
    *Café Le TensorFlow 2*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，接下来的两个章节将会比较复杂且技术性较强。然而，如果你不能完全理解所有内容也不用担心，因为接下来的解释将通过一个更易于理解且全面的实际示例进行补充，这个示例将解释我们改进过的新餐厅
    *Café Le TensorFlow 2* 中如何完成一个订单。
- en: TensorFlow 2 architecture – What happens during graph build?
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 2 架构 – 图构建过程中发生了什么？
- en: Let’s now understand what TensorFlow does when you execute TensorFlow operations.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来了解当你执行 TensorFlow 操作时，TensorFlow 会做些什么。
- en: When you call a function decorated by `tf.function()`, such as the `layer()`
    function, there is quite a bit happening in the background. First, TensorFlow
    will trace all the TensorFlow operations taking place in the function and build
    the computational graph automatically.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用一个由 `tf.function()` 装饰的函数时，比如 `layer()` 函数，后台会发生很多事情。首先，TensorFlow 会追踪函数中所有发生的
    TensorFlow 操作，并自动构建计算图。
- en: In fact, `tf.function()` will return a function that executes the built dataflow
    graph when invoked. Therefore, `tf.function()` is a multi-stage process, where
    it first builds the dataflow graph and then executes it. Additionally, since TensorFlow
    traces each line in the function, if something goes wrong, TensorFlow can point
    to the exact line that is causing the issue.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`tf.function()` 会返回一个在调用时执行已构建数据流图的函数。因此，`tf.function()` 是一个多阶段的过程，它首先构建数据流图，然后执行它。此外，由于
    TensorFlow 跟踪函数中的每一行代码，如果发生问题，TensorFlow 可以指明导致问题的确切行。
- en: 'In our sigmoid example, the computational, or dataflow, graph would look like
    *Figure 2.2*. A single element or vertex of the graph is called a **node.** There
    are two main types of objects in this graph: *operations* and *tensors*. In the
    preceding example, `tf.nn.sigmoid` is an operation and `h` is a tensor:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Sigmoid 示例中，计算图或数据流图看起来像*图 2.2*。图的单个元素或顶点称为**节点**。这个图中有两种主要类型的对象：*操作*和*张量*。在前面的示例中，`tf.nn.sigmoid`
    是一个操作，`h` 是一个张量：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
- en: 'Figure 2.2: A computational graph of the client'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：客户端的计算图
- en: The preceding graph shows the order of operations as well as how inputs flow
    through them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图展示了操作的顺序以及输入如何流经这些操作。
- en: 'Keep in mind that `tf.function()` or AutoGraph is not a silver bullet that
    turns any arbitrary Python function using TensorFlow operations into a computational
    graph; it has its limitations. For example, the current version cannot handle
    recursive calls. To see a full list of the eager mode capabilities, refer to the
    following link: [https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md](https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`tf.function()` 或 AutoGraph 并不是一个万能的解决方案，不能将任何使用 TensorFlow 操作的任意 Python
    函数转换为计算图；它有其局限性。例如，当前版本无法处理递归调用。要查看 eager 模式的完整功能列表，请参考以下链接：[https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md](https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md)。
- en: Now we know that TensorFlow is skilled at creating a nice computational graph,
    with all the dependencies and operations so that it knows exactly how, when, and
    where the data flows. However, we did not quite answer how this graph is executed.
    In fact, TensorFlow does quite a bit behind the scenes. For example, the graph
    might be divided into subgraphs, and subsequently into even finer pieces, to achieve
    parallelization. These subgraphs or pieces will then be assigned to workers that
    will perform the assigned task.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道 TensorFlow 擅长创建一个包含所有依赖和操作的漂亮计算图，这样它就能准确知道数据如何、何时以及在哪里流动。然而，我们还没有完全回答这个图是如何执行的。事实上，TensorFlow
    在幕后做了很多工作。例如，图可能会被划分成子图，并进一步拆分成更细的部分，以实现并行化。这些子图或部分将被分配给执行指定任务的工作进程。
- en: TensorFlow architecture – what happens when you execute the graph?
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 架构——执行图时发生了什么？
- en: The computational graph uses the `tf.GraphDef` protocol to canonicalize the
    dataflow graph and send it to the distributed master. The distributed master would
    perform the actual operation execution and parameter updates in a single-process
    setting. In a distributed setting, the master would delegate these tasks to worker
    processes/devices and manage these worker processes. `tf.GraphDef` is a standardized
    representation of the graph specific to TensorFlow. The distributed master sees
    all computations in the graph and divides the computations into different devices
    (for example, different GPUs and CPUs). TensorFlow operations have multiple kernels.
    A kernel is a device-specific implementation of a certain operation. For example,
    the `tf.matmul()` function will be implemented differently to run on the CPU or
    GPU since, on a GPU, you can achieve much better performance due to more parallelization.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图使用 `tf.GraphDef` 协议来标准化数据流图并将其发送到分布式主节点。分布式主节点将在单进程环境中执行实际的操作执行和参数更新。在分布式环境中，主节点将把这些任务委派给工作进程/设备并管理这些工作进程。`tf.GraphDef`
    是特定于 TensorFlow 的图的标准化表示。分布式主节点可以看到图中的所有计算，并将计算分配到不同的设备（例如，不同的 GPU 和 CPU）。TensorFlow
    操作有多个内核。内核是特定于设备的某个操作的实现。例如，`tf.matmul()` 函数会根据是在 CPU 还是 GPU 上运行进行不同的实现，因为在 GPU
    上可以通过更多的并行化来实现更好的性能。
- en: Next, the computational graph will be broken into subgraphs and pruned by the
    distributed master. Although decomposing the computational graph in *Figure 2.2*
    appears too trivial in our example, the computational graph can exponentially
    grow in real-world solutions with many hidden layers. Additionally, it becomes
    important to break the computational graph into multiple pieces and shave off
    any redundant computations in order to get results faster (for example, in a multi-device
    setting).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，计算图将被分解为子图，并由分布式主节点进行修剪。尽管在我们的示例中，分解*图2.2*中的计算图看起来过于简单，但在实际应用中，计算图可能会在包含多个隐藏层的解决方案中呈指数级增长。此外，为了更快地获得结果（例如，在多设备环境中），将计算图分解成多个部分，并去除任何冗余计算，变得尤为重要。
- en: Executing the graph or a subgraph (if the graph is divided into subgraphs) is
    called a single *task*, where each task is allocated to a single worker (which
    could be a single process or an entire device). These workers can run as a single
    process in a multi-process device (for example, a multi-processing CPU), or run
    on different devices (for example, CPUs and GPUs). In a distributed setting, we
    would have multiple workers executing tasks (for example, multiple workers training
    the model on different batches of data). On the contrary, we have only one set
    of parameters. So how do multiple workers manage to update the same set of parameters?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 执行图或子图（如果图被分为多个子图）称为单个*任务*，每个任务被分配给一个工作进程（该进程可以是一个单独的进程或一个完整的设备）。这些工作进程可以在多进程设备中作为单个进程运行（例如，多核CPU），或在不同的设备上运行（例如，CPU和GPU）。在分布式环境中，我们会有多个工作进程执行任务（例如，多个工作进程在不同的数据批次上训练模型）。相反，我们只有一组参数。那么，多个工作进程如何管理更新同一组参数呢？
- en: 'To solve this, there is one worker that is considered the parameter server
    and will hold the main copy of the parameters. The workers will copy the parameters
    over, update them, and send them back to the parameter server. Typically, the
    parameter server will define some resolution strategy to resolve multiple updates
    coming from multiple workers (for example, taking the mean). These details were
    provided so you can understand the complexity that has gone into TensorFlow. However,
    our book will be based on using TensorFlow in a single-process/worker setting.
    In this setting, the organization of the distributed master, workers, and the
    parameter server is much more straightforward and is absorbed mostly by a special
    session implementation used by TensorFlow. This general workflow of a TensorFlow
    client is depicted in *Figure 2.3*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，会有一个工作进程被视为参数服务器，并持有参数的主要副本。其他工作进程将复制这些参数，更新它们，然后将更新后的参数发送回参数服务器。通常，参数服务器会定义一些解决策略来处理来自多个工作进程的多个更新（例如，取平均值）。这些细节的提供是为了帮助你理解TensorFlow中涉及的复杂性。然而，我们的书籍将基于在单进程/单工作进程设置中使用TensorFlow。在这种设置下，分布式主节点、工作进程和参数服务器的组织方式要简单得多，并且大部分都由TensorFlow使用的特殊会话实现来吸收。TensorFlow客户端的这一通用工作流程在*图2.3*中得到了展示：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_02.jpg](img/B14070_02_03.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_02.jpg](img/B14070_02_03.png)'
- en: 'Figure 2.3: The generic execution of a TensorFlow client. A TensorFlow client
    starts with a graph that gets sent to the distributed master. The master spins
    up worker processes to perform actual tasks and parameter updates'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：TensorFlow客户端的通用执行。TensorFlow客户端从一个图开始，图被发送到分布式主节点。主节点启动工作进程来执行实际任务和参数更新。
- en: 'Once the calculation is done, the session brings back the updated data to the
    client from the parameter server. The architecture of TensorFlow is shown in *Figure
    2.4*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算完成，会话将从参数服务器中将更新后的数据返回给客户端。TensorFlow的架构如*图2.4*所示：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_04.png](img/B14070_02_04.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_04.png](img/B14070_02_04.png)'
- en: 'Figure 2.4: TensorFlow framework architecture. This explanation is based on
    the official TensorFlow documentation found at: [https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：TensorFlow框架架构。此解释基于官方的TensorFlow文档，文档链接为：[https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md)
- en: Most of the changes introduced in TensorFlow 2 can be attributed to front-end
    changes. That is, how the dataflow graph is built and when the graph is executed.
    The way the graph is executed remains more or less the same in TensorFlow 1 and
    2.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2 中引入的大部分变化都可以归结为前端的变化。也就是说，数据流图是如何构建的，以及何时执行图。图的执行方式在 TensorFlow
    1 和 2 中基本保持不变。
- en: Now we know what happens end-to-end from the moment you execute `tf.function()`,
    but this was a very technical explanation, and nothing explains something better
    than a good analogy. Therefore, we will try to understand TensorFlow 2 with an
    analogy to our new and improved Café Le TensorFlow 2.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了从你执行 `tf.function()` 这一刻起发生的端到端过程，但这只是一个非常技术性的解释，最好的理解方式是通过一个好的类比。因此，我们将尝试通过一个类比来理解
    TensorFlow 2，就像我们对新升级版的 Café Le TensorFlow 2 的理解一样。
- en: Café Le TensorFlow 2 – understanding TensorFlow 2 with an analogy
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Café Le TensorFlow 2 – 通过类比来理解 TensorFlow 2
- en: Let’s say the owners renovated our previous Café Le TensorFlow (this is an analogy
    from the first edition) and reopened it as Café Le TensorFlow 2\. The word around
    the town is that it’s much more opulent than it used to be. Remembering the great
    experience you had before, you book a table instantly and go there to grab a seat.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设老板们对我们之前的 Café Le TensorFlow（这是第一版的类比）进行了翻新，并重新开业为 Café Le TensorFlow 2。镇上传闻它比以前更加奢华。记得之前那次美好的体验后，你立刻预定了座位并赶去那里占个座。
- en: You want to order a *chicken burger with extra cheese and no tomatoes*. And
    you realize the café is indeed fancy. There’re no waiters here, but a voice-enabled
    tablet for each table into which you say what you want. This will get converted
    to a standard format that the chefs will understand (for example, table number,
    menu item ID, quantity, and special requirements).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你想点一份*加奶酪、不加番茄的鸡肉汉堡*。然后你意识到这家咖啡馆确实很高档。这里没有服务员，每桌都有一个语音启用的平板电脑，你可以对它说出你想要的。这会被转化为厨师能理解的标准格式（例如，桌号、菜单项
    ID、数量和特别要求）。
- en: Here, you represent the TensorFlow 2 program. The ability of the voice-enabled
    tablet that converts your voice (or TensorFlow operations) to the standard format
    (or GraphDef format) is analogous to the AutoGraph feature.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，你代表了 TensorFlow 2 程序。将你的语音（或 TensorFlow 操作）转化为标准格式（或 GraphDef 格式）的语音启用平板电脑功能，类似于
    AutoGraph 特性。
- en: Now comes the best part. As soon as you start speaking, a manager will be looking
    at your order and assigning various tasks to chefs. The manager is responsible
    for making sure things happen as quickly as possible. The kitchen manager makes
    decisions, such as how many chefs are required to make the dish and which chefs
    are the best candidates for the job. The kitchen manager represents the distributed
    master.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在到了最精彩的部分。一旦你开始说话，经理就会查看你的订单，并将各种任务分配给厨师。经理负责确保一切尽可能快地完成。厨房经理做出决策，例如需要多少厨师来制作这道菜，哪些厨师是最合适的。厨房经理代表着分布式的主节点。
- en: Each chef has a cook whose responsibility it is to provide the chef with the
    right ingredients, equipment, and so forth. So, the kitchen manager takes the
    order to a single chef and a cook (a burger is not that hard to prepare) and asks
    them to prepare the dish. The chef looks at the order and tells the cook what
    is needed. So, the cook first finds the things that will be required (for example,
    buns, patties, and onions) and keeps them close to fulfill the chef’s requests
    as soon as possible. Moreover, the chef might also ask to keep the intermediate
    results (for example, cut vegetables) of the dish temporarily until the chef needs
    it back again. In our example, the chef is the operation executor, and the cook
    is the parameter server.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每个厨师都有一个助手，负责为厨师提供所需的食材、设备等。因此，厨房经理会将订单交给一位厨师和一位助手（比如，汉堡的准备并不难），并要求他们制作这道菜。厨师查看订单后，告诉助手需要什么。然后，助手首先找到所需的物品（例如，面包、肉饼和洋葱），并将它们放在手边，以便尽快完成厨师的要求。此外，厨师可能还会要求暂时保存菜肴的中间结果（例如，切好的蔬菜），直到厨师再次需要它们。在我们的例子中，厨师是操作执行者，而助手是参数服务器。
- en: This café is full of surprises. As you are speaking out your order (that is,
    invoking Python functions that have TensorFlow operations), you see it getting
    prepared in real time through the tablet on your table (that is, eager execution).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这家咖啡馆充满了惊喜。当你说出你的订单（也就是说，调用包含 TensorFlow 操作的 Python 函数）时，你通过桌上的平板电脑实时看到订单正在被准备（也就是急切执行）。
- en: 'The best thing about this video feed is that, if you see that the chef did
    not put enough cheese, you know exactly why the burger wasn’t as good as expected.
    So, you can either order another one or provide specific feedback. This is a great
    improvement over how TensorFlow 1 did things, where they would take your order
    and you would not see anything until the full burger had been prepared. This process
    is shown in *Figure 2.5*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个视频教程的最棒之处在于，如果你看到厨师没有放足够的奶酪，你就能立刻明白为什么汉堡不如预期的好。所以，你可以选择再点一个或者给出具体的反馈。这比 TensorFlow
    1 的做法要好得多，因为他们会先接受你的订单，然后你在汉堡准备好之前什么也看不见。这个过程在*图 2.5*中展示：
- en: '![](img/B14070_02_05.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_02_05.png)'
- en: 'Figure 2.5: The restaurant analogy illustrated'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：餐厅类比示意图
- en: Let’s now have a look back at how TensorFlow 1 used to work.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下 TensorFlow 1 的工作方式。
- en: 'Flashback: TensorFlow 1'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾：TensorFlow 1
- en: We said numerous times that TensorFlow 2 is very different from TensorFlow 1\.
    But we still don’t know what it used to be like. Therefore, let’s now do a bit
    of time traveling to see how the same sigmoid computation could have been implemented
    in TensorFlow 1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多次提到过，TensorFlow 2 与 TensorFlow 1 的区别非常大。但我们仍然不知道它以前是怎样的。现在，让我们做一场时光旅行，看看同样的
    sigmoid 计算在 TensorFlow 1 中是如何实现的。
- en: '**Warning**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**'
- en: You will not be able to execute the following code in TensorFlow 2.x as it stands.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你无法直接在 TensorFlow 2.x 中执行以下代码。
- en: 'First, we’ll define a `graph` object, which we will populate with operations
    and variables later:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个 `graph` 对象，稍后我们将向其中添加操作和变量：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `graph` object contains the computational graph that connects the various
    inputs and outputs we define in our program to get the final desired output. This
    is the same graph we discussed earlier. Also, we’ll define a `session` object
    that takes the defined graph as the input, which executes the graph. In other
    words, compared to TensorFlow 2, the `graph` object and the `session` object do
    what happens when you invoke them decorated by `tf.function()`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph` 对象包含了计算图，它将我们程序中定义的各种输入和输出连接起来，从而得到最终期望的输出。这就是我们之前讨论的那个图。同时，我们还会定义一个
    `session` 对象，作为输入传递给已定义的图，用以执行这个图。换句话说，相较于 TensorFlow 2，`graph` 对象和 `session`
    对象做的事情就是当你调用它们并用 `tf.function()` 装饰时发生的事情。'
- en: 'Now we’ll define a few tensors, namely `x`, `W`, `b`, and `h`. There are several
    different ways that you can define tensors in TensorFlow 1\. Here, we will look
    at three such different approaches:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义几个张量，即 `x`、`W`、`b` 和 `h`。在 TensorFlow 1 中，你可以用多种不同的方式定义张量。这里，我们将介绍三种不同的方法：
- en: First, `x` is a placeholder. Placeholders, as the name suggests, are not initialized
    with any value. Rather, we will provide the value on the fly at the time of the
    graph execution. If you remember from the TensorFlow 2 sigmoid exercise, we fed
    `x` (which was a NumPy array) directly to the function `layer(x, w, b)`. Unlike
    in TensorFlow 2, you cannot feed NumPy arrays directly to TensorFlow 1 graphs
    or operations.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，`x` 是一个占位符。占位符，顾名思义，未初始化任何值。相反，我们会在图执行时动态地提供其值。如果你记得 TensorFlow 2 中的 sigmoid
    练习，我们直接将 `x`（它是一个 NumPy 数组）传递给函数 `layer(x, w, b)`。与 TensorFlow 2 不同，在 TensorFlow
    1 中，不能直接将 NumPy 数组传递给图或操作。
- en: Next, we have the variables `W` and `b`. Variables are defined similarly to
    TensorFlow 2 with some minor changes in the syntax.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们有变量 `W` 和 `b`。变量的定义方式与 TensorFlow 2 类似，只是语法上有一些小的变化。
- en: Finally, we have `h`, which is an immutable tensor produced by performing some
    operations on `x`, `W`, and `b`. Note that you will not see the value of `h` immediately
    as you needed to manually execute the graph in TensorFlow 1.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们有 `h`，它是一个不可变的张量，由对 `x`、`W` 和 `b` 执行一些操作生成。请注意，你不会立即看到 `h` 的值，因为在 TensorFlow
    1 中，你需要手动执行图才能查看其值。
- en: 'These tensors are defined as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些张量的定义如下：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The lifetime of variables in TensorFlow 1 was managed by the session object,
    meaning that variables lived in memory for as long as the session lived (even
    after losing references to them in the code). However, in TensorFlow 2, variables
    are removed soon after the variables are not referenced in the code, just like
    in Python.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 1 中变量的生命周期由 session 对象管理，这意味着变量在 session 存在期间会一直驻留在内存中（即使代码中不再引用它们）。然而，在
    TensorFlow 2 中，变量会在代码中不再引用后很快被移除，就像在 Python 中一样。
- en: 'Next, we’ll run an initialization operation that initializes the variables
    in the graph, `W` and `b`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将运行一个初始化操作，用于初始化图中的变量 `W` 和 `b`：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we will execute the graph to obtain the final output we need, `h`. This
    is done by running `session.run(...)`, where we provide the value to the placeholder
    as an argument of the `session.run()` command:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we close the session, releasing any resources held by the `session`
    object:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the full code of this TensorFlow 1 example:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, before TensorFlow 2 the user had to:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Define the computational graph using various TensorFlow data structures (for
    example, `tf.placeholder`) and operations (for example, `tf.matmul()`)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute the required part of the graph using `session.run()` to fetch the results
    by feeding the correct data into the session
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In conclusion, TensorFlow 1.x had several limitations:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Coding with TensorFlow 1 did not provide the same intuitive “Pythonic” feeling
    as you needed to define the computational graph first and then invoke the execution
    of it. This is known as declarative programming.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The design in TensorFlow 1 made it very hard to break the code down into manageable
    functions as the user needed to define the graph fully, before doing any computations.
    This resulted in very large functions or pieces of code containing very large
    computational graphs.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It was very difficult to do real-time debugging of the code as TensorFlow had
    its own runtime that used `session.run()`.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But also, it was not without some advantages, such as the efficiency brought
    about by declaring the full computational graph upfront. Knowing all the computations
    in advance meant TensorFlow 1 could perform all sorts of optimizations (for example,
    graph pruning) to run the graph efficiently.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this part of the chapter, we discussed our first example in TensorFlow2 and
    the architecture of TensorFlow. Finally, we compared and contrasted TensorFlow
    1 and 2\. Next, we will discuss the various building blocks of TensorFlow 2.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Inputs, variables, outputs, and operations
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we are returning from our journey into TensorFlow 1 and stepping back to
    TensorFlow 2\. Let’s proceed to the most common elements that comprise a TensorFlow
    2 program. If you read any of the millions of TensorFlow clients available on
    the internet, the TensorFlow-related code all falls into one of these buckets:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs**: Data used to train and test our algorithms'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variables**: Mutable tensors, mostly defining the parameters of our algorithms'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outputs**: Immutable tensors storing both terminal and intermediate outputs'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations**: Various transformations for inputs to produce the desired outputs'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our earlier sigmoid example, we can find instances of all these categories.
    We list the respective TensorFlow elements and the notation used in the sigmoid
    example in *Table 2.1*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '| **TensorFlow element** | **Value from example client** |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| Inputs | `x` |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| Variables | `W` and `b` |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| Outputs | `h` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| Operations | `tf.matmul(...)`, `tf.nn.sigmoid(...)` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: 'Table 2.1: The different types of TensorFlow primitives we have encountered
    so far'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections explain each of these TensorFlow elements listed in
    the table in more detail.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Defining inputs in TensorFlow
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three different ways you can feed data to a TensorFlow program:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Feeding data as NumPy arrays
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feeding data as TensorFlow tensors
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `tf.data` API to create an input pipeline
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will discuss a few different ways you can feed data to TensorFlow operations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Feeding data as NumPy arrays
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the simplest way to feed data into a TensorFlow program. Here, you pass
    a NumPy array as an input to the TensorFlow operation and the result is executed
    immediately. This is exactly what we did in the sigmoid example. If you look at
    `x`, it is a NumPy array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Feeding data as tensors
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second method is like the first one, but the type of data is different.
    Here, we are defining `x` as a TensorFlow tensor.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this in action, let’s modify our sigmoid example. Remember that we defined
    `x` as:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Instead, let’s define this as a tensor that contains specific values:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Also, the full code would become as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let’s now discuss how we can define data pipelines in TensorFlow.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Building a data pipeline using the tf.data API
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`tf.data` provides you with a convenient way to build data pipelines in TensorFlow.
    Input pipelines are designed for more heavy-duty programs that need to process
    a lot of data. For example, if you have a small dataset (for example, the MNIST
    dataset) that fits into the memory, input pipelines would be excessive. However,
    when working with complex data or problems, where you might need to work with
    large datasets that do not fit in memory, augment the data (for example, for adjusting
    image contrast/brightness), numerically transform it (for example, standardize),
    and so on. The `tf.data` API provides convenient functions that can be used to
    easily load and transform your data. Furthermore, it streamlines your data ingestion
    code with the model training.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the `tf.data` API offers various options to enhance the performance
    of your data pipeline, such as multi-processing and pre-fetching data. Pre-fetching
    refers to bringing data into the memory before it’s required and keeping it ready.
    We will discuss these methods in more detail as they are used in the upcoming
    chapters.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating an input pipeline, we intend to perform the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Source the data from a data source (for example, an in-memory NumPy array, CSV
    file on disk, or individual files such as images).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply various transformations to the data (for example, cropping/resizing image
    data).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate the resulting dataset element/batch-wise. Batching is required as deep
    learning models are trained on randomly sampled batches of data. As the datasets
    these models are trained on are large, they typically do not fit in memory.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s write an input pipeline using TensorFlow’s `tf.data` API. In this example,
    we have three text files (`iris.data.1`, `iris.data.2`, and `iris.data.3`) in
    CSV format, each file having 50 lines and each line having 4 floating-point numbers
    (in other words, various lengths associated with a flower) and a string label
    separated by commas (an example line would be `5.6,2.9,3.6,1.3,Iris-versicolor`).
    We will now use the `tf.data` API to read data from these files. We also know
    that some of this data is corrupted (as with any real-life machine learning project).
    In our case, some data points have negative lengths. So, let’s first write a pipeline
    to go through the data row by row and print the corrupted outputs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: For more information, refer to the official TensorFlow page on importing data
    at [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import a few important libraries as before:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we will define a list containing the filenames:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we will use one of the dataset readers provided in TensorFlow. The dataset
    reader takes in a list of filenames and another list that specifies the data types
    of each column in the dataset. As we saw previously, we have four floating numbers
    and one string:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we will organize our data into inputs and labels as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We are using lambda functions to separate out `x1,x2,x3,x4` into one dataset
    and `y` to another dataset, along with the `dataset.map()` function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Lambda functions are a special type of function that allow you to define some
    computations succinctly. With lambda functions, you don’t need to name your function,
    which can be quite handy if you are using a certain function only once in your
    code. The format of the lambda function looks like:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '`lambda <arguments>: <result returned after the computation>`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you need to write a function that adds two numbers, simply
    write:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '`lambda x, y: x+y`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Here, `tf.stack()` stacks individual tensors (here, the individual feature)
    to a single tensor. When using the `map` function, you first need to visualize
    what needs to be done to a single item in the dataset (a single item in our case
    is a single row from the dataset), and write the transformation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'The map function is very simple but powerful. All it does is transform a set
    of given inputs into a new set of values. For example, if you have a list, `xx`,
    that contains a list of numbers and want to convert them to power 2 element-wise,
    you can write something like `xx_pow = map(lambda x: x**2, xx)`. And this can
    be very easily parallelized as there’s no dependency between items.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can iterate through this dataset, examining individual data points,
    as you would iterate through a normal Python list. Here, we are printing out all
    the corrupted items:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Since you don’t want those corrupted inputs in your dataset, you can use the
    `dataset.filter()` function to filter out those corrupted entries as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here we are checking whether the minimum element in `x` is greater than zero;
    if not, those elements will be filtered out of the dataset.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful function is `dataset.batch()`. When training deep neural networks,
    we often traverse the dataset in batches, not individual items. `dataset.batch()`
    provides a convenient way to do that:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, if you print the shape of a single element in your dataset, you should
    get the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now that we have examined the three different methods you can use to define
    inputs in TensorFlow, let’s see how we can define variables in TensorFlow.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Defining variables in TensorFlow
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variables play an important role in TensorFlow. A variable is essentially a
    tensor with a specific shape defining how many dimensions the variable will have
    and the size of each dimension. However, unlike a regular TensorFlow tensor, variables
    are *mutable*; meaning that the value of the variables can change after they are
    defined. This is an ideal property to have to implement the parameters of a learning
    model (for example, neural network weights), where the weights change slightly
    after each step of learning. For example, if you define a variable with `x = tf.Variable(0,dtype=tf.int32)`,
    you can change the value of that variable using a TensorFlow operation such as
    `tf.assign(x,x+1)`. However, if you define a tensor such as `x = tf.constant(0,dtype=tf.int32)`,
    you cannot change the value of the tensor, as you could for a variable. It should
    stay `0` until the end of the program execution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Variable creation is quite simple. In our sigmoid example, we already created
    two variables, `W` and `b`. When creating a variable, a few things are extremely
    important. We will list them here and discuss each in detail in the following
    paragraphs:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Variable shape
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial values
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data type
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name (optional)
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variable shape is a list of the `[x,y,z,...]` format. Each value in the
    list indicates how large the corresponding dimension or axis is. For instance,
    if you require a 2D tensor with 50 rows and 10 columns as the variable, the shape
    would be equal to `[50,10]`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The dimensionality of the variable (that is, the length of the `shape` vector)
    is recognized as the rank of the tensor in TensorFlow. Do not confuse this with
    the rank of a matrix.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Tensor rank in TensorFlow indicates the dimensionality of the tensor; for a
    two-dimensional matrix, *rank* = 2.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, a variable requires an *initial* value to be initialized with. TensorFlow
    provides several different initializers for our convenience, including constant
    initializers and normal distribution initializers. Here are a few popular TensorFlow
    initializers you can use to initialize variables:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.initializers.Zeros`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.initializers.Constant`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.initializers.RandomNormal`'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.initializers.GlorotUniform`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The shape of the variable can be provided as a part of the initializer as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The data type plays an important role in determining the size of a variable.
    There are many different data types, including the commonly used `tf.bool`, `tf.uint8`,
    `tf.float32`, and `tf.int32`. Each data type has a number of bits required to
    represent a single value with that type. For example, `tf.uint8` requires 8 bits,
    whereas `tf.float32` requires 32 bits. It is common practice to use the same data
    types for computations, as doing otherwise can lead to data type mismatches. So,
    if you have two different data types for two tensors that you need to transform,
    you have to explicitly convert one tensor to the other tensor’s type using the
    `tf.cast(...)` operation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.cast(...)` operation is designed to cope with such situations. For example,
    if you have an `x` variable with the `tf.int32` type, which needs to be converted
    to `tf.float32`, employ `tf.cast(x,dtype=tf.float32)` to convert `x` to `tf.float32`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the *name* of the variable will be used as an ID to identify that variable
    in the graph. If you ever visualize the computational graph, the variable will
    appear by the argument passed to the `name` keyword. If you do not specify a name,
    TensorFlow will use the default naming scheme.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the Python variable `tf.Variable` is assigned to is not known by
    the computational graph, and is not a part of TensorFlow variable naming. Consider
    this example where you specify a TensorFlow variable as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, the TensorFlow graph will know this variable by the name `b` and not `a`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, let’s talk about how to define TensorFlow outputs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Defining outputs in TensorFlow
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TensorFlow outputs are usually tensors, and the result of a transformation
    to either an input, or a variable, or both. In our example, `h` is an output,
    where `h = tf.nn.sigmoid(tf.matmul(x,W) + b)`. It is also possible to give such
    outputs to other operations, forming a chained set of operations. Furthermore,
    they do not necessarily have to be TensorFlow operations. You also can use standard
    Python arithmetic with TensorFlow. Here is an example:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Below, we explain various operations available in TensorFlow and how to use
    them.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Defining operations in TensorFlow
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An operation in TensorFlow takes one or more inputs and produces one or more
    outputs. If you take a look at the TensorFlow API at [https://www.tensorflow.org/api_docs/python/tf](https://www.tensorflow.org/api_docs/python/tf),
    you will see that TensorFlow has a massive collection of operations available.
    Here, we will take a look at a selected few of the myriad TensorFlow operations.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Comparison operations
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Comparison operations are useful for comparing two tensors. The following code
    example includes a few useful comparison operations.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the working of these operations, let’s consider two example tensors,
    `x` and `y`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, let’s look at some mathematical operations.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical operations
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow allows you to perform math operations on tensors that range from
    the simple to the complex. We will discuss a few of the mathematical operations
    made available in TensorFlow. The complete set of operations is available at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, we will look at the scatter operation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Updating (scattering) values in tensors
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A scatter operation, which refers to changing the values at certain indices
    of a tensor, is very common in scientific computing problems. This functionality
    was originally provided through an intimidating `tf.scatter_nd()` function, which
    can be difficult to understand.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in recent TensorFlow versions, you can perform scatter operations
    via array indexing and slicing using NumPy-like syntax. Let’s see a few examples.
    Say you have the TensorFlow variable v, which is a [3,2] matrix:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can change the 0^(th) row of this tensor with:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'which results in:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can change the value at index [1,1] with:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'which results in:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can perform row slicing with:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'which results in:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: It is important to remember that the scatter operation (performed via the `assign()`
    operation) can only be performed on `tf.Variables`, which are mutable structures.
    Remember that `tf.Tensor`/`tf.EagerTensor` are immutable objects.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Collecting (gathering) values from a tensor
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A gather operation is very similar to a scatter operation. Remember that scattering
    is about assigning values to tensors, whereas gathering retrieves the values of
    a tensor. Let’s understand this through an example. Say you have a TensorFlow
    tensor, `t`:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can obtain the 0^(th) row of `t` with:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'which will return:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can also perform row-slicing with:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'which will return:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Unlike the scatter operation, the gather operation works both on `tf.Variable`
    and `tf.Tensor` structures.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Neural network-related operations
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s look at several useful neural network-related operations that we
    will use heavily in the following chapters. The operations we will discuss here
    range from simple element-wise transformations (that is, activations) to computing
    partial derivatives of a set of parameters with respect to another value. We will
    also implement a simple neural network as an exercise.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear activations used by neural networks
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nonlinear activations enable neural networks to perform well at numerous tasks.
    Typically, there is a nonlinear activation transformation (that is, activation
    layer) after each layer output in a neural network (except for the last layer).
    A nonlinear transformation helps a neural network to learn various nonlinear patterns
    that are present in data. This is very useful for complex real-world problems,
    where data often has more complex nonlinear patterns, in contrast to linear patterns.
    If not for the nonlinear activations between layers, a deep neural network would
    be a bunch of linear layers stacked on top of each other. Also, a set of linear
    layers can essentially be compressed to a single bigger linear layer.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, if not for the nonlinear activations, we cannot create a neural
    network with more than one layer.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s observe the importance of nonlinear activation through an example. First,
    recall the computation for the neural networks we saw in the sigmoid example.
    If we disregard b, it will be this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '`h = sigmoid(W*x)`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume a three-layer neural network (having `W1`, `W2`, and `W3` as layer weights)
    where each layer does the preceding computation; we can summarize the full computation
    as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '`h = sigmoid(W3*sigmoid(W2*sigmoid(W1*x)))`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we remove the nonlinear activation (that is, sigmoid), we get this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '`h = (W3 * (W2 * (W1 *x))) = (W3*W2*W1)*x`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: So, without the nonlinear activations, the three layers can be brought down
    to a single linear layer.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll list two commonly used nonlinear activations in neural networks (in
    other words, sigmoid and ReLU) and how they can be implemented in TensorFlow:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The functional form of these computations is visualized in *Figure 2.6*:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_02_06.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: The functional forms of sigmoid (left) and ReLU (right) activations'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the convolution operation.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A convolution operation is a widely used signal-processing technique. For images,
    convolution is used to produce different effects (such as blurring), or extract
    features (such as edges) from an image. An example of edge detection using convolution
    is shown in *Figure 2.7*. This is achieved by shifting a convolution filter on
    top of an image to produce a different output at each location (see *Figure* *2.8*
    later in this section). Specifically, at each location, we do element-wise multiplication
    of the elements in the convolution filter with the image patch (the same size
    as the convolution filter) that overlaps with the convolution filter and takes
    the sum of the multiplication:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_06.png](img/B14070_02_07.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Using the convolution operation for edge detection in an image
    (Source: [https://en.wikipedia.org/wiki/Kernel_(image_processing)](https://en.wikipedia.org/wiki/Kernel_(image_processing)))'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the implementation of the convolution operation:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here, the apparently excessive number of square brackets used might make you
    think that the example can be made easy to follow by getting rid of these redundant
    brackets. Unfortunately, that is not the case. For the `tf.nn.conv2d(...)` operation,
    TensorFlow requires `input`, `filters`, and `strides` to be of an exact format.
    We will now go through each argument in `tf.conv2d(input, filters, strides, padding)`
    in more detail:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '**input**: This is typically a 4D tensor where the dimensions should be ordered
    as `[batch_size, height, width, channels]`:'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**batch_size**: This is the amount of data (for example, inputs such as images,
    and words) in a single batch of data. We normally process data in batches as large
    datasets are used for learning. At a given training step, we randomly sample a
    small batch of data that approximately represents the full dataset. And doing
    this for many steps allows us to approximate the full dataset quite well. This
    `batch_size` parameter is the same as the one we discussed in the TensorFlow input
    pipeline example.'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height and width**: This is the height and the width of the input.'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**channels**: This is the depth of an input (for example, for an RGB image,
    the number of channels will be 3—a channel for each color).'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**filters**: This is a 4D tensor that represents the convolution window of
    the convolution operation. The filter dimensions should be `[height, width, in_channels,
    out_channels]`:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**height and width**: This is the height and the width of the filter (often
    smaller than that of the input)'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**in_channels**: This is the number of channels of the input to the layer'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**out_channels**: This is the number of channels to be produced in the output
    of the layer'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**strides**: This is a list with four elements, where the elements are `[batch_stride,
    height_stride, width_stride, channels_stride]`. The `strides` argument denotes
    how many elements to skip during a single shift of the convolution window on the
    input. Usually, you don’t have to worry about `batch_stride` and `channels_stride`.
    If you do not completely understand what `strides` is, you can use the default
    value of `1`.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding**: This can be one of `[''SAME'', ''VALID'']`. It decides how to
    handle the convolution operation near the boundaries of the input. The `VALID`
    operation performs the convolution without padding. If we were to convolve an
    input of *n* length with a convolution window of size *h*, this will result in
    an output of size (*n-h+1 < n*). The diminishing of the output size can severely
    limit the depth of neural networks. `SAME` pads zeros to the boundary such that
    the output will have the same height and width as the input.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To gain a better understanding of what filter size, stride, and padding are,
    refer to *Figure 2.8*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_07.png](img/B14070_02_08.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: The convolution operation. Note how the kernel is moved over the
    input to compute values at each position'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the pooling operation.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: The pooling operation
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A pooling operation behaves similarly to the convolution operation, but the
    final output is different. Instead of outputting the sum of the element-wise multiplication
    of the filter and the image patch, we now take the maximum element of the image
    patch for that location (see *Figure 2.9*):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_08.png](img/B14070_02_09.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: The max-pooling operation'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Defining loss
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that, for a neural network to learn something useful, a loss needs
    to be defined. The loss represents how close or far away the predictions are from
    actual targets. There are several functions for automatically calculating the
    loss in TensorFlow, two of which are shown in the following code. The `tf.nn.l2_loss`
    function is the mean squared error loss, and `tf.nn.softmax_cross_entropy_with_logits`
    is another type of loss that actually gives better performance in classification
    tasks. And by logits here, we mean the unnormalized output of the neural network
    (that is, the linear output of the last layer of the neural network):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, we discussed several important operations intertwined with neural networks,
    such as the convolution operation and the pooling operation. We will now discuss
    how a sub-library in TensorFlow known as Keras can be used to build models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras: The model building API of TensorFlow'
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras was developed as a separate library that provides high-level building
    blocks to build models conveniently. It was initially platform-agnostic and supported
    many softwares (for example, TensorFlow and Theano).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: However, TensorFlow acquired Keras and now is an integral part of TensorFlow
    for building models effortlessly.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras’s primary focus is model building. For that, Keras provides several different
    APIs with varying degrees of flexibility and complexity. Choosing the right API
    for the job will require sound knowledge of the limitations of each API as well
    as experience. The APIs provided by Keras are:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Sequential API – The most easy-to-use API. In this API, you simply stack layers
    on top of each other to create a model.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional API – The functional API provides more flexibility by allowing you
    to define custom models that can have multiple input layers/multiple output layers.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-classing API – The sub-classing API enables you to define custom reusable
    layers/models as Python classes. This is the most flexible API, but it requires
    strong familiarity with the API and raw TensorFlow operations to use it correctly.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not confuse the Keras TensorFlow sub-module ([https://www.tensorflow.org/api_docs/python/tf/keras](https://www.tensorflow.org/api_docs/python/tf/keras))
    with the external Keras library ([https://keras.io/](https://keras.io/)). They
    share roots in terms of where they’ve come from, but they are not the same. You
    will run into strange issues if you treat them as the same during your development.
    In this book, we exclusively use `tf.keras`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: One of the most innate concepts in Keras is that a model is composed of one
    or more layers connected in a specific way. Here, we will briefly go through what
    the code looks like, using different APIs to develop models. You are not expected
    to fully understand the code below. Rather, focus on the code style to spot any
    differences between the three methods.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Sequential API
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using the Sequential API, you simply define your model as a list of layers.
    Here, the first element in the list is the closest to the input, where the last
    is the output layer:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding code, we have three layers. The first layer has 500 output
    nodes and takes in a vector of 784 elements as the input. The second layer is
    automatically connected to the first one, whereas the last layer is connected
    to the second layer. All of these layers are fully-connected layers, where all
    input nodes are connected to all output nodes.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Functional API
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the Functional API, we do things differently. We first define one or more
    input layers, and other layers that carry computations. Then we connect the inputs
    to outputs ourselves, as shown in the following code:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In the code, we start with an input layer that accepts a 784 element-long vector.
    The input is passed to a Dense layer that has 500 nodes. The output of that layer
    is assigned to `out_1`. Then `out_1` is passed to another Dense layer, which outputs
    `out_2`. Next, a Dense layer with 10 nodes outputs the final output. Finally,
    the model is defined as a `tf.keras.models.Model` object that takes two arguments:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: inputs – One or more input layers
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: outputs – One or more outputs produced by any `tf.keras.layers` type object
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is identical to what was defined in the previous section. One of the
    benefits of the Functional API is that you can create far more complex models
    as you’re not bounded to have layers as a list. Because of this freedom, you can
    have multiple inputs connecting to many layers in many different ways and potentially
    produce many outputs as well.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Sub-classing API
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we will use the sub-classing API to define a model. With sub-classing,
    you define your model as a Python object that inherits from the base object, `tf.keras.Model`.
    When using sub-classing, you need to define two important functions: `__init__()`,
    which will specify any special parameters, layers, and so on required to successfully
    perform the computations, and `call()`, which defines the computations that need
    to happen in the model:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Here, you can see that our model has three layers, just like all the previous
    models we defined. Next, the call function defines how these layers connect to
    produce the final output. The sub-classing API is considered the most difficult
    to master, mainly due to the freedom allowed by the method. However, the rewards
    are immense once you learn the API as it enables you to define very complex models/layers
    as unit computations that can be reused later. Now that you understand how each
    API works, let’s implement a neural network using Keras and train it on a dataset.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our first neural network
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Great! Now that you’ve learned the architecture and foundations of TensorFlow,
    it’s high time that we move on and implement something slightly more complex.
    Let’s implement a neural network. Specifically, we will implement a fully connected
    neural network model (FCNN), which we discussed in *Chapter 1*, *Introduction
    to Natural Language Processing*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: One of the stepping stones to the introduction of neural networks is to implement
    a neural network that is able to classify digits. For this task, we will be using
    the famous MNIST dataset made available at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: You might feel a bit skeptical regarding our using a computer vision task rather
    than an NLP task. However, vision tasks can be implemented with less preprocessing
    and are easy to understand.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: As this is our first encounter with neural networks, we will see how to implement
    this model using Keras. Keras is the high-level submodule that provides a layer
    of abstraction over TensorFlow. Therefore, you can implement neural networks with
    much less effort with Keras than using TensorFlow’s raw operations. To run the
    examples end to end, you can find the full exercise in the `tensorflow_introduction.ipynb`
    file in the `Ch02-Understanding-TensorFlow` folder. The next step is to prepare
    the data.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to download the dataset. TensorFlow out of the box provides
    convenient functions to download data and MNIST is one of those supported datasets.
    We will be performing four important steps during the data preparation:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data and storing it as `numpy.ndarray` objects. We will create
    a folder named data within our `ch2` directory and store the data there.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping the images so that 2D grayscale images in the dataset will be converted
    to 1D vectors.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing the images to have a zero-mean and unit-variance (also known as
    **whitening**).
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding the integer class labels. One-hot encoding refers to the process
    of representing integer class labels as a vector. For example, if you have 10
    classes and a class label of 3 (where labels range from 0-9), your one-hot encoded
    vector will be `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code performs these functions for us:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You can see that we are using the `tf.keras.datasets.mnist.load_data()` function
    provided by TensorFlow to download the training and testing data. It will be downloaded
    to a folder named `data` within the `Ch02-Understanding-TensorFlow` folder. This
    will provide four output tensors:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '`x_train` – A 60000 x 28 x 28 sized tensor where each image is 28 x 28'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_train` – A 60000 sized vector, where each element is a class label between
    0-9'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x_test` – A 10000 x 28 x 28 sized tensor'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_test` – A 10000 sized vector'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the data is downloaded, we reshape the 28 x 28 sized images into a 1D vector.
    This is because we will be implementing a fully connected neural network. Fully
    connected neural networks take a 1D vector as the input. Therefore, all the pixels
    in the image will be arranged as a sequence of pixels in order to feed into the
    model. Finally, if you look at the range of values present in the `x_train` and
    `x_test` tensors, they will be in the range of 0-255 (typical grayscale range).
    We would bring these values to a zero mean unit-variance range by subtracting
    the mean of each image and dividing by the standard deviation.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the neural network with Keras
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now examine how to implement the type of neural network we discussed in
    *Chapter 1, Introduction to Natural Language Processing*, with Keras. The network
    is a fully connected neural network with 3 layers having 500, 250, and 10 nodes,
    respectively. The first two layers will use ReLU activation, whereas the last
    layer uses softmax. To implement this, we are going to use the simplest of the
    Keras APIs available to us – the Sequential API.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the full exercise in the `tensorflow_introduction.ipynb` file
    in the `Ch02-Understanding-TensorFlow` folder:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You can see that all it takes is a single line in the Keras Sequential API
    to define the model we just defined. Keras provides various types of layers. You
    can see the full list of layers available to you at [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers).
    For a fully connected network, we only need Dense layers that mimic the computations
    of a hidden layer in a fully connected network. With the model defined, you need
    to compile this model with an appropriate loss function, an optimizer, and, optionally,
    performance metrics:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: With the model defined and compiled, we can now train our model on the prepared
    data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training a model could not be easier in Keras. Once the data is prepared, all
    you need to do is call the `model.fit()` function with the required arguments:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`model.fit()` accepts several important arguments. We will go through them
    in more detail here:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '`x` – An input tensor. In our case, this is a 60000 x 784 sized tensor.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y` – The one-hot encoded label tensor. In our case, this is a 60000 x 10 sized
    tensor.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` – Deep learning models are trained with batches of data (in other
    words, stochastically) as opposed to feeding the full dataset at once. The batch
    size defines how many examples are included in a single batch. The larger the
    batch size, the better the accuracy of your model would be generally.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs` – Deep learning models iterate through the dataset in batches several
    times. The number of times iterated through the dataset is known as the number
    of epochs. In our example, this is set to 10.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_split` – When training deep learning models, a validation set is
    used to monitor performance, where the validation set acts as a proxy for real-world
    performance. `validation_split` defines how much of the full dataset is to be
    used as the validation subset. In our example, this is set to 20% of the total
    dataset size.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s what the training loss and validation accuracy look like over the number
    of epochs we trained the model (*Figure 2.10*):'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_02_10.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Training loss and validation accuracy over 10 epochs as the model
    is trained'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Next up is testing our model on some unseen data.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Testing the model
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Testing the model is also straightforward. During testing, we measure the loss
    and the accuracy of the model on the test dataset. In order to evaluate the model
    on a dataset, Keras models provide a convenient function called `evaluate()`:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The arguments expected by the `model.evaluate()` function are already covered
    during our discussion of `model.fit()`:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '`x` – An input tensor. In our case, this is a 10000 x 784 sized tensor.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y` – The one-hot encoded label tensor. In our case, this is a 10000 x 10 sized
    tensor.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` – Batch size defines how many examples are included in a single
    batch. The larger the batch size, the better the accuracy of your model would
    be generally.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will get a loss of 0.138 and an accuracy of 98%. You will not get the exact
    same values due to various randomness present in the model, as well as during
    training.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we went through an end-to-end example of training a neural
    network. We prepared the data, trained the model on that data, and finally tested
    it on some unseen data.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you took your first steps to solving NLP tasks by understanding
    the primary underlying platform (TensorFlow) on which we will be implementing
    our algorithms. First, we discussed the underlying details of TensorFlow architecture.
    Next, we discussed the essential ingredients of a meaningful TensorFlow program.
    We got to know some new features in TensorFlow 2, such as the AutoGraph feature,
    in depth. We then discussed more exciting elements in TensorFlow such as data
    pipelines and various TensorFlow operations.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we discussed the TensorFlow architecture by lining up the explanation
    with an example TensorFlow program; the sigmoid example. In this TensorFlow program,
    we used the AutoGraph feature to generate a TensorFlow graph; that is, using the
    `tf.function()` decorator over the function that performs the TensorFlow operations.
    Then, a `GraphDef` object was created representing the graph and sent to the distributed
    master. The distributed master looked at the graph, decided which components to
    use for the relevant computation, and divided it into several subgraphs to make
    the computations faster. Finally, workers executed subgraphs and returned the
    result immediately.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we discussed the various elements that comprise a typical TensorFlow
    client: inputs, variables, outputs, and operations. Inputs are the data we feed
    to the algorithm for training and testing purposes. We discussed three different
    ways of feeding inputs: using NumPy arrays, preloading data as TensorFlow tensors,
    and using `tf.data` to define an input pipeline. Then we discussed TensorFlow
    variables, how they differ from other tensors, and how to create and initialize
    them. Following this, we discussed how variables can be used to create intermediate
    and terminal outputs.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed several available TensorFlow operations, including mathematical
    operations, matrix operations, and neural network-related operations that will
    be used later in the book.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, we discussed Keras, a sub-module in TensorFlow that supports building
    models. We learned that there are three different APIs for building models: the
    Sequential API, the Functional API, and the Sub-classing API. We learned that
    the Sequential API is the easiest to use, whereas the Sub-classing API takes much
    more effort. However, the Sequential API is very restrictive in terms of the type
    of models that can be implemented with it.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we implemented a neural network using all the concepts learned previously.
    We used a three-layer neural network to classify a MNIST digit dataset, and we
    used Keras (a high-level sub-module in TensorFlow) to implement this model.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to use the fully connected neural network
    we implemented in this chapter for learning the semantic, numerical word representation
    of words.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
