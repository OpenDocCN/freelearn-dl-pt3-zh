["```\nimport tensorflow as tf\ncluster = tf.train.ClusterSpec({\n   \"worker\":[\"worker0.example.com:2222\",\n           \"worker1.example.com:2222\"]\n})\n```", "```\nserver = tf.train.Server(cluster, job_name = \"worker\", task_index = 1)\n```", "```\nimport tensorflow as tf\n\n# define Clusters with two workers\ncluster = tf.train.ClusterSpec({\n    \"worker\": [\n        \"localhost:2222\",\n        \"localhost:2223\"\n         ]})\n\n# define Servers\nworker0 = tf.train.Server(cluster, job_name=\"worker\", task_index=0)\nworker1 = tf.train.Server(cluster, job_name=\"worker\", task_index=1)\n\nwith tf.device(\"/job:worker/task:1\"):\n    a = tf.constant(3.0, dtype=tf.float32)\n    b = tf.constant(4.0) \n    add_node = tf.add(a,b)\n\nwith tf.device(\"/job:worker/task:0\"):\n    mul_node = a * b\n\nwith tf.Session(\"grpc://localhost:2222\") as sess:\n    result = sess.run([add_node, mul_node])\n    print(result)\n```", "```\nimport sys\nimport tensorflow as tf\n# Add other module libraries you may need\n```", "```\ncluster = tf.train.ClusterSpec(\n          {'ps':['192.168.1.3:2222'],\n           'worker': ['192.168.1.4:2222',\n                      '192.168.1.5:2222',\n                      '192.168.1.6:2222',\n                      '192.168.1.7:2222']\n })\n```", "```\njob = sys.argv[1]\ntask_idx = sys.argv[2]\n```", "```\nserver = tf.train.Server(cluster, job_name=job, task_index= int(task_idx))\n```", "```\nif job == 'ps':  \n    # Makes the parameter server wait \n    # until the Server shuts down\n    server.join()\nelse:\n    # Executes only on worker machines    \n    with tf.device(tf.train.replica_device_setter(cluster=cluster, worker_device='/job:worker/task:'+task_idx)):\n        #build your model here like you are working on a single machine\n\n    with tf.Session(server.target):\n        # Train the model \n```", "```\npython tensorflow_distributed_dl.py ps 0\n```", "```\npython tensorflow_distributed_dl.py worker 0\n```", "```\npython tensorflow_distributed_dl.py worker 1\n```", "```\npython tensorflow_distributed_dl.py worker 2\n```", "```\npython tensorflow_distributed_dl.py worker 3\n```", "```\ncluster = TFCluster.run(sc, map_fn, args, num_executors, num_ps, tensorboard, input_mode)\n```", "```\ncluster.train(dataRDD, num_epochs)\n```", "```\ndef main(args, ctx):\n    # Load training and eval data\n    mnist = tf.contrib.learn.datasets.mnist.read_data_sets(args.data_dir)\n    train_data = mnist.train.images # Returns np.array\n    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n    eval_data = mnist.test.images # Returns np.array\n    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n\n    # Create the Estimator\n    mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=args.model)\n\n    # Set up logging for predictions\n    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n\n    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n    logging_hook = tf.train.LoggingTensorHook( tensors=tensors_to_log, every_n_iter=50)\n\n    # Train the model\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n         x={\"x\": train_data}, y=train_labels, \n         batch_size=args.batch_size, num_epochs=None, \n         shuffle=True)\n\n      eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n         x={\"x\": eval_data},\n         y=eval_labels,\n         num_epochs=1,\n         shuffle=False)\n\n    #Using tf.estimator.train_and_evaluate\n    train_spec = tf.estimator.TrainSpec(\n        input_fn=train_input_fn, \n        max_steps=args.steps, \n        hooks=[logging_hook])\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=eval_input_fn)\n    tf.estimator.train_and_evaluate(\n        mnist_classifier, train_spec, eval_spec)\n\n```", "```\nfrom pyspark.context import SparkContext\nfrom pyspark.conf import SparkConf\nfrom tensorflowonspark import TFCluster\nimport argparse\n```", "```\nsc = SparkContext(conf=SparkConf()\n        .setAppName(\"mnist_spark\"))\nexecutors = sc._conf.get(\"spark.executor.instances\")\nnum_executors = int(executors) if executors is not None else 1\n```", "```\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--batch_size\", \n            help=\"number of records per batch\", \n            type=int, default=100)\nparser.add_argument(\"--cluster_size\", \n            help=\"number of nodes in the cluster\", \n            type=int, default=num_executors)\nparser.add_argument(\"--data_dir\", \n            help=\"path to MNIST data\", \n            default=\"MNIST-data\")\nparser.add_argument(\"--model\", \n            help=\"path to save model/checkpoint\", \n            default=\"mnist_model\")\nparser.add_argument(\"--num_ps\", \n            help=\"number of PS nodes in cluster\", \n            type=int, default=1)\nparser.add_argument(\"--steps\", \n            help=\"maximum number of steps\", \n            type=int, default=1000)\nparser.add_argument(\"--tensorboard\", \n            help=\"launch tensorboard process\", \n            action=\"store_true\")\n\nargs = parser.parse_args()\n```", "```\ncluster = TFCluster.run(sc, main, args, \n        args.cluster_size, args.num_ps, \n        tensorboard=args.tensorboard, \n        input_mode=TFCluster.InputMode.TENSORFLOW, \n        log_dir=args.model, master_node='master')\n```", "```\ncluster.shutdown()\n```", "```\n${SPARK_HOME}/bin/spark-submit \\\n--master ${MASTER} \\\n--conf spark.cores.max=${TOTAL_CORES} \\\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\n--conf spark.task.maxFailures=1 \\\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\n${TFoS_HOME}/examples/mnist/estimator/mnist_TFoS.py \\\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\n--model ${TFoS_HOME}/mnist_model\n```", "```\nSparkSession.builder().getOrCreate()\n```", "```\nimport findspark\nfindspark.init('/home/ubuntu/spark-2.4.0-bin-hadoop2.7')\n\nimport os\nSUBMIT_ARGS = \"--packages databricks:spark-deep-learning:1.3.0-spark2.4-s_2.11 pyspark-shell\"\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName(\"ImageClassification\") \\\n    .config(\"spark.executor.memory\", \"70g\") \\\n    .config(\"spark.driver.memory\", \"50g\") \\\n    .config(\"spark.memory.offHeap.enabled\",True) \\\n    .config(\"spark.memory.offHeap.size\",\"16g\") \\\n    .getOrCreate()\n```", "```\nimport pyspark.sql.functions as f\nimport sparkdl as dl\nfrom pyspark.ml.image import ImageSchema\n\ndfbuses = ImageSchema.readImages('buses/').withColumn('label', f.lit(0))\ndfcars = ImageSchema.readImages('cars/').withColumn('label', f.lit(1))\n```", "```\ndfbuses.show(5)\ndfcars.show(5)\n```", "```\ntrainDFbuses, testDFbuses = dfbuses.randomSplit([0.60,0.40], seed = 123)\ntrainDFcars, testDFcars = dfcars.randomSplit([0.60,0.40], seed = 122)\n```", "```\ntrainDF = trainDFbuses.unionAll(trainDFcars)\ntestDF = testDFbuses.unionAll(testDFcars)\n```", "```\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nvectorizer = dl.DeepImageFeaturizer(inputCol=\"image\",\n         outputCol=\"features\", \n        modelName=\"InceptionV3\")\n\nlogreg = LogisticRegression(maxIter=30, labelCol=\"label\")\npipeline = Pipeline(stages=[vectorizer, logreg])\npipeline_model = pipeline.fit(trainDF)\n```", "```\npredictDF = pipeline_model.transform(testDF)\npredictDF.select('prediction', 'label').show(n = testDF.toPandas().shape[0], truncate=False)\npredictDF.crosstab('prediction', 'label').show()\n```", "```\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nscoring = predictDF.select(\"prediction\", \"label\")\naccuracy_score = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nrate = accuracy_score.evaluate(scoring)*100\nprint(\"accuracy: {}%\" .format(round(rate,2)))\n```"]