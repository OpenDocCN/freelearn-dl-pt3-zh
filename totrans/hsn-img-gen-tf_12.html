<html><head></head><body>
		<div id="_idContainer192">
			<h1 id="_idParaDest-166"><em class="italic"><a id="_idTextAnchor175"/>Chapter 9</em>: Video Synthesis</h1>
			<p>We have learned about and built many models for image generation, including state-of-the-art <strong class="bold">StyleGAN</strong> and <strong class="bold">Self-Attention GAN</strong> (<strong class="bold">SAGAN</strong>) models, in previous chapters. You have now learned about most if not all of the important techniques used to generate images, and we can now move on to video generation (synthesis). In essence, video is simply a series of still images. Therefore, the most basic video generation method is to generate images individually and put them together in a sequence to make a video. <strong class="bold">Video synthesis</strong> is a complex and broad topic in its own right, and we won't be able to cover everything in a single chapter. </p>
			<p>In this chapter, we will get an overview of video synthesis. We will then implement what is probably the most well-known video generation technique, <strong class="bold">deepfake</strong>. We will use this to swap a person's face in a video with someone else's face. I'm sure you have seen such fake videos before. If you haven't, then just search for the word <strong class="source-inline">deepfake</strong> online and you'll be impressed by how real some of them seem.</p>
			<p>We will cover the following in this chapter:</p>
			<ul>
				<li>Video synthesis overview</li>
				<li>Implementing face image processing</li>
				<li>Building a deepfake model</li>
				<li>Swapping faces</li>
				<li>Improving DeepFakes with GANs</li>
			</ul>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor176"/>Technical requirements</h1>
			<p>The code for this chapter can be accessed here:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter09">https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter09</a></p>
			<p>The notebook used in this chapter is this one:</p>
			<ul>
				<li><strong class="source-inline">ch9_deepfake.ipynb</strong></li>
			</ul>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor177"/>Video synthesis overview</h1>
			<p>Let's say your doorbell rings while you're watching a video, so you pause the video and go to answer the door. What would you see on your screen when you come back? A still picture where <a id="_idIndexMarker603"/>everything is frozen and not moving. If you press the play button and pause it again quickly, you will see another image that looks very similar to the previous one but with slight differences. Yes – when you play a series of images sequentially, you get a video. </p>
			<p>We say that image data has three dimensions, or <em class="italic">(H, W, C)</em>; video data has four dimensions, <em class="italic">(T, H, W, C)</em>, where <em class="italic">T</em> is the temporal (time) dimension. It's also the case that video is just a big <em class="italic">batch</em> of images, except that we cannot shuffle the batch. There must be temporal consistency between the images; I'll explain this further.</p>
			<p>Let's say we extract images from some video datasets and train an unconditional GAN to generate images from random noise input. As you can imagine, the images will look very different from each other. As a result, the video made from those images would be unwatchable. Like image generation, video generation can also be classified as unconditional or conditional. </p>
			<p>In unconditional video synthesis, not only does the model need to generate good-quality content but it must also keep the temporal content or movement in check. As a result, the output video is generally quite short for some simple video content. Unconditional video synthesis is still not quite mature enough yet for practical application.</p>
			<p>On the other hand, conditional video synthesis conditions on input content and therefore generates better-quality results. As we learned in <a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a><em class="italic">, Image-to-Image Translation</em>, there is very little randomness in images generated by pix2pix. The lack of randomness may be a drawback in some applications, but the consistency in generated images is a plus in video synthesis. Thus, many video synthesis models are conditioned on images or videos. In particular, conditional face video synthesis has achieved great results and has had a real impact in commercial applications. We will now look at some of the most common forms of face video synthesis.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor178"/>Understanding face video synthesis</h2>
			<p>The most <a id="_idIndexMarker604"/>common forms of face video synthesis are <strong class="bold">face re-enactment</strong> and <strong class="bold">face swapping</strong>. It is <a id="_idIndexMarker605"/>best to explain the difference <a id="_idIndexMarker606"/>between them by using pictures as follows:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/B14538_09_1.jpg" alt="Figure 9.1 – Face re-enactment and face swapping &#13;&#10;(Source: Y. Nirkin et al., 2019, “FSGAN: Subject Agnostic Face Swapping and Reenactment,” https://arxiv.org/pdf/1908.05932)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Face re-enactment and face swapping (Source: Y. Nirkin et al., 2019, “FSGAN: Subject Agnostic Face Swapping and Reenactment,” <a href="https://arxiv.org/pdf/1908.05932">https://arxiv.org/pdf/1908.05932</a>)</p>
			<p>The top row shows how face re-enactment works. In face re-enactment, we want to transfer the expression of the face in the target video (right) to the face of the source image (left) to produce the image in the middle. Digital puppetry is already used in computer animation and movie production, where the facial expression of an actor is used to control a digital avatar. Face re-enactment using AI has the potential to make this happen more easily. The bottom row shows face swapping. This time, we want to keep the facial expression of the target video but use the face from the source image. </p>
			<p>Although <a id="_idIndexMarker607"/>technically different, face re-enactment and face swapping are similar. In terms of generated video, both could be used to create a <em class="italic">fake video</em>. As the name suggests, face swapping swaps just the face but not the head. Therefore, both the target and source faces should have a similar shape to increase the fidelity of the fake video. You can use this as a visual cue to differentiate between face swapping and face re-enactment videos. Face re-enactment is technically more challenging and it doesn't always require a driving video; it can use facial landmarks or sketches instead. We will introduce one such model in the next chapter. In the rest of this chapter, we will focus on implementing face swapping with the deepfake algorithm. </p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor179"/>DeepFake overview</h2>
			<p>Many of you may have seen online videos where the face of an actor has been swapped with another <a id="_idIndexMarker608"/>celebrity's face. Quite often, that celebrity is the actor Nicolas Cage, and the resulting videos are quite hilarious to watch. This all started around the end of 2017, when an anonymous user named <em class="italic">deepfakes</em> posted the algorithm (which was later named after the username) on the social news website Reddit. This was quite unusual, considering that almost all of the breakthrough machine learning algorithms of the past decade had their origins in academia. </p>
			<p>People have used the deepfake algorithm to create all sorts of videos, including some for TV advertisements and movies. However, as these fake videos can be very convincing, they have also raised some ethical issues. Researchers have demonstrated that it is possible to create fake videos to make the former US president Barack say things that he did not say. People have genuine reasons to be worried about deepfake and researchers have also been devising ways to detect these fake videos. You will want to understand how deepfake works, either to create funny videos or to combat fake news videos. So, let's crack on!</p>
			<p>The deepfake algorithm can be roughly broken into two parts:</p>
			<ol>
				<li><strong class="bold">A deep learning model</strong> to perform face image translation. We will first collect datasets for two people, say, <strong class="bold">A</strong> and <strong class="bold">B</strong>, and use an autoencoder to train them <a id="_idIndexMarker609"/>separately to learn their latent code, as shown in the following figure. There is a shared encoder, but we use separate <a id="_idIndexMarker610"/>decoders for different people. The top diagram in the figure shows the training architecture. The bottom diagram shows the face swap. <p>Firstly, <strong class="bold">Face A</strong> (the source) is encoded into a small latent face (the latent code). The latent code contains face representations such as the head pose (angle), facial expression, eyes open or shut, and more. We will then use decoder <strong class="bold">B</strong> to convert the latent code into <strong class="bold">Face B</strong>. The aim is to generate <strong class="bold">Face B</strong> using the pose and expression of <strong class="bold">Face A</strong>:</p><div id="_idContainer181" class="IMG---Figure"><img src="image/B14538_09_2.jpg" alt="Figure 9.2 – deepfake using autoencoders. (Top) Training with one encoder and two decoders. (Bottom) Reconstructing Face B from A (Redrawn from: T.T. Nguyen et al, 2019, “Deep Learning for deepfakes Creation and Detection: A Survey,” https://arxiv.org/abs/1909.11573) "/></div><p class="figure-caption">Figure 9.2 – deepfake using autoencoders. (Top) Training with one encoder and two decoders. (Bottom) Reconstructing Face B from A (Redrawn from: T.T. Nguyen et al, 2019, “Deep Learning for deepfakes Creation and Detection: A Survey,” https://arxiv.org/abs/1909.11573) </p><p>In a normal <a id="_idIndexMarker611"/>image generation setting, a model <a id="_idIndexMarker612"/>is basically what we need for production. All we need to do is to send an input image to the model to produce an output image. But the production pipeline for deepfake is more involved, as will be described later.</p></li>
				<li>We'll <a id="_idIndexMarker613"/>need a collection <a id="_idIndexMarker614"/>of traditional computer vision techniques to perform pre- and post-processing, including these:<p>a) Face detection</p><p>b) Face landmark detection</p><p>c) Face alignment</p><p>d) Face warping</p><p>e) Face mask detection</p><p>The following figure shows the deepfake production pipeline:</p></li>
			</ol>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/B14538_09_3.jpg" alt="Figure 9.3 – DeepFake production pipeline (Source: Y. Li, S. Lyu, 2019, “Exposing deepfake Videos By Detecting Face Warping Artifacts,” https://arxiv.org/abs/1811.00656)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – DeepFake production pipeline (Source: Y. Li, S. Lyu, 2019, “Exposing deepfake Videos By Detecting Face Warping Artifacts,” https://arxiv.org/abs/1811.00656)</p>
			<p>The steps can be grouped into three stages:</p>
			<ol>
				<li value="1">Steps <em class="italic">(a)</em> to <em class="italic">(f)</em> are pre-processing steps to extract and align the source face from the image.</li>
				<li>There is a face swap to produce target <em class="italic">face (g)</em>.</li>
				<li>Steps <em class="italic">(h)</em> to <em class="italic">(j)</em> are post-processing steps to <em class="italic">paste</em> the target face into the image.</li>
			</ol>
			<p>We learned <a id="_idIndexMarker615"/>about and built autoencoders in <a href="B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Variational Autoencoder</em>, and therefore it is relatively easy for <a id="_idIndexMarker616"/>us to build one for deepfake. On the other hand, many of the aforementioned computer vision techniques have not been introduced before in this book. Therefore, in the next section, we will implement the face processing steps one by one. Then, we will implement the autoencoder and finally implement all of the techniques together to produce a deepfake video.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor180"/>Implementing face image processing</h1>
			<p>We will use <a id="_idIndexMarker617"/>mainly two Python libraries – <strong class="bold">dlib</strong> and <strong class="bold">OpenCV</strong> – to implement <a id="_idIndexMarker618"/>most of the face processing tasks. OpenCV is good for general-purpose <a id="_idIndexMarker619"/>computer vision tasks and includes low-level functions and algorithms. While <strong class="source-inline">dlib</strong> was originally a C++ toolkit for machine learning, it also has a Python interface, and it is the go-to machine learning Python library for facial landmark detection. Most of the image processing code used in this chapter is adapted from <a href="https://github.com/deepfakes/faceswap">https://github.com/deepfakes/faceswap</a>. </p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor181"/>Extracting image from video</h2>
			<p>The first thing in the production pipeline is to extract images from video. A video is made up <a id="_idIndexMarker620"/>of a series of images separated by a fixed time interval. If you check a video file's properties, you may find something <a id="_idIndexMarker621"/>that says <strong class="bold">frame rate = 25 fps</strong>. <strong class="bold">FPS</strong> indicates the number of image <strong class="bold">frames per second</strong> in a video, and 25 fps is the standard video frame rate. That means 25 images are played within a 1-second duration, or every image is played for 1/25 = 0.04 seconds. There are many software packages and tools to split video into images, and <strong class="bold">ffmpeg</strong> is<a id="_idTextAnchor182"/> <a id="_idIndexMarker622"/>one such tool. The following command shows how to split a <strong class="source-inline">.mp4</strong> video file into directory/images and name them using a number sequence – for example, <strong class="source-inline">image_0001.png</strong>, <strong class="source-inline">image_0002.png</strong>, and so on:</p>
			<p class="source-code">ffmpeg  -i video.mp4 /images/image_%04d.png</p>
			<p>Alternatively, we can also use OpenCV to read the video frame by frame and save the frames into individual image files as shown in the following code:</p>
			<p class="source-code">import cv2</p>
			<p class="source-code">cap = cv2.VideoCapture('video.mp4')</p>
			<p class="source-code">count = 0 </p>
			<p class="source-code">while cap.isOpened():</p>
			<p class="source-code">    ret,frame = cap.read()</p>
			<p class="source-code">    cv2.imwrite(“images/image_%04d.png” % count, frame)</p>
			<p class="source-code">    count += 1</p>
			<p>We will use the extracted images for all subsequent processing and not worry about the source video anymore.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor183"/>Detecting and localizing a face</h2>
			<p>Traditional <a id="_idIndexMarker623"/>computer vision techniques detect <a id="_idIndexMarker624"/>faces by using the <strong class="bold">Histogram of Oriented Gradients</strong> (<strong class="bold">HOG</strong>). The <a id="_idIndexMarker625"/>gradient of a pixel image can be calculated by taking the difference of the preceding and following pixels in the horizontal and vertical directions. The magnitude and direction of a gradient tells us about the lines and corners of a face. We can then use the HOG as a feature descriptor to detect the shape of a face. A modern approach is, of course, to use a CNN, which is more accurate but slower.</p>
			<p><strong class="source-inline">face_recognition</strong> is a library built on top of <strong class="source-inline">dlib</strong>. By default, it uses the HOG of <strong class="source-inline">dlib</strong> as a face detector, but it also has the option to use a CNN. Using it is simple, as shown here:</p>
			<p class="source-code">import face_recognition</p>
			<p class="source-code">coords = face_recognition.face_locations(image, model='cnn')[0]</p>
			<p>This will <a id="_idIndexMarker626"/>return a list of coordinates for each of <a id="_idIndexMarker627"/>the faces detected in the image. In our code, we assume that there is only one face in the image. The coordinates returned are in <strong class="source-inline">css</strong> format, (top, right, bottom, left), so we'll need an additional step to convert them into <strong class="source-inline">dlib.rectangle</strong> objects for the <strong class="source-inline">dlib</strong> facial landmarks detector, as follows: </p>
			<p class="source-code">def _css_to_rect(css):</p>
			<p class="source-code">    return dlib.rectangle(css[3], css[0], css[1], css[2])</p>
			<p class="source-code">face_coords = _css_to_rect(coords)</p>
			<p>We can read the bounding box coordinates from <strong class="source-inline">dlib.rectangle</strong> and crop the face from the image as follows:</p>
			<p class="source-code">def crop_face(image, coords, pad=0):</p>
			<p class="source-code">    x_min = coords.left() - pad</p>
			<p class="source-code">    x_max = coords.right() + pad</p>
			<p class="source-code">    y_min = coords.top() - pad</p>
			<p class="source-code">    y_max = coords.bottom() + pad</p>
			<p class="source-code">    return image[y_min:y_max, x_min:x_max]</p>
			<p>If a face <a id="_idIndexMarker628"/>is detected in the image, we can then <a id="_idIndexMarker629"/>move on to the next step to detect facial landmarks.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor184"/>Detecting facial landmarks</h2>
			<p><strong class="bold">Facial landmarks</strong> are locations <a id="_idIndexMarker630"/>of interesting points (also known as <strong class="bold">keypoints</strong>) on <a id="_idIndexMarker631"/>an image of a human face. The points are around <a id="_idIndexMarker632"/>the edges of the chin, eyebrows, nose bridge, nose tip, eyes, and lips. The following figure shows an example of 68 points of facial landmarks produced by the <strong class="source-inline">dlib</strong> model:</p>
			<p class="figure-caption"><img src="image/B14538_09_4.png" alt="Figure 9.4 – The 68 points the dlib facial landmarks, for the chin, eyebrows, nose bridge, nose tip, eyes, and lips"/></p>
			<p class="figure-caption">Figure 9.4 – The 68 points the dlib facial landmarks, for the chin, eyebrows, nose bridge, nose tip, eyes, and lips</p>
			<p><strong class="source-inline">dlib</strong> makes facial landmarks detection easy. We only need to download and load the model into <strong class="source-inline">dlib</strong> before using it as shown in the following code snippet:</p>
			<p class="source-code">predictor = dlib.shape_predictor( 		    'shape_predictor_68_face_landmarks.dat') </p>
			<p class="source-code">face_shape = predictor(face_image, face_coords)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will also pass the face coordinates into the predictor to tell it where the face is. This means, we don't need to crop out the face before calling the function.  </p>
			<p>Facial landmarks <a id="_idIndexMarker633"/>are very useful features in machine learning problems. For example, if we want to know a person's facial expression, we could use the lips keypoints as input features to the machine learning algorithm to detect whether the mouth is open. This is more effective and efficient than looking at every single pixel in the image. We can also use facial landmarks to estimate the head pose. </p>
			<p>In DeepFake, we use facial landmarks to perform face alignment, which I will explain shortly. Before that, we will need to convert the landmarks from <strong class="source-inline">lib</strong> format into a NumPy array:</p>
			<p class="source-code">def shape_to_np(shape):</p>
			<p class="source-code">    coords = []</p>
			<p class="source-code">    for i in range(0, shape.num_parts):        </p>
			<p class="source-code">        coords.append((shape.part(i).x, shape.part(i).y))</p>
			<p class="source-code">    return np.array(coords)</p>
			<p class="source-code">face_shape = shape_to_np(face_shape)</p>
			<p>Now we have everything we need for face alignment.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor185"/>Aligning a face</h2>
			<p>Naturally, faces in video appear in various poses, such as looking to the left or open-mouthed. In order <a id="_idIndexMarker634"/>to make it easier for the autoencoder to learn, we will align the faces to the center of the cropped image, looking straight at <a id="_idIndexMarker635"/>the camera. This is known as <strong class="bold">face alignment</strong>. We can see it as a form of data normalization. The deepfake author defined a set of facial landmarks as <a id="_idIndexMarker636"/>a reference face and called this the <em class="italic">mean face</em>. The mean face includes all of the 68 <strong class="source-inline">dlib</strong> landmarks except for the first 18 points of the chin. This is because people have vastly different chin shapes, which could skew the alignment results, so they are not used as a reference. </p>
			<p class="callout-heading">Mean face</p>
			<p class="callout">If you still remember, we looked at mean faces in <a href="B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a><em class="italic">, Getting Started with Image Generation Using TensorFlow</em>. They were generated by directly sampling from the dataset, so not exactly the same way as used in <strong class="source-inline">dlib</strong>. Anyway, feel free to go to take a look if you have forgotten what mean faces look like. </p>
			<p>We will need to perform the following operations on the face to align it with the mean face's position and angle:</p>
			<ul>
				<li>Rotation</li>
				<li>Scale</li>
				<li>Translation (shift in location)</li>
			</ul>
			<p>These operations can be represented using a 2×3 affine transformation matrix. The affine matrix <em class="italic">M</em> is composed of matrices <em class="italic">A</em> and <em class="italic">B</em> as shown in the following equation:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/Formula_09_001.jpg" alt=""/>
				</div>
			</div>
			<p>Matrix <em class="italic">A</em> contains the parameters for linear transformation (scale and rotation), while matrix <em class="italic">B</em> is used for translation. deepfake uses an algorithm from S. Umeyama to estimate the parameters. The source code of the algorithm is contained in a single file that I have included in our GitHub repository. We call the function by passing the detected facial landmarks and the mean face landmarks as shown in the following code. As explained earlier, we omit the chin landmarks as they are not included in the mean face:</p>
			<p class="source-code">from umeyama import umeyama def get_align_mat(face_landmarks):    return umeyama(face_landmarks[17:], \  			   mean_landmarks, False)[0:2]affine_matrix <a id="_idTextAnchor186"/>= get_align_mat(face_image)</p>
			<p>We can <a id="_idIndexMarker637"/>now pass the affine matrix into <strong class="source-inline">cv2.warpAffine()</strong> to perform affine transformation, as shown in the following code:</p>
			<p class="source-code">def align_face(face_image, affine_matrix, size, padding=50):</p>
			<p class="source-code">    affine_matrix = affine_matrix * \ 				(size[0] - 2 * padding) </p>
			<p class="source-code">    affine_matrix[:, 2] += padding</p>
			<p class="source-code">    aligned_face = cv2.warpAffine(face_image,  						  affine_matrix,  						  (size, size))    return aligned_face</p>
			<p>The following figure shows the faces before and after alignment:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B14538_09_5.jpg" alt="Figure 9.5 – (Left) Author’s face with facial landmarks and face detection bounding box. (Right) Aligned face"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – (Left) Author's face with facial landmarks and face detection bounding box. (Right) Aligned face</p>
			<p>The bounding boxes in the figure show the face detection at work. The picture on the left is also marked with facial landmarks. On the right is the face after alignment. We can see <a id="_idIndexMarker638"/>that the face has now been scaled larger to fit the mean face. In fact, the alignment output has the face more zoomed in, covering only the area between the eyebrows and the chin. I added padding to zoom out a little to include the bounding box in the final image. We can see from the bounding box that the face has been rotated so that it appears vertical. Next, we will learn about the last image pre-processing step: face warping. </p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor187"/>Face warping</h2>
			<p>We'll need <a id="_idIndexMarker639"/>two images to train an autoencoder, the input image and the target image. In deepfake, the target image is the aligned face, while the input image is a warped version of the aligned face. A face in the image does not change its shape after the affine transformation that we implemented in the preceding section, but warping, for example, twisting one side of the face, can change the shape of a face. deepfake warps faces to imitate the variety of face poses in real video as data augmentation. </p>
			<p>In image processing, transformation is the mapping of a pixel from one location in a source image to a different location in a target image. For example, translation and rotation is a one-to-one mapping that changes location and angle but retains size and shape. For warping, the mapping can be irregular, and the same point can be mapped to multiple points, which can give the effect of twisting and bending. The following diagram shows <a id="_idIndexMarker640"/>an example of mapping that warps an image from dimensions 256×256 to 64×64:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B14538_09_6.jpg" alt="Figure 9.6 – Mapping to demonstrate warping&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Mapping to demonstrate warping</p>
			<p>We will perform some random warping to twist a face slightly but not so much as to cause major distortion. The following code shows how to perform a face warp. You don't have to understand every line of the code; it is sufficient to know that it uses the mapping as previously described to warp a face into a smaller dimension:</p>
			<p class="source-code">coverage = 200 range_ = numpy.linspace(128 - coverage//2, 128 + coverage//2, 5)mapx = numpy.broadcast_to(range_, (5, 5))</p>
			<p class="source-code">mapy = mapx.T</p>
			<p class="source-code">mapx = mapx + numpy.random.normal(size=(5, 5), scale=5)</p>
			<p class="source-code">mapy = mapy + numpy.random.normal(size=(5, 5), scale=5)</p>
			<p class="source-code">interp_mapx = cv2.resize(mapx, (80, 80))\ 				   [8:72, 8:72].astype('float32')</p>
			<p class="source-code">interp_mapy = cv2.resize(mapy, (80, 80))[8:72,\ 				   8:72].astype('float32')</p>
			<p class="source-code">warped_image = cv2.remap(image, interp_mapx,  				   interp_mapy, cv2.INTER_LINEAR)</p>
			<p>I guess <a id="_idIndexMarker641"/>most people think that deepfake is just a deep neural network but do not realize there are so many image processing steps involved. Luckily, OpenCV and <strong class="source-inline">dlib</strong> make things easy for us. Now, we can move on to build the whole deep neural network model.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor188"/>Building a DeepFake model</h1>
			<p>The deep learning <a id="_idIndexMarker642"/>model used in the original deepfake is an autoencoder-based one. There are a total of two autoencoders, one for each face domain. They share the same encoder, so there is a total of one encoder and two decoders in the model. The autoencoders expect an image size of 64×64 for both the input and the output. Now, let's build the encoder. </p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor189"/>Building the encoder</h2>
			<p>As we learned in the previous chapter, the encoder is responsible for converting high-dimensional <a id="_idIndexMarker643"/>images into a low-dimensional representation. We'll first write a function to encapsulate the convolutional layer; leaky ReLU activation is used for downsampling:</p>
			<p class="source-code">def downsample(filters):</p>
			<p class="source-code">    return Sequential([</p>
			<p class="source-code">        Conv2D(filters, kernel_size=5, strides=2, 			padding='same'),</p>
			<p class="source-code">        LeakyReLU(0.1)])</p>
			<p>In the usual autoencoder implementation, the output of the encoder is a 1D vector with a size of about 100 to 200, but deepfake uses larger dimensions of 1,024. In addition, it reshapes the 1D latent vector and upscales it back into 3D activation. Therefore, the output of the encoder is not a 1D vector of size (1,024) but a tensor of size (8, 8, 512), as shown in the following code:</p>
			<p class="source-code">def Encoder(z_dim=1024):</p>
			<p class="source-code">    inputs = Input(shape=IMAGE_SHAPE)</p>
			<p class="source-code">    x = inputs</p>
			<p class="source-code">    x = downsample(128)(x)</p>
			<p class="source-code">    x = downsample(256)(x)</p>
			<p class="source-code">    x = downsample(512)(x)</p>
			<p class="source-code">    x = downsample(1024)(x)</p>
			<p class="source-code">    x = Flatten()(x)</p>
			<p class="source-code">    x = Dense(z_dim)(x)</p>
			<p class="source-code">    x = Dense(4 * 4 * 1024)(x)</p>
			<p class="source-code">    x = Reshape((4, 4, 1024))(x)  </p>
			<p class="source-code">    x = UpSampling2D((2,2))(x)</p>
			<p class="source-code">    out = Conv2D(512, kernel_size=3, strides=1, 			 padding='same')(x)        </p>
			<p class="source-code">    return Model(inputs=inputs, outputs=out, name='encoder')</p>
			<p>We can <a id="_idIndexMarker644"/>see that the encoder can be grouped into three stages:</p>
			<ol>
				<li value="1">There <a id="_idIndexMarker645"/>are convolutional layers, which downscale a <strong class="source-inline">(64, 64, 3)</strong> image all the way to <strong class="source-inline">(4, 4, 1024)</strong>.</li>
				<li>There <a id="_idIndexMarker646"/>are two dense layers. The first one produces a latent vector of size <strong class="source-inline">1024</strong>, then the second one projects it to a higher dimension, which gets reshaped to <strong class="source-inline">(4, 4, 1024)</strong>.</li>
				<li>The <a id="_idIndexMarker647"/>upsampling and convolution layers bring the output to size <strong class="source-inline">(8, 8, 512)</strong>.</li>
			</ol>
			<p>This can be understood better by looking at the following model summary:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B14538_09_7.jpg" alt="Figure 9.7 – Model summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Model summary</p>
			<p>The <a id="_idIndexMarker648"/>next step is to construct the decoder.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor190"/>Building the decoder</h2>
			<p>The decoder's input <a id="_idIndexMarker649"/>comes from the encoder's output, so it expects a tensor of size <strong class="source-inline">(8, 8, 512)</strong>. We use several layers of upsampling to upscale the activations gradually to the target image dimension of <strong class="source-inline">(64, 64, 3)</strong>:</p>
			<ol>
				<li value="1">Similar to before, we will first write a function for the upsampling block that contains an upsampling function, a convolutional layer, and leaky ReLU, as shown in the following code:<p class="source-code">def upsample(filters, name=''):</p><p class="source-code">    return Sequential([</p><p class="source-code">        UpSampling2D((2,2)),</p><p class="source-code">        Conv2D(filters, kernel_size=3, strides=1, 			padding='same'),        </p><p class="source-code">        LeakyReLU(0.1)</p><p class="source-code">    ], name=name)</p></li>
				<li>Then we stack the upsampling blocks together. The final layer is a convolutional layer that brings the channel number to <strong class="source-inline">3</strong> to match the RGB color channels:<p class="source-code">def Decoder(input_shape=(8, 8 ,512)):</p><p class="source-code">    inputs = Input(shape=input_shape)</p><p class="source-code">    x = inputs    </p><p class="source-code">    x = upsample(256,”Upsample_1”)(x)</p><p class="source-code">    x = upsample(128,”Upsample_2”)(x)</p><p class="source-code">    x = upsample(64,”Upsample_3”)(x)</p><p class="source-code">    out = Conv2D(filters=3, kernel_size=5, 			 padding='same', 			 activation='sigmoid')(x)    </p><p class="source-code">    return Model(inputs=inputs, outputs=out, 			 name='decoder')</p><p>The decoder model summary is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B14538_09_8.jpg" alt="Figure 9.8 – Keras model summary of the decoder&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Keras model summary of the decoder</p>
			<p>Next, we will put <a id="_idIndexMarker650"/>the encoder and decoders together to construct the autoencoders. </p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor191"/>Training the autoencoders</h2>
			<p>As mentioned <a id="_idIndexMarker651"/>earlier, the DeepFake model consists of two autoencoders that share the same encoder. To construct the autoencoders, the first step is to instantiate the encoder and decoders:</p>
			<p class="source-code">class deepfake:</p>
			<p class="source-code">    def __init__(self, z_dim=1024):</p>
			<p class="source-code">        self.encoder = Encoder(z_dim)</p>
			<p class="source-code">        self.decoder_a = Decoder()</p>
			<p class="source-code">        self.decoder_b = Decoder()</p>
			<p>Then, we build two separate autoencoders by joining the encoder with the respective decoders as follows:</p>
			<p class="source-code">        x = Input(shape=IMAGE_SHAPE)</p>
			<p class="source-code">        self.ae_a = Model(x, self.decoder_a(self.encoder(x)),</p>
			<p class="source-code">                          name=”Autoencoder_A”)</p>
			<p class="source-code">        self.ae_b = Model(x, self.decoder_b(self.encoder(x)),</p>
			<p class="source-code">                          name=”Autoencoder_B”)</p>
			<p class="source-code">        optimizer = Adam(5e-5, beta_1=0.5, beta_2=0.999)</p>
			<p class="source-code">        self.ae_a.compile(optimizer=optimizer, loss='mae')</p>
			<p class="source-code">        self.ae_b.compile(optimizer=optimizer, loss='mae')</p>
			<p>The next step is to prepare the training dataset. Although the autoencoder has an input image size of 64 × 64, the image preprocessing pipeline expects images of 256 × 256. We will need about 300 images for each face domain. There is a link in the GitHub repository to where you can download some prepared images. </p>
			<p>Alternatively, you can <a id="_idIndexMarker652"/>also create datasets yourself by cropping the faces from collected images or video using the image processing techniques that we learned earlier. The faces in the dataset do not need to be aligned as the alignment will be performed in the image pre-processing pipeline. The image pre-processing generator will return two images – an aligned face and a warped version, both at a resolution of 64×64. </p>
			<p>We can now pass the two generators into <strong class="source-inline">train_step()</strong> to train the autoencoder models as follows:</p>
			<p class="source-code">def train_step(self, gen_a, gen_b):</p>
			<p class="source-code">    warped_a, target_a = next(gen_a)</p>
			<p class="source-code">    warped_b, target_b = next(gen_b)</p>
			<p class="source-code">    loss_a = self.ae_a.train_on_batch(warped_a, target_a)</p>
			<p class="source-code">    loss_b = self.ae_b.train_on_batch(warped_b, target_b)</p>
			<p class="source-code">    return loss_a, loss_b</p>
			<p>Writing and training the autoencoder is probably the easiest part of the deepfake pipeline. We don't <a id="_idIndexMarker653"/>need a lot of data; probably about 300 images per face domain is sufficient. Of course, more data should provide better results. As both the dataset and model aren't big, the training can happen relatively quickly even without the use of a GPU. Once we have a trained model, the final step is to perform the face swap. </p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor192"/>Swapping faces</h1>
			<p>Here comes <a id="_idIndexMarker654"/>the last step of the deepfake pipeline, but let's first recap the pipeline. The deepfake production pipeline involves three main stages:</p>
			<ol>
				<li value="1">Extract a face from an image using <strong class="source-inline">dlib</strong> and OpenCV.</li>
				<li>Translate the face using the trained encoder and decoders.</li>
				<li>Swap the new face back into the original image.</li>
			</ol>
			<p>The new face generated by the autoencoder is an aligned face of size 64×64, so we will need to warp it to the position, size, and angle of the face in the original image. We'll use the affine matrix obtained from <em class="italic">step 1</em> in the face extraction stage. We'll use <strong class="source-inline">cv2.warpAffine</strong> like before, but this time, the <strong class="source-inline">cv2.WARP_INVERSE_MAP</strong> flag is used to reverse the direction of image transformation as follows:</p>
			<p class="source-code">h, w, _ = image.shape</p>
			<p class="source-code">size = 64</p>
			<p class="source-code">new_image = np.zeros_like(image, dtype=np.uint8)</p>
			<p class="source-code">new_image = cv2.warpAffine(np.array(new_face, 						   dtype=np.uint8)</p>
			<p class="source-code">                          mat*size, (w, h), </p>
			<p class="source-code">                          new_image,</p>
			<p class="source-code">                          flags=cv2.WARP_INVERSE_MAP,</p>
			<p class="source-code">                          borderMode=cv2.BORDER_TRANSPARENT)</p>
			<p>However, directly pasting the new face onto the original image will create artifacts around the edges. This will be especially obvious if any part of the new face (which is a square 64×64) exceeds the original face boundary. To alleviate the artifacts, we will trim the new face with a face mask. </p>
			<p>The first mask we <a id="_idIndexMarker655"/>will create is one that contours around the facial landmarks in the original image. The following code will first find the contours of given facial landmarks and then fill inside of the contour with ones (1) and return it as a hull mask:</p>
			<p class="source-code">def get_hull_mask(image, landmarks):</p>
			<p class="source-code">    hull = cv2.convexHull(face_shape)</p>
			<p class="source-code">    hull_mask = np.zeros_like(image, dtype=float)</p>
			<p class="source-code">    hull_mask = cv2.fillConvexPoly(hull_mask,  						   hull,(1,1,1))</p>
			<p class="source-code">    return hull_mask</p>
			<p>As the <strong class="bold">hull mask</strong> is bigger <a id="_idIndexMarker656"/>than the new face square, we will need to trim the hull mask to fit the new square. To do this, we can create a rectangle mask from the new face and multiply it with the hull mask. The following diagram shows an example of the mask for the image:</p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B14538_09_9.jpg" alt="Figure 9.9 – (Left to right) (a) Original image (b) Rectangular mask of new face (c) Hull mask of original face (d) Combined mask"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – (Left to right) (a) Original image (b) Rectangular mask of new face (c) Hull mask of original face (d) Combined mask</p>
			<p>Then we <a id="_idIndexMarker657"/>use the mask to remove the face from the original image and fill it in with the new face using the following code:</p>
			<p class="source-code">def apply_face(image, new_image, mask):</p>
			<p class="source-code">    base_image = np.copy(image).astype(np.float32)</p>
			<p class="source-code">    foreground = cv2.multiply(mask, new_image)</p>
			<p class="source-code">    background = cv2.multiply(1 - mask, base_image)</p>
			<p class="source-code">    output_image = cv2.add(foreground, background)</p>
			<p class="source-code">    return output_image</p>
			<p>The resulting face may still not look perfect. For instance, if the two faces have vastly different skin tone or shading, then we may need to use further and more sophisticated methods to iron out the artifacts. </p>
			<p>This concludes the face swapping. We do this for each of the images extracted from a video, then we convert the images back into a video sequence. One way to do so is to use <strong class="source-inline">ffmpeg</strong> as follows:</p>
			<p class="source-code">ffmpeg -start_number 1 -i image_%04d.png -vcodec mpeg4 output.mp4</p>
			<p>The deepfake model and the computer vision techniques used in this chapter are fairly basic, as I wanted <a id="_idIndexMarker658"/>to make it easy to understand. Therefore, this code may not produce a realistic fake video. If you are keen on generating good fake videos, I would <a id="_idIndexMarker659"/>recommend you visit the <a href="https://github.com/deepfakes/faceswap">https://github.com/deepfakes/faceswap</a> GitHub repository, on which a big part of this chapter's code is based. Next, we will quickly look at how deepfake can be improved by using GANs.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor193"/>Improving DeepFakes with GANs</h1>
			<p>The output <a id="_idIndexMarker660"/>image of deepfake's autoencoders can be a little blurry, so how can we improve that? To recap, the deepfake algorithm can <a id="_idIndexMarker661"/>be broken into two main techniques – face image processing and face generation. The latter can be thought of as an image-to-image translation problem, and we learned a lot about that in <a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a><em class="italic">, Image-to-Image Translation</em>. Therefore, the natural thing to do would <a id="_idIndexMarker662"/>be to use a GAN to improve the quality. One helpful model is <strong class="bold">faceswap-GAN</strong>, and we will now go over a high-level overview of it. The autoencoder from the original deepfake is enhanced with residual blocks and self-attention blocks (see <a href="B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156"><em class="italic">Chapter 8</em></a><em class="italic">, Self-Attention for Image Generation</em>) and used as a generator in faceswap-GAN. The discriminator architecture is as follows:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/B14538_09_10.jpg" alt="Figure 9.10 - faceswap-GAN’s discriminator architecture (Redrawn from: https://github.com/shaoanlu/faceswap-GAN)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 - faceswap-GAN's discriminator architecture (Redrawn from: https://github.com/shaoanlu/faceswap-GAN)</p>
			<p>We can <a id="_idIndexMarker663"/>learn a lot about the discriminator by looking <a id="_idIndexMarker664"/>at the preceding diagram alone. First, the input tensor has a channel dimension of <strong class="source-inline">6</strong>, which suggests it is a stack of two images – real and fake. Then there are two blocks of self-attention layers. The output has a shape of 8×8×1, so each of the output features looks at patches of the input image. In other words, the discriminator is PatchGAN with self-attention layers.</p>
			<p>The <a id="_idIndexMarker665"/>following <a id="_idIndexMarker666"/>diagram shows the encoder and decoder architecture:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B14538_09_11.jpg" alt="Figure 9.11 - faceswap-GAN’s encoder and decoder architecture (Redrawn from: https://github.com/shaoanlu/faceswap-GAN)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11 - faceswap-GAN's encoder and decoder architecture (Redrawn from: https://github.com/shaoanlu/faceswap-GAN)</p>
			<p>There aren't a lot of changes to the encoder and decoder. Self-attention layers are added to both the encoder and decoder, and one residual block is added to the decoder.</p>
			<p>The losses used in training are these:</p>
			<ul>
				<li><strong class="bold">Least-squares</strong> (<strong class="bold">LS</strong>) <strong class="bold">loss</strong> is the adversarial loss.</li>
				<li><strong class="bold">Perception loss</strong> is the VGG features an L2 loss between the real and fake faces.</li>
				<li>L1 reconstruction loss.</li>
				<li><strong class="bold">Edge loss</strong> is the L2 loss of the gradients (in the <em class="italic">x</em> and <em class="italic">y</em> directions) around the eyes. This helps the model to generate realistic eyes.</li>
			</ul>
			<p>One thing that I've been trying to achieve with this book is to instill you with the knowledge <a id="_idIndexMarker667"/>of most, if not all, of the fundamental building blocks of image generation. Once you know them, implementing a model <a id="_idIndexMarker668"/>is just like putting Lego bricks together. As we are already familiar with the losses (apart from edge loss), residual blocks, and self-attention blocks, I trust that you can now implement <a id="_idIndexMarker669"/>this model yourself, if you wish to. For interested readers, you can refer to the original implementation at <a href="https://github.com/shaoanlu/faceswap-GAN">https://github.com/shaoanlu/faceswap-GAN</a>. </p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor194"/>Summary</h1>
			<p>Congratulations! We have now finished all the coding in this book. We have learned how to use <strong class="source-inline">dlib</strong> to detect faces and facial landmarks and how to use OpenCV to warp and align a face. We also learned how to use warping and masking to do face swapping. As a matter of fact, we spent most of the chapter learning about face image processing and spent very little time on the deep learning side. We have implemented autoencoders by reusing and modifying the autoencoder code from the previous chapter. </p>
			<p>Finally, we went over an example of improving deepfake by using GANs. faceswap-GAN improves deepfake by adding a residual block, a self-attention block, and a discriminator for adversarial training, all of which we have already learned about in previous chapters.</p>
			<p>In the next chapter, which is also the final chapter, we will review the techniques we have learned in this book and look at some of the pitfalls in training GANs for real-world applications. Then, we will go over a few important GAN architectures, looking at image inpainting and text-to-image synthesis. Finally, we will look at up-and-coming applications such as video retargeting and 3D-to-2D rendering. There won't be any coding in the next chapter, so you can sit back and relax. Cheers!</p>
		</div>
	</body></html>