- en: Appendix 2 – Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following are the answers to the questions mentioned at the end of each
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 1 – Fundamentals of Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In supervised and unsupervised learning, the model (agent) learns based on the
    given training dataset, whereas, in **reinforcement learning** (**RL**), the agent
    learns by directly interacting with the environment. Thus RL is essentially an
    interaction between the agent and its environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The environment is the world of the agent. The agent stays within the environment.
    For instance, in the chess game, the chessboard is the environment since the chess
    player (agent) learns to play chess within the chessboard (environment). Similarly,
    in the Super Mario Bros game, the world of Mario is called the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The deterministic policy maps the state to one particular action, whereas the
    stochastic policy maps the state to the probability distribution over an action
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent interacts with the environment by performing actions, starting from
    the initial state until they reach the final state. This agent-environment interaction
    starting from the initial state until the final state is called an episode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The discount factor helps us in preventing the return reaching up to infinity
    by deciding how much importance we give to future rewards and immediate rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value function (value of a state) is the expected return of the trajectory
    starting from that state whereas the Q function (the Q value of a state-action
    pair) is the expected return of the trajectory starting from that state and action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a deterministic environment, we can be sure that when an agent performs an
    action *a* in state *s*, then it always reaches state ![](img/B15558_12_016.png).
    In a stochastic environment, we cannot say that by performing some action *a*
    in state *s*, the agent always reaches state ![](img/B15558_12_016.png) because
    there will be some randomness associated with the stochastic environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 2 – A Guide to the Gym Toolkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Gym toolkit provides a variety of environments for training the RL agent
    ranging from classic control tasks to Atari game environments. We can train our
    RL agent to learn in these simulated environments using various RL algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can create a Gym environment using the `make` function. The `make` function
    requires the environment ID as a parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We learned that the action space consists of all the possible actions in the
    environment. We can obtain the action space by using `env.action_space`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can visualize the Gym environment using the `render()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some classic control environments offered by Gym include the cart pole balancing
    environment, the pendulum, and the mountain car environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can generate an episode by selecting an action in each state using the `step()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The state space of the Atari environment will be either the game screen's pixel
    values or the RAM of the Atari machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can record the agent's gameplay using the Monitor wrapper. It takes three
    parameters—the environment, the directory where we want to save our recordings,
    and the force option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 3 – The Bellman Equation and Dynamic Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bellman equation states that the value of a state can be obtained as a sum
    of the immediate reward and the discounted value of the next state. Similar to
    the Bellman equation of the value function, the Bellman equation of the Q function
    states that the Q value of a state-action pair can be obtained as a sum of the
    immediate reward and the discounted Q value of the next state-action pair.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Bellman expectation equation gives the Bellman value and Q functions whereas
    the Bellman optimality equation gives the optimal Bellman value and Q functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value function can be derived from the Q function as ![](img/B15558_19_003.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Q function can be derived from the value function as ![](img/B15558_19_004.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the value iteration method, we perform the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the optimal value function by taking maximum over Q function, that is,
    ![](img/B15558_03_088.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the optimal policy from the computed optimal value function
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the policy iteration method, we perform the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the random policy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value function using the given policy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract a new policy using the value function obtained from *step 2*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the extracted policy is the same as the policy used in *step 2* then stop,
    else send the extracted new policy to *step 2* and repeat *steps 2* to *4*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In the value iteration method, first, we compute the optimal value function
    by taking the maximum over the Q function iteratively. Once we find the optimal
    value function then we will use it to extract the optimal policy. In the policy
    iteration method, we will try to compute the optimal value function using the
    policy iteratively. Once we have found the optimal value function then the policy
    that was used to create the optimal value function will be extracted as the optimal
    policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 4 – Monte Carlo Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Monte Carlo method, we approximate the value of a state by taking the
    average return of a state across *N* episodes instead of taking the expected return.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To compute the value function using the dynamic programming method, we need
    to know the model dynamics, and when we don't know the model dynamics, we use
    model-free methods. The Monte Carlo method is a model-free method meaning that
    it doesn't require the model dynamics (transition probability) to compute the
    value function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a prediction task, we evaluate the given policy by predicting the value function
    or Q function, which helps us to understand the expected return an agent would
    get if it used the given policy. However, in a control task, our goal is to find
    the optimal policy and are not given any policy as input, so we start by initializing
    a random policy and try to find the optimal policy iteratively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the MC prediction method, the value of a state and value of a state-action
    pair can be computed by just taking the average return of the state and an average
    return of state-action pair across several episodes respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In first-visit MC, we compute the return only for the first time the state is
    visited in the episode. In every-visit MC, we compute the return every time the
    state is visited in the episode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the environment is non-stationary, we don't have to take the return of
    the state from all the episodes and compute the average. As the environment is
    non-stationary, we can ignore returns from earlier episodes and use only the returns
    from the latest episodes for computing the average. Thus, we can compute the value
    of the state using the incremental mean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the on-policy method, we generate episodes using one policy and also improve
    the same policy iteratively to find the optimal policy, while with the off-policy
    Monte Carlo control method, we use two different policies for generating the episode
    (the behavior policy) and for finding the optimal policy (the target policy).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An epsilon-greedy policy is one where we select a random action (exploration)
    with probability epsilon, and we select the best action (exploitation) with probability
    1-epsilon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 5 – Understanding Temporal Difference Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike the Monte Carlo method, the **Temporal Difference** (**TD**) learning
    method makes use of bootstrapping so that we don't have to wait until the end
    of the episode to compute the value of a state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The TD learning algorithm takes the benefits of both the dynamic programming
    and the Monte Carlo methods into account. That is, just like the dynamic programming
    method, we perform bootstrapping so that we don't have to wait till the end of
    an episode to compute the state value or Q value and just like the Monte Carlo
    method, it is a model-free method, and so it does not require the model dynamics
    of the environment to compute the state value or Q value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The TD error can be defined as the difference between the target value and predicted
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The TD learning update rule is given as ![](img/B15558_05_010.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a TD prediction task, given a policy, we estimate the value function using
    the given policy. So, we can say what the expected return an agent can obtain
    in each state if it acts according to the given policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SARSA** is an on-policy TD control algorithm and it stands for **State-Action-Reward-State-Action**.
    The update rule for computing the Q function using SARSA is given as ![](img/B15558_18_045.png).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SARSA is an on-policy algorithm, meaning that we use a single epsilon-greedy
    policy for selecting an action in the environment and also to compute the Q value
    of the next state-action pair, whereas Q learning is an off-policy algorithm meaning
    that we use an epsilon-greedy policy for selecting an action in the environment,
    but to compute the Q value of the next state-action pair we use a greedy policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 6 – Case Study – The MAB Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Multi-Armed Bandit** (**MAB**) problem is one of the classic problems
    in RL. A MAB is a slot machine where we pull the arm (lever) and get a payout
    (reward) based on some probability distribution. A single slot machine is called
    a one-armed bandit, and when there are multiple slot machines, it is called a
    MAB or *k*-armed bandit, where *k* denotes the number of slot machines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the epsilon-greedy policy, we select the best arm with probability 1-epsilon,
    and we select the random arm with probability epsilon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In softmax exploration, the arm will be selected based on the probability. However,
    in the initial rounds we will not know the correct average reward of each arm,
    so selecting the arm based on the probability of average reward will be inaccurate
    in the initial rounds. So to avoid this we introduce a new parameter called *T*.
    *T* is called the temperature parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The upper confidence bound is computed as ![](img/B15558_19_009.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the value of ![](img/B15558_19_010.png) is higher than ![](img/B15558_19_011.png),
    then we will have a high probability closer to 1 than 0\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The steps involved in the Thomson sampling method are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the beta distribution with alpha and beta set to equal values for
    all the *k* arms
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a value from the beta distribution of all the *k* arms
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pull the arm whose sampled value is high
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If we win the game then update the alpha value of the distribution as ![](img/B15558_19_012.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If we lose the game then update the beta value of the distribution as ![](img/B15558_19_013.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *5* for several numbers of rounds
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: With contextual bandits, we take actions based on the state of the environment
    and the state holds the context. Contextual bandits are widely used for personalizing
    content according to the user's behavior. They are also used to solve the cold-start
    problems faced in recommendation systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 7 – Deep Learning Foundations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The activation function is used to introduce non-linearity to neural networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The softmax function is basically a generalization of the sigmoid function.
    It is usually applied to the final layer of the network and while performing multi-class
    classification tasks. It gives the probabilities of each class being output and
    thus, the sum of softmax values will always equal 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The epoch specifies the number of times the neural network sees our whole training
    data. So, we can say one epoch is equal to one forward pass and one backward pass
    for all training samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RNNs are widely applied for use cases that involve sequential data, such as
    time series, text, audio, speech, video, weather, and much more. They have been
    greatly used in various **Natural Language Processing** (**NLP**) tasks, such
    as language translation, sentiment analysis, text generation, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While backpropagating the RNN, we multiply the weights and derivative of the
    tanh function at every time step. When we multiply smaller numbers at every step
    while moving backward, our gradient becomes infinitesimally small and leads to
    a number that the computer can't handle; this is called the vanishing gradient
    problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pooling layer reduces spatial dimensions by keeping only the important features.
    The different types of pooling operation include max pooling, average pooling,
    and sum pooling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose, we want our GAN to generate handwritten digits. First, we will take
    a dataset containing a collection of handwritten digits; say, the MNIST dataset.
    The generator learns the distribution of images in our dataset. It learns the
    distribution of handwritten digits in our training set. We feed random noise to
    the generator and it will convert the random noise into a new handwritten digit
    similar to the one in our training set. The goal of the discriminator is to perform
    a classification task. Given an image, it classifies it as real or fake; that
    is, whether the image is from the training set or has been generated by the generator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 8 – A Primer on TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A TensorFlow session is used to execute computational graphs with operations
    on the node and tensors to its edges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variables are the containers used to store values. Variables will be used as
    input to several other operations in the computational graph. We can think of
    placeholders as variables, where we only define the type and dimension, but will
    not assign the value. Values for the placeholders will be fed at runtime. We feed
    the data to the computational graphs using placeholders. Placeholders are defined
    with no values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TensorBoard is TensorFlow's visualization tool that can be used to visualize
    the computational graph. It can also be used to plot various quantitative metrics
    and the results of several intermediate calculations. When we are training a really
    deep neural network, it would become confusing when we have to debug the model.
    As we can visualize the computational graph in TensorBoard, we can easily understand,
    debug, and optimize such complex models. It also supports sharing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eager execution in TensorFlow is more Pythonic and allows for rapid prototyping.
    Unlike the graph mode, where we need to construct a graph every time to perform
    any operation, eager execution follows the imperative programming paradigm, where
    any operation can be performed immediately without having to create a graph, just
    like we do in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Building a model in Keras involves four important steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A functional model provides more flexibility than a sequential model. For instance,
    in a functional model, we can easily connect any layer to another layer, whereas,
    in a sequential model, each layer is in a stack of one above another.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 9 – Deep Q Network and Its Variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the environment consists of a large number of states and actions, it will
    be very expensive to compute the Q value of all possible state-action pairs in
    an exhaustive fashion. So, we use a deep Q network for approximating the Q function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a buffer called the replay buffer to collect the agent's experience and
    based on this experience, we train our network. The replay buffer is usually implemented
    as a queue structure (first in, first out) rather than a list. So, if the buffer
    is full and the new experience comes in, we remove the old experience and add
    the new experience into the buffer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the target and predicted values depend on the same parameter ![](img/B15558_12_006.png),
    it will cause instability in the mean squared error and the network will learn
    poorly. It also causes a lot of divergence during training. So, we use a target
    network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike with DQNs, in double DQNs, we compute the target value using two Q functions.
    One Q function parameterized by the main network parameter ![](img/B15558_12_006.png)
    selects the action that has the maximum Q value, and the other Q function parameterized
    by the target network parameter ![](img/B15558_12_025.png) computes the Q value
    using the action selected by the main network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transition with a high TD error implies that the transition is not correct
    and so we need to learn more about that transition to minimize the error. A transition
    with a low TD error implies that the transition is already good. We can always
    learn more from our mistakes rather than only focusing on what we are already
    good at, right? Similarly, we can learn more from the transitions with a high
    TD error than those with a low TD error. Thus, we can assign a higher priority
    to the transitions with a high TD error and a lower priority to transitions that
    got a low TD error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The advantage function can be defined as the difference between the Q function
    and the value function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LSTM layer is in the DQN so that we can retain information about the past
    states as long as it is required. Retaining information about the past states
    helps us when we have the problem of **Partially Observable Markov Decision Processes**
    (**POMDPs**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 10 – Policy Gradient Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the value-based method, we extract the optimal policy from the optimal Q function
    (Q values).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is difficult to compute optimal policy using the value-based method when
    our action space is continuous. So, we use the policy-based method. In the policy-based
    method, we compute the optimal policy without the Q function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the policy gradient method, we select actions based on the action probability
    distribution given by the network and if we win the episode, that is, if we get
    a high return, then we assign high probabilities to all the actions of the episode,
    else we assign low probabilities to all the actions of the episode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The policy gradient is computed as ![](img/B15558_19_017.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reward-to-go is basically the return of the trajectory starting from the state
    *s*[t]. It is computed as ![](img/B15558_10_126.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The policy gradient with the baseline function is a policy gradient method that
    uses the baseline function to reduce the variance in the return.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The baseline function *b* gives us the expected return from the state the agent
    is in, then subtracting *b* on every step will reduce the variance in the return.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 11 – Actor-Critic Methods – A2C and A3C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The actor-critic method is one of the most popular algorithms in deep RL. Several
    modern deep RL algorithms are designed based on the actor-critic method. The actor-critic
    method lies at the intersection of value-based and policy-based methods. That
    is, it takes advantage of both value-based and policy-based methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the actor-critic method, the actor computes the optimal policy and the critic
    evaluates the policy computed by the actor network by estimating the value function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the policy gradient method with baseline, first, we generate complete episodes
    (trajectories), and then we update the parameter of the network, whereas, in the
    actor-critic method, we update the network parameter at every step of the episode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the actor network, we compute the gradient as ![](img/B15558_19_019.png)**.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **advantage actor-critic** (**A2C**), we compute the policy gradient with
    the advantage function and the advantage function is the difference between the Q
    function and the value function, that is, *Q*(*s*, *a*) – *V*(*s*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The word asynchronous implies the way A3C works. That is, instead of having
    a single agent that tries to learn the optimal policy, here, we have multiple
    agents that interact with the environment. Since we have multiple agents interacting
    with the environment at the same time, we provide copies of the environment to
    every agent so that each agent can interact with its own copy of the environment.
    So, all these multiple agents are called worker agents and we have a separate
    agent called the global agent. All the worker agents report to the global agent
    asynchronously and the global agent aggregates the learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In A2C, we can have multiple worker agents, each interacting with its own copies
    of the environment, and all the worker agents perform the synchronous updates,
    unlike A3C where the worker agents perform asynchronous updates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 12 – Learning DDPG, TD3, and SAC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DDPG consists of an actor and critic. The actor is a policy network and uses
    the policy gradient method for learning the optimal policy. The critic is a DQN
    and it evaluates the action produced by the actor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The critic is basically a DQN. The goal of the critic is to evaluate the action
    produced by the actor network. The critic evaluates the action produced by the
    actor using the Q value computed by the DQN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key features of TD3 includes clipped double Q learning, delayed policy updates,
    and target policy smoothing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of using one critic network, we use two main critic networks for computing
    the Q value and we use two target critic networks for computing the target value.
    We compute two target Q values using two target critic networks and use the minimum
    value out of these two while computing the loss. This helps in preventing the
    overestimation of the target Q value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DDPG method produces different target values even for the same action, thus
    the variance of the target value will be high even for the same action, so we
    reduce this variance by adding some noise to the target action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the SAC method, we use a slightly modified version of the objective function
    with the entropy term as ![](img/B15558_19_020.png) and it is often called **maximum
    entropy RL** or **entropy regularized RL**. Adding an entropy term is also often
    referred to as an entropy bonus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The role of the critic network is to evaluate the policy produced by the actor.
    Instead of using only the Q function to evaluate the actor's policy, the critic
    in SAC uses both the Q function and the value function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 13 – TRPO, PPO, and ACKTR Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The trust region implies the region where our actual function *f*(*x*) and approximated
    function ![](img/B15558_13_038.png) are close together. So, we can say that our
    approximation will be accurate if our approximated function ![](img/B15558_13_038.png)
    is in the trust region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TRPO is a policy gradient algorithm, and it acts as an improvement to policy
    gradient with baseline. TRPO tries to make a large policy update while imposing
    a KL constraint that the old policy and the new policy should not vary from each
    other too much. TRPO guarantees monotonic policy improvement, guaranteeing that
    there will always be a policy improvement on every iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just like gradient descent, conjugate gradient descent also tries to find the
    minimum of the function; however, the search direction of conjugate gradient descent
    will be different from gradient descent and conjugate gradient descent attains
    convergence in *N* iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The update rule of TRPO is given as ![](img/B15558_13_240.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PPO acts as an improvement to the TRPO algorithm and is simple to implement.
    Similar to TRPO, PPO ensures that the policy updates are in the trust region.
    But unlike TRPO, PPO does not use any constraint in the objective function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the PPO clipped method, in order to ensure that the policy updates are in
    the trust region, that is, the new policy is not far away from the old policy,
    PPO adds a new function called the clipping function, which ensures that the new
    and old policies are not far away from each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-FAC approximates the Fisher information matrix as a block diagonal matrix
    where each block contains the derivatives. Then each block is approximated as
    a Kronecker product of two matrices, which is known as Kronecker factorization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 14 – Distributional Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a distributional RL, instead of selecting an action based on the expected
    return, we select the action based on the distribution of the return, which is
    often called the value distribution or return distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In categorical DQN, we feed the state and support of the distribution as the
    input and the network returns the probabilities of the value distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The authors of the categorical DQN suggest that it will be efficient to choose
    the number of support *N* as 51 and so the categorical DQN is also known as the
    C51 algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inverse CDF is also known as the quantile function. Inverse CDF as the name
    suggests is the inverse of the cumulative distribution function. That is, in CDF,
    given the support *x*, we obtain the cumulative probability ![](img/B15558_12_056.png),
    whereas in inverse CDF, given cumulative probability ![](img/B15558_12_056.png),
    we obtain the support *x*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a categorical DQN, along with the state, we feed the fixed support at equally
    spaced intervals as an input to the network, and it returns the non-uniform probabilities.
    However, in a QR-DQN, along with the state, we feed the fixed uniform probabilities
    as an input to the network and it returns the support at variable locations (unequally
    spaced support).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The D4PG is similar to DDPG except for the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a distributional DQN in the critic network instead of using the regular
    DQN to estimate the Q values.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate *N*-step returns in the target instead of calculating one-step
    returns.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We use prioritized experience replay and add importance to the gradient updates
    in the critic network.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of using one actor, we use *L* independent actors, each of which acts
    in parallel, collecting experience and storing the experience in the replay buffer.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 15 – Imitation Learning and Inverse RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the simplest and most naive ways to perform imitation learning is by
    treating an imitation learning task as a supervised learning task. First, we collect
    a set of expert demonstrations, then we train a classifier to perform the same
    action performed by the expert in a particular state. We can view this as a big
    multiclass classification problem and train our agent to perform the action performed
    by the expert in the respective state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In DAgger, we aggregate the dataset over a series of iterations and train the
    classifier on the aggregated dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In DQfD, we fill the replay buffer with expert demonstrations and pre-train
    the agent. Note that these expert demonstrations are used only for pretraining
    the agent. Once the agent is pre-trained, the agent will interact with the environment
    and gather more experience and make use of it for learning. Thus DQfD consists
    of two phases, which are pre-training and training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IRL is used when it is hard to design the reward function. In RL, we try to
    find the optimal policy given the reward function but in IRL, we try to learn
    the reward function given the expert demonstrations. Once we have derived the
    reward function from the expert demonstrations using IRL, then we can use the
    reward function to train our agent to learn the optimal policy using any RL algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can represent the state with a feature vector *f*. Let's say we have a state
    *s*; then its feature vector can be defined as *f*[s].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In GAIL, the role of the generator is to generate a policy by learning the occupancy
    measure of the expert policy, and the role of the discriminator is to classify
    whether the generated policy is from the expert policy or the agent policy. So,
    we train the generator using TRPO. The discriminator is basically a neural network
    that tells us whether the policy generated by the generator is the expert policy
    or the agent policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 16 – Deep Reinforcement Learning with Stable Baselines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stable Baselines is an improved implementation of OpenAI Baselines. Stable Baselines
    is a high-level library that is easier to use than OpenAI Baselines, and it also
    includes state-of-the-art deep RL algorithms along with offering several useful
    features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can save the agent as `agent.save()` and load the trained agent as `agent.load()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generally train our agent in a single environment per step but with Stable
    Baselines, we can train our agent in multiple environments per step. This helps
    our agent to learn quickly. Now, our states, actions, reward, and done will be
    in the form of a vector since we are training our agent in multiple environments.
    So, we call this a vectorized environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In SubprocVecEnv, we run each environment in a different process, whereas in
    DummyVecEnv, we run each environment in the same process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With Stable Baselines, it is easier to view the computational graph of our model
    in TensorBoard. In order to do that, we just need to pass the directory where
    we need to store our log files while instantiating the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With Stable Baselines, we can easily record a video of our agent using the `VecVideoRecorder`
    module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 17 – Reinforcement Learning Frontiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Meta learning produces a versatile AI model that can learn to perform various
    tasks without having to train them from scratch. We train our meta-learning model
    on various related tasks with a few data points, so for a new but related task,
    it can make use of the learning obtained from the previous tasks and we don't
    have to train it from scratch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model-Agnostic Meta Learning** (**MAML**) is one of the most popularly used
    meta-learning algorithms and it has created a major breakthrough in meta-learning
    research. The basic idea of MAML is to find a better initial model parameter so
    that with good initial parameters, the model can learn quickly on new tasks with
    fewer gradient steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the outer loop of MAML, we update the model parameter as ![](img/B15558_19_026.png)
    and it is known as a meta objective.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The meta training set basically acts as a training set in the outer loop and
    is used to update the model parameter in the outer loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In hierarchical RL, we decompose a large problem into small subproblems in a
    hierarchy. The different methods used in hierarchical RL include state-space decomposition,
    state abstraction, and temporal abstraction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With an imagination augmented agent, before taking any action in an environment,
    the agent imagines the consequences of taking the action and if they think the
    action will provide a good reward, they will perform the action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
