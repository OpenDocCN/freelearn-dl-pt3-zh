- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the era of TensorFlow, the landscape of deep learning was markedly different.
    Data professionals had fewer comprehensive tools to aid in the development, training,
    and deployment of neural networks. This posed challenges in experimenting with
    various architectures and tuning model settings to solve complex tasks, as data
    experts often had to construct their models from scratch. The process was time-consuming,
    with some experts spending days or even weeks developing effective models. Another
    bottleneck was the difficulty in deploying trained models, which made the practical
    application of neural networks challenging during those early days.
  prefs: []
  type: TYPE_NORMAL
- en: But today, everything has changed; with TensorFlow, you can do lots of amazing
    things. In this chapter, we will begin by examining the TensorFlow ecosystem and
    discussing, at a high level, the various components relevant to building state-of-the-art
    applications with TensorFlow. We will begin our journey by setting up our workspace
    to meet the requirements of the exam and our upcoming experiments. We will also
    learn what TensorFlow is all about, understand the concept of tensors, explore
    basic data representation and operations in TensorFlow, and build our first model
    using this powerful tool. We will conclude this chapter by looking at how to debug
    and solve error messages in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the basics of TensorFlow, including
    what tensors are and how to perform basic data operations with them. You will
    be equipped to confidently build your first model with TensorFlow and debug and
    solve any error messages that might arise in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up our environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hello World in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging and solving error messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using `python >= 3.8.0` along with the following packages, which
    can be installed using the `pip` `install` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow>=2.7.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow-datasets==4.4.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pillow==8.4.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas==1.3.4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy==1.21.4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scipy==1.7.3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code bundle for this book is available at the following GitHub link: [https://github.com/PacktPublishing/TensorFlow-Developer-Certificate](https://github.com/PacktPublishing/TensorFlow-Developer-Certificate).
    Also, solutions to all exercises can be found in the GitHub repo itself. If you
    are new to Google Colab, here is a great resource to get you started quickly:
    [https://www.youtube.com/watch?v=inN8seMm7UI&list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL](https://www.youtube.com/watch?v=inN8seMm7UI&list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL).'
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we examined the different types of applications we could
    build with our knowledge of **machine learning** (**ML**), from chatbots to facial
    recognition systems and from house price prediction to detecting fraud in the
    banking industry – these are some of the exciting applications we can build using
    deep learning frameworks such as TensorFlow. The question we would logically ask
    is what exactly is TensorFlow? And why should we learn it at all?
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow** is an open source end-to-end framework for building deep learning
    applications. It was developed by a team of data professionals at Google in 2011
    and made openly available in 2015\. TensorFlow is a flexible, scalable solution
    that enables us to build models with ease using the Keras API. It allows us to
    access a large array of pretrained deep learning models, thus making it the framework
    of choice for many data professionals in the industry and academia. Currently,
    TensorFlow is used at powerhouses such as Google, DeepMind, Airbnb, Intel, and
    so many more companies.'
  prefs: []
  type: TYPE_NORMAL
- en: Today, with TensorFlow, you can easily train a deep learning model on a single
    PC, using a cloud service such as AWS, or using distributed training with a cluster
    of computers. Model building is just a part of what data professionals do; what
    about visualizing, deploying, and monitoring our models? TensorFlow has a wide
    range of tools to cater to these processes, such as TensorBoard, TensorFlow lite,
    TensorFlow.js, TensorFlow Hub, and **TensorFlow Extended** (**TFX**). These tools
    enable data professionals to build and deploy scalable, low-latency, ML-powered
    applications across various domains – on the web, on mobile, and on edge devices.
    To support TensorFlow developers, there is comprehensive documentation and a large
    community of developers who report bugs and contribute to the further development
    and improvement of this framework.
  prefs: []
  type: TYPE_NORMAL
- en: Another central feature of the TensorFlow ecosystem is its access to a diverse
    array of datasets, cutting across different ML problem types such as image data,
    text data, and time-series data. These datasets are available via TensorFlow Datasets,
    and they are a great way to master the use of TensorFlow in solving real-world
    problems. In subsequent chapters, we will be exploring how to build models to
    solve computer vision, natural language processing, and time-series forecasting
    problems using a range of datasets available within the TensorFlow ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have explored some indispensable tools in the TensorFlow ecosystem. It is
    always a good idea to take a tour of these features (and the new ones that will
    be added) on the official website: [https://www.tensorflow.org/](https://www.tensorflow.org/).
    However, in the exam, you will not be quizzed on this. The idea here is to get
    familiar with what is available in the ecosystem. The exam focuses on modeling
    with TensorFlow so we will only use tools in the ecosystem such as TensorFlow
    Datasets, the Keras API, and TensorFlow Hub to meet this objective.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we examine data representations in TensorFlow, let’s set up our work
    environment. We will begin by importing TensorFlow and checking the version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this block of code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Hurray! We have successfully imported TensorFlow. Next, let us import NumPy
    and a couple of data types, as we will be using them shortly in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully completed all our import steps without errors. We will
    now look at data representations in TensorFlow as our working environment is fully
    set up.
  prefs: []
  type: TYPE_NORMAL
- en: Data representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our quest to solve complex tasks using ML, we come across diverse types of
    raw data. Our primary role involves transforming this raw data (which could be
    text, images, audio, or video) into numerical representations. These representations
    allow our ML models to easily digest and learn the underlying patterns in the
    data efficiently. To achieve this, this is where TensorFlow and its fundamental
    data structure, tensors, come into play. While numerical data is commonly used
    in training models, our models are also adept at efficiently handling binary and
    categorical data. For such data types, we apply techniques such as one-hot encoding
    to transform them into a model-friendly format.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensors** are multi-dimensional arrays designed for numerical data representation;
    although they share some similarities with NumPy arrays, they possess certain
    unique features that give them an advantage in deep learning tasks. One of these
    key advantages is their ability to utilize hardware acceleration from GPUs and
    TPUs to significantly speed up computational operations, which is especially useful
    when working with input data such as images, text, and videos, as we will see
    in later chapters of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us take a quick look at a real-world example. Let’s say we are building
    an automobile recognition system, as illustrated in *Figure 2**.1*. We would begin
    with collecting images of cars of various sizes, shapes, and colors. To train
    our model to recognize these different automobiles, we would transform each image
    into input tensors that encapsulate the height, width, and color channels. When
    we train the model on these input tensors, it learns patterns based on the pixel
    value representations of the cars in our training set. Once the model completes
    the training, we can use the trained model to identify cars of different shapes,
    colors, and sizes. If we now feed the trained model with the image of a car, it
    returns an output tensor that can be decoded into a human-readable format to enable
    us to identify the type of car that it is.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Data representation in TensorFlow](img/B18118_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Data representation in TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: Now that we get the intuition, let’s examine and drill down into more details
    about tensors. We will start by learning a few ways to generate tensors next.
  prefs: []
  type: TYPE_NORMAL
- en: Creating tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a couple of ways we can generate tensors in TensorFlow. However,
    we will focus on creating tensor objects using `tf.constant`, `tf.Variable`, and
    `tf.range`. Recall that we have already imported TensorFlow, NumPy, and data types
    in the section on setting up our working environment. Next, let us run the following
    code to generate our first tensor using `tf.constant`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this code, we generate our first tensor. If all goes well, we will
    get an output that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Excellent! Don’t worry, we will discuss the output and form a clearer picture
    as we proceed. But for now, let us generate a similar tensor object using the
    `tf.Variable` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `a_variable` variable returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the input in both cases is the same, `tf.constant` and `tf.Variable`
    are different. Tensors generated using `tf.constant` cannot be changed, whereas
    `tf.Variable` tensors can be reassigned in the future. We will touch more on this
    shortly as we go further in our exploration of tensors. In the meantime, let us
    look at another way of generating tensors using `tf.range`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`a_range` returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Great! From the output, if we visually compare all three methods used for generating
    tensors, we can easily conclude that the output of `a_constant` and `a_range`
    is the same and is slightly different from the output of `a_variable`. This difference
    becomes clearer when performing tensor operations. To see this in action, let’s
    begin exploring tensor operations, starting with tensor rank.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor rank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are not from a mathematical background, relax. We will cover everything
    together and we won’t be discussing rocket science here – that’s a promise. The
    rank of a tensor identifies the number of dimensions of the tensor. A tensor with
    a rank of `0` is called a scalar, as it has no dimensions. A vector is a rank
    `1` tensor as it has only one dimension, while a matrix of a two-dimension tensor
    has a rank of `2`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Tensor rank](img/B18118_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Tensor rank
  prefs: []
  type: TYPE_NORMAL
- en: 'We have practiced how to generate tensors using three different functions.
    For context, we can safely define a scalar as a quantity that has only magnitude
    but no direction. Examples of scalar quantities are time, mass, energy, and speed;
    these quantities have a single numeric value, for example, `1`, `23.4`, or `50`.
    Let us return to our notebook and generate a scalar using the `tf.constant` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We start by creating a scalar, which is a single value that returns the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: From the returned output, we can see that the shape has no value since the output
    is a scalar quantity with a single numeric output. If we try out a value of `4`,
    the `numpy` output will be `4`, while other output properties will remain the
    same since `4` is still a scalar quantity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have seen what a scalar (rank `0` tensor) is, let us go a step
    further by looking at a vector. For context, a vector quantity has both magnitude
    and direction. Examples of vectors are acceleration, velocity, and force. Let
    us jump back into our notebook and try to generate a vector of four numbers. For
    a change, this time, we will use floats since we can generate tensors with floats.
    Also, if you noticed, the default data type returned has been `int32` for integers,
    which we have previously used to generate tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'From our result, we see the data type returned is `float32` with a shape of
    `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let us generate a matrix. A matrix is an array of numbers listed in rows
    and columns. Let us try out a matrix in our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding matrix is a 2 x 2 matrix, which we can infer by inspecting the
    `shape` output. We see that the data type is also `int32`. Let us generate a higher-dimensional
    tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a 2 x 3 x 2 tensor, with a data type of `int32`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You should play around with some tensors. Try making some tensors with `tf.Variable`
    and see whether you can reproduce our results so far. Next, let us see how we
    can interpret the properties of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have established an understanding of scalars, vectors, and tensors,
    let us explore how to interpret tensor outputs in detail. Previously, we examined
    tensors with a piecemeal approach. Here, we will learn how to identify the key
    properties of a tensor – its rank, shape, and data type – from its printed representation.
    When we print a tensor, it displays the variable name, shape, and data type. Thus
    far, we utilized default arguments when creating tensors. Let us make some adjustments
    to see how this changes the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `tf.``V``ariable` to generate a scalar tensor, selecting `float16`
    as the data type and naming it `TDC`. (If you are wondering what **TDC** means,
    it is the **TensorFlow Developer Certificate**.) Next, we will run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When we examine the output, we can see the name of the tensor is now `TDC:
    0`, and the shape of the tensor is `0` since the tensor is of rank `0`. The data
    type we selected was `float16`. And finally, the tensor has a `numpy` value of
    `1.1` also. This example shows how we can configure properties such as data type
    and name when constructing tensors in TensorFlow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us look at a vector and see what information we can learn from its
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, again, we included arguments and the name of the tensor and we changed
    the default data type. From the output, we can see the result is similar to what
    we got with the scalar quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the tensor has the name `''Vector:0`, the shape has a value of `4` (which
    corresponds to the count of the number of entries), and the tensor has a data
    type of `float16`. To have some fun, you can experiment with different configurations
    and see the impact the changes you make have on the returned output; this is an
    excellent way to learn and understand how things work. When we print the result
    of a tensor output, we can see the different properties of the tensor, like when
    we examined the scalar and vector quantities. However, by leveraging TensorFlow
    functions, we can gain more information about a tensor. Let us start by using
    the `tf.rank()` function to inspect the rank of a scalar, vector, and matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the preceding code to generate a scalar, vector, and matrix. After this,
    we print the rank of each of them using the `tf.rank` function. Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned output is a tensor object that displays the rank of the tensors
    along with the shape and the data type of the tensor. To access the rank of the
    tensor as a numeric value, we have to use `.numpy()` on the returned tensor to
    retrieve the actual rank of the tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'However, an easier way to directly obtain the rank of a tensor without the
    need for reevaluating is by using `ndim`. Let’s see this next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let us proceed by printing out the data type of all three quantities
    using the `dtype` argument to generate the data type for each of our tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: When we run the code, we get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we can see the data types. Next, let us look at
    the shape of our tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'From the results, we can see that the scalar has no shape value while the vector
    has a shape value of one unit, and our matrix has a shape value of two units.
    Next, let us compute the number of elements in each of our tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the scalar has only 1 count since it is a single unit; our vector
    and matrix both have `4` in them and hence they have 4 numeric values in each
    of them. Now, we can confidently use different ways to investigate the properties
    of tensors. Let us proceed to implement basic operations with tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Basic tensor operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now know that TensorFlow is a powerful tool for deep learning. One big hurdle
    with learning TensorFlow is understanding what tensor operations are and why we
    need them. We have established that tensors are fundamental data structures in
    TensorFlow and they can be used to store, manipulate, and analyze data in ML models.
    On the other hand, tensor operations are mathematical operations that can be applied
    to tensors in order to manipulate, decode, or analyze data. These operations range
    from simple operations such as element-wise operations to more complex computations
    performed within the layers of a neural network. Let us look at some tensor operations.
    We will start with changing data types. Then, we will look at indexing and aggregating
    tensors. Finally, we will carry out element-wise operations on tensors, reshaping
    tensors, and matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Changing data types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s say we have a tensor and we want to change the data type from `int32`
    to `float32`, perhaps to accommodate some operation that would require the decimal
    numbers. Fortunately, in TensorFlow, there is a way around this problem. Remember
    that we identified that the default data type for integers is `int32` and for
    decimal numbers, it is `float32`. Let us return to Google Colab and see how we
    can get this done in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We generated a vector of integers, which produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the data type is `int32`. Let us proceed with a data type operation,
    changing the data type to `float32`. We use the `tf.cast()` function and we set
    the data type argument to `float32`. Let us implement this in our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The operation returns a data type of `float32`. We can also see the `numpy`
    array is now an array of decimal numbers and not integers anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You can try it out with `int16` or `float64` and see how it goes. When you are
    done, let us move on with indexing in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by creating a 2 x 2 matrix, which we will use to walk through our
    indexing operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the returned output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'What if we want to extract some information from the matrix? Let’s say we want
    to extract `[1,2]`. How do we go about this? Not to worry: we can apply indexing
    to get the desired information. Let us get it done in our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the returned output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'What if we want to extract value `2` from the matrix? Let us see how we can
    get it done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the returned output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have successfully extracted the value we wanted using indexing. To extract
    all the values in the matrix shown in *Figure 2**.3*, we can use indexing to extract
    the desired element in the 2 x 2 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Matrix indexing](img/B18118_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Matrix indexing
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us look at another example of indexing – this time, using the `tf.slice()`
    function to extract information from a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate a tensor, `c`. Then, we use the `tf.slice` function to slice the
    vector, starting at index `2` with a size or count of `4`. When we run the code,
    we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the result contains values from index `2`, and we take 4 elements
    in the vector to generate our slice. Next, let us look at how to expand the dimension
    of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Remember, in Python, we start counting from 0, not 1.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding a matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We already now know how to check the dimension of the matrix using `ndim`. So,
    let us see how we can expand the dimension of this matrix. We continue using our
    `a` matrix, which is a 2 x 2 matrix, as shown in *Figure 2**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – A 2x2 matrix](img/B18118_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – A 2x2 matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the following code to expand the dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `expand_dims()` function, and the code expands the dimensions of
    the `a` tensor along the `0` axis. This is useful when you want to add a new dimension
    to the tensor – for example, when you want to convert a 2D tensor into a 3D tensor
    (a technique that will be applied in [*Chapter 7*](B18118_07.xhtml#_idTextAnchor146),
    *Image Classification with Convolutional Neural Networks*, where we will work
    on an interesting classic image dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If you take a look at the shape of our output tensor, we can now see it has
    an extra dimension of `1` at the `0` axis. Let us proceed by examining the shape
    of the tensor when we expand across different axes, so we can understand how this
    works better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the code to see how the dimension has expanded across the `0`,
    `1`, and `-1` axes, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In the first line of code, the dimensions of `a` are expanded by 1 on the `0`
    axis. This means that the dimensions of `a` will now be 1 x 2 x 2, adding an extra
    dimension at the beginning of the tensor. The second line of code is expanding
    the dimensions of `a` by 1 on the `1` axis. This means that the dimensions of
    `a` will now be 2 x 1 x 2; here, we are adding an extra dimension in the second
    position of the tensor. The third line of code is expanding the dimensions of
    `a` by 1 on the `-1` axis. This means that the dimensions of `a` will now be 2
    x 2 x 1, thereby adding an extra dimension at the end of our tensor. We have now
    explained how to expand the dimension of a matrix. Next, let us look at tensor
    aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us continue our journey by understanding how to aggregate tensors. We start
    by generating some random numbers by importing the `random` library. Then, we
    generate a range from 1 to 100 in which we generate 50 random numbers. We will
    now use these random numbers to generate a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'When we print `a`, we get the following numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s say we want to find the smallest number in our tensor. It may be difficult
    to manually read through all the numbers and tell me in 5 seconds which is the
    smallest. What if our range of values was up to a thousand or a million? Manually
    checking would take up all our time. Thankfully, in TensorFlow, we can find not
    just the minimum in one strike but we can also find the maximum value, the sum
    of all values, the mean, and much more. Let us do this together in the Colab notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We use these functions to extract the details we require in one click, which
    generates the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have used TensorFlow to extract some important details, we know
    that the smallest value in our vector is 1, the largest value is 99, the sum of
    our vector is 2,273, and the mean value is 45\. Not bad, right? What if we want
    to find the position that holds the minimum and maximum value in a vector? How
    do we go about this?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `tf.argmin` and `tf.argmax` functions to generate the index of the
    lowest value and the index of the highest value, respectively. The output is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'From the result of the `print` statement, we can tell that the lowest value
    is at index `14` and the highest value is at index `44`. If we manually inspect
    the array, we will see that this is true. Also, we can pass the index position
    into the array to get the lowest and highest value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the code, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: There are a few other functions you can try out. The TensorFlow documentation
    gives us a lot to try out and have fun with. Next, let us explore how to transpose
    and reshape tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Transposing and reshaping tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us look at how to transpose and reshape a matrix. First, let’s generate
    a 3 x 4 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the code, we get this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We can reshape the matrix by using `tf.reshape`. Since the matrix has 12 values
    in it, we can use 2 x 2 x 3\. If we multiply the values, we get a total of 12:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also reshape the matrix by changing the `shape` argument in the `tf.reshape`
    function to a 4 x 3 matrix or a 1 x 2 x 6 matrix. You can also try out a few other
    possibilities with regard to reshaping this matrix. Next, let us look at how to
    transpose this matrix using `tf.transpose()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: From the output, we can see that transposing flips the axes. We now have a 4
    x 3 matrix rather than our initial 3 x 4 matrices. Next, let us look at element-wise
    matrix operations.
  prefs: []
  type: TYPE_NORMAL
- en: Element-wise operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by creating a simple vector in Colab:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us display our output so we can see what happens when we perform element-wise
    operations on the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This is our initial output. Now, let us try out a few element-wise operations
    and see what happens next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the results for the addition, subtraction, multiplication, and division
    operations. These operations are carried out on each element in our vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Next, let us look at matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us look at matrix multiplication and see how it works in TensorFlow. We
    return to our notebook in Colab and generate matrix `a`, which is a 3 x 2 matrix,
    and matrix `b`, which is a 2 x 3 matrix. We will use these for our matrix operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let us multiply matrix `a` and `b` and see what our result will look like
    in TensorFlow by using `tf.matmul` in our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `tf.matmul` function for matrix multiplication in TensorFlow. Here,
    we see the output of this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Great! Now, what if we want to multiply matrix `a` by itself? What will our
    result look like? If we tried this out, we will get an error because the shape
    of the matrix does not conform to the rule of matrix multiplication. The rule
    requires that matrix `a` should be made up of *i* rows x *m* columns, and matrix
    `b` should be made up of *m* rows x *n* columns, where the value of *m* must be
    the same in both matrices. The new matrix will have a shape of *i* x *n*, as shown
    in *Figure 2**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Matrix multiplication](img/B18118_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Matrix multiplication
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can see why we cannot multiply matrix `a` by itself, because the number
    of rows in the first matrix must be equal to the number of columns in the second
    matrix. However, we can fulfill the requirement of the matrix multiplication rule
    by either transposing or reshaping matric `a` if we want to multiply `a` by itself.
    Let us try this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'When we transpose matrix `a`, we swap the rows and columns of the matrix based
    on the `perm` parameter, which we set to `[1,0]`. When we execute the `matmul`
    function using `a` and the transpose of `a`, we get a new matrix that complies
    with the rule of matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: How about we try out matrix multiplication using `reshape`? Give this a shot
    and compare your result with our working Colab notebook results. We have looked
    at a whole lot of operations already. How about we build our first model? Let
    us do that next.
  prefs: []
  type: TYPE_NORMAL
- en: Hello World in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot of basic operations in TensorFlow. Now, let’s build our
    first model in TensorFlow. For this example, let us say you are part of a research
    team studying the correlation between the number of hours a student studied in
    a term and their final grade. Of course, this is a theoretical scenario and there
    are a lot more factors that come into play when it comes to how well a student
    will perform. However, in this case, we will take only one attribute as the determinant
    of success – hours of study. After a term of study, we successfully collated the
    hours of study of students and their corresponding grades, as shown in *Table
    2.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '| Hours of Study | 20 | 23 | 25 | 28 | 30 | 37 | 40 | 43 | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| Test Score | 45 | 51 | 55 | 61 | 65 | 79 | 85 | 91 | 97 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Students’ performance table
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want to build a model to predict how well a student will perform in
    the future based on the hours of study they put in. Ready? Let’s do this together
    now:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build this together by opening the accompanying notebook called `hello
    world`. First, we import TensorFlow. Remember in [*Chapter 1*](B18118_01.xhtml#_idTextAnchor014),
    *Introduction to Machine Learning*, we talked about features and labels. Here,
    we just have one feature – hours of study – and our label or target variable is
    the test score. Using the powerful Keras API, in a few lines of code, we will
    build and train a model to get predictions. Let’s get started:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We start by importing TensorFlow and the Keras API; don’t worry about all the
    terms, we will unpack everything in detail in [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065),
    *Linear Regression with TensorFlow*. The goal here is to show you how we build
    a basic model. Don’t worry so much about the technicalities; running the code
    and seeing how it works is the goal here. After importing the necessary libraries,
    we continue with our tradition of printing our TensorFlow version. The code runs
    fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we proceed to import `numpy` for carrying out mathematical operations
    and `matplotlib` for visualizing our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We run the code and we get no errors, so we are good to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up a list of `X` and `y` values representing our hours of study and
    test scores, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get a good sense of data distribution, we use `matplotlib` to visualize
    our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code block plots a graph of `X` (hours of study) against `y` (test score)
    and displays the title (`Exam Performance graph`) of our plot. We use the `show()`
    function to display the graph, as shown in *Figure 2**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Students’ performance plot](img/B18118_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Students’ performance plot
  prefs: []
  type: TYPE_NORMAL
- en: From the plot, we can see the data shows a linear relationship. This assumption
    is not so bad considering we would logically expect a student who works harder
    to score better marks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without getting into a debate about whether this theory holds, let us use the
    Keras API to build a simple model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We build a one-layer model, which we call `study_model`, and we convert our
    list of `X` and `y` values into a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we fit our model and run it for 2,500 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we run the model, it should take less than 5 minutes. We can see that
    the loss drops rapidly initially and gradually flattens out at around 2,000, epochs
    as shown in *Figure 2**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Model loss plot](img/B18118_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Model loss plot
  prefs: []
  type: TYPE_NORMAL
- en: 'And just like that, we have trained a model that can be used to determine how
    a student will perform at the end of a term. This is a very basic task and feels
    like using a hammer on a fly. However, let us try our model out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this code, we generate the result for a student who studied for 38
    hours. Remember our model was not trained on this value. So, let us see what our
    model thinks this student will score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Our model predicted that this student would score 81.07 marks. Good result,
    but how do we know whether our model was right or wrong? If you look at *Figure
    2**.6*, you may guess that our predicted result should be around this score, but
    you may also have figured out that we used *2x + 5 = y* to generate our `y` values.
    If we input `X=38`, we get *2(38) + 5= 81*. Our model did an excellent job of
    getting the correct score with a minute error of .07; however, we had to train
    it for a very long time to achieve this result for a very simple task. In the
    coming chapters, we will learn how to train a much better model using techniques
    such as normalization and, of course, with a larger dataset, where we will work
    with a training set and a validation set and make predictions on a test set. The
    goal here was to get a feel of what is to come, so try out a few numbers to see
    how the model will perform. Do not go above 47, as you will get a score above
    100.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have built our first model, let us look at how to debug and solve
    error messages. This is something you will encounter many times if you decide
    to pursue a career in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging and solving error messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you go through the exercises or walk through the code in this book, in any
    other resource, or in your own personal projects, you will quickly realize how
    often code breaks, and mastering how to resolve these errors will help you to
    move quickly through your learning process or when building projects. First, when
    you get an error, it is important to check what the error message is. Next is
    to understand the meaning of the error message. Let us look at some errors that
    a few students stumbled upon when implementing basic operations in TensorFlow.
    Let’s run the following code to generate a new vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code will throw the error shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Example of an error](img/B18118_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Example of an error
  prefs: []
  type: TYPE_NORMAL
- en: From the error message, we can see that there is no attribute called `variable`
    in TensorFlow. This draws our attention to where the error is coming from and
    we immediately notice that we wrote `variable` instead of `Variable` with a capital
    *V*, as stipulated in the documentation. However, if we are not able to debug
    this ourselves, we can click on the **SEARCH STACK OVERFLOW** button, as this
    is a good place to find solutions to everyday coding problems we might encounter.
    The odds are someone else has faced the same problem and a solution can be found
    on Stack Overflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us click on the link and see what we can find on Stack Overflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Stack Overflow solution for AttributeError](img/B18118_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – Stack Overflow solution for AttributeError
  prefs: []
  type: TYPE_NORMAL
- en: Hurray! On Stack Overflow, we see the solution to the problem and a link to
    the documentation for more details. Remember, it is best to first look at the
    error message and see whether you can resolve it yourself before heading to Stack
    Overflow. If you put this into practice, as well as dedicate time to reading the
    documentation, you will get better and better at debugging issues and make fewer
    mistakes, but you will still need Stack Overflow or the documentation. It comes
    with the terrain. Before we draw the curtains on this chapter, let us summarize
    quickly what we learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the TensorFlow ecosystem at a high level. We looked
    at some of the key components that make TensorFlow the platform of choice for
    building deep learning applications and solutions for many ML engineers, researchers,
    and enthusiasts. Next, we discussed what tensors are and how they are useful in
    our models. After this, we looked at a few ways of creating tensors. We explored
    various tensor properties and we saw how to implement some basic tensor operations
    with TensorFlow. We built a simple model and used it to make predictions. Finally,
    we looked at how to debug and solve error messages in TensorFlow and ML at large.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at regression modeling in a hands-on manner.
    We will learn how to extend our simple model to solve a regression problem for
    a company’s HR department. Also, what you have learned about debugging could prove
    useful in the next chapter – see you there.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s test what we have learned in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are tensors?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a matrix using `tf.``V``ariable` with the `tf.float64` data type and
    name the variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate 15 random numbers between 1 and 20 and extract the lowest number, the
    highest number, the mean, and the index with the lowest and highest numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a 4 x 3 matrix, and multiply the matrix by its transpose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, you can check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amr, T., 2020\. *Hands-On Machine Learning with scikit-learn and Scientific
    Python Toolkits*. [S.l.]: Packt Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow* *guide*: [https://www.TensorFlow.org/guide](https://www.TensorFlow.org/guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
