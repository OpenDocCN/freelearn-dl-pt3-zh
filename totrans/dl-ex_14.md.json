["```\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport pickle as pkl\n\nimport numpy as np\nimport tensorflow as tf\n```", "```\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist_dataset = input_data.read_data_sets('MNIST_data')\n```", "```\nOutput:\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n```", "```\n# Defining the model input for the generator and discrimator\ndef inputs_placeholders(discrimator_real_dim, gen_z_dim):\n    real_discrminator_input = tf.placeholder(tf.float32, (None, discrimator_real_dim), name=\"real_discrminator_input\")\n    generator_inputs_z = tf.placeholder(tf.float32, (None, gen_z_dim), name=\"generator_input_z\")\n\n    return real_discrminator_input, generator_inputs_z\n```", "```\nwith tf.variable_scope('scopeName', reuse=False):\n    # Write your code here\n```", "```\ndef generator(gen_z, gen_out_dim, num_hiddern_units=128, reuse_vars=False, leaky_relu_alpha=0.01):\n\n    ''' Building the generator part of the network\n\n        Function arguments\n        ---------\n        gen_z : the generator input tensor\n        gen_out_dim : the output shape of the generator\n        num_hiddern_units : Number of neurons/units in the hidden layer\n        reuse_vars : Reuse variables with tf.variable_scope\n        leaky_relu_alpha : leaky ReLU parameter\n\n        Function Returns\n        -------\n        tanh_output, logits_layer: \n    '''\n    with tf.variable_scope('generator', reuse=reuse_vars):\n\n        # Defining the generator hidden layer\n        hidden_layer_1 = tf.layers.dense(gen_z, num_hiddern_units, activation=None)\n\n        # Feeding the output of hidden_layer_1 to leaky relu\n        hidden_layer_1 = tf.maximum(hidden_layer_1, leaky_relu_alpha*hidden_layer_1)\n\n        # Getting the logits and tanh layer output\n        logits_layer = tf.layers.dense(hidden_layer_1, gen_out_dim, activation=None)\n        tanh_output = tf.nn.tanh(logits_layer)\n\n        return tanh_output, logits_layer\n```", "```\ndef discriminator(disc_input, num_hiddern_units=128, reuse_vars=False, leaky_relu_alpha=0.01):\n    ''' Building the discriminator part of the network\n\n        Function Arguments\n        ---------\n        disc_input : discrminator input tensor\n        num_hiddern_units : Number of neurons/units in the hidden layer\n        reuse_vars : Reuse variables with tf.variable_scope\n        leaky_relu_alpha : leaky ReLU parameter\n\n        Function Returns\n        -------\n        sigmoid_out, logits_layer: \n    '''\n    with tf.variable_scope('discriminator', reuse=reuse_vars):\n\n        # Defining the generator hidden layer\n        hidden_layer_1 = tf.layers.dense(disc_input, num_hiddern_units, activation=None)\n\n        # Feeding the output of hidden_layer_1 to leaky relu\n        hidden_layer_1 = tf.maximum(hidden_layer_1, leaky_relu_alpha*hidden_layer_1)\n\n        logits_layer = tf.layers.dense(hidden_layer_1, 1, activation=None)\n        sigmoid_out = tf.nn.sigmoid(logits_layer)\n\n        return sigmoid_out, logits_layer\n```", "```\n# size of discriminator input image\n#28 by 28 will flattened to be 784\ninput_img_size = 784 \n\n# size of the generator latent vector\ngen_z_size = 100\n\n# number of hidden units for the generator and discriminator hidden layers\ngen_hidden_size = 128\ndisc_hidden_size = 128\n\n#leaky ReLU alpha parameter which controls the leak of the function\nleaky_relu_alpha = 0.01\n\n# smoothness of the label \nlabel_smooth = 0.1\n```", "```\ntf.reset_default_graph()\n\n# creating the input placeholders for the discrminator and generator\nreal_discrminator_input, generator_input_z = inputs_placeholders(input_img_size, gen_z_size)\n\n#Create the generator network\ngen_model, gen_logits = generator(generator_input_z, input_img_size, gen_hidden_size, reuse_vars=False, leaky_relu_alpha=leaky_relu_alpha)\n\n# gen_model is the output of the generator\n#Create the generator network\ndisc_model_real, disc_logits_real = discriminator(real_discrminator_input, disc_hidden_size, reuse_vars=False, leaky_relu_alpha=leaky_relu_alpha)\ndisc_model_fake, disc_logits_fake = discriminator(gen_model, disc_hidden_size, reuse_vars=True, leaky_relu_alpha=leaky_relu_alpha)\n```", "```\ntf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_layer, labels=labels))\n```", "```\n labels = tf.ones_like(tensor) * (1 - smooth)\n```", "```\n\n# calculating the losses of the discrimnator and generator\ndisc_labels_real = tf.ones_like(disc_logits_real) * (1 - label_smooth)\ndisc_labels_fake = tf.zeros_like(disc_logits_fake)\n\ndisc_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(labels=disc_labels_real, logits=disc_logits_real)\ndisc_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(labels=disc_labels_fake, logits=disc_logits_fake)\n\n#averaging the disc loss\ndisc_loss = tf.reduce_mean(disc_loss_real + disc_loss_fake)\n\n#averaging the gen loss\ngen_loss = tf.reduce_mean(\n    tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=tf.ones_like(disc_logits_fake), \n        logits=disc_logits_fake))\n```", "```\n\n# building the model optimizer\n\nlearning_rate = 0.002\n\n# Getting the trainable_variables of the computational graph, split into Generator and Discrimnator parts\ntrainable_vars = tf.trainable_variables()\ngen_vars = [var for var in trainable_vars if var.name.startswith(\"generator\")]\ndisc_vars = [var for var in trainable_vars if var.name.startswith(\"discriminator\")]\n\ndisc_train_optimizer = tf.train.AdamOptimizer().minimize(disc_loss, var_list=disc_vars)\ngen_train_optimizer = tf.train.AdamOptimizer().minimize(gen_loss, var_list=gen_vars)\n```", "```\ntrain_batch_size = 100\nnum_epochs = 100\ngenerated_samples = []\nmodel_losses = []\n\nsaver = tf.train.Saver(var_list = gen_vars)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for e in range(num_epochs):\n        for ii in range(mnist_dataset.train.num_examples//train_batch_size):\n            input_batch = mnist_dataset.train.next_batch(train_batch_size)\n\n            # Get images, reshape and rescale to pass to D\n            input_batch_images = input_batch[0].reshape((train_batch_size, 784))\n            input_batch_images = input_batch_images*2 - 1\n\n            # Sample random noise for G\n            gen_batch_z = np.random.uniform(-1, 1, size=(train_batch_size, gen_z_size))\n\n            # Run optimizers\n            _ = sess.run(disc_train_optimizer, feed_dict={real_discrminator_input: input_batch_images, generator_input_z: gen_batch_z})\n            _ = sess.run(gen_train_optimizer, feed_dict={generator_input_z: gen_batch_z})\n\n        # At the end of each epoch, get the losses and print them out\n        train_loss_disc = sess.run(disc_loss, {generator_input_z: gen_batch_z, real_discrminator_input: input_batch_images})\n        train_loss_gen = gen_loss.eval({generator_input_z: gen_batch_z})\n\n        print(\"Epoch {}/{}...\".format(e+1, num_epochs),\n              \"Disc Loss: {:.3f}...\".format(train_loss_disc),\n              \"Gen Loss: {:.3f}\".format(train_loss_gen)) \n\n        # Save losses to view after training\n        model_losses.append((train_loss_disc, train_loss_gen))\n\n        # Sample from generator as we're training for viegenerator_inputs_zwing afterwards\n        gen_sample_z = np.random.uniform(-1, 1, size=(16, gen_z_size))\n        generator_samples = sess.run(\n                       generator(generator_input_z, input_img_size, reuse_vars=True),\n                       feed_dict={generator_input_z: gen_sample_z})\n\n        generated_samples.append(generator_samples)\n        saver.save(sess, './checkpoints/generator_ck.ckpt')\n\n# Save training generator samples\nwith open('train_generator_samples.pkl', 'wb') as f:\n    pkl.dump(generated_samples, f)\n```", "```\nOutput:\n.\n.\n.\nEpoch 71/100... Disc Loss: 1.078... Gen Loss: 1.361\nEpoch 72/100... Disc Loss: 1.037... Gen Loss: 1.555\nEpoch 73/100... Disc Loss: 1.194... Gen Loss: 1.297\nEpoch 74/100... Disc Loss: 1.120... Gen Loss: 1.730\nEpoch 75/100... Disc Loss: 1.184... Gen Loss: 1.425\nEpoch 76/100... Disc Loss: 1.054... Gen Loss: 1.534\nEpoch 77/100... Disc Loss: 1.457... Gen Loss: 0.971\nEpoch 78/100... Disc Loss: 0.973... Gen Loss: 1.688\nEpoch 79/100... Disc Loss: 1.324... Gen Loss: 1.370\nEpoch 80/100... Disc Loss: 1.178... Gen Loss: 1.710\nEpoch 81/100... Disc Loss: 1.070... Gen Loss: 1.649\nEpoch 82/100... Disc Loss: 1.070... Gen Loss: 1.530\nEpoch 83/100... Disc Loss: 1.117... Gen Loss: 1.705\nEpoch 84/100... Disc Loss: 1.042... Gen Loss: 2.210\nEpoch 85/100... Disc Loss: 1.152... Gen Loss: 1.260\nEpoch 86/100... Disc Loss: 1.327... Gen Loss: 1.312\nEpoch 87/100... Disc Loss: 1.069... Gen Loss: 1.759\nEpoch 88/100... Disc Loss: 1.001... Gen Loss: 1.400\nEpoch 89/100... Disc Loss: 1.215... Gen Loss: 1.448\nEpoch 90/100... Disc Loss: 1.108... Gen Loss: 1.342\nEpoch 91/100... Disc Loss: 1.227... Gen Loss: 1.468\nEpoch 92/100... Disc Loss: 1.190... Gen Loss: 1.328\nEpoch 93/100... Disc Loss: 0.869... Gen Loss: 1.857\nEpoch 94/100... Disc Loss: 0.946... Gen Loss: 1.740\nEpoch 95/100... Disc Loss: 0.925... Gen Loss: 1.708\nEpoch 96/100... Disc Loss: 1.067... Gen Loss: 1.427\nEpoch 97/100... Disc Loss: 1.099... Gen Loss: 1.573\nEpoch 98/100... Disc Loss: 0.972... Gen Loss: 1.884\nEpoch 99/100... Disc Loss: 1.292... Gen Loss: 1.610\nEpoch 100/100... Disc Loss: 1.103... Gen Loss: 1.736\n```", "```\nfig, ax = plt.subplots()\nmodel_losses = np.array(model_losses)\nplt.plot(model_losses.T[0], label='Disc loss')\nplt.plot(model_losses.T[1], label='Gen loss')\nplt.title(\"Model Losses\")\nplt.legend()\n```", "```\ndef view_generated_samples(epoch_num, g_samples):\n    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n\n    print(gen_samples[epoch_num][1].shape)\n\n    for ax, gen_image in zip(axes.flatten(), g_samples[0][epoch_num]):\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        img = ax.imshow(gen_image.reshape((28,28)), cmap='Greys_r')\n\n    return fig, axes\n\n```", "```\n# Load samples from generator taken while training\nwith open('train_generator_samples.pkl', 'rb') as f:\n    gen_samples = pkl.load(f)\n```", "```\n_ = view_generated_samples(-1, gen_samples)\n```", "```\nrows, cols = 10, 6\nfig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n\nfor gen_sample, ax_row in zip(gen_samples[::int(len(gen_samples)/rows)], axes):\n    for image, ax in zip(gen_sample[::int(len(gen_sample)/cols)], ax_row):\n        ax.imshow(image.reshape((28,28)), cmap='Greys_r')\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n```", "```\n# Sampling from the generator\nsaver = tf.train.Saver(var_list=g_vars)\n\nwith tf.Session() as sess:\n\n    #restoring the saved checkpints\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    gen_sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n    generated_samples = sess.run(\n                   generator(generator_input_z, input_img_size, reuse_vars=True),\n                   feed_dict={generator_input_z: gen_sample_z})\nview_generated_samples(0, [generated_samples])\n```"]