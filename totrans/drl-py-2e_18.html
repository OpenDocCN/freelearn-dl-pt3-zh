<html><head></head><body>
  <div id="_idContainer3284">
    <h1 id="_idParaDest-464" class="chapterTitle">Appendix 1 – Reinforcement Learning Algorithms </h1>
    <p class="normal">Let's have a look at all the reinforcement learning algorithms we have learned about in this book. </p>
    <h1 id="_idParaDest-465" class="title">Reinforcement learning algorithm</h1>
    <p class="normal">The steps <a id="_idIndexMarker1541"/>involved in a typical reinforcement learning algorithm are given as follows:</p>
    <ol>
      <li class="numbered">First, the agent interacts with the environment by performing an action.</li>
      <li class="numbered">The agent performs an action and moves from one state to another.</li>
      <li class="numbered">Then the agent will receive a reward based on the action it performed.</li>
      <li class="numbered">Based on the reward, the agent will understand whether the action is good or bad.</li>
      <li class="numbered">If the action was good, that is, if the agent received a positive reward, then the agent will prefer performing that action, else the agent will try performing other actions that can result in a positive reward. So reinforcement learning is basically a trial-and-error learning process.</li>
    </ol>
    <h1 id="_idParaDest-466" class="title">Value Iteration</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1542"/>of value iteration is given as follows:</p>
    <ol>
      <li class="numbered">Compute the optimal value function by taking maximum over the Q function, that is, <img src="../Images/B15558_18_001.png" alt="" style="height: 1.58em;"/></li>
      <li class="numbered">Extract the optimal policy from the computed optimal value function</li>
    </ol>
    <h1 id="_idParaDest-467" class="title">Policy Iteration</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1543"/>of policy iteration is given as follows:</p>
    <ol>
      <li class="numbered">Initialize a random policy </li>
      <li class="numbered">Compute the value function using the given policy </li>
      <li class="numbered">Extract a new policy using the value function obtained from <em class="italic">step 2</em></li>
      <li class="numbered">If the extracted policy is the same as the policy used in <em class="italic">step 2</em> then stop, else send the extracted new policy to <em class="italic">step 2</em> and repeat <em class="italic">steps 2</em> to <em class="italic">4</em></li>
    </ol>
    <h1 id="_idParaDest-468" class="title">First-Visit MC Prediction</h1>
    <p class="normal">The <a id="_idIndexMarker1544"/>algorithm of first-visit MC prediction is given as follows: </p>
    <ol>
      <li class="numbered">Let total_return(<em class="italic">s</em>) be the sum of the return of a state across several episodes and <em class="italic">N</em>(<em class="italic">s</em>) be the counter, that is, the number of times a state is visited across several episodes. Initialize total_return(<em class="italic">s</em>) and <em class="italic">N</em>(<em class="italic">s</em>) as zero for all the states. The policy <img src="../Images/B15558_18_002.png" alt="" style="height: 0.84em;"/> is given as input.</li>
      <li class="numbered">For <em class="italic">M</em> number of iterations: <ol>
          <li class="numbered-l2">Generate an episode using the policy <img src="../Images/B15558_04_054.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all rewards obtained in the episode in a list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:</li>
        </ol>
        <p class="bullet-para">If the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> is occurring for the first time in the episode:</p>
        <ol>
          <li class="numbered-l2">Compute the return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
          <li class="numbered-l2">Update the total return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>)</li>
          <li class="numbered-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
        </ol>
      </li>
      <li class="numbered">Compute the value of a state by just taking the average, that is:<figure class="mediaobject"><img src="../Images/B15558_04_056.png" alt="" style="height: 2.51em;"/></figure>
      </li>
    </ol>
    <h1 id="_idParaDest-469" class="title">Every-Visit MC Prediction</h1>
    <p class="normal">The <a id="_idIndexMarker1545"/>algorithm of every-visit MC prediction is given as follows:</p>
    <ol>
      <li class="numbered">Let total_return(<em class="italic">s</em>) be the sum of the return of a state across several episodes and <em class="italic">N</em>(<em class="italic">s</em>) be the counter, that is, the number of times a state is visited across several episodes. Initialize total_return(<em class="italic">s</em>) and <em class="italic">N</em>(<em class="italic">s</em>) as zero for all the states. The policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/> is given as input.</li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using the policy <img src="../Images/B15558_14_001.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:
        <ol>
          <li class="numbered-l2">Compute the return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
          <li class="numbered-l2">Update the total return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>)</li>
          <li class="numbered-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
        </ol>
      </li></ol>
</li>
      <li class="numbered">Compute the value of a state by just taking the average, that is:<figure class="mediaobject"><img src="../Images/B15558_04_056.png" alt="" style="height: 2.51em;"/></figure>
      </li>
    </ol>
    <h1 id="_idParaDest-470" class="title">MC Prediction – the Q Function</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1546"/>for MC prediction of the Q function is given <a id="_idIndexMarker1547"/>as follows:</p>
    <ol>
      <li class="numbered">Let total_return(<em class="italic">s</em>, <em class="italic">a</em>) be the sum of the return of a state-action pair across several episodes and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) be the number of times a state-action pair is visited across several episodes. Initialize total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) for all state-action pairs to zero. The policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/> is given as input. </li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:
        <ol>
          <li class="numbered-l2">Compute the return for the state-action pair, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
          <li class="numbered-l2">Update the total return of the state-action pair, total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>)</li>
          <li class="numbered-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
        </ol>
      </li></ol>
</li>
      <li class="numbered"> Compute <a id="_idIndexMarker1548"/>the Q function (Q value) by just taking the average, that is:<figure class="mediaobject"><img src="../Images/B15558_04_067.png" alt="" style="height: 2.51em;"/></figure>
      </li>
    </ol>
    <h1 id="_idParaDest-471" class="title">MC Control Method</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1549"/>for the MC control method is given as follows:</p>
    <ol>
      <li class="numbered">Let total_return(<em class="italic">s</em>, <em class="italic">a</em>) be the sum of the return of a state-action pair across several episodes and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) be the number of times a state-action pair is visited across several episodes. Initialize total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) for all state-action pairs to zero and initialize a random policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/>. </li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using policy <img src="../Images/B15558_04_032.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode: 
        <p class="bullet-para">If (<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) is occurring for the first time in the episode:</p>
        <ol>
          <li class="numbered-l2">Compute the return of a state-action pair,<em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
          <li class="numbered-l2">Update the total return of the state-action pair as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>)</li>
          <li class="numbered-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
          <li class="numbered-l2">Compute the Q value by just taking the average, that is, <img src="../Images/B15558_18_013.png" alt="" style="height: 2.51em;"/></li>
        </ol></li>
          <li class="numbered-l2">Compute <a id="_idIndexMarker1550"/>the new updated policy from <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/> using the Q function:
        <figure class="mediaobject"><img src="../Images/B15558_18_015.png" alt="" style="height: 1.49em;"/></figure>
      </li></ol></li>
    </ol>
    <h1 id="_idParaDest-472" class="title">On-Policy MC Control – Exploring starts</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1551"/>for on-policy MC control by exploring <a id="_idIndexMarker1552"/>the starts method is given as follows:</p>
    <ol>
      <li class="numbered">Let total_return(<em class="italic">s</em>, <em class="italic">a</em>) be the sum of the return of a state-action pair across several episodes and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) be the number of times a state-action pair is visited across several episodes. Initialize total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) for all state-action pairs to zero and initialize a random policy <img src="../Images/B15558_04_099.png" alt="" style="height: 0.84em;"/>.</li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Select the initial state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and initial action <em class="italic">a</em><sub class="Subscript--PACKT-">0</sub> randomly such that all state-action pairs have a probability greater than 0</li>
          <li class="numbered-l2">Generate an episode from the selected initial state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and action <em class="italic">a</em><sub class="Subscript--PACKT-">0</sub> using policy <img src="../Images/B15558_04_099.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:
        <p class="bullet-para">If (<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) is occurring for the first time in the episode:</p>
        <ol>
          <li class="numbered-l2">Compute the return of a state-action pair, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
          <li class="numbered-l2">Update the total return of the state-action pair as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>)</li>
          <li class="numbered-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
          <li class="numbered-l2">Compute the Q value by just taking the average, that is,<img src="../Images/B15558_18_013.png" alt="" style="height: 2.51em;"/></li>
          <li class="numbered-l2">Compute <a id="_idIndexMarker1553"/>the updated policy from <img src="../Images/B15558_04_099.png" alt="" style="height: 0.84em;"/> using <a id="_idIndexMarker1554"/>the Q function:
        <figure class="mediaobject"><img src="../Images/B15558_18_020.png" alt="" style="height: 1.49em;"/></figure></li>
        </ol>
      </li>
    </ol> </li>
        </ol>
    <h1 id="_idParaDest-473" class="title">On-Policy MC Control – Epsilon-Greedy</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1555"/>for on-policy <a id="_idIndexMarker1556"/>MC control with the epsilon-greedy policy is given as follows:</p>
    <ol>
      <li class="numbered">Let total_return(<em class="italic">s</em>, <em class="italic">a</em>) be the sum of the return of a state-action pair across several episodes and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) be the number of times a state-action pair is visited across several episodes. Initialize total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) for all state-action pairs to zero and initialize a random policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/>.</li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:
        <p class="bullet-para">If (<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) is occurring for the first time in the episode:</p>
        <ol>
          <li class="numbered-l2">Compute the return of a state-action pair, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:]).</li>
          <li class="numbered-l2">Update the total return of the state-action pair as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>).</li>
          <li class="numbered-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1.</li>
          <li class="numbered-l2">Compute the Q value by just taking the average, that is,<img src="../Images/B15558_18_013.png" alt="" style="height: 2.51em;"/></li>
        </ol>
        </li>
          <li class="numbered-l2">Compute <a id="_idIndexMarker1557"/>the updated policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/> using the Q function. Let <img src="../Images/B15558_14_185.png" alt="" style="height: 1.49em;"/>. The policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/> selects the best action <img src="../Images/B15558_18_027.png" alt="" style="height: 1.11em;"/> with probability <img src="../Images/B15558_04_123.png" alt="" style="height: 1.11em;"/>, and a <a id="_idIndexMarker1558"/>random action with probability <img src="../Images/B15558_13_276.png" alt="" style="height: 0.93em;"/>.</li>
        </ol></li></ol>

    <h1 id="_idParaDest-474" class="title">Off-Policy MC Control</h1>
    <p class="normal">The <a id="_idIndexMarker1559"/>algorithm for the off-policy MC control method is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the Q function <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) with random values and set the behavior policy <em class="italic">b</em> to be epsilon-greedy, set the target policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> to be greedy policy and initialize the cumulative weights as <em class="italic">C</em>(<em class="italic">s</em>, <em class="italic">a</em>) = 0</li>
      <li class="numbered">For <em class="italic">M</em> number of episodes:<ol>
          <li class="numbered-l2">Generate an episode using the behavior policy <em class="italic">b</em></li>
          <li class="numbered-l2">Initialize return <em class="italic">R</em> to 0 and weight <em class="italic">W</em> to 1</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode, <em class="italic">t</em> = <em class="italic">T</em> – 1, <em class="italic">T</em> – 2, . . . , 0:
        <ol>
          <li class="numbered-l2">Compute the return as <em class="italic">R</em> = <em class="italic">R</em> + <em class="italic">r</em><sub class="" style="font-style: italic;">t+1</sub></li>
          <li class="numbered-l2">Update the cumulative weights to <em class="italic">C</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">C</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) +<em class="italic">W</em></li>
          <li class="numbered-l2">Update the Q value to <img src="../Images/B15558_18_031.png" alt="" style="height: 2.4em;"/></li>
          <li class="numbered-l2">Compute the target policy <img src="../Images/B15558_18_032.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">If <img src="../Images/B15558_18_033.png" alt="" style="height: 1.11em;"/> then break</li>
          <li class="numbered-l2">Update the weight to <img src="../Images/B15558_18_034.png" alt="" style="height: 2.4em;"/></li>
        </ol></li>
        </ol>
      </li>
      <li class="numbered">Return <a id="_idIndexMarker1560"/>the target policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/></li>
    </ol>
    <h1 id="_idParaDest-475" class="title">TD Prediction</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1561"/>for the TD prediction method is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the value function <em class="italic">V</em>(<em class="italic">s</em>) with random values. A policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/> is given.</li>
      <li class="numbered">For each episode:<ol>
          <li class="numbered-l2">Initialize the state <em class="italic">s</em></li>
          <li class="numbered-l2">For each step in the episode:
        <ol>
          <li class="numbered-l2">Perform an action <em class="italic">a</em> in the state <em class="italic">s</em> according to given policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/>, get the reward <em class="italic">r</em>, and move to the next state <img src="../Images/B15558_12_264.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update the value of the state to <img src="../Images/B15558_18_039.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update <img src="../Images/B15558_18_040.png" alt="" style="height: 1.2em;"/> (this step implies we are changing the next state <img src="../Images/B15558_18_041.png" alt="" style="height: 1.2em;"/> to the current state <em class="italic">s</em>)</li>
          <li class="numbered-l2">If <em class="italic">s</em> is not <a id="_idIndexMarker1562"/>a terminal state, repeat <em class="italic">steps 1</em> to <em class="italic">4</em></li>
        </ol></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-476" class="title">On-Policy TD Control – SARSA</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1563"/>for on-policy TD control – SARSA is <a id="_idIndexMarker1564"/>given as follows:</p>
    <ol>
      <li class="numbered">Initialize the Q function <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) with random values</li>
      <li class="numbered">For each episode:<ol>
          <li class="numbered-l2">Initialize the state <em class="italic">s</em></li>
          <li class="numbered-l2">Extract a policy from <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) and select an action <em class="italic">a</em> to perform in the state <em class="italic">s</em></li>
          <li class="numbered-l2">For each step in the episode:
        <ol>
          <li class="numbered-l2">Perform the action <em class="italic">a</em>, move to the new state <img src="../Images/B15558_18_041.png" alt="" style="height: 1.2em;"/>, and observe the reward <em class="italic">r</em></li>
          <li class="numbered-l2">In the state <img src="../Images/B15558_18_043.png" alt="" style="height: 1.2em;"/>, select the action <img src="../Images/B15558_18_044.png" alt="" style="height: 1.2em;"/> using the epsilon-greedy policy</li>
          <li class="numbered-l2">Update the Q value to <img src="../Images/B15558_18_045.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update <img src="../Images/B15558_18_046.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_18_047.png" alt="" style="height: 1.2em;"/> (update the next state <img src="../Images/B15558_03_021.png" alt="" style="height: 1.2em;"/>-action <img src="../Images/B15558_18_049.png" alt="" style="height: 1.2em;"/> pair to the current state <em class="italic">s</em>-action <em class="italic">a</em> pair)</li>
          <li class="numbered-l2">If <em class="italic">s</em> is not the terminal state, repeat <em class="italic">steps 1</em> to <em class="italic">5</em> </li>
        </ol></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-477" class="title">Off-Policy TD Control – Q Learning</h1>
    <p class="normal">The <a id="_idIndexMarker1565"/>algorithm for off-policy TD control – Q learning <a id="_idIndexMarker1566"/>is given as follows:</p>
    <ol>
      <li class="numbered">Initialize a Q function <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) with random values</li>
      <li class="numbered">For each episode:<ol>
          <li class="numbered-l2">Initialize the state <em class="italic">s</em></li>
          <li class="numbered-l2">For each step in the episode:
        <ol>
          <li class="numbered-l2">Extract a policy from <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) and select an action <em class="italic">a</em> to perform in the state <em class="italic">s</em></li>
          <li class="numbered-l2">Perform the action <em class="italic">a</em>, move to the new state <img src="../Images/B15558_18_050.png" alt="" style="height: 1.2em;"/>, and observe the reward <em class="italic">r</em></li>
          <li class="numbered-l2">Update the Q value to <img src="../Images/B15558_18_051.png" alt="" style="height: 1.58em;"/></li>
          <li class="numbered-l2">Update <img src="../Images/B15558_18_052.png" alt="" style="height: 1.2em;"/> (update the next state <img src="../Images/B15558_18_053.png" alt="" style="height: 1.2em;"/> to the current state <em class="italic">s</em>)</li>
          <li class="numbered-l2">If <em class="italic">s</em> is not a terminal state, repeat <em class="italic">steps 1</em> to <em class="italic">5</em></li>
        </ol></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-478" class="title">Deep Q Learning</h1>
    <p class="normal">The <a id="_idIndexMarker1567"/>algorithm for deep Q learning is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the main network parameter <img src="../Images/B15558_10_037.png" alt="" style="height: 1.11em;"/> with random values</li>
      <li class="numbered">Initialize the target network parameter <img src="../Images/B15558_09_061.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> </li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, perform <em class="italic">step 5</em></li>
      <li class="numbered">For <a id="_idIndexMarker1568"/>each step in the episode, that is, for <em class="italic">t</em> = 0, . . ., <em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Observe the state <em class="italic">s</em> and select an action using the epsilon-greedy policy, that is, with probability epsilon, select random action <em class="italic">a</em>, and with probability 1-epsilon, select the action as <img src="../Images/B15558_09_072.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">Perform the selected action and move to the next state <img src="../Images/B15558_14_036.png" alt="" style="height: 1.2em;"/> and obtain the reward <em class="italic">r</em></li>
          <li class="numbered-l2">Store the transition information in the replay buffer <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the target value, that is, <img src="../Images/B15558_18_062.png" alt="" style="height: 1.58em;"/></li>
          <li class="numbered-l2">Compute the loss, <img src="../Images/B15558_09_035.png" alt="" style="height: 3.33em;"/></li>
          <li class="numbered-l2">Compute the gradients of the loss and update the main network parameter <img src="../Images/B15558_12_330.png" alt="" style="height: 1.11em;"/> using gradient descent, <img src="../Images/B15558_18_065.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Freeze the target network parameter <img src="../Images/B15558_18_066.png" alt="" style="height: 1.2em;"/> for several time steps and then update it by just copying the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-479" class="title">Double DQN</h1>
    <p class="normal">The <a id="_idIndexMarker1569"/>algorithm for double DQN is given as follows: </p>
    <ol>
      <li class="numbered">Initialize the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> with random values</li>
      <li class="numbered">Initialize the target network parameter <img src="../Images/B15558_09_059.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat <em class="italic">step 5</em></li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0, . . ., <em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Observe the state <em class="italic">s</em> and select an action using the epsilon-greedy policy, that is, with probability epsilon, select random action <em class="italic">a</em> with probability 1-epsilon, and select the action as <img src="../Images/B15558_18_072.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">Perform the selected action and move to the next state <img src="../Images/B15558_18_073.png" alt="" style="height: 1.2em;"/> and obtain the reward <em class="italic">r</em></li>
          <li class="numbered-l2">Store the transition information in the replay buffer <img src="../Images/B15558_18_074.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_14_098.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the target value, that is, <img src="../Images/B15558_18_076.png" alt="" style="height: 1.58em;"/></li>
          <li class="numbered-l2">Compute the loss, <img src="../Images/B15558_09_035.png" alt="" style="height: 3.33em;"/></li>
          <li class="numbered-l2">Compute the gradients of the loss and update the main network parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/> using gradient descent: <img src="../Images/B15558_18_079.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Freeze the target network parameter <img src="../Images/B15558_09_061.png" alt="" style="height: 1.2em;"/> for several time steps and then update it by just copying the main network parameter <img src="../Images/B15558_10_066.png" alt="" style="height: 1.11em;"/></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-480" class="title">REINFORCE Policy Gradient</h1>
    <p class="normal">The <a id="_idIndexMarker1570"/>algorithm for REINFORCE policy gradient is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the network parameter <img src="../Images/B15558_15_152.png" alt="" style="height: 1.11em;"/> with random values</li>
      <li class="numbered">Generate some <em class="italic">N</em> number of trajectories <img src="../Images/B15558_10_058.png" alt="" style="height: 1.67em;"/> following the policy <img src="../Images/B15558_13_124.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">Compute the return of the trajectory <img src="../Images/B15558_18_085.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Compute the gradients <img src="../Images/B15558_18_086.png" alt="" style="height: 3.33em;"/></li>
      <li class="numbered">Update the network parameter, <img src="../Images/B15558_11_005.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to <em class="italic">5</em> for several iterations</li>
    </ol>
    <h1 id="_idParaDest-481" class="title">Policy Gradient with Reward-To-Go</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1571"/>for policy gradient with reward-to-go is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> with random values</li>
      <li class="numbered">Generate some <em class="italic">N</em> number of trajectories <img src="../Images/B15558_10_058.png" alt="" style="height: 1.67em;"/> following the policy <img src="../Images/B15558_10_111.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">Compute the return (reward-to-go) <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub></li>
      <li class="numbered">Compute the gradients <img src="../Images/B15558_10_128.png" alt="" style="height: 3.33em;"/></li>
      <li class="numbered">Update the network parameter: <img src="../Images/B15558_18_092.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to <em class="italic">5</em> for several iterations</li>
    </ol>
    <h1 id="_idParaDest-482" class="title">REINFORCE with Baseline</h1>
    <p class="normal">The <a id="_idIndexMarker1572"/>algorithm for REINFORCE with baseline is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the policy network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> and value network parameter <img src="../Images/B15558_13_234.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Generate some <em class="italic">N</em> number of trajectories <img src="../Images/B15558_10_058.png" alt="" style="height: 1.67em;"/> following the policy <img src="../Images/B15558_13_124.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">Compute the return (reward-to-go) <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub></li>
      <li class="numbered">Compute the policy gradient, <img src="../Images/B15558_10_163.png" alt="" style="height: 3.33em;"/></li>
      <li class="numbered">Update the policy network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> using gradient ascent, <img src="../Images/B15558_18_099.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Compute the mean squared error of the value network, <img src="../Images/B15558_18_100.png" alt="" style="height: 3.33em;"/></li>
      <li class="numbered">Update <a id="_idIndexMarker1573"/>the value network parameter <img src="../Images/B15558_12_213.png" alt="" style="height: 1.11em;"/> using gradient descent, <img src="../Images/B15558_11_013.png" alt="" style="height: 1.29em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to <em class="italic">7</em> for several iterations</li>
    </ol>
    <h1 id="_idParaDest-483" class="title">Advantage Actor Critic</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1574"/>for the advantage actor critic method is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the actor network parameter <img src="../Images/B15558_10_037.png" alt="" style="height: 1.11em;"/> and critic network parameter <img src="../Images/B15558_10_148.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat <em class="italic">step 3</em></li>
      <li class="numbered">For each step in the episode, that is, for, <em class="italic">t</em> = 0, . . ., <em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Select an action using the policy <img src="../Images/B15558_11_017.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Take the action <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub> in the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, and observe the reward <em class="italic">r</em> and move to the next state <img src="../Images/B15558_18_106.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Compute the policy gradients: <img src="../Images/B15558_11_009.png" alt="" style="height: 1.4em;"/></li>
          <li class="numbered-l2">Update the actor network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> using gradient ascent: <img src="../Images/B15558_11_005.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the loss of the critic network: <img src="../Images/B15558_18_110.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Compute gradients <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> and update the critic network parameter <img src="../Images/B15558_12_371.png" alt="" style="height: 1.11em;"/> using gradient descent: <img src="../Images/B15558_11_013.png" alt="" style="height: 1.29em;"/></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-484" class="title">Asynchronous Advantage Actor-Critic</h1>
    <p class="normal">The steps <a id="_idIndexMarker1575"/>involved in <strong class="keyword">Advantage Actor-Critic</strong> (<strong class="keyword">A3C</strong>) are <a id="_idIndexMarker1576"/>given below:</p>
    <ol>
      <li class="numbered">The worker agent interacts with its own copies of the environment</li>
      <li class="numbered">Each worker follows a different policy and collects the experience</li>
      <li class="numbered">Next, the worker agents compute the loss of the actor and critic networks</li>
      <li class="numbered">After computing the loss, they calculates the gradients of the loss and sends those gradients to the global agent asynchronously</li>
      <li class="numbered">The global agent updates its parameter with the gradients received from the worker agents</li>
      <li class="numbered">Now, the <a id="_idIndexMarker1577"/>updated parameter from the global agent will be sent to the worker agents periodically</li>
    </ol>
    <h1 id="_idParaDest-485" class="title">Deep Deterministic Policy Gradient</h1>
    <p class="normal">The <a id="_idIndexMarker1578"/>algorithm for <strong class="keyword">Deep Deterministic Policy Gradient</strong> (<strong class="keyword">DDPG</strong>) is given as follows:</p>
    <ol>
      <li class="numbered">Initialize <a id="_idIndexMarker1579"/>the main critic network parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/> and main actor network parameter <img src="../Images/B15558_13_234.png" alt="" style="height: 1.11em;"/> </li>
      <li class="numbered">Initialize the target critic network parameter <img src="../Images/B15558_14_246.png" alt="" style="height: 1.2em;"/> by just copying the main critic network parameter <img src="../Images/B15558_09_123.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the target actor network parameter <img src="../Images/B15558_18_117.png" alt="" style="height: 1.2em;"/> by just copying the main actor network parameter <img src="../Images/B15558_13_234.png" alt="" style="height: 1.11em;"/>.</li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat <em class="italic">steps 6</em> to <em class="italic">7</em></li>
      <li class="numbered">Initialize an Ornstein-Uhlenbeck random process <img src="../Images/B15558_18_120.png" alt="" style="height: 1.11em;"/> for action space exploration</li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0, . . ., <em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Select action <em class="italic">a</em> based on the policy <img src="../Images/B15558_18_121.png" alt="" style="height: 1.2em;"/> and exploration noise, that is, <img src="../Images/B15558_12_061.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Perform the selected action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_14_105.png" alt="" style="height: 1.2em;"/> and get the reward <em class="italic">r</em>, and store this transition information in the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the target value of the critic, that is, <img src="../Images/B15558_12_095.png" alt="" style="height: 1.76em;"/></li>
          <li class="numbered-l2">Compute the loss of the critic network <img src="../Images/B15558_12_047.png" alt="" style="height: 2.87em;"/></li>
          <li class="numbered-l2">Compute the gradient of the loss <img src="../Images/B15558_10_028.png" alt="" style="height: 1.11em;"/> and update the critic network parameter using gradient descent, <img src="../Images/B15558_18_129.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the gradient of the actor network <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> and update the actor network parameter using gradient ascent, <img src="../Images/B15558_18_131.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update the target critic and target actor network parameters, <img src="../Images/B15558_18_132.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_18_133.png" alt="" style="height: 1.2em;"/></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-486" class="title">Twin Delayed DDPG</h1>
    <p class="normal">The <a id="_idIndexMarker1580"/>algorithm for <strong class="keyword">Twin Delayed DDPG</strong> (<strong class="keyword">TD3</strong>) is given <a id="_idIndexMarker1581"/>as follows: </p>
    <ol>
      <li class="numbered">Initialize two main critic networks parameters, <img src="../Images/B15558_12_211.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/>, and the main actor network parameter <img src="../Images/B15558_12_213.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize two target critic networks parameters, <img src="../Images/B15558_12_214.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_320.png" alt="" style="height: 1.2em;"/>, by copying the main critic network parameters <img src="../Images/B15558_18_139.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/>, respectively</li>
      <li class="numbered">Initialize the target actor network parameter <img src="../Images/B15558_18_141.png" alt="" style="height: 1.2em;"/> by copying the main actor network parameter <img src="../Images/B15558_12_213.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_18_143.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat <em class="italic">step 6</em></li>
      <li class="numbered">For <a id="_idIndexMarker1582"/>each step in the episode, that is, for <em class="italic">t</em> = 0, . . ., <em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Select action <em class="italic">a</em> based on the policy <img src="../Images/B15558_18_121.png" alt="" style="height: 1.2em;"/> and with exploration noise <img src="../Images/B15558_18_145.png" alt="" style="height: 0.93em;"/>, that is, <img src="../Images/B15558_12_224.png" alt="" style="height: 1.29em;"/> where, <img src="../Images/B15558_18_147.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Perform the selected action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_18_043.png" alt="" style="height: 1.2em;"/>, get the reward <em class="italic">r</em>, and store the transition information in the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_12_374.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Select the action <img src="../Images/B15558_18_151.png" alt="" style="height: 1.11em;"/> for computing the target value <img src="../Images/B15558_18_152.png" alt="" style="height: 1.29em;"/> where <img src="../Images/B15558_12_269.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the target value of the critic, that is, <img src="../Images/B15558_12_230.png" alt="" style="height: 1.67em;"/></li>
          <li class="numbered-l2">Compute the loss of the critic network <img src="../Images/B15558_12_227.png" alt="" style="height: 2.96em;"/>.</li>
          <li class="numbered-l2">Compute the gradients of the loss <img src="../Images/B15558_12_234.png" alt="" style="height: 1.29em;"/> and minimize the loss using gradient descent, <img src="../Images/B15558_12_235.png" alt="" style="height: 1.49em;"/> </li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <em class="italic">d</em> =0, then:<ol>
              <li class="numbered-l2">Compute the gradient of the objective function <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> and update the actor network parameter using gradient ascent, <img src="../Images/B15558_18_131.png" alt="" style="height: 1.2em;"/></li>
              <li class="numbered-l2">Update the target critic networks parameter and target actor network parameter as <img src="../Images/B15558_18_132.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_18_161.png" alt="" style="height: 1.2em;"/>, respectively</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-487" class="title">Soft Actor-Critic </h1>
    <p class="normal">The <a id="_idIndexMarker1583"/>algorithm for <strong class="keyword">Soft Actor-Critic</strong> (<strong class="keyword">SAC</strong>) is given as follows: </p>
    <ol>
      <li class="numbered">Initialize <a id="_idIndexMarker1584"/>the main value network parameter <img src="../Images/B15558_12_302.png" alt="" style="height: 1.11em;"/>, the Q network parameters <img src="../Images/B15558_12_216.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_206.png" alt="" style="height: 1.11em;"/>, and the actor network parameter <img src="../Images/B15558_14_245.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the target value network <img src="../Images/B15558_12_364.png" alt="" style="height: 1.2em;"/> by just copying the main value network parameter <img src="../Images/B15558_12_302.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat <em class="italic">step 5</em></li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0, . . ., <em class="italic">T</em> – 1<ol>
          <li class="numbered-l2">Select action <em class="italic">a</em> based on the policy <img src="../Images/B15558_18_169.png" alt="" style="height: 1.2em;"/>, that is, <img src="../Images/B15558_18_170.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Perform the selected action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_09_126.png" alt="" style="height: 1.2em;"/>, get the reward <em class="italic">r</em>, and store the transition information in the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer </li>
          <li class="numbered-l2">Compute target state value <img src="../Images/B15558_12_380.png" alt="" style="height: 1.58em;"/></li>
          <li class="numbered-l2">Compute the loss of value network <img src="../Images/B15558_12_323.png" alt="" style="height: 2.87em;"/> and update the parameter using gradient descent, <img src="../Images/B15558_18_175.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Compute the target Q value <img src="../Images/B15558_18_176.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Compute the loss of the Q networks <img src="../Images/B15558_12_383.png" alt="" style="height: 2.96em;"/> and update the parameter using gradient descent, <img src="../Images/B15558_12_386.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">Compute gradients of the actor objective function,<img src="../Images/B15558_18_179.png" alt="" style="height: 1.2em;"/> and update the parameter using gradient ascent, <img src="../Images/B15558_18_180.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Update the target value network parameter, <img src="../Images/B15558_18_181.png" alt="" style="height: 1.2em;"/></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-488" class="title">Trust Region Policy Optimization</h1>
    <p class="normal">The <a id="_idIndexMarker1585"/>algorithm for <strong class="keyword">Trust Region Policy Optimization</strong> (<strong class="keyword">TRPO</strong>) is given as follows:</p>
    <ol>
      <li class="numbered">Initialize <a id="_idIndexMarker1586"/>the policy network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> and value network parameter <img src="../Images/B15558_18_183.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Generate <em class="italic">N</em> number of trajectories <img src="../Images/B15558_10_058.png" alt="" style="height: 1.67em;"/> following the policy <img src="../Images/B15558_10_111.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">Compute the return (reward-to-go) <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub></li>
      <li class="numbered">Compute the advantage value <em class="italic">A</em><sub class="" style="font-style: italic;">t</sub></li>
      <li class="numbered">Compute the policy gradients <img src="../Images/B15558_13_232.png" alt="" style="height: 3.33em;"/></li>
      <li class="numbered">Compute <img src="../Images/B15558_13_238.png" alt="" style="height: 1.2em;"/> using the conjugate gradient method</li>
      <li class="numbered">Update the policy network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> using the update rule <img src="../Images/B15558_13_240.png" alt="" style="height: 3.51em;"/></li>
      <li class="numbered">Compute the mean squared error of the value network, <img src="../Images/B15558_18_100.png" alt="" style="height: 3.33em;"/></li>
      <li class="numbered">Update the value network parameter <img src="../Images/B15558_13_289.png" alt="" style="height: 1.11em;"/> using gradient descent, <img src="../Images/B15558_10_150.png" alt="" style="height: 1.2em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to <em class="italic">9</em> for several iterations</li>
    </ol>
    <h1 id="_idParaDest-489" class="title">PPO-Clipped</h1>
    <p class="normal">The <a id="_idIndexMarker1587"/>algorithm for the PPO-clipped method is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the policy network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> and value network parameter <img src="../Images/B15558_10_152.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Collect some <em class="italic">N</em> number of trajectories <img src="../Images/B15558_10_058.png" alt="" style="height: 1.67em;"/> following the policy <img src="../Images/B15558_10_036.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">Compute the return (reward-to-go) <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub></li>
      <li class="numbered">Compute the gradient of the objective function <img src="../Images/B15558_09_043.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Update the policy network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> using gradient ascent, <img src="../Images/B15558_13_316.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Compute the mean squared error of the value network, <img src="../Images/B15558_10_166.png" alt="" style="height: 3.33em;"/></li>
      <li class="numbered">Compute the gradient of the value network <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/></li>
      <li class="numbered">Update the value network parameter <img src="../Images/B15558_13_289.png" alt="" style="height: 1.11em;"/> using gradient descent, <img src="../Images/B15558_10_150.png" alt="" style="height: 1.2em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to <em class="italic">8 </em>for several iterations</li>
    </ol>
    <h1 id="_idParaDest-490" class="title">PPO-Penalty</h1>
    <p class="normal">The <a id="_idIndexMarker1588"/>algorithm for the PPO-penalty method is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the policy network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> and value network parameter <img src="../Images/B15558_14_249.png" alt="" style="height: 1.11em;"/> and initialize the penalty coefficient <img src="../Images/B15558_13_309.png" alt="" style="height: 1.11em;"/> and the target KL divergence <img src="../Images/B15558_18_207.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For iterations <img src="../Images/B15558_18_208.png" alt="" style="height: 1.11em;"/>:<ol>
          <li class="numbered-l2">Collect some <em class="italic">N</em> number of trajectories following the policy <img src="../Images/B15558_13_124.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Compute the return (reward-to-go) <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub></li>
          <li class="numbered-l2">Compute <img src="../Images/B15558_18_210.png" alt="" style="height: 2.69em;"/> </li>
          <li class="numbered-l2">Compute the gradient of the objective function <img src="../Images/B15558_18_211.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Update the policy network parameter <img src="../Images/B15558_18_212.png" alt="" style="height: 1.11em;"/> using gradient ascent, <img src="../Images/B15558_13_286.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">If <em class="italic">d</em> is greater than or equal to <img src="../Images/B15558_18_214.png" alt="" style="height: 1.11em;"/>, then we set <img src="../Images/B15558_18_215.png" alt="" style="height: 1.11em;"/>; if <em class="italic">d</em> is lesser than or equal to <img src="../Images/B15558_18_216.png" alt="" style="height: 1.11em;"/>, then we set, <img src="../Images/B15558_18_217.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the mean squared error of the value network: <img src="../Images/B15558_10_166.png" alt="" style="height: 3.33em;"/></li>
          <li class="numbered-l2">Compute the gradients of the value network <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update the value network parameter <img src="../Images/B15558_13_234.png" alt="" style="height: 1.11em;"/> using gradient descent, <img src="../Images/B15558_10_150.png" alt="" style="height: 1.2em;"/></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-491" class="title">Categorical DQN</h1>
    <p class="normal">The <a id="_idIndexMarker1589"/>algorithm for a categorical DQN is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> with random values</li>
      <li class="numbered">Initialize the target network parameter <img style="height: 1.2em" src="../Images/B15558_12_025.png" alt=""/> by copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_15_027.png" alt="" style="height: 1.11em;"/>, the number of atoms, and also <img src="../Images/B15558_18_226.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_18_227.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, perform <em class="italic">step 5</em></li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0, . . ., <em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Feed the state <em class="italic">s</em> and support values to the main categorical DQN parameterized by <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>, and get the probability value for each support value. Then compute the Q value as <img src="../Images/B15558_14_103.png" alt="" style="height: 2.4em;"/>.</li>
          <li class="numbered-l2">After computing the Q value, select an action using the epsilon-greedy policy, that is, with probability epsilon, select a random action <em class="italic">a</em> and with probability 1-epsilon, select an action as <img src="../Images/B15558_18_230.png" alt="" style="height: 1.4em;"/>.</li>
          <li class="numbered-l2">Perform the selected action and move to the next state <img src="../Images/B15558_12_376.png" alt="" style="height: 1.2em;"/> and obtain the reward <em class="italic">r.</em></li>
          <li class="numbered-l2">Store the transition information in the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Randomly sample a transition from the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Feed the next state <img src="../Images/B15558_12_376.png" alt="" style="height: 1.2em;"/> and support values to the target categorical DQN parameterized by <img style="height: 1.2em" src="../Images/B15558_12_025.png" alt=""/> and get the probability value for each support. Then compute the value as <img src="../Images/B15558_18_236.png" alt="" style="height: 2.4em;"/>.</li>
          <li class="numbered-l2">After computing the Q value, we select the best action in the state <img src="../Images/B15558_12_376.png" alt="" style="height: 1.2em;"/> as the one that has the maximum Q value <img src="../Images/B15558_14_112.png" alt="" style="height: 1.4em;"/>.</li>
          <li class="numbered-l2">Initialize the array <em class="italic">m</em> with zero values with its shape as the number of support.</li>
          <li class="numbered-l2">For <em class="italic">j</em> in the range of the number of support values:
        <ol>
          <li class="numbered-l2">Compute the target support value: <img src="../Images/B15558_18_239.png" alt="" style="height: 1.76em;"/></li>
          <li class="numbered-l2">Compute the value of <em class="italic">b</em>: <img src="../Images/B15558_18_240.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the lower bound and upper bound: <img src="../Images/B15558_14_047.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Distribute the probability on the lower bound: <img src="../Images/B15558_14_116.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">Distribute the probability on the upper bound: <img src="../Images/B15558_14_117.png" alt="" style="height: 1.49em;"/></li>
        </ol>
 </li>
          <li class="numbered-l2">Compute <a id="_idIndexMarker1590"/>the cross entropy loss <img src="../Images/B15558_14_131.png" alt="" style="height: 2.69em;"/>.</li>
          <li class="numbered-l2">Minimize the loss using gradient descent and update the parameter of the main network</li>
          <li class="numbered-l2">Freeze the target network parameter <img src="../Images/B15558_18_066.png" alt="" style="height: 1.2em;"/> for several time steps and then update it by just copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-492" class="title">Distributed Distributional DDPG </h1>
    <p class="normal">The <strong class="keyword">Distributed Distributional Deep Deterministic Policy Gradient</strong> (<strong class="keyword">D4PG</strong>) algorithm is given <a id="_idIndexMarker1591"/>as follows:</p>
    <ol>
      <li class="numbered">Initialize <a id="_idIndexMarker1592"/>the critic network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> and the actor network parameter <img src="../Images/B15558_18_248.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the target critic network parameter <img src="../Images/B15558_18_066.png" alt="" style="height: 1.2em;"/> and the target actor network parameter <img src="../Images/B15558_18_250.png" alt="" style="height: 1.2em;"/> by copying from <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_283.png" alt="" style="height: 1.11em;"/>, respectively</li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Launch <em class="italic">L</em> number of actors</li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat <em class="italic">step 6</em></li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0, . . ., <em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the target value distribution of the critic, that is, <img src="../Images/B15558_18_255.png" alt="" style="height: 3.71em;"/></li>
          <li class="numbered-l2">Compute the loss of the critic network and calculate the gradient as <img src="../Images/B15558_14_229.png" alt="" style="height: 3.07em;"/></li>
          <li class="numbered-l2">After computing gradients, update the critic network parameter using gradient descent: <img src="../Images/B15558_12_052.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the gradient of the actor network <img src="../Images/B15558_11_014.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Update the actor network parameter by gradient ascent: <img src="../Images/B15558_12_068.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <img src="../Images/B15558_18_260.png" alt="" style="height: 1.2em;"/> then:<p class="bullet-para">Update <a id="_idIndexMarker1593"/>the target critic and target actor network parameters using soft replacement as <img src="../Images/B15558_18_261.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_077.png" alt="" style="height: 1.2em;"/>, respectively</p>
          </li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <img src="../Images/B15558_14_261.png" alt="" style="height: 1.11em;"/> then:<p class="numbered-l2">Replicate the network weights to the actors</p>
          </li>
        </ol>
      </li>
    </ol>
    <p class="normal">We <a id="_idIndexMarker1594"/>perform the following steps in the actor network:</p>
    <ol>
      <li class="numbered">Select action <em class="italic">a</em> based on the policy <img src="../Images/B15558_14_262.png" alt="" style="height: 1.4em;"/> and exploration noise, that is, <img src="../Images/B15558_18_265.png" alt="" style="height: 1.29em;"/></li>
      <li class="numbered">Perform the selected action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/> and get the reward <em class="italic">r</em>, and store the transition information in the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 1 </em>and <em class="italic">2</em> until the learner finishes</li>
    </ol>
    <h1 id="_idParaDest-493" class="title">DAgger</h1>
    <p class="normal">The <a id="_idIndexMarker1595"/>algorithm for DAgger is given as follows:</p>
    <ol>
      <li class="numbered">Initialize an empty dataset <img src="../Images/B15558_18_268.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize a policy <img src="../Images/B15558_18_269.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For iterations <em class="italic">i</em> = 1 to <em class="italic">N</em>:<ol>
          <li class="numbered-l2">Create a policy <img src="../Images/B15558_18_270.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Generate a trajectory using the policy <img src="../Images/B15558_15_086.png" alt="" style="height: 0.84em;"/>.</li>
          <li class="numbered-l2">Create a dataset <img src="../Images/B15558_18_272.png" alt="" style="height: 1.11em;"/> by collecting states visited by the policy <img src="../Images/B15558_18_273.png" alt="" style="height: 0.84em;"/> and the actions of those states provided by the expert <img src="../Images/B15558_15_053.png" alt="" style="height: 0.84em;"/>. Thus, <img src="../Images/B15558_15_090.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Aggregate the dataset as <img src="../Images/B15558_18_276.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Train a <a id="_idIndexMarker1596"/>classifier on the updated dataset <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> and extract a new policy <img src="../Images/B15558_18_278.png" alt="" style="height: 1.11em;"/>.</li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-494" class="title">Deep Q learning from demonstrations</h1>
    <p class="normal">The <a id="_idIndexMarker1597"/>algorithm for <strong class="keyword">Deep Q Learning from Demonstrations</strong> (<strong class="keyword">DQfD</strong>) is given as follows:</p>
    <ol>
      <li class="numbered">Initialize <a id="_idIndexMarker1598"/>the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the target network parameter <img src="../Images/B15558_18_280.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/> with the expert demonstrations</li>
      <li class="numbered">Set <em class="italic">d</em>, the number of time steps we want to delay updating the target network parameter</li>
      <li class="numbered"><strong class="keyword">Pre-training phase</strong>: For steps <em class="italic">t</em> = 1, 2, . . ., <em class="italic">T</em>:<ol>
          <li class="numbered-l2">Sample a minibatch of experience from the replay buffer <img src="../Images/B15558_12_088.png" alt="" style="height: 1.11em;"/> </li>
          <li class="numbered-l2">Compute the loss <em class="italic">J</em>(<em class="italic">Q</em>)</li>
          <li class="numbered-l2">Update the parameter of the network using gradient descent</li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <em class="italic">d</em> = 0:
        <p class="bullet-para">Update the target network parameter <img src="../Images/B15558_18_284.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></p></li>
        </ol>
      </li>
      <li class="numbered"><strong class="keyword">Training phase</strong>: For steps <em class="italic">t</em> =1, 2, . . ., <em class="italic">T</em>:<ol>
          <li class="numbered-l2">Select an action</li>
          <li class="numbered-l2">Perform the selected action and move to the next state, observe the reward, and store this transition information in the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Sample a minibatch of experience from the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/> with prioritization</li>
          <li class="numbered-l2">Compute the loss <em class="italic">J</em>(<em class="italic">Q</em>)</li>
          <li class="numbered-l2">Update the parameter of the network using gradient descent </li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <em class="italic">d</em> = 0:
        <p class="bullet-para">Update <a id="_idIndexMarker1599"/>the target network parameter <img src="../Images/B15558_18_066.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></p></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-495" class="title">MaxEnt Inverse Reinforcement Learning</h1>
    <p class="normal">The <a id="_idIndexMarker1600"/>algorithm for maximum entropy inverse reinforcement learning is given as follows: </p>
    <ol>
      <li class="numbered">Initialize the parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> and gather the expert demonstrations <img src="../Images/B15558_15_027.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of iterations:<ol>
          <li class="numbered-l2">Compute the reward function <img src="../Images/B15558_18_292.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Compute the policy using the value iteration with the reward function obtained in the previous step</li>
          <li class="numbered-l2">Compute the state visitation frequency <img src="../Images/B15558_18_293.png" alt="" style="height: 1.11em;"/> using the policy obtained in the previous step</li>
          <li class="numbered-l2">Compute the gradient with respect to <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>, that is, <img src="../Images/B15558_15_176.png" alt="" style="height: 2.69em;"/></li>
          <li class="numbered-l2">Update the value of <img src="../Images/B15558_09_106.png" alt="" style="height: 1.11em;"/> as <img src="../Images/B15558_18_297.png" alt="" style="height: 1.11em;"/></li>
        </ol>
      </li>
    </ol>
    <h1 id="_idParaDest-496" class="title">MAML in Reinforcement Learning</h1>
    <p class="normal">The algorithm <a id="_idIndexMarker1601"/>for MAML in the reinforcement learning setting is given as follows:</p>
    <ol>
      <li class="numbered">Say we have a model <em class="italic">f</em> parameterized by a parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/> and we have a distribution over tasks <em class="italic">p</em>(<em class="italic">T</em>). First, we randomly initialize the model parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>.</li>
      <li class="numbered">Sample a batch of tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> from a distribution of tasks, that is, <em class="italic">T</em><sub class="" style="font-style: italic;">i </sub><em class="italic">~ p(T).</em></li>
      <li class="numbered">For each task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>:<ol>
          <li class="numbered-l2">Sample <em class="italic">k</em> trajectories using <img src="../Images/B15558_17_022.png" alt="" style="height: 1.11em;"/> and prepare the training dataset: <img src="../Images/B15558_17_128.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Train the model <img src="../Images/B15558_18_302.png" alt="" style="height: 1.11em;"/> on the training dataset <img src="../Images/B15558_17_089.png" alt="" style="height: 1.29em;"/> and compute the loss</li>
          <li class="numbered-l2">Minimize the loss using gradient descent and get the optimal parameter <img src="../Images/B15558_17_056.png" alt="" style="height: 1.2em;"/> as <img src="../Images/B15558_18_305.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Sample <em class="italic">k</em> trajectories using <img src="../Images/B15558_17_041.png" alt="" style="height: 1.29em;"/> and prepare the test dataset: <img src="../Images/B15558_17_115.png" alt="" style="height: 1.29em;"/></li>
        </ol>
      </li>
      <li class="numbered">Now, we minimize the loss on the test dataset <img src="../Images/B15558_18_308.png" alt="" style="height: 1.29em;"/>. Parameterize the model <em class="italic">f</em> with the optimal parameter <img src="../Images/B15558_18_309.png" alt="" style="height: 1.2em;"/> calculated in the previous step and compute the loss <img src="../Images/B15558_17_137.png" alt="" style="height: 1.29em;"/>. Calculate the gradients of the loss and update our randomly initialized <a id="_idIndexMarker1602"/>parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> using our test (meta-training) dataset: <img src="../Images/B15558_17_139.png" alt="" style="height: 2.78em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to <em class="italic">4</em> for several iterations.</li>
    </ol>
  </div>
</body></html>