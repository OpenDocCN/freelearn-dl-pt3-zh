<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer387">
<h1 class="chapterNumber">9</h1>
<h1 class="chapterTitle" id="_idParaDest-247">Generative Models</h1>
<p class="normal">Generative models are a type of machine learning algorithm that is used to create data. They are used to generate new data that is similar to the data that was used to train the model. They can be used to create new data for testing or to fill in missing data. Generative models are used in many applications, such as density estimation, image synthesis, and natural language processing. The VAE discussed in <em class="chapterRef">Chapter 8</em>, <em class="italic">Autoencoders</em>, was one type of generative model; in this chapter, we will discuss a wide range of generative models, <strong class="keyWord">Generative Adversarial Networks</strong> (<strong class="keyWord">GANs</strong>) and their variants, flow-based models, and diffusion models.</p>
<p class="normal">GANs have been defined as <em class="italic">the most interesting idea in the last 10 years in ML</em> (<a href="https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning"><span class="url">https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning</span></a>) by Yann LeCun, one of the fathers of deep learning. GANs are able to learn how to reproduce synthetic data that looks real. For instance, computers can learn how to paint and create realistic images. The idea was originally proposed by Ian Goodfellow (for more information, refer to <em class="italic">NIPS 2016 Tutorial: Generative Adversarial Networks</em>, by I. Goodfellow, 2016); he has worked with the University of Montreal, Google Brain, and OpenAI, and is presently working in Apple Inc. as the Director of Machine Learning.</p>
<p class="normal">In this chapter, we will cover different types of GANs; the chapter will introduce you to flow-based models and diffusion models, and additionally, you will see some of their implementation in TensorFlow. Broadly, we will cover the following topics:</p>
<ul>
<li class="bulletList">What is a GAN?</li>
<li class="bulletList">Deep convolutional GANs</li>
<li class="bulletList">InfoGAN</li>
<li class="bulletList">SRGAN</li>
<li class="bulletList">CycleGAN</li>
<li class="bulletList">Applications of GANs</li>
<li class="bulletList">Flow-based generative models</li>
<li class="bulletList">Diffusion models for data generation</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp9"><span class="url">https://packt.link/dltfchp9</span></a></p>
</div>
<p class="normal">Let’s begin!</p>
<h1 class="heading-1" id="_idParaDest-248">What is a GAN?</h1>
<p class="normal">The ability of GANs to learn<a id="_idIndexMarker912"/> high-dimensional, complex data distributions has made them very popular with researchers in recent years. Between 2016, when they were first proposed by Ian Goodfellow, to March 2022, we have more than 100,000 research papers related to GANs, just in the space of 6 years!</p>
<p class="normal">The applications of GANs include creating images, videos, music, and even natural languages. They have been employed in tasks like image-to-image translation, image super-resolution, drug discovery, and even next-frame prediction in video. They have been especially successful in the task of synthetic data generation – both for training the deep learning models and assessing the adversarial attacks.</p>
<p class="normal">The key idea of GAN can be easily understood by considering it analogous to “art forgery,” which is the process of creating works of art that are falsely credited to other usually more famous artists. GANs train two neural nets simultaneously. The generator <em class="italic">G(Z)</em> is the one that makes the forgery, and the discriminator <em class="italic">D(Y)</em> is the one that can judge how realistic the reproductions are, based on its observations of authentic pieces of art and copies. <em class="italic">D(Y)</em> takes an input <em class="italic">Y</em> (for instance, an image), and expresses a vote to judge how real the input is. In general, a value close to 1 denotes “real,” while a value close to 0 denotes “forgery.” <em class="italic">G(Z)</em> takes an input from random noise <em class="italic">Z</em> and it trains itself to fool <em class="italic">D</em> into thinking that whatever <em class="italic">G(Z)</em> produces is real.</p>
<p class="normal">The goal of training the discriminator <em class="italic">D(Y)</em> is to maximize <em class="italic">D(Y)</em> for every image from the true data distribution and to minimize <em class="italic">D(Y)</em> for every image not from the true data distribution. So, <em class="italic">G</em> and <em class="italic">D</em> play opposite games, hence the name <strong class="keyWord">adversarial training</strong>. Note that we train <em class="italic">G</em> and <em class="italic">D</em> in an alternating manner, where each one of their objectives is expressed as a loss function optimized via a gradient descent. The generative model continues to improve its forgery capabilities, and the discriminative model continues to improve its forgery recognition capabilities. The discriminator network (usually a standard convolutional neural network) tries to classify if an input image is real or generated. The<a id="_idIndexMarker913"/> important new idea is to backpropagate through both the discriminator and the generator to adjust the generator’s parameters in such a way that the generator can learn how to fool the discriminator more often. In the end, the generator will learn how to produce images that are indistinguishable from the real ones:</p>
<figure class="mediaobject"><img alt="A black sign with white text  Description automatically generated" height="228" src="../Images/B18331_09_01.png" width="480"/></figure>
<p class="packt_figref">Figure 9.1: Basic architecture of a GAN</p>
<p class="normal">Of course, GANs involve working towards equilibrium in a game involving two players. Let us first understand what we mean by equilibrium here. When we start, one of the two players is hopefully better than the other. This pushes the other to improve and this way, both the generator and discriminator push each other towards improvement.</p>
<p class="normal">Eventually, we reach a state where the improvement is not significant in either player. We check this by plotting the loss function, to see when the two losses (gradient loss and discriminator loss) reach a plateau. We don’t want the game to be skewed too heavily one way; if the forger were to immediately learn how to fool the judge on every occasion, then the forger has <em class="italic">nothing more to learn</em>. Practically training GANs is really hard, and a lot of research is being done in analyzing GAN convergence; check this site: <a href="https://avg.is.tuebingen.mpg.de/projects/convergence-and-stability-of-gan-training"><span class="url">https://avg.is.tuebingen.mpg.de/projects/convergence-and-stability-of-gan-training</span></a> for details on convergence and stability of different types of GANs. In generative applications of GAN, we want the generator to learn a little better than the discriminator.</p>
<p class="normal">Let’s now delve deep into how GANs learn. Both the discriminator and generator take turns to learn. The learning can be divided into two steps:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Here the discriminator, <em class="italic">D(x)</em>, learns. The generator, <em class="italic">G(z)</em>, is used to generate fake images from random noise <em class="italic">z</em> (which follows some prior distribution <em class="italic">P(z)</em>). The fake images from the generator and the real images from the training dataset are both fed to the discriminator, and it performs supervised learning trying to separate fake from real. If <em class="italic">P</em><sub class="subscript">data</sub> (<em class="italic">x</em>) is the training dataset distribution, then the discriminator network tries to maximize its <a id="_idIndexMarker914"/>objective so that <em class="italic">D(x)</em> is close to 1 when the input data is real and close to zero when the input data is fake.</li>
<li class="numberedList">In the next step, the generator network learns. Its goal is to fool the discriminator network into thinking that generated <em class="italic">G(z)</em> is real, that is, force <em class="italic">D(G(z))</em> close to 1.</li>
</ol>
<p class="normal">The two steps are repeated sequentially. Once the training ends, the discriminator is no longer able to discriminate between real and fake data and the generator becomes a pro in creating data very similar to the training data. The stability between discriminator and generator is an actively researched problem.</p>
<p class="normal">Now that you have got an idea of what GANs are, let’s look at a practical application of a GAN in which “handwritten” digits are generated.</p>
<h2 class="heading-2" id="_idParaDest-249">MNIST using GAN in TensorFlow</h2>
<p class="normal">Let us build a simple GAN <a id="_idIndexMarker915"/>capable of generating handwritten digits. We will use the MNIST handwritten digits to train the <a id="_idIndexMarker916"/>network. We will need to import TensorFlow<a id="_idIndexMarker917"/> modules; to keep the code clean, we export all the classes that we will require from the TensorFlow framework:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.datasets <span class="hljs-keyword">import</span> mnist
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Input, Dense, Reshape, Flatten, Dropout
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> BatchNormalization, Activation, ZeroPadding2D
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> LeakyReLU
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> UpSampling2D, Conv2D
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential, Model
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> initializers
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
<p class="normal">We use the TensorFlow Keras dataset to access the MNIST data. The data contains 60,000 training images of<a id="_idIndexMarker918"/> handwritten digits each of size 28 × 28. The pixel value of the digits lies between 0-255; we normalize the input values such that each pixel has a value in the range [-1, 1]:</p>
<pre class="programlisting code"><code class="hljs-code">randomDim = <span class="hljs-number">10</span>
(X_train, _), (_,  _) = mnist.load_data()
X_train = (X_train.astype(np.float32) - <span class="hljs-number">127.5</span>)/<span class="hljs-number">127.5</span>
</code></pre>
<p class="normal">We will use a simple <strong class="keyWord">multi-layered perceptron</strong> (<strong class="keyWord">MLP</strong>) and we will feed it an image as a flat vector of size 784, so we<a id="_idIndexMarker919"/> reshape the training data:</p>
<pre class="programlisting code"><code class="hljs-code">X_train = X_train.reshape(<span class="hljs-number">60000</span>, <span class="hljs-number">784</span>)
</code></pre>
<p class="normal">Now we will need to <a id="_idIndexMarker920"/>build a generator and discriminator. The purpose of the generator is to take in a noisy input and generate an image similar to the training dataset. The size of the noisy input is decided by the variable <code class="inlineCode">randomDim</code>; you can initialize it to any integral value. Conventionally, people set it to 100. For our implementation, we tried a value of 10. This input is fed to a dense layer with <code class="inlineCode">256</code> neurons with LeakyReLU activation. We next add another dense layer with <code class="inlineCode">512</code> hidden neurons, followed by the third hidden layer with <code class="inlineCode">1024</code> neurons, and finally the output layer with <code class="inlineCode">784</code> neurons. You can change the number of neurons in the hidden layers and see how the performance changes; however, the number of neurons in the output unit has to match the number of pixels in the training images. The corresponding generator is then:</p>
<pre class="programlisting code"><code class="hljs-code">generator = Sequential()
generator.add(Dense(<span class="hljs-number">256</span>, input_dim=randomDim))
generator.add(LeakyReLU(<span class="hljs-number">0.2</span>))
generator.add(Dense(<span class="hljs-number">512</span>))
generator.add(LeakyReLU(<span class="hljs-number">0.2</span>))
generator.add(Dense(<span class="hljs-number">1024</span>))
generator.add(LeakyReLU(<span class="hljs-number">0.2</span>))
generator.add(Dense(<span class="hljs-number">784</span>, activation=<span class="hljs-string">'tanh'</span>))
</code></pre>
<p class="normal">Similarly, we build a discriminator. Notice now (<em class="italic">Figure 9.1</em>) that the discriminator takes in the images, either from the training set or images generated by the generator, thus its input size is <code class="inlineCode">784</code>. Additionally, here we are using a TensorFlow initializer to initialize the weights of the dense layer, we are using a normal distribution with a standard deviation of 0.02 and a mean <a id="_idIndexMarker921"/>of 0. As mentioned in <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, there are many initializers available in the TensorFlow framework. The output of the discriminator<a id="_idIndexMarker922"/> is a single bit, with <code class="inlineCode">0</code> signifying a fake image (generated by generator) and <code class="inlineCode">1</code> signifying that the image is from the training dataset:</p>
<pre class="programlisting code"><code class="hljs-code">discriminator = Sequential()
discriminator.add(Dense(<span class="hljs-number">1024</span>, input_dim=<span class="hljs-number">784</span>, kernel_initializer=initializers.RandomNormal(stddev=<span class="hljs-number">0.02</span>))
)
discriminator.add(LeakyReLU(<span class="hljs-number">0.2</span>))
discriminator.add(Dropout(<span class="hljs-number">0.3</span>))
discriminator.add(Dense(<span class="hljs-number">512</span>))
discriminator.add(LeakyReLU(<span class="hljs-number">0.2</span>))
discriminator.add(Dropout(<span class="hljs-number">0.3</span>))
discriminator.add(Dense(<span class="hljs-number">256</span>))
discriminator.add(LeakyReLU(<span class="hljs-number">0.2</span>))
discriminator.add(Dropout(<span class="hljs-number">0.3</span>))
discriminator.add(Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>))
</code></pre>
<p class="normal">Next, we combine the<a id="_idIndexMarker923"/> generator and discriminator together to form a GAN. In the GAN, we ensure that the discriminator weights are fixed by setting the <code class="inlineCode">trainable</code> argument to <code class="inlineCode">False</code>:</p>
<pre class="programlisting code"><code class="hljs-code">discriminator.trainable = <span class="hljs-literal">False</span>
ganInput = Input(shape=(randomDim,))
x = generator(ganInput)
ganOutput = discriminator(x)
gan = Model(inputs=ganInput, outputs=ganOutput)
</code></pre>
<p class="normal">The trick to training the two is that we first train the discriminator separately; we use binary cross-entropy loss for the discriminator. Later, we freeze the weights of the discriminator and train the combined GAN; this results in the training of the generator. The loss this time is also binary cross-entropy:</p>
<pre class="programlisting code"><code class="hljs-code">discriminator.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>)
gan.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>)
</code></pre>
<p class="normal">Let us now perform the training. For each epoch, we take a sample of random noise first, feed it to the generator, and the generator produces a fake image. We combine the generated fake images <a id="_idIndexMarker924"/>and the actual training images in a batch with their specific labels and use them to train the discriminator first on the given batch:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">epochs=</span><span class="hljs-number">1</span><span class="hljs-params">, batchSize=</span><span class="hljs-number">128</span>):
    batchCount = <span class="hljs-built_in">int</span>(X_train.shape[<span class="hljs-number">0</span>] / batchSize)
    <span class="hljs-built_in">print</span> (<span class="hljs-string">'Epochs:'</span>, epochs)
    <span class="hljs-built_in">print</span> (<span class="hljs-string">'Batch size:'</span>, batchSize)
    <span class="hljs-built_in">print</span> (<span class="hljs-string">'Batches per epoch:'</span>, batchCount)
    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, epochs+<span class="hljs-number">1</span>):
        <span class="hljs-built_in">print</span> (<span class="hljs-string">'-'</span>*<span class="hljs-number">15</span>, <span class="hljs-string">'Epoch %d'</span> % e, <span class="hljs-string">'-'</span>*<span class="hljs-number">15</span>)
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batchCount):
            <span class="hljs-comment"># Get a random set of input noise and images</span>
            noise = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=[batchSize,
            randomDim])
            imageBatch = X_train[np.random.randint(<span class="hljs-number">0</span>,
            X_train.shape[<span class="hljs-number">0</span>], size=batchSize)]
            <span class="hljs-comment"># Generate fake MNIST images</span>
            generatedImages = generator.predict(noise)
            <span class="hljs-comment"># print np.shape(imageBatch), np.shape(generatedImages)</span>
            X = np.concatenate([imageBatch, generatedImages])
            <span class="hljs-comment"># Labels for generated and real data</span>
            yDis = np.zeros(<span class="hljs-number">2</span>*batchSize)
            <span class="hljs-comment"># One-sided label smoothing</span>
            yDis[:batchSize] = <span class="hljs-number">0.9</span>
            <span class="hljs-comment"># Train discriminator</span>
            discriminator.trainable = <span class="hljs-literal">True</span>
            dloss = discriminator.train_on_batch(X, yDis)
</code></pre>
<p class="normal">If you notice, while <a id="_idIndexMarker925"/>assigning labels, instead of <code class="inlineCode">0</code>/<code class="inlineCode">1</code> we used <code class="inlineCode">0</code>/<code class="inlineCode">0.9</code> – this<a id="_idIndexMarker926"/> is called label smoothing. It has been found <a id="_idIndexMarker927"/>that keeping a soft target improves both generalization and learning speed (<em class="italic">When does label smoothing help?</em>, Muller et al. NeurIPS 2019).</p>
<p class="normal">Now, in the same <code class="inlineCode">for</code> loop, we will train the generator. We want the images generated by the generator to be detected as real by the discriminator, so we use a random vector (noise) as input to the generator; this generates a fake image and then trains the GAN such that the discriminator perceives the image as real (the output is <code class="inlineCode">1</code>):</p>
<pre class="programlisting code"><code class="hljs-code">            <span class="hljs-comment"># Train generator</span>
            noise = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=[batchSize,
            randomDim])
            yGen = np.ones(batchSize)
            discriminator.trainable = <span class="hljs-literal">False</span>
            gloss = gan.train_on_batch(noise, yGen)
</code></pre>
<p class="normal">Cool trick, right? If you wish to, you can save the generator and discriminator loss as well as the generated<a id="_idIndexMarker928"/> images. Next, we are saving the losses for each epoch and generating images after every 20 epochs:</p>
<pre class="programlisting code"><code class="hljs-code">        <span class="hljs-comment"># Store loss of most recent batch from this epoch</span>
        dLosses.append(dloss)
        gLosses.append(gloss)
        <span class="hljs-keyword">if</span> e == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> e % <span class="hljs-number">20</span> == <span class="hljs-number">0</span>:
               saveGeneratedImages(e)
</code></pre>
<p class="normal">We can now train the <a id="_idIndexMarker929"/>GAN by calling the <code class="inlineCode">train</code> function. In the following graph, you can see the plot of both generative and <a id="_idIndexMarker930"/>discriminative loss as the GAN is learning:</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="446" src="../Images/B18331_09_02.png" width="544"/></figure>
<p class="packt_figref">Figure 9.2: Discriminator and generator loss plots</p>
<p class="normal">And handwritten digits generated by our GAN:</p>
<figure class="mediaobject"><img alt="Calendar  Description automatically generated" height="313" src="../Images/B18331_09_03_01.png" width="826"/></figure>
<figure class="mediaobject"><img alt="A picture containing text, grater, kitchenware  Description automatically generated" height="312" src="../Images/B18331_09_03_02.png" width="826"/></figure>
<p class="packt_figref">Figure 9.3: Generated handwritten digits</p>
<p class="normal">You can see from the<a id="_idIndexMarker931"/> preceding figures that as the epochs increase, the<a id="_idIndexMarker932"/> handwritten digits generated by the<a id="_idIndexMarker933"/> GAN become more and more realistic.</p>
<p class="normal">To plot the loss and the generated images of the handwritten digits, we define two helper functions,<code class="inlineCode"> plotLoss()</code> and <code class="inlineCode">saveGeneratedImages()</code>. Their code is given as follows:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Plot the loss from each batch</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">plotLoss</span>(<span class="hljs-params">epoch</span>):
    plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))
    plt.plot(dLosses, label=<span class="hljs-string">'Discriminitive loss'</span>)
    plt.plot(gLosses, label=<span class="hljs-string">'Generative loss'</span>)
    plt.xlabel(<span class="hljs-string">'Epoch'</span>)
    plt.ylabel(<span class="hljs-string">'Loss'</span>)
    plt.legend()
    plt.savefig(<span class="hljs-string">'images/gan_loss_epoch_%d.png'</span> % epoch)
<span class="hljs-comment"># Create a wall of generated MNIST images</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">saveGeneratedImages</span>(<span class="hljs-params">epoch, examples=</span><span class="hljs-number">100</span><span class="hljs-params">, dim=(</span><span class="hljs-number">10</span><span class="hljs-params">, </span><span class="hljs-number">10</span><span class="hljs-params">), figsize=(</span><span class="hljs-number">10</span><span class="hljs-params">, </span><span class="hljs-number">10</span><span class="hljs-params">)</span>):
    noise = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=[examples, randomDim])
    generatedImages = generator.predict(noise)
    generatedImages = generatedImages.reshape(examples, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)
    plt.figure(figsize=figsize)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(generatedImages.shape[<span class="hljs-number">0</span>]):
        plt.subplot(dim[<span class="hljs-number">0</span>], dim[<span class="hljs-number">1</span>], i+<span class="hljs-number">1</span>)
        plt.imshow(generatedImages[i], interpolation=<span class="hljs-string">'nearest'</span>,
        cmap=<span class="hljs-string">'gray_r'</span>)
        plt.axis(<span class="hljs-string">'off'</span>)
    plt.tight_layout()
    plt.savefig(<span class="hljs-string">'images/gan_generated_image_epoch_%d.png'</span> % epoch)
</code></pre>
<p class="normal">The <code class="inlineCode">saveGeneratedImages</code> function saves images in the <code class="inlineCode">images</code> folder, so make sure you have created the<a id="_idIndexMarker934"/> folder in your current <a id="_idIndexMarker935"/>working directory. The complete code for this can be found in the notebook <code class="inlineCode">VanillaGAN.ipynb</code> at the GitHub repo for this<a id="_idIndexMarker936"/> chapter. In the coming sections, we will cover some recent GAN architectures and implement them in TensorFlow.</p>
<h1 class="heading-1" id="_idParaDest-250">Deep convolutional GAN (DCGAN)</h1>
<p class="normal">Proposed in 2016, DCGANs have<a id="_idIndexMarker937"/> become one of the most popular and successful GAN architectures. The main idea of the design was using convolutional layers without the use of pooling layers or the end classifier layers. The convolutional strides and transposed convolutions are employed for the downsampling (the reduction of dimensions) and upsampling (the increase of dimensions. In GANs, we do this with the help of a transposed convolution layer. To know more about transposed convolution layers, refer to the paper <em class="italic">A guide to convolution arithmetic for deep learning</em> by Dumoulin and Visin) of images.</p>
<p class="normal">Before going into the details of the DCGAN architecture and its capabilities, let us point out the major changes that were introduced in the paper:</p>
<ul>
<li class="bulletList">The network consisted of all convolutional layers. The pooling layers were replaced by strided convolutions (i.e., instead of one single stride while using the convolutional layer, we increased the number of strides to two) in the discriminator and transposed convolutions in the generator.</li>
<li class="bulletList">The fully connected classifying layers after the convolutions are removed.</li>
<li class="bulletList">To help with the gradient flow, batch normalization is done after every convolutional layer.</li>
</ul>
<p class="normal">The basic idea of DCGANs is the <a id="_idIndexMarker938"/>same as the vanilla GAN: we have a generator that takes in noise of 100 dimensions; the noise is projected and reshaped, and is then passed through convolutional layers. <em class="italic">Figure 9.4</em> shows the generator architecture:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="346" src="../Images/B18331_09_04.png" width="826"/></figure>
<p class="packt_figref">Figure 9.4: Visualizing the architecture of a generator</p>
<p class="normal">The discriminator network takes in the images (either generated by the generator or from the real dataset), and the images undergo convolution followed by batch normalization. At each convolution step, the images get downsampled using strides. The final output of the convolutional layer is flattened and feeds a one-neuron classifier layer. </p>
<p class="normal">In <em class="italic">Figure 9.5</em>, you can see the discriminator:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="309" src="../Images/B18331_09_05.png" width="879"/></figure>
<p class="packt_figref">Figure 9.5: Visualizing the architecture of a discriminator</p>
<p class="normal">The generator and the discriminator are combined together to form the DCGAN. The training follows in the same manner as before; that is, we first train the discriminator on a mini-batch, then freeze the discriminator and train the generator. The process is repeated iteratively for a<a id="_idIndexMarker939"/> few thousand epochs. The authors found that we get more stable results with the Adam optimizer and a learning rate of 0.002.</p>
<p class="normal">Next, we’ll implement a DCGAN for generating handwritten digits.</p>
<h2 class="heading-2" id="_idParaDest-251">DCGAN for MNIST digits</h2>
<p class="normal">Let us now build a<a id="_idIndexMarker940"/> DCGAN for generating handwritten digits. We first see the code for the generator. The generator is built by adding the layers sequentially. The first layer is a dense layer that takes the noise of 100 dimensions as an input. The 100-dimensional input is expanded to a flat vector of size 128 × 7 × 7. This is done so that finally, we get an output of size 28 × 28, the standard size of MNIST handwritten digits. The vector is reshaped to a tensor of size 7 × 7 × 128. This vector is then upsampled using the TensorFlow Keras <code class="inlineCode">UpSampling2D</code> layer. Please note that this layer simply scales up the image by doubling rows and columns. The layer has no weights, so it is computationally cheap.</p>
<p class="normal">The Upsampling2D layer will now double the rows and columns of the 7 × 7 × 128 (rows × columns × channels) image, yielding an output of size 14 × 14 × 128. The upsampled image is passed to a convolutional layer. This convolutional layer learns to fill in the details in the upsampled image. The output of a convolution is passed to batch normalization for better gradient flow. The batch normalized output then undergoes ReLU activation in all the intermediate layers. We repeat the structure, that is, upsampling | convolution | batch normalization | ReLU. In the following generator, we have two such <a id="_idIndexMarker941"/>structures, the first with 128 filters, and the second with 64 filters in the convolution operation. The final output is obtained from a pure convolutional layer with 3 filters and tan hyperbolic activation, yielding an image of size 28 × 28 × 1:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">build_generator</span>(<span class="hljs-params">self</span>):
    model = Sequential()
    model.add(Dense(<span class="hljs-number">128</span> * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, activation=<span class="hljs-string">"relu"</span>,
    input_dim=self.latent_dim))
    model.add(Reshape((<span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">128</span>)))
    model.add(UpSampling2D())
    model.add(Conv2D(<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>))
    model.add(BatchNormalization(momentum=<span class="hljs-number">0.8</span>))
    model.add(Activation(<span class="hljs-string">"relu"</span>))
    model.add(UpSampling2D())
    model.add(Conv2D(<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>))
    model.add(BatchNormalization(momentum=<span class="hljs-number">0.8</span>))
    model.add(Activation(<span class="hljs-string">"relu"</span>))
    model.add(Conv2D(self.channels, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>))
    model.add(Activation(<span class="hljs-string">"tanh"</span>))
    model.summary()
    noise = Input(shape=(self.latent_dim,))
    img = model(noise)
    <span class="hljs-keyword">return</span> Model(noise, img)
</code></pre>
<p class="normal">The resultant generator model is as follows:</p>
<pre class="programlisting con"><code class="hljs-con">Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_3 (Conv2D)           (None, 14, 14, 32)        320       
                                                                 
 leaky_re_lu (LeakyReLU)     (None, 14, 14, 32)        0         
                                                                 
 dropout (Dropout)           (None, 14, 14, 32)        0         
                                                                 
 conv2d_4 (Conv2D)           (None, 7, 7, 64)          18496     
                                                                 
 zero_padding2d (ZeroPadding  (None, 8, 8, 64)         0         
 2D)                                                             
                                                                 
 batch_normalization_2 (Batc  (None, 8, 8, 64)         256       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 64)          0         
                                                                 
 dropout_1 (Dropout)         (None, 8, 8, 64)          0         
                                                                 
 conv2d_5 (Conv2D)           (None, 4, 4, 128)         73856     
                                                                 
 batch_normalization_3 (Batc  (None, 4, 4, 128)        512       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         
                                                                 
 dropout_2 (Dropout)         (None, 4, 4, 128)         0         
                                                                 
 conv2d_6 (Conv2D)           (None, 4, 4, 256)         295168    
                                                                 
 batch_normalization_4 (Batc  (None, 4, 4, 256)        1024      
 hNormalization)                                                 
                                                                 
 leaky_re_lu_3 (LeakyReLU)   (None, 4, 4, 256)         0         
                                                                 
 dropout_3 (Dropout)         (None, 4, 4, 256)         0         
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense_1 (Dense)             (None, 1)                 4097      
                                                                 
=================================================================
Total params: 393,729
Trainable params: 392,833
Non-trainable params: 896
</code></pre>
<p class="normal">You can also experiment with the transposed convolution layer. This layer not only upsamples the input image <a id="_idIndexMarker942"/>but also learns how to fill in details during the training. Thus, you can replace upsampling and convolution layers with a single transposed convolution layer. The transpose convolutional layer performs an inverse convolution operation. You can read about it in more detail in the paper: <em class="italic">A guide to convolution arithmetic for deep learning</em> (<a href="https://arxiv.org/abs/1603.07285"><span class="url">https://arxiv.org/abs/1603.07285</span></a>).</p>
<p class="normal">Now that we have a generator, let us see the code to build the discriminator. The discriminator is similar to a standard convolutional neural network but with one major change: instead of max pooling, we use convolutional layers with strides of 2. We also add dropout layers to avoid overfitting, and batch normalization for better accuracy and fast convergence. The activation<a id="_idIndexMarker943"/> layer is leaky ReLU. In the following network, we use three such convolutional layers, with filters of 32, 64, and 128 respectively. The output of the third convolutional layer is flattened and fed to a dense layer with a single unit.</p>
<p class="normal">The output of this unit classifies the image as fake or real:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">build_discriminator</span>(<span class="hljs-params">self</span>):
    model = Sequential()
    model.add(Conv2D(<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>,
    input_shape=self.img_shape, padding=<span class="hljs-string">"same"</span>))
    model.add(LeakyReLU(alpha=<span class="hljs-number">0.2</span>))
    model.add(Dropout(<span class="hljs-number">0.25</span>))
    model.add(Conv2D(<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"same"</span>))
    model.add(ZeroPadding2D(padding=((<span class="hljs-number">0</span>,<span class="hljs-number">1</span>),(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>))))
    model.add(BatchNormalization(momentum=<span class="hljs-number">0.8</span>))
    model.add(LeakyReLU(alpha=<span class="hljs-number">0.2</span>))
    model.add(Dropout(<span class="hljs-number">0.25</span>))
    model.add(Conv2D(<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">"same"</span>))
    model.add(BatchNormalization(momentum=<span class="hljs-number">0.8</span>))
    model.add(LeakyReLU(alpha=<span class="hljs-number">0.2</span>))
    model.add(Dropout(<span class="hljs-number">0.25</span>))
    model.add(Conv2D(<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">"same"</span>))
    model.add(BatchNormalization(momentum=<span class="hljs-number">0.8</span>))
    model.add(LeakyReLU(alpha=<span class="hljs-number">0.2</span>))
    model.add(Dropout(<span class="hljs-number">0.25</span>))
    model.add(Flatten())
    model.add(Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>))
    model.summary()
    img = Input(shape=self.img_shape)
    validity = model(img)
<span class="hljs-keyword">    return</span> Model(img, validity)
</code></pre>
<p class="normal">The resultant discriminator network is:</p>
<pre class="programlisting con"><code class="hljs-con">Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 6272)              633472    
                                                                 
 reshape (Reshape)           (None, 7, 7, 128)         0         
                                                                 
 up_sampling2d (UpSampling2D  (None, 14, 14, 128)      0         
 )                                                               
                                                                 
 conv2d (Conv2D)             (None, 14, 14, 128)       147584    
                                                                 
 batch_normalization (BatchN  (None, 14, 14, 128)      512       
 ormalization)                                                   
                                                                 
 activation (Activation)     (None, 14, 14, 128)       0         
                                                                 
 up_sampling2d_1 (UpSampling  (None, 28, 28, 128)      0         
 2D)                                                             
                                                                 
 conv2d_1 (Conv2D)           (None, 28, 28, 64)        73792     
                                                                 
 batch_normalization_1 (Batc  (None, 28, 28, 64)       256       
 hNormalization)                                                 
                                                                 
 activation_1 (Activation)   (None, 28, 28, 64)        0         
                                                                 
 conv2d_2 (Conv2D)           (None, 28, 28, 1)         577       
                                                                 
 activation_2 (Activation)   (None, 28, 28, 1)         0         
                                                                 
=================================================================
Total params: 856,193
Trainable params: 855,809
Non-trainable params: 384
_________________________________________________________________
</code></pre>
<p class="normal">The complete GAN is made <a id="_idIndexMarker944"/>by combining the two:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">DCGAN</span>():
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, rows, cols, channels, z = </span><span class="hljs-number">10</span>):
        <span class="hljs-comment"># Input shape</span>
        self.img_rows = rows
        self.img_cols = cols
        self.channels = channels
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.latent_dim = z
        optimizer = Adam(<span class="hljs-number">0.0002</span>, <span class="hljs-number">0.5</span>)
        <span class="hljs-comment"># Build and compile the discriminator</span>
        self.discriminator = self.build_discriminator()
        self.discriminator.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">’binary_crossentropy’</span>,
            optimizer=optimizer,
            metrics=[<span class="hljs-string">‘accuracy’</span>])
        <span class="hljs-comment"># Build the generator</span>
        self.generator = self.build_generator()
        <span class="hljs-comment"># The generator takes noise as input and generates imgs</span>
        z = Input(shape=(self.latent_dim,))
        img = self.generator(z)
        <span class="hljs-comment"># For the combined model we will only train the generator</span>
        self.discriminator.trainable = <span class="hljs-literal">False</span>
        <span class="hljs-comment"># The discriminator takes generated images as input and determines validity</span>
        valid = self.discriminator(img)
        <span class="hljs-comment"># The combined model  (stacked generator and discriminator)</span>
        <span class="hljs-comment"># Trains the generator to fool the discriminator</span>
        self.combined = Model(z, valid)
        self.combined.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">’binary_crossentropy’</span>, optimizer=optimizer)
</code></pre>
<p class="normal">As you might have noticed, we are defining here the <code class="inlineCode">binary_crossentropy</code> loss object, which we will use later to define the generator and discriminator losses. Optimizers for both the generator and <a id="_idIndexMarker945"/>discriminator is defined in this <code class="inlineCode">init</code> method. And finally, we define a TensorFlow checkpoint that we will use to save the two models (generator and discriminator) as the model trains.</p>
<p class="normal">The GAN is trained in the same manner as before; at each step, first, random noise is fed to the generator. The output of the generator is added with real images to initially train the discriminator, and then the generator is trained to give an image that can fool the discriminator. </p>
<p class="normal">The process is repeated for<a id="_idIndexMarker946"/> the next batch of images. The GAN takes between a few hundred to thousands of epochs to train:</p>
<pre class="programlisting code"><code class="hljs-code">   <span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self, epochs, batch_size=</span><span class="hljs-number">256</span><span class="hljs-params">, save_interval=</span><span class="hljs-number">50</span>):
        <span class="hljs-comment"># Load the dataset</span>
        (X_train, _), (_, _) = mnist.load_data()
        <span class="hljs-comment"># Rescale -1 to 1</span>
        X_train = X_train / <span class="hljs-number">127.5</span> - <span class="hljs-number">1.</span>
        X_train = np.expand_dims(X_train, axis=<span class="hljs-number">3</span>)
        <span class="hljs-comment"># Adversarial ground truths</span>
        valid = np.ones((batch_size, <span class="hljs-number">1</span>))
        fake = np.zeros((batch_size, <span class="hljs-number">1</span>))
        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
            <span class="hljs-comment"># ---------------------</span>
            <span class="hljs-comment">#  Train Discriminator</span>
            <span class="hljs-comment"># ---------------------</span>
            <span class="hljs-comment"># Select a random half of images</span>
            idx = np.random.randint(<span class="hljs-number">0</span>, X_train.shape[<span class="hljs-number">0</span>], batch_size)
            imgs = X_train[idx]
            <span class="hljs-comment"># Sample noise and generate a batch of new images</span>
            noise = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (batch_size, self.latent_dim))
            gen_imgs = self.generator.predict(noise)
            <span class="hljs-comment"># Train the discriminator (real classified as ones and generated as zeros)</span>
            d_loss_real = self.discriminator.train_on_batch(imgs, valid)
            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)
            d_loss = <span class="hljs-number">0.5</span> * np.add(d_loss_real, d_loss_fake)
            <span class="hljs-comment"># ---------------------</span>
            <span class="hljs-comment">#  Train Generator</span>
            <span class="hljs-comment"># ---------------------</span>
            <span class="hljs-comment"># Train the generator (wants discriminator to mistake images as real)</span>
            g_loss = self.combined.train_on_batch(noise, valid)
            <span class="hljs-comment"># Plot the progress</span>
            <span class="hljs-built_in">print</span> (<span class="hljs-string">"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]"</span> % (epoch, d_loss[<span class="hljs-number">0</span>], <span class="hljs-number">100</span>*d_loss[<span class="hljs-number">1</span>], g_loss))
            <span class="hljs-comment"># If at save interval =&gt; save generated image samples</span>
            <span class="hljs-keyword">if</span> epoch % save_interval == <span class="hljs-number">0</span>:
                self.save_imgs(epoch)
</code></pre>
<p class="normal">Lastly, we need a helper function to save images:</p>
<pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">save_imgs</span>(<span class="hljs-params">self, epoch</span>):
        r, c = <span class="hljs-number">5</span>, <span class="hljs-number">5</span>
        noise = np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (r * c, self.latent_dim))
        gen_imgs = self.generator.predict(noise)
        <span class="hljs-comment"># Rescale images 0 - 1</span>
        gen_imgs = <span class="hljs-number">0.5</span> * gen_imgs + <span class="hljs-number">0.5</span>
        fig, axs = plt.subplots(r, c)
        cnt = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(r):
            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(c):
                axs[i,j].imshow(gen_imgs[cnt, :,:,<span class="hljs-number">0</span>], cmap=<span class="hljs-string">'gray'</span>)
                axs[i,j].axis(<span class="hljs-string">'off'</span>)
                cnt += <span class="hljs-number">1</span>
        fig.savefig(<span class="hljs-string">"images/dcgan_mnist_%d.png"</span> % epoch)
        plt.close()
</code></pre>
<p class="normal">Let us now train <a id="_idIndexMarker947"/>our GAN:</p>
<pre class="programlisting code"><code class="hljs-code">dcgan = DCGAN(<span class="hljs-number">28</span>,<span class="hljs-number">28</span>,<span class="hljs-number">1</span>)
dcgan.train(epochs=<span class="hljs-number">5000</span>, batch_size=<span class="hljs-number">256</span>, save_interval=<span class="hljs-number">50</span>)
</code></pre>
<p class="normal">The images generated by our GAN as it learned to fake handwritten digits are:</p>
<figure class="mediaobject"><img alt="" height="213" src="../Images/B18331_09_06.png" width="879"/> </figure>
<p class="packt_figref">Figure 9.6: Images generated by GAN – initial attempt</p>
<p class="normal">The preceding images were<a id="_idIndexMarker948"/> the initial attempts by the GAN. As it learned through the following 10 epochs, the quality of digits generated improved manyfold:</p>
<figure class="mediaobject"><img alt="" height="196" src="../Images/B18331_09_07.png" width="825"/> </figure>
<p class="packt_figref">Figure 9.7: Images generated by GAN after 6, 8, and 10 epochs</p>
<p class="normal">The complete code is available in <code class="inlineCode">DCGAN.ipynb</code> in the GitHub repo. We can take the concepts discussed here and apply them to images in other domains. One of the interesting works on images was reported in the paper, <em class="italic">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em>, Alec Radford, Luke Metz, Soumith Chintala, 2015. Quoting the abstract:</p>
<blockquote class="packt_quote">
<p class="quote">In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.</p>
<p class="cite">—Radford et al., 2015</p>
</blockquote>
<p class="normal">Following are some of the<a id="_idIndexMarker949"/> interesting results of applying DCGANs to a celebrity image dataset:</p>
<figure class="mediaobject"><img alt="A collage of a person's face  Description automatically generated" height="441" src="../Images/B18331_09_08_1.png" width="793"/></figure>
<figure class="mediaobject"><img alt="A collage of a person's face  Description automatically generated" height="459" src="../Images/B18331_09_08_2.png" width="826"/></figure>
<p class="packt_figref">Figure 9.8: Generated celebrity images using DCGAN</p>
<p class="normal">Another interesting paper is <em class="italic">Semantic Image Inpainting with Perceptual and Contextual Losses</em>, by Raymond A. Yeh et al. in 2016. Just as content-aware fill is a tool used by photographers to fill in unwanted<a id="_idIndexMarker950"/> or missing parts of images, in this paper they used a DCGAN for image completion.</p>
<p class="normal">As mentioned earlier, a lot of research is happening around GANs. In the next section, we will explore some of the interesting GAN architectures proposed in recent years.</p>
<h1 class="heading-1" id="_idParaDest-252">Some interesting GAN architectures</h1>
<p class="normal">Since their inception, a lot<a id="_idIndexMarker951"/> of interest has been generated in GANs, and as a result, we are seeing a lot of modifications and experimentation with GAN training, architecture, and applications. In this section, we will explore some interesting GANs proposed in recent years.</p>
<h2 class="heading-2" id="_idParaDest-253">SRGAN</h2>
<p class="normal">Remember seeing a crime thriller<a id="_idIndexMarker952"/> where our hero asks the computer guy to <a id="_idIndexMarker953"/>magnify the faded image of the crime scene? With the zoom, we can see the criminal’s face in detail, including the weapon used and anything engraved upon it! Well, <strong class="keyWord">Super Resolution GANs</strong> (<strong class="keyWord">SRGANs</strong>) can perform similar magic. Magic in the sense that because GANs show that it is possible to get high-resolution images, the final results depend on the camera resolution used. Here, a GAN is trained in such a way that it can generate a photorealistic high-resolution image when given a low-resolution image. The SRGAN architecture consists of three neural networks: a very deep generator network (which uses Residual modules; see ResNets in <em class="chapterRef">Chapter 20</em>, <em class="italic">Advanced Convolutional Neural Networks</em>), a discriminator network, and a pretrained VGG-16 network.</p>
<p class="normal">SRGANs use the perceptual loss function (developed by Johnson et al; you can find the link to the paper in the <em class="italic">References</em> section). In SRGAN, the authors first downsampled a high-resolution image and used the generator to get its “high-resolution” version. The discriminator was trained to differentiate between the real high-resolution image and the generated high-resolution image. The difference in the feature map activations in high layers of a VGG network between the network output and the high-resolution parts comprises the perceptual loss function. Besides perceptual loss, the authors further added content loss and an adversarial loss so that images generated look more natural and the finer details more artistic. The perceptual loss is defined as the weighted sum of the content loss and adversarial loss:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_09_001.png" style="height: 1.25em !important;" width="392"/></p>
<p class="normal">The first term on the right-hand side is the content loss, obtained using the feature maps generated by pretrained VGG 19. Mathematically, it is the Euclidean distance between the feature map of the reconstructed image (that is, the one generated by the generator) and the original high-resolution reference image. The second term on the RHS is the adversarial loss. It is the standard generative loss term, designed to ensure that images generated by the generator can fool the discriminator. You can see in the following figure that the image generated by the SRGAN is much closer to the original high-resolution image with a PSNR value of 37.61:</p>
<figure class="mediaobject"><img alt="" height="279" src="../Images/B18331_09_09.png" width="794"/></figure>
<p class="packt_figref">Figure 9.9: An example following the paper Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, Ledig et al.</p>
<p class="normal">Another noteworthy<a id="_idIndexMarker954"/> architecture is CycleGAN; proposed in 2017, it can perform the task of image translation. Once trained<a id="_idIndexMarker955"/> you can translate an image from one domain to another domain. For example, when trained on a horse and zebra dataset, if you give it an image with horses in the foreground, the CycleGAN can convert the horses to zebras with the same background. We will explore it next.</p>
<h2 class="heading-2" id="_idParaDest-254">CycleGAN</h2>
<p class="normal">Have you ever imagined how some <a id="_idIndexMarker956"/>scenery would look if Van Gogh or Manet had painted it? We have many scenes and landscapes painted by Van Gogh/Manet, but<a id="_idIndexMarker957"/> we do not have any collection of input-output pairs. A CycleGAN performs the image translation, that is, transfers an image given in one domain (scenery, for example) to another domain (a Van Gogh painting of the same scene, for instance) in the absence of training examples. The CycleGAN’s ability to perform image translation in the absence of training pairs is what makes it unique.</p>
<p class="normal">To achieve image translation, the authors used a very simple yet effective procedure. They made use of two GANs, the generator of each GAN performing the image translation from one domain to another.</p>
<p class="normal">To elaborate, let us say the input is <em class="italic">X</em>, then the generator of the first GAN performs a mapping <img alt="" height="42" src="../Images/B18331_09_002.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="150"/>; thus, its output would be <em class="italic">Y = G(X)</em>. The generator of the second GAN performs an inverse mapping <img alt="" height="42" src="../Images/B18331_09_003.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="150"/>, resulting in <em class="italic">X = F(Y)</em>. Each discriminator is trained to distinguish between real images and synthesized images. The idea is shown as follows:</p>
<figure class="mediaobject"><img alt="A picture containing text, clock, watch  Description automatically generated" height="230" src="../Images/B18331_09_10.png" width="657"/></figure>
<p class="packt_figref">Figure 9.10: Cycle-consistency loss </p>
<p class="normal">To train the combined GANs, the <a id="_idIndexMarker958"/>authors added, besides the conventional GAN adversarial loss, a forward cycle-consistency loss (left figure) and a backward <a id="_idIndexMarker959"/>cycle-consistency loss (right figure). This ensures that if an image <em class="italic">X</em> is given as input, then after the two translations <em class="italic">F(G(X)) ~ X</em> the obtained image is the same, <em class="italic">X</em> (similarly the backward cycle-consistency loss ensures that <em class="italic">G(F(Y)) ~ Y</em>).</p>
<p class="normal">Following are some of the successful image translations by CycleGANs [7]:</p>
<figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" height="466" src="../Images/B18331_09_11.png" width="421"/></figure>
<p class="packt_figref">Figure 9.11: Examples of some successful CycleGAN image translations</p>
<p class="normal">Following are a few more <a id="_idIndexMarker960"/>examples; you can see the translation of seasons (summer <img alt="" height="38" src="../Images/B18331_09_004.png" style="height: 0.95em !important; vertical-align: 0.01em !important;" width="33"/> winter), photo <img alt="" height="38" src="../Images/B18331_09_004.png" style="height: 0.95em !important; vertical-align: 0.01em !important;" width="33"/> painting and vice versa, and horses <img alt="" height="38" src="../Images/B18331_09_004.png" style="height: 0.95em !important; vertical-align: 0.01em !important;" width="33"/> zebras and vice versa [7]:</p>
<figure class="mediaobject"><img alt="A picture containing text, screenshot, display, different  Description automatically generated" height="399" src="../Images/B18331_09_12.png" width="826"/></figure>
<p class="packt_figref">Figure 9.12: Further examples of CycleGAN translations</p>
<p class="normal">Later in the chapter, we<a id="_idIndexMarker961"/> will also explore a TensorFlow implementation of CycleGANs. Next, we talk about the InfoGAN, a conditional GAN where the <a id="_idIndexMarker962"/>GAN not only generates an image, but you also have a control variable to control the images generated.</p>
<h2 class="heading-2" id="_idParaDest-255">InfoGAN</h2>
<p class="normal">The GAN architectures<a id="_idIndexMarker963"/> that we have considered up to now provide us with little or no control over the generated images. The InfoGAN changes this; it provides control over various attributes of the<a id="_idIndexMarker964"/> images generated. The InfoGAN uses the concepts from information theory such that the noise term is transformed into latent code that provides predictable and systematic control over the output.</p>
<p class="normal">The generator in an InfoGAN takes two inputs: the latent space <em class="italic">Z</em> and a latent code <em class="italic">c</em>, thus the output of the generator is <em class="italic">G(Z,c)</em>. The GAN is trained such that it maximizes the mutual information between the latent code <em class="italic">c</em> and the generated image <em class="italic">G(Z,c)</em>. The following figure shows the architecture of the InfoGAN:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="389" src="../Images/B18331_09_13.png" width="684"/></figure>
<p class="packt_figref">Figure 9.13: The architecture of the InfoGAN, visualized</p>
<p class="normal">The concatenated vector <em class="italic">(Z,c)</em> is fed to the generator. <em class="italic">Q(c|X)</em> is also a neural network. Combined with the generator, it works to form a mapping between random noise <em class="italic">Z</em> and its latent code <em class="italic">c_hat</em>. It aims to<a id="_idIndexMarker965"/> estimate <em class="italic">c</em> given <em class="italic">X</em>. This is achieved by adding a regularization term to the objective function of the conventional GAN:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_09_007.png" style="height: 1.25em !important;" width="808"/></p>
<p class="normal">The term <em class="italic">V</em><sub class="subscript">G</sub><em class="italic">(D,G)</em> is the loss function of the conventional GAN, and the second term is the regularization term, where <img alt="" height="42" src="../Images/B18331_09_008.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="21"/> is a constant. Its value was set to 1 in the paper, and <em class="italic">I(c;G(Z,c))</em> is the mutual information <a id="_idIndexMarker966"/>between the latent code <em class="italic">c</em> and the generator-generated image <em class="italic">G(Z,c)</em>.</p>
<p class="normal">Following are the exciting results of the InfoGAN on the MNIST dataset:</p>
<figure class="mediaobject"><img alt="" height="387" src="../Images/B18331_09_14.png" width="781"/></figure>
<p class="packt_figref">Figure 9.14: Results of using the InfoGAN on the MNIST dataset. Here, different rows correspond to different random samples of fixed latent codes and noise </p>
<p class="normal">Now, that we have seen some <a id="_idIndexMarker967"/>exciting GAN architectures, let us explore some cool applications of GAN.</p>
<h1 class="heading-1" id="_idParaDest-256">Cool applications of GANs</h1>
<p class="normal">We have seen that the<a id="_idIndexMarker968"/> generator can learn how to forge data. This means that it learns how to create new synthetic data that is created by the network that appears to be authentic and human-made. Before going into the details of some GAN code, we would like to share the results of the paper [6] (code is available online at <a href="https://github.com/hanzhanggit/StackGAN"><span class="url">https://github.com/hanzhanggit/StackGAN</span></a>) where a GAN has been used to synthesize forged images starting from a text description. The results are impressive: the first column is the real image in the test set and all the rest of the columns are the images generated from the same text description by Stage-I and Stage-II of StackGAN. More examples are available on YouTube (<a href="https://www.youtube.com/watch?v=SuRyL5vhCIM&amp;feature=youtu.be"><span class="url">https://www.youtube.com/watch?v=SuRyL5vhCIM&amp;feature=youtu.be</span></a>):</p>
<figure class="mediaobject"><img alt="A picture containing text, bird, outdoor, standing  Description automatically generated" height="229" src="../Images/B18331_09_15.png" width="826"/></figure>
<p class="packt_figref">Figure 9.15: Image generation of birds, using GANs</p>
<figure class="mediaobject"><img alt="A group of flowers  Description automatically generated with low confidence" height="229" src="../Images/B18331_09_16.png" width="826"/></figure>
<p class="packt_figref">Figure 9.16: Image generation of flowers, using GANs</p>
<p class="normal">Now let us see how a GAN can learn to “forge” the MNIST dataset. In this case, it is a combination of GAN and CNNs used for the generator and discriminator networks. In the beginning, the <a id="_idIndexMarker969"/>generator creates nothing understandable, but after a few iterations, synthetic forged numbers are progressively clearer and clearer. In this image, the panels are ordered by increasing training epochs and you can see the quality improving among the panels:</p>
<figure class="mediaobject"><img alt="A picture containing text, furniture  Description automatically generated" height="277" src="../Images/B18331_09_17.png" width="826"/></figure>
<p class="packt_figref">Figure 9.17: Illegible initial outputs of the GAN</p>
<p class="normal">As the training<a id="_idIndexMarker970"/> progresses, you can see in <em class="italic">Figure 9.17</em> that the digits start taking a more recognizable form:</p>
<figure class="mediaobject"><img alt="A picture containing text, computer, keyboard, electronics  Description automatically generated" height="282" src="../Images/B18331_09_18.png" width="826"/></figure>
<p class="packt_figref">Figure 9.18: Improved outputs of the GAN, following further iterations</p>
<figure class="mediaobject"><img alt="A picture containing keyboard, computer, desk, electronics  Description automatically generated" height="281" src="../Images/B18331_09_19.png" width="825"/></figure>
<p class="packt_figref">Figure 9.19: Final outputs of the GAN, showing significant improvement from previous iterations</p>
<p class="normal">After 10,000 epochs, you can see that the handwritten digits are even more realistic. </p>
<p class="normal">One of the coolest uses <a id="_idIndexMarker971"/>of GANs is doing arithmetic on faces in the generator’s vector <em class="italic">Z</em>. In other words, if we stay in the space of synthetic forged images, it is possible to see things like this: <em class="italic">[smiling woman] - [neutral woman] + [neutral man] = [smiling man]</em>, or like this: <em class="italic">[man with glasses] - [man without glasses] + [woman without glasses] = [woman with glasses]</em>. This was shown in the paper <em class="italic">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em> by Alec Radford and his colleagues in 2015. All images in this work are generated by a version of GAN. They are NOT REAL. The full paper is available here: <a href="http://arxiv.org/abs/1511.06434"><span class="url">http://arxiv.org/abs/1511.06434</span></a>. Following are some examples from the paper. The authors also share their code in this GitHub repo: <a href="https://github.com/Newmu/dcgan_code"><span class="url">https://github.com/Newmu/dcgan_code</span></a>:</p>
<figure class="mediaobject"><img alt="A collage of a person's face  Description automatically generated with medium confidence" height="858" src="../Images/B18331_09_20.png" width="826"/></figure>
<p class="packt_figref">Figure 9.20: Image arithmetic using GANs</p>
<p class="normal"><strong class="keyWord">Bedrooms</strong>: Generated<a id="_idIndexMarker972"/> bedrooms after five epochs of training:</p>
<figure class="mediaobject"><img alt="A collage of a house  Description automatically generated with low confidence" height="362" src="../Images/B18331_09_21.png" width="724"/></figure>
<p class="packt_figref">Figure 9.21: Generated bedrooms using GAN after 5 epochs of training</p>
<p class="normal"><strong class="keyWord">Album covers</strong>: These images are<a id="_idIndexMarker973"/> generated by the GAN, but look like authentic album covers:</p>
<figure class="mediaobject"><img alt="A picture containing text, different, bunch, booth  Description automatically generated" height="654" src="../Images/B18331_09_22.png" width="654"/></figure>
<p class="packt_figref">Figure 9.22: Album covers generated using DCGAN</p>
<p class="normal">Another cool application of GANs<a id="_idIndexMarker974"/> is the generation of artificial faces. NVIDIA introduced a model in 2018, which it named StyleGAN (the second version, StyleGAN2, was released in February 2020, and the third version in 2021), which it showed can be used to generate realistic-looking images of people. Below you can see some of the realistic-looking fake people’s faces generated by StyleGAN obtained after training of 1,000 epochs; for better results, you will need to train more:</p>
<figure class="mediaobject"><img alt="" height="113" src="../Images/B18331_09_23.png" width="809"/></figure>
<p class="packt_figref">Figure 9.23: Fake faces generated by StyleGAN</p>
<p class="normal">Not only does it generate fake images but like InfoGAN, you can control the features from coarse to grain. This is the official video released by NVIDIA showing how features affect the results: <a href="https://www.youtube.com/watch?v=kSLJriaOumA"><span class="url">https://www.youtube.com/watch?v=kSLJriaOumA</span></a>. They were able to do this by adding a non-linear mapping network after the Latent variable <em class="italic">Z</em>. The mapping network transformed the latent variable to a mapping of the same size; the output of the mapping vector is<a id="_idIndexMarker975"/> fed to different layers of the generator network, and this allows the StyleGAN to control different visual features. To know more about StyleGAN, you should read the paper <em class="italic">A style-based generator architecture for Generative Adversarial Networks</em> from NVIDIA Labs [10].</p>
<h1 class="heading-1" id="_idParaDest-257">CycleGAN in TensorFlow</h1>
<p class="normal">In this section, we will<a id="_idIndexMarker976"/> implement a CycleGAN in TensorFlow. The CycleGAN requires a special dataset, a paired dataset, from one domain of images to <a id="_idIndexMarker977"/>another domain. So, besides the necessary modules, we will use <code class="inlineCode">tensorflow_datasets</code> as well. Also, we will make use of the library <code class="inlineCode">tensorflow_examples</code>, we will directly use the generator and the discriminator from the <code class="inlineCode">pix2pix</code> model defined in <code class="inlineCode">tensorflow_examples</code>. The code here is adapted from the code here <a href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb"><span class="url">https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb</span></a>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
<span class="hljs-keyword">from</span> tensorflow_examples.models.pix2pix <span class="hljs-keyword">import</span> pix2pix
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> clear_output
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
<p class="normal">TensorFlow’s <code class="inlineCode">Dataset</code> API contains a list of datasets. It has many paired datasets for CycleGANs, such as horse to zebra, apples to oranges, and so on. You can access the complete list here: <a href="https://www.tensorflow.org/datasets/catalog/cycle_gan"><span class="url">https://www.tensorflow.org/datasets/catalog/cycle_gan</span></a>. For our code, we will be using <code class="inlineCode">summer2winter_yosemite</code>, which contains images of Yosemite (USA) in summer (Dataset A) and <a id="_idIndexMarker978"/>winter (Dataset B). We will train the CycleGAN to convert an input image of summer to winter and vice versa. </p>
<p class="normal">Let us load the <a id="_idIndexMarker979"/>data and get train and test images:</p>
<pre class="programlisting code"><code class="hljs-code">dataset, metadata = tfds.load(<span class="hljs-string">'cycle_gan/summer2winter_yosemite'</span>,
                              with_info=<span class="hljs-literal">True</span>, as_supervised=<span class="hljs-literal">True</span>)
train_summer, train_winter = dataset[<span class="hljs-string">'trainA'</span>], dataset[<span class="hljs-string">'trainB'</span>]
test_summer, test_winter = dataset[<span class="hljs-string">'testA'</span>], dataset[<span class="hljs-string">'testB'</span>]
</code></pre>
<p class="normal">We need to set some hyperparameters:</p>
<pre class="programlisting code"><code class="hljs-code">BUFFER_SIZE = <span class="hljs-number">1000</span>
BATCH_SIZE = <span class="hljs-number">1</span>
IMG_WIDTH = <span class="hljs-number">256</span>
IMG_HEIGHT = <span class="hljs-number">256</span>
EPOCHS = <span class="hljs-number">100</span>
LAMBDA = <span class="hljs-number">10</span>
AUTOTUNE = tf.data.AUTOTUNE
</code></pre>
<p class="normal">The images need to be normalized before we train the network. For better performance, we also add random jittering to the train images; the images are first resized to size 286x286, then we randomly crop them back to the size 256x256, and finally apply the random jitter:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">normalize</span>(<span class="hljs-params">input_image, label</span>):
    input_image = tf.cast(input_image, tf.float32)
    input_image = (input_image / <span class="hljs-number">127.5</span>) - <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> input_image
<span class="hljs-keyword">def</span> <span class="hljs-title">random_crop</span>(<span class="hljs-params">image</span>):
    cropped_image = tf.image.random_crop(image, size=[IMG_HEIGHT,
    IMG_WIDTH, <span class="hljs-number">3</span>])
    <span class="hljs-keyword">return</span> cropped_image
<span class="hljs-keyword">def</span> <span class="hljs-title">random_jitter</span>(<span class="hljs-params">image</span>):
    <span class="hljs-comment"># resizing to 286 x 286 x 3</span>
    image = tf.image.resize(image, [<span class="hljs-number">286</span>, <span class="hljs-number">286</span>],
    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    <span class="hljs-comment"># randomly cropping to 256 x 256 x 3</span>
    image = random_crop(image)
    <span class="hljs-comment"># random mirroring</span>
    image = tf.image.random_flip_left_right(image)
    <span class="hljs-keyword">return</span> image
</code></pre>
<p class="normal">The augmentation (random crop and jitter) is done only to the train images; therefore, we will need to <a id="_idIndexMarker980"/>separate functions for preprocessing the images, one <a id="_idIndexMarker981"/>for train data, and the other for test data:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_image_train</span>(<span class="hljs-params">image, label</span>):
    image = random_jitter(image)
    image = normalize(image)
    <span class="hljs-keyword">return</span> image
<span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_image_test</span>(<span class="hljs-params">image, label</span>):
    image = normalize(image)
    <span class="hljs-keyword">return</span> image
</code></pre>
<p class="normal">The preceding functions, when applied to images, will normalize them in the range [-1,1] and apply augmentation to train images. Let us apply this to our train and test datasets and create a data generator that will provide images for training in batches:</p>
<pre class="programlisting code"><code class="hljs-code">train_summer = train_summer.cache().<span class="hljs-built_in">map</span>(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(
    BUFFER_SIZE).batch(BATCH_SIZE)
train_winter = train_winter.cache().<span class="hljs-built_in">map</span>(
    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(
    BUFFER_SIZE).batch(BATCH_SIZE)
test_summer = test_summer.<span class="hljs-built_in">map</span>(
    preprocess_image_test,
    num_parallel_calls=AUTOTUNE).cache().shuffle(
    BUFFER_SIZE).batch(BATCH_SIZE)
test_winter = test_winter.<span class="hljs-built_in">map</span>(
    preprocess_image_test,
    num_parallel_calls=AUTOTUNE).cache().shuffle(
    BUFFER_SIZE).batch(BATCH_SIZE)
</code></pre>
<p class="normal">In the preceding code, the argument <code class="inlineCode">num_parallel_calls</code> allows one to take benefit from multiple CPU cores in the system; one should set its value to the number of CPU cores in your system. If you are not sure, use the <code class="inlineCode">AUTOTUNE = tf.data.AUTOTUNE</code> value so that TensorFlow<a id="_idIndexMarker982"/> dynamically determines the right number for you.</p>
<p class="normal">As mentioned in the <a id="_idIndexMarker983"/>beginning, we use a generator and discriminator from the <code class="inlineCode">pix2pix</code> model defined in the <code class="inlineCode">tensorflow_examples</code> module. We will have two generators and two discriminators:</p>
<pre class="programlisting code"><code class="hljs-code">OUTPUT_CHANNELS = <span class="hljs-number">3</span>
generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type=<span class="hljs-string">'instancenorm'</span>)
generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type=<span class="hljs-string">'instancenorm'</span>)
discriminator_x = pix2pix.discriminator(norm_type=<span class="hljs-string">'instancenorm'</span>, target=<span class="hljs-literal">False</span>)
discriminator_y = pix2pix.discriminator(norm_type=<span class="hljs-string">'instancenorm'</span>, target=<span class="hljs-literal">False</span>)
</code></pre>
<p class="normal">Before moving ahead with the model definition, let us see the images. Each image is processed before plotting so that its intensity is normal:</p>
<pre class="programlisting code"><code class="hljs-code">to_winter = generator_g(sample_summer)
to_summer = generator_f(sample_winter)
plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))
contrast = <span class="hljs-number">8</span>
imgs = [sample_summer, to_winter, sample_winter, to_summer]
title = [<span class="hljs-string">'Summer'</span>, <span class="hljs-string">'To Winter'</span>, <span class="hljs-string">'Winter'</span>, <span class="hljs-string">'To Summer'</span>]
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(imgs)):
  plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, i+<span class="hljs-number">1</span>)
  plt.title(title[i])
  <span class="hljs-keyword">if</span> i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:
    plt.imshow(imgs[i][<span class="hljs-number">0</span>] * <span class="hljs-number">0.5</span> + <span class="hljs-number">0.5</span>)
  <span class="hljs-keyword">else</span>:
    plt.imshow(imgs[i][<span class="hljs-number">0</span>] * <span class="hljs-number">0.5</span> * contrast + <span class="hljs-number">0.5</span>)
plt.show()
</code></pre>
<figure class="mediaobject"><img alt="A picture containing painted  Description automatically generated" height="694" src="../Images/B18331_09_24.png" width="700"/></figure>
<p class="packt_figref">Figure 9.24: The input of GAN 1 and output of GAN 2 in CycleGAN architecture before training</p>
<p class="normal">We next <a id="_idIndexMarker984"/>define the loss and<a id="_idIndexMarker985"/> optimizers. We retain the same loss functions for generator and discriminator as we did in DCGAN:</p>
<pre class="programlisting code"><code class="hljs-code">loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">def</span> <span class="hljs-title">discriminator_loss</span>(<span class="hljs-params">real, generated</span>):
    real_loss = loss_obj(tf.ones_like(real), real)
    generated_loss = loss_obj(tf.zeros_like(generated), generated)
    total_disc_loss = real_loss + generated_loss
    <span class="hljs-keyword">return</span> total_disc_loss * <span class="hljs-number">0.5</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">generator_loss</span>(<span class="hljs-params">generated</span>):
    <span class="hljs-keyword">return</span> loss_obj(tf.ones_like(generated), generated)
</code></pre>
<p class="normal">Since there are now <a id="_idIndexMarker986"/>four models, two generators and two discriminators, we need to define four optimizers:</p>
<pre class="programlisting code"><code class="hljs-code">generator_g_optimizer = tf.keras.optimizers.Adam(<span class="hljs-number">2e-4</span>, beta_1=<span class="hljs-number">0.5</span>)
generator_f_optimizer = tf.keras.optimizers.Adam(<span class="hljs-number">2e-4</span>, beta_1=<span class="hljs-number">0.5</span>)
discriminator_x_optimizer = tf.keras.optimizers.Adam(<span class="hljs-number">2e-4</span>, beta_1=<span class="hljs-number">0.5</span>)
discriminator_y_optimizer = tf.keras.optimizers.Adam(<span class="hljs-number">2e-4</span>, beta_1=<span class="hljs-number">0.5</span>)
</code></pre>
<p class="normal">Additionally, in the CycleGAN, we require to define two more loss functions, first the cycle-consistency loss; we can use the same function for forward and backward cycle-consistency loss calculation. The cycle-consistency loss ensures that the result is close to the original input:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_cycle_loss</span>(<span class="hljs-params">real_image, cycled_image</span>):
    loss1 = tf.reduce_mean(tf.<span class="hljs-built_in">abs</span>(real_image - cycled_image))
    <span class="hljs-keyword">return</span> LAMBDA * loss1
</code></pre>
<p class="normal">We also need to define an identity loss, which ensures that if an image <em class="italic">Y</em> is fed to the generator, it would yield the real image <em class="italic">Y</em> or an image similar to <em class="italic">Y</em>. Thus, if we give our summer image generator an image of summer as input, it should not change it much:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">identity_loss</span>(<span class="hljs-params">real_image, same_image</span>):
    loss = tf.reduce_mean(tf.<span class="hljs-built_in">abs</span>(real_image - same_image))
    <span class="hljs-keyword">return</span> LAMBDA * <span class="hljs-number">0.5</span> * loss
</code></pre>
<p class="normal">Now we define the<a id="_idIndexMarker987"/> function that trains the generator and discriminator in a batch, a pair of images at a time. The two discriminators and the two generators are trained via this function with the help of the tape gradient. The<a id="_idIndexMarker988"/> training step can be divided into four parts:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Get the output images<a id="_idIndexMarker989"/> from the two generators.</li>
<li class="numberedList">Calculate the losses.</li>
<li class="numberedList">Calculate the gradients.</li>
<li class="numberedList">And finally, apply the gradients:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">train_step</span>(<span class="hljs-params">real_x, real_y</span>):
    <span class="hljs-comment"># persistent is set to True because the tape is used</span>
<span class="hljs-comment">    # more than</span> <span class="hljs-comment">once to calculate the gradients.</span>
  <span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> tape:
    <span class="hljs-comment"># Generator G translates X -&gt; Y</span>
    <span class="hljs-comment"># Generator F translates Y -&gt; X.</span>
    
    fake_y = generator_g(real_x, training=<span class="hljs-literal">True</span>)
    cycled_x = generator_f(fake_y, training=<span class="hljs-literal">True</span>)
    fake_x = generator_f(real_y, training=<span class="hljs-literal">True</span>)
    cycled_y = generator_g(fake_x, training=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># same_x and same_y are used for identity loss.</span>
    same_x = generator_f(real_x, training=<span class="hljs-literal">True</span>)
    same_y = generator_g(real_y, training=<span class="hljs-literal">True</span>)
    disc_real_x = discriminator_x(real_x, training=<span class="hljs-literal">True</span>)
    disc_real_y = discriminator_y(real_y, training=<span class="hljs-literal">True</span>)
    disc_fake_x = discriminator_x(fake_x, training=<span class="hljs-literal">True</span>)
    disc_fake_y = discriminator_y(fake_y, training=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># calculate the loss</span>
    gen_g_loss = generator_loss(disc_fake_y)
    gen_f_loss = generator_loss(disc_fake_x)
    
    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + \
    calc_cycle_loss(real_y, cycled_y)
    
    <span class="hljs-comment"># Total generator loss = adversarial loss + cycle loss</span>
    total_gen_g_loss = gen_g_loss + total_cycle_loss + \
    identity_loss(real_y, same_y)
    total_gen_f_loss = gen_f_loss + total_cycle_loss + \
    identity_loss(real_x, same_x)
    disc_x_loss = discriminator_loss(disc_real_x,
    disc_fake_x)
    disc_y_loss = discriminator_loss(disc_real_y,
    disc_fake_y)
    <span class="hljs-comment"># Calculate the gradients for generator and discriminator</span>
    generator_g_gradients = tape.gradient(total_gen_g_loss,
    generator_g.trainable_variables)
    generator_f_gradients = tape.gradient(total_gen_f_loss,
    generator_f.trainable_variables)
    discriminator_x_gradients = tape.gradient(disc_x_loss,
    discriminator_x.trainable_variables)
    discriminator_y_gradients = tape.gradient(disc_y_loss,
    discriminator_y.trainable_variables)
    <span class="hljs-comment"># Apply the gradients to the optimizer</span>
    generator_g_optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(generator_g_gradients, generator_g.trainable_variables))
    generator_f_optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(generator_f_gradients, generator_f.trainable_variables))
    discriminator_x_optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(discriminator_x_gradients, discriminator_x.trainable_variables))
    discriminator_y_optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(discriminator_y_gradients, discriminator_y.trainable_variables))
</code></pre>
</li>
</ol>
<p class="normal">We define<a id="_idIndexMarker990"/> checkpoints to save the model weights. Since it <a id="_idIndexMarker991"/>can take a while to train a sufficiently good CycleGAN, we save the checkpoints, and if we start next, we can start with loading the existing checkpoints – this will ensure that model starts learning from where it left:</p>
<pre class="programlisting code"><code class="hljs-code">checkpoint_path = <span class="hljs-string">"./checkpoints/train"</span>
ckpt = tf.train.Checkpoint(generator_g=generator_g,
                           generator_f=generator_f,
                           discriminator_x=discriminator_x,
                           discriminator_y=discriminator_y,
                           generator_g_optimizer=generator_g_optimizer,
generator_f_optimizer=generator_f_optimizer,
discriminator_x_optimizer=discriminator_x_optimizer,
discriminator_y_optimizer=discriminator_y_optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=<span class="hljs-number">5</span>)
<span class="hljs-comment"># if a checkpoint exists, restore the latest checkpoint.</span>
<span class="hljs-keyword">if</span> ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    <span class="hljs-built_in">print</span> (<span class="hljs-string">'Latest checkpoint restored!!'</span>)
</code></pre>
<p class="normal">Let us now combine it<a id="_idIndexMarker992"/> all and train the network for 100<a id="_idIndexMarker993"/> epochs. Please remember that in the paper, the test network was trained for 200 epochs, so our results will not be that good:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):
    start = time.time()
    n = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> image_x, image_y <span class="hljs-keyword">in</span> tf.data.Dataset.<span class="hljs-built_in">zip</span>((train_summer, train_winter)):
        train_step(image_x, image_y)
        <span class="hljs-keyword">if</span> n % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span> (<span class="hljs-string">'.'</span>, end=<span class="hljs-string">''</span>)
        n += <span class="hljs-number">1</span>
    clear_output(wait=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Using a consistent image (sample_summer) so that the progress of</span>
<span class="hljs-comment">    # the model</span> <span class="hljs-comment">is clearly visible.</span>
    generate_images(generator_g, sample_summer)
    <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:
        ckpt_save_path = ckpt_manager.save()
        <span class="hljs-built_in">print</span> (<span class="hljs-string">'Saving checkpoint for epoch {} at {}'</span>.<span class="hljs-built_in">format</span>(epoch+<span class="hljs-number">1</span>,
                                                             ckpt_save_path))
    <span class="hljs-built_in">print</span> (<span class="hljs-string">'Time taken for epoch {} is {} sec\n'</span>.<span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>,
                                                        time.time()-start))
</code></pre>
<p class="normal">You can see some of the images generated by our CycleGAN. Generator <em class="italic">A</em> takes in summer photos and converts them to winter, while generator <em class="italic">B</em> takes in winter photos and converts them to summer:</p>
<figure class="mediaobject"><img alt="A picture containing window, indoor, different  Description automatically generated" height="319" src="../Images/B18331_09_25.png" width="826"/></figure>
<p class="packt_figref">Figure 9.25: Images using CycleGAN after training</p>
<p class="normal">We suggest you <a id="_idIndexMarker994"/>experiment with other datasets in the<a id="_idIndexMarker995"/> TensorFlow CycleGAN datasets. Some will be easy like apples and oranges, but some will require much more training. The authors also maintain a GitHub repository where they have shared their own implementation in PyTorch along with the links to implementations in other frameworks including TensorFlow: <a href="https://github.com/junyanz/CycleGAN"><span class="url">https://github.com/junyanz/CycleGAN</span></a>.</p>
<h1 class="heading-1" id="_idParaDest-258">Flow-based models for data generation</h1>
<p class="normal">While both VAEs (<em class="chapterRef">Chapter 8</em>, <em class="italic">Autoencoders</em>) and GANs do a good job of data generation, they do not explicitly learn the probability density function of the input data. GANs learn by converting the unsupervised <a id="_idIndexMarker996"/>problem to a<a id="_idIndexMarker997"/> supervised learning problem. </p>
<p class="normal">VAEs try to learn by optimizing the maximum log-likelihood of the data by maximizing the <strong class="keyWord">Evidence Lower Bound</strong> (<strong class="keyWord">ELBO</strong>). Flow-based models differ from the two in that they explicitly learn data distribution <img alt="" height="50" src="../Images/B18331_09_009.png" style="height: 1.25em !important; vertical-align: -0.30em !important;" width="75"/>. This offers an advantage over VAEs and GANs, because this makes it possible to use flow-based models for tasks like filling incomplete data, sampling data, and even identifying bias in data distributions. Flow-based models accomplish this by maximizing the log-likelihood estimation. To understand how, let us delve a little into its math.</p>
<p class="normal">Let <img alt="" height="50" src="../Images/B18331_09_010.png" style="height: 1.25em !important; vertical-align: -0.30em !important;" width="96"/> be the probability density of data <em class="italic">D</em>, and let <img alt="" height="50" src="../Images/B18331_09_011.png" style="height: 1.25em !important; vertical-align: -0.30em !important;" width="100"/> be the probability density approximated by our model <em class="italic">M</em>. The goal of a flow-based model is to find the model parameters <img alt="" height="42" src="../Images/B18331_09_012.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="38"/> such that<a id="_idIndexMarker998"/> the distance between<a id="_idIndexMarker999"/> two is minimum, i.e.:</p>
<p class="center"><img alt="" height="63" src="../Images/B18331_09_013.png" style="height: 1.57em !important; vertical-align: 0.05em !important;" width="633"/></p>
<p class="normal">If we use the KL divergence as our distance metrics, the expression above reduces to:</p>
<p class="center"><img alt="" height="63" src="../Images/B18331_09_015.png" style="height: 1.57em !important; vertical-align: 0.05em !important;" width="517"/></p>
<p class="normal">This equation represents minimizing the <strong class="keyWord">Negative Log-Likelihood </strong>(<strong class="keyWord">NLL</strong>) (equivalent to maximizing log-likelihood estimation.)</p>
<p class="normal">The basic architecture of flow-based models consists of a series of invertible functions, as shown in the figure below. The challenge is to find the function <em class="italic">f(x)</em>, such that its inverse <em class="italic">f</em><sup class="superscript">-1</sup><em class="italic">(x)</em> generates <em class="italic">x</em>’, the reconstructed version of the input <em class="italic">x</em>:</p>
<figure class="mediaobject"><img alt="" height="174" src="../Images/B18331_09_26.png" width="827"/></figure>
<p class="packt_figref">Figure 9.26: Architecture of flow-based model</p>
<p class="normal">There are mainly two ways flow-based models are implemented:</p>
<ul>
<li class="bulletList">Normalized Flow: Here, the basic idea is to use a series of simple invertible functions to transform the complex input. As we flow through the sequence of transformations, we repeatedly substitute the variable with a new one, as per the change of variables theorem (<a href="https://archive.lib.msu.edu/crcmath/math/math/c/c210.htm"><span class="url">https://archive.lib.msu.edu/crcmath/math/math/c/c210.htm</span></a>), and finally, we obtain a probability distribution of the target variable. The path that the variables z<sub class="subscript">i</sub> traverse is the flow and the complete chain formed by the successive distributions is called the normalizing<a id="_idIndexMarker1000"/> flow. 
    <p class="normal">The <strong class="keyWord">RealNVP</strong> (<strong class="keyWord">Real-valued Non-Volume Preserving</strong>) model proposed by Dinh et al., 2017, <strong class="keyWord">NICE</strong> (<strong class="keyWord">Non-linear Independent Components Estimation</strong>) by Dinh et al., 2015, and <a id="_idIndexMarker1001"/>Glow by Knigma and Dhariwal, 2018, use the normalized flow trick:</p>
<figure class="mediaobject"><img alt="A picture containing diagram  Description automatically generated" height="205" src="../Images/B18331_09_27.png" width="640"/></figure>
<p class="packt_figref">Figure 9.27: Normalizing flow model: https://lilianweng.github.io/posts/2018-10-13-flow-models/</p>
</li>
</ul>
<ul>
<li class="bulletList">Autoregressive Flow: Models like <strong class="keyWord">MADE</strong> (<strong class="keyWord">Masked Autoencoder for Distribution Estimation</strong>), PixelRNN, and wavenet are<a id="_idIndexMarker1002"/> based on autoregressive <a id="_idIndexMarker1003"/>models. Here, each dimension in a vector variable is dependent on the previous dimensions. Thus, the probability of observing <img alt="" height="46" src="../Images/B18331_09_016.png" style="height: 1.15em !important; vertical-align: -0.20em !important;" width="29"/> depends only on <img alt="" height="46" src="../Images/B18331_09_017.png" style="height: 1.15em !important; vertical-align: -0.20em !important;" width="217"/>, and therefore, the product of these conditional<a id="_idIndexMarker1004"/> probabilities gives us the probability of the entire sequence.</li>
</ul>
<p class="normal">Lilian Weng’s blog (<a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/"><span class="url">https://lilianweng.github.io/posts/2018-10-13-flow-models/</span></a>) provides a very good description of flow-based models.</p>
<h1 class="heading-1" id="_idParaDest-259">Diffusion models for data generation</h1>
<p class="normal">The 2021 <a id="_idIndexMarker1005"/>paper <em class="italic">Diffusion Models Beat GANs on Image synthesis</em> by two OpenAI research scientists Prafulla <a id="_idIndexMarker1006"/>Dhariwal and Alex Nichol garnered a lot of interest in diffusion models for data generation. </p>
<p class="normal">Using<a id="_idIndexMarker1007"/> the <strong class="keyWord">Frechet Inception Distance</strong> (<strong class="keyWord">FID</strong>) as the metrics for evaluation of generated images, they were able to achieve an FID score of 3.85 on a diffusion model trained on ImageNet data:</p>
<figure class="mediaobject"><img alt="A collage of animals  Description automatically generated with medium confidence" height="301" src="../Images/B18331_09_28.png" width="603"/></figure>
<p class="packt_figref">Figure 9.28: Selected samples of images generated from ImageNet (FID 3.85). Image Source: Dhariwal, Prafulla, and Alexander Nichol. “Diffusion models beat GANs on image synthesis.” <em class="italic">Advances in Neural Information Processing Systems</em> 34 (2021)</p>
<p class="normal">The idea behind<a id="_idIndexMarker1008"/> diffusion models is very simple. We take our input image <img alt="" height="46" src="../Images/B18331_09_018.png" style="height: 1.15em !important; vertical-align: -0.20em !important;" width="38"/>, and <a id="_idIndexMarker1009"/>at each time step (forward step), we add a Gaussian noise to it (diffusion of noise) such that after <img alt="" height="42" src="../Images/B18331_09_019.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="25"/> time steps, the original image is no longer decipherable. And then find a model that can, starting from a noisy input, perform the reverse diffusion and generate a clear image:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="166" src="../Images/B18331_09_29.png" width="879"/></figure>
<p class="packt_figref">Figure 9.29: Graphical model as a Markov chain for the forward and reverse diffusion process</p>
<p class="normal">The only problem is that while the conditional probabilities <img alt="" height="50" src="../Images/B18331_09_020.png" style="height: 1.25em !important; vertical-align: -0.30em !important;" width="171"/> can be obtained using the reparameterization trick, the reverse conditional probability <img alt="" height="50" src="../Images/B18331_09_021.png" style="height: 1.25em !important; vertical-align: -0.30em !important;" width="171"/> is unknown. We train a neural network model <img alt="" height="46" src="../Images/B18331_09_022.png" style="height: 1.15em !important; vertical-align: -0.20em !important;" width="38"/> to approximate these conditional probabilities. Below is the training and the sampling algorithm used by Ho et al., 2020, in their <em class="italic">Denoising Diffusion Probabilistic Models</em> paper:</p>
<table class="table-container" id="table001-3">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Algorithm 1 Training</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Algorithm 2 Sampling</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">1. <strong class="keyWord">repeat</strong></p>
<p class="normal">2. <img alt="" height="46" src="../Images/B18331_09_023.png" style="height: 1.15em !important; vertical-align: -0.25em !important;" width="167"/></p>
<p class="normal">3. <img alt="" height="46" src="../Images/B18331_09_024.png" style="height: 1.15em !important; vertical-align: -0.25em !important;" width="367"/></p>
<p class="normal">4. <img alt="" height="46" src="../Images/B18331_09_025.png" style="height: 1.15em !important; vertical-align: -0.25em !important;" width="179"/></p>
<p class="normal">5. Take gradient descent step on <img alt="" height="67" src="../Images/B18331_09_026.png" style="height: 1.68em !important; vertical-align: 0.02em !important;" width="567"/></p>
<p class="normal">6.<strong class="keyWord"> until</strong> converged</p>
</td>
<td class="table-cell">
<p class="normal">1. <img alt="" height="46" src="../Images/B18331_09_027.png" style="height: 1.15em !important; vertical-align: -0.25em !important;" width="200"/></p>
<p class="normal">2.<strong class="keyWord"> for</strong> <em class="italic">t</em> = <em class="italic">T</em>, ..., 1 <strong class="keyWord">do</strong></p>
<p class="normal">3. <img alt="" height="46" src="../Images/B18331_09_028.png" style="height: 1.15em !important; vertical-align: -0.25em !important;" width="479"/></p>
<p class="normal">4. <img alt="" height="104" src="../Images/B18331_09_029.png" style="height: 2.60em !important; vertical-align: -0.96em !important;" width="613"/></p>
<p class="normal">5.<strong class="keyWord"> end for</strong></p>
<p class="normal">6.<strong class="keyWord"> return x</strong><sub class="bold">0</sub> </p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 9.1: Training and sampling steps used by Ho et al., 2020</p>
<p class="normal">Diffusion models offer both<a id="_idIndexMarker1010"/> tractability and flexibility – two conflicting objectives in generative models. However, they rely on a long Markov chain of diffusion steps and thus are <a id="_idIndexMarker1011"/>computationally expensive. There is a lot of traction in diffusion models, and we hope that in the near future there will be algorithms that can give as fast sampling as GANs.</p>
<h1 class="heading-1" id="_idParaDest-260">Summary</h1>
<p class="normal">This chapter explored one of the most exciting deep neural networks of our times: GANs. Unlike discriminative networks, GANs have the ability to generate images based on the probability distribution of the input space. We started with the first GAN model proposed by Ian Goodfellow and used it to generate handwritten digits. We next moved to DCGANs where convolutional neural networks were used to generate images and we saw the remarkable pictures of celebrities, bedrooms, and even album artwork generated by DCGANs. Finally, the chapter delved into some awesome GAN architectures: the SRGAN, CycleGAN, InfoGAN, and StyleGAN. The chapter also included an implementation of the CycleGAN in TensorFlow 2.0.</p>
<p class="normal">In this chapter and the ones before it, we have been continuing with different unsupervised learning models, with both autoencoders and GANs examples of self-supervised learning; the next chapter will further detail the difference between self-supervised, joint, and contrastive learning.</p>
<h1 class="heading-1" id="_idParaDest-261">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Goodfellow, Ian J. (2014). <em class="italic">On Distinguishability Criteria for Estimating Generative Models</em>. arXiv preprint arXiv:1412.6515: <a href="https://arxiv.org/pdf/1412.6515.pdf"><span class="url">https://arxiv.org/pdf/1412.6515.pdf</span></a></li>
<li class="numberedList">Dumoulin, Vincent, and Visin, Francesco. (2016). <em class="italic">A guide to convolution arithmetic for deep learning</em>. arXiv preprint arXiv:1603.07285: <a href="https://arxiv.org/abs/1603.07285"><span class="url">https://arxiv.org/abs/1603.07285</span></a></li>
<li class="numberedList">Salimans, Tim, et al. (2016). <em class="italic">Improved Techniques for Training GANs</em>. Advances in neural information processing systems: <a href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf%20"><span class="url">http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf</span></a></li>
<li class="numberedList">Johnson, Justin, Alahi, Alexandre, and Fei-Fei, Li. (2016). <em class="italic">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</em>. European conference on computer vision. Springer, Cham: <a href="https://arxiv.org/abs/1603.08155"><span class="url">https://arxiv.org/abs/1603.08155</span></a></li>
<li class="numberedList">Radford, Alec, Metz, Luke., and Chintala, Soumith. (2015). <em class="italic">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em>. arXiv preprint arXiv:1511.06434: <a href="https://arxiv.org/abs/1511.06434"><span class="url">https://arxiv.org/abs/1511.06434</span></a></li>
<li class="numberedList">Ledig, Christian, et al. (2017). <em class="italic">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</em>. Proceedings of the IEEE conference on computer vision and pattern recognition: <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf"><span class="url">http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf</span></a></li>
<li class="numberedList">Zhu, Jun-Yan, et al. (2017). <em class="italic">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</em>. Proceedings of the IEEE international conference on computer vision: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf"><span class="url">http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf</span></a></li>
<li class="numberedList">Karras, Tero, Laine, Samuli, and Aila, Timo. (2019). <em class="italic">A style-based generator architecture for generative adversarial networks</em>. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401-4410.</li>
<li class="numberedList">Chen, Xi, et al. (2016). <em class="italic">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</em>. Advances in neural information processing systems: <a href="https://arxiv.org/abs/1606.03657"><span class="url">https://arxiv.org/abs/1606.03657</span></a></li>
<li class="numberedList">TensorFlow implementation of the StyleGAN: <a href="https://github.com/NVlabs/stylegan"><span class="url">https://github.com/NVlabs/stylegan</span></a></li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="165" src="../Images/QR_Code18312172242788196873.png" width="177"/></p>
</div>
</div>
</body></html>