["```\n[batch_size, image_width, image_height, channels]\n```", "```\n[batch_size, 28, 28, 1].\n```", "```\ninput_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n```", "```\n[5, 28, 28, 1]\n```", "```\nconv_layer1 = tf.layers.conv2d(\n inputs=input_layer,\n filters=20,\n kernel_size=[5, 5],\n padding=\"same\",\n activation=tf.nn.relu)\n```", "```\n[batch_size, 28, 28, 1]\n```", "```\n[batch_size, 28, 28, 20]\n```", "```\npool_layer1 = tf.layers.max_pooling2d(inputs=conv_layer1, pool_size=[2, 2], strides=2)\n```", "```\n[batch_size, image_width, image_height, channels]\n```", "```\n[batch_size, 28, 28, 20]\n```", "```\n[batch_size, 14, 14, 20]\n```", "```\n[batch_size, features]\n```", "```\npool1_flat = tf.reshape(pool_layer1, [-1, 14 * 14 * 20])\n```", "```\n [batch_size, 3136]\n```", "```\ndense_layer = tf.layers.dense(inputs=pool1_flat, units=1024, activation=tf.nn.relu)\n```", "```\nlogits_layer = tf.layers.dense(inputs=dense_layer, units=10)\n```", "```\n[batch_size, 10]\n```", "```\ntf.argmax(input=logits_layer, axis=1)\n```", "```\n[batch_size, 10]\n```", "```\ntf.nn.softmax(logits_layer, name=\"softmax_tensor\")\n```", "```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport math\n```", "```\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist_data = input_data.read_data_sets('data/MNIST/', one_hot=True)\n```", "```\nOutput:\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting data/MNIST/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting data/MNIST/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting data/MNIST/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/MNIST/t10k-labels-idx1-ubyte.gz\n```", "```\nprint(\"- Number of images in the training set:\\t\\t{}\".format(len(mnist_data.train.labels)))\nprint(\"- Number of images in the test set:\\t\\t{}\".format(len(mnist_data.test.labels)))\nprint(\"- Number of images in the validation set:\\t{}\".format(len(mnist_data.validation.labels)))\n```", "```\n- Number of images in the training set: 55000\n- Number of images in the test set: 10000\n- Number of images in the validation set: 5000\n```", "```\nmnist_data.test.cls_integer = np.argmax(mnist_data.test.labels, axis=1)\n```", "```\n# Default size for the input monocrome images of MNIST\nimage_size = 28\n\n# Each image is stored as vector of this size.\nimage_size_flat = image_size * image_size\n\n# The shape of each image\nimage_shape = (image_size, image_size)\n\n# All the images in the mnist dataset are stored as a monocrome with only 1 channel\nnum_channels = 1\n\n# Number of classes in the MNIST dataset from 0 till 9 which is 10\nnum_classes = 10\n```", "```\ndef plot_imgs(imgs, cls_actual, cls_predicted=None):\n    assert len(imgs) == len(cls_actual) == 9\n\n    # create a figure with 9 subplots to plot the images.\n    fig, axes = plt.subplots(3, 3)\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n\n    for i, ax in enumerate(axes.flat):\n        # plot the image at the ith index\n        ax.imshow(imgs[i].reshape(image_shape), cmap='binary')\n\n        # labeling the images with the actual and predicted classes.\n        if cls_predicted is None:\n            xlabel = \"True: {0}\".format(cls_actual[i])\n        else:\n            xlabel = \"True: {0}, Pred: {1}\".format(cls_actual[i], cls_predicted[i])\n\n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # Show the classes as the label on the x-axis.\n        ax.set_xlabel(xlabel)\n\n    plt.show()\n```", "```\n# Visualizing 9 images form the test set.\nimgs = mnist_data.test.images[0:9]\n\n# getting the actual classes of these 9 images\ncls_actual = mnist_data.test.cls_integer[0:9]\n\n#plotting the images\nplot_imgs(imgs=imgs, cls_actual=cls_actual)\n```", "```\ndef new_weights(shape):\n    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n```", "```\ndef new_biases(length):\n    return tf.Variable(tf.constant(0.05, shape=[length]))\n```", "```\ndef conv_layer(input, # the output of the previous layer.\n                   input_channels, \n                   filter_size, \n                   filters, \n                   use_pooling=True): # Use 2x2 max-pooling.\n\n    # preparing the accepted shape of the input Tensor.\n    shape = [filter_size, filter_size, input_channels, filters]\n\n    # Create weights which means filters with the given shape.\n    filters_weights = new_weights(shape=shape)\n\n    # Create new biases, one for each filter.\n    filters_biases = new_biases(length=filters)\n\n    # Calling the conve2d function as we explained above, were the strides parameter\n    # has four values the first one for the image number and the last 1 for the input image channel\n    # the middle ones represents how many pixels the filter should move with in the x and y axis\n    conv_layer = tf.nn.conv2d(input=input,\n                         filter=filters_weights,\n                         strides=[1, 1, 1, 1],\n                         padding='SAME')\n\n    # Adding the biase to the output of the conv_layer.\n    conv_layer += filters_biases\n\n    # Use pooling to down-sample the image resolution?\n    if use_pooling:\n        # reduce the output feature map by max_pool layer\n        pool_layer = tf.nn.max_pool(value=conv_layer,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding='SAME')\n\n    # feeding the output to a ReLU activation function.\n    relu_layer = tf.nn.relu(pool_layer)\n\n    # return the final results after applying relu and the filter weights\n    return relu_layer, filters_weights\n```", "```\ndef flatten_layer(layer):\n    # Get the shape of layer.\n    shape = layer.get_shape()\n\n    # We need to flatten the layer which has the shape of The shape [num_images, image_height, image_width, num_channels]\n    # so that it has the shape of [batch_size, num_features] where number_features is image_height * image_width * num_channels\n\n    number_features = shape[1:4].num_elements()\n\n    # Reshaping that to be fed to the fully connected layer\n    flatten_layer = tf.reshape(layer, [-1, number_features])\n\n    # Return both the flattened layer and the number of features.\n    return flatten_layer, number_features\n```", "```\ndef fc_layer(input, # the flatten output.\n                 num_inputs, # Number of inputs from previous layer\n                 num_outputs, # Number of outputs\n                 use_relu=True): # Use ReLU on the output to remove negative values\n\n    # Creating the weights for the neurons of this fc_layer\n    fc_weights = new_weights(shape=[num_inputs, num_outputs])\n    fc_biases = new_biases(length=num_outputs)\n\n    # Calculate the layer values by doing matrix multiplication of\n    # the input values and fc_weights, and then add the fc_bias-values.\n    fc_layer = tf.matmul(input, fc_weights) + fc_biases\n\n    # if use RelU parameter is true\n    if use_relu:\n        relu_layer = tf.nn.relu(fc_layer)\n        return relu_layer\n\n    return fc_layer\n```", "```\ninput_values = tf.placeholder(tf.float32, shape=[None, image_size_flat], name='input_values')\n```", "```\n[num_images, image_height, image_width, num_channels]\n```", "```\ninput_image = tf.reshape(input_values, [-1, image_size, image_size, num_channels])\n```", "```\ny_actual = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_actual')\n```", "```\ny_actual_cls_integer = tf.argmax(y_actual, axis=1)\n```", "```\nconv_layer_1, conv1_weights = \\\n        conv_layer(input=input_image,\n                   input_channels=num_channels,\n                   filter_size=filter_size_1,\n                   filters=filters_1,\n                   use_pooling=True)\n```", "```\nconv_layer_1\n```", "```\nOutput:\n<tf.Tensor 'Relu:0' shape=(?, 14, 14, 16) dtype=float32>\n```", "```\nconv_layer_2, conv2_weights = \\\n         conv_layer(input=conv_layer_1,\n                   input_channels=filters_1,\n                   filter_size=filter_size_2,\n                   filters=filters_2,\n                   use_pooling=True)\n```", "```\nflatten_layer, number_features = flatten_layer(conv_layer_2)\n```", "```\nflatten_layer\n```", "```\nOutput:\n<tf.Tensor 'Reshape_1:0' shape=(?, 1764) dtype=float32>\n```", "```\nfc_layer_1 = fc_layer(input=flatten_layer,\n                         num_inputs=number_features,\n                         num_outputs=fc_num_neurons,\n                         use_relu=True)\n```", "```\nfc_layer_1\n```", "```\nOutput:\n<tf.Tensor 'Relu_2:0' shape=(?, 128) dtype=float32>\n```", "```\nfc_layer_2 = fc_layer(input=fc_layer_1,\n                         num_inputs=fc_num_neurons,\n                         num_outputs=num_classes,\n                         use_relu=False)\n```", "```\nfc_layer_2\n```", "```\nOutput:\n<tf.Tensor 'add_3:0' shape=(?, 10) dtype=float32>\n```", "```\ny_predicted = tf.nn.softmax(fc_layer_2)\n```", "```\ny_predicted_cls_integer = tf.argmax(y_predicted, axis=1)\n```", "```\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc_layer_2,\n                                                        labels=y_actual)\n```", "```\nmodel_cost = tf.reduce_mean(cross_entropy)\n```", "```\nmodel_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(model_cost)\n```", "```\nmodel_correct_prediction = tf.equal(y_predicted_cls_integer, y_actual_cls_integer)\n```", "```\nmodel_accuracy = tf.reduce_mean(tf.cast(model_correct_prediction, tf.float32))\n```", "```\nsession = tf.Session()\n```", "```\nsession.run(tf.global_variables_initializer())\n```", "```\ntrain_batch_size = 64\n```", "```\n# number of optimization iterations performed so far\ntotal_iterations = 0\n\ndef optimize(num_iterations):\n    # Update globally the total number of iterations performed so far.\n    global total_iterations\n\n    for i in range(total_iterations,\n                   total_iterations + num_iterations):\n\n        # Generating a random batch for the training process\n        # input_batch now contains a bunch of images from the training set and\n        # y_actual_batch are the actual labels for the images in the input batch.\n        input_batch, y_actual_batch = mnist_data.train.next_batch(train_batch_size)\n\n        # Putting the previous values in a dict format for Tensorflow to automatically assign them to the input\n        # placeholders that we defined above\n        feed_dict = {input_values: input_batch,\n                           y_actual: y_actual_batch}\n\n        # Next up, we run the model optimizer on this batch of images\n        session.run(model_optimizer, feed_dict=feed_dict)\n\n        # Print the training status every 100 iterations.\n        if i % 100 == 0:\n            # measuring the accuracy over the training set.\n            acc_training_set = session.run(model_accuracy, feed_dict=feed_dict)\n\n            #Printing the accuracy over the training set\n            print(\"Iteration: {0:>6}, Accuracy Over the training set: {1:>6.1%}\".format(i + 1, acc_training_set))\n\n    # Update the number of iterations performed so far\n    total_iterations += num_iterations\n```", "```\ndef plot_errors(cls_predicted, correct):\n\n    # cls_predicted is an array of the predicted class number of each image in the test set.\n\n    # Extracting the incorrect images.\n    incorrect = (correct == False)\n\n    # Get the images from the test-set that have been\n    # incorrectly classified.\n    images = mnist_data.test.images[incorrect]\n\n    # Get the predicted classes for those incorrect images.\n    cls_pred = cls_predicted[incorrect]\n\n    # Get the actual classes for those incorrect images.\n    cls_true = mnist_data.test.cls_integer[incorrect]\n\n    # Plot 9 of these images\n    plot_imgs(imgs=imgs[0:9],\n                cls_actual=cls_actual[0:9],\n                cls_predicted=cls_predicted[0:9])\n```", "```\ndef plot_confusionMatrix(cls_predicted):\n\n # cls_predicted is an array of the predicted class number of each image in the test set.\n\n # Get the actual classes for the test-set.\n cls_actual = mnist_data.test.cls_integer\n\n # Generate the confusion matrix using sklearn.\n conf_matrix = confusion_matrix(y_true=cls_actual,\n y_pred=cls_predicted)\n\n # Print the matrix.\n print(conf_matrix)\n\n # visualizing the confusion matrix.\n plt.matshow(conf_matrix)\n\n plt.colorbar()\n tick_marks = np.arange(num_classes)\n plt.xticks(tick_marks, range(num_classes))\n plt.yticks(tick_marks, range(num_classes))\n plt.xlabel('Predicted class')\n plt.ylabel('True class')\n\n # Showing the plot\n plt.show()\n```", "```\n# measuring the accuracy of the trained model over the test set by splitting it into small batches\ntest_batch_size = 256\n\ndef test_accuracy(show_errors=False,\n                        show_confusionMatrix=False):\n\n    #number of test images \n    number_test = len(mnist_data.test.images)\n\n    # define an array of zeros for the predicted classes of the test set which\n    # will be measured in mini batches and stored it.\n    cls_predicted = np.zeros(shape=number_test, dtype=np.int)\n\n    # measuring the predicted classes for the testing batches.\n\n    # Starting by the batch at index 0.\n    i = 0\n\n    while i < number_test:\n        # The ending index for the next batch to be processed is j.\n        j = min(i + test_batch_size, number_test)\n\n        # Getting all the images form the test set between the start and end indices\n        input_images = mnist_data.test.images[i:j, :]\n\n        # Get the acutal labels for those images.\n        actual_labels = mnist_data.test.labels[i:j, :]\n\n        # Create a feed-dict with the corresponding values for the input placeholder values\n        feed_dict = {input_values: input_images,\n                     y_actual: actual_labels}\n\n        cls_predicted[i:j] = session.run(y_predicted_cls_integer, feed_dict=feed_dict)\n\n        # Setting the start of the next batch to be the end of the one that we just processed j\n        i = j\n\n    # Get the actual class numbers of the test images.\n    cls_actual = mnist_data.test.cls_integer\n\n    # Check if the model predictions are correct or not\n    correct = (cls_actual == cls_predicted)\n\n    # Summing up the correct examples\n    correct_number_images = correct.sum()\n\n    # measuring the accuracy by dividing the correclty classified ones with total number of images in the test set.\n    testset_accuracy = float(correct_number_images) / number_test\n\n    # showing the accuracy.\n    print(\"Accuracy on Test-Set: {0:.1%} ({1} / {2})\".format(testset_accuracy, correct_number_images, number_test))\n\n    # showing some examples form the incorrect ones.\n    if show_errors:\n        print(\"Example errors:\")\n        plot_errors(cls_predicted=cls_predicted, correct=correct)\n\n    # Showing the confusion matrix of the test set predictions\n    if show_confusionMatrix:\n        print(\"Confusion Matrix:\")\n        plot_confusionMatrix(cls_predicted=cls_predicted)\n```", "```\ntest_accuracy()\n```", "```\nOutput:\nAccuracy on Test-Set: 4.1% (410 / 10000)\n```", "```\noptimize(num_iterations=1)\nOutput:\nIteration: 1, Accuracy Over the training set: 4.7%\ntest_accuracy()\nOutput\nAccuracy on Test-Set: 4.4% (437 / 10000)\n```", "```\noptimize(num_iterations=9999) #We have already performed 1 iteration.\n```", "```\nIteration: 7301, Accuracy Over the training set: 96.9%\nIteration: 7401, Accuracy Over the training set: 100.0%\nIteration: 7501, Accuracy Over the training set: 98.4%\nIteration: 7601, Accuracy Over the training set: 98.4%\nIteration: 7701, Accuracy Over the training set: 96.9%\nIteration: 7801, Accuracy Over the training set: 96.9%\nIteration: 7901, Accuracy Over the training set: 100.0%\nIteration: 8001, Accuracy Over the training set: 98.4%\nIteration: 8101, Accuracy Over the training set: 96.9%\nIteration: 8201, Accuracy Over the training set: 100.0%\nIteration: 8301, Accuracy Over the training set: 98.4%\nIteration: 8401, Accuracy Over the training set: 98.4%\nIteration: 8501, Accuracy Over the training set: 96.9%\nIteration: 8601, Accuracy Over the training set: 100.0%\nIteration: 8701, Accuracy Over the training set: 98.4%\nIteration: 8801, Accuracy Over the training set: 100.0%\nIteration: 8901, Accuracy Over the training set: 98.4%\nIteration: 9001, Accuracy Over the training set: 100.0%\nIteration: 9101, Accuracy Over the training set: 96.9%\nIteration: 9201, Accuracy Over the training set: 98.4%\nIteration: 9301, Accuracy Over the training set: 98.4%\nIteration: 9401, Accuracy Over the training set: 100.0%\nIteration: 9501, Accuracy Over the training set: 100.0%\nIteration: 9601, Accuracy Over the training set: 98.4%\nIteration: 9701, Accuracy Over the training set: 100.0%\nIteration: 9801, Accuracy Over the training set: 100.0%\nIteration: 9901, Accuracy Over the training set: 100.0%\nIteration: 10001, Accuracy Over the training set: 98.4%\n```", "```\ntest_accuracy(show_errors=True,\n                    show_confusionMatrix=True)\n```", "```\nOutput:\nAccuracy on Test-Set: 92.8% (9281 / 10000)\nExample errors:\n\n```", "```\nConfusion Matrix:\n[[ 971    0    2    2    0    4    0    1    0    0]\n [   0 1110    4    2    1    2    3    0   13    0]\n [  12    2  949   15   16    3    4   17   14    0]\n [   5    3   14  932    0   34    0   13    6    3]\n [   1    2    3    0  931    1    8    2    3   31]\n [  12    1    4   13    3  852    2    1    3    1]\n [  21    4    5    2   18   34  871    1    2    0]\n [   1   10   26    5    5    0    0  943    2   36]\n [  16    5   10   27   16   48    5   13  815   19]\n [  12    5    5   11   38   10    0   18    3  907]]\n```"]