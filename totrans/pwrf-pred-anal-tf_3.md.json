["```\n    import os\n    import random\n    from random import choice, shuffle\n    import pandas as pd\n    import numpy as np\n    import tensorflow as tf\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import axes3d, Axes3D\n    ```", "```\n    random.seed(12345)\n    train = pd.read_csv(os.path.join('input', 'saratoga.csv'))\n    x_train = np.array(train.iloc[:, 1:], dtype='float32')\n    ```", "```\n    def kmeans(x, n_features, n_clusters, n_max_steps=1000, early_stop=0.0):\n        input_vec = tf.constant(x, dtype=tf.float32)\n        centroids = tf.Variable(tf.slice(tf.random_shuffle(input_vec), [0, 0], [n_clusters, -1]), dtype=tf.float32)\n        old_centroids = tf.Variable(tf.zeros([n_clusters, n_features]), dtype=tf.float32)\n        centroid_distance = tf.Variable(tf.zeros([n_clusters, n_features]))\n        expanded_vectors = tf.expand_dims(input_vec, 0)\n        exanded_centroids = tf.expand_dims(centroids, 1)\n        distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2)\n        assignments = tf.argmin(distances, 0)\n        means = tf.concat([tf.reduce_mean(\n            tf.gather(input_vec, tf.reshape(tf.where(tf.equal(assignments, c)), [1, -1])),\n            reduction_indices=[1]) for c in range(n_clusters)], 0)\n        save_old_centroids = tf.assign(old_centroids, centroids)\n        update_centroids = tf.assign(centroids, means)\n        init_op = tf.global_variables_initializer()\n        performance = tf.assign(centroid_distance, tf.subtract(centroids, old_centroids))\n        check_stop = tf.reduce_sum(tf.abs(performance))\n        with tf.Session() as sess:\n            sess.run(init_op)\n            for step in range(n_max_steps):\n                sess.run(save_old_centroids)\n                _, centroid_values, assignment_values = sess.run(\n                    [update_centroids, centroids, assignments])            \n                sess.run(check_stop)\n                current_stop_coeficient = check_stop.eval()\n                if current_stop_coeficient <= early_stop:\n                    break\n        return centroid_values, assignment_values\n    ```", "```\n    centers, cluster_assignments = kmeans(x_train, len(x_train[0]), 10)\n    pca_model = PCA(n_components=3)\n    reduced_data = pca_model.fit_transform(x_train)\n    reduced_centers = pca_model.transform(centers)\n    ```", "```\n    plt.subplot(212, projection='3d')\n    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2], c=cluster_assignments)\n    plt.title(\"Clusters\")\n    plt.show()\n    >>>\n    ```", "```\n    def kmeans(x, n_features, n_clusters, n_max_steps=1000, early_stop=0.0):\n        input_vec = tf.constant(x, dtype=tf.float32)\n        centroids = tf.Variable(tf.slice(tf.random_shuffle(input_vec), [0, 0], [n_clusters, -1]), dtype=tf.float32)\n        old_centroids = tf.Variable(tf.zeros([n_clusters, n_features]), dtype=tf.float32)\n        centroid_distance = tf.Variable(tf.zeros([n_clusters, n_features]))\n        expanded_vectors = tf.expand_dims(input_vec, 0)\n        expanded_centroids = tf.expand_dims(centroids, 1)\n        distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2)\n        assignments = tf.argmin(distances, 0)\n        means = tf.concat([tf.reduce_mean(        tf.gather(input_vec, tf.reshape(tf.where(tf.equal(assignments, c)), [1, -1])),\n            reduction_indices=[1]) for c in range(n_clusters)], 0)\n        save_old_centroids = tf.assign(old_centroids, centroids)\n        update_centroids = tf.assign(centroids, means)\n        init_op = tf.global_variables_initializer()\n\n        performance = tf.assign(centroid_distance, tf.subtract(centroids, old_centroids))\n        check_stop = tf.reduce_sum(tf.abs(performance))\n        calc_wss = tf.reduce_sum(tf.reduce_min(distances, 0))\n        with tf.Session() as sess:\n            sess.run(init_op)\n            for step in range(n_max_steps):\n                sess.run(save_old_centroids)\n                _, centroid_values, assignment_values = sess.run(\n                    [update_centroids, centroids, assignments])            \n                sess.run(calc_wss)\n                sess.run(check_stop)\n                current_stop_coeficient = check_stop.eval()\n                wss = calc_wss.eval()\n                print(step, current_stop_coeficient)\n                if current_stop_coeficient <= early_stop:\n                    break\n        return centroid_values, assignment_values, wss\n    ```", "```\n    wcss_list = []\n    for i in range(2, 10):\n        centers, cluster_assignments, wcss = kmeans(x_train, len(x_train[0]), i)\n        wcss_list.append(wcss)\n    ```", "```\n    plt.figure(figsize=(12, 24))\n    plt.subplot(211)\n    plt.plot(range(2, 10), wcss_list)\n    plt.xlabel('No of Clusters')\n    plt.ylabel('WCSS')\n    plt.title(\"WCSS vs Clusters\")\n    >>>\n    ```", "```\n    match_filenames_once(pattern,name=None)\n    ```", "```\n    import \n    tensorflow as tf\n    import numpy as np\n    from bregman.suite import *\n    from tensorflow.python.framework import ops\n    import warnings\n    import random\n    ```", "```\n    filenames = tf.train.match_filenames_once('./audio_dataset/*.wav')\n    count_num_files = tf.size(filenames)\n    filename_queue = tf.train.string_input_producer(filenames)\n    reader = tf.WholeFileReader()\n    filename, file_contents = reader.read(filename_queue)\n    chromo = tf.placeholder(tf.float32)\n    max_freqs = tf.argmax(chromo, 0)\n    ```", "```\n    chromo = tf.placeholder(tf.float32) \n    max_freqs = tf.argmax(chromo, 0)\n    ```", "```\n    def get_next_chromogram(sess):\n        audio_file = sess.run(filename)\n        F = Chromagram(audio_file, nfft=16384, wfft=8192, nhop=2205)\n        return F.X, audio_file\n    ```", "```\n    def extract_feature_vector(sess, chromo_data):\n        num_features, num_samples = np.shape(chromo_data)\n        freq_vals = sess.run(max_freqs, feed_dict={chromo: chromo_data})\n        hist, bins = np.histogram(freq_vals, bins=range(num_features + \n    ))\n        normalized_hist = hist.astype(float) / num_samples\n        return normalized_hist\n    ```", "```\n    def get_dataset(sess):\n        num_files = sess.run(count_num_files)\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n        xs = list()\n        names = list()\n        plt.figure()\n        for _ in range(num_files):\n            chromo_data, filename = get_next_chromogram(sess)\n            plt.subplot(1, 2, 1)\n            plt.imshow(chromo_data, cmap='Greys', interpolation='nearest')\n            plt.title('Visualization of Sound Spectrum')\n            plt.subplot(1, 2, 2)\n            freq_vals = sess.run(max_freqs, feed_dict={chromo: chromo_data})\n            plt.hist(freq_vals)\n            plt.title('Histogram of Notes')\n            plt.xlabel('Musical Note')\n            plt.ylabel('Count')\n            plt.savefig('{}.png'.format(filename))\n            plt.clf()\n            plt.clf()\n            names.append(filename)\n            x = extract_feature_vector(sess, chromo_data)\n            xs.append(x)\n        xs = np.asmatrix(xs)\n        return xs, names\n    ```", "```\n    def initial_cluster_centroids(X, k):\n        return X[0:k, :]\n    ```", "```\n    def assign_cluster(X, centroids):\n        expanded_vectors = tf.expand_dims(X, 0)\n        expanded_centroids = tf.expand_dims(centroids, 1)\n        distances = tf.reduce_sum(tf.square(tf.subtract(expanded_vectors, expanded_centroids)), 2)\n        calc_wss = tf.reduce_sum(tf.reduce_min(distances, 0))\n        mins = tf.argmin(distances, 0)\n        return mins, calc_wss\n    ```", "```\n    def recompute_centroids(X, Y):\n        sums = tf.unsorted_segment_sum(X, Y, k)\n        counts = tf.unsorted_segment_sum(tf.ones_like(X), Y, k)\n        return sums / counts\n    ```", "```\n    init_op = tf.local_variables_initializer()\n    ```", "```\n    def audioClustering(k, max_iterations ): \n        with tf.Session() as sess:\n            sess.run(init_op)\n            X, names = get_dataset(sess)\n            centroids = initial_cluster_centroids(X, k)\n            i, converged = 0, False\n            while not converged and i < max_iterations:\n                i += 1.\n                Y, wcss_updated = assign_cluster(X, centroids)        \n                centroids = sess.run(recompute_centroids(X, Y))\n            wcss = wcss_updated.eval()        \n            print(zip(sess.run(Y)), names) \n        return wcss\n    ```", "```\n    wcss_list = []\n    k_list = []\n    ```", "```\n    for k in range(2, 9):\n        random.seed(12345)\n        wcss = audioClustering(k, 100)\n        wcss_list.append(wcss)\n        k_list.append(k)\n    ```", "```\n     ([(0,), (1,), (1,), (0,), (1,), (0,), (0,), (0,), (0,), (0,), (0,)],\n    ['./audio_dataset/scream_1.wav', './audio_dataset/Crash-Cymbal-3.\n    wav', './audio_dataset/Ride_Cymbal_1.wav', './audio_dataset/Ride_\n    Cymbal_2.wav', './audio_dataset/Crash-Cymbal-2.wav', './audio_\n    dataset/Ride_Cymbal_3.wav', './audio_dataset/scream_3.wav', './\n    audio_dataset/scream_2.wav', './audio_dataset/cough_2.wav', './audio_\n    dataset/cough_1.wav', './audio_dataset/Crash-Cymbal-1.wav'])\n\n    ([(0,), (1,), (2,), (2,), (2,), (2,), (2,), (1,), (2,), (2,), (2,)],\n    ['./audio_dataset/Ride_Cymbal_2.wav', './audio_dataset/Crash-\n    Cymbal-3.wav', './audio_dataset/cough_1.wav', './audio_dataset/Crash-\n    Cymbal-2.wav', './audio_dataset/scream_2.wav', './audio_dataset/\n    Ride_Cymbal_3.wav', './audio_dataset/Crash-Cymbal-1.wav', './\n    udio_\n    dataset/Ride_Cymbal_1.wav', './audio_dataset/cough_2.wav', './audio_\n    dataset/scream_1.wav', './audio_dataset/scream_3.wav'])\n\n    ([(0,), (1,), (2,), (3,), (2,), (2,), (2,), (2,), (2,), (2,), (2,)],\n    ['./audio_dataset/Ride_Cymbal_2.wav', './audio_dataset/Ride_Cymbal_3.\n    wav', './audio_dataset/cough_1.wav', './audio_dataset/Crash-Cymbal-1.\n    wav', './audio_dataset/scream_3.wav', './audio_dataset/cough_2.wav',\n    './audio_dataset/Crash-Cymbal-2.wav', './audio_dataset/Ride_Cymbal_1.\n    wav', './audio_dataset/Crash-Cymbal-3.wav', './audio_dataset/\n    scream_1.wav', './audio_dataset/scream_2.wav'])\n\n    ([(0,), (1,), (2,), (3,), (4,), (0,), (0,), (4,), (0,), (0,), (0,)],\n    ['./audio_dataset/cough_1.wav', './audio_dataset/scream_1.wav', './\n    audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/Ride_Cymbal_2.\n    wav', './audio_dataset/Crash-Cymbal-3.wav', './audio_dataset/\n    scream_2.wav', './audio_dataset/cough_2.wav', './audio_dataset/\n    Ride_Cymbal_1.wav', './audio_dataset/Crash-Cymbal-2.wav', './audio_\n    dataset/Ride_Cymbal_3.wav', './audio_dataset/scream_3.wav'])\n\n    ([(0,), (1,), (2,), (3,), (4,), (5,), (2,), (2,), (2,), (4,), (2,)],\n    ['./audio_dataset/scream_3.wav', './audio_dataset/Ride_Cymbal_2.wav',\n    './audio_dataset/cough_1.wav', './audio_dataset/Crash-Cymbal-2.wav',\n    './audio_dataset/Crash-Cymbal-3.wav', './audio_dataset/scream_2.wav',\n    './audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/cough_2.wav',\n    './audio_dataset/Ride_Cymbal_3.wav', './audio_dataset/Ride_Cymbal_1.\n    wav', './audio_dataset/scream_1.wav'])\n\n    ([(0,), (1,), (2,), (3,), (4,), (5,), (6,), (5,), (6,), (5,), (5,)],\n    ['./audio_dataset/cough_2.wav', './audio_dataset/Ride_Cymbal_3.\n    av',\n    './audio_dataset/scream_1.wav', './audio_dataset/Ride_Cymbal_2.wav',\n    './audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/cough_1.wav',\n    './audio_dataset/scream_2.wav', './audio_dataset/Crash-Cymbal-3.wav',\n    './audio_dataset/scream_3.wav', './audio_dataset/Ride_Cymbal_1.wav',\n    './audio_dataset/Crash-Cymbal-2.wav'])\n\n    ([(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (6,), (6,), (1,)],\n    ['./audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/scream_3.\n    wav', './audio_dataset/Ride_Cymbal_3.wav', './audio_dataset/\n    Crash-Cymbal-3.wav', './audio_dataset/Crash-Cymbal-2.wav', './\n    audio_dataset/cough_2.wav', './audio_dataset/cough_1.wav', './audio_\n    dataset/Ride_Cymbal_1.wav', './audio_dataset/Ride_Cymbal_2.wav', './\n    audio_dataset/scream_1.wav', './audio_dataset/scream_2.wav'])\n\n    ([(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (1,), (7,)],\n    ['./audio_dataset/scream_2.wav', './audio_dataset/Ride_Cymbal_1.wav',\n    './audio_dataset/Crash-Cymbal-2.wav', './audio_dataset/Ride_Cymbal_3.\n    wav', './audio_dataset/Ride_Cymbal_2.wav', './audio_dataset/scream_3.\n    wav', './audio_dataset/Crash-Cymbal-1.wav', './audio_dataset/cough_1.\n    wav', './audio_dataset/cough_2.wav', './audio_dataset/Crash-Cymbal-3.\n    wav', './audio_dataset/scream_1.wav'])\n    ```", "```\n    dict_list = zip(k_list, wcss_list)\n    my_dict = dict(dict_list)\n    print(my_dict)\n    ```", "```\n    {2: 2.8408628007260428, 3: 2.3755930780867365, 4: 0.9031724736903582,\n    5: 0.7849431270192495, 6: 0.872767581979385, 7: 0.62019339653673422,\n    8: 0.70075249251166494, 9: 0.86645706880532057}\n    ```", "```\n    Ride_Cymbal_1.wav => 2\n    Ride_Cymbal_2.wav => 0 \n    cough_1.wav => 2 \n    cough_2.wav =>2\n    Crash-Cymbal-1.wav =>3\n    Crash-Cymbal-2.wav => 2\n    scream_1.wav => 2 \n    scream_2.wav => 2  \n    ```", "```\n    import matplotlib.pyplot as plt\n    import numpy as np \n    import random\n    import os\n    import tensorflow as tf\n    import requests\n    from tensorflow.python.framework import ops\n    import warnings\n    ```", "```\n    warnings.filterwarnings(\"ignore\")\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n    ops.reset_default_graph()\n    ```", "```\n    housing_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n    housing_header = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n    num_features = len(housing_header)\n    housing_file = requests.get(housing_url)\n    housing_data = [[float(x) for x in y.split(' ') if len(x)>=1] for y in housing_file.text.split('\\n') if len(y)>=1]\n    ```", "```\n    y_vals = np.transpose([np.array([y[len(housing_header)-1] for y in housing_data])])\n    x_vals = np.array([[x for i,x in enumerate(y) if housing_header[i] in housing_header] for y in housing_data])\n    ```", "```\n    print(y_vals)\n    >>>\n    [[ 24\\. ]\n    [ 21.6]\n    [ 34.7]\n    [ 33.4]\n    [ 36.2]\n    [ 28.7]\n    [ 22.9]\n    [ 27.1]\n    [ 16.5]\n    [ 18.9]\n    [ 15\\. ]\n    …]\n    ```", "```\n    print(x_vals)\n    >>>\n    [[  6.32000000e-03   1.80000000e+01   2.31000000e+00 ...,   3.96900000e+02\n        4.98000000e+00   2.40000000e+01]\n     [  2.73100000e-02   0.00000000e+00   7.07000000e+00 ...,   3.96900000e+02\n        9.14000000e+00   2.16000000e+01]\n     [  2.72900000e-02   0.00000000e+00   7.07000000e+00 ...,   3.92830000e+02\n        4.03000000e+00   3.47000000e+01]\n     ..., \n     [  6.07600000e-02   0.00000000e+00   1.19300000e+01 ...,   3.96900000e+02\n        5.64000000e+00   2.39000000e+01]\n     [  1.09590000e-01   0.00000000e+00   1.19300000e+01 ...,   3.93450000e+02\n    ```", "```\n        6.48000000e+00   2.20000000e+01]\n     [  4.74100000e-02   0.00000000e+00   1.19300000e+01 ...,   3.96900000e+02\n        7.88000000e+00   1.19000000e+01]]\n    ```", "```\n    x_vals = (x_vals - x_vals.min(0)) / x_vals.ptp(0)\n    ```", "```\n    print(x_vals)\n    >>>\n    [[  0.00000000e+00   1.80000000e-01   6.78152493e-02 ...,   1.00000000e+008.96799117e-02   4.22222222e-01]\n     [  2.35922539e-04   0.00000000e+00   2.42302053e-01 ...,   1.00000000e+002.04470199e-01   3.68888889e-01]\n     [  2.35697744e-04   0.00000000e+00   2.42302053e-01 ...,   9.89737254e-016.34657837e-02   6.60000000e-01] ..., \n     [  6.11892474e-04   0.00000000e+00   4.20454545e-01 ...,   1.00000000e+001.07891832e-01   4.20000000e-01]\n     [  1.16072990e-03   0.00000000e+00   4.20454545e-01 ...,   9.91300620e-01\n        1.31070640e-01   3.77777778e-01]\n     [  4.61841693e-04   0.00000000e+00   4.20454545e-01 ...,   1.00000000e+00\n        1.69701987e-01   1.53333333e-01]]\n    ```", "```\n    train_indices = np.random.choice(len(x_vals), int(len(x_vals)*0.75), replace=False)\n    test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n    x_vals_train = x_vals[train_indices]\n    x_vals_test = x_vals[test_indices]\n    y_vals_train = y_vals[train_indices]\n    y_vals_test = y_vals[test_indices]\n    ```", "```\n    batch_size=len(x_vals_test)\n    ```", "```\n    x_data_train = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\n    x_data_test = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\n    y_target_train = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n    y_target_test = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n    ```", "```\n    distance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), axis=2)\n    ```", "```\n    def kNN(k): \n        topK_X, topK_indices = tf.nn.top_k(tf.negative(distance), k=k)\n        x_sums = tf.expand_dims(tf.reduce_sum(topK_X, 1), 1)\n        x_sums_repeated = tf.matmul(x_sums,tf.ones([1, k], tf.float32))\n        x_val_weights = tf.expand_dims(tf.div(topK_X, x_sums_repeated), 1)\n        topK_Y = tf.gather(y_target_train, topK_indices)\n        prediction = tf.squeeze(tf.matmul(x_val_weights,topK_Y), axis=[1])\n       mse = tf.div(tf.reduce_sum(tf.square(tf.subtract(prediction, y_target_test))), batch_size)\n        num_loops = int(np.ceil(len(x_vals_test)/batch_size))\n        init_op = tf.global_variables_initializer()\n        with tf.Session() as sess:\n                sess.run(init_op) \n                for i in range(num_loops):\n                    min_index = i*batch_size\n                    max_index = min((i+1)*batch_size,len(x_vals_train))\n                    x_batch = x_vals_test[min_index:max_index]\n                    y_batch = y_vals_test[min_index:max_index]\n                    predictions = sess.run(prediction, feed_dict={x_\n    data_train: x_vals_train, x_data_test: x_batch, y_target_train: y_vals_train, y_target_test: y_batch})\n                    batch_mse = sess.run(mse, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch, y_target_train: y_vals_train, y_target_test: y_batch})           \n        return batch_mse\n    ```", "```\n    mse_list = []\n    k_list = []\n    def getOptimalMSE_K():\n        mse = 0.0\n        for k in range(2, 11):\n            mse = kNN(k)  \n            mse_list.append(mse)\n            k_list.append(k)\n        return k_list, mse_list \n    ```", "```\n    k_list, mse_list  = getOptimalMSE_K()\n    dict_list = zip(k_list, mse_list)\n    my_dict = dict(dict_list)\n    print(my_dict)\n    optimal_k = min(my_dict, key=my_dict.get)\n    >>>\n    {2: 7.6624126, 3: 10.184645, 4: 8.9112329, 5: 11.29573, 6: 13.341181, 7: 14.406253, 8: 13.923589, 9: 14.915736, 10: 13.920851}\n    ```", "```\n    print(\"Optimal K value: \", optimal_k)\n    mse = min(mse_list)\n    print(\"Minimum mean square error: \", mse)\n    >>>\n    Optimal K value: 2 minimum mean square error: 7.66241\n    ```", "```\n    def bestKNN(k): \n        topK_X, topK_indices = tf.nn.top_k(tf.negative(distance), k=k)\n        x_sums = tf.expand_dims(tf.reduce_sum(topK_X, 1), 1)\n        x_sums_repeated = tf.matmul(x_sums,tf.ones([1, k], tf.float32))\n        x_val_weights = tf.expand_dims(tf.div(topK_X, x_sums_repeated), 1)\n        topK_Y = tf.gather(y_target_train, topK_indices)\n        prediction = tf.squeeze(tf.matmul(x_val_weights,topK_Y), axis=[1])\n        num_loops = int(np.ceil(len(x_vals_test)/batch_size))\n        init_op = tf.global_variables_initializer()\n        with tf.Session() as sess:\n                sess.run(init_op) \n                for i in range(num_loops):\n                    min_index = i*batch_size\n                    max_index = min((i+1)*batch_size,len(x_vals_train))\n                    x_batch = x_vals_test[min_index:max_index]\n                    y_batch = y_vals_test[min_index:max_index]\n    ```", "```\n        return predictions, y_batch\n    ```", "```\n    predicted_labels, actual_labels = bestKNN(optimal_k)\n    ```", "```\n    def getAccuracy(testSet, predictions):\n     correct = 0\n     for x in range(len(testSet)):\n         if(np.round(testSet[x]) == np.round(predictions[x])):\n                    correct += 1\n     return (correct/float(len(testSet))) * 100.0\n    accuracy = getAccuracy(actual_labels, predicted_labels)\n    print('Accuracy: ' + repr(accuracy) + '%')\n    >>>\n    Accuracy: 17.322834645669293%\n    ```", "```\n    bins = np.linspace(5, 50, 45)\n    plt.hist(predicted_labels, bins, alpha=1.0, facecolor='red', label='Prediction')\n    plt.hist(actual_labels, bins, alpha=1.0, facecolor='green', label='Actual')\n    plt.title('predicted vs actual values')\n    plt.xlabel('Median house price in $1,000s')\n    plt.ylabel('count')\n    plt.legend(loc='upper right')\n    plt.show()\n    >>>\n    ```"]