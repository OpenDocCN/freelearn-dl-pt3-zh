- en: DIY - A Web DL Production Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we saw how to use some notable **Deep Learning** (**DL**)
    platforms, such as **Amazon Web Services** (**AWS**), **Google Cloud Platform**
    (**GCP**), and Microsoft Azure, to enable DL in our web applications. We then
    saw how to make websites secure using DL. However, in production, the challenge
    is often not just building the predictive model—the real problems arise when you
    want to update a model that is already sending responses to users. How much time
    and business can you lose in the 30 seconds or 1 minute that it may take to replace
    the model file? What if there are models customized for each user? That might
    even mean billions of models for a platform such as Facebook.
  prefs: []
  type: TYPE_NORMAL
- en: You need to have definite solutions for updating models in production. Also,
    since the ingested data may not be in the format that the training is performed
    in, you need to define flows of data, such that they are morphed in a seamless
    manner for usage.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the methods by which we update models in production
    and the thought that goes into choosing each method. We will begin with a brief
    overview and then demonstrate some famous tools for creating DL data flows. Finally,
    we will implement our own demonstration of online learning or incremental learning
    to establish a method for updating a model in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of DL in production methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular tools for deploying ML in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a demonstration DL web production environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the project to Heroku
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security, monitoring, and performance optimizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can access the code for this chapter at [https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll need the following software to run the code used in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.6+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flask 1.1.12+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All other installations will be made during the course of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of DL in production methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Be it DL or classic **Machine Learning** (**ML**), when it comes to using models
    in production, things can get challenging. The main reason is that data fuels
    ML and data can change over time. When an ML model is deployed in production,
    it is re-trained at certain intervals as the data keeps changing over time. Therefore,
    re-training ML is not a luxury but a necessity when you are thinking of production-based
    purposes. DL is only a sub-field of ML and it is no exception to the previous
    statements. There are two popular methods that ML models are trained on—batch
    learning and online learning, especially when they are in production.
  prefs: []
  type: TYPE_NORMAL
- en: We will be discussing online learning in the next section. For this section,
    let's introduce ourselves to the concept of batch learning. In batch learning,
    we start by training an ML model on a specific chunk of data and when the model
    is done training on that chunk, it is supplied with the next chunk of data and
    this process continues until all the chunks are exhausted. These chunks are referred
    to as batches.
  prefs: []
  type: TYPE_NORMAL
- en: In real-life projects, you will be dealing with large volumes of data all the
    time. It would not be ideal to fit those datasets in memory at once. Batch learning
    comes to our aid in situations such as this one. There are disadvantages to using
    batch learning and we will get to them in the next section. You may wonder (or
    may not, as well), but yes, we perform batch learning whenever we train a neural
    network in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Just like training, the concepts of batches can be applied to serving ML models,
    as well. Serving ML models here means using machine models to make predictions
    on unseen data points. This is also known as inference. Now, model serving can
    be of two types—online serving, where the prediction needs to be made as soon
    as the model is met with the data point(s) (we cannot afford latency here), and
    offline serving, where a batch of data points is first gathered and the batch
    is run through the model to get predictions. Note that in the second case, we
    can opt in for a bit of latency.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are several engineering aspects as well that are directly attached
    to production ML systems. Discussing them is beyond the scope of this book, but
    you are encouraged to check online for courses by the GCP team.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to summarize and further understand the preceding discussion with
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95a14b78-836f-49f9-9709-4c20f0a2152e.png)'
  prefs: []
  type: TYPE_IMG
- en: This diagram depicts the requirements of your AI backend and the various parameters
    that can affect the choice of the solution that you make. We will discuss all
    of the aspects and choices available, as in this diagram, in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have four major types of solutions that you may usually find in implementations
    of DL in production:'
  prefs: []
  type: TYPE_NORMAL
- en: A web API service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at each of them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: A web API service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have a model that is trained by a separate script on the backend and is stored
    as a model and then deployed as an API-based service. Here, we're looking at a
    solution that produces results *on-demand* but the training occurs offline (not
    in the execution span of the portion of code responsible for responding to the
    client queries). Web APIs respond to a single query at a time and yield singular
    results.
  prefs: []
  type: TYPE_NORMAL
- en: This is by far the most commonly used method for deploying DL in production
    since it allows accurate training performed offline by data scientists and a short
    deployment script to create an API. In this book, we have mostly carried out deployments
    of this kind.
  prefs: []
  type: TYPE_NORMAL
- en: Online learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another form of on-demand predictions via the backend is online learning. However,
    in this methodology, the learning happens during the execution of the server script
    and so the model keeps changing with every relevant query. While such a method
    is dynamic and unlikely to become stale, it is often less accurate than its static
    counterpart—web APIs. Online learning, too, yields a single result at a time.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have demonstrated an example of online learning. We will
    discuss the tools that are helpful for online learning in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Batch forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this method, a number of predictions are made at once and stored on the server,
    ready to be fetched and used when the user needs them. However, as a static training
    method, this method allows training the model offline and so offers greater accuracy
    to the training, similar to web APIs.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, batch forecasting can be understood as a batch version of web
    APIs; however, the predictions are not served by an API. Rather, the predictions
    are stored and fetched from a database.
  prefs: []
  type: TYPE_NORMAL
- en: Auto ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making predictions is only one part of the entire process of having DL in production.
    A data scientist is also responsible for cleaning and organizing the data, creating
    a pipeline, and optimizations. Auto ML is a way of eliminating the need for such
    repetitive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Auto ML is a batch forecasting method where the need for human intervention
    is removed. So, the data, as it comes, goes through a pipeline and the forecasts
    are regularly updated. So, this method provides more up-to-date predictions than
    the batch forecasting method.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss some tools for rapidly realizing some of the methods we have
    presented.
  prefs: []
  type: TYPE_NORMAL
- en: Popular tools for deploying ML in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be discussing some popular tools used for putting ML
    in production systems. The core utility provided by these tools is automating
    the learning-prediction-feedback pipeline and facilitating the monitoring of the
    model's quality and performance. While it is very much possible to create your
    own tools for this, it is highly recommended that you use any of the following
    tools, as per the requirements of your software.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by discussing `creme`.
  prefs: []
  type: TYPE_NORMAL
- en: creme
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`creme` is a Python library that allows us to perform online learning efficiently.
    Before we look at `creme` in action, let''s have a brief discussion about online
    learning itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adfb193f-163c-4f21-9dae-af3df778d861.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In online learning, ML models are trained on one instance at a time, instead
    of being trained on a batch of data (which is also known as batch learning). To
    be able to appreciate the use of online learning, it''s important to understand
    the cons of batch learning:'
  prefs: []
  type: TYPE_NORMAL
- en: In production, we need to re-train ML models on new data over time. Batch learning
    forces us to do this but this comes at a cost. The cost not only lies in computational
    resources but also the fact that the models are re-trained from scratch. Training
    models from scratch is not always useful in production environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The features and labels of data can change over time. Batch learning does not
    allow us to train ML models that can support dynamic features and labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is exactly where we need to use online learning, which enables us to do
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Train ML models using only one instance at a time. So, we won't require a batch
    of data to train an ML model; it can be trained instantaneously using data as
    it becomes available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train ML models with dynamic features and labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Online learning has got several other names, but they all do the same thing:'
  prefs: []
  type: TYPE_NORMAL
- en: Incremental learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterative learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out-of-core learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`creme`, as mentioned earlier, is a Python library for performing online learning.
    It is an extremely useful thing to keep in your ML toolbox, especially when you
    are dealing with a production environment. `creme` is heavily inspired by scikit-learn
    (which is a very popular ML library in Python), which makes it very easy to use.
    To get a comprehensive introduction to `creme`, you are encouraged to check out
    the official GitHub repository for `creme` at [https://github.com/creme-ml/creme](https://github.com/creme-ml/creme).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enough talking! Let''s go ahead and first install `creme`. It can be done by
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the latest version of `creme`, you can use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at a quick example by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first make a few necessary imports from the `creme` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the naming convention of `creme` is similar to that of the `sklearn`
    library for an easier migration experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then fetch a dataset provided by the `creme` module itself to the data variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will be working on this dataset, which contains information about bike-ride
    sharing.
  prefs: []
  type: TYPE_NORMAL
- en: While the dataset is included in the `creme` library, you can read more about
    it at [https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we build a pipeline using `creme`, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the use of the `|=` and `+=` operators. `creme` makes it possible to
    use these operators, which makes understanding the data pipeline very intuitive.
    We can obtain a detailed representation of the pipeline built in the previous
    code block by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also get a visual representation of this pipeline by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/385f0fc6-4b0a-46d6-8414-c73c8c3a4c38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we run the training and obtain the scoring metric at an interval of
    every 30,000 row of the dataset. On the production server, this code will result
    in batch forecasting at every 1 minute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So, `creme` makes it very simple to create batch forecasting and online learning
    deployments in production with its lucid syntax and debugging facilities.
  prefs: []
  type: TYPE_NORMAL
- en: We'll now discuss another popular tool—Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an effective ML practitioner, you will need to programmatically handle workflows
    such as the previous one and be able to automate them, as well. Airflow provides
    you with a platform to efficiently do this. This link—[https://airflow.apache.org](https://airflow.apache.org)—is
    an excerpt taken from Airflow's official website. Airflow is a platform used to
    programmatically author, schedule, and monitor workflows.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this is that tasks represented on **Directed Acyclic Graphs**
    (**DAGs**) can easily be distributed across available resources (often known as
    workers). It also makes it easier to visualize your entire workflow and this turns
    out to be very helpful, especially when a workflow is very complicated. If you
    need a refresher on DAGs, the article at [https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html)
    can help. This will become much clearer when you see this implemented in a little
    while.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are designing an ML workflow, you need to think of many different
    things, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The data collection pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data preprocessing pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making the data available to the ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluation pipelines for the ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring the model, along with other things
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For now, let''s go ahead and install Airflow by executing the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Although Airflow is Python-based, it is absolutely possible to use Airflow to
    define workflows that incorporate different languages for different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, you can invoke the admin panel of Airflow and view the list
    of DAGs on it, as well as manage them and trigger a lot of other useful functions,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, you must first initialize the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a number of tables being created on a `SQLite3` database. If
    successful, you will be able to start the web server by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Open `http://localhost:8080` on your browser. You will be presented with a
    screen as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f932f02-915b-4c68-84fc-1b54864cb4fe.png)'
  prefs: []
  type: TYPE_IMG
- en: A number of example DAGs are presented. You can try running them for a brief
    play!
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss a very popular tool called AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DL or AI solutions are not limited to building cutting-edge accurate models
    in Jupyter Notebook when it comes to industrial usage. There are several steps
    in the formation of AI solutions, beginning with collecting raw data, converting
    the data into a format that can be used with predictive models, creating predictions,
    building an application around the model, and monitoring and updating the model
    in production. AutoML aims to automate this process by automating the pre-deployment
    tasks. Often, AutoML is mostly about orchestrating the data and Bayesian hyperparameter
    optimization. AutoML only sometimes means a fully automated learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'One famous library available for AutoML is provided by `H2O.ai` and it is called
    `H2O.AutoML`. To use it, we can install it using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`H2O.AutoML` is very simple to understand due to the similarity of its syntax
    with other popular ML libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a demonstration DL web environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now take a deep dive into building a sample production application that
    uses online learning on the backend. We will be creating an application that can
    predict heart diseases, based on the Cleveland dataset. We will then deploy this
    model to Heroku, which is a cloud container-based service. Finally, we will demonstrate
    the online learning feature of the application.
  prefs: []
  type: TYPE_NORMAL
- en: You can find out more about Heroku by going to [https://heroku.com](https://heroku.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s list the steps that we will be covering:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a predictive model on Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a backend for the web application that predicts on the saved model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a frontend for the web application that invokes incremental learning on
    the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the model on the server side incrementally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the application to Heroku.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will begin with the zeroth step; that is, observing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The UCI Heart Disease dataset contains 303 samples, with 76 attributes in each.
    However, most of the research work on the dataset has been centered around a simplified
    version of the Cleveland dataset with 13 attributes, as defined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Age
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chest pain type:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typical angina
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Atypical angina
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-anginal pain
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Asymptomatic
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Resting blood pressure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serum cholesterol in mg/dl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fasting blood sugar > 120 mg/dl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resting electrocardiographic results:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Having ST-T wave abnormality (T wave inversions and/or ST elevation or depression
    of > 0.05 mV)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Showing probable or definite left ventricular hypertrophy by Estes' criteria
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum heart rate achieved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise-induced angina
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oldpeak = ST depression induced by exercise relative to rest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The slope of the peak exercise ST segment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of major vessels (0-3) colored by fluoroscopy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thal: 3 = normal; 6 = fixed defect; 7 = reversible defect'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There will be a final column, which is the target we will be predicting. This
    will make the problem at hand a classification between normal and affected patients.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about the Cleveland dataset at [https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease).
  prefs: []
  type: TYPE_NORMAL
- en: Let's now begin building the heart disease detection model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a predictive model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this subsection, we will begin by building a simple neural network using
    Keras, which will classify, from a given input, the probability that a patient
    has heart disease.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Importing the necessary modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We have imported the `pandas` and `numpy` modules. Along with these, we have
    imported the `train_test_split` method from the scikit-learn library to help us
    quickly split the dataset into training and testing parts.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Loading the dataset and observing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s load the dataset, assuming it to be stored in a folder named `data`
    that is on the same directory level as that of the directory containing our Jupyter
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll quickly observe the DataFrame to see whether all the columns have been
    imported correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output in the Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f9f805b-e515-4714-9e23-d04c4b7fd49c.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe the 14 columns and see that they have been imported correctly.
    A basic **Exploratory Data Analysis** (**EDA**) would reveal that the dataset
    does not contain any missing values. However, the raw UCI Cleveland dataset does
    contain missing values contrary to the version we're using, which has been preprocessed
    and is readily available in this form on the internet. You can find a copy of
    it in the repository of this chapter on GitHub at [http://tiny.cc/HoPforDL-Ch-11](http://tiny.cc/HoPforDL-Ch-11).
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Separating the target variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll now splice out the target variable from the dataset, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will perform scaling on the features.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – Performing scaling on the features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you might have observed in the sample of the dataset in the preceding step,
    the values in the training columns are not in a common or comparable range. We
    will be performing scaling on the columns to bring them to a uniform range distribution,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The target is in the range of `0` to `1` and so does not require scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – Splitting the dataset into test and train datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll then split the dataset into training and testing parts, using the following
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We have allotted 20% of the dataset to testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Step 6 – Creating a neural network object in sklearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we create an instance of the classifier model by instantiating a new
    object of the `MLPClassifier` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We have arbitrarily set the maximum number of iterations to `200`. This may
    not be reached if the convergence happens earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Step 7 – Performing the training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we perform the training and note the observed accuracy of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding block of code in Jupyter Notebook is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83f2a674-113d-4694-ba19-9ba8dc7cc57c.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that after training on all of the 241 samples in the processed dataset,
    the accuracy is expected to reach 83.60%. Notice the `partial_fit` method in the
    preceding block of code. This is a method of the model that allows fitting a simple
    sample to the model. The more commonly used `fit` method is, in fact, a wrapper
    around the `partial_fit` method, iterating over the entire dataset and training
    one sample in each iteration. It is one of the most instrumental parts of our
    demonstration of incremental learning using the scikit-learn library.
  prefs: []
  type: TYPE_NORMAL
- en: 'To quickly see the format that the model provides an output in, we run the
    following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7af1eb56-d3b4-4e26-a4fc-804077759dcd.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that a sample with a predicted output of `0` means that the person does
    not have a heart disease, while a sample with an output of `1` means that the
    person is suffering from a heart disease.
  prefs: []
  type: TYPE_NORMAL
- en: We will now begin to convert this Jupyter notebook into a script that can perform
    learning on-demand incrementally. However, we will first build the frontend of
    this project so that we can understand the requirements from the backend.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the frontend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will take a bottom-up approach here and design the frontend of our sample
    application first. This is merely done for the sake of understanding why we write
    a few methods in the backend script differently from how we did in previous chapters.
    You would obviously create the backend script first when developing the real application.
  prefs: []
  type: TYPE_NORMAL
- en: We'll have a very stripped-down frontend, merely comprising a button that invokes
    incremental training of the application and a placeholder displaying the accuracy
    score of the model trained up to a given number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick peek at what we are building:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e081a94-5c7d-4e85-a20a-e1576ec3cea9.png)'
  prefs: []
  type: TYPE_IMG
- en: As you might interpret from the preceding screenshot of the application we will
    be building, we will have two buttons—one will add 25 samples from the training
    dataset to the partially trained model and the other will reset the training to
    0 samples (this is, actually, 1 sample in the implementation, to avoid common
    errors caused by 0; but this has minimal effect on the demonstration).
  prefs: []
  type: TYPE_NORMAL
- en: Let's create a Flask project folder named, say, `app`. We then create the `templates`
    folder and create `index.html` inside it. Another file, named `app.py`, is created
    in the `app` folder. We will create more files in this folder for deployment on
    Heroku.
  prefs: []
  type: TYPE_NORMAL
- en: We will not be writing the complete code of the `index.html` file, but we'll
    take a look at the two functions calling the API of the backend via Ajax triggers.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the entire code at [http://tiny.cc/HoPforDL-Ch-11-index](http://tiny.cc/HoPforDL-Ch-11-index).
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe lines `109` to `116` in `index.html`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The preceding piece of JavaScript (jQuery) code creates a `click` handler on
    a button with the `train-btn` ID. It calls the `/train_batch` API on the backend.
    We will be creating this API while we are developing the backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting block of code in this file is lines `138` to `145`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set a `click` handler on the button with a `reset-btn` ID to fire a
    request to the `/reset` API. This is an easily forgotten side of incremental learning,
    which asks for the decrement of the training; that is, it resets the trained model
    to an untrained state.
  prefs: []
  type: TYPE_NORMAL
- en: We now know the APIs we will need to build on the backend. Let's build those
    in the next section!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the backend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will work on creating the required APIs along with the
    server script for the demonstration. Edit the `app.py` file in the root folder
    of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will make some necessary imports to the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the imports here are very similar to the imports we made during
    model creation in the Jupyter notebook. This is explained due to the fact that
    we're only converting the Jupyter notebook code into a server script for the backend
    demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then load the dataset onto a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll quickly run through the rest of the code, where we will split the dataset,
    scale the columns, and train the model on a certain number of samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Notice that in the preceding code, we train the model on `100` samples from
    the dataset. This would make the model fairly accurate, but obviously, with scope
    for improvement, which we will trigger using the `/train_batch` API, which adds
    25 samples to the training of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set a few variables to use the script, as well as instantiating the
    `Flask` server object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now create the `/train_batch` API, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `train_batch()` function increments the learning of the model by `25` samples
    or the remaining samples of the dataset. It returns the current score of the model
    on the 20% test split of the dataset. Notice again the usage of the `partial_fit`
    method used for 25 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create the `/reset` API, which will reset the model to an untrained
    state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This API, again, returns the score of the model after the reset. It should be
    as expected—very poor—assuming the dataset is balanced in its categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now write the code to start the Flask server for this app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, we''re ready to test whether the app works by running it
    from a console. To do so, open a new terminal window and enter the following command
    in the `app` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Once the server is running, you can view the application at `http://localhost:5000`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will deploy the project to Heroku.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the project to Heroku
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will take a look at how we can deploy our demonstration
    app to Heroku. In the following steps, we will create an account on Heroku and
    add the modifications required to the code, which will make it eligible to host
    on the platform:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, visit [https://id.heroku.com/login](https://id.heroku.com/login) to
    get the login screen for Heroku. If you do not have a user account already, you
    can go through the sign-up process to create one for free:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ec63d54a-25e8-40aa-bd4c-c8b071847486.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now create a `Procfile` file. In this step, we create a blank file
    called `Procfile` in the `app` directory. Once created, we add the following line
    to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This file is used during the deployment of the project to Heroku. The preceding
    line instructs the Heroku system to use the `gunicorn` server and run the file
    called `app.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then freeze the requirements of the project. Heroku looks for the `requirements.txt`
    file to automatically download and install the required packages for the project.
    To create the list of requirements, use the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This creates a list of packages in a file named `requirements.txt` in the project's
    root folder.
  prefs: []
  type: TYPE_NORMAL
- en: You may want to leave some packages from being included in the `requirements.txt`
    file. A good method for working with projects such as this is to use virtual environments
    so that only the required packages are available in the environment and so `requirements.txt`
    only contains them. However, this solution might not always be feasible. In such
    cases, feel free to manually edit `requirements.txt` and remove the lines that
    include packages that are not relevant to the project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The directory structure of the project should currently look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now, we'll need to install the Heroku CLI on our local system. Follow the instructions
    provided at [https://devcenter.heroku.com/articles/heroku-cli](https://devcenter.heroku.com/articles/heroku-cli)
    to install Heroku on your system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we''ll initialize `git` on the directory. To do so, use the following
    command in the root directory of the project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We then initialize the Heroku version management on the project. We open a
    terminal window and navigate to the project directory. Use the following command
    to initialize the version manager provided by Heroku for this project and to register
    it with your currently logged-in user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will end by displaying the URL that your project will be hosted
    on. Along with that, a `.git` URL is displayed, which is used to track the versions
    of your project. You can push/pull from this `.git` URL to change your project
    and trigger redeployment. The output will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add files to `git` and push to Heroku. You are now ready to push the
    files to the Heroku `git` item for deployment. We use the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create the deployment and you will see a long output stream. The
    stream is a log of events happening during the deployment of your project—installing
    packages, determining the runtime, and starting the listening script. Once you
    get a successful deployment message, you will be able to view your application
    on the URL provided by Heroku in the previous step. If you are unable to remember
    it, you can use the following command to trigger it to open in a browser from
    the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now see a new window or tab open in your default browser with the
    deployed code. If anything goes wrong, you''ll be able to see the deployment logs
    in the Heroku dashboard, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11361907-2e81-41e5-ab97-90f7bd2cf2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an actual screenshot from a failed build while deploying the code presented
    in this chapter. You should be able to make out the error at the end of the log.
  prefs: []
  type: TYPE_NORMAL
- en: If the build deploys successfully, you will see a successful deployment message
    at the end of the logs.
  prefs: []
  type: TYPE_NORMAL
- en: Security measures, monitoring techniques, and performance optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will talk about the security measures, monitoring techniques,
    and performance optimizations that can be integrated into a DL solution in production.
    These functionalities are essential to maintaining solutions that depend on AI
    backends. While we have discussed the security methods facilitated by DL in previous
    chapters, we will discuss the possible security threats that could be posed to
    an AI backend.
  prefs: []
  type: TYPE_NORMAL
- en: One of the largest security threats to AI backends is from noisy data. In most
    of the methodologies for having AI in production, it is important to regularly
    check for new types of noise in the dataset that it is trained on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a very important message for all developers who love the Python `pickle`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8e91eed-3467-461c-8b35-17eaee574e35.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot is taken from the official Python documentation at
    [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate a simple example of why pickling in production might be dangerous,
    consider the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: What the preceding code does is simple—it attempts to wipe out your home directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Warning: anyone who runs the preceding code is solely responsible for the results
    of their actions.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example and associated warning implicate a general security threat
    in AI backends and almost every automated system—the hazards of untrusted input.
    So, it is important that any data that might be put into the model, whether in
    training or testing, is properly validated to make sure it won't cause any critical
    issues with the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also very important that continuous monitoring is carried out for models
    in production. Models often get stale and obsolete and run the risk of making
    outdated predictions after a while. It is important to keep a check on the relevance
    of the predictions made by the AI models. Consider a person who only knows about
    CD-ROMs and floppy disks. Over time, we came up with USB drives and solid-state
    disks. This person would not be able to make any intelligent decisions about recent
    devices. Similarly, a **Natural Language Processing** (**NLP**) model trained
    on text dumps from the early 2000s would not be able to understand a conversation
    where somebody asks *Can you please WhatsApp me the wiki link for Avengers: Endgame?*.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, how can you come up with optimizations for the performance of the AI
    backend?
  prefs: []
  type: TYPE_NORMAL
- en: 'Web developers are mostly concerned with this question. Everything needs to
    be lightning-fast when in production. Some of the tricks to speed up AI models
    in production are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Break down the dataset into the lowest number of features that you can make
    a fairly accurate prediction by. This is the core idea of feature selection performed
    by several algorithms, such as principal component analysis and other heuristic
    methods. Often, not all of the data that is fed into a system is relevant or is
    only slightly relevant to make the predictions based on it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider hosting your model on a separate, powerful cloud server with autoscaling
    enabled on it. This will ensure that your model doesn't waste resources on serving
    the pages for the website and only handles the AI-based queries. Autoscaling will
    take care of the sudden increased or steeply decreased workloads on the backend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online learning and auto ML methods are subject to slowness induced by the size
    of the dataset. Make sure you have in place constraints that do not allow a blowup
    of the size of the data being churned by dynamically learning systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the methodologies that we can use to deploy DL models
    in production. We looked at the different methods in detail and some famous tools
    that are useful in making it easier to deploy to production and manage the models
    there. We covered a demonstration of online learning using the Flask and `sklearn`
    libraries. We also discussed the post-deployment requisites and some examples
    for the most common tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will demonstrate an end-to-end sample application—a
    customer support chatbot—using Dialogflow integrated into a website.
  prefs: []
  type: TYPE_NORMAL
