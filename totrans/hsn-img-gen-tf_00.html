<html><head></head><body>
		<div id="_idContainer005">
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>Preface</h1>
			<p class="author-quote">Any sufficiently advanced technology is indistinguishable from magic.</p>
			<p class="author-quote">                                                                                                – Arthur C. Clarke</p>
			<p>This phrase best describes image generation using <strong class="bold">artificial</strong> <strong class="bold">intelligence</strong> (<strong class="bold">AI</strong>). The field of deep learning—a subset of artificial intelligence—has been developing rapidly in the last decade. Now we can generate artificial but faces that are indistinguishable from real people's faces, and to generate realistic paintings from simple brush strokes. Most of these abilities are owed to a type of deep neural network known as a <strong class="bold">generative</strong> <strong class="bold">adversarial</strong> <strong class="bold">network</strong> (<strong class="bold">GAN</strong>). With this hands-on book, you'll not only develop image generation skills but also gain a solid understanding of the underlying principles.</p>
			<p>The book starts with an introduction to the fundamentals of image generation using TensorFlow covering variational autoencoders and GANs. As you progress through the chapters, you'll learn to build models for different applications for performing face swaps using deep fakes, neural style transfer, image-to-image translation, turning simple images into photorealistic images, and much more. You'll also understand how and why to construct state-of-the-art deep neural networks using advanced techniques such as spectral normalization and self-attention layer before working with advanced models for face generation and editing. You'll also be introduced to photo restoration, text-to-image synthesis, video retargeting, and neural rendering. Throughout the book, you'll learn to implement models from scratch in TensorFlow 2.x, including PixelCNN, VAE, DCGAN, WGAN, pix2pix, CycleGAN, StyleGAN, GauGAN, and BigGAN.</p>
			<p>By the end of this book, you'll be well-versed in TensorFlow and image generative technologies.</p>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>Who this book is for</h1>
			<p>This book is for deep learning engineers, practitioners, and researchers who have basic knowledge of convolutional neural networks and want to use it to learn various image generation techniques using TensorFlow 2.x. You'll also find this book useful if you are an image processing professional or computer vision engineer looking to explore state-of-the-art architectures to improve and enhance images and videos. Knowledge of Python and TensorFlow is required to get the best out of the book.</p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>How to use this book</h1>
			<p>There are many online tutorials available teaching the basics of GANs. However, the models tend to be rather simple and suitable only for toy datasets. At the other end of the spectrum, there are also free codes available for state-of-the-art models to generate realistic images. Nevertheless, the code tends to be complex, and the lack of explanation makes it difficult for beginners to understand. Many of the “Git cloners” who downloaded the codes had no clue how to tweak the models to make them work for their applications. This book aims to bridge that gap.</p>
			<p>We will start with learning the basic principles and immediately implement the code to put them to the test. You'll be able to see the result of your work instantly. All the necessary code to build a model is laid bare in a single Jupyter notebook. This is to make it easier for you to go through the flow of the code and to modify and test the code in an interactive manner. I believe writing from scratch is the best way to learn and master deep learning. There are between one to three models in each chapter, and we will write all of them from scratch. When you finish this book, not only will you be familiar with image generation but you will also be an expert in TensorFlow 2.</p>
			<p>The chapters are arranged in roughly chronological order of the history of GANs, where the chapters may build upon knowledge from previous chapters. Therefore, it is best to read the chapters in order, especially the first three chapters, which cover the fundamentals. After that, you may jump to chapters that interest you more. Should you feel confused by the acronyms during the reading, you can refer to the summary of GAN techniques listed in the last chapter.</p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>What this book covers</h1>
			<p><a href="B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Image Generation Using TensorFlow</em>, walks through the basics of pixel probability and uses it to build our first model to generate handwritten digits.</p>
			<p><a href="B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a>, <em class="italic">Variational Autoencoder</em>, explains how to build a <strong class="bold">variational autoencoder</strong> (<strong class="bold">VAE</strong>) and use it to generate and edit faces.</p>
			<p><a href="B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Generative Adversarial Network</em>, introduces the fundamentals of GANs and builds a DCGAN to generate photorealistic images. We'll then learn about new adversarial loss to stabilize the training.</p>
			<p><a href="B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084"><em class="italic">Chapter 4</em></a>, <em class="italic">Image-to-Image Translation</em>, covers a lot of models and interesting applications. We will first implement pix2pix to convert sketches to photorealistic photos. Then we'll use CycleGAN to transform a horse to a zebra. Lastly, we will use BicycleGAN to generate a variety of shoes.</p>
			<p><a href="B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104"><em class="italic">Chapter 5</em></a>, <em class="italic">Style Transfer</em>, explains how to extract the style from a painting and transfer it into a photo. We'll also learn advanced techniques to make neural style transfer run faster in runtime, and to use it in state-of-the-art GANs. </p>
			<p><a href="B14538_06_Final_JM_ePub.xhtml#_idTextAnchor124"><em class="italic">Chapter 6</em></a>, <em class="italic">AI Painter</em>, goes through the underlying principles of image editing and transformation using <strong class="bold">interactive GAN</strong> (<strong class="bold">iGAN</strong>) as an example. Then we will build a GauGAN to create photorealistic building facades from a simple segmentation map.</p>
			<p><a href="B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 7</em></a>, <em class="italic">High Fidelity Face Generation</em>, shows how to build a StyleGAN using techniques from style transfer. However, before that, we will learn to grow the network layer progressively using a Progressive GAN.</p>
			<p><a href="B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156"><em class="italic">Chapter 8</em></a>, <em class="italic">Self-Attention for Image Generation</em>, shows how to build self-attention into a <strong class="bold">Self-Attention GAN</strong> (<strong class="bold">SAGAN</strong>) and a BigGAN for conditional image generation.</p>
			<p><a href="B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175"><em class="italic">Chapter 9</em></a>, <em class="italic">Video Synthesis</em>, demonstrates how to use autoencoders to create a deepfake video. Along the way, we'll learn how to use OpenCV and dlib for face processing.</p>
			<p><a href="B14538_10_Final_JM_ePub.xhtml#_idTextAnchor195"><em class="italic">Chapter 10</em></a>, <em class="italic">Road Ahead</em>, reviews and summarizes the generative techniques we have learned. Then we will look at how they are used as the basis of up-and-coming applications, including text-to-image-synthesis, video compression, and video retargeting.</p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>To get the most out of this book</h1>
			<p>Readers should have basic knowledge of deep learning training pipelines, such as training convolutional neural networks for image classification. This book will mainly use high-level Keras APIs in TensorFlow 2, which is easy to learn. Should you need to refresh or learn TensorFlow 2, there are many free tutorials available online, such as the one on the official TensorFlow website, <a href="https://www.tensorflow.org/tutorials/keras/classification">https://www.tensorflow.org/tutorials/keras/classification</a>.</p>
			<div>
				<div id="_idContainer004" class="IMG---Figure">
					<img src="image/01.jpg" alt=""/>
				</div>
			</div>
			<p>Training deep neural networks is computationally intensive. You can train the first few simple models using the CPU only. However, as we progress to more complex models and datasets in later chapters, the model training could take a few days before you start to see satisfactory results. To get the most out of this book, you should have access to the GPU to accelerate the model training time. There are also free cloud services, such as Google's Colab, that provide GPUs on which you can upload and run the code. </p>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access the code via the GitHub repository (link available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.</strong></p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Download the example code files</h1>
			<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0">https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0</a>. In case there's an update to the code, it will be updated on the existing GitHub repository.</p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Download the color images</h1>
			<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://static.packt-cdn.com/downloads/9781838826789_ColorImages.pdf">https://static.packt-cdn.com/downloads/9781838826789_ColorImages.pdf</a>.</p>
			<h1 id="_idParaDest-14"><a id="_idTextAnchor013"/>Conventions used</h1>
			<p>There are a number of text conventions used throughout this book.</p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: “This is done using <strong class="source-inline">tf.gather(self.beta, labels)</strong>, which is conceptually equivalent to <strong class="source-inline">beta = self.beta[labels]</strong>, as follows.”</p>
			<p>A block of code is set as follows:</p>
			<p class="source-code">attn = tf.matmul(theta, phi, transpose_b=True)attn = tf.nn.softmax(attn)</p>
			<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
			<p class="source-code">self.conv_theta = Conv2D(c//8, 1, padding='same',              	                   <strong class="bold">kernel_constraint=SpectralNorm()</strong>,  	 	                   name='Conv_Theta')</p>
			<p>Any command-line input or output is written as follows:</p>
			<p class="source-code">$ mkdir css</p>
			<p class="source-code">$ cd css</p>
			<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: “From the preceding architecture diagram, we can see that <strong class="bold">G1</strong>'s encoder output concatenates with <strong class="bold">G1</strong>'s features and feeds into the decoder part of <strong class="bold">G2</strong> to generate high-resolution images.”</p>
			<p class="callout-heading">Tips or important notes	</p>
			<p class="callout">Appear like this.</p>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Get in touch</h1>
			<p>Feedback from our readers is always welcome.</p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, mention the book title in the subject of your message and email us at <strong class="source-inline">customercare@packtpub.com</strong>.</p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit www.packtpub.com/support/errata, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <strong class="source-inline">copyright@packt.com</strong> with a link to the material.</p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit authors.packtpub.com.</p>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Reviews</h1>
			<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
			<p>For more information about Packt, please visit packt.com.</p>
		</div>
	</body></html>