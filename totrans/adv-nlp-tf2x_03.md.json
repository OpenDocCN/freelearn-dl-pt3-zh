["```\n# alternate: download the file from the browser and put # in the same directory as this notebook\n!wget https://gmb.let.rug.nl/releases/gmb-2.2.0.zip\n!unzip gmb-2.2.0.zip \n```", "```\nimport os\ndata_root = './gmb-2.2.0/data/'\nfnames = []\nfor root, dirs, files in os.walk(data_root):\n    for filename in files:\n        if filename.endswith(\".tags\"):\n            fnames.append(os.path.join(root, filename))\nfnames[:2]\n['./gmb-2.2.0/data/p57/d0014/en.tags', './gmb-2.2.0/data/p57/d0382/en.tags'] \n```", "```\n!mkdir ner \n```", "```\nimport csv\nimport collections\n\nner_tags = collections.Counter()\niob_tags = collections.Counter()\ndef strip_ner_subcat(tag):\n    # NER tags are of form {cat}-{subcat}\n    # eg tim-dow. We only want first part\n    return tag.split(\"-\")[0] \n```", "```\ndef iob_format(ners):\n    # converts IO tags into IOB format\n    # input is a sequence of IO NER tokens\n    # convert this: O, PERSON, PERSON, O, O, LOCATION, O\n    # into: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n    iob_tokens = []\n    for idx, token in enumerate(ners):\n        if token != 'O':  # !other\n            if idx == 0:\n                token = \"B-\" + token #start of sentence\n            elif ners[idx-1] == token:\n                token = \"I-\" + token  # continues\n            else:\n                token = \"B-\" + token\n        iob_tokens.append(token)\n        iob_tags[token] += 1\n    return iob_tokens \n```", "```\ntotal_sentences = 0\noutfiles = []\nfor idx, file in enumerate(fnames):\n    with open(file, 'rb') as content:\n        data = content.read().decode('utf-8').strip()\n        sentences = data.split(\"\\n\\n\")\n        print(idx, file, len(sentences))\n        total_sentences += len(sentences)\n\n        with open(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file), 'w') as outfile:\n            outfiles.append(\"./ner/\"+str(idx)+\"-\"+ os.path.basename(file))\n            writer = csv.writer(outfile)\n\n            for sentence in sentences: \n                toks = sentence.split('\\n')\n                words, pos, ner = [], [], []\n\n                for tok in toks:\n                    t = tok.split(\"\\t\")\n                    words.append(t[0])\n                    pos.append(t[1])\n                    ner_tags[t[3]] += 1\n                    ner.append(strip_ner_subcat(t[3]))\n                writer.writerow([\" \".join(words), \n                                 \" \".join(iob_format(ner)), \n                                 \" \".join(pos)]) \n```", "```\nprint(\"total number of sentences: \", total_sentences) \n```", "```\ntotal number of sentences:  62010 \n```", "```\nprint(ner_tags)\nprint(iob_tags) \n```", "```\nCounter({'O': 1146068, 'geo-nam': 58388, 'org-nam': 48034, 'per-nam': 23790, 'gpe-nam': 20680, 'tim-dat': 12786, 'tim-dow': 11404, 'per-tit': 9800, 'per-fam': 8152, 'tim-yoc': 5290, 'tim-moy': 4262, 'per-giv': 2413, 'tim-clo': 891, 'art-nam': 866, 'eve-nam': 602, 'nat-nam': 300, 'tim-nam': 146, 'eve-ord': 107, 'org-leg': 60, 'per-ini': 60, 'per-ord': 38, 'tim-dom': 10, 'art-add': 1, 'per-mid': 1})\nCounter({'O': 1146068, 'B-geo': 48876, 'B-tim': 26296, 'B-org': 26195, 'I-per': 22270, 'B-per': 21984, 'I-org': 21899, 'B-gpe': 20436, 'I-geo': 9512, 'I-tim': 8493, 'B-art': 503, 'B-eve': 391, 'I-art': 364, 'I-eve': 318, 'I-gpe': 244, 'B-nat': 238, 'I-nat': 62}) \n```", "```\nimport glob\nimport pandas as pd\n# could use `outfiles` param as well\nfiles = glob.glob(\"./ner/*.tags\")\ndata_pd = pd.concat([pd.read_csv(f, header=None, \n                                 names=[\"text\", \"label\", \"pos\"]) \n                for f in files], ignore_index = True) \n```", "```\ndata_pd.info() \n```", "```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 62010 entries, 0 to 62009\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    62010 non-null  object\n 1   label   62010 non-null  object\n 2   pos     62010 non-null  object\ndtypes: object(3)\nmemory usage: 1.4+ MB \n```", "```\n### Keras tokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntext_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False,\n                     split=' ', oov_token='<OOV>')\npos_tok = Tokenizer(filters='\\t\\n', lower=False,\n                    split=' ', oov_token='<OOV>')\nner_tok = Tokenizer(filters='\\t\\n', lower=False,\n                    split=' ', oov_token='<OOV>') \n```", "```\ntext_tok.fit_on_texts(data_pd['text'])\npos_tok.fit_on_texts(data_pd['pos'])\nner_tok.fit_on_texts(data_pd['label']) \n```", "```\nner_config = ner_tok.get_config()\ntext_config = text_tok.get_config()\nprint(ner_config) \n```", "```\n{'num_words': None, 'filters': '\\t\\n', 'lower': False, 'split': ' ', 'char_level': False, 'oov_token': '<OOV>', 'document_count': 62010, 'word_counts': '{\"B-geo\": 48876, \"O\": 1146068, \"I-geo\": 9512, \"B-per\": 21984, \"I-per\": 22270, \"B-org\": 26195, \"I-org\": 21899, \"B-tim\": 26296, \"I-tim\": 8493, \"B-gpe\": 20436, \"B-art\": 503, \"B-nat\": 238, \"B-eve\": 391, \"I-eve\": 318, \"I-art\": 364, \"I-gpe\": 244, \"I-nat\": 62}', 'word_docs': '{\"I-geo\": 7738, \"O\": 61999, \"B-geo\": 31660, \"B-per\": 17499, \"I-per\": 13805, \"B-org\": 20478, \"I-org\": 11011, \"B-tim\": 22345, \"I-tim\": 5526, \"B-gpe\": 16565, \"B-art\": 425, \"B-nat\": 211, \"I-eve\": 201, \"B-eve\": 361, \"I-art\": 207, \"I-gpe\": 224, \"I-nat\": 50}', 'index_docs': '{\"10\": 7738, \"2\": 61999, \"3\": 31660, \"7\": 17499, \"6\": 13805, \"5\": 20478, \"8\": 11011, \"4\": 22345, \"11\": 5526, \"9\": 16565, \"12\": 425, \"17\": 211, \"15\": 201, \"13\": 361, \"14\": 207, \"16\": 224, \"18\": 50}', 'index_word': '{\"1\": \"<OOV>\", \"2\": \"O\", \"3\": \"B-geo\", \"4\": \"B-tim\", \"5\": \"B-org\", \"6\": \"I-per\", \"7\": \"B-per\", \"8\": \"I-org\", \"9\": \"B-gpe\", \"10\": \"I-geo\", \"11\": \"I-tim\", \"12\": \"B-art\", \"13\": \"B-eve\", \"14\": \"I-art\", \"15\": \"I-eve\", \"16\": \"I-gpe\", \"17\": \"B-nat\", \"18\": \"I-nat\"}', 'word_index': '{\"<OOV>\": 1, \"O\": 2, \"B-geo\": 3, \"B-tim\": 4, \"B-org\": 5, \"I-per\": 6, \"B-per\": 7, \"I-org\": 8, \"B-gpe\": 9, \"I-geo\": 10, \"I-tim\": 11, \"B-art\": 12, \"B-eve\": 13, \"I-art\": 14, \"I-eve\": 15, \"I-gpe\": 16, \"B-nat\": 17, \"I-nat\": 18}'} \n```", "```\ntext_vocab = eval(text_config['index_word'])\nner_vocab = eval(ner_config['index_word'])\nprint(\"Unique words in vocab:\", len(text_vocab))\nprint(\"Unique NER tags in vocab:\", len(ner_vocab)) \n```", "```\nUnique words in vocab: 39422\nUnique NER tags in vocab: 18 \n```", "```\nx_tok = text_tok.texts_to_sequences(data_pd['text'])\ny_tok = ner_tok.texts_to_sequences(data_pd['label']) \n```", "```\n# now, pad sequences to a maximum length\nfrom tensorflow.keras.preprocessing import sequence\nmax_len = 50\nx_pad = sequence.pad_sequences(x_tok, padding='post',\n                              maxlen=max_len)\ny_pad = sequence.pad_sequences(y_tok, padding='post',\n                              maxlen=max_len)\nprint(x_pad.shape, y_pad.shape) \n```", "```\n(62010, 50) (62010, 50) \n```", "```\nnum_classes = len(ner_vocab) + 1\nY = tf.keras.utils.to_categorical(y_pad, num_classes=num_classes)\nY.shape \n```", "```\n(62010, 50, 19) \n```", "```\n# Length of the vocabulary \nvocab_size = len(text_vocab) + 1 \n# The embedding dimension\nembedding_dim = 64\n# Number of RNN units\nrnn_units = 100\n#batch size\nBATCH_SIZE=90\n# num of NER classes\nnum_classes = len(ner_vocab)+1 \n```", "```\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense\ndropout=0.2\ndef build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, classes):\n  model = tf.keras.Sequential([\n    Embedding(vocab_size, embedding_dim, mask_zero=True,\n                              batch_input_shape=[batch_size,\n None]),\n    Bidirectional(LSTM(units=rnn_units,\n                           return_sequences=True,\n                           dropout=dropout,  \n                           kernel_initializer=\\\n                            tf.keras.initializers.he_normal())),\n    **TimeDistributed(Dense(rnn_units, activation=****'relu'****)),**\n    Dense(num_classes, activation=\"softmax\")\n  ]) \n```", "```\nmodel = build_model_bilstm(\n                        vocab_size = vocab_size,\n                        embedding_dim=embedding_dim,\n                        rnn_units=rnn_units,\n                        batch_size=BATCH_SIZE,\n                        classes=num_classes)\nmodel.summary()\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n metrics=[\"accuracy\"]) \n```", "```\nModel: \"sequential_1\"\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_9 (Embedding)      (90, None, 64)            2523072   \n_________________________________________________________________\nbidirectional_9 (Bidirection (90, None, 200)           132000    \n_________________________________________________________________\ntime_distributed_6 (TimeDist (None, None, 100)         20100     \n_________________________________________________________________\ndense_16 (Dense)             (None, None, 19)          1919      \n=================================================================\nTotal params: 2,677,091\nTrainable params: 2,677,091\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\n# to enable TensorFlow to process sentences properly\nX = x_pad\n# create training and testing splits\ntotal_sentences = 62010\ntest_size = round(total_sentences / BATCH_SIZE * 0.2)\nX_train = X[BATCH_SIZE*test_size:]\nY_train = Y[BATCH_SIZE*test_size:]\nX_test = X[0:BATCH_SIZE*test_size]\nY_test = Y[0:BATCH_SIZE*test_size] \n```", "```\nmodel.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=15) \n```", "```\nTrain on 49590 samples\nEpoch 1/15\n49590/49590 [==============================] - 20s 409us/sample - loss: 0.1736 - accuracy: 0.9113\n...\nEpoch 8/15\n49590/49590 [==============================] - 15s 312us/sample - loss: 0.0153 - accuracy: 0.9884\n...\nEpoch 15/15\n49590/49590 [==============================] - 15s 312us/sample - loss: 0.0065 - accuracy: 0.9950 \n```", "```\nmodel.evaluate(X_test, Y_test, batch_size=BATCH_SIZE) \n```", "```\n12420/12420 [==============================] - 3s 211us/sample - loss: 0.0926 - accuracy: 0.9624 \n```", "```\n!pip install tensorflow_addons==0.11.2 \n```", "```\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K\nclass CRFLayer(Layer):\n  \"\"\"\n  Computes the log likelihood during training\n  Performs Viterbi decoding during prediction\n  \"\"\"\n  def __init__(self,\n               label_size, mask_id=0,\n               trans_params=None, name='crf',\n               **kwargs):\n    super(CRFLayer, self).__init__(name=name, **kwargs)\n    self.label_size = label_size\n    self.mask_id = mask_id\n    self.transition_params = None\n\n    if trans_params is None:  # not reloading pretrained params\n        self.transition_params = tf.Variable(\ntf.random.uniform(shape=(label_size, label_size)),\n                trainable=False)\n    else:\n        self.transition_params = trans_params \n```", "```\ndef call(self, inputs, seq_lengths, training=None):\n\n    if training is None:\n        training = K.learning_phase()\n\n    # during training, this layer just returns the logits\n    if training:\n        return inputs\n    return inputs  # to be replaced later \n```", "```\nfrom tensorflow.keras import Model, Input, Sequential\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed\nfrom tensorflow.keras.layers import Dropout, Bidirectional\nfrom tensorflow.keras import backend as K \n```", "```\nclass NerModel(tf.keras.Model):\n    def __init__(self, hidden_num, vocab_size, label_size, \n                 embedding_size, \n                 name='BilstmCrfModel', **kwargs):\n        super(NerModel, self).__init__(name=name, **kwargs)\n        self.num_hidden = hidden_num\n        self.vocab_size = vocab_size\n        self.label_size = label_size\n        self.embedding = Embedding(vocab_size, embedding_size, \n                                   mask_zero=True, \n                                   name=\"embedding\")\n        self.biLSTM =Bidirectional(LSTM(hidden_num, \n                                   return_sequences=True), \n                                   name=\"bilstm\")\n        self.dense = TimeDistributed(tf.keras.layers.Dense(\n                                     label_size), name=\"dense\")\n        self.crf = CRFLayer(self.label_size, name=\"crf\") \n```", "```\ndef call(self, text, labels=None, training=None):\n        seq_lengths = tf.math.reduce_sum(\ntf.cast(tf.math.not_equal(text, 0), dtype=tf.int32), axis=-1) \n\n        if training is None:\n            training = K.learning_phase()\n        inputs = self.embedding(text)\n        bilstm = self.biLSTM(inputs)\n        logits = self.dense(bilstm)\n        outputs = self.crf(logits, seq_lengths, training)\n        return outputs \n```", "```\n def loss(self, y_true, y_pred):\n    y_pred = tf.convert_to_tensor(y_pred)\n    y_true = tf.cast(self.get_proper_labels(y_true), y_pred.dtype)\n    seq_lengths = self.get_seq_lengths(y_true)\n    log_likelihoods, self.transition_params =\\ \ntfa.text.crf_log_likelihood(y_pred,\n               y_true, seq_lengths)\n    # save transition params\n    self.transition_params = tf.Variable(self.transition_params, \n      trainable=False)\n    # calc loss\n    loss = - tf.reduce_mean(log_likelihoods)\n    return loss \n```", "```\n def get_proper_labels(self, y_true):\n    shape = y_true.shape\n    if len(shape) > 2:\n        return tf.argmax(y_true, -1, output_type=tf.int32)\n    return y_true \n```", "```\n def get_seq_lengths(self, matrix):\n    # matrix is of shape (batch_size, max_seq_len)\n    mask = tf.not_equal(matrix, self.mask_id)\n    seq_lengths = tf.math.reduce_sum(\n                                    tf.cast(mask, dtype=tf.int32), \n                                    axis=-1)\n    return seq_lengths \n```", "```\n# Length of the vocabulary \nvocab_size = len(text_vocab) + 1 \n# The embedding dimension\nembedding_dim = 64\n# Number of RNN units\nrnn_units = 100\n#batch size\nBATCH_SIZE=90\n# num of NER classes\nnum_classes = len(ner_vocab) + 1\nblc_model = NerModel(rnn_units, vocab_size, num_classes, \nembedding_dim, dynamic=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) \n```", "```\n# create training and testing splits\ntotal_sentences = 62010\ntest_size = round(total_sentences / BATCH_SIZE * 0.2)\nX_train = x_pad[BATCH_SIZE*test_size:]\nY_train = Y[BATCH_SIZE*test_size:]\nX_test = x_pad[0:BATCH_SIZE*test_size]\nY_test = Y[0:BATCH_SIZE*test_size]\nY_train_int = tf.cast(Y_train, dtype=tf.int32)\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train_int))\ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True) \n```", "```\nloss_metric = tf.keras.metrics.Mean()\nepochs = 5\n# Iterate over epochs.\nfor epoch in range(epochs):\n    print('Start of epoch %d' % (epoch,))\n    # Iterate over the batches of the dataset.\n    for step, (text_batch, labels_batch) in enumerate(\ntrain_dataset):\n        labels_max = tf.argmax(labels_batch, -1, \noutput_type=tf.int32)\n        with tf.GradientTape() as tape:\n            **logits = blc_model(text_batch, training=****True****)**\n            loss = blc_model.crf.loss(labels_max, logits)\n            grads = tape.gradient(loss, \nblc_model.trainable_weights)\n            optimizer.apply_gradients(zip(grads, \nblc_model.trainable_weights))\n\n            loss_metric(loss)\n        if step % 50 == 0:\n          print('step %s: mean loss = %s' % \n(step, loss_metric.result())) \n```", "```\nStart of epoch 0\nstep 0: mean loss = tf.Tensor(71.14853, shape=(), dtype=float32)\nstep 50: mean loss = tf.Tensor(31.064453, shape=(), dtype=float32)\n...\nStart of epoch 4\nstep 0: mean loss = tf.Tensor(4.4125915, shape=(), dtype=float32)\nstep 550: mean loss = tf.Tensor(3.8311224, shape=(), dtype=float32) \n```", "```\nblc_model.summary() \n```", "```\nModel: \"BilstmCrfModel\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        multiple                  2523072   \n_________________________________________________________________\nbilstm (Bidirectional)       multiple                  132000    \n_________________________________________________________________\ndense (TimeDistributed)      multiple                  3819      \n_________________________________________________________________\ncrf (CRFLayer)               multiple                  361       \n=================================================================\nTotal params: 2,659,252\nTrainable params: 2,658,891\nNon-trainable params: 361\n_________________________________________________________________ \n```", "```\nWriting in The Washington Post newspaper , Mr. Ushakov also \nsaid it is inadmissible to move in the direction of demonizing Russia . \n```", "```\nO O B-org I-org I-org O O B-per B-org O O O O O O O O O O O O B-geo O \n```", "```\nO O O B-geo I-org O O B-per I-per O O O O O O O O O O O O B-geo O \n```", "```\nO O B-org I-org I-org O O B-per I-per O O O O O O O O O O O O B-geo O \n```", "```\n def call(self, inputs, seq_lengths, training=None):\n    if training is None:\n        training = K.learning_phase()\n\n    # during training, this layer just returns the logits\n    if training:\n        return inputs\n\n    **# viterbi decode logic to return proper** \n    **# results at inference**\n    **_, max_seq_len, _ = inputs.shape**\n    **seqlens = seq_lengths**\n    **paths = []**\n    **for** **logit, text_len** **in****zip****(inputs, seqlens):**\n        **viterbi_path, _ = tfa.text.viterbi_decode(logit[:text_len],** \n                                              **self.transition_params)**\n        **paths.append(self.pad_viterbi(viterbi_path, max_seq_len))**\n    **return** **tf.convert_to_tensor(paths)** \n```", "```\n def pad_viterbi(self, viterbi, max_seq_len):\n    if len(viterbi) < max_seq_len:\n        viterbi = viterbi + [self.mask_id] * \\\n                                (max_seq_len - len(viterbi))\n    return viterbi \n```", "```\nY_test_int = tf.cast(Y_test, dtype=tf.int32)\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test,\n                                                   Y_test_int))\ntest_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\nout = blc_model.predict(test_dataset.take(1)) \n```", "```\ntext_tok.sequences_to_texts([X_test[2]]) \n```", "```\n['Writing in The Washington Post newspaper , Mr. Ushakov also said it is inadmissible to move in the direction of demonizing Russia . <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>'] \n```", "```\nprint(\"Ground Truth: \", \nner_tok.sequences_to_texts([tf.argmax(Y_test[2], \n                                     -1).numpy()]))\nprint(\"Prediction: \", ner_tok.sequences_to_texts([out[2]])) \n```", "```\nGround Truth:  ['O O B-org I-org I-org O O **B-per B-org** O O O O O O O O O O O O B-geo O <OOV> <SNIP> <OOV>']\nPrediction:  ['O O B-org I-org I-org O O **B-per I-per** O O O O O O O O O O O O B-geo O <OOV> <SNIP> <OOV>'] \n```", "```\ndef np_precision(pred, true):\n    # expect numpy arrays\n    assert pred.shape == true.shape\n    assert len(pred.shape) == 2\n    mask_pred = np.ma.masked_equal(pred, 0)\n    mask_true = np.ma.masked_equal(true, 0)\n    acc = np.equal(mask_pred, mask_true)\n    return np.mean(acc.compressed().astype(int)) \n```", "```\nnp_precision(out, tf.argmax(Y_test[:BATCH_SIZE], -1).numpy()) \n```", "```\n0.9664461247637051 \n```"]