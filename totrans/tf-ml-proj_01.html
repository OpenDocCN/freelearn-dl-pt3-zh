<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Overview of TensorFlow and Machine Learning</h1>
                </header>
            
            <article>
                
<p>TensorFlow is a popular library for implementing machine learning-based solutions. It includes a low-level API known as TensorFlow core and many high-level APIs, including two of the most popular ones, known as TensorFlow Estimators and Keras. In this chapter, we will learn about the basics of TensorFlow and build a machine learning model using logistic regression to classify handwritten digits as an example.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>TensorFlow core:
<ul>
<li>Tensors in TensorFlow core</li>
<li>Constants</li>
<li>Placeholders</li>
<li>Operations</li>
<li>Tensors from Python objects</li>
<li>Variables</li>
<li>Tensors from library functions</li>
</ul>
</li>
</ul>
<ul>
<li>Computation graphs:
<ul>
<li>Lazy loading and execution order</li>
<li>Graphs on multiple devices <span>– C</span>PU and GPGPU</li>
<li>Working with multiple graphs</li>
</ul>
</li>
<li>Machine learning, classification, and logistic regression</li>
<li>Logistic regression examples in TensorFlow</li>
<li>Logistic regression examples in Keras</li>
</ul>
<div class="packt_infobox"><span>You can follow the code examples in this chapter by using the Jupyter Notebook named </span><kbd>ch-01_Overview_of_TensorFlow_and_Machine_Learning.ipynb</kbd><span> that's included in the code bundle.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is TensorFlow?</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow</strong> is a popular open source library that's used for implementing machine learning and deep learning. It was initially built at Google for internal consumption and was released publicly on November 9, 2015. Since then, TensorFlow has been extensively used to develop<span> </span>machine<span> </span>learning and deep learning models in several business domains. </p>
<p>To use TensorFlow in our projects, we need to learn how to program using the TensorFlow API. TensorFlow has multiple APIs that can be used to interact with the library. The TensorFlow APIs are<span> </span>divided<span> </span>into two levels:</p>
<ul>
<li><strong>Low-level API</strong>: The API known as TensorFlow core provides fine-grained lower level functionality. Because of this, this low-level API offers complete control while being used on models. We will cover TensorFlow core in this chapter.</li>
<li><strong>High-level API</strong>: These APIs<span> </span>provide<span> </span>high-level functionalities that have been built on TensorFlow core and are comparatively easier to learn and implement. Some high-level APIs include Estimators, Keras, TFLearn, TFSlim, and Sonnet. We will also cover Keras in this chapter.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The TensorFlow core</h1>
                </header>
            
            <article>
                
<p>The <strong>TensorFlow core</strong> is the lower-level API on which the<span> </span>higher-level TensorFlow modules are built. In this section, we will go over a quick overview of TensorFlow core and learn about the basic elements of TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tensors</h1>
                </header>
            
            <article>
                
<p><strong>Tensors</strong><span> </span>are the basic components in TensorFlow. A tensor is a multidimensional collection of data elements. It is generally identified by shape, type, and rank. <strong>Rank</strong><span> refers to</span> the number of<span> </span>dimensions<span> </span>of a tensor, while <strong>shape</strong><span> refers to the</span><span> </span>size of each dimension. You may have seen several examples of tensors before, such as in a zero-dimensional collection (also known as a scalar), a one-dimensional <span>collection</span><span> </span>(also known as a vector), and a two-dimensional <span>collection</span><span> </span>(also known as a matrix).</p>
<p>A scalar value is a tensor of rank 0 and shape []. A vector, or a one-dimensional array, is a tensor of rank 1 and shape [<kbd>number_of_columns</kbd>] or [<kbd>number_of_rows</kbd>]. A matrix, or a two-dimensional array, is a tensor of rank 2 and shape [<kbd>number_of_rows</kbd>, <kbd>number_of_columns</kbd>]. A three-dimensional<span> </span>array<span> </span>is a tensor of rank 3. In the same way, an n-dimensional array is a tensor of rank<span> </span>n.</p>
<p>A tensor can store data of one type in all of its dimensions, and the<span> </span>data<span> </span>type of a tensor is the same as the data type of its elements.</p>
<div class="mce-root packt_tip">The data types that can be found in the TensorFlow library are described at the following link: <a href="https://www.tensorflow.org/api_docs/python/tf/DType">https://www.tensorflow.org/api_docs/python/tf/DType</a>.</div>
<p>The following are the most commonly used data types in TensorFlow:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>TensorFlow Python API data type</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>tf.float16</kbd></td>
<td>16-bit floating point (half-precision)</td>
</tr>
<tr>
<td><kbd>tf.float32</kbd></td>
<td>32-bit floating point (<span>single-precision)</span></td>
</tr>
<tr>
<td><kbd>tf.float64</kbd></td>
<td>64-bit floating point (<span>double-precision)</span></td>
</tr>
<tr>
<td><kbd>tf.int8</kbd></td>
<td>8-bit integer (<span>signed)</span></td>
</tr>
<tr>
<td><kbd>tf.int16</kbd></td>
<td>16-bit integer <span>(</span><span>signed)</span></td>
</tr>
<tr>
<td><kbd>tf.int32</kbd></td>
<td>32-bit integer <span>(</span><span>signed)</span></td>
</tr>
<tr>
<td><kbd>tf.int64</kbd></td>
<td>64-bit integer <span>(</span><span>signed)</span></td>
</tr>
</tbody>
</table>
<div class="packt_tip">Use TensorFlow data types for defining tensors i<span>nstead of native data types from Python or data types from NumPy.</span></div>
<p>Tensors can be created in the following ways:</p>
<ul>
<li>By defining constants, operations, and variables, and passing the values to their constructor</li>
<li>By defining placeholders and passing the values to<span> </span><kbd>session.run()</kbd></li>
<li>By converting Python objects, such as scalar values, lists, NumPy arrays, and pandas DataFrames, with the <kbd>tf.convert_to_tensor()</kbd><span> </span>function</li>
</ul>
<p>Let's explore different ways of creating Tensors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constants</h1>
                </header>
            
            <article>
                
<p>The constant valued tensors are created using the <kbd>tf.constant()</kbd> function, and has the<span> </span>following<span> </span>definition:</p>
<pre>tf.constant(<br/>  value,<br/>  dtype=None,<br/>  shape=None,<br/>  name='const_name',<br/>  verify_shape=False<br/>  )</pre>
<p>Let's create some constants with the following code:</p>
<pre>const1=tf.constant(34,name='x1')<br/>const2=tf.constant(59.0,name='y1')<br/>const3=tf.constant(32.0,dtype=tf.float16,name='z1')</pre>
<p>Let's take a look at the preceding code in detail:</p>
<ul>
<li>The first line of code defines a constant tensor,<span> </span><kbd>const1</kbd>, stores a value of <kbd>34</kbd>, and names it <kbd>x1</kbd>.</li>
<li>The second line of code defines a constant tensor,<span> </span><kbd>const2</kbd>, stores a value of <kbd>59.0</kbd>, and names it <kbd>y1</kbd>.</li>
<li>The third line of code <span>defines the data type as</span><span> </span><kbd>tf.float16</kbd><span> </span><span>for</span><span> </span><kbd>const3</kbd><span>.</span> Use the<span> </span><kbd>dtype</kbd> parameter or place the data type as the second argument to denote the data type. </li>
</ul>
<p>Let's print the constants<span> </span><kbd>const1</kbd>,<span> </span><kbd>const2</kbd>, and<span> </span><kbd>const3</kbd>:</p>
<pre>print('const1 (x): ',const1)<br/>print('const2 (y): ',const2)<br/>print('const3 (z): ',const3)</pre>
<p>When we print these constants, we get the following output:</p>
<pre><strong>const1 (x):  Tensor("x:0", shape=(), dtype=int32)</strong><br/><strong>const2 (y):  Tensor("y:0", shape=(), dtype=float32)</strong><br/><strong>const3 (z):  Tensor("z:0", shape=(), dtype=float16)</strong></pre>
<div class="packt_infobox">Upon printing the previously defined tensors, we can see that the data types of<span> </span><kbd>const1</kbd><span> </span>and<span> </span><kbd>const2</kbd><span> </span>are automatically deduced by TensorFlow.</div>
<p>To print the values of these constants, we can execute them in a TensorFlow session with the <kbd>tfs.run()</kbd><span> </span>command:</p>
<pre>print('run([const1,const2,c3]) : ',tfs.run([const1,const2,const3]))</pre>
<p class="mce-root">We will see the following output:</p>
<pre><strong>run([const1,const2,const3]) : [34, 59.0, 32.0]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operations</h1>
                </header>
            
            <article>
                
<p>The TensorFlow library contains several built-in<span> </span>operations that can be applied on tensors. An operation node can be defined by passing input values and saving the output in another tensor. To understand this better, let's define two operations, <kbd>op1</kbd> and<span> </span><kbd>op2</kbd>:</p>
<pre>op1 = tf.add(const2, const3)<br/>op2 = tf.multiply(const2, const3)</pre>
<p>Let's print<span> </span><kbd>op1</kbd><span> </span>and<span> </span><kbd>op2</kbd>:</p>
<pre>print('op1 : ', op1)<br/>print('op2 : ', op2)</pre>
<p>The output is as follows, and shows <span>that <kbd>op1</kbd> and <kbd>op2</kbd> are defined as tensors:</span></p>
<div class="cell code_cell rendered selected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_stream output_stdout">
<pre>op1 :  Tensor("Add:0", shape=(), dtype=float32)<br/>op2 :  Tensor("Mul:0", shape=(), dtype=float32)</pre></div>
</div>
</div>
</div>
</div>
<p>To print the output from executing these operations, the <kbd>op1</kbd> and <kbd>op2</kbd> tensors have to be executed in a TensorFlow session:</p>
<pre>print('run(op1) : ', tfs.run(op1))<br/>print('run(op2) : ', tfs.run(op2))</pre>
<p>The output is as follows:</p>
<pre>run(op1) :  91.0
run(op2) :  1888.0</pre>
<p>Some of the built-in operations of TensorFlow include arithmetic operations, math functions, and complex number operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placeholders</h1>
                </header>
            
            <article>
                
<p>While constants store the value at the time of<span> </span>defining<span> </span>the tensor, placeholders allow you to create empty tensors so that the values can be provided at runtime. The TensorFlow library provides the<span> </span><kbd>tf.placeholder()</kbd><span> </span>function with the following signature to create placeholders:</p>
<pre>tf.placeholder(<br/>  dtype,<br/>  shape=None,<br/>  name=None<br/>  )</pre>
<p>As an example, let's create two placeholders and print them:</p>
<pre>p1 = tf.placeholder(tf.float32)<br/>p2 = tf.placeholder(tf.float32)<br/>print('p1 : ', p1)<br/>print('p2 : ', p2)</pre>
<p>The following output shows that each placeholder has been created as a tensor:</p>
<pre>p1 :  Tensor("Placeholder:0", dtype=float32)<br/>p2 :  Tensor("Placeholder_1:0", dtype=float32)</pre>
<p>Let's define an operation using these placeholders:</p>
<pre>mult_op = p1 * p2</pre>
<p class="mce-root">In TensorFlow, shorthand symbols can be used for various operations. In the preceding code, <kbd>p1 * p2</kbd><span> </span>is shorthand for<span> </span><kbd>tf.multiply(p1,p2)</kbd>:</p>
<pre>print('run(mult_op,{p1:13.4, p2:61.7}) : ',tfs.run(mult_op,{p1:13.4, p2:61.7}))</pre>
<p>The preceding command runs <kbd>mult_op</kbd><span> </span>in the TensorFlow session and feeds the values dictionary (the second argument to the <kbd>run()</kbd><span> </span>operation) with the values for<span> </span><kbd>p1</kbd><span> </span>and<span> </span><kbd>p2</kbd>.</p>
<p>The output is as follows: </p>
<pre>run(mult_op,{p1:13.4, p2:61.7}) :  826.77997</pre>
<p class="mce-root">We can also specify the values dictionary by using the<span> </span><kbd>feed_dict</kbd> parameter in the <kbd>run()</kbd><span> </span>operation:</p>
<pre>feed_dict={p1: 15.4, p2: 19.5}<br/>print('run(mult_op,feed_dict = {p1:15.4, p2:19.5}) : ',<br/>      tfs.run(mult_op, feed_dict=feed_dict))</pre>
<p>The output is as follows:</p>
<pre>run(mult_op,feed_dict = {p1:15.4, p2:19.5}) :  300.3</pre>
<p>Let's look at one final example, which is of a vector being fed to the same operation:</p>
<pre>feed_dict={p1: [2.0, 3.0, 4.0], p2: [3.0, 4.0, 5.0]}<br/>print('run(mult_op,feed_dict={p1:[2.0,3.0,4.0], p2:[3.0,4.0,5.0]}):',<br/>      tfs.run(mult_op, feed_dict=feed_dict))</pre>
<p>The output is as follows:</p>
<pre>run(mult_op,feed_dict={p1:[2.0,3.0,4.0],p2:[3.0,4.0,5.0]}):[  6.  12.  20.]</pre>
<p>The elements of the two input vectors are multiplied in an element-wise fashion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tensors from Python objects</h1>
                </header>
            
            <article>
                
<p>Tensors can be created from<span> </span>Python<span> </span>objects such as lists, NumPy arrays, and pandas DataFrames. To create tensors from Python objects, use the <kbd>tf.convert_to_tensor()</kbd> function with the following definition:</p>
<pre>tf.convert_to_tensor(<br/>  value,<br/>  dtype=None,<br/>  name=None,<br/>  preferred_dtype=None<br/>  )</pre>
<p>Let's practice doing this by creating some tensors and printing their definitions and values:</p>
<ol>
<li>Define a 0-D tensor:</li>
</ol>
<pre style="padding-left: 60px">tf_t=tf.convert_to_tensor(5.0,dtype=tf.float64)<br/><br/>print('tf_t : ',tf_t)<br/>print('run(tf_t) : ',tfs.run(tf_t))</pre>
<p style="padding-left: 60px">The output is as follows: </p>
<pre style="padding-left: 60px">tf_t : Tensor("Const_1:0", shape=(), dtype=float64)<br/>run(tf_t) : 5.0</pre>
<ol start="2">
<li>Define a 1-D tensor:</li>
</ol>
<pre style="padding-left: 60px">a1dim = np.array([1,2,3,4,5.99])<br/>print("a1dim Shape : ",a1dim.shape)<br/><br/>tf_t=tf.convert_to_tensor(a1dim,dtype=tf.float64)<br/><br/>print('tf_t : ',tf_t)<br/>print('tf_t[0] : ',tf_t[0])<br/>print('tf_t[0] : ',tf_t[2])<br/>print('run(tf_t) : \n',tfs.run(tf_t))</pre>
<p style="padding-left: 60px"><span>The output is as follows</span><span>:</span></p>
<pre style="padding-left: 60px">a1dim Shape :  (5,)<br/>tf_t :  Tensor("Const_2:0", shape=(5,), dtype=float64)<br/>tf_t[0] :  Tensor("strided_slice:0", shape=(), dtype=float64)<br/>tf_t[0] :  Tensor("strided_slice_1:0", shape=(), dtype=float64)<br/>run(tf_t) : <br/> [ 1.    2.    3.    4.    5.99]</pre>
<ol start="3">
<li>Define a 2-D tensor:</li>
</ol>
<pre style="padding-left: 60px">a2dim = np.array([(1,2,3,4,5.99),<br/>                  (2,3,4,5,6.99),<br/>                  (3,4,5,6,7.99)<br/>                 ])<br/>print("a2dim Shape : ",a2dim.shape)<br/><br/>tf_t=tf.convert_to_tensor(a2dim,dtype=tf.float64)<br/><br/>print('tf_t : ',tf_t)<br/>print('tf_t[0][0] : ',tf_t[0][0])<br/>print('tf_t[1][2] : ',tf_t[1][2])<br/>print('run(tf_t) : \n',tfs.run(tf_t))</pre>
<p style="padding-left: 60px"><span>The output is as follows:</span></p>
<div class="cell code_cell rendered unselected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_stream output_stdout">
<pre style="padding-left: 60px">a2dim Shape :  (3, 5)<br/>tf_t :  Tensor("Const_3:0", shape=(3, 5), dtype=float64)<br/>tf_t[0][0] :  Tensor("strided_slice_3:0", shape=(), dtype=float64)<br/>tf_t[1][2] :  Tensor("strided_slice_5:0", shape=(), dtype=float64)<br/>run(tf_t) : <br/> [[ 1.    2.    3.    4.    5.99]<br/>  [ 2.    3.    4.    5.    6.99]<br/>  [ 3.    4.    5.    6.    7.99]]</pre></div>
</div>
</div>
</div>
</div>
<ol start="4">
<li>Define a 3-D tensor:</li>
</ol>
<pre style="padding-left: 60px">a3dim = np.array([[[1,2],[3,4]],<br/>                  [[5,6],[7,8]]<br/>                 ])<br/>print("a3dim Shape : ",a3dim.shape)<br/><br/>tf_t=tf.convert_to_tensor(a3dim,dtype=tf.float64)<br/><br/>print('tf_t : ',tf_t)<br/>print('tf_t[0][0][0] : ',tf_t[0][0][0])<br/>print('tf_t[1][1][1] : ',tf_t[1][1][1])<br/>print('run(tf_t) : \n',tfs.run(tf_t))</pre>
<p class="mce-root" style="padding-left: 60px"><span>The output is as follows:</span></p>
<pre style="padding-left: 60px">a3dim Shape :  (2, 2, 2)<br/>tf_t :  Tensor("Const_4:0", shape=(2, 2, 2), dtype=float64)<br/>tf_t[0][0][0] :  Tensor("strided_slice_8:0", shape=(), dtype=float64)<br/>tf_t[1][1][1] :  Tensor("strided_slice_11:0", shape=(), dtype=float64)<br/>run(tf_t) : <br/> [[[ 1.  2.][ 3.  4.]]<br/>  [[ 5.  6.][ 7.  8.]]]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variables</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we learned how to define tensor<span> </span>objects<span> </span>of different types, such as constants, operations, and placeholders. The values of parameters need to be held in an updatable memory location while building and training models with TensorFlow. Such updatable memory locations for tensors are known as variables in TensorFlow.</p>
<p>To summarize this, TensorFlow variables are tensor objects in that their values can be modified during the execution of the program.</p>
<p>Although<span> </span><kbd>tf.Variable</kbd><span> seems to be</span> similar to<span> </span><kbd>tf.placeholder</kbd>, they have certain differences. These are listed in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><kbd><span>tf.placeholder</span></kbd></p>
</td>
<td>
<p><kbd><span>tf.Variable</span></kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>tf.placeholder</kbd><span> </span><span>defines the input data that does not get updated over time</span></p>
</td>
<td>
<p><kbd>tf.Variable</kbd><span> </span><span>defines values that get updated over time</span></p>
</td>
</tr>
<tr>
<td>
<p><kbd>tf.placeholder</kbd><span> </span><span>does not need to be provided with an initial value at the time of definition</span></p>
</td>
<td>
<p><kbd>tf.Variable</kbd><span> </span><span>needs an initial value to be provided at the time of definition</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In TensorFlow, a variable can be created with the API function<span> </span><kbd>tf.Variable()</kbd>. Let's look at an example of using placeholders and variables and create the following model in TensorFlow:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/276d3b44-1c56-4249-8e14-0b3625e49ba8.png" style="width:9.83em;height:1.67em;"/></p>
<ol>
<li>Define the model parameters<span> </span><kbd>w</kbd><span> </span>and<span> </span><kbd>b</kbd><span> </span>as variables with the initial values <kbd>[.3]</kbd><span> </span>and<span> </span><kbd>[-0.3]</kbd>:</li>
</ol>
<pre style="padding-left: 60px">w = tf.Variable([.3], tf.float32)<br/>b = tf.Variable([-.3], tf.float32)</pre>
<ol start="2">
<li>Define the input placeholder <kbd>x</kbd><span> </span>and the output operation node<span> </span><kbd>y</kbd>:</li>
</ol>
<pre style="padding-left: 60px">x = tf.placeholder(tf.float32)<br/>y = w * x + b</pre>
<ol start="3">
<li>Print the variables and placeholders<span> </span><kbd>w</kbd>, <kbd>v</kbd>, <kbd>x</kbd>, and<span> </span><kbd>y</kbd>:</li>
</ol>
<pre style="padding-left: 60px">print("w:",w)<br/>print("x:",x)<br/>print("b:",b)<br/>print("y:",y)</pre>
<p style="padding-left: 60px">The output depicts the <span><span>type of nodes</span></span> as <kbd>Variable</kbd>, <kbd>Placeholder</kbd>, or operation node, as follows:</p>
<pre style="padding-left: 60px">w: &lt;tf.Variable 'Variable:0' shape=(1,) dtype=float32_ref&gt;<br/>x: Tensor("Placeholder_2:0", dtype=float32)<br/>b: &lt;tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref&gt;<br/>y: Tensor("add:0", dtype=float32)</pre>
<p>The preceding output indicates that<span> </span><kbd>x</kbd><span> </span>is a <kbd>Placeholder</kbd> tensor, <kbd>y</kbd><span> </span>is an operation tensor, and<span> that </span><kbd>w</kbd><span> </span>and<span> </span><kbd>b</kbd><span> </span>are variables with a shape of<span> </span><kbd>(1,)</kbd><span> </span>and a data type of<span> </span><kbd>float32</kbd>.</p>
<p>The variables in a TensorFlow session have to be initialized before they can be used. We can either initialize a single variable by running its initializer operation or we can initialize all or a group of variables.</p>
<p>For example, to initialize the<span> </span><kbd>w</kbd> variable, we can use the following code:</p>
<pre>tfs.run(w.initializer)</pre>
<p>TensorFlow provides a convenient function that can initialize all of the variables:</p>
<pre>tfs.run(tf.global_variables_initializer())</pre>
<div class="packt_tip">TensorFlow also provides the <kbd>tf.variables_initializer()</kbd> function so that you can initialize a specific set of variables.</div>
<p>The global convenience function for initializing these variables can be executed in an alternative way. Instead of executing inside the<span> </span><kbd>run()</kbd><span> </span>function of a session object, the run function of the object returned by the initializer function itself can be executed:</p>
<pre>tf.global_variables_initializer().run()</pre>
<p>After the variables have been initialized, execute the model to get the output for the input values of <kbd>x = [1,2,3,4]</kbd>:</p>
<pre>print('run(y,{x:[1,2,3,4]}) : ',tfs.run(y,{x:[1,2,3,4]}))</pre>
<p><span>The output is as follows:</span></p>
<pre>run(y,{x:[1,2,3,4]}) :  [ 0.          0.30000001  0.60000002  0.90000004]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tensors generated from library functions</h1>
                </header>
            
            <article>
                
<p>TensorFlow provides various functions to generate tensors with pre-populated values. The generated values from these functions can be stored in a constant or variable tensor. Such generated values can also be provided to the tensor constructor at the time of initialization.</p>
<p>As an example, let's generate a 1-D tensor that's been pre-populated with <kbd>100</kbd> zeros:</p>
<pre>a=tf.zeros((100,))<br/>print(tfs.run(a))</pre>
<p>Some of the TensorFlow library functions that populate these tensors with different values at the time of their definition are listed as follows:</p>
<ul>
<li>Populating all of the elements of a tensor with similar values: <kbd>tf.ones_like()</kbd><span>, <kbd>tf.ones()</kbd>,<kbd> tf.fill()</kbd>, </span><kbd>tf.zeros()</kbd>, and<kbd>tf.zeros_like()</kbd> </li>
<li>Populating tensors with sequences: <span><kbd>tf.range()</kbd>,and </span><kbd>tf.lin_space()</kbd></li>
<li>Populating tensors with a probability distribution: <span><kbd>tf.random_uniform()</kbd>, </span><kbd>tf.random_normal()</kbd>, <span><kbd>tf.random_gamma()</kbd>,and </span><kbd>tf.truncated_normal()</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Obtaining variables with the tf.get_variable()</h1>
                </header>
            
            <article>
                
<p>If a variable is defined with a name that has already been used for another variable, then an exception is thrown by TensorFlow. <span>The</span> <kbd>tf.get_variable()</kbd> function makes it convenient and safe to create a variable <span>in place of using the</span> <kbd>tf<span>.Variable()</span></kbd> function. The<span> </span><kbd>tf.get_variable()</kbd><span> function </span>returns a variable that has been defined with a given name. If the variable with the given name does not exist, then it will create the variable with the specified initializer and shape.</p>
<p>Consider the following example:</p>
<pre>w = tf.get_variable(name='w',shape=[1],dtype=tf.float32,initializer=[.3])<br/>b = tf.get_variable(name='b',shape=[1],dtype=tf.float32,initializer=[-.3])</pre>
<p>The initializer can either be a list of values or another tensor. An initializer can also be one of the built-in initializers. Some of these are as follows:</p>
<ul>
<li><kbd>tf.ones_initializer</kbd></li>
<li><kbd>tf.constant_initializer</kbd></li>
<li><kbd>tf.zeros_initializer</kbd></li>
<li><kbd>tf.truncated_normal_initializer</kbd></li>
<li><kbd>tf.random_normal_initializer</kbd></li>
<li><kbd>tf.random_uniform_initializer</kbd></li>
<li><kbd>tf.uniform_unit_scaling_initializer</kbd></li>
<li><kbd>tf.orthogonal_initializer</kbd></li>
</ul>
<p>The<span> </span><kbd>tf.get_variable()</kbd><span> function only returns the</span> global variables when the code is run across multiple machines in<span> dis</span><span>tributed TensorFlow</span>. The local variables can be retrieved by using the<span> </span><kbd>tf.get_local_variable()</kbd> function.</p>
<div class="packt_infobox"><strong>Sharing or reusing variables</strong>:<span> </span>Getting variables that have already been defined promotes reuse. However, an exception will be thrown if the reuse flags are not set by using<span> </span><kbd>tf.variable_scope.reuse_variable()</kbd><span> </span>or<span> </span><kbd>tf.variable.scope(reuse=True)</kbd>.</div>
<p><span>Now that we have learned how to define tensors, constants, operations, placeholders, and variables, let's learn about the next level of abstraction in TensorFlow that combines these basic elements to form a basic unit of computation: the computation graph.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computation graph</h1>
                </header>
            
            <article>
                
<p>A<span> </span><strong>computation graph</strong><span> </span>is the basic unit of computation in TensorFlow. A computation graph consists of<span> </span>nodes<span> </span>and edges. Each node represents an instance of <kbd>tf.Operation</kbd>, while each edge represents an instance of <kbd>tf.Tensor</kbd> that gets transferred between the nodes.</p>
<p>A model in TensorFlow contains a computation graph. First, you must<span> </span>create<span> </span>the graph with the nodes representing variables, constants, placeholders, and operations, and then provide the graph to the TensorFlow execution engine. The TensorFlow execution engine finds the first set of nodes that it can execute. The execution of these nodes starts the execution of the nodes that follow the sequence of the computation graph.</p>
<p>Thus, TensorFlow-based programs are made up of performing two types of activities on computation graphs:</p>
<ul>
<li>Defining the computation graph</li>
<li>Executing the computation graph</li>
</ul>
<p>A TensorFlow program starts execution with a default graph. Unless another graph is explicitly specified, a new node gets implicitly added to the default graph. Explicit access to the default graph can be obtained using the following command:</p>
<pre>graph = tf.get_default_graph()</pre>
<p>For example, the following computation graph represents the addition of three inputs to produce the output, that is,<span> </span><img class="fm-editor-equation" src="assets/14e21277-f407-42a8-a84f-64349368adf2.png" style="width:10.25em;height:1.42em;"/>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-294 image-border" src="assets/8fdf5a9c-c960-445b-9059-3863ee9eb841.png" style="width:27.92em;height:13.08em;"/></div>
<p>In TensorFlow, the add operation node in the preceding diagram would correspond to the code <kbd>y = tf.add( x1 + x2 + x3 )</kbd>.</p>
<p>The variables, constants, and placeholders get added to the graph as and when they are created. After defining the computation graph, a session object is instantiated that <em>executes</em><span> </span>the operation objects and<span> </span><em>evaluates</em><span> </span>the tensor objects.</p>
<p>Let's define and execute a computation graph to calculate<span> </span><img class="fm-editor-equation" src="assets/b44e70e9-36d0-4a12-9e1d-42496c5e860d.png" style="width:9.25em;height:1.67em;"/>, just like we saw in the preceding example: </p>
<pre># Linear Model y = w * x + b<br/># Define the model parameters<br/>w = tf.Variable([.3], tf.float32)<br/>b = tf.Variable([-.3], tf.float32)<br/># Define model input and output<br/>x = tf.placeholder(tf.float32)<br/>y = w * x + b<br/>output = 0<br/><br/>with tf.Session() as tfs:<br/>   # initialize and print the variable y<br/>   tf.global_variables_initializer().run()<br/>   output = tfs.run(y,{x:[1,2,3,4]})<br/>print('output : ',output)</pre>
<p>Creating and using a session in the <kbd>with</kbd> block ensures that the session is automatically closed when the block is finished. Otherwise, the session has to be explicitly closed with the <kbd>tfs.close()</kbd><span> </span>command, where<span> </span><kbd>tfs</kbd><span> </span>is the session name.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The order of execution and lazy loading</h1>
                </header>
            
            <article>
                
<p>The nodes in a computation graph are executed in their order of dependency. If node <em>x</em> depends on node <em>y</em>, then <em>x</em> is executed before <em>y </em>when the execution of <em>y</em> is requested. A node is only executed if either the node itself or another node depending on it is invoked for execution. This execution philosophy is known as lazy loading. As the name implies, the node objects are not instantiated and initialized until they are actually required.</p>
<p>Often, it is necessary to control the order of the execution of the nodes in a computation graph. This can be done with the <kbd>tf.Graph.control_dependencies()</kbd> function. For example, if the graph has the nodes <kbd>l</kbd><em>,</em> <kbd>m</kbd><em>,</em> <kbd>n</kbd><em>,</em> and <kbd>o</kbd>, and we want to execute <kbd>n</kbd> and <kbd>o</kbd> before <kbd>l</kbd> and <kbd>m</kbd>, then we would use the following code:</p>
<pre>with graph_variable.control_dependencies([n,o]):<br/>  # other statements here</pre>
<p>This makes sure that any node in the preceding <kbd>with</kbd> block is executed after nodes<span> </span><kbd>n</kbd><span> </span>and<span> </span><kbd>o</kbd><span> </span>have been executed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Executing graphs across compute devices – CPU and GPGPU</h1>
                </header>
            
            <article>
                
<p>A graph can be partitioned into several parts,<span> </span>and<span> </span>each part can be placed and executed on different devices, such as a CPU or GPU. All of the devices that are available for graph execution can be listed with the following command:</p>
<pre>from tensorflow.python.client import device_lib<br/>print(device_lib.list_local_devices())</pre>
<p>The output is listed as follows (the output for your machine will be different because this will depend on the available compute devices in your specific system):</p>
<pre>[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 12900903776306102093
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 611319808
locality {
  bus_id: 1
}
incarnation: 2202031001192109390
physical_device_desc: "device: 0, name: Quadro P5000, pci bus id: 0000:01:00.0, compute capability: 6.1"
]</pre>
<p>The devices in TensorFlow are identified with the string<span> </span><kbd>/device:&lt;device_type&gt;:&lt;device_idx&gt;</kbd>. In the last output,<span> </span><kbd>CPU</kbd><span> </span>and<span> </span><kbd>GPU</kbd><span> </span>denote the device type, and<span> </span><kbd>0</kbd><span> </span>denotes the device index.</p>
<p>One thing to note about the last output is that it shows only one CPU, whereas our computer has 8 CPUs. The reason for this is that TensorFlow implicitly distributes the code across the CPU units and thus, by default,<span> </span><kbd>CPU:0</kbd><span> </span>denotes all of the CPUs available to TensorFlow. When TensorFlow starts executing graphs, it runs the independent paths within each graph in a separate thread, with each thread running on a separate CPU. We can restrict the number of threads used for this purpose by changing the number of<span> </span><kbd>inter_op_parallelism_threads</kbd>. Similarly, if, within an independent path, an operation is capable of running on multiple threads, TensorFlow will launch that specific operation on multiple threads. The number of threads in this pool can be changed by setting the number of<span> </span><kbd>intra_op_parallelism_threads</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placing graph nodes on specific compute devices</h1>
                </header>
            
            <article>
                
<p>To enable the logging of variable placement by defining a config object, set the<span> </span><kbd>log_device_placement</kbd><span> </span>property to<span> </span><kbd>true</kbd>, and then pass this<span> </span><kbd>config</kbd> object to the session as follows:</p>
<pre>tf.reset_default_graph()<br/><br/><span># Define model parameters<br/></span>w = tf.Variable([.3], tf.float32)<br/>b = tf.Variable([-.3], tf.float32)<br/><span># Define model input and output<br/></span>x = tf.placeholder(tf.float32)<br/>y = w * x + b<br/><br/>config = tf.ConfigProto()<br/>config.log_device_placement=True<br/><br/><span>with tf.Session(config=config) as tfs:<br/></span><span>   </span><span># initialize and print the variable y<br/></span><span>   </span>tfs.run(global_variables_initializer())<br/>   print('output',tfs.run(y,{x:[1,2,3,4]}))</pre>
<p>The output from the console window of the Jupyter Notebook is listed as follows:</p>
<pre>b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0<br/>b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0<br/>b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0<br/>w: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0<br/>w/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0<br/>mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0<br/>add: (Add): /job:localhost/replica:0/task:0/device:GPU:0<br/>w/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0<br/>init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0<br/>x: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0<br/>b/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0<br/>Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0<br/>w/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0<br/>Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0</pre>
<p>Thus, by default, TensorFlow creates the variable and operations nodes on a device so that it can get the highest performance. <span>These variables and operations can be placed on specific devices by using the </span><kbd>tf.device()</kbd><span> function. Let's place the graph on the CPU:</span></p>
<pre>tf.reset_default_graph()<br/><br/><span>with tf.device('/device:CPU:0'):<br/></span><span>    </span><span># Define model parameters<br/></span><span>    </span>w = tf.get_variable(name='w',initializer=[.3], dtype=tf.float32)<br/>    b = tf.get_variable(name='b',initializer=[-.3], dtype=tf.float32)<br/>    <span># Define model input and output<br/></span><span>    </span>x = tf.placeholder(name='x',dtype=tf.float32)<br/>    y = w * x + b<br/><br/>config = tf.ConfigProto()<br/>config.log_device_placement=True<br/><br/><span>with tf.Session(config=config) as tfs:<br/></span><span>   </span><span># initialize and print the variable y<br/></span><span>   </span>tfs.run(tf.global_variables_initializer())<br/>   print('output',tfs.run(y,{x:[1,2,3,4]}))</pre>
<p>In the Jupyter console, we can see that the variables have been placed on the CPU and that execution also takes place on the CPU:</p>
<pre><span>b: </span>(VariableV2): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>b/read: </span>(Identity): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>b/Assign: </span>(Assign): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>w: </span>(VariableV2): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>w/read: </span>(Identity): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>mul: </span>(Mul): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>add: </span>(Add): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>w/Assign: </span>(Assign): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>init: </span>(NoOp): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>x: </span>(Placeholder): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>b/initial_value: </span>(Const): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>Const_1: </span>(Const): /job:localhost/replica:0/task:0/device:CPU:0<br/><span>w/initial_value: </span>(Const): /job:localhost/replica:0/task:0/device:CPU<strong>:0</strong><br/><span>Const: </span>(Const): /job:localhost/replica:0/task:0/device:CPU:0</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simple placement</h1>
                </header>
            
            <article>
                
<p>TensorFlow follows the following rules for placing the variables on devices:</p>
<pre>If the graph was previously run, <br/>    then the node is left on the device where it was placed earlier<br/>Else If the tf.device() block is used,<br/>    then the node is placed on the specified device<br/>Else If the GPU is present<br/>    then the node is placed on the first available GPU<br/>Else If the GPU is not present<br/>    then the node is placed on the CPU</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic placement</h1>
                </header>
            
            <article>
                
<p>The<span> </span><kbd>tf.device()</kbd><span> function </span>can be provided with a function name in place of a device string. If a function name is provided, then the function has to return the device string. This way of providing a device string through a custom function allows complex algorithms to be used for placing the variables on different devices. For example, TensorFlow provides a round robin device setter function in<span> </span><kbd>tf.train.replica_device_setter()</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Soft placement</h1>
                </header>
            
            <article>
                
<p>If a TensorFlow operation is placed on the GPU, then the execution engine must have the GPU implementation of that operation, known as the <strong>kernel</strong>. If the kernel is not present, then the placement results in a runtime error. Also, if the requested GPU device does not exist, then a runtime error is raised. The best way to handle such errors is to allow the operation to be placed on the CPU if requesting the GPU device results in an error. This can be achieved by setting the following<span> </span><kbd>config</kbd><span> </span>value:</p>
<pre>config.allow_soft_placement = True</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GPU memory handling</h1>
                </header>
            
            <article>
                
<p>At the start of the TensorFlow session, by default, a session grabs all of the GPU memory, even if the operations and variables are placed only on one GPU in a multi-GPU system. If another session starts execution at the same time, it will receive an out-of-memory error. This can be solved in multiple ways:</p>
<ul>
<li>For multi-GPU systems, set the environment variable<span> </span><kbd>CUDA_VISIBLE_DEVICES=&lt;list of device idx&gt;</kbd>:</li>
</ul>
<pre style="padding-left: 60px">os.environ['CUDA_VISIBLE_DEVICES']='0'</pre>
<p style="padding-left: 60px">The code that's executed after this setting will be able to grab all of the memory of the visible GPU.</p>
<ul>
<li>For letting the session grab a part of the memory of the GPU, use the config option <kbd>per_process_gpu_memory_fraction</kbd><span> </span>to allocate a percentage of the memory:</li>
</ul>
<pre style="padding-left: 60px">config.gpu_options.per_process_gpu_memory_fraction = 0.5</pre>
<p style="padding-left: 60px">This will allocate 50% of the memory in all of the GPU devices.</p>
<ul>
<li>By combining both of the preceding strategies, you can make only a certain percentage, alongside just some of the GPU, visible to the process.</li>
<li>Limit the TensorFlow process to grab only the minimum required memory at the start of the process. As the process executes further, set a config option to allow for the growth of this memory:</li>
</ul>
<pre style="padding-left: 60px">config.gpu_options.allow_growth = True</pre>
<p style="padding-left: 60px">This option only allows for the allocated memory to grow, so the memory is never released back.</p>
<div class="packt_infobox">To find out more about learning techniques for distributing computation across multiple compute devices, refer to our book, <em>Mastering TensorFlow</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiple graphs</h1>
                </header>
            
            <article>
                
<p>We can create our own graphs, which are separate from the default graph, and execute them in a session. However, creating<span> </span>and<span> </span>executing multiple graphs is not recommended, because of the following disadvantages:</p>
<ul>
<li>Creating and using multiple graphs in the same program would require multiple TensorFlow sessions, and each session would consume heavy resources</li>
<li>Data cannot be directly passed in-between graphs</li>
</ul>
<p>Hence, the recommended approach is to have multiple subgraphs in a single graph. In case we wish to use our own graph instead of the default graph, we can do so with the <kbd>tf.graph()</kbd><span> </span>command. In the following example, we create our own graph, <kbd>g</kbd>, and execute it as the default graph:</p>
<pre>g = tf.Graph()<br/>output = 0<br/><br/># Assume Linear Model y = w * x + b<br/><br/>with g.as_default():<br/> # Define model parameters<br/> w = tf.Variable([.3], tf.float32)<br/> b = tf.Variable([-.3], tf.float32)<br/> # Define model input and output<br/> x = tf.placeholder(tf.float32)<br/> y = w * x + b<br/><br/>with tf.Session(graph=g) as tfs:<br/> # initialize and print the variable y<br/> tf.global_variables_initializer().run()<br/> output = tfs.run(y,{x:[1,2,3,4]})<br/> <br/>print('output : ',output)</pre>
<p>Now, let's put this learning into practice and implement the classification of handwritten digital images with TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning, classification, and logistic regression</h1>
                </header>
            
            <article>
                
<p>Let's now learn about machine learning, classification, and logistic regression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning</h1>
                </header>
            
            <article>
                
<p>Machine learning refers to the application of algorithms to make computers learn from data. The models that are learned by computers are used to make predictions and forecasts. Machine learning has been successfully applied in a variety of areas, such as natural language processing, self-driving vehicles, image and speech recognition, chatbots, and computer vision.</p>
<p>Machine learning algorithms are broadly categorized into three types:</p>
<ul>
<li><strong>Supervised learning</strong>: In supervised learning, the machine learns the model from a training dataset that consists of features and labels. The supervised learning problems are generally of two types: <em>regression</em> and <em>classification</em>. Regression refers to predicting future values based on the model, while classification refers to predicting the categories of the input values.</li>
<li><strong>Unsupervised learning</strong>: In unsupervised learning, the machine learns the model from a training dataset that consists of features only. One of the most common types of unsupervised learning is known as <strong>clustering</strong>. Clustering refers to dividing the input data into multiple groups, thus producing clusters or segments.</li>
<li><strong>Reinforcement learning</strong>: In reinforcement learning, the agent starts with an initial model and then continuously learns the model based on the feedback from the environment. A reinforcement learning agent learns or updates the model by applying supervised or unsupervised learning techniques as part of the reinforcement learning algorithms.</li>
</ul>
<p>These machine learning problems are abstracted to the following equation in one form or another:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ce510bf8-62f2-44b1-8114-679eae1cbd33.png" style="width:4.67em;height:1.42em;"/></div>
<p>Here,<span> </span><em>y</em><span> represents</span> the <em>target</em> and<span> </span><em>x</em><span> represents</span> the <em>feature</em>. If<span> </span><em>x</em><span> </span>is a collection of features, it is also called a feature vector and denoted with<span> </span><em>X</em>. The model is the function<span> </span><em>f</em><span> </span>that maps features to targets. Once the computer learns<span> </span><em>f</em>, it can use the new values of<span> </span><em>x</em><span> </span>to predict the values of<span> </span><em>y</em>.</p>
<p>The preceding simple equation can be rewritten in the context of linear models for machine learning as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cca6472f-8449-4fc9-a9ef-16ff8a8f76d7.png" style="width:6.67em;height:1.50em;"/></div>
<p>Here, <em>w</em> is known as the weight and <em>b</em> is known as the bias. Thus, the machine learning problem now can be stated as a problem of finding w and <em>b</em> from the current values of <em>X</em> so that the equation can now be used to predict the values of <em>y</em>. </p>
<p>Regression analysis or regression modeling<span> </span>refers<span> </span>to the methods and techniques used to estimate relationships among variables. The variables that are used as input for regression models are called independent variables, predictors, or features, and the output variables from regression models are called dependent variables or targets. Regression models are defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4f835c01-2187-4d76-aa73-58a00674429d.png" style="width:7.08em;height:1.50em;"/></div>
<p>Where <em>Y</em><span> </span>is the target variable, <em>X</em><span> </span>is a vector of features, and <em>β</em><span> </span>is a vector of parameters (<em>w</em>,<em>b</em> in the preceding equation).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification</h1>
                </header>
            
            <article>
                
<p>Classification is one of the classical problems in machine learning. Data<span> </span>under<span> </span>consideration could belong to one class or another, for example, if the images provided are data, they could be pictures of cats or dogs. Thus, the classes, in this case, are cats and dogs. Classification means identifying the label or class of the objects under consideration. Classification falls under the umbrella of supervised machine learning. In classification problems, a training dataset is provided that has features or inputs and their corresponding outputs or labels. Using this training dataset, a model is trained; in other words,<span> </span>the parameters<span> </span>of the model are computed. The trained model is then used on new data to find its correct labels.</p>
<p>Classification problems can be of two types:<span> </span><strong>binary class</strong><span> </span>or<span> </span><strong>multiclass</strong>. Binary class<span> </span>means<span> </span>that the data is to be classified<span> </span>into<span> </span>two distinct and discrete labels; for example, the patient has cancer or the patient does not have cancer, and the images are of cats or dogs and so on. Multiclass<span> </span>means<span> that </span>the data is to be classified among multiple classes, for example, an email classification problem will divide emails into social media emails, work-related emails, personal emails, family-related emails, spam emails, shopping offer emails, and so on. Another example would be of pictures of digits; each picture could be labeled between 0 and 9, depending on what digit the picture represents. In this chapter, we will look at examples of both kinds of classification.</p>
<p>The most popular method for<span> </span>classification<span> </span>is logistic regression. Logistic<span> </span>regression<span> </span>is a probabilistic and linear classifier. The probability that the vector of input features belongs to a specific class can be described mathematically by the following equation: </p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9a303e0f-ce6b-4910-9193-b20ff0d71ff8.png" style="width:15.58em;height:1.75em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">In the <span>preceding </span>equation, the following applies<span>:</span></p>
<ul>
<li><em>Y</em><span> </span>represents the output</li>
<li><em>i</em><span> </span>represents one of the classes</li>
<li><em>x</em><span> </span>represents the inputs</li>
<li><em>w</em><span> </span><span>represents the weights</span></li>
<li><em>b</em><span> </span>represents the biases</li>
<li><em>z</em><span> </span>represents the regression equation<span> </span><img class="fm-editor-equation" src="assets/c31c0e53-eceb-4e8b-99aa-4ba0c19db77b.png" style="width:9.25em;height:1.42em;"/></li>
<li><span><em>ϕ</em> represents the smoothing function (or model, in our case)</span></li>
</ul>
<p>The<span> </span><span><em>ϕ(z)</em></span> function represents the probability that<span> </span><em>x</em><span> </span>belongs to class<span> </span><em>i</em><span> </span>when<span> </span><em>w</em><span> </span>and<span> </span><em>b</em><span> </span>are given<span>. Thus, the model has to be trained to maximize the value of this probability.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression for binary classification</h1>
                </header>
            
            <article>
                
<p>For binary classification, the<span> </span>model<span> </span>function <em>ϕ(z)</em> is defined as<em> </em>the sigmoid function, which can be described as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dcf29118-cfc3-4624-bc9b-5350dd509600.png" style="width:21.83em;height:3.75em;"/></div>
<p>The sigmoid function transforms the <em>y</em> value to be<span> </span><span>between the range [0,1]. Thus, the value of <em>y=ϕ(z)</em> can be used to predict the class: if <em>y</em> &gt; 0.5, then the object belongs to 1, otherwise the object belongs to 0.</span></p>
<p>The<span> </span>model<span> </span>training means to search for the parameters that minimize the loss function, which can either be the sum of squared errors or the sum of mean squared errors. For logistic regression, the likelihood is maximized as follows:</p>
<p class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/cab647bc-44e9-466c-bfda-61edd8576e9d.png" style="width:10.67em;height:1.42em;"/></p>
<p>However, as it is easier to maximize the log-likelihood, we use the <span>log-likelihood (</span><em>l(w)</em><span>) </span>as the cost function. The loss function (<em>J(w)</em>) is written as<span> </span><em>-l(w)</em>, and can be minimized by using optimization algorithms such as gradient descent.</p>
<p>The loss function for binary logistic regression is written mathematically as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3d3c322a-f6bc-4f4c-b2d7-778782788a8c.png" style="width:36.17em;height:4.00em;"/></div>
<p>Here,<span> </span><em>ϕ(z)</em><span> </span>is the sigmoid function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression for multiclass classification</h1>
                </header>
            
            <article>
                
<p>When more than two<span> </span>classes<span> are </span>involved, logistic regression is<span> </span>known <span>as </span>multinomial logistic regression. In multinomial logistic regression, instead of sigmoid, use<span> </span>the softmax<span> </span>function, which can be described mathematically as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/825ae940-aa27-4241-ad2d-7443037331a7.png" style="width:27.83em;height:5.17em;"/></div>
<p>The softmax function produces the probabilities for each class so that the probabilities vector adds up to <em>1</em>. At the time of inference, the class with<span> </span>the highest<span> </span>softmax value becomes the output or predicted class. The loss function, as we discussed earlier, is the negative log-likelihood function,<span> </span><em>-l(w)</em><span>, </span>that can be minimized by the optimizers, such as gradient descent.</p>
<p><span>The loss function for multinomial logistic regression is written formally as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/632dd8f0-9c91-453c-a5c0-ed267c5d80b3.png" style="width:18.58em;height:4.25em;"/></div>
<p><span>Here,</span><span> </span><em>ϕ(z)</em><span> </span><span>is the softmax</span><span> function.</span></p>
<p><span>We will implement this loss function in the next section. </span>In the following section, we will dig into our example for multiclass classification with logistic regression in TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression with TensorFlow</h1>
                </header>
            
            <article>
                
<p><span>One of the most popular examples regarding </span>multiclass<span> classification is to label the images of handwritten digits. The classes, or labels, in this example are <em>{0,1,2,3,4,5,6,7,8,9}</em>. The dataset that we are going to use is popularly known as MNIST and is available from the following link: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>. The MNIST dataset has 60,000 images for training and 10,000 images for testing. The images in the dataset appear as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-295 image-border" src="assets/95ce1802-2f03-4e6a-ac4b-a4289bc12086.png" style="width:13.83em;height:10.83em;"/></p>
<ol>
<li>First, we must import <kbd>datasetslib</kbd>, a library that was written by us to help with examples in this book (<span>available as a submodule of this book's GitHub repository)</span>:</li>
</ol>
<pre style="padding-left: 60px">DSLIB_HOME = <span>'../datasetslib'<br/></span><span>import </span>sys<br/><span>if not </span>DSLIB_HOME <span>in </span>sys.path:<br/>    sys.path.append(DSLIB_HOME)<br/>%reload_ext autoreload<br/>%autoreload <span>2<br/></span><span>import </span>datasetslib <span>as </span>dslib<br/><br/><span>from </span>datasetslib.utils <span>import </span>imutil<br/><span>from </span>datasetslib.utils <span>import </span>nputil<br/><span>from </span>datasetslib.mnist <span>import </span>MNIST</pre>
<ol start="2">
<li>Set the path to the <kbd>datasets</kbd> folder in our home directory, which is where we want all of the <kbd>datasets</kbd> to be stored:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>datasets_root = os.path.join(os.path.expanduser('~'),'datasets')</pre>
<ol start="3">
<li>Get the MNIST data using our <kbd>datasetslib</kbd> and print the shapes to ensure that the data is loaded properly:</li>
</ol>
<pre style="padding-left: 60px">mnist=MNIST()<br/><br/>x_train,y_train,x_test,y_test=mnist.load_data()<br/><br/>mnist.y_onehot = True<br/>mnist.x_layout = imutil.LAYOUT_NP<br/>x_test = mnist.load_images(x_test)<br/>y_test = nputil.onehot(y_test)<br/><br/>print('Loaded x and y')<br/>print('Train: x:{}, y:{}'.format(len(x_train),y_train.shape))<br/>print('Test: x:{}, y:{}'.format(x_test.shape,y_test.shape))</pre>
<ol start="4">
<li>Define the hyperparameters for training the model:</li>
</ol>
<pre style="padding-left: 60px">learning_rate = <span>0.001<br/></span>n_epochs = <span>5<br/></span>mnist.batch_size = <span>100</span></pre>
<ol start="5">
<li>Define the placeholders and parameters for our simple model:</li>
</ol>
<pre style="padding-left: 60px"><span># define input images<br/></span>x = tf.placeholder(<span>dtype</span>=tf.float32, <span>shape</span>=[<span>None</span>, mnist.n_features])<br/><span># define output labels<br/></span>y = tf.placeholder(<span>dtype</span>=tf.float32, <span>shape</span>=[<span>None</span>, mnist.n_classes])<br/><span><br/></span><span># model parameters<br/></span>w = tf.Variable(tf.zeros([mnist.n_features, mnist.n_classes]))<br/>b = tf.Variable(tf.zeros([mnist.n_classes]))</pre>
<ol start="6">
<li>Define the model with <kbd>logits</kbd> and <kbd>y_hat</kbd>:</li>
</ol>
<pre style="padding-left: 60px">logits = tf.add(tf.matmul(x, w), b)<br/>y_hat = tf.nn.softmax(logits)</pre>
<ol start="7">
<li>Define the <kbd>loss</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">epsilon = tf.keras.backend.epsilon()<br/>y_hat_clipped = tf.clip_by_value(y_hat, epsilon, <span>1 </span>- epsilon)<br/>y_hat_log = tf.log(y_hat_clipped)<br/>cross_entropy = -tf.reduce_sum(y * y_hat_log, <span>axis</span>=<span>1</span>)<br/>loss_f = tf.reduce_mean(cross_entropy)</pre>
<ol start="8">
<li>Define the <kbd>optimizer</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">optimizer = tf.train.GradientDescentOptimizer<br/>optimizer_f = optimizer(<span>learning_rate</span>=learning_rate).minimize(loss_f)</pre>
<ol start="9">
<li>Define the function to check the accuracy of the trained model:</li>
</ol>
<pre style="padding-left: 60px">predictions_check = tf.equal(tf.argmax(y_hat, <span>1</span>), tf.argmax(y, <span>1</span>))<br/>accuracy_f = tf.reduce_mean(tf.cast(predictions_check, tf.float32))</pre>
<ol start="10">
<li>Run the <kbd>training</kbd> loop for each epoch in a TensorFlow session:</li>
</ol>
<pre style="padding-left: 60px">n_batches = <span>int</span>(<span>60000</span>/mnist.batch_size)<br/><br/><span>with </span>tf.Session() <span>as </span>tfs:<br/>    tf.global_variables_initializer().run()<br/>    <span>for </span>epoch <span>in </span><span>range</span>(n_epochs):<br/>        mnist.reset_index()<br/>        <span>for </span>batch <span>in </span><span>range</span>(n_batches):<br/>            x_batch, y_batch = mnist.next_batch()<br/>            feed_dict={x: x_batch, y: y_batch}<br/>            batch_loss,_ = tfs.run([loss_f, optimizer_f],<span>feed_dict</span>=feed_dict )<br/>            <span>#print('Batch loss:{}'.format(batch_loss))<br/></span></pre>
<ol start="11">
<li>Run the evaluation function for each epoch with the test data in the same TensorFlow session that was created previously:</li>
</ol>
<pre style="padding-left: 60px">feed_dict = {x: x_test, y: y_test}<br/>accuracy_score = tfs.run(accuracy_f, <span>feed_dict</span>=feed_dict)<br/><span>print</span>(<span>'epoch {0:04d}  accuracy={1:.8f}'<br/></span><span>      </span>.format(epoch, accuracy_score))</pre>
<p>We get the following output:</p>
<pre>epoch 0000 accuracy=0.73280001 epoch 0001 accuracy=0.72869998 epoch 0002 accuracy=0.74550003 epoch 0003 accuracy=0.75260001 epoch 0004 accuracy=0.74299997</pre>
<p>There you go. We just trained our very first logistic regression model using TensorFlow for classifying handwritten digit images and got 74.3% accuracy.</p>
<p>Now, let's see how writing the same model in Keras makes this process even easier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression with Keras</h1>
                </header>
            
            <article>
                
<p><span><strong>Keras</strong> is a high-level library that is available as part of TensorFlow. In this section, we will rebuild the same model we built earlier with TensorFlow core with Keras:</span></p>
<ol>
<li><span>Keras takes data in a different format, and so we must first reformat the data using </span><kbd>datasetslib</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px">x_train_im = mnist.load_images(x_train)<br/><br/>x_train_im, x_test_im = x_train_im / <span>255.0</span>, x_test / <span>255.0</span></pre>
<p style="padding-left: 60px">In the preceding code, we are loading the training images in memory before both the training and test images are scaled, which we do by dividing them by <kbd>255</kbd>.</p>
<ol start="2">
<li>Then, we build the model:</li>
</ol>
<pre style="padding-left: 60px">model = tf.keras.models.Sequential([<br/>    tf.keras.layers.Flatten(),<br/>    tf.keras.layers.Dense(<span>10</span>, <span>activation</span>=tf.nn.softmax)<br/>])</pre>
<ol start="3">
<li>Compile the model with the <kbd>sgd</kbd> optimizer. Set the categorical entropy as the <kbd>loss</kbd> function and the accuracy as a metric to test the model:</li>
</ol>
<pre style="padding-left: 60px">model.compile(<span>optimizer</span>=<span>'sgd'</span>,<br/>              <span>loss</span>=<span>'sparse_categorical_crossentropy'</span>,<br/>              <span>metrics</span>=[<span>'accuracy'</span>])</pre>
<ol start="4">
<li>Train the model for <kbd>5</kbd> epochs with the training set of images and labels:</li>
</ol>
<pre style="padding-left: 60px">model.fit(x_train_im, y_train, <span>epochs</span>=<span>5</span>)<br/><br/>Epoch 1/5
60000/60000 [==============================] - 3s 45us/step - loss: 0.7874 - acc: 0.8095
Epoch 2/5
60000/60000 [==============================] - 3s 42us/step - loss: 0.4585 - acc: 0.8792
Epoch 3/5
60000/60000 [==============================] - 2s 42us/step - loss: 0.4049 - acc: 0.8909
Epoch 4/5
60000/60000 [==============================] - 3s 42us/step - loss: 0.3780 - acc: 0.8965
Epoch 5/5
60000/60000 [==============================] - 3s 42us/step - loss: 0.3610 - acc: 0.9012
10000/10000 [==============================] - 0s 24us/step</pre>
<ol start="5">
<li>Evaluate the model with the test data:</li>
</ol>
<pre style="padding-left: 60px">model.evaluate(x_test_im, nputil.argmax(y_test))</pre>
<p>We get the following evaluation scores as output:</p>
<div class="output_area">
<div class="output_subarea output_text output_result">
<pre>[0.33530342621803283, 0.9097]</pre></div>
</div>
<p>Wow! Using Keras, we can achieve higher accuracy. We achieved approximately 90% accuracy. This is because Keras internally sets many optimal values for us so that we can quickly start building models.</p>
<div class="packt_infobox">To learn more about Keras and to look at more examples, refer to the book <em>Mastering TensorFlow,</em> from Packt Publications.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we briefly covered the TensorFlow library. We covered the TensorFlow data model elements, such as constants, variables, and placeholders, and how they can be used to build TensorFlow computation graphs. We learned how to create tensors from Python objects. Tensor objects can also be generated as specific values, sequences, or random valued distributions from various TensorFlow library functions. </p>
<p>We covered the TensorFlow programming model, which includes defining and executing computation graphs. These computation graphs have nodes and edges. The nodes represent operations and edges represent tensors that transfer data from one node to another. We covered how to create and execute graphs, the order of execution, and how to execute graphs on multiple compute devices, such as CPU and GPU.</p>
<p>We also learned about machine learning and implemented a classification algorithm to identify the handwritten digits dataset. The algorithm we implemented is known as multinomial logistic regression. We used both TensorFlow core and Keras to implement the logistic regression algorithm.</p>
<p>Starting from the next chapter, we will look at many projects that will be implemented using TensorFlow and Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p class="mce-root">Enhance your understanding by practicing the following questions:</p>
<ol>
<li>Modify the logistic regression model that was given in this chapter so that you can use different training rates and observe how it impacts training</li>
<li>Use different optimizer functions and observe the impact of different functions on training time and accuracy</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>We suggest the reader learn more by reading the following materials:</p>
<ul>
<li><em>Mastering TensorFlow</em> by Armando Fandango.</li>
<li>TensorFlow tutorials at <a href="https://www.tensorflow.org/tutorials/">https://www.tensorflow.org/tutorials/</a>.<a href="https://www.tensorflow.org/tutorials/"/></li>
<li>
<div class="book-top-block-info-title float-left">
<p><em>TensorFlow 1.x Deep Learning Cookbook</em> by <span>Antonio Gulli and Amita Kapoor</span></p>
</div>
</li>
</ul>


            </article>

            
        </section>
    </body></html>