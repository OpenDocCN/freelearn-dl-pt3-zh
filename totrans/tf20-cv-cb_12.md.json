["```\n$> pip install Pillow git+https://github.com/tensorflow/docs\n```", "```\n    import os\n    import pathlib\n    from glob import glob\n    import numpy as np\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Model\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    def load_images_and_labels(image_paths, \n                               target_size=(64, 64)):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, \n                              target_size=target_size)\n            image = img_to_array(image)\n            label = image_path.split(os.path.sep)[-2]\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    def build_network(width, height, depth, classes):\n        input_layer = Input(shape=(width, height, depth))\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(input_layer)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=512)(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.25)(x)\n        x = Dense(units=classes)(x)\n        output = Softmax()(x)\n        return Model(input_layer, output)\n    ```", "```\n    def plot_model_history(model_history, metric, \n                           plot_name):\n        plt.style.use('seaborn-darkgrid')\n        plotter = tfdocs.plots.HistoryPlotter()\n        plotter.plot({'Model': model_history}, \n                      metric=metric)\n        plt.title(f'{metric.upper()}')\n        plt.ylim([0, 1])\n        plt.savefig(f'{plot_name}.png')\n        plt.close()\n    ```", "```\n    SEED = 999\n    np.random.seed(SEED)\n    ```", "```\n    base_path = (pathlib.Path.home() / '.keras' / \n                  'datasets' /\n                 '101_ObjectCategories')\n    images_pattern = str(base_path / '*' / '*.jpg')\n    image_paths = [*glob(images_pattern)]\n    image_paths = [p for p in image_paths if\n                   p.split(os.path.sep)[-2] !='BACKGROUND_Google']\n    CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}\n    ```", "```\n    X, y = load_images_and_labels(image_paths)\n    X = X.astype('float') / 255.0\n    y = LabelBinarizer().fit_transform(y)\n    ```", "```\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y,\n                                         test_size=0.2,\n                                      random_state=SEED)\n    ```", "```\n    BATCH_SIZE = 64\n    STEPS_PER_EPOCH = len(X_train) // BATCH_SIZE\n    EPOCHS = 40\n    ```", "```\n    augmenter = ImageDataGenerator(horizontal_flip=True,\n                                   rotation_range=30,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   fill_mode='nearest')\n    ```", "```\n    NUM_MODELS = 5\n    ensemble_preds = []\n    ```", "```\n    for n in range(NUM_MODELS):\n        print(f'Training model {n + 1}/{NUM_MODELS}')\n        model = build_network(64, 64, 3, len(CLASSES))\n        model.compile(loss='categorical_crossentropy',\n                      optimizer='rmsprop',\n                      metrics=['accuracy'])\n    ```", "```\n        train_generator = augmenter.flow(X_train, y_train,\n                                         BATCH_SIZE)\n        hist = model.fit(train_generator,\n                         steps_per_epoch=STEPS_PER_EPOCH,\n                         validation_data=(X_test, y_test),\n                         epochs=EPOCHS,\n                         verbose=2)\n    ```", "```\n        predictions = model.predict(X_test, \n                                   batch_size=BATCH_SIZE)\n        accuracy = accuracy_score(y_test.argmax(axis=1),\n                                  predictions.argmax(axis=1))\n        print(f'Test accuracy (Model #{n + 1}): {accuracy}')\n        plot_model_history(hist, 'accuracy', f'model_{n +1}')\n        ensemble_preds.append(predictions)\n    ```", "```\n    ensemble_preds = np.average(ensemble_preds, axis=0)\n    ensemble_acc = accuracy_score(y_test.argmax(axis=1),\n                             ensemble_preds.argmax(axis=1))\n    print(f'Test accuracy (ensemble): {ensemble_acc}')\n    ```", "```\n    Test accuracy (Model #1): 0.6658986175115207\n    Test accuracy (Model #2): 0.6751152073732719\n    Test accuracy (Model #3): 0.673963133640553\n    Test accuracy (Model #4): 0.6491935483870968\n    Test accuracy (Model #5): 0.6756912442396313\n    ```", "```\nTest accuracy (ensemble): 0.7223502304147466\n```", "```\n$> pip install Pillow\n```", "```\n    import os\n    import pathlib\n    from glob import glob\n    import numpy as np\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Model\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    def load_images_and_labels(image_paths, \n                               target_size=(64, 64)):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, \n                            target_size=target_size)\n            image = img_to_array(image)\n            label = image_path.split(os.path.sep)[-2]\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    def build_network(width, height, depth, classes):\n        input_layer = Input(shape=(width, height, depth))\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(input_layer)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=512)(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.25)(x)\n        x = Dense(units=classes)(x)\n        output = Softmax()(x)\n        return Model(input_layer, output)\n    ```", "```\n    def flip_augment(image, num_test=10):\n        augmented = []\n        for i in range(num_test):\n            should_flip = np.random.randint(0, 2)\n            if should_flip:\n                flipped = np.fliplr(image.copy())\n                augmented.append(flipped)\n            else:\n                augmented.append(image.copy())\n        return np.array(augmented)\n    ```", "```\n    SEED = 84\n    np.random.seed(SEED)\n    ```", "```\n    base_path = (pathlib.Path.home() / '.keras' / \n                 'datasets' /'101_ObjectCategories')\n    images_pattern = str(base_path / '*' / '*.jpg')\n    image_paths = [*glob(images_pattern)]\n    image_paths = [p for p in image_paths if\n                   p.split(os.path.sep)[-2] \n                   !='BACKGROUND_Google']\n    CLASSES = {p.split(os.path.sep)[-2] for p in \n               image_paths}\n    ```", "```\n    X, y = load_images_and_labels(image_paths)\n    X = X.astype('float') / 255.0\n    y = LabelBinarizer().fit_transform(y)\n    ```", "```\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y, test_size=0.2,\n                                      random_state=SEED)\n    ```", "```\n    BATCH_SIZE = 64\n    EPOCHS = 40\n    ```", "```\n    augmenter = ImageDataGenerator(horizontal_flip=True)\n    ```", "```\n    model = build_network(64, 64, 3, len(CLASSES))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    ```", "```\n    train_generator = augmenter.flow(X_train, y_train,\n                                     BATCH_SIZE)\n    model.fit(train_generator,\n              steps_per_epoch=len(X_train) // BATCH_SIZE,\n              validation_data=(X_test, y_test),\n              epochs=EPOCHS,\n              verbose=2)\n    ```", "```\n    predictions = model.predict(X_test,\n                                batch_size=BATCH_SIZE)\n    accuracy = accuracy_score(y_test.argmax(axis=1), \n                              predictions.argmax(axis=1))\n    print(f'Accuracy, without TTA: {accuracy}')\n    ```", "```\n    predictions = []\n    NUM_TEST = 10\n    ```", "```\n    for index in range(len(X_test)):\n        batch = flip_augment(X_test[index], NUM_TEST)\n        sample_predictions = model.predict(batch)\n    ```", "```\n        sample_predictions = np.argmax(\n            np.sum(sample_predictions, axis=0))\n        predictions.append(sample_predictions)\n    ```", "```\n    accuracy = accuracy_score(y_test.argmax(axis=1), \n                              predictions)\n    print(f'Accuracy with TTA: {accuracy}')\n    ```", "```\n    Accuracy, without TTA: 0.6440092165898618\n    Accuracy with TTA: 0.6532258064516129\n    ```", "```\n$> pip install Pillow\n```", "```\n    import os\n    import pathlib\n    from glob import glob\n    import numpy as np\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Model\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    def load_images_and_labels(image_paths, \n                               target_size=(64, 64)):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, \n                             target_size=target_size)\n            image = img_to_array(image)\n            label = image_path.split(os.path.sep)[-2]\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    def build_network(width, height, depth, classes):\n        input_layer = Input(shape=(width, height, depth))\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(input_layer)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=512)(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.25)(x)\n        x = Dense(units=classes)(x)\n        output = Softmax()(x)\n        return Model(input_layer, output)\n    ```", "```\n    def rank_n(predictions, labels, n):\n        score = 0.0\n        for prediction, actual in zip(predictions, labels):\n            prediction = np.argsort(prediction)[::-1]\n            if actual in prediction[:n]:\n                score += 1\n        return score / float(len(predictions))\n    ```", "```\n    SEED = 42\n    np.random.seed(SEED)\n    ```", "```\n    base_path = (pathlib.Path.home() / '.keras' / 'datasets' /\n                 '101_ObjectCategories')\n    images_pattern = str(base_path / '*' / '*.jpg')\n    image_paths = [*glob(images_pattern)]\n    image_paths = [p for p in image_paths if\n            p.split(os.path.sep)[-2] !='BACKGROUND_Google']\n    CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}\n    ```", "```\n    X, y = load_images_and_labels(image_paths)\n    X = X.astype('float') / 255.0\n    y = LabelBinarizer().fit_transform(y)\n    ```", "```\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y,\n                                         test_size=0.2,\n                                      random_state=SEED)\n    ```", "```\n    BATCH_SIZE = 64\n    EPOCHS = 40\n    ```", "```\n    augmenter = ImageDataGenerator(horizontal_flip=True,\n                                   rotation_range=30,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   fill_mode='nearest')\n    ```", "```\n    model = build_network(64, 64, 3, len(CLASSES))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    ```", "```\n    train_generator = augmenter.flow(X_train, y_train,\n                                     BATCH_SIZE)\n    model.fit(train_generator,\n              steps_per_epoch=len(X_train) // BATCH_SIZE,\n              validation_data=(X_test, y_test),\n              epochs=EPOCHS,\n              verbose=2)\n    ```", "```\n    predictions = model.predict(X_test, \n                                batch_size=BATCH_SIZE)\n    ```", "```\n    y_test = y_test.argmax(axis=1)\n    for n in [1, 3, 5, 10]:\n        rank_n_accuracy = rank_n(predictions, y_test, n=n) * 100\n        print(f'Rank-{n}: {rank_n_accuracy:.2f}%')\n    ```", "```\n    Rank-1: 64.29%\n    Rank-3: 78.05%\n    Rank-5: 83.01%\n    Rank-10: 89.69%\n    ```", "```\n$> pip install Pillow\n```", "```\n    import os\n    import pathlib\n    from glob import glob\n    import numpy as np\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Model\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.losses import CategoricalCrossentropy\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    def load_images_and_labels(image_paths, \n                               target_size=(64, 64)):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, \n                             target_size=target_size)\n            image = img_to_array(image)\n            label = image_path.split(os.path.sep)[-2]\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    def build_network(width, height, depth, classes):\n        input_layer = Input(shape=(width, height, depth))\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(input_layer)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=512)(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.25)(x)\n        x = Dense(units=classes)(x)\n        output = Softmax()(x)\n        return Model(input_layer, output)\n    ```", "```\n    SEED = 9\n    np.random.seed(SEED)\n    ```", "```\n    base_path = (pathlib.Path.home() / '.keras' / 'datasets'         \n                  /'101_ObjectCategories')\n    images_pattern = str(base_path / '*' / '*.jpg')\n    image_paths = [*glob(images_pattern)]\n    image_paths = [p for p in image_paths if\n            p.split(os.path.sep)[-2] !='BACKGROUND_Google']\n    CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}\n    ```", "```\n    X, y = load_images_and_labels(image_paths)\n    X = X.astype('float') / 255.0\n    y = LabelBinarizer().fit_transform(y)\n    ```", "```\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y,\n                                         test_size=0.2,\n                                       random_state=SEED)\n    ```", "```\n    BATCH_SIZE = 128\n    EPOCHS = 40\n    ```", "```\n    augmenter = ImageDataGenerator(horizontal_flip=True,\n                                   rotation_range=30,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   fill_mode='nearest')\n    ```", "```\n    for with_label_smoothing in [False, True]:\n        model = build_network(64, 64, 3, len(CLASSES))\n    ```", "```\n        if with_label_smoothing:\n            factor = 0.1\n        else:\n            factor = 0\n    ```", "```\n        loss = CategoricalCrossentropy(label_smoothing=factor)\n    ```", "```\n        model.compile(loss=loss,\n                      optimizer='rmsprop',\n                      metrics=['accuracy'])\n        train_generator = augmenter.flow(X_train, y_train,\n                                         BATCH_SIZE)\n        model.fit(train_generator,\n                  steps_per_epoch=len(X_train) // \n                  BATCH_SIZE,\n                  validation_data=(X_test, y_test),\n                  epochs=EPOCHS,\n                  verbose=2)\n    ```", "```\n        predictions = model.predict(X_test, \n                                   batch_size=BATCH_SIZE)\n        accuracy = accuracy_score(y_test.argmax(axis=1),\n                                  predictions.argmax(axis=1))\n        print(f'Test accuracy '\n              f'{\"with\" if with_label_smoothing else \n               \"without\"} '\n              f'label smoothing: {accuracy * 100:.2f}%')\n    ```", "```\n    Test accuracy without label smoothing: 65.09%\n    Test accuracy with label smoothing: 65.78%\n    ```", "```\n    import numpy as np\n    import tensorflow as tf\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras.callbacks import ModelCheckpoint\n    from tensorflow.keras.datasets import fashion_mnist as fm\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.models import *\n    ```", "```\n    def load_dataset():\n        (X_train, y_train), (X_test, y_test) = fm.load_data()\n        X_train = X_train.astype('float32') / 255.0\n        X_test = X_test.astype('float32') / 255.0\n        X_train = np.expand_dims(X_train, axis=3)\n        X_test = np.expand_dims(X_test, axis=3)\n        label_binarizer = LabelBinarizer()\n        y_train = label_binarizer.fit_transform(y_train)\n        y_test = label_binarizer.fit_transform(y_test)\n    ```", "```\n        (X_train, X_val,\n         y_train, y_val) = train_test_split(X_train, y_train,\n                                            train_size=0.8)\n    ```", "```\n        train_ds = (tf.data.Dataset\n                    .from_tensor_slices((X_train, \n                                         y_train)))\n        val_ds = (tf.data.Dataset\n                  .from_tensor_slices((X_val, y_val)))\n        test_ds = (tf.data.Dataset\n                   .from_tensor_slices((X_test, y_test)))\n        train_ds = (train_ds.shuffle(buffer_size=BUFFER_SIZE)\n                    .batch(BATCH_SIZE)\n                    .prefetch(buffer_size=BUFFER_SIZE))\n        val_ds = (val_ds\n                  .batch(BATCH_SIZE)\n                  .prefetch(buffer_size=BUFFER_SIZE))\n        test_ds = test_ds.batch(BATCH_SIZE)\n        return train_ds, val_ds, test_ds\n    ```", "```\n    def build_network():\n        input_layer = Input(shape=(28, 28, 1))\n        x = Conv2D(filters=20,\n                   kernel_size=(5, 5),\n                   padding='same',\n                   strides=(1, 1))(input_layer)\n        x = ELU()(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=(2, 2),\n                         strides=(2, 2))(x)\n        x = Dropout(0.5)(x)\n        x = Conv2D(filters=50,\n                   kernel_size=(5, 5),\n                   padding='same',\n                   strides=(1, 1))(x)\n        x = ELU()(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=(2, 2),\n                         strides=(2, 2))(x)\n        x = Dropout(0.5)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=500)(x)\n        x = ELU()(x)\n        x = Dropout(0.5)(x)\n        x = Dense(10)(x)\n        output = Softmax()(x)\n        return Model(inputs=input_layer, outputs=output)\n    ```", "```\n    def train_and_checkpoint(checkpointer):\n        train_dataset, val_dataset, test_dataset = load_dataset()\n\n        model = build_network()\n        model.compile(loss='categorical_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy'])\n        model.fit(train_dataset,\n                  validation_data=val_dataset,\n                  epochs=EPOCHS,\n                  callbacks=[checkpointer])\n    ```", "```\n    BATCH_SIZE = 256\n    BUFFER_SIZE = 1024\n    EPOCHS = 100\n    ```", "```\n    checkpoint_pattern = (\n        'save_all/model-ep{epoch:03d}-loss{loss:.3f}'\n        '-val_loss{val_loss:.3f}.h5')\n    checkpoint = ModelCheckpoint(checkpoint_pattern,\n                                 monitor='val_loss',\n                                 verbose=1,\n                                 save_best_only=False,\n                                 mode='min')\n    train_and_checkpoint(checkpoint)\n    ```", "```\n    checkpoint_pattern = (\n        'best_only/model-ep{epoch:03d}-loss{loss:.3f}'\n        '-val_loss{val_loss:.3f}.h5')\n    checkpoint = ModelCheckpoint(checkpoint_pattern,\n                                 monitor='val_loss',\n                                 verbose=1,\n                                 save_best_only=True,\n                                 mode='min')\n    train_and_checkpoint(checkpoint)\n    ```", "```\n    checkpoint_pattern = 'overwrite/model.h5'\n    checkpoint = ModelCheckpoint(checkpoint_pattern,\n                                 monitor='val_loss',\n                                 verbose=1,\n                                 save_best_only=True,\n                                 mode='min')\n    train_and_checkpoint(checkpoint)\n    ```", "```\n    import time\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras.datasets import fashion_mnist as fm\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.losses import categorical_crossentropy\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.optimizers import RMSprop\n    from tensorflow.keras.utils import to_categorical\n    ```", "```\n    def load_dataset():\n        (X_train, y_train), (X_test, y_test) = fm.load_data()\n        X_train = X_train.astype('float32') / 255.0\n        X_test = X_test.astype('float32') / 255.0\n        # Reshape grayscale to include channel dimension.\n        X_train = np.expand_dims(X_train, axis=-1)\n        X_test = np.expand_dims(X_test, axis=-1)\n        y_train = to_categorical(y_train)\n        y_test = to_categorical(y_test)\n        return (X_train, y_train), (X_test, y_test)\n    ```", "```\n    def build_network():\n        input_layer = Input(shape=(28, 28, 1))\n        x = Conv2D(filters=20,\n                   kernel_size=(5, 5),\n                   padding='same',\n                   strides=(1, 1))(input_layer)\n        x = ELU()(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=(2, 2),\n                         strides=(2, 2))(x)\n        x = Dropout(0.5)(x)\n        x = Conv2D(filters=50,\n                   kernel_size=(5, 5),\n                   padding='same',\n                   strides=(1, 1))(x)\n        x = ELU()(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=(2, 2),\n                         strides=(2, 2))(x)\n        x = Dropout(0.5)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=500)(x)\n        x = ELU()(x)\n        x = Dropout(0.5)(x)\n        x = Dense(10)(x)\n        output = Softmax()(x)\n        return Model(inputs=input_layer, outputs=output)\n    ```", "```\n    def training_step(X, y, model, optimizer):\n        with tf.GradientTape() as tape:\n            predictions = model(X)\n            loss = categorical_crossentropy(y, predictions)\n        gradients = tape.gradient(loss, \n                              model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients,\n                              model.trainable_variables))\n    ```", "```\n    BATCH_SIZE = 256\n    EPOCHS = 100\n    ```", "```\n    (X_train, y_train), (X_test, y_test) = load_dataset()\n    ```", "```\n    optimizer = RMSprop()\n    model = build_network()\n    ```", "```\n    for epoch in range(EPOCHS):\n        print(f'Epoch {epoch + 1}/{EPOCHS}')\n        start = time.time()\n    ```", "```\n        for i in range(int(len(X_train) / BATCH_SIZE)):\n            X_batch = X_train[i * BATCH_SIZE:\n                              i * BATCH_SIZE + BATCH_SIZE]\n            y_batch = y_train[i * BATCH_SIZE:\n                              i * BATCH_SIZE + BATCH_SIZE]\n            training_step(X_batch, y_batch, model, \n                          optimizer)\n    ```", "```\n        elapsed = time.time() - start\n        print(f'\\tElapsed time: {elapsed:.2f} seconds.')\n    ```", "```\n    model.compile(loss=categorical_crossentropy,\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    results = model.evaluate(X_test, y_test)\n    print(f'Loss: {results[0]}, Accuracy: {results[1]}')\n    ```", "```\n    Loss: 1.7750033140182495, Accuracy: 0.9083999991416931\n    ```", "```\n$> pip install Pillow opencv-python imutils\n```", "```\n    import cv2\n    import imutils \n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras.applications import *\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    class GradGAM(object):\n        def __init__(self, model, class_index, \n                     layer_name=None):\n            self.class_index = class_index\n            if layer_name is None:\n                for layer in reversed(model.layers):\n                    if len(layer.output_shape) == 4:\n                        layer_name = layer.name\n                        break\n            self.grad_model = \n                      self._create_grad_model(model,\n\n                                           layer_name)\n    ```", "```\n        def _create_grad_model(self, model, layer_name):\n            return Model(inputs=[model.inputs],\n                         outputs=[\n                           model.get_layer(layer_name).\n                              output,model.output])\n    ```", "```\n        def compute_heatmap(self, image, epsilon=1e-8):\n            with tf.GradientTape() as tape:\n                inputs = tf.cast(image, tf.float32)\n                conv_outputs, preds = self.grad_model(inputs)\n                loss = preds[:, self.class_index]\n    ```", "```\n    grads = tape.gradient(loss, conv_outputs)\n    ```", "```\n            guided_grads = (tf.cast(conv_outputs > 0, \n                          'float32') *\n                            tf.cast(grads > 0, 'float32') *\n                            grads)\n    ```", "```\n            conv_outputs = conv_outputs[0]\n            guided_grads = guided_grads[0]\n            weights = tf.reduce_mean(guided_grads, \n                                     axis=(0, 1))\n            cam = tf.reduce_sum(\n                tf.multiply(weights, conv_outputs),\n                axis=-1)\n    ```", "```\n            height, width = image.shape[1:3]\n            heatmap = cv2.resize(cam.numpy(), (width, \n                                  height))\n            min = heatmap.min()\n            max = heatmap.max()\n            heatmap = (heatmap - min) / ((max - min) + \n                                                   epsilon)\n            heatmap = (heatmap * 255.0).astype('uint8')\n            return heatmap\n    ```", "```\n        def overlay_heatmap(self,\n                            heatmap,\n                            image, alpha=0.5,\n                            colormap=cv2.COLORMAP_VIRIDIS):\n            heatmap = cv2.applyColorMap(heatmap, colormap)\n            output = cv2.addWeighted(image,\n                                     alpha,\n                                     heatmap,\n                                     1 - alpha,\n                                     0)\n            return heatmap, output\n    ```", "```\n    model = ResNet50(weights='imagenet')\n    ```", "```\n    image = load_img('dog.jpg', target_size=(224, 224))\n    image = img_to_array(image)\n    image = np.expand_dims(image, axis=0)\n    image = imagenet_utils.preprocess_input(image)\n    ```", "```\n    predictions = model.predict(image)\n    i = np.argmax(predictions[0])\n    ```", "```\n    cam = GradGAM(model, i)\n    heatmap = cam.compute_heatmap(image)\n    ```", "```\n    original_image = cv2.imread('dog.jpg')\n    heatmap = cv2.resize(heatmap, (original_image.shape[1],\n                                   original_image.shape[0]))\n    heatmap, output = cam.overlay_heatmap(heatmap, \n                                          original_image,\n                                          alpha=0.5)\n    ```", "```\n    decoded = imagenet_utils.decode_predictions(predictions)\n    _, label, probability = decoded[0][0]\n    ```", "```\n    cv2.rectangle(output, (0, 0), (340, 40), (0, 0, 0), -1)\n    cv2.putText(output, f'{label}: {probability * 100:.2f}%',\n                (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n                (255, 255, 255), 2)\n    ```", "```\n    output = np.hstack([original_image, heatmap, output])\n    output = imutils.resize(output, height=700)\n    cv2.imwrite('output.jpg', output)\n    ```"]