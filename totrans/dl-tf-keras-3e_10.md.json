["```\n[MASK] in the sentence \"The capital of France is [MASK].\":\n```", "```\nfrom transformers import BertTokenizer, TFBertForMaskedLM\nimport tensorflow as tf\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\nmodel = TFBertForMaskedLM.from_pretrained(\"bert-base-cased\")\ninputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"tf\")\nlogits = model(**inputs).logits\nmask_token_index = tf.where(inputs.input_ids == tokenizer.mask_token_id)[0][1]\npredicted_token_id = tf.math.argmax(logits[:, mask_token_index], axis=-1)\nprint(tokenizer.convert_ids_to_tokens(predicted_token_id)[0]) \n```", "```\n the CLIP modelâ€™s ability to compare images and text. Here, we take an image of two cats side by side and compare it to two text strings: \"a photo of a cat\" and \"a photo of a dog\". CLIP can compare the image with the two text strings and correctly determine that the probability that the image is similar to the string \"a photo of a cat\" is 0.995 as opposed to a probability of 0.005 for the image being similar to the string \"a photo of a dog\":\n```", "```\nimport tensorflow as tf\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, TFCLIPModel\nmodel = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [\"a photo of a cat\", \"a photo of a dog\"]\ninputs = processor(text=texts, images=image, return_tensors=\"tf\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image\nprobs = tf.nn.softmax(logits_per_image, axis=1)\nprint(probs.numpy()) \n```", "```\n and query encodings for positive pairs and minimizes it for negative pairs.\n```"]