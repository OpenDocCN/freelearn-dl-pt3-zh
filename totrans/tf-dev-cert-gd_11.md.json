["```\n    import pandas as pd\n    ```", "```\n    import tensorflow_datasets as tfds\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    ```", "```\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    ```", "```\n    from tensorflow.keras.utils import to_categorical\n    ```", "```\n    import tensorflow_hub as hub\n    ```", "```\n    from sklearn.model_selection import train_test_split\n    ```", "```\n    from tensorflow.keras.models import Sequential\n    ```", "```\n    from tensorflow.keras.layers import Embedding, SimpleRNN, Conv1D, GlobalMaxPooling1D, LSTM, GRU, Bidirectional, Dense, Flatten\n    ```", "```\n    # Load the dataset\n    ```", "```\n    dataset, info = tfds.load('ag_news_subset',\n    ```", "```\n        with_info=True, as_supervised=True)\n    ```", "```\n    train_dataset, test_dataset = dataset['train'],\n    ```", "```\n        dataset['test']\n    ```", "```\n    # Tokenize and pad the sequences\n    ```", "```\n    tokenizer = Tokenizer(num_words=20000,\n    ```", "```\n        oov_token=\"<OOV>\")\n    ```", "```\n    train_texts = [x[0].numpy().decode(\n    ```", "```\n        'utf-8') for x in train_dataset]\n    ```", "```\n    tokenizer.fit_on_texts(train_texts)\n    ```", "```\n    sequences = tokenizer.texts_to_sequences(train_texts)\n    ```", "```\n    sequences = pad_sequences(sequences, padding='post')\n    ```", "```\n    # Convert labels to one-hot encoding\n    ```", "```\n    train_labels = [label.numpy() for _, label in train_dataset]\n    ```", "```\n    train_labels = to_categorical(train_labels,\n    ```", "```\n        num_classes=4)4  # assuming 4 classes\n    ```", "```\n    # Split the training set into training and validation sets\n    ```", "```\n    train_sequences, val_sequences, train_labels,\n    ```", "```\n        val_labels = train_test_split(sequences,\n    ```", "```\n            train_labels, test_size=0.2)\n    ```", "```\n    vocab_size=20000\n    ```", "```\n    embedding_dim =64\n    ```", "```\n    max_length=sequences.shape[1]\n    ```", "```\n    # Define the DNN model\n    ```", "```\n    model_dnn = Sequential([\n    ```", "```\n        Embedding(vocab_size, embedding_dim,\n    ```", "```\n            input_length=max_length),\n    ```", "```\n        Flatten(),\n    ```", "```\n        tf.keras.layers.Dense(64, activation='relu'),\n    ```", "```\n        Dense(16, activation='relu'),\n    ```", "```\n        Dense(4, activation='softmax')\n    ```", "```\n    ])\n    ```", "```\n    # Define the CNN model\n    ```", "```\n    model_cnn = Sequential([\n    ```", "```\n        Embedding(vocab_size, embedding_dim,\n    ```", "```\n            input_length=max_length),\n    ```", "```\n        Conv1D(128, 5, activation='relu'),\n    ```", "```\n        GlobalMaxPooling1D(),\n    ```", "```\n        tf.keras.layers.Dense(64, activation='relu'),\n    ```", "```\n        Dense(4, activation='softmax')\n    ```", "```\n    ])\n    ```", "```\n    # Define the LSTM model\n    ```", "```\n    model_lstm = Sequential([\n    ```", "```\n        Embedding(vocab_size, embedding_dim,\n    ```", "```\n            input_length=max_length),\n    ```", "```\n        LSTM(32, return_sequences=True),\n    ```", "```\n        LSTM(32),\n    ```", "```\n        tf.keras.layers.Dense(64, activation='relu'),\n    ```", "```\n        Dense(4, activation='softmax')\n    ```", "```\n    ])\n    ```", "```\n    # Define the BiLSTM model\n    ```", "```\n    model_BiLSTM = Sequential([\n    ```", "```\n        Embedding(vocab_size, embedding_dim,\n    ```", "```\n            input_length=max_length),\n    ```", "```\n        Bidirectional(LSTM(32, return_sequences=True)),\n    ```", "```\n        Bidirectional(LSTM(16)),\n    ```", "```\n        tf.keras.layers.Dense(64, activation='relu'),\n    ```", "```\n        Dense(4, activation='softmax')])\n    ```", "```\n    models = [model_cnn, model_dnn, model_lstm,\n    ```", "```\n        model_BiLSTM]\n    ```", "```\n    for model in models:\n    ```", "```\n        model.compile(loss='categorical_crossentropy',\n    ```", "```\n            optimizer='adam', metrics=['accuracy'])\n    ```", "```\n        model.fit(train_sequences, train_labels,epochs=10,\n    ```", "```\n            validation_data=(val_sequences, val_labels),\n    ```", "```\n            verbose=False\n    ```", "```\n    # Evaluate the model\n    ```", "```\n    test_texts = [x[0].numpy().decode(\n    ```", "```\n        'utf-8') for x in test_dataset]\n    ```", "```\n    test_sequences = tokenizer.texts_to_sequences(\n    ```", "```\n        test_texts)\n    ```", "```\n    test_sequences = pad_sequences(test_sequences,\n    ```", "```\n        padding='post', maxlen=sequences.shape[1])\n    ```", "```\n    test_labels = [label.numpy() for _, label in test_dataset]\n    ```", "```\n    test_labels = to_categorical(test_labels,\n    ```", "```\n        num_classes=4)\n    ```", "```\n    model_names = [\"Model_CNN\", \"Model_DNN\", \"Model_LSTM\",\n    ```", "```\n        \"Model_BiLSTM\"]\n    ```", "```\n    for i, model in enumerate(models):\n    ```", "```\n        loss, accuracy = model.evaluate(test_sequences,\n    ```", "```\n            test_labels)\n    ```", "```\n        print(\"Model Evaluation -\", model_names[i])\n    ```", "```\n        print(\"Loss:\", loss)\n    ```", "```\n        print(\"Accuracy:\", accuracy)\n    ```", "```\n        print()\n    ```", "```\n238/238 [==============================] - 1s 4ms/step - loss: 0.7756 - accuracy: 0.8989\nModel Evaluation - Model_CNN\nLoss: 0.7755934000015259\nAccuracy: 0.8989473581314087\n238/238 [==============================] - 1s 2ms/step - loss: 0.7091 - accuracy: 0.8896\nModel Evaluation - Model_DNN\nLoss: 0.7091193199157715\nAccuracy: 0.8896052837371826\n238/238 [==============================] - 2s 7ms/step - loss: 0.3211 - accuracy: 0.9008\nModel Evaluation - Model_LSTM\nLoss: 0.32113003730773926\nAccuracy: 0.9007894992828369\n238/238 [==============================] - 4s 10ms/step - loss: 0.5618 - accuracy: 0.8916\nModel Evaluation - Model_BiLSTM\nLoss: 0.5618014335632324\nAccuracy: 0.8915789723396301\n```", "```\n    import numpy as np\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    from tensorflow.keras.models import Sequential\n    ```", "```\n    from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n    ```", "```\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    ```", "```\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    ```", "```\n    import tensorflow_datasets as tfds\n    ```", "```\n    !wget http://nlp.stanford.edu/data/glove.6B.zip\n    ```", "```\n    !unzip glove.6B.zip -d glove.6B\n    ```", "```\n    dataset, info = tfds.load('ag_news_subset',\n    ```", "```\n        with_info=True, as_supervised=True)\n    ```", "```\n    train_dataset, test_dataset = dataset['train'],\n    ```", "```\n        dataset['test']\n    ```", "```\n    tokenizer = Tokenizer(num_words=20000,\n    ```", "```\n        oov_token=\"<OOV>\")\n    ```", "```\n    train_texts = [x[0].numpy().decode(\n    ```", "```\n        'utf-8') for x in train_dataset]\n    ```", "```\n    tokenizer.fit_on_texts(train_texts)\n    ```", "```\n    train_sequences = tokenizer.texts_to_sequences(\n    ```", "```\n        train_texts)\n    ```", "```\n    train_sequences = pad_sequences(train_sequences,\n    ```", "```\n        padding='post')\n    ```", "```\n    max_length = train_sequences.shape[1]\n    ```", "```\n    test_texts = [x[0].numpy().decode(\n    ```", "```\n        'utf-8') for x in test_dataset]\n    ```", "```\n    test_sequences = tokenizer.texts_to_sequences(\n    ```", "```\n        test_texts)\n    ```", "```\n    test_sequences = pad_sequences(test_sequences,\n    ```", "```\n        padding='post', maxlen=max_length)\n    ```", "```\n    vocab_size = len(tokenizer.word_index) + 1\n    ```", "```\n    embedding_dim = 50\n    ```", "```\n    # Download GloVe embeddings and prepare embedding matrix\n    ```", "```\n    with open('/content/glove.6B/glove.6B.50d.txt', 'r', encoding='utf-8') as f:\n    ```", "```\n        for line in f:\n    ```", "```\n            values = line.split()\n    ```", "```\n            word = values[0]\n    ```", "```\n            if word in tokenizer.word_index:\n    ```", "```\n                idx = tokenizer.word_index[word]\n    ```", "```\n                embedding_matrix[idx] = np.array(\n    ```", "```\n                    values[1:], dtype=np.float32)\n    ```", "```\n    model_lstm = Sequential([\n    ```", "```\n        Embedding(vocab_size, embedding_dim,\n    ```", "```\n            input_length=max_length,\n    ```", "```\n            weights=[embedding_matrix], trainable=False),\n    ```", "```\n        LSTM(32, return_sequences=True),\n    ```", "```\n        LSTM(32),\n    ```", "```\n        Dense(64, activation='relu'),\n    ```", "```\n        Dense(4, activation='softmax')\n    ```", "```\n    ])\n    ```", "```\n    model_lstm.compile(optimizer='adam',\n    ```", "```\n        loss='categorical_crossentropy',\n    ```", "```\n        metrics=['accuracy'])\n    ```", "```\n    # Convert labels to one-hot encoding\n    ```", "```\n    train_labels = tf.keras.utils.to_categorical(\n    ```", "```\n        [label.numpy() for _, label in train_dataset])\n    ```", "```\n    test_labels = tf.keras.utils.to_categorical(\n    ```", "```\n        [label.numpy() for _, label in test_dataset])\n    ```", "```\n    model_lstm.fit(train_sequences, train_labels,\n    ```", "```\n        epochs=10, validation_split=0.2)\n    ```", "```\n    loss, accuracy = model_lstm.evaluate(test_sequences,\n    ```", "```\n        test_labels)\n    ```", "```\n    print(\"Loss:\", loss)\n    ```", "```\n    print(\"Accuracy:\", accuracy)\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    ```", "```\n    from tensorflow.keras.preprocessing.sequence import pad_sequences\n    ```", "```\n    from tensorflow.keras.models import Sequential\n    ```", "```\n    from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n    ```", "```\n    import numpy as np\n    ```", "```\n    text = open('stories.txt').read().lower()\n    ```", "```\n    tokenizer = Tokenizer()\n    ```", "```\n    tokenizer.fit_on_texts([text])\n    ```", "```\n    total_words = len(tokenizer.word_index) + 1\n    ```", "```\n    input_sequences = []\n    ```", "```\n    for line in text.split('\\n'):\n    ```", "```\n        token_list = tokenizer.texts_to_sequences(\n    ```", "```\n            [line])[0]\n    ```", "```\n        for i in range(1, len(token_list)):\n    ```", "```\n            n_gram_sequence = token_list[:i+1]\n    ```", "```\n            input_sequences.append(n_gram_sequence)\n    ```", "```\n    max_sequence_len = max([len(x) for x in input_sequences])\n    ```", "```\n    input_sequences = np.array(pad_sequences(\n    ```", "```\n        input_sequences, maxlen=max_sequence_len,\n    ```", "```\n        padding='pre'))\n    ```", "```\n    predictors, label = input_sequences[:,:-1],\n    ```", "```\n        input_sequences[:,-1]\n    ```", "```\n    label = tf.keras.utils.to_categorical(label,\n    ```", "```\n        num_classes=total_words)\n    ```", "```\n    model = Sequential([\n    ```", "```\n        Embedding(total_words, 200,\n    ```", "```\n            input_length=max_sequence_len-1),\n    ```", "```\n        Bidirectional(LSTM(200)),\n    ```", "```\n        Dense(total_words, activation='softmax')\n    ```", "```\n    ])\n    ```", "```\n    model.compile(loss='categorical_crossentropy',\n    ```", "```\n        optimizer='adam', metrics=['accuracy'])\n    ```", "```\n    history = model.fit(predictors, label, epochs=300,\n    ```", "```\n        verbose=0)\n    ```", "```\n    def generate_text(seed_text, next_words, model, max_sequence_len):\n    ```", "```\n        for _ in range(next_words):\n    ```", "```\n            token_list = tokenizer.texts_to_sequences(\n    ```", "```\n                [seed_text])[0]\n    ```", "```\n            token_list = pad_sequences([token_list],\n    ```", "```\n                maxlen=max_sequence_len-1, padding='pre')\n    ```", "```\n            # Get the predictions\n    ```", "```\n            predictions = model.predict(token_list)\n    ```", "```\n            # Get the index with the maximum prediction value\n    ```", "```\n            predicted = np.argmax(predictions)\n    ```", "```\n            output_word = \"\"\n    ```", "```\n            for word,index in tokenizer.word_index.items():\n    ```", "```\n                if index == predicted:\n    ```", "```\n                    output_word = word\n    ```", "```\n                    break\n    ```", "```\n            seed_text += \" \" + output_word\n    ```", "```\n        return seed_text\n    ```", "```\n    input_text= \"In the hustle and bustle of ipoti\"\n    ```", "```\n    print(generate_text(input_text, 50, model,\n    ```", "```\n        max_sequence_len))\n    ```", "```\nIn the hustle and bustle of ipoti the city the friends also learned about the wider context of the ancient world including the people who had lived and worshipped in the area they explored nearby archaeological sites and museums uncovering artifacts and stories that shed light on the lives and beliefs of those who had come before\n```"]