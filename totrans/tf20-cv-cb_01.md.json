["```\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Input\n    from tensorflow.keras.datasets import mnist\n    from tensorflow.keras.layers import Dense\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.models import Sequential\n    ```", "```\n    layers = [Dense(256, input_shape=(28 * 28 * 1,), \n                    activation='sigmoid'),\n              Dense(128, activation='sigmoid'),\n              Dense(10, activation='softmax')]\n    sequential_model_list = Sequential(layers)\n    ```", "```\n    sequential_model = Sequential()\n    sequential_model.add(Dense(256, \n                         input_shape=(28 * 28 * 1,), \n                         activation='sigmoid'))\n    sequential_model.add(Dense(128, activation='sigmoid'))\n    sequential_model.add(Dense(10, activation='softmax'))\n    ```", "```\n    input_layer = Input(shape=(28 * 28 * 1,))\n    dense_1 = Dense(256, activation='sigmoid')(input_layer)\n    dense_2 = Dense(128, activation='sigmoid')(dense_1)\n    predictions = Dense(10, activation='softmax')(dense_2)\n    functional_model = Model(inputs=input_layer, \n                             outputs=predictions)\n    ```", "```\n    class ClassModel(Model):\n        def __init__(self):\n            super(ClassModel, self).__init__()\n            self.dense_1 = Dense(256, activation='sigmoid')\n            self.dense_2 = Dense(256, activation='sigmoid')\n            self.predictions = Dense(10,activation='softmax')\n        def call(self, inputs, **kwargs):\n            x = self.dense_1(inputs)\n            x = self.dense_2(x)\n      return self.predictions(x)\n    class_model = ClassModel()\n    ```", "```\n    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n    X_train = X_train.reshape((X_train.shape[0], 28 * 28 * \n                               1))\n    X_test = X_test.reshape((X_test.shape[0], 28 * 28 * \n                              1))\n    X_train = X_train.astype('float32') / 255.0\n    X_test = X_test.astype('float32') / 255.0\n    ```", "```\n    label_binarizer = LabelBinarizer()\n    y_train = label_binarizer.fit_transform(y_train)\n    y_test = label_binarizer.fit_transform(y_test)\n    ```", "```\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8)\n    ```", "```\n    models = {\n        'sequential_model': sequential_model,\n        'sequential_model_list': sequential_model_list,\n        'functional_model': functional_model,\n        'class_model': class_model\n    }\n    for name, model in models.items():\n        print(f'Compiling model: {name}')\n        model.compile(loss='categorical_crossentropy', \n                      optimizer='adam', \n                      metrics=['accuracy'])\n        print(f'Training model: {name}')\n        model.fit(X_train, y_train,\n                  validation_data=(X_valid, y_valid),\n                  epochs=50,\n                  batch_size=256,\n                  verbose=0)\n        _, accuracy = model.evaluate(X_test, y_test, \n                                     verbose=0)\n        print(f'Testing model: {name}. \\nAccuracy: \n              {accuracy}')\n        print('---')\n    ```", "```\n$> pip install Pillow\n```", "```\n    import glob\n    import os\n    import tarfile\n    import matplotlib.pyplot as plt\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    from tensorflow.keras.preprocessing.image \n    import load_img, img_to_array\n    from tensorflow.keras.utils import get_file\n    ```", "```\n    DATASET_URL = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz?sequence=4&isAllowed=y'\n    DATA_NAME = 'cinic10'\n    FILE_EXTENSION = 'tar.gz'\n    FILE_NAME = '.'.join([DATA_NAME, FILE_EXTENSION])\n    ```", "```\n    downloaded_file_location = get_file(origin=DATASET_URL, fname=FILE_NAME, extract=False)\n    # Build the path to the data directory based on the location of the downloaded file.\n    data_directory, _ = downloaded_file_location.rsplit(os.path.sep, maxsplit=1)\n    data_directory = os.path.sep.join([data_directory, \n                                       DATA_NAME])\n    # Only extract the data if it hasn't been extracted already\n    if not os.path.exists(data_directory):\n        tar = tarfile.open(downloaded_file_location)\n        tar.extractall(data_directory)\n    ```", "```\n    data_pattern = os.path.sep.join([data_directory, \n                                     '*/*/*.png'])\n    image_paths = list(glob.glob(data_pattern))\n    print(f'There are {len(image_paths):,} images in the \n          dataset')\n    ```", "```\n    There are 270,000 images in the dataset\n    ```", "```\n    sample_image = load_img(image_paths[0])\n    print(f'Image type: {type(sample_image)}')\n    print(f'Image format: {sample_image.format}')\n    print(f'Image mode: {sample_image.mode}')\n    print(f'Image size: {sample_image.size}')\n    ```", "```\n    Image type: <class 'PIL.PngImagePlugin.PngImageFile'>\n    Image format: PNG\n    Image mode: RGB\n    Image size: (32, 32)\n    ```", "```\n    sample_image_array = img_to_array(sample_image)\n    print(f'Image type: {type(sample_image_array)}')\n    print(f'Image array shape: {sample_image_array.shape}')\n    ```", "```\n    Image type: <class 'numpy.ndarray'>\n    Image array shape: (32, 32, 3)\n    ```", "```\n    plt.imshow(sample_image_array / 255.0)\n    ```", "```\n    image_generator = ImageDataGenerator(horizontal_flip=True, rescale=1.0 / 255.0)\n    ```", "```\n    iterator = (image_generator\n            .flow_from_directory(directory=data_directory, \n                                     batch_size=10))\n    for batch, _ in iterator:\n        plt.figure(figsize=(5, 5))\n        for index, image in enumerate(batch, start=1):\n            ax = plt.subplot(5, 5, index)\n            plt.imshow(image)\n            plt.axis('off')\n        plt.show()\n        break\n    ```", "```\n    import os\n    import tarfile\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras.utils import get_file\n    ```", "```\n    DATASET_URL = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz?sequence=4&isAllowed=y'\n    DATA_NAME = 'cinic10'\n    FILE_EXTENSION = 'tar.gz'\n    FILE_NAME = '.'.join([DATA_NAME, FILE_EXTENSION])\n    ```", "```\n    downloaded_file_location = get_file(origin=DATASET_URL, fname=FILE_NAME, extract=False)\n    # Build the path to the data directory based on the location of the downloaded file.\n    data_directory, _ = downloaded_file_location.rsplit(os.path.sep, maxsplit=1)\n    data_directory = os.path.sep.join([data_directory, \n                                      DATA_NAME])\n    # Only extract the data if it hasn't been extracted already\n    if not os.path.exists(data_directory):\n        tar = tarfile.open(downloaded_file_location)\n        tar.extractall(data_directory)\n    ```", "```\n    data_pattern = os.path.sep.join([data_directory, '*/*/*.png'])\n    image_dataset = tf.data.Dataset.list_files(data_pattern)\n    ```", "```\n    for file_path in image_dataset.take(1):\n        sample_path = file_path.numpy()\n    sample_image = tf.io.read_file(sample_path)\n    ```", "```\n    sample_image = tf.image.decode_png(sample_image, \n                                       channels=3)\n    sample_image = sample_image.numpy()\n    ```", "```\n    plt.imshow(sample_image / 255.0)\n    ```", "```\n    plt.figure(figsize=(5, 5))\n    for index, image_path in enumerate(image_dataset.take(10), start=1):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_png(image, channels=3)\n        image = tf.image.convert_image_dtype(image, \n                                             np.float32)\n        ax = plt.subplot(5, 5, index)\n        plt.imshow(image)\n        plt.axis('off')\n    plt.show()\n    plt.close()\n    ```", "```\n    import json\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Model\n    from tensorflow.keras.datasets import mnist\n    from tensorflow.keras.layers import BatchNormalization\n    from tensorflow.keras.layers import Conv2D\n    from tensorflow.keras.layers import Dense\n    from tensorflow.keras.layers import Dropout\n    from tensorflow.keras.layers import Flatten\n    from tensorflow.keras.layers import Input\n    from tensorflow.keras.layers import MaxPooling2D\n    from tensorflow.keras.layers import ReLU\n    from tensorflow.keras.layers import Softmax\n    from tensorflow.keras.models import load_model\n    from tensorflow.keras.models import model_from_json\n    ```", "```\n    def load_data():\n     (X_train, y_train), (X_test, y_test) = mnist.load_data()\n        # Normalize data.\n        X_train = X_train.astype('float32') / 255.0\n        X_test = X_test.astype('float32') / 255.0\n        # Reshape grayscale to include channel dimension.\n        X_train = np.expand_dims(X_train, axis=3)\n        X_test = np.expand_dims(X_test, axis=3)\n        # Process labels.\n        label_binarizer = LabelBinarizer()\n        y_train = label_binarizer.fit_transform(y_train)\n        y_test = label_binarizer.fit_transform(y_test)\n        return X_train, y_train, X_test, y_test\n    ```", "```\n    def build_network():\n        input_layer = Input(shape=(28, 28, 1))\n        convolution_1 = Conv2D(kernel_size=(2, 2),\n                               padding='same',\n                               strides=(2, 2),\n                               filters=32)(input_layer)\n        activation_1 = ReLU()(convolution_1)\n        batch_normalization_1 = BatchNormalization()    \n                                (activation_1)\n        pooling_1 = MaxPooling2D(pool_size=(2, 2),\n                                  strides=(1, 1),\n           padding='same')(batch_normalization_1)\n        dropout = Dropout(rate=0.5)(pooling_1)\n        flatten = Flatten()(dropout)\n        dense_1 = Dense(units=128)(flatten)\n        activation_2 = ReLU()(dense_1)\n        dense_2 = Dense(units=10)(activation_2)\n        output = Softmax()(dense_2)\n        network = Model(inputs=input_layer, outputs=output)\n        return network\n    ```", "```\n    def evaluate(model, X_test, y_test):\n        _, accuracy = model.evaluate(X_test, y_test, \n                                     verbose=0)\n        print(f'Accuracy: {accuracy}')\n    ```", "```\n    X_train, y_train, X_test, y_test = load_data()\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8)\n    model = build_network()\n    ```", "```\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam', \n                  metrics=['accuracy'])\n    model.fit(X_train, y_train, \n              validation_data=(X_valid, y_valid), \n              epochs=50, \n              batch_size=1024, \n              verbose=0)\n    ```", "```\n    # Saving model and weights as HDF5.\n    model.save('model_and_weights.hdf5')\n    # Loading model and weights as HDF5.\n    loaded_model = load_model('model_and_weights.hdf5')\n    # Predicting using loaded model.\n    evaluate(loaded_model, X_test, y_test)\n    ```", "```\n    Accuracy: 0.9836000204086304\n    ```", "```\n$> pip install Pillow pydot\n```", "```\n    from PIL import Image\n    from tensorflow.keras import Model\n    from tensorflow.keras.layers import BatchNormalization\n    from tensorflow.keras.layers import Conv2D\n    from tensorflow.keras.layers import Dense\n    from tensorflow.keras.layers import Dropout\n    from tensorflow.keras.layers import Flatten\n    from tensorflow.keras.layers import Input\n    from tensorflow.keras.layers import LeakyReLU\n    from tensorflow.keras.layers import MaxPooling2D\n    from tensorflow.keras.layers import Softmax\n    from tensorflow.keras.utils import plot_model\n    ```", "```\n    input_layer = Input(shape=(64, 64, 3), \n                        name='input_layer')\n    ```", "```\n    convolution_1 = Conv2D(kernel_size=(2, 2),\n                           padding='same',\n                           strides=(2, 2),\n                           filters=32,\n                           name='convolution_1')(input_layer)\n    activation_1 = LeakyReLU(name='activation_1')(convolution_1)\n    batch_normalization_1 = BatchNormalization(name='batch_normalization_1')(activation_1)\n    pooling_1 = MaxPooling2D(pool_size=(2, 2),\n                             strides=(1, 1),\n                             padding='same',\n                             name='pooling_1')(batch_\n                             normalization_1)\n    ```", "```\n    convolution_2 = Conv2D(kernel_size=(2, 2),\n                           padding='same',\n                           strides=(2, 2),\n                           filters=64,\n                           name='convolution_2')(pooling_1)\n    activation_2 = LeakyReLU(name='activation_2')(convolution_2)\n    batch_normalization_2 = BatchNormalization(name='batch_normalization_2')(activation_2)\n    pooling_2 = MaxPooling2D(pool_size=(2, 2),\n                             strides=(1, 1),\n                             padding='same',\n                             name='pooling_2')\n                            (batch_normalization_2)\n    dropout = Dropout(rate=0.5, name='dropout')(pooling_2)\n    ```", "```\n    flatten = Flatten(name='flatten')(dropout)\n    dense_1 = Dense(units=256, name='dense_1')(flatten)\n    activation_3 = LeakyReLU(name='activation_3')(dense_1)\n    dense_2 = Dense(units=128, name='dense_2')(activation_3)\n    activation_4 = LeakyReLU(name='activation_4')(dense_2)\n    dense_3 = Dense(units=3, name='dense_3')(activation_4)\n    output = Softmax(name='output')(dense_3)\n    model = Model(inputs=input_layer, outputs=output, \n                  name='my_model')\n    ```", "```\n    print(model.summary())\n    ```", "```\n    plot_model(model, \n               show_shapes=True, \n               show_layer_names=True, \n               to_file='my_model.jpg')\n    model_diagram = Image.open('my_model.jpg')\n    ```", "```\n$> pip install Pillow pydot\n```", "```\n$> pip install git+https://github.com/tensorflow/docs\n```", "```\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import tensorflow as tf\n    import tensorflow_docs as tfdocs\n    import tensorflow_docs.plots\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras import Model\n    from tensorflow.keras.datasets import fashion_mnist as fm\n    from tensorflow.keras.layers import BatchNormalization\n    from tensorflow.keras.layers import Conv2D\n    from tensorflow.keras.layers import Dense\n    from tensorflow.keras.layers import Dropout\n    from tensorflow.keras.layers import ELU\n    from tensorflow.keras.layers import Flatten\n    from tensorflow.keras.layers import Input\n    from tensorflow.keras.layers import MaxPooling2D\n    from tensorflow.keras.layers import Softmax\n    from tensorflow.keras.models import load_model\n    from tensorflow.keras.utils import plot_model\n    ```", "```\n    def load_dataset():\n        (X_train, y_train), (X_test, y_test) = fm.load_data()\n        X_train = X_train.astype('float32') / 255.0\n        X_test = X_test.astype('float32') / 255.0\n        # Reshape grayscale to include channel dimension.\n        X_train = np.expand_dims(X_train, axis=3)\n        X_test = np.expand_dims(X_test, axis=3)\n        label_binarizer = LabelBinarizer()\n        y_train = label_binarizer.fit_transform(y_train)\n        y_test = label_binarizer.fit_transform(y_test)\n        (X_train, X_val,\n         y_train, y_val) = train_test_split(X_train, y_train, \n\n                            train_size=0.8)\n        train_ds = (tf.data.Dataset\n                    .from_tensor_slices((X_train, \n                                         y_train)))\n        val_ds = (tf.data.Dataset\n                  .from_tensor_slices((X_val, y_val)))\n        test_ds = (tf.data.Dataset\n                   .from_tensor_slices((X_test, y_test)))\n    ```", "```\n    def build_network():\n        input_layer = Input(shape=(28, 28, 1))\n        x = Conv2D(filters=20, \n                   kernel_size=(5, 5),\n                   padding='same', \n                   strides=(1, 1))(input_layer)\n        x = ELU()(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=(2, 2), \n                         strides=(2, 2))(x)\n        x = Dropout(0.5)(x)\n        x = Conv2D(filters=50, \n                   kernel_size=(5, 5), \n                   padding='same', \n                   strides=(1, 1))(x)\n        x = ELU()(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=(2, 2), \n                         strides=(2, 2))(x)\n        x = Dropout(0.5)(x)\n        x = Flatten()(x)\n        x = Dense(units=500)(x)\n        x = ELU()(x)\n        x = Dropout(0.5)(x)\n        x = Dense(10)(x)\n        output = Softmax()(x)\n        model = Model(inputs=input_layer, outputs=output)\n        return model\n    ```", "```\n    def plot_model_history(model_history, metric, ylim=None):\n        plt.style.use('seaborn-darkgrid')\n        plotter = tfdocs.plots.HistoryPlotter()\n        plotter.plot({'Model': model_history}, metric=metric)\n        plt.title(f'{metric.upper()}')\n        if ylim is None:\n            plt.ylim([0, 1])\n        else:\n            plt.ylim(ylim)\n        plt.savefig(f'{metric}.png')\n        plt.close()\n    ```", "```\n    BATCH_SIZE = 256\n    BUFFER_SIZE = 1024\n    train_dataset, val_dataset, test_dataset = load_dataset()\n    train_dataset = (train_dataset\n                     .shuffle(buffer_size=BUFFER_SIZE)\n                     .batch(BATCH_SIZE)\n                     .prefetch(buffer_size=BUFFER_SIZE))\n    val_dataset = (val_dataset\n                   .batch(BATCH_SIZE)\n                   .prefetch(buffer_size=BUFFER_SIZE))\n    test_dataset = test_dataset.batch(BATCH_SIZE)\n    ```", "```\n    EPOCHS = 100\n    model = build_network()\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model_history = model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS, verbose=0)\n    ```", "```\n    plot_model_history(model_history, 'loss', [0., 2.0])\n    plot_model_history(model_history, 'accuracy')\n    ```", "```\n    plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n    ```", "```\n    model.save('model.hdf5')\n    ```", "```\n    loaded_model = load_model('model.hdf5')\n    results = loaded_model.evaluate(test_dataset, verbose=0)\n    print(f'Loss: {results[0]}, Accuracy: {results[1]}')\n    ```", "```\n    Loss: 0.2943768735975027, Accuracy: 0.9132000207901001\n    ```"]