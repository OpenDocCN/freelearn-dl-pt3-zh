["```\ntrainval_image_dir = os.path.join('data', 'train2014', 'train2014')\ntrainval_captions_dir = os.path.join('data', 'annotations_trainval2014', 'annotations')\ntest_image_dir = os.path.join('data', 'val2017', 'val2017')\ntest_captions_dir = os.path.join('data', 'annotations_trainval2017', 'annotations')\ntrainval_captions_filepath = os.path.join(trainval_captions_dir, 'captions_train2014.json')\ntest_captions_filepath = os.path.join(test_captions_dir, 'captions_val2017.json') \n```", "```\nall_filepaths = np.array([os.path.join(trainval_image_dir, f) for f in os.listdir(trainval_image_dir)])\nrand_indices = np.arange(len(all_filepaths))\nnp.random.shuffle(rand_indices)\nsplit = int(len(all_filepaths)*0.8)\ntrain_filepaths, valid_filepaths = all_filepaths[rand_indices[:split]], all_filepaths[rand_indices[split:]] \n```", "```\nprint(f\"Train dataset size: {len(train_filepaths)}\")\nprint(f\"Valid dataset size: {len(valid_filepaths)}\") \n```", "```\nTrain dataset size: 66226\nValid dataset size: 16557 \n```", "```\nwith open(trainval_captions_filepath, 'r') as f:\n    trainval_data = json.load(f)\ntrainval_captions_df = pd.json_normalize(trainval_data, \"annotations\") \n```", "```\ntrainval_captions_df[\"image_filepath\"] = trainval_captions_df[\"image_id\"].apply(\n    lambda x: os.path.join(trainval_image_dir, \n    'COCO_train2014_'+format(x, '012d')+'.jpg')\n)\ntrain_captions_df = trainval_captions_df[trainval_captions_df[\"image_filepath\"].isin(train_filepaths)] \n```", "```\ndef preprocess_captions(image_captions_df):\n    \"\"\" Preprocessing the captions \"\"\"\n\n    image_captions_df[\"preprocessed_caption\"] = \"[START] \" + \n    image_captions_df[\"caption\"].str.lower().str.replace('[^\\w\\s]','') \n    + \" [END]\"\n    return image_captions_df \n```", "```\ntrain_captions_df = preprocess_captions(train_captions_df) \n```", "```\nvalid_captions_df = trainval_captions_df[\n    trainval_captions_df[\n        \"image_filepath\"\n    ].isin(valid_filepaths)\n]\nvalid_captions_df = preprocess_captions(valid_captions_df)\nwith open(test_captions_filepath, 'r') as f:\n    test_data = json.load(f)\n\ntest_captions_df = pd.json_normalize(test_data, \"annotations\")\ntest_captions_df[\"image_filepath\"] = test_captions_df[\"image_id\"].apply(\n    lambda x: os.path.join(test_image_dir, format(x, '012d')+'.jpg')\n)\ntest_captions_df = preprocess_captions(test_captions_df) \n```", "```\nn_samples = 1000\ntrain_image_stats_df = train_captions_df.loc[:n_samples, \"image_filepath\"].apply(lambda x: Image.open(x).size)\ntrain_image_stats_df = pd.DataFrame(train_image_stats_df.tolist(), index=train_image_stats_df.index)\ntrain_image_stats_df.describe() \n```", "```\ntrain_vocabulary = train_captions_df[\"preprocessed_caption\"].str.split(\" \").explode().value_counts()\nprint(len(train_vocabulary[train_vocabulary>=25])) \n```", "```\n3629 \n```", "```\nfrom tokenizers import BertWordPieceTokenizer \n```", "```\n# Initialize an empty BERT tokenizer\ntokenizer = BertWordPieceTokenizer(\n    unk_token=\"[UNK]\",\n    clean_text=False,\n    lowercase=False,\n) \n```", "```\n train_captions_df[\"preprocessed_caption\"].tolist(),\n    vocab_size=4000,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n) \n```", "```\n# Encoding a sentence\nexample_captions = valid_captions_df[\"preprocessed_caption\"].iloc[:10].tolist()\nexample_tokenized_captions = tokenizer.encode_batch(example_captions)\nfor caption, tokenized_cap in zip(example_captions, example_tokenized_captions):\n    print(f\"{caption} -> {tokenized_cap.tokens}\") \n```", "```\n[START] an empty kitchen with white and black appliances [END] -> ['[START]', 'an', 'empty', 'kitchen', 'with', 'white', 'and', 'black', 'appliances', '[END]']\n[START] a white square kitchen with tile floor that needs repairs  [END] -> ['[START]', 'a', 'white', 'square', 'kitchen', 'with', 'tile', 'floor', 'that', 'need', '##s', 'rep', '##air', '##s', '[END]']\n[START] a few people sit on a dim transportation system  [END] -> ['[START]', 'a', 'few', 'people', 'sit', 'on', 'a', 'dim', 'transport', '##ation', 'system', '[END]']\n[START] a person protected from the rain by their umbrella walks down the road [END] -> ['[START]', 'a', 'person', 'prote', '##cted', 'from', \n'the', 'rain', 'by', 'their', 'umbrella', 'walks', 'down', 'the', 'road', '[END]']\n[START] a white kitchen in a home with the light on [END] -> ['[START]', 'a', 'white', 'kitchen', 'in', 'a', 'home', 'with', 'the', 'light', 'on', '[END]'] \n```", "```\nvocab = tokenizer.get_vocab()\nfor token in [\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\"]:\n    print(f\"{token} -> {vocab[token]}\") \n```", "```\n[UNK] -> 1\n[PAD] -> 0\n[START] -> 2\n[END] -> 3 \n```", "```\ndef parse_image(filepath, resize_height, resize_width):\n    \"\"\" Reading an image from a given filepath \"\"\"\n\n    # Reading the image\n    image = tf.io.read_file(filepath)\n    # Decode the JPEG, make sure there are 3 channels in the output\n    image = tf.io.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    # Resize the image to 224x224\n    image = tf.image.resize(image, [resize_height, resize_width])\n\n    # Bring pixel values to [-1, 1]\n    image = image*2.0 - 1.0\n\n    return image \n```", "```\ndef generate_tokenizer(captions_df, n_vocab):\n    \"\"\" Generate the tokenizer with given captions \"\"\"\n\n    # Define the tokenizer\n    tokenizer = BertWordPieceTokenizer(\n        unk_token=\"[UNK]\",\n        clean_text=False,\n        lowercase=False,\n    )\n\n    # Train the tokenizer\n    tokenizer.train_from_iterator(\n        captions_df[\"preprocessed_caption\"].tolist(),\n        vocab_size=n_vocab,\n        special_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n    )\n\n    return tokenizer \n```", "```\ndef generate_tf_dataset(\n    image_captions_df, tokenizer=None, n_vocab=5000, pad_length=33, batch_size=32, training=False\n):\n    \"\"\" Generate the tf.data.Dataset\"\"\"\n\n    # If the tokenizer is not available, create one\n    if not tokenizer:\n        tokenizer = generate_tokenizer(image_captions_df, n_vocab)\n\n    # Get the caption IDs using the tokenizer\n    image_captions_df[\"caption_token_ids\"] = [enc.ids for enc in \n    tokenizer.encode_batch(image_captions_df[\"preprocessed_caption\"])]\n\n    vocab = tokenizer.get_vocab()\n\n    # Add the padding to short sentences and truncate long ones\n    image_captions_df[\"caption_token_ids\"] = \n    image_captions_df[\"caption_token_ids\"].apply(\n        lambda x: x+[vocab[\"[PAD]\"]]*(pad_length - len(x) + 2) if \n        pad_length + 2 >= len(x) else x[:pad_length + 1] + [x[-1]]\n    ) \n\n    # Create a dataset with images and captions\n    dataset = tf.data.Dataset.from_tensor_slices({\n        \"image_filepath\": image_captions_df[\"image_filepath\"],\n        \"caption_token_ids\": \n        np.array(image_captions_df[\"caption_token_ids\"].tolist())\n    })\n\n    # Each sample in our dataset consists of (image, caption token \n    # IDs, position IDs), (caption token IDs offset by 1)\n    dataset = dataset.map(\n        lambda x: (\n            (parse_image(x[\"image_filepath\"], 224, 224), \n            x[\"caption_token_ids\"][:-1], tf.range(pad_length+1, \n            dtype='float32')), x[\"caption_token_ids\"]\n        )\n    )\n\n    # Shuffle and batch data in the training mode\n    if training:\n        dataset = dataset.shuffle(buffer_size=batch_size*10)\n\n    dataset = dataset.batch(batch_size)\n\n    return dataset, tokenizer \n```", "```\nn_vocab=4000\nbatch_size=2\nsample_dataset, sample_tokenizer = generate_tf_dataset(train_captions_df, n_vocab=n_vocab, pad_length=10, batch_size=batch_size, training=True)\nfor i in sample_dataset.take(1):\n    print(i) \n```", "```\n(\n    (\n        <tf.Tensor: shape=(2, 224, 224, 3), dtype=float32, numpy=\n         array([[[[-0.2051357 , -0.22082198, -0.31493968],\n         [-0.2015593 , -0.21724558, -0.31136328],\n         [-0.17017174, -0.18585801, -0.2799757 ],\n         ...,\n         [-0.29620153, -0.437378  , -0.6155298 ],\n         [-0.28843057, -0.41392076, -0.6178423 ],\n         [-0.29654706, -0.43772352, -0.62483776]],\n        [[-0.8097613 , -0.6725868 , -0.55734015],\n         [-0.7580646 , -0.6420185 , -0.55782473],\n         [-0.77606916, -0.67418844, -0.5419755 ],\n         ...,\n         [-0.6400192 , -0.4753132 , -0.24786222],\n         [-0.70908225, -0.5426947 , -0.31580424],\n         [-0.7206869 , -0.5324516 , -0.3128438 ]]]], dtype=float32)>,\n\n        <tf.Tensor: shape=(2, 11), dtype=int32, numpy=\n         array([[   2,   24,  356,  114,  488, 1171, 1037, 2820,  566,  445,  116],\n         [   2,   24, 1357, 2142,   63, 1473,  495,  282,  116,   24,  301]])>, \n        <tf.Tensor: shape=(2, 11), dtype=float32, numpy=\n         array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n         [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n      dtype=float32)>\n    ), \n    <tf.Tensor: shape=(2, 12), dtype=int32, numpy=\n     array([[   2,   24,  356,  114,  488, 1171, 1037, 2820,  566,  445,  116,\n           3],\n     [   2,   24, 1357, 2142,   63, 1473,  495,  282,  116,   24,  301,\n           3]])>\n) \n```", "```\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K \n```", "```\nimage_encoder = hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_s16_fe/1\", trainable=False) \n```", "```\nimage_input = tf.keras.layers.Input(shape=(224, 224, 3))\nimage_features = image_encoder(image_input) \n```", "```\nprint(f\"Final representation shape: {image_features.shape}\") \n```", "```\nFinal representation shape: (None, 384) \n```", "```\nclass SelfAttentionLayer(tf.keras.layers.Layer):\n    \"\"\" Defines the computations in the self attention layer \"\"\"\n\n    def __init__(self, d):        \n        super(SelfAttentionLayer, self).__init__()\n        # Feature dimensionality of the output\n        self.d = d\n\n    def build(self, input_shape):\n        # Query weight matrix\n        self.Wq = self.add_weight(\n            shape=(input_shape[-1], self.d), \n            initializer='glorot_uniform',\n            trainable=True, dtype='float32'\n        )        \n        # Key weight matrix\n        self.Wk = self.add_weight(\n            shape=(input_shape[-1], self.d), \n            initializer='glorot_uniform',\n            trainable=True, dtype='float32'\n        )\n        # Value weight matrix\n        self.Wv = self.add_weight(\n            shape=(input_shape[-1], self.d), \n            initializer='glorot_uniform',\n            trainable=True, dtype='float32'\n        )\n\n    def call(self, q_x, k_x, v_x, mask=None):\n\n        q = tf.matmul(q_x,self.Wq) #[None, t, d]\n        k = tf.matmul(k_x,self.Wk) #[None, t, d]\n        v = tf.matmul(v_x,self.Wv) #[None, t, d]\n\n        # Computing the final output\n        h = tf.keras.layers.Attention(causal=True)([\n            q, #q\n            v, #v\n            k, #k\n        ], mask=[None, mask]) \n        # [None, t, t] . [None, t, d] => [None, t, d]\n        return h \n```", "```\nclass TransformerDecoderLayer(tf.keras.layers.Layer):\n    \"\"\" The Decoder layer \"\"\"\n\n    def __init__(self, d, n_heads):\n        super(TransformerDecoderLayer, self).__init__()\n        # Feature dimensionality\n        self.d = d\n\n        # Dimensionality of a head\n        self.d_head = int(d/n_heads) \n\n        # Number of heads\n        self.n_heads = n_heads\n\n        # Actual attention heads\n        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in \n        range(self.n_heads)]\n\n        # Fully connected layers\n        self.fc1_layer = tf.keras.layers.Dense(512, activation='relu')\n        self.fc2_layer = tf.keras.layers.Dense(d)\n\n        self.add_layer = tf.keras.layers.Add()\n        self.norm1_layer = tf.keras.layers.LayerNormalization()\n        self.norm2_layer = tf.keras.layers.LayerNormalization()\n\n    def _compute_multihead_output(self, x):\n        \"\"\" Computing the multi head attention output\"\"\"\n        outputs = [head(x, x, x) for head in self.attn_heads]\n        outputs = tf.concat(outputs, axis=-1)\n        return outputs\n\n    def call(self, x):\n\n        # Multi head attention layer output\n        h1 = self._compute_multihead_output(x)\n\n        h1_add = self.add_layer([x, h1])\n        h1_norm = self.norm1_layer(h1_add)\n\n        # Fully connected outputs\n        h2_1 = self.fc1_layer(h1_norm)\n        h2_2 = self.fc2_layer(h2_1)\n\n        h2_add = self.add_layer([h1, h2_2])\n        h2_norm = self.norm2_layer(h2_add)\n\n        return h2_norm \n```", "```\ncaption_input = tf.keras.layers.Input(shape=(None,))\nposition_input = tf.keras.layers.Input(shape=(None,)) \n```", "```\nd_model = 384\n# Token embeddings\ninput_embedding = tf.keras.layers.Embedding(len(tokenizer.get_vocab()), d_model, mask_zero=True) \n```", "```\nposition_embedding = tf.keras.layers.Lambda(\n    lambda x: tf.where(\n        tf.math.mod(tf.repeat(tf.expand_dims(x, axis=-1), d_model, \n        axis=-1), 2)==0,\n        tf.math.sin(\n            tf.expand_dims(x, axis=-1) /\n            10000**(2*tf.reshape(tf.range(d_model, \n            dtype='float32'),[1,1, -1])/d_model)\n        ),\n        tf.math.cos(\n            tf.expand_dims(x, axis=-1) /\n            10000**(2*tf.reshape(tf.range(d_model, \n            dtype='float32'),[1,1, -1])/d_model)\n        )\n    )\n) \n```", "```\nx = PE(pos, i) = sin(pos/10000**(2i/d))\ny = PE(pos, i) = cos(pos/10000**(2i/d)) \n```", "```\ntf.math.sin(\n            tf.expand_dims(x, axis=-1) /\n            10000**(2*tf.reshape(tf.range(d_model, \n            dtype='float32'),[1,1, -1])/d_model)\n        ) \n```", "```\nembed_out = input_embedding(caption_input) + position_embedding(position_input) \n```", "```\nimage_caption_embed_out = tf.keras.layers.Concatenate(axis=1)([tf.expand_dims(image_features,axis=1), embed_out]) \n```", "```\nout = image_caption_embed_out\nfor l in range(4):\n    out  = TransformerDecoderLayer(d_model, 64)(out) \n```", "```\nfinal_out = tf.keras.layers.Dense(n_vocab, activation='softmax')(out) \n```", "```\nfull_model = tf.keras.models.Model(inputs=[image_input, caption_input, position_input], outputs=final_out)\nfull_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')\nfull_model.summary() \n```", "```\nn_vocab = 4000\nbatch_size=96\ntrain_fraction = 0.6\nvalid_fraction = 0.2 \n```", "```\ntokenizer = generate_tokenizer(\n    train_captions_df, n_vocab=n_vocab\n) \n```", "```\nbleu_metric = BLEUMetric(tokenizer=tokenizer) \n```", "```\nsampled_validation_captions_df = valid_captions_df.sample(frac=valid_fraction) \n```", "```\nfor e in range(5):\n    print(f\"Epoch: {e+1}\")\n\n    train_dataset, _ = generate_tf_dataset(\n        train_captions_df.sample(frac=train_fraction), \n        tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, \n        training=True\n    )\n    valid_dataset, _ = generate_tf_dataset(\n        sampled_validation_captions_df, tokenizer=tokenizer, \n        n_vocab=n_vocab, batch_size=batch_size, training=False\n    )\n\n    full_model.fit(\n        train_dataset,\n        epochs=1\n    )\n\n    valid_loss, valid_accuracy, valid_bleu = [], [], []\n    for vi, v_batch in enumerate(valid_dataset):\n        print(f\"{vi+1} batches processed\", end='\\r')\n        loss, accuracy = full_model.test_on_batch(v_batch[0], \n        v_batch[1])\n        batch_predicted = full_model(v_batch[0])\n        bleu_score = \n        bleu_metric.calculate_bleu_from_predictions(v_batch[1], \n        batch_predicted)\n        valid_loss.append(loss)\n        valid_accuracy.append(accuracy)\n        valid_bleu.append(bleu_score)\n\n    print(\n        f\"\\nvalid_loss: {np.mean(valid_loss)} - valid_accuracy: \n        {np.mean(valid_accuracy)} - valid_bleu: {np.mean(valid_bleu)}\"\n    ) \n```", "```\nEpoch: 1\n2071/2071 [==============================] - 1945s 903ms/step - loss: 1.3344 - accuracy: 0.7625\n173 batches processed\nvalid_loss: 1.1388846477332142 - valid_accuracy: 0.7819634135058849 - valid_bleu: 0.09385878526196685\nEpoch: 2\n2071/2071 [==============================] - 1854s 894ms/step - loss: 1.0860 - accuracy: 0.7878\n173 batches processed\nvalid_loss: 1.090059520192229 - valid_accuracy: 0.7879036186058397 - valid_bleu: 0.10231472779803133\nEpoch: 3\n2071/2071 [==============================] - 1855s 895ms/step - loss: \n1.0610 - accuracy: 0.7897\n173 batches processed\nvalid_loss: 1.0627685799075 - valid_accuracy: 0.7899546606003205 - valid_bleu: 0.10398145099074609\nEpoch: 4\n2071/2071 [==============================] - 1937s 935ms/step - loss: 1.0479 - accuracy: 0.7910\n173 batches processed\nvalid_loss: 1.0817485169179177 - valid_accuracy: 0.7879597275932401 - valid_bleu: 0.10308500219058511\nEpoch: 5\n2071/2071 [==============================] - 1864s 899ms/step - loss: 1.0244 - accuracy: 0.7937\n173 batches processed\nvalid_loss: 1.0498641329693656 - valid_accuracy: 0.79208166544148 - valid_bleu: 0.10667336005789202 \n```", "```\nbleu_metric = BLEUMetric(tokenizer=tokenizer)\ntest_dataset, _ = generate_tf_dataset(\n    test_captions_df, tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, training=False\n)\ntest_loss, test_accuracy, test_bleu = [], [], []\nfor ti, t_batch in enumerate(test_dataset):\n    print(f\"{ti+1} batches processed\", end='\\r')\n    loss, accuracy = full_model.test_on_batch(t_batch[0], t_batch[1])\n    batch_predicted = full_model.predict_on_batch(t_batch[0])\n    bleu_score = bleu_metric.calculate_bleu_from_predictions(t_batch[1], batch_predicted)\n    test_loss.append(loss)\n    test_accuracy.append(accuracy)\n    test_bleu.append(bleu_score)\nprint(\n    f\"\\ntest_loss: {np.mean(test_loss)} - test_accuracy: {np.mean(test_accuracy)} - test_bleu: {np.mean(test_bleu)}\"\n) \n```", "```\n261 batches processed\ntest_loss: 1.057080413646625 - test_accuracy: 0.7914185857407434 - test_bleu: 0.10505496256163914 \n```", "```\nn_samples = 10\ntest_dataset, _ = generate_tf_dataset(\n    test_captions_df.sample(n=n_samples), tokenizer=tokenizer, \n    n_vocab=n_vocab, batch_size=n_samples, training=False\n) \n```", "```\ndef generate_caption(model, image_input, tokenizer, n_samples):\n    # 2 -> [START]\n    batch_tokens = np.repeat(np.array([[2]]), n_samples, axis=0)\n\n    for i in range(30):\n        if np.all(batch_tokens[:,-1] == 3):\n            break\n\n        position_input = tf.repeat(tf.reshape(tf.range(i+1),[1,-1]), \n        n_samples, axis=0)\n        probs = full_model((image_input, batch_tokens, \n        position_input)).numpy()\n        batch_tokens = np.argmax(probs, axis=-1)\n\n    predicted_text = []\n    for sample_tokens in batch_tokens:\n        sample_predicted_token_ids = sample_tokens.ravel()\n        sample_predicted_tokens = []\n        for wid in sample_predicted_token_ids:\n            sample_predicted_tokens.append(tokenizer.id_to_token(wid))\n            if wid == 3:\n                break\n        sample_predicted_text = \" \".join([tok for tok in \n        sample_predicted_tokens])\n        sample_predicted_text = sample_predicted_text.replace(\" ##\", \n        \"\")\n        predicted_text.append(sample_predicted_text)\n\n    return predicted_text \n```", "```\nfor batch in test_dataset.take(1):\n    (batch_image_input, _, _), batch_true_caption = batch\nbatch_predicted_text = generate_caption(full_model, batch_image_input, tokenizer, n_samples) \n```", "```\nfig, axes = plt.subplots(n_samples, 2, figsize=(8,30))\nfor i,(sample_image_input, sample_true_caption, sample_predicated_caption) in enumerate(zip(batch_image_input, batch_true_caption, batch_predicted_text)):\n\n    sample_true_caption_tokens  = [tokenizer.id_to_token(wid) for wid in\n    sample_true_caption.numpy().ravel()]\n\n    sample_true_text = []\n    for tok in sample_true_caption_tokens:\n        sample_true_text.append(tok)\n        if tok == '[END]':\n            break\n\n    sample_true_text = \" \".join(sample_true_text).replace(\" ##\", \"\")\n    axes[i][0].imshow(((sample_image_input.numpy()+1.0)/2.0))\n    axes[i][0].axis('off')\n\n    true_annotation = f\"TRUE: {sample_true_text}\"\n    predicted_annotation = f\"PRED: {sample_predicated_caption}\"\n    axes[i][1].text(0, 0.75, true_annotation, fontsize=18)\n    axes[i][1].text(0, 0.25, predicted_annotation, fontsize=18)\n    axes[i][1].axis('off') \n```"]