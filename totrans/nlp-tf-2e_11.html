<html><head></head><body>
  <div id="_idContainer579" class="Basic-Text-Frame">
    <h1 class="chapterNumber">11</h1>
    <h1 id="_idParaDest-277" class="chapterTitle">Image Captioning with Transformers</h1>
    <p class="normal">Transformer models changed the playing field for many NLP problems. They have redefined the state of the art by a significant margin, compared to the previous leaders: RNN-based models. We have already studied Transformers and understood what makes them tick. Transformers have access to the whole sequence of items (e.g. a sequence of tokens), as opposed to RNN-based models that look at one item at a time, making them well-suited for sequential problems. Following their success in the field of NLP, researchers have successfully used Transformers to solve computer vision problems. Here we will learn how to use Transformers to solve a multi-modal problem involving both images and text: image captioning.</p>
    <p class="normal">Automated image captioning, or image annotation, has a wide variety of applications. One of the most prominent applications is image retrieval in search engines. Automated image captioning can be used to retrieve all the images belonging to a certain class (for example, a cat) as per the user’s request. Another application can be in social media where, when an image is uploaded by a user, the image is automatically captioned so that the user can either refine the generated caption or post it as it is.</p>
    <p class="normal">In this chapter, we will learn to caption images using machine learning, where a model is trained to generate a sequence of tokens (i.e. a caption) when given an image. We will first understand how Transformer models are used in computer vision, and then extend our understanding to solve the problem of generating captions for images. For generating captions for images, we will use<a id="_idIndexMarker1047"/> a popular dataset for image captioning tasks known as <strong class="keyWord">Microsoft Common Objects in Context</strong> (<strong class="keyWord">MS-COCO</strong>). </p>
    <p class="normal">Solving this will require two Transformer models: one to generate an image representation and the other to generate the relevant caption. Once the image representation is generated, it will be fed as one of the inputs to the text-based Transformer model. The text-based Transformer model will be trained to predict the next token in the caption given the current caption, at a given time step.</p>
    <p class="normal">We will generate three datasets: training, validation, and testing datasets. We use the training dataset to train the model and the validation set to monitor performance during training. Finally we use the test dataset to generate captions for a set of unseen images.</p>
    <p class="normal">Looking at the image caption generation pipeline at a very high level, we have two main components:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">A pretrained Vision Transformer model that takes in an image and produces a 1D hidden representation of the image</li>
      <li class="numberedList">A text-based Transformer decoder model that can decode the hidden image representation to a series of token IDs</li>
    </ol>
    <p class="normal">We will use a pretrained Transformer model to generate image representations. Known as the Vision Transformer (ViT), it has been trained on the ImageNet dataset and has delivered great performance on the ImageNet classification task.</p>
    <p class="normal">Specifically, this chapter will cover the following main topics:</p>
    <ul>
      <li class="bulletList">Getting to know the data</li>
      <li class="bulletList">Downloading the data</li>
      <li class="bulletList">Processing and tokenizing data</li>
      <li class="bulletList">Defining a <code class="inlineCode">tf.data.Dataset</code></li>
      <li class="bulletList">The machine learning pipeline for image caption generation</li>
      <li class="bulletList">Implementing the model with TensorFlow</li>
      <li class="bulletList">Training the model</li>
      <li class="bulletList">Evaluating the results quantitatively</li>
      <li class="bulletList">Evaluating the model</li>
      <li class="bulletList">Captions generated for test images</li>
    </ul>
    <h1 id="_idParaDest-278" class="heading-1">Getting to know the data</h1>
    <p class="normal">Let’s first understand the data we are working with both directly and indirectly. There are two datasets<a id="_idIndexMarker1048"/> we will rely on:</p>
    <ul>
      <li class="bulletList">The<a id="_idIndexMarker1049"/> ILSVRC ImageNet dataset (<a href="http://image-net.org/download"><span class="url">http://image-net.org/download</span></a>)</li>
      <li class="bulletList">The MS-COCO<a id="_idIndexMarker1050"/> dataset (<a href="http://cocodataset.org/#download"><span class="url">http://cocodataset.org/#download</span></a>)</li>
    </ul>
    <p class="normal">We will not engage the first dataset directly, but it is essential for caption learning. This dataset contains images and their respective class labels (for example, cat, dog, and car). We will use a CNN that is already trained on this dataset, so we do not have to download and train on this dataset from scratch. Next we will use the MS-COCO dataset, which contains images and their respective captions. We will directly learn from this dataset by mapping the image to a fixed-size feature vector, using the Vision Transformer, and then map this vector to the corresponding caption using a text-based Transformer (we will discuss this process in detail later).</p>
    <h2 id="_idParaDest-279" class="heading-2">ILSVRC ImageNet dataset</h2>
    <p class="normal">ImageNet is an image dataset<a id="_idIndexMarker1051"/> that contains a large set<a id="_idIndexMarker1052"/> of images (~1 million) and their respective labels. These images belong to 1,000 different categories. This dataset is very expressive and contains almost all the objects found in the images we want to generate captions for. <em class="italic">Figure 11.1</em> shows some of the classes available in the ImageNet dataset:</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_01.png" alt="ILSVRC ImageNet dataset"/></figure>
    <p class="packt_figref">Figure 11.1: A small sample of the ImageNet dataset</p>
    <p class="normal">ImageNet is a good dataset<a id="_idIndexMarker1053"/> to train on, in order to obtain image encodings<a id="_idIndexMarker1054"/> that are required for caption generation. We say we use this dataset indirectly because we will use a pretrained Transformer that is trained on this dataset. Therefore, we will not be downloading, nor training the model on this dataset, by ourselves.</p>
    <h2 id="_idParaDest-280" class="heading-2">The MS-COCO dataset</h2>
    <p class="normal">Now we will move <a id="_idIndexMarker1055"/>on to the dataset that<a id="_idIndexMarker1056"/> we will actually be using, which is called <strong class="keyWord">MS-COCO</strong> (short for <strong class="keyWord">Microsoft - Common Objects in Context</strong>). We will use the training<a id="_idIndexMarker1057"/> dataset from the year 2014 and the validation set from 2017. We use datasets belonging to different times to avoid using large datasets for this exercise. As described earlier, this dataset consists of images and their respective descriptions. The dataset is quite large (for example, the training dataset consists of ~120,000 samples and can measure over 15 GB). Datasets are updated every year, and a competition is then held to recognize the team that achieves state-of-the-art performance. Using the full dataset is important when the objective is to achieve state-of-the-art performance. However, in our case, we want to learn a reasonable model that is able to suggest what is in an image generally. Therefore, we will use a smaller dataset (~40,000 images and ~200,000 captions) to train our model.  <em class="italic">Figure 11.2</em> includes some of the samples available:</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.2: A small sample of the MS-COCO dataset</p>
    <p class="normal">For learning with and testing<a id="_idIndexMarker1058"/> our end-to-end image caption generation model, we will use<a id="_idIndexMarker1059"/> the 2017 validation dataset, provided on the official MS-COCO dataset website.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">In practice, you should use separate datasets for testing and validation, to avoid data leakage during testing. Using the same data for validation and testing can lead the model to incorrectly represent its generalizability to the real world.</p>
    </div>
    <p class="normal">In <em class="italic">Figure 11.3</em>, we can see some of the images found in the validation set. These are some hand-picked examples from the validation set representing a variety of different objects and scenes:</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_03.png" alt="The MS-COCO dataset"/></figure>
    <p class="packt_figref">Figure 11.3: Unseen images that we will use to test the image caption generation capability of our algorithm</p>
    <h1 id="_idParaDest-281" class="heading-1">Downloading the data</h1>
    <p class="normal">The MS-COCO dataset<a id="_idIndexMarker1060"/> we will be using is quite large. Therefore, we will manually download these datasets. To do that, follow the instructions below:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Create a folder called <code class="inlineCode">data</code> in the <code class="inlineCode">Ch11-Image-Caption-Generation</code> folder </li>
      <li class="numberedList">Download the 2014 Train images set (<a href="http://images.cocodataset.org/zips/train2014.zip"><span class="url">http://images.cocodataset.org/zips/train2014.zip</span></a>) containing 83K images (<code class="inlineCode">train2014.zip</code>) </li>
      <li class="numberedList">Download the 2017 Val images set (<a href="http://images.cocodataset.org/zips/val2017.zip"><span class="url">http://images.cocodataset.org/zips/val2017.zip</span></a>) containing 5K images (<code class="inlineCode">val2017.zip</code>) </li>
      <li class="numberedList">Download the annotation sets for 2014 (<code class="inlineCode">annotations_trainval2014.zip</code>) (<a href="http://images.cocodataset.org/annotations/annotations_trainval2014.zip"><span class="url">http://images.cocodataset.org/annotations/annotations_trainval2014.zip</span></a>) and 2017 (<code class="inlineCode">annotations_trainval2017.zip</code>) (<a href="http://images.cocodataset.org/annotations/annotations_trainval2017.zip"><span class="url">http://images.cocodataset.org/annotations/annotations_trainval2017.zip</span></a>)</li>
      <li class="numberedList">Copy the downloaded zip files to the <code class="inlineCode">Ch11-Image-Caption-Generation/data</code> folder</li>
      <li class="numberedList">Extract the zip files using the <strong class="screenText">Extract to</strong> option so that it unzips the content within a sub-folder</li>
    </ol>
    <p class="normal">Once you complete the above steps, you should have the following subfolders:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">data/train2014</code> – Contains the training images</li>
      <li class="bulletList"><code class="inlineCode">data/annotations_trainval2014</code> – Contains the captions of the training images</li>
      <li class="bulletList"><code class="inlineCode">data/val2017</code> – Contains the validation images</li>
      <li class="bulletList"><code class="inlineCode">data/annotations_trainval2017</code> – Contains the captions of the validation images</li>
    </ul>
    <h1 id="_idParaDest-282" class="heading-1">Processing and tokenizing data</h1>
    <p class="normal">With the data downloaded and placed in the correct folders, let’s define the directories containing<a id="_idIndexMarker1061"/> the required data:</p>
    <pre class="programlisting code"><code class="hljs-code">trainval_image_dir = os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'train2014'</span>, <span class="hljs-string">'train2014'</span>)
trainval_captions_dir = os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'</span><span class="hljs-string">annotations_trainval2014'</span>, <span class="hljs-string">'annotations'</span>)
test_image_dir = os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'val2017'</span>, <span class="hljs-string">'val2017'</span>)
test_captions_dir = os.path.join(<span class="hljs-string">'data'</span>, <span class="hljs-string">'annotations_trainval2017'</span>, <span class="hljs-string">'annotations'</span>)
trainval_captions_filepath = os.path.join(trainval_captions_dir, <span class="hljs-string">'captions_train2014.json'</span>)
test_captions_filepath = os.path.join(test_captions_dir, <span class="hljs-string">'captions_val2017.json'</span>)
</code></pre>
    <p class="normal">Here we have defined <a id="_idIndexMarker1062"/>the directories containing training and testing images as well as the file paths of the JSON files that contain the captions of the training and testing images.</p>
    <h2 id="_idParaDest-283" class="heading-2">Preprocessing data</h2>
    <p class="normal">As the next step, let’s split the training<a id="_idIndexMarker1063"/> set in to train and validation sets. We will use 80% of the original set as training data and 20% as the validation data (randomly chosen):</p>
    <pre class="programlisting code"><code class="hljs-code">all_filepaths = np.array([os.path.join(trainval_image_dir, f) <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> os.listdir(trainval_image_dir)])
rand_indices = np.arange(<span class="hljs-built_in">len</span>(all_filepaths))
np.random.shuffle(rand_indices)
split = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(all_filepaths)*<span class="hljs-number">0.8</span>)
train_filepaths, valid_filepaths = all_filepaths[rand_indices[:split]], all_filepaths[rand_indices[split:]] 
</code></pre>
    <p class="normal">We can print the dataset sizes and see what we ended up with:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">f"Train dataset size: </span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(train_filepaths)}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Valid dataset size: </span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(valid_filepaths)}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">This will print:</p>
    <pre class="programlisting con"><code class="hljs-con">Train dataset size: 66226
Valid dataset size: 16557
</code></pre>
    <p class="normal">Now let’s read the captions and create a pandas DataFrame using them. Our DataFrame will have four important columns:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">image_id</code> – Identifies an image (used to generate the file path)</li>
      <li class="bulletList"><code class="inlineCode">image_filepath</code> – File location of the image identified by <code class="inlineCode">image_id</code></li>
      <li class="bulletList"><code class="inlineCode">caption</code> – Original caption</li>
      <li class="bulletList"><code class="inlineCode">preprocessed_caption</code> – Caption after some simple preprocessing</li>
    </ul>
    <p class="normal">First we will load the data in the JSON file and get the data into a DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(trainval_captions_filepath, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
    trainval_data = json.load(f)
trainval_captions_df = pd.json_normalize(trainval_data, <span class="hljs-string">"annotations"</span>)
</code></pre>
    <p class="normal">The data we’re looking<a id="_idIndexMarker1064"/> for in the file is found under a key called <code class="inlineCode">"annotations"</code>. Under <code class="inlineCode">"</code><code class="inlineCode">annotations"</code> we have a list of dictionaries each having the <code class="inlineCode">image_id</code>, <code class="inlineCode">id</code>, and <code class="inlineCode">caption</code>. The function <code class="inlineCode">pd.json_normalize()</code> takes in the loaded data and converts that to a <code class="inlineCode">pd.DataFrame</code>.</p>
    <p class="normal">We then create the column called <code class="inlineCode">image_filepath</code> by prefixing the root directory path to the <code class="inlineCode">image_id</code> and appending the extension <code class="inlineCode">.jpg</code>. </p>
    <p class="normal">We will only keep the data points where the <code class="inlineCode">image_filepath</code> values are in the training images we stored in <code class="inlineCode">train_filepaths</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">trainval_captions_df[<span class="hljs-string">"image_filepath"</span>] = trainval_captions_df[<span class="hljs-string">"image_id"</span>].apply(
    <span class="hljs-keyword">lambda</span> x: os.path.join(trainval_image_dir, 
<span class="hljs-keyword">    </span><span class="hljs-string">'COCO_train2014_'</span>+<span class="hljs-built_in">format</span>(x, <span class="hljs-string">'012d'</span>)+<span class="hljs-string">'.jpg'</span>)
)
train_captions_df = trainval_captions_df[trainval_captions_df[<span class="hljs-string">"image_filepath"</span>].isin(train_filepaths)]
</code></pre>
    <p class="normal">We now define a function called <code class="inlineCode">preprocess_captions()</code> that processes the original caption:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_captions</span>(<span class="hljs-params">image_captions_df</span>):
    <span class="hljs-string">""" Preprocessing the captions """</span>
    
    image_captions_df[<span class="hljs-string">"preprocessed_caption"</span>] = <span class="hljs-string">"[START] "</span> + 
    image_captions_df[<span class="hljs-string">"caption"</span>].<span class="hljs-built_in">str</span>.lower().<span class="hljs-built_in">str</span>.replace(<span class="hljs-string">'[^\w\s]'</span>,<span class="hljs-string">''</span>) 
    + <span class="hljs-string">" [END]"</span>
    <span class="hljs-keyword">return</span> image_captions_df
</code></pre>
    <p class="normal">In the above code, we:</p>
    <ul>
      <li class="bulletList">Added two special tokens, <code class="inlineCode">[START]</code> and <code class="inlineCode">[END]</code>, to denote the start and the end of each caption respectively</li>
      <li class="bulletList">Converted the captions to lowercase</li>
      <li class="bulletList">Removed everything that is not a word, character, or space</li>
    </ul>
    <p class="normal">We then call this function<a id="_idIndexMarker1065"/> on the training dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">train_captions_df = preprocess_captions(train_captions_df)
</code></pre>
    <p class="normal">We then follow a similar process for both validation and test data:</p>
    <pre class="programlisting code"><code class="hljs-code">valid_captions_df = trainval_captions_df[
    trainval_captions_df[
        <span class="hljs-string">"image_filepath"</span>
<span class="hljs-string">    </span>].isin(valid_filepaths)
]
valid_captions_df = preprocess_captions(valid_captions_df)
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(test_captions_filepath, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
    test_data = json.load(f)
    
test_captions_df = pd.json_normalize(test_data, <span class="hljs-string">"annotations"</span>)
test_captions_df[<span class="hljs-string">"image_filepath"</span>] = test_captions_df[<span class="hljs-string">"image_id"</span>].apply(
    <span class="hljs-keyword">lambda</span> x: os.path.join(test_image_dir, <span class="hljs-built_in">format</span>(x, <span class="hljs-string">'012d'</span>)+<span class="hljs-string">'.jpg'</span>)
)
test_captions_df = preprocess_captions(test_captions_df)
</code></pre>
    <p class="normal">Let’s check the data in <code class="inlineCode">training_captions_df</code> (<em class="italic">Figure 11.4</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.4: Data contained in training_captions_df</p>
    <p class="normal">This data shows important information such as where the image is located in the file structure, the original caption, and the preprocessed caption. </p>
    <p class="normal">Let’s also analyze some statistics about the images. We will take a small sample of the first 1,000 images from the training dataset and look at image sizes:</p>
    <pre class="programlisting code"><code class="hljs-code">n_samples = <span class="hljs-number">1000</span>
train_image_stats_df = train_captions_df.loc[:n_samples, <span class="hljs-string">"image_filepath"</span>].apply(<span class="hljs-keyword">lambda</span> x: Image.<span class="hljs-built_in">open</span>(x).size)
train_image_stats_df = pd.DataFrame(train_image_stats_df.tolist(), index=train_image_stats_df.index)
train_image_stats_df.describe()
</code></pre>
    <p class="normal">This will produce <em class="italic">Figure 11.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.5: Statistics about the size of the images in the training dataset</p>
    <p class="normal">We can see that most images<a id="_idIndexMarker1066"/> have a resolution of 640x640. We will later need to resize images to 224x224 to match the model’s input requirements. We’ll also look at our vocabulary size:</p>
    <pre class="programlisting code"><code class="hljs-code">train_vocabulary = train_captions_df[<span class="hljs-string">"preprocessed_caption"</span>].<span class="hljs-built_in">str</span>.split(<span class="hljs-string">" "</span>).explode().value_counts()
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(train_vocabulary[train_vocabulary&gt;=<span class="hljs-number">25</span>]))
</code></pre>
    <p class="normal">This prints:</p>
    <pre class="programlisting con"><code class="hljs-con">3629
</code></pre>
    <p class="normal">This tells us that 3,629 words occur <a id="_idIndexMarker1067"/>at least 25 times in our train dataset. We use this as our vocabulary size.</p>
    <h2 id="_idParaDest-284" class="heading-2">Tokenizing data</h2>
    <p class="normal">Since we are developing<a id="_idIndexMarker1068"/> Transformer models, we need a robust tokenizer similar to the ones used by popular models like BERT. Hugging Face’s <code class="inlineCode">tokenizers</code> library provides us with a range of tokenizers that are easy to use. Let’s understand how we can use one of these tokenizers for our purpose. You can import it using:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> BertWordPieceTokenizer
</code></pre>
    <p class="normal">Next, let’s define the <code class="inlineCode">BertWordPieceTokenizer</code>. We will pass the following arguments when doing so:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">unk_token</code> – Defines a token to be used for out-of-vocabulary words</li>
      <li class="bulletList"><code class="inlineCode">clean_text</code> – Whether to perform simple preprocessing steps to clean text</li>
      <li class="bulletList"><code class="inlineCode">lowercase</code> – Whether to lowercase the text</li>
    </ul>
    <p class="normal">These arguments can be seen in the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Initialize an empty BERT tokenizer</span>
tokenizer = BertWordPieceTokenizer(
    unk_token=<span class="hljs-string">"[UNK]"</span>,
    clean_text=<span class="hljs-literal">False</span>,
    lowercase=<span class="hljs-literal">False</span>,
)
</code></pre>
    <p class="normal">With the tokenizer defined, we can call the <code class="inlineCode">train_from_iterator()</code> function to train the tokenizer on our dataset:</p>
    <p class="normal">tokenizer.train_from_iterator(</p>
    <pre class="programlisting code"><code class="hljs-code">    train_captions_df[<span class="hljs-string">"preprocessed_caption"</span>].tolist(),
    vocab_size=<span class="hljs-number">4000</span>,
    special_tokens=[<span class="hljs-string">"[PAD]"</span>, <span class="hljs-string">"[UNK]"</span>, <span class="hljs-string">"[START]"</span>, <span class="hljs-string">"[END]"</span>]
)
</code></pre>
    <p class="normal">The <code class="inlineCode">train_from_iterator()</code> function takes in several arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">iterator</code> – An iterable that produces a string (containing the caption) as one item.</li>
      <li class="bulletList"><code class="inlineCode">vocab_size</code> – Size of the vocabulary.</li>
      <li class="bulletList"><code class="inlineCode">special_tokens</code> – Special tokens that will be used in our data. Specifically we use <code class="inlineCode">[PAD]</code> (to denote padding), <code class="inlineCode">[UNK]</code> (to denote OOV tokens), <code class="inlineCode">[START]</code> (to denote the start), and <code class="inlineCode">[END]</code> (to denote the end). These tokens will get assigned lower IDs starting from 0.</li>
    </ul>
    <p class="normal">Once the tokenizer is trained, we<a id="_idIndexMarker1069"/> can use it to convert strings of text to sequences of tokens. Let’s convert a few example sentences to sequences of tokens using the trained tokenizer:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Encoding a sentence</span>
example_captions = valid_captions_df[<span class="hljs-string">"preprocessed_caption"</span>].iloc[:<span class="hljs-number">10</span>].tolist()
example_tokenized_captions = tokenizer.encode_batch(example_captions)
<span class="hljs-keyword">for</span> caption, tokenized_cap <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(example_captions, example_tokenized_captions):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"</span><span class="hljs-subst">{caption}</span><span class="hljs-string"> -&gt; </span><span class="hljs-subst">{tokenized_cap.tokens}</span><span class="hljs-string">"</span>) 
</code></pre>
    <p class="normal">This will print:</p>
    <pre class="programlisting con"><code class="hljs-con">[START] an empty kitchen with white and black appliances [END] -&gt; ['[START]', 'an', 'empty', 'kitchen', 'with', 'white', 'and', 'black', 'appliances', '[END]']
[START] a white square kitchen with tile floor that needs repairs  [END] -&gt; ['[START]', 'a', 'white', 'square', 'kitchen', 'with', 'tile', 'floor', 'that', 'need', '##s', 'rep', '##air', '##s', '[END]']
[START] a few people sit on a dim transportation system  [END] -&gt; ['[START]', 'a', 'few', 'people', 'sit', 'on', 'a', 'dim', 'transport', '##ation', 'system', '[END]']
[START] a person protected from the rain by their umbrella walks down the road [END] -&gt; ['[START]', 'a', 'person', 'prote', '##cted', 'from', 
'the', 'rain', 'by', 'their', 'umbrella', 'walks', 'down', 'the', 'road', '[END]']
[START] a white kitchen in a home with the light on [END] -&gt; ['[START]', 'a', 'white', 'kitchen', 'in', 'a', 'home', 'with', 'the', 'light', 'on', '[END]']
</code></pre>
    <p class="normal">You can see how the tokenizer has learned its own vocabulary and is tokenizing string sentences. The words that contain <code class="inlineCode">##</code> in front mean they must be combined with the previous token (without spaces) to get the final result. For example, the final string from the tokens <code class="inlineCode">'image'</code>, <code class="inlineCode">'</code><code class="inlineCode">cap'</code> and <code class="inlineCode">'##tion'</code> is <code class="inlineCode">'image caption'</code>. Let’s see which IDs the special tokens we defined are mapped to:</p>
    <pre class="programlisting code"><code class="hljs-code">vocab = tokenizer.get_vocab()
<span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> [<span class="hljs-string">"[UNK]"</span>, <span class="hljs-string">"[PAD]"</span>, <span class="hljs-string">"[START]"</span>, <span class="hljs-string">"[END]"</span>]:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"</span><span class="hljs-subst">{token}</span><span class="hljs-string"> -&gt; </span><span class="hljs-subst">{vocab[token]}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">This will output:</p>
    <pre class="programlisting con"><code class="hljs-con">[UNK] -&gt; 1
[PAD] -&gt; 0
[START] -&gt; 2
[END] -&gt; 3
</code></pre>
    <p class="normal">Now let’s look at how<a id="_idIndexMarker1070"/> we can define a TensorFlow data pipeline using the processed data.</p>
    <h1 id="_idParaDest-285" class="heading-1">Defining a tf.data.Dataset</h1>
    <p class="normal">Now let’s look at how<a id="_idIndexMarker1071"/> we can create a <code class="inlineCode">tf.data.Dataset</code> using the data. We will<a id="_idIndexMarker1072"/> first write a few helper functions. Namely, we’ll define:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">parse_image()</code> to load and process an image from a <code class="inlineCode">filepath</code></li>
      <li class="bulletList"><code class="inlineCode">generate_tokenizer()</code> to generate a tokenizer trained on the data passed to the function</li>
    </ul>
    <p class="normal">First let’s discuss the <code class="inlineCode">parse_image()</code> function. It takes three arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">filepath</code> – Location of the image</li>
      <li class="bulletList"><code class="inlineCode">resize_height</code> – Height to resize the image to</li>
      <li class="bulletList"><code class="inlineCode">resize_width</code> – Width to resize the image to</li>
    </ul>
    <p class="normal">The function is defined as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_image</span>(<span class="hljs-params">filepath, resize_height, resize_width</span>):
    <span class="hljs-string">""" Reading an image from a given filepath """</span>
    
    <span class="hljs-comment"># Reading the image</span>
    image = tf.io.read_file(filepath)
    <span class="hljs-comment"># Decode the JPEG, make sure there are 3 channels in the output</span>
    image = tf.io.decode_jpeg(image, channels=<span class="hljs-number">3</span>)
    image = tf.image.convert_image_dtype(image, tf.float32)
    <span class="hljs-comment"># Resize the image to 224x224</span>
    image = tf.image.resize(image, [resize_height, resize_width])
    
    <span class="hljs-comment"># Bring pixel values to [-1, 1]</span>
    image = image*<span class="hljs-number">2.0</span> - <span class="hljs-number">1.0</span>
        
    <span class="hljs-keyword">return</span> image
</code></pre>
    <p class="normal">We are mostly relying on <code class="inlineCode">tf.image</code> functions<a id="_idIndexMarker1073"/> to load and process the image. This function<a id="_idIndexMarker1074"/> specifically:</p>
    <ul>
      <li class="bulletList">Reads the image from the <code class="inlineCode">filepath</code></li>
      <li class="bulletList">Decodes the bytes in the JPEG image to a <code class="inlineCode">uint8</code> tensor and converts to a <code class="inlineCode">float32 dtype</code> tensor. </li>
    </ul>
    <p class="normal">By the end of these steps, we’ll have an image whose pixel values are between 0 and 1. Next, we:</p>
    <ul>
      <li class="bulletList">Resize the image to a given height and width</li>
      <li class="bulletList">Finally normalize the image so that the pixel values are between -1 and 1 (as required by the ViT model we’ll be using)</li>
    </ul>
    <p class="normal">With that we define the second helper function. This function encapsulates the functionality of the <code class="inlineCode">BertWordPieceTokenizer</code> we have discussed previously:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_tokenizer</span>(<span class="hljs-params">captions_df, n_vocab</span>):
    <span class="hljs-string">""" Generate the tokenizer with given captions """</span>
    
    <span class="hljs-comment"># Define the tokenizer</span>
    tokenizer = BertWordPieceTokenizer(
        unk_token=<span class="hljs-string">"[UNK]"</span>,
        clean_text=<span class="hljs-literal">False</span>,
        lowercase=<span class="hljs-literal">False</span>,
    )
    
    <span class="hljs-comment"># Train the tokenizer</span>
    tokenizer.train_from_iterator(
        captions_df[<span class="hljs-string">"preprocessed_caption"</span>].tolist(),
        vocab_size=n_vocab,
        special_tokens=[<span class="hljs-string">"[PAD]"</span>, <span class="hljs-string">"[UNK]"</span>, <span class="hljs-string">"[START]"</span>, <span class="hljs-string">"[END]"</span>]
    )
    
    <span class="hljs-keyword">return</span> tokenizer
</code></pre>
    <p class="normal">With that we can define our main data function to generate the TensorFlow data pipeline:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_tf_dataset</span>(
    <span class="hljs-params">image_captions_df, tokenizer=</span><span class="hljs-literal">None</span><span class="hljs-params">, n_vocab=</span><span class="hljs-number">5000</span><span class="hljs-params">, pad_length=</span><span class="hljs-number">33</span><span class="hljs-params">, batch_size=</span><span class="hljs-number">32</span><span class="hljs-params">, training=</span><span class="hljs-literal">False</span>
):
    <span class="hljs-string">""" Generate the tf.data.Dataset"""</span>
    
    <span class="hljs-comment"># If the tokenizer is not available, create one</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> tokenizer:
        tokenizer = generate_tokenizer(image_captions_df, n_vocab)
        
    <span class="hljs-comment"># Get the caption IDs using the tokenizer</span>
    image_captions_df[<span class="hljs-string">"caption_token_ids"</span>] = [enc.ids <span class="hljs-keyword">for</span> enc <span class="hljs-keyword">in</span> 
    tokenizer.encode_batch(image_captions_df[<span class="hljs-string">"preprocessed_caption"</span>])]
    
    vocab = tokenizer.get_vocab()
    
    <span class="hljs-comment"># Add the padding to short sentences and truncate long ones</span>
    image_captions_df[<span class="hljs-string">"caption_token_ids"</span>] = 
    image_captions_df[<span class="hljs-string">"caption_token_ids"</span>].apply(
        <span class="hljs-keyword">lambda</span> x: x+[vocab[<span class="hljs-string">"</span><span class="hljs-string">[PAD]"</span>]]*(pad_length - <span class="hljs-built_in">len</span>(x) + <span class="hljs-number">2</span>) <span class="hljs-keyword">if</span> 
        pad_length + <span class="hljs-number">2</span> &gt;= <span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">else</span> x[:pad_length + <span class="hljs-number">1</span>] + [x[-<span class="hljs-number">1</span>]]
    ) 
    
    <span class="hljs-comment"># Create a dataset with images and captions</span>
    dataset = tf.data.Dataset.from_tensor_slices({
        <span class="hljs-string">"</span><span class="hljs-string">image_filepath"</span>: image_captions_df[<span class="hljs-string">"image_filepath"</span>],
        <span class="hljs-string">"caption_token_ids"</span>: 
<span class="hljs-string">        </span>np.array(image_captions_df[<span class="hljs-string">"caption_token_ids"</span>].tolist())
    })
    
    <span class="hljs-comment"># Each sample in our dataset consists of (image, caption token </span>
<span class="hljs-comment">    # IDs, position IDs), (caption token IDs offset by 1)</span>
    dataset = dataset.<span class="hljs-built_in">map</span>(
        <span class="hljs-keyword">lambda</span> x: (
            (parse_image(x[<span class="hljs-string">"image_filepath"</span>], <span class="hljs-number">224</span>, <span class="hljs-number">224</span>), 
            x[<span class="hljs-string">"caption_token_ids"</span>][:-<span class="hljs-number">1</span>], tf.<span class="hljs-built_in">range</span>(pad_length+<span class="hljs-number">1</span>, 
            dtype=<span class="hljs-string">'float32'</span>)), x[<span class="hljs-string">"caption_token_ids"</span>]
        )
    )
    
    <span class="hljs-comment"># Shuffle and batch data in the training mode</span>
    <span class="hljs-keyword">if</span> training:
        dataset = dataset.shuffle(buffer_size=batch_size*<span class="hljs-number">10</span>)
    
    dataset = dataset.batch(batch_size)
    
    <span class="hljs-keyword">return</span> dataset, tokenizer
</code></pre>
    <p class="normal">This function takes the following arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">image_captions_df</code> – A pandas DataFrame containing image file paths and processed captions</li>
      <li class="bulletList"><code class="inlineCode">tokenizer</code> – An optional tokenizer that will be used to tokenize the captions</li>
      <li class="bulletList"><code class="inlineCode">n_vocab</code> – The vocabulary size </li>
      <li class="bulletList"><code class="inlineCode">pad_length</code> – The length to pad captions</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code> – Batch size to batch the data</li>
      <li class="bulletList"><code class="inlineCode">training</code> – Whether the data pipeline should be run in training mode or not. In training mode, we shuffle data whereas otherwise, we do not</li>
    </ul>
    <p class="normal">First this function generates<a id="_idIndexMarker1075"/> a tokenizer if a new tokenizer has not been<a id="_idIndexMarker1076"/> passed. Next we create a column called “<code class="inlineCode">caption_token_ids</code>" in our DataFrame, which is created by calling the <code class="inlineCode">encode_batch()</code> function of the tokenizer on the <code class="inlineCode">preprocessed_caption</code> column. We then perform padding on the <code class="inlineCode">caption_token_ids</code> column. We add the <code class="inlineCode">[PAD]</code> token ID if a caption is shorter than <code class="inlineCode">pad_length</code>, or truncate it if it’s longer. We then create a <code class="inlineCode">tf.data.Dataset</code> using the <code class="inlineCode">from_tensor_slices() </code>function. </p>
    <p class="normal">Each sample in this dataset will be a dictionary with the key <code class="inlineCode">image_filepath</code> and <code class="inlineCode">caption_token_ids</code> and values containing corresponding values. Once we do this, we have the ingredients to get the actual data. We will call the <code class="inlineCode">tf.data.Dataset.map()</code> function to:</p>
    <ul>
      <li class="bulletList">Call <code class="inlineCode">parse_image()</code> on each <code class="inlineCode">image_filepath</code> to produce the actual image</li>
      <li class="bulletList">Return all caption token IDs, except the last, as inputs</li>
      <li class="bulletList">A range from 0 to the number of tokens, representing position of each input token ID (used to get positional embeddings for the Transformer)</li>
      <li class="bulletList">Return all caption token IDs as the targets</li>
    </ul>
    <p class="normal">Let’s understand what the inputs and outputs are going to look like for an example. Say you have the caption <em class="italic">a brown bear</em>. Here’s how the inputs and outputs going to look for our Transformer decoder (<em class="italic">Figure 11.6</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.6: How inputs and targets are organized for the model</p>
    <p class="normal">Finally, if in training mode, we shuffle<a id="_idIndexMarker1077"/> the dataset using a <code class="inlineCode">buffer_size</code> of 10 times<a id="_idIndexMarker1078"/> the batch size. Then we batch the dataset using the <code class="inlineCode">batch_size</code> provided when calling the function. Let’s call this function on our training dataset to see what we get:</p>
    <pre class="programlisting code"><code class="hljs-code">n_vocab=<span class="hljs-number">4000</span>
batch_size=<span class="hljs-number">2</span>
sample_dataset, sample_tokenizer = generate_tf_dataset(train_captions_df, n_vocab=n_vocab, pad_length=<span class="hljs-number">10</span>, batch_size=batch_size, training=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sample_dataset.take(<span class="hljs-number">1</span>):
    <span class="hljs-built_in">print</span>(i)
</code></pre>
    <p class="normal">Which will output:</p>
    <pre class="programlisting con"><code class="hljs-con">(
    (
        &lt;tf.Tensor: shape=(2, 224, 224, 3), dtype=float32, numpy=
         array([[[[-0.2051357 , -0.22082198, -0.31493968],
         [-0.2015593 , -0.21724558, -0.31136328],
         [-0.17017174, -0.18585801, -0.2799757 ],
         ...,
         [-0.29620153, -0.437378  , -0.6155298 ],
         [-0.28843057, -0.41392076, -0.6178423 ],
         [-0.29654706, -0.43772352, -0.62483776]],
        [[-0.8097613 , -0.6725868 , -0.55734015],
         [-0.7580646 , -0.6420185 , -0.55782473],
         [-0.77606916, -0.67418844, -0.5419755 ],
         ...,
         [-0.6400192 , -0.4753132 , -0.24786222],
         [-0.70908225, -0.5426947 , -0.31580424],
         [-0.7206869 , -0.5324516 , -0.3128438 ]]]], dtype=float32)&gt;,
 
        &lt;tf.Tensor: shape=(2, 11), dtype=int32, numpy=
         array([[   2,   24,  356,  114,  488, 1171, 1037, 2820,  566,  445,  116],
         [   2,   24, 1357, 2142,   63, 1473,  495,  282,  116,   24,  301]])&gt;, 
        &lt;tf.Tensor: shape=(2, 11), dtype=float32, numpy=
         array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],
         [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],
      dtype=float32)&gt;
    ), 
    &lt;tf.Tensor: shape=(2, 12), dtype=int32, numpy=
     array([[   2,   24,  356,  114,  488, 1171, 1037, 2820,  566,  445,  116,
           3],
     [   2,   24, 1357, 2142,   63, 1473,  495,  282,  116,   24,  301,
           3]])&gt;
)
</code></pre>
    <p class="normal">Here we can see<a id="_idIndexMarker1079"/> the inputs and outputs<a id="_idIndexMarker1080"/> organized into a nested tuple. It has the format <code class="inlineCode">((image, input caption token IDs, position IDs), target caption token IDs)</code>. For example, we have produced a data pipeline with a batch size of 2, a pad length of 10, and a vocabulary size of 4,000. We can see the image batch has the shape [2, 224, 224, 3], input caption token IDs and the position IDs have the shape [2, 11], and finally, the target caption token IDs are of shape [2, 12]. It is important to note that we use an additional buffer for padding length to incorporate the <code class="inlineCode">[START]</code> and <code class="inlineCode">[END]</code> tags. Therefore, the resulting tensors use a caption length of 12 (i.e. 10+2). The most important thing to note here is the length of the input and target captions. Input captions have one item less than the target captions as shown by the lengths. This is because, the first item in our input captions would be the image feature vector. This brings the length of input tokens to be equal to the length of target tokens.</p>
    <p class="normal">With the data pipeline<a id="_idIndexMarker1081"/> out of the way, we<a id="_idIndexMarker1082"/> will discuss the mechanics of the model we’ll be using.</p>
    <h1 id="_idParaDest-286" class="heading-1">The machine learning pipeline for image caption generation</h1>
    <p class="normal">Here we will look at the<a id="_idIndexMarker1083"/> image caption generation<a id="_idIndexMarker1084"/> pipeline at a very high level and then discuss it piece by piece until we have the full model. The image caption generation framework<a id="_idIndexMarker1085"/> consists of two main components:</p>
    <ul>
      <li class="bulletList">A pretrained Vision Transformer model to produce an image representation</li>
      <li class="bulletList">A text-based decoder model that can decode the image representation to a series of token IDs. This uses a text tokenizer to convert tokens to token IDs and vice versa</li>
    </ul>
    <p class="normal">Though the Transformer models were initially used for text-based NLP problems, they have out-grown the domain of text data and have been used in other areas such as image data and audio data. </p>
    <p class="normal">Here we will be using one Transformer model that can process image data and another that can process text data.</p>
    <h2 id="_idParaDest-287" class="heading-2">Vision Transformer (ViT)</h2>
    <p class="normal">First, let’s look at the Transformer<a id="_idIndexMarker1086"/> generating the encoded<a id="_idIndexMarker1087"/> vector representations of images. We will be using a pretrained <strong class="keyWord">Vision Transformer</strong> (<strong class="keyWord">ViT</strong>) to achieve this. This model has been trained on the ImageNet dataset we discussed above. Let’s understand the architecture of this model.</p>
    <p class="normal">Originally, the ViT was proposed in the paper <em class="italic">An Image is Worth 16X16 Words: Transformers for Image Recognition at Scale</em> by Dosovitskiy et al (<a href="https://arxiv.org/pdf/2010.11929.pdf"><span class="url">https://arxiv.org/pdf/2010.11929.pdf</span></a>). This can be considered the first substantial step toward adapting Transformers for computer vision problems. This model is called the Vision Transformer model.</p>
    <p class="normal">The idea is to decompose<a id="_idIndexMarker1088"/> an image into small patches of 16x16 and consider<a id="_idIndexMarker1089"/> each as a separate token. Each image path is flattened to a 1D vector and their position is encoded by a positional encoding mechanism similar to the original Transformer. But images are 2D structures; is it enough to have 1D positional information, and not 2D positional information? The authors argue that a 1D positional encoding was adequate and 2D positional encoding did not provide a significant boost. Once the image is broken into patches of 16x16 and flattened, each image can be presented as a sequence of tokens, just like a textual input sequence (<em class="italic">Figure 11.7</em>). </p>
    <p class="normal">Then the model is pretrained<a id="_idIndexMarker1090"/> in a self-supervised fashion, using a vision dataset called JFT-300M (<a href="https://paperswithcode.com/dataset/jft-300m"><span class="url">https://paperswithcode.com/dataset/jft-300m</span></a>). The paper proposes<a id="_idIndexMarker1091"/> an elegant way to train the ViT in a semi-supervised fashion using image data. Similar to how NLP problems represent a unit of text as a token, a token is a patch of an image (i.e. a sequence of continuous values where values are normalized pixels). Then the ViT is pretrained to predict the mean 3-bit RGB color of a given image patch. Each channel (i.e. red, green, and blue) is represented with 3 bits (each bit having a value of 0 or 1), which gives 512 possibilities or classes. In other words, for a given image, patches (similar to how tokens are treated in NLP) are masked randomly (using the same approach as BERT), and the model is asked to predict the mean 3-bit RGB color of that image patch. </p>
    <p class="normal">After pretraining, the model can be fine-tuned for a task-specific problem by fitting a classification or a regression head on top of the ViT, just like BERT. The ViT also has the <code class="inlineCode">[CLS]</code> token at the beginning of the sequence, which will be used as the input representation for downstream vision models that are plugged on top of the ViT. </p>
    <p class="normal"><em class="italic">Figure 11.7</em> illustrates the mechanics of the ViT:</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.7: The Vision Transformer model</p>
    <p class="normal">The model we’ll be using<a id="_idIndexMarker1092"/> here originated<a id="_idIndexMarker1093"/> from the paper <em class="italic">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers</em> by Steiner et al (<a href="https://arxiv.org/pdf/2106.10270.pdf"><span class="url">https://arxiv.org/pdf/2106.10270.pdf</span></a>). It proposes several variants of the ViT model. Specifically, we<a id="_idIndexMarker1094"/> will use the ViT-S/16 architecture. ViT-S is the second smallest ViT model with 12 layers and a hidden output dimensionality of 384; in total it has 22.2M parameters. The number 16 here means that the model is trained on image patches of 16x16. The model has been fine-tuned using the ImageNet dataset we discussed earlier. We will use the feature extractor part of the model for the purpose of image captioning.</p>
    <h2 id="_idParaDest-288" class="heading-2">Text-based decoder Transformer</h2>
    <p class="normal">The text-based decoder’s primary<a id="_idIndexMarker1095"/> purpose is to predict<a id="_idIndexMarker1096"/> the next token in the sequence given the previous tokens. This decoder is mostly similar to the BERT we used in the previous chapter. Let’s refresh our memory on what the Transformer model is composed of. The Transformer consists of several stacked layers. Each layer has:</p>
    <ul>
      <li class="bulletList">A self-attention layer – Generates a hidden<a id="_idIndexMarker1097"/> representation for each token<a id="_idIndexMarker1098"/> position by taking in the input token and attending to the tokens at other positions in the sequence</li>
      <li class="bulletList">A fully connected subnetwork – Generates an<a id="_idIndexMarker1099"/> element-wise non-linear<a id="_idIndexMarker1100"/> hidden representation by propagating the self-attention layer’s output through two fully connected layers</li>
    </ul>
    <p class="normal">In addition to these, the network uses residual connections and layer normalization techniques to enhance performance. When speaking of inputs, the model uses two types of input embeddings to inform the model:</p>
    <ul>
      <li class="bulletList">Token embeddings – Each token is represented<a id="_idIndexMarker1101"/> with an embedding vector that is jointly trained with the model</li>
      <li class="bulletList">Position embeddings – Each token position<a id="_idIndexMarker1102"/> is represented by an ID and a corresponding embedding for that position</li>
    </ul>
    <p class="normal">Compared to the BERT we used in the previous chapter, a key difference in our model is how we use the self-attention mechanism. When using BERT, the self-attention layer was able to pay attention in a bidirectional manner (i.e. pay attention to tokens on both sides of the current input). However, in the decoder-based model, it can only pay attention to the tokens to the left of the current token. In other words, the attention mechanism only has access to inputs seen up to the current input.</p>
    <h2 id="_idParaDest-289" class="heading-2">Putting everything together</h2>
    <p class="normal">Let’s now learn how to put the two models together. We will use the following procedure to train the model end to end:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">We generate the image encoding via the ViT model. It generates a single representation of 384 items for an image.</li>
      <li class="numberedList">This representation, along with all the caption tokens except the last, goes into the decoder as the inputs.</li>
      <li class="numberedList">Given the current input token, the decoder predicts the next token. At the end of this process we will have the full image caption.</li>
    </ol>
    <p class="normal">Another alternative for connecting<a id="_idIndexMarker1103"/> the ViT and the text decoder models<a id="_idIndexMarker1104"/> is by providing direct access to the ViT’s full sequence of encoder outputs as a part of the attention mechanism of the decoder. In this work, not to overcomplicate our discussion, we only use a single output from the ViT model as an input to the decoder. </p>
    <h1 id="_idParaDest-290" class="heading-1">Implementing the model with TensorFlow</h1>
    <p class="normal">We will now implement the model we just studied. First let’s import a few things:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_hub <span class="hljs-keyword">as</span> hub
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow.keras.backend <span class="hljs-keyword">as</span> K
</code></pre>
    <h2 id="_idParaDest-291" class="heading-2">Implementing the ViT model</h2>
    <p class="normal">Next, we are going<a id="_idIndexMarker1105"/> to download the pretrained ViT model <a id="_idIndexMarker1106"/>from TensorFlow Hub. We will be using a model submitted by Sayak Paul. The model is available at <a href="https://tfhub.dev/sayakpaul/vit_s16_fe/1"><span class="url">https://tfhub.dev/sayakpaul/vit_s16_fe/1</span></a>. You can see other Vision Transformer<a id="_idIndexMarker1107"/> models available at <a href="https://tfhub.dev/sayakpaul/collections/vision_transformer/1"><span class="url">https://tfhub.dev/sayakpaul/collections/vision_transformer/1</span></a>.</p>
    <pre class="programlisting code"><code class="hljs-code">image_encoder = hub.KerasLayer(<span class="hljs-string">"https://tfhub.dev/sayakpaul/vit_s16_fe/1"</span>, trainable=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">We then define an input layer to input images and pass that to the <code class="inlineCode">image_encoder</code> to get the final feature vector for that image:</p>
    <pre class="programlisting code"><code class="hljs-code">image_input = tf.keras.layers.Input(shape=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>))
image_features = image_encoder(image_input)
</code></pre>
    <p class="normal">You can look at the size of the final image representation by running:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">f"Final representation shape: </span><span class="hljs-subst">{image_features.shape}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">which will output:</p>
    <pre class="programlisting con"><code class="hljs-con">Final representation shape: (None, 384)
</code></pre>
    <p class="normal">Next we will look at the details<a id="_idIndexMarker1108"/> of how to implement<a id="_idIndexMarker1109"/> the text-based Transformer model, which will take in the image representation to generate the image caption.</p>
    <h2 id="_idParaDest-292" class="heading-2">Implementing the text-based decoder</h2>
    <p class="normal">Here we will implement a Transformer decoder model<a id="_idIndexMarker1110"/> from the ground up. This is different from how we used Transformer<a id="_idIndexMarker1111"/> models before, where we downloaded a pretrained model and used them. </p>
    <p class="normal">Before we implement the model itself, we are going to implement two custom Keras layers: one for the self-attention mechanism and the other one to capture the functionality of a single layer in the Transformer model. Let’s start with the self-attention layer.</p>
    <h3 id="_idParaDest-293" class="heading-3">Defining the self-attention layer</h3>
    <p class="normal">Here we define the self-attention layer<a id="_idIndexMarker1112"/> using the Keras subclassing API:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">SelfAttentionLayer</span>(tf.keras.layers.Layer):
    <span class="hljs-string">""" Defines the computations in the self attention layer """</span>
    
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, d</span>):        
        <span class="hljs-built_in">super</span>(SelfAttentionLayer, self).__init__()
        <span class="hljs-comment"># Feature dimensionality of the output</span>
        self.d = d
    
    <span class="hljs-keyword">def</span> <span class="hljs-title">build</span>(<span class="hljs-params">self, input_shape</span>):
        <span class="hljs-comment"># Query weight matrix</span>
        self.Wq = self.add_weight(
            shape=(input_shape[-<span class="hljs-number">1</span>], self.d), 
            initializer=<span class="hljs-string">'glorot_uniform'</span>,
            trainable=<span class="hljs-literal">True</span>, dtype=<span class="hljs-string">'float32'</span>
        )        
        <span class="hljs-comment"># Key weight matrix</span>
        self.Wk = self.add_weight(
            shape=(input_shape[-<span class="hljs-number">1</span>], self.d), 
            initializer=<span class="hljs-string">'glorot_uniform'</span>,
            trainable=<span class="hljs-literal">True</span>, dtype=<span class="hljs-string">'float32'</span>
        )
        <span class="hljs-comment"># Value weight matrix</span>
        self.Wv = self.add_weight(
            shape=(input_shape[-<span class="hljs-number">1</span>], self.d), 
            initializer=<span class="hljs-string">'glorot_uniform'</span>,
            trainable=<span class="hljs-literal">True</span>, dtype=<span class="hljs-string">'float32'</span>
        )
    
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, q_x, k_x, v_x, mask=</span><span class="hljs-literal">None</span>):
        
        q = tf.matmul(q_x,self.Wq) <span class="hljs-comment">#[None, t, d]</span>
        k = tf.matmul(k_x,self.Wk) <span class="hljs-comment">#[None, t, d]</span>
        v = tf.matmul(v_x,self.Wv) <span class="hljs-comment">#[None, t, d]</span>
        
        <span class="hljs-comment"># Computing the final output</span>
        h = tf.keras.layers.Attention(causal=<span class="hljs-literal">True</span>)([
            q, <span class="hljs-comment">#q</span>
            v, <span class="hljs-comment">#v</span>
            k, <span class="hljs-comment">#k</span>
        ], mask=[<span class="hljs-literal">None</span>, mask]) 
<span class="hljs-comment">        # [None, t, t] . [None, t, d] =&gt; [None, t, d]</span>
        <span class="hljs-keyword">return</span> h
</code></pre>
    <p class="normal">Here we have to populate<a id="_idIndexMarker1113"/> the logic for three functions:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">__init__()</code> and <code class="inlineCode">__build__()</code> – Define various hyperparameters and layer initialization specific logic</li>
      <li class="bulletList"><code class="inlineCode">call() </code>– Computations that need to happen when the layer is called </li>
    </ul>
    <p class="normal">You can see that we define the dimensionality of the attention output, <code class="inlineCode">d</code>, as an argument to the <code class="inlineCode">__init__()</code> method. Next in the <code class="inlineCode">__build__()</code> method, we define three weight matrices, <code class="inlineCode">Wq</code>, <code class="inlineCode">Wk</code>, and <code class="inlineCode">Wv</code>. If you remember our discussion from the previous chapter, these represent the weights of the query, key, and value respectively. </p>
    <p class="normal">Finally, in the <code class="inlineCode">call</code> method we have the logic. It takes four inputs: query, key, value inputs, and an optional mask for values. We then compute the latent <code class="inlineCode">q</code>, <code class="inlineCode">k</code>, and <code class="inlineCode">v</code> by multiplying with the corresponding weight matrices <code class="inlineCode">Wq</code>, <code class="inlineCode">Wk</code>, and <code class="inlineCode">Wv</code>. To compute attention, we will be using the out-of-the-box layer <code class="inlineCode">tf.keras.layers.Attention</code>. We used a similar layer to compute the Bahdanau attention mechanism in <em class="chapterRef">Chapter 9</em><em class="italic">, Sequence-to-Sequence Learning – Neural Machine Translation</em>. The <code class="inlineCode">tf.keras.layers.Attention()</code> layer has several arguments. One that we care about here is setting <code class="inlineCode">causal=True</code>. </p>
    <p class="normal">By doing this, we are instructing the layer to mask the tokens to the right of the current token. This essentially prevents the decoder from leaking information about future tokens. Next, the layer takes in the following arguments during the call:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">inputs</code> – A list of inputs containing the query, value, and key in that order</li>
      <li class="bulletList"><code class="inlineCode">mask</code> – A list of two items containing the masks for the query and value </li>
    </ul>
    <p class="normal">Finally it returns the output<a id="_idIndexMarker1114"/> of the attention layer <code class="inlineCode">h</code>. Next, we will implement the computations of a Transformer layer.</p>
    <h3 id="_idParaDest-294" class="heading-3">Defining the Transformer layer</h3>
    <p class="normal">With the self-attention layer, let’s capture<a id="_idIndexMarker1115"/> the computations of a single Transformer layer in the following class. It uses self-attention, fully connected layers, and other optimization techniques to compute the output:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">TransformerDecoderLayer</span>(tf.keras.layers.Layer):
    <span class="hljs-string">""" The Decoder layer """</span>
    
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, d, n_heads</span>):
        <span class="hljs-built_in">super</span>(TransformerDecoderLayer, self).__init__()
        <span class="hljs-comment"># Feature dimensionality</span>
        self.d = d
        
        <span class="hljs-comment"># Dimensionality of a head</span>
        self.d_head = <span class="hljs-built_in">int</span>(d/n_heads) 
        
        <span class="hljs-comment"># Number of heads</span>
        self.n_heads = n_heads
        
        <span class="hljs-comment"># Actual attention heads</span>
        self.attn_heads = [SelfAttentionLayer(self.d_head) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> 
        <span class="hljs-built_in">range</span>(self.n_heads)]
        
        <span class="hljs-comment"># Fully connected layers</span>
        self.fc1_layer = tf.keras.layers.Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'relu'</span>)
        self.fc2_layer = tf.keras.layers.Dense(d)
        
        self.add_layer = tf.keras.layers.Add()
        self.norm1_layer = tf.keras.layers.LayerNormalization()
        self.norm2_layer = tf.keras.layers.LayerNormalization()
        
    <span class="hljs-keyword">def</span> <span class="hljs-title">_compute_multihead_output</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-string">""" Computing the multi head attention output"""</span>
        outputs = [head(x, x, x) <span class="hljs-keyword">for</span> head <span class="hljs-keyword">in</span> self.attn_heads]
        outputs = tf.concat(outputs, axis=-<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> outputs
        
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, x</span>):
        
        
        <span class="hljs-comment"># Multi head attention layer output</span>
        h1 = self._compute_multihead_output(x)
        
        h1_add = self.add_layer([x, h1])
        h1_norm = self.norm1_layer(h1_add)
        
        <span class="hljs-comment"># Fully connected outputs</span>
        h2_1 = self.fc1_layer(h1_norm)
        h2_2 = self.fc2_layer(h2_1)
        
        h2_add = self.add_layer([h1, h2_2])
        h2_norm = self.norm2_layer(h2_add)
        
        
        <span class="hljs-keyword">return</span> h2_norm
</code></pre>
    <p class="normal">The <code class="inlineCode">TransformerDecoderLayer</code> performs the following steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Using the given<a id="_idIndexMarker1116"/> input, the layer computes a multi-head attention output. A multi-head attention output is generated by computing attention outputs with several smaller heads and concatenating those outputs to a single output (<code class="inlineCode">h1</code>).</li>
      <li class="numberedList">Next we add the original input <code class="inlineCode">x</code> to <code class="inlineCode">h1</code> to form a residual connection, (<code class="inlineCode">h1_add</code>).</li>
      <li class="numberedList">This is followed by a layer normalization step that normalizes (<code class="inlineCode">h1_norm</code>).</li>
      <li class="numberedList"><code class="inlineCode">h1_norm</code> goes through a fully connected layer to produce <code class="inlineCode">h2_1</code>.</li>
      <li class="numberedList"><code class="inlineCode">h2_1</code> goes through another fully connected layer to produce <code class="inlineCode">h2_2</code>.</li>
      <li class="numberedList">Then we create another residual connection by adding <code class="inlineCode">h1</code> and <code class="inlineCode">h2_2</code> to produce <code class="inlineCode">h2_add</code>.</li>
      <li class="numberedList">Finally we perform layer<a id="_idIndexMarker1117"/> normalization to produce <code class="inlineCode">h2_norm</code>, which is the final output of this custom layer.</li>
    </ol>
    <h3 id="_idParaDest-295" class="heading-3">Defining the full decoder</h3>
    <p class="normal">With all the utility<a id="_idIndexMarker1118"/> layers implemented, we can implement the text decoder. We will define two input layers. The first takes in a sequence of tokens as the input and the second takes in a sequence of positions (0-index based) to denote the position of each token. You can see that both layers are defined such that they can take in an arbitrary length sequence as an input. This will serve an important purpose as we will see later during inference:</p>
    <pre class="programlisting code"><code class="hljs-code">caption_input = tf.keras.layers.Input(shape=(<span class="hljs-literal">None</span>,))
position_input = tf.keras.layers.Input(shape=(<span class="hljs-literal">None</span>,))
</code></pre>
    <p class="normal">Next we define the embeddings. Our embedding vectors will have a length of 384 to match the ViT model’s output dimensionality. We defined two embedding layers: the token embedding layer and the positional embedding layer:</p>
    <pre class="programlisting code"><code class="hljs-code">d_model = <span class="hljs-number">384</span>
<span class="hljs-comment"># Token embeddings</span>
input_embedding = tf.keras.layers.Embedding(<span class="hljs-built_in">len</span>(tokenizer.get_vocab()), d_model, mask_zero=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">The token embedding layer works just as we have seen several times. It produces an embedding vector for each token in the sequence. We mask inputs with <code class="inlineCode">ID 0</code> as they represent the padded tokens. Next let’s understand how we can implement the positional embeddings:</p>
    <pre class="programlisting code"><code class="hljs-code">position_embedding = tf.keras.layers.Lambda(
    <span class="hljs-keyword">lambda</span> x: tf.where(
        tf.math.mod(tf.repeat(tf.expand_dims(x, axis=-<span class="hljs-number">1</span>), d_model, 
        axis=-<span class="hljs-number">1</span>), <span class="hljs-number">2</span>)==<span class="hljs-number">0</span>,
        tf.math.sin(
            tf.expand_dims(x, axis=-<span class="hljs-number">1</span>) /
            <span class="hljs-number">10000</span>**(<span class="hljs-number">2</span>*tf.reshape(tf.<span class="hljs-built_in">range</span>(d_model, 
<span class="hljs-number">            </span>dtype=<span class="hljs-string">'</span><span class="hljs-string">float32'</span>),[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>])/d_model)
        ),
        tf.math.cos(
            tf.expand_dims(x, axis=-<span class="hljs-number">1</span>) /
            <span class="hljs-number">10000</span>**(<span class="hljs-number">2</span>*tf.reshape(tf.<span class="hljs-built_in">range</span>(d_model, 
<span class="hljs-number">            </span>dtype=<span class="hljs-string">'float32'</span>),[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>])/d_model)
        )
    )
)
</code></pre>
    <p class="normal">We have already discussed how positional<a id="_idIndexMarker1119"/> embeddings are calculated. The original Transformer paper uses the following equations to generate positional embeddings:</p>
    <p class="center"><img src="../Images/B14070_11_001.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_11_002.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Here <em class="italic">pos</em> denotes the position in the sequence and <em class="italic">i</em> denotes the <em class="italic">i</em><sup class="superscript">th</sup> feature dimension (<code class="inlineCode">0&lt; i&lt;d_model</code>). Even-numbered features use a sine function, where odd-numbered features use a cosine function. Computing this as a layer requires some effort. Let’s slowly break down the logic. First we compute the following two tensors (let’s refer to them with <code class="inlineCode">x</code> and <code class="inlineCode">y</code> for ease):</p>
    <pre class="programlisting code"><code class="hljs-code">x = PE(pos, i) = sin(pos/<span class="hljs-number">10000</span>**(2i/d))
y = PE(pos, i) = cos(pos/<span class="hljs-number">10000</span>**(2i/d))
</code></pre>
    <p class="normal">We use the <code class="inlineCode">tf.where(cond, x, y)</code> function to select values element-wise from <code class="inlineCode">x</code> and <code class="inlineCode">y</code> using a Boolean matrix <code class="inlineCode">cond</code> of the same size. For a given position, if <code class="inlineCode">cond</code> is <code class="inlineCode">True</code>, select <code class="inlineCode">x</code>, and if <code class="inlineCode">cond</code> is <code class="inlineCode">False</code>, select <code class="inlineCode">y</code>. Here we use the condition as <code class="inlineCode">pos%2 == 0</code>, which provides <code class="inlineCode">True</code> for even positions and <code class="inlineCode">False</code> for odd positions.</p>
    <p class="normal">In order to make sure we produce tensors with correct shapes, we utilize the broadcasting capabilities of TensorFlow. </p>
    <p class="normal">Let’s understand a little bit how broadcasting has helped. Take the computation:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.math.sin(
            tf.expand_dims(x, axis=-<span class="hljs-number">1</span>) /
            <span class="hljs-number">10000</span>**(<span class="hljs-number">2</span>*tf.reshape(tf.<span class="hljs-built_in">range</span>(d_model, 
<span class="hljs-number">            </span>dtype=<span class="hljs-string">'float32'</span>),[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>])/d_model)
        )
</code></pre>
    <p class="normal">Here we need a <code class="inlineCode">[batch size, time steps, d_model]</code>-sized output. <code class="inlineCode">tf.expand_dims(x, axis=-1)</code> produces<a id="_idIndexMarker1120"/> a <code class="inlineCode">[batch size, time steps, 1]</code>-sized output. <code class="inlineCode">10000**(2*tf.reshape(tf.range(d_model, dtype='float32'),[1,1, -1])/d_model)</code> produces a <code class="inlineCode">[1, 1, d_model]</code>-sized output. Dividing the first output by the second provides us with a tensor of size <code class="inlineCode">[batch size, time steps, d_model]</code>. This is because the broadcasting capability of TensorFlow allows it to perform operations between arbitrary-sized dimensions and dimensions of size 1. You can imagine TensorFlow copying the dimension of size 1, n many times to perform an operation with an n-sized dimension. But in reality it does this more efficiently. </p>
    <p class="normal">Once the token and positional embeddings are computed. We add them element-wise to get the final embeddings:</p>
    <pre class="programlisting code"><code class="hljs-code">embed_out = input_embedding(caption_input) + position_embedding(position_input)
</code></pre>
    <p class="normal">If you remember, the first input to the decoder is the image feature vector followed by caption tokens. Therefore, we need to concatenate <code class="inlineCode">image_features</code> (produced by the ViT) with the <code class="inlineCode">embed_out</code> to get the full sequence of inputs:</p>
    <pre class="programlisting code"><code class="hljs-code">image_caption_embed_out = tf.keras.layers.Concatenate(axis=<span class="hljs-number">1</span>)([tf.expand_dims(image_features,axis=<span class="hljs-number">1</span>), embed_out])
</code></pre>
    <p class="normal">Then we define four Transformer decoder layers and compute the hidden output of those layers:</p>
    <pre class="programlisting code"><code class="hljs-code">out = image_caption_embed_out
<span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):
    out  = TransformerDecoderLayer(d_model, <span class="hljs-number">64</span>)(out)
</code></pre>
    <p class="normal">We use a <code class="inlineCode">Dense</code> layer having <code class="inlineCode">n_vocab</code> output nodes and a <em class="italic">softmax</em> activation to compute the final output:</p>
    <pre class="programlisting code"><code class="hljs-code">final_out = tf.keras.layers.Dense(n_vocab, activation=<span class="hljs-string">'softmax'</span>)(out)
</code></pre>
    <p class="normal">Finally, we define the full model. It takes in:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">image_input </code>– A batch of images of size 224x224x3</li>
      <li class="bulletList"><code class="inlineCode">caption_input </code>– The token IDs of the caption (except the last token)</li>
      <li class="bulletList"><code class="inlineCode">position_input </code>– A batch of position IDs representing each token position</li>
    </ul>
    <p class="normal">And gives <code class="inlineCode">final_out</code> as the output:</p>
    <pre class="programlisting con"><code class="hljs-con">full_model = tf.keras.models.Model(inputs=[image_input, caption_input, position_input], outputs=final_out)
full_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')
full_model.summary()
</code></pre>
    <p class="normal">Now we have defined<a id="_idIndexMarker1121"/> the full model (<em class="italic">Figure 11.8</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.8: Code references overlaid on the illustration of the full model</p>
    <h1 id="_idParaDest-296" class="heading-1">Training the model</h1>
    <p class="normal">Now that the data pipeline<a id="_idIndexMarker1122"/> and the model are defined, training it is quite easy. First let’s define a few parameters:</p>
    <pre class="programlisting code"><code class="hljs-code">n_vocab = <span class="hljs-number">4000</span>
batch_size=<span class="hljs-number">96</span>
train_fraction = <span class="hljs-number">0.6</span>
valid_fraction = <span class="hljs-number">0.2</span>
</code></pre>
    <p class="normal">We use a vocabulary <a id="_idIndexMarker1123"/>size of 4,000 and a batch size of 96. To speed up the training we’ll only use 60% of training data and 20% of validation data. However, you could increase these to get better results. Then we get the tokenizer trained on the full training dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">tokenizer = generate_tokenizer(
    train_captions_df, n_vocab=n_vocab
)
</code></pre>
    <p class="normal">Next we define the BLEU metric. This is the same BLEU computation from <em class="chapterRef">Chapter 9</em>, <em class="italic">Sequence-to-Sequence Learning – Neural Machine Translation</em>, with some minor differences. Therefore, we will not repeat the discussion here.</p>
    <pre class="programlisting code"><code class="hljs-code">bleu_metric = BLEUMetric(tokenizer=tokenizer)
</code></pre>
    <p class="normal">Sample the smaller set of validation data outside the training loop to keep the set constant:</p>
    <pre class="programlisting code"><code class="hljs-code">sampled_validation_captions_df = valid_captions_df.sample(frac=valid_fraction)
</code></pre>
    <p class="normal">Next we train the model for 5 epochs:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Epoch: </span><span class="hljs-subst">{e+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string">"</span>)
    
    train_dataset, _ = generate_tf_dataset(
        train_captions_df.sample(frac=train_fraction), 
        tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, 
        training=<span class="hljs-literal">True</span>
    )
    valid_dataset, _ = generate_tf_dataset(
        sampled_validation_captions_df, tokenizer=tokenizer, 
        n_vocab=n_vocab, batch_size=batch_size, training=<span class="hljs-literal">False</span>
    )
    
    full_model.fit(
        train_dataset,
        epochs=<span class="hljs-number">1</span>
    )
    
    valid_loss, valid_accuracy, valid_bleu = [], [], []
    <span class="hljs-keyword">for</span> vi, v_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_dataset):
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"</span><span class="hljs-subst">{vi+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> batches processed"</span>, end=<span class="hljs-string">'\r'</span>)
        loss, accuracy = full_model.test_on_batch(v_batch[<span class="hljs-number">0</span>], 
        v_batch[<span class="hljs-number">1</span>])
        batch_predicted = full_model(v_batch[<span class="hljs-number">0</span>])
        bleu_score = 
        bleu_metric.calculate_bleu_from_predictions(v_batch[<span class="hljs-number">1</span>], 
        batch_predicted)
        valid_loss.append(loss)
        valid_accuracy.append(accuracy)
        valid_bleu.append(bleu_score)
        
    <span class="hljs-built_in">print</span>(
        <span class="hljs-string">f"\nvalid_loss: </span><span class="hljs-subst">{np.mean(valid_loss)}</span><span class="hljs-string"> - valid_accuracy: </span>
<span class="hljs-string">        </span><span class="hljs-subst">{np.mean(valid_accuracy)}</span><span class="hljs-string"> - valid_bleu: </span><span class="hljs-subst">{np.mean(valid_bleu)}</span><span class="hljs-string">"</span>
    )
</code></pre>
    <p class="normal">In each iteration, we generate a <code class="inlineCode">train_dataset</code> and a <code class="inlineCode">valid_dataset</code>. Note that the training set is sampled randomly<a id="_idIndexMarker1124"/> in each epoch, resulting in different data points, while the validation set is constant. Also note that we are passing the previously generated tokenizer as an argument to the data pipeline function. We call the <code class="inlineCode">full_model.fit()</code> function with the train dataset to train it for a single epoch within the loop. Finally we iterate through the batches of the validation dataset and compute loss, accuracy, and BLEU values for each batch. Then we print out the mean value of those batch metrics. The output looks like below:</p>
    <pre class="programlisting con"><code class="hljs-con">Epoch: 1
2071/2071 [==============================] - 1945s 903ms/step - loss: 1.3344 - accuracy: 0.7625
173 batches processed
valid_loss: 1.1388846477332142 - valid_accuracy: 0.7819634135058849 - valid_bleu: 0.09385878526196685
Epoch: 2
2071/2071 [==============================] - 1854s 894ms/step - loss: 1.0860 - accuracy: 0.7878
173 batches processed
valid_loss: 1.090059520192229 - valid_accuracy: 0.7879036186058397 - valid_bleu: 0.10231472779803133
Epoch: 3
2071/2071 [==============================] - 1855s 895ms/step - loss: 
1.0610 - accuracy: 0.7897
173 batches processed
valid_loss: 1.0627685799075 - valid_accuracy: 0.7899546606003205 - valid_bleu: 0.10398145099074609
Epoch: 4
2071/2071 [==============================] - 1937s 935ms/step - loss: 1.0479 - accuracy: 0.7910
173 batches processed
valid_loss: 1.0817485169179177 - valid_accuracy: 0.7879597275932401 - valid_bleu: 0.10308500219058511
Epoch: 5
2071/2071 [==============================] - 1864s 899ms/step - loss: 1.0244 - accuracy: 0.7937
173 batches processed
valid_loss: 1.0498641329693656 - valid_accuracy: 0.79208166544148 - valid_bleu: 0.10667336005789202
</code></pre>
    <p class="normal">Let’s go through the results. We can see<a id="_idIndexMarker1125"/> that the training loss and validation losses have more or less gone down consistently. We have a training and validation accuracy of ~80%. Finally, the <code class="inlineCode">valid_bleu</code> score is around 0.10. You can see the state<a id="_idIndexMarker1126"/> of the art for a few models here: <a href="https://paperswithcode.com/sota/image-captioning-on-coco"><span class="url">https://paperswithcode.com/sota/image-captioning-on-coco</span></a>. You can see that a BLEU-4 score of 39 has been reached by the UNIMO model. It is important to note that, in reality, our BLEU score is higher than what’s reported here. This is because each image has multiple captions. And when computing the BLEU score with multiple references, you compute BLEU for each and take the max. We have only considered one caption per image when computing the BLEU score. Additionally, our model was far less complicated and trained on a small fraction of the data available. If you would like to increase model performance, you can expose the full training set, and experiment with larger ViT models and data augmentation techniques to improve performance. </p>
    <p class="normal">Next let’s discuss some of the different metrics used to measure the quality of sequences in the context of image captioning.</p>
    <h1 id="_idParaDest-297" class="heading-1">Evaluating the results quantitatively</h1>
    <p class="normal">There are many different techniques<a id="_idIndexMarker1127"/> for evaluating the quality and the relevancy of the captions generated. We will briefly discuss several such metrics we can use to evaluate the captions. We will discuss four metrics: BLEU, ROGUE, METEOR, and CIDEr. </p>
    <p class="normal">All these measures share a key objective, to measure the adequacy (the meaning of the generated text) and fluency (the grammatical correctness of text) of the generated<a id="_idIndexMarker1128"/> text. To calculate all these measures, we will use a candidate sentence and a reference sentence, where a candidate sentence is the sentence/phrase predicted by our algorithm and the reference sentence is the true sentence/phrase we want to compare with.</p>
    <h2 id="_idParaDest-298" class="heading-2">BLEU</h2>
    <p class="normal"><strong class="keyWord">Bilingual Evaluation Understudy</strong> (<strong class="keyWord">BLEU</strong>) was proposed by Papineni<a id="_idIndexMarker1129"/> and others in <em class="italic">BLEU: A Method for Automatic Evaluation of Machine Translation</em>, <em class="italic">Proceedings of the 40</em><sup class="superscript">th</sup> <em class="italic">Annual Meeting of the Association for Computational Linguistics (ACL)</em>, <em class="italic">Philadelphia</em>, <em class="italic">July (2002): 311-318</em>. It measures the n-gram similarity between reference and candidate phrases, in a position-independent manner. This means that a given n-gram from the candidate is present anywhere in the reference sentence and is considered to be a match. BLEU calculates the n-gram similarity in terms of precision. BLEU comes in several variations (BLEU-1, BLEU-2, BLEU-3, and so on), denoting the value of <em class="italic">n</em> in the n-gram. </p>
    <p class="center"><img src="../Images/B14070_11_003.png" alt="" style="height: 2.70em !important;"/></p>
    <p class="normal">Here, <em class="italic">Count(n-gram)</em> is the number of total occurrences of a given n-gram in the candidate sentence. <em class="italic">Count</em><sub class="subscript">clip</sub> <em class="italic">(n-gram)</em> is a measure that calculates <em class="italic">Count(n-gram)</em> for a given n-gram and clips that value by a maximum value. The maximum value for an n-gram is calculated as the number of occurrences of that n-gram in the reference sentence. For example, consider these two sentences:</p>
    <ul>
      <li class="bulletList">Candidate: <strong class="keyWord">the</strong> the the the the the the</li>
      <li class="bulletList">Reference: <strong class="keyWord">the</strong> cat sat on <strong class="keyWord">the</strong> mat
    <p class="normal"><em class="italic">Count(“the”) = 7</em></p>
    <p class="normal"><em class="italic">Count </em><sub class="subscript">clip</sub><em class="italic"> (“the”)=2</em></p></li>
    </ul>
    <p class="normal">Note that the entity, <img src="../Images/B14070_11_004.png" alt="" style="height: 2.20em !important; vertical-align: -0.81em !important;"/>, is a form of precision. In fact, it is called the modified n-gram precision. When multiple references are present, the BLEU is considered to be the maximum: </p>
    <p class="center"><img src="../Images/B14070_11_005.png" alt="" style="height: 1.35em !important;"/></p>
    <p class="normal">However, the modified n-gram precision tends to be higher for smaller candidate phrases because this entity is divided by the number of n-grams in the candidate phrase. This means that this measure will incline the model to produce shorter phrases. To avoid this, a penalty term, <em class="italic">BP</em>, is added to the preceding term that penalizes short candidate phrases as well. BLEU possesses several limitations such as BLEU ignores synonyms when calculating the score and does not consider recall, which is also an important metric to measure<a id="_idIndexMarker1130"/> accuracy. Furthermore, BLEU appears to be a poor choice for certain languages. However, this is a simple metric that has been found to correlate well with human judgment as well in most situations. </p>
    <h2 id="_idParaDest-299" class="heading-2">ROUGE</h2>
    <p class="normal"><strong class="keyWord">Recall-Oriented Understudy for Gisting Evaluations</strong> (<strong class="keyWord">ROUGE</strong>), proposed by Chin-Yew Lin in <em class="italic">ROUGE: A Package for Automatic Evaluation of Summaries</em>, <em class="italic">Proceedings of the Workshop on Text Summarization Branches Out (2004)</em>, can be identified<a id="_idIndexMarker1131"/> as a variant of BLEU, and uses recall as the basic performance metric. The ROUGE metric looks like the following: </p>
    <p class="center"><img src="../Images/B14070_11_006.png" alt="" style="height: 2.60em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_11_007.png" alt="" style="height: 1.15em !important; vertical-align: -0.32em !important;"/> is the number of n-grams from candidates that were present in the reference, and <img src="../Images/B14070_11_008.png" alt="" style="height: 1.35em !important; vertical-align: -0.35em !important;"/> is the total n-grams present in the reference. If there exist multiple references, <em class="italic">ROUGE-N</em> is calculated as follows: </p>
    <p class="center"><img src="../Images/B14070_11_009.png" alt="" style="height: 1.35em !important;"/></p>
    <p class="normal">Here, <em class="italic">ref</em><sub class="subscript">i</sub> is a single reference from the pool of available references. There are numerous variants of the ROUGE measure that introduce various improvements to the standard ROUGE metric. ROUGE-L computes the score based on the longest common subsequence found between the candidate and reference sentence pairs. Note that the longest common subsequence does not need to be continuous in this case. Next, ROUGE-W calculates the score based on the longest common subsequence, which is penalized by the amount of fragmentation present within the subsequence. ROUGE also suffers from limitations<a id="_idIndexMarker1132"/> such as not considering precision in the calculations of the score.</p>
    <h2 id="_idParaDest-300" class="heading-2">METEOR</h2>
    <p class="normal"><strong class="keyWord">Metric for Evaluation of Translation with Explicit Ordering</strong> (<strong class="keyWord">METEOR</strong>), proposed by Michael Denkowski<a id="_idIndexMarker1133"/> and Alon Lavie in <em class="italic">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</em>, <em class="italic">Proceedings of the Ninth Workshop on Statistical Machine Translation (2014): 376-380</em>, is a more advanced evaluation metric that performs alignments for a candidate and a reference sentence. METEOR is different from BLEU and ROUGE in the sense that METEOR takes the position of words into account. When computing similarities between a candidate sentence and a reference sentence, the following cases are considered as matches:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Exact</strong>: The word from the candidate exactly matches the word from the reference sentence</li>
      <li class="bulletList"><strong class="keyWord">Stem</strong>: A stemmed word (for example, <em class="italic">walk</em> of the word <em class="italic">walked</em>) matches the word from the reference sentence</li>
      <li class="bulletList"><strong class="keyWord">Synonym</strong>: The word from a candidate sentence is a synonym for the word from the reference sentence</li>
    </ul>
    <p class="normal">To calculate the METEOR score, the matches between a reference sentence and a candidate sentence can be shown, as in <em class="italic">Figure 11.10</em>, with the help of a table. Then, precision (<em class="italic">P</em>) and recall (<em class="italic">R</em>) values are calculated based on the number of matches, present in the candidate and reference sentences. Finally, the harmonic mean of <em class="italic">P</em> and <em class="italic">R</em> is used to compute the METEOR score: </p>
    <p class="center"><img src="../Images/B14070_11_010.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_11_015.png" alt="" style="height: 1.05em !important; vertical-align: -0.09em !important;"/>, <img src="../Images/B14070_11_016.png" alt="" style="height: 1.15em !important; vertical-align: -0.14em !important;"/>, and <img src="../Images/B14070_11_017.png" alt="" style="height: 1.15em !important; vertical-align: -0.13em !important;"/>are tunable parameters, and <em class="italic">frag</em> penalizes fragmented matches, in order to prefer candidate sentences that have fewer gaps in matches as well as those that closely follow the order of words of the reference sentence. The <em class="italic">frag</em> is calculated by looking at the number of crosses in the final unigram mapping (<em class="italic">Figure 11.9</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_09.png" alt="METEOR"/></figure>
    <p class="packt_figref">Figure 11.9: Different possible alignments for two strings</p>
    <p class="normal">For example, we can see<a id="_idIndexMarker1134"/> that the left side has 7 crosses, whereas the right side has 10 crosses, which means the right-side alignment will be more penalized than the left side.</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_10.png" alt="METEOR"/></figure>
    <p class="packt_figref">Figure 11.10: The METEOR word matching table</p>
    <p class="normal">You can see that we denoted matches between the candidate sentence and the reference sentence in circles<a id="_idIndexMarker1135"/> and ovals. For example, we denote exact matches with a solid black circle, synonyms with a dashed hollow circle, and stemmed matches with dotted circles.</p>
    <p class="normal">METEOR is computationally more complex, but has often been found to correlate with human judgment more than BLEU, suggesting that METEOR is a better evaluation metric than BLEU.</p>
    <h2 id="_idParaDest-301" class="heading-2">CIDEr</h2>
    <p class="normal"><strong class="keyWord">Consensus-based Image Description Evaluation</strong> (<strong class="keyWord">CIDEr</strong>), proposed by Ramakrishna Vedantam and others in <em class="italic">CIDEr: Consensus-based Image Description Evaluation</em>, <em class="italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, <em class="italic">2015</em>, is another measure that evaluates<a id="_idIndexMarker1136"/> the consensus of a candidate sentence to a given set of reference statements. CIDEr is defined to measure the grammaticality, saliency, and accuracy (that is, precision and recall) of a candidate sentence.</p>
    <p class="normal">First, CIDEr weighs each n-gram found in both the candidate and reference sentences by means of TF-IDF, so that more common n-grams (for example, the words <em class="italic">a</em> and <em class="italic">the</em>) will have a smaller weight, whereas rare words will have a higher weight. Finally, CIDEr is calculated as the cosine similarity between the vectors formed by TF-IDF-weighted n-grams found in the candidate sentence and the reference sentence: </p>
    <p class="center"><img src="../Images/B14070_11_011.png" alt="" style="height: 2.92em !important;"/></p>
    <p class="normal">Here, <em class="italic">cand</em> is the candidate sentence, <em class="italic">ref</em> is the set of reference sentences, <em class="italic">ref</em><sub class="subscript">j</sub> is the <em class="italic">j</em><sup class="superscript">th</sup> sentence of <em class="italic">ref</em>, and <em class="italic">m</em> is the number of reference sentences for a given candidate. Most importantly, <img src="../Images/B14070_11_012.png" alt="" style="height: 1.15em !important; vertical-align: -0.24em !important;"/> is the TF-IDF values calculated for all the n-grams in the candidate sentence and formed as a vector. <img src="../Images/B14070_11_013.png" alt="" style="height: 1.45em !important; vertical-align: -0.36em !important;"/> is the same vector for the reference sentence, <em class="italic">ref</em><sub class="subscript">i</sub>. <img src="../Images/B14070_11_014.png" alt="" style="height: 1.15em !important; vertical-align: -0.32em !important;"/> denotes the magnitude of the vector.</p>
    <p class="normal">Overall, it should be noted that there is no clear-cut winner that is able to perform well across all the different tasks that are found in NLP. These metrics are significantly task-dependent and should be carefully chosen depending on the task. Here we’ll be using the BLEU score for our model.</p>
    <h1 id="_idParaDest-302" class="heading-1">Evaluating the model</h1>
    <p class="normal">With the model trained, let’s test the model<a id="_idIndexMarker1137"/> on our unseen test dataset. Testing logic is almost identical to the validation logic we discussed earlier during model training. Therefore we will not repeat our discussion here.</p>
    <pre class="programlisting code"><code class="hljs-code">bleu_metric = BLEUMetric(tokenizer=tokenizer)
test_dataset, _ = generate_tf_dataset(
    test_captions_df, tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, training=<span class="hljs-literal">False</span>
)
test_loss, test_accuracy, test_bleu = [], [], []
<span class="hljs-keyword">for</span> ti, t_batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(test_dataset):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"</span><span class="hljs-subst">{ti+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> batches processed"</span>, end=<span class="hljs-string">'\r'</span>)
    loss, accuracy = full_model.test_on_batch(t_batch[<span class="hljs-number">0</span>], t_batch[<span class="hljs-number">1</span>])
    batch_predicted = full_model.predict_on_batch(t_batch[<span class="hljs-number">0</span>])
    bleu_score = bleu_metric.calculate_bleu_from_predictions(t_batch[<span class="hljs-number">1</span>], batch_predicted)
    test_loss.append(loss)
    test_accuracy.append(accuracy)
    test_bleu.append(bleu_score)
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">f"\ntest_loss: </span><span class="hljs-subst">{np.mean(test_loss)}</span><span class="hljs-string"> - test_accuracy: </span><span class="hljs-subst">{np.mean(test_accuracy)}</span><span class="hljs-string"> - test_bleu: </span><span class="hljs-subst">{np.mean(test_bleu)}</span><span class="hljs-string">"</span>
)
</code></pre>
    <p class="normal">This will output:</p>
    <pre class="programlisting con"><code class="hljs-con">261 batches processed
test_loss: 1.057080413646625 - test_accuracy: 0.7914185857407434 - test_bleu: 0.10505496256163914
</code></pre>
    <p class="normal">Great, we can see the model<a id="_idIndexMarker1138"/> is showing a similar performance to what it did on the validation data. This means our model has not overfitted data, and should perform reasonably well in the real world. Let’s now generate captions for a few sample images.</p>
    <h1 id="_idParaDest-303" class="heading-1">Captions generated for test images</h1>
    <p class="normal">With the help of metrics<a id="_idIndexMarker1139"/> such as accuracy and BLEU, we have ensured<a id="_idIndexMarker1140"/> our model is performing well. But, one of the most important tasks a trained model has to perform is generating outputs for new data. We will learn how we can use our model to generate actual captions. Let’s first understand how we can generate captions at a conceptual level. It’s quite straightforward to generate the image representation using an image. The tricky part is adapting the text decoder to generate captions. As you can imagine, the decoder inference needs to work in a different setting than the training. This is because at inference we don’t have caption tokens to input to the model. </p>
    <p class="normal">The way we predict<a id="_idIndexMarker1141"/> with our model<a id="_idIndexMarker1142"/> is by starting with the image and a starting caption that has the single token <code class="inlineCode">[START]</code>. We feed these two inputs to the model to generate the next token. We then combine the new token with the current input and predict the next token. We keep going this way until we reach a certain number of steps or the model outputs <code class="inlineCode">[END]</code> (<em class="italic">Figure 11.11</em>). If you remember, we developed the model in a way that it can accept an arbitrary length token sequence. This is extremely helpful during inference as at each time step, the length of the sequence increases.</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 11.11: How the decoder of the trained model is used to generate a new caption for a given image</p>
    <p class="normal">We will choose a small dataset of 10 samples from the test dataset and generate captions:</p>
    <pre class="programlisting code"><code class="hljs-code">n_samples = <span class="hljs-number">10</span>
test_dataset, _ = generate_tf_dataset(
    test_captions_df.sample(n=n_samples), tokenizer=tokenizer, 
    n_vocab=n_vocab, batch_size=n_samples, training=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">Next let’s define<a id="_idIndexMarker1143"/> a function<a id="_idIndexMarker1144"/> called <code class="inlineCode">generate_captions()</code>. This function takes in:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">model</code> – The trained model</li>
      <li class="bulletList"><code class="inlineCode">image_input</code> – A batch of input images</li>
      <li class="bulletList"><code class="inlineCode">tokenizer</code> – Trained tokenizer</li>
      <li class="bulletList"><code class="inlineCode">n_samples</code> – Number of samples in the batch</li>
    </ul>
    <p class="normal">As we can see in the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_caption</span>(<span class="hljs-params">model, image_input, tokenizer, n_samples</span>):
    <span class="hljs-comment"># 2 -&gt; [START]</span>
    batch_tokens = np.repeat(np.array([[<span class="hljs-number">2</span>]]), n_samples, axis=<span class="hljs-number">0</span>)
    
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30</span>):
        <span class="hljs-keyword">if</span> np.<span class="hljs-built_in">all</span>(batch_tokens[:,-<span class="hljs-number">1</span>] == <span class="hljs-number">3</span>):
            <span class="hljs-keyword">break</span>
            
        position_input = tf.repeat(tf.reshape(tf.<span class="hljs-built_in">range</span>(i+<span class="hljs-number">1</span>),[<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>]), 
        n_samples, axis=<span class="hljs-number">0</span>)
        probs = full_model((image_input, batch_tokens, 
        position_input)).numpy()
        batch_tokens = np.argmax(probs, axis=-<span class="hljs-number">1</span>)
    
    predicted_text = []
    <span class="hljs-keyword">for</span> sample_tokens <span class="hljs-keyword">in</span> batch_tokens:
        sample_predicted_token_ids = sample_tokens.ravel()
        sample_predicted_tokens = []
        <span class="hljs-keyword">for</span> wid <span class="hljs-keyword">in</span> sample_predicted_token_ids:
            sample_predicted_tokens.append(tokenizer.id_to_token(wid))
            <span class="hljs-keyword">if</span> wid == <span class="hljs-number">3</span>:
                <span class="hljs-keyword">break</span>
        sample_predicted_text = <span class="hljs-string">" "</span>.join([tok <span class="hljs-keyword">for</span> tok <span class="hljs-keyword">in</span> 
        sample_predicted_tokens])
        sample_predicted_text = sample_predicted_text.replace(<span class="hljs-string">" ##"</span>, 
        <span class="hljs-string">""</span>)
        predicted_text.append(sample_predicted_text)
    
    <span class="hljs-keyword">return</span> predicted_text
</code></pre>
    <p class="normal">This function starts with a single caption token ID. The ID 2 maps to the token <code class="inlineCode">[START]</code>. We predict for 30 steps or if the last token is <code class="inlineCode">[END]</code> (mapped to token ID 3). We generate position inputs for the batch of data by creating a range sequence from 0 to <em class="italic">i</em> and repeating that <code class="inlineCode">n_sample</code> times across the batch dimension. We then predict the token probabilities by feeding the inputs to the model.</p>
    <p class="normal">We can now use<a id="_idIndexMarker1145"/> this function<a id="_idIndexMarker1146"/> to generate captions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> test_dataset.take(<span class="hljs-number">1</span>):
    (batch_image_input, _, _), batch_true_caption = batch
batch_predicted_text = generate_caption(full_model, batch_image_input, tokenizer, n_samples)
</code></pre>
    <p class="normal">Let’s now visualize the captions side by side with the image inputs. Additionally, we’ll show the ground truth captions: </p>
    <pre class="programlisting code"><code class="hljs-code">fig, axes = plt.subplots(n_samples, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">30</span>))
<span class="hljs-keyword">for</span> i,(sample_image_input, sample_true_caption, sample_predicated_caption) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(batch_image_input, batch_true_caption, batch_predicted_text)):
    
    sample_true_caption_tokens  = [tokenizer.id_to_token(wid) <span class="hljs-keyword">for</span> wid <span class="hljs-keyword">in</span>
<span class="hljs-keyword">   </span> sample_true_caption.numpy().ravel()]
    
    sample_true_text = []
    <span class="hljs-keyword">for</span> tok <span class="hljs-keyword">in</span> sample_true_caption_tokens:
        sample_true_text.append(tok)
        <span class="hljs-keyword">if</span> tok == <span class="hljs-string">'[END]'</span>:
            <span class="hljs-keyword">break</span>
    
    sample_true_text = <span class="hljs-string">" "</span>.join(sample_true_text).replace(<span class="hljs-string">" ##"</span>, <span class="hljs-string">""</span>)
    axes[i][<span class="hljs-number">0</span>].imshow(((sample_image_input.numpy()+<span class="hljs-number">1.0</span>)/<span class="hljs-number">2.0</span>))
    axes[i][<span class="hljs-number">0</span>].axis(<span class="hljs-string">'</span><span class="hljs-string">off'</span>)
    
    true_annotation = <span class="hljs-string">f"TRUE: </span><span class="hljs-subst">{sample_true_text}</span><span class="hljs-string">"</span>
    predicted_annotation = <span class="hljs-string">f"PRED: </span><span class="hljs-subst">{sample_predicated_caption}</span><span class="hljs-string">"</span>
    axes[i][<span class="hljs-number">1</span>].text(<span class="hljs-number">0</span>, <span class="hljs-number">0.75</span>, true_annotation, fontsize=<span class="hljs-number">18</span>)
    axes[i][<span class="hljs-number">1</span>].text(<span class="hljs-number">0</span>, <span class="hljs-number">0.25</span>, predicted_annotation, fontsize=<span class="hljs-number">18</span>)
    axes[i][<span class="hljs-number">1</span>].axis(<span class="hljs-string">'off'</span>)
</code></pre>
    <p class="normal">You will get a plot similar to the following. The images <em class="italic">sampled</em> will be randomly sampled every time it runs. The results of this run can be seen in <em class="italic">Figure 11.12</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_11_12.png" alt=""/> </figure>
    <p class="packt_figref">Figure 11.12: Captions generated on a sample of test data</p>
    <p class="normal">We can see that our model<a id="_idIndexMarker1147"/> does a good job of generating<a id="_idIndexMarker1148"/> captions. In general, we can see that the model can identify objects and activities portrayed in the images. It is also important to remember that each of our images has multiple captions associated<a id="_idIndexMarker1149"/> with it. Therefore, the predicted <a id="_idIndexMarker1150"/>captions do not necessarily need to match the ground truth caption shown in the image. </p>
    <h1 id="_idParaDest-304" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we focused on a very interesting task that involves generating captions for given images. Our image-captioning model was one of the most complex models in this book, which included the following:</p>
    <ul>
      <li class="bulletList">A vision Transformer model that produces an image representation</li>
      <li class="bulletList">A text-based Transformer decoder </li>
    </ul>
    <p class="normal">Before we began with the model, we analyzed our dataset to understand various characteristics such as image sizes and the vocabulary size. Then we understood how we can use a tokenizer to tokenize captions strings. We then used this knowledge to build a TensorFlow data pipeline.</p>
    <p class="normal">We discussed each component in detail. The Vision Transformer (ViT) takes in an image and produces a hidden representation of that image. Specifically, the ViT breaks an image into a sequence of 16x16 patches of pixels. After that, it treats each patch as a token embedding to the Transformer (along with positional information) to produce a representation of each patch. It also incorporates the [CLS] token at the beginning to provide a holistic representation of the image. </p>
    <p class="normal">Next the text decoder takes in the image representation along with caption tokens as inputs. The objective of the decoder becomes to predict the next token at each time step. We were able to reach a BLEU-4 score of just above 0.10 for the validation dataset.</p>
    <p class="normal">Thereafter, we discussed several different metrics (BLEU, ROUGE, METEOR, and CIDEr), which we can use to quantitatively evaluate the generated captions, and we saw that as we ran our algorithm through the training data, the BLEU-4 score increased over time. Additionally, we visually inspected the generated captions and saw that our ML pipeline progressively gets better at captioning images.</p>
    <p class="normal">Next, we evaluated our model on the test dataset and validated that it demonstrates similar performance on test data as expected. Finally, we learned how we can use the trained model to generate captions for unseen images. </p>
    <p class="normal">We have reached the end of the book. We have covered many different topics in natural language and discussed state-of-the-art models and techniques that help us to solve problems. </p>
    <p class="normal">In the Appendix, we will discuss some mathematical concepts related to machine learning, followed by an explanation of how to use the visualization tool TensorBoard to visualize word vectors.</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="Chapter_11.xhtml"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="Chapter_11.xhtml"><span class="url">https://packt.link/nlp</span></a></p>
    <p class="center"><span class="url"><img src="../Images/QR_Code5143653472357468031.png" alt=""/></span></p>
    <figure class="mediaobject"> </figure>
  </div>
</body></html>