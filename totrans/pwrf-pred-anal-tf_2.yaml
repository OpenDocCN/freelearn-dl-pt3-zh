- en: Chapter 2. Putting Data in Place – Supervised Learning for Predictive Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this lesson, we will discuss supervised learning from the theoretical and
    practical perspective. In particular, we will revisit the linear regression model
    for regression analysis discussed in [Lesson 1](ch01.html "Chapter 1. From Data
    to Decisions – Getting Started with TensorFlow"), *From Data to Decisions – Getting
    Started with TensorFlow*, using a real dataset. Then we will see how to develop
    Titanic survival predictive models using **Logistic Regression** (**LR**), Random
    Forests, and **Support Vector Machines** (**SVMs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this lesson:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning for predictive analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear regression for predictive analytics: revisited'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression for predictive analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests for predictive analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVMs for predictive analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparative analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised Learning for Predictive Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on the nature of the learning feedback available, the machine learning
    process is typically classified into three broad categories: supervised learning,
    unsupervised learning, and reinforcement learning—see figure 1\. A predictive
    model based on supervised learning algorithms can make predictions based on a
    labelled dataset that map inputs to outputs aligning with the real world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a dataset for spam filtering usually contains spam messages as
    well as not-spam messages. Therefore, we could know which messages in the training
    set are spam and which are ham. Nevertheless, we might have the opportunity to
    use this information to train our model in order to classify new unseen messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised Learning for Predictive Analytics](img/02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Machine learning tasks (containing a few algorithms only)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the schematic diagram of supervised learning. After
    the algorithm has found the required patterns, those patterns can be used to make
    predictions for unlabeled test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised Learning for Predictive Analytics](img/02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Supervised learning in action'
  prefs: []
  type: TYPE_NORMAL
- en: Examples include classification and regression for solving supervised learning
    problems so that predictive models can be built for predictive analytics based
    on them. We will provide several examples of supervised learning like linear regression,
    logistic regression, random forest, decision trees, Naive Bayes, multilayer perceptron,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this lesson, we will mainly focus on the supervised learning algorithms for
    predictive analytics. Let's start from the very simple linear regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression – Revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Lesson 1](ch01.html "Chapter 1. From Data to Decisions – Getting Started
    with TensorFlow"), *From Data to Decisions – Getting Started with TensorFlow*
    we have seen an example of linear regression. We have observed how to work TensorFlow
    on the randomly generated dataset, that is, fake data. We have seen that the regression
    is a type of supervised machine learning for predicting the continuous-valued
    output. However, running a linear regression on fake data is just like buying
    a new car and never driving it. This awesome machinery begs to manifest itself
    in the real world!
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, many datasets are available online to test your new-found knowledge
    of regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The University of Massachusetts Amherst supplies small datasets of various
    types: [http://www.umass.edu/statdata/statdata/](http://www.umass.edu/statdata/statdata/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaggle contains all types of large-scale data for machine learning competitions:
    [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data.gov is an open data initiative by the US government, which contains many
    interesting and practical datasets: [https://catalog.data.gov](https://catalog.data.gov)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, in this section, by defining a set of models, we will see how to
    reduce the search space of possible functions. Moreover, TensorFlow takes advantage
    of the differentiable property of the functions by running its efficient gradient
    descent optimizers to learn the parameters. To avoid overfitting our data, we
    regularize the cost function by penalizing larger valued parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression is shown in [Lesson 1](ch01.html "Chapter 1. From Data
    to Decisions – Getting Started with TensorFlow"), *From Data to Decision – Getting
    Started with TensorFlow*, shows some tensors that just contained a single scalar
    value, but you can, of course, perform computations on arrays of any shape. In
    TensorFlow, operations such as addition and multiplication take two inputs and
    produce an output. In contrast, constants and variables do not take any input.
    We will also see an example of how TensorFlow can manipulate 2D arrays to perform
    linear regression like operations.
  prefs: []
  type: TYPE_NORMAL
- en: Problem Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Online movie ratings and recommendations have become a serious business around
    the world. For example, Hollywood generates about $10 billion at the U.S. box
    office each year. Websites like Rotten Tomatoes aggregates movie reviews into
    one overall rating and also reports poor opening weekends. Although a single movie
    critic or a single negative review can't make or break a film, thousands of reviews
    and critics do.
  prefs: []
  type: TYPE_NORMAL
- en: Rotten Tomatoes, Metacritic, and IMDb have their own way of aggregating film
    reviews and distinct rating systems. On the other hand, Fandango, an NBCUniversal
    subsidiary uses a five-star rating system in which most of the movies get at least
    three stars, according to a FiveThirtyEight analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'An exploratory analysis of the dataset used by Fandango shows that out of 510
    films, 437 films got at least one review where, hilariously, 98% had a 3-star
    rating or higher and 75 percent had a 4-star rating or higher. This implies, that
    using Fandango''s standards it''s almost impossible for a movie to be a flop at
    the box office. Therefore, Fandango''s rating is biased and skewed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Problem Statement](img/02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Fandango''s lopsided ratings curve'
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: [https://fivethirtyeight.com/features/fandango-movies-ratings/](https://fivethirtyeight.com/features/fandango-movies-ratings/))'
  prefs: []
  type: TYPE_NORMAL
- en: Since the ratings from Fandango are unreliable, we will instead predict our
    own ratings based on IMDb ratings. More specifically, this is a multivariate regression
    problem, since our predictive model will use multiple features to make the rating
    prediction having many predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the data is small enough to fit in memory, so plain batch learning
    should do just fine. Considering these factors and need, we will see that linear
    regression will meet our requirements. However, for more robust regression, you
    can still use deep neural network based regression techniques such as deep belief
    networks Regressor.
  prefs: []
  type: TYPE_NORMAL
- en: Using Linear Regression for Movie Rating Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, the first task is downloading the Fandango's rating dataset from GitHub
    at [https://github.com/fivethirtyeight/data/tree/master/fandango](https://github.com/fivethirtyeight/data/tree/master/fandango).
    It contains every film that has a Rotten Tomatoes rating, an RT user rating, a
    Metacritic score, a Metacritic user score, IMDb score, and at least 30 fan reviews
    on Fandango.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Description of the columns in fandango_score_comparison.csv'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset has 22 columns that can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `FILM` | Name of the film. |'
  prefs: []
  type: TYPE_TB
- en: '| `RottenTomatoes` | Corresponding Tomatometer score for the film by Rotten
    Tomatoes. |'
  prefs: []
  type: TYPE_TB
- en: '| `RottenTomatoes_User` | Rotten Tomatoes user score for the film. |'
  prefs: []
  type: TYPE_TB
- en: '| `Metacritic` | Metacritic critic score for the film. |'
  prefs: []
  type: TYPE_TB
- en: '| `Metacritic_User` | Metacritic user score for the film. |'
  prefs: []
  type: TYPE_TB
- en: '| `IMDB` | IMDb user score for the film. |'
  prefs: []
  type: TYPE_TB
- en: '| `Fandango_Stars` | A number of stars the film had on its Fandango movie page.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Fandango_Ratingvalue` | The Fandango rating value for the film, as pulled
    from the HTML of each page. This is the actual average score the movie obtained.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `RT_norm` | Tomatometer score for the film. It is normalized to a 0 to 5
    point system. |'
  prefs: []
  type: TYPE_TB
- en: '| `RT_user_norm` | Rotten Tomatoes user score for the film. It is normalized
    to a 0 to 5 point system. |'
  prefs: []
  type: TYPE_TB
- en: '| `Metacritic_norm` | The Metacritic critic scores for the film. It is normalized
    to a 0 to 5 point system. |'
  prefs: []
  type: TYPE_TB
- en: '| `Metacritic_user_nom` | Metacritic user score for the film, normalized to
    a0 to 5 point system. |'
  prefs: []
  type: TYPE_TB
- en: '| `IMDB_norm` | IMDb user score for the film which is normalized to a 0 to
    5 point system. |'
  prefs: []
  type: TYPE_TB
- en: '| `RT_norm_round` | Rotten Tomatoes Tomatometer score for the film which is
    normalized to a 0 to 5 point system and rounded to the nearest half-star. |'
  prefs: []
  type: TYPE_TB
- en: '| `RT_user_norm_round` | Rotten Tomatoes user score for the film, normalized
    to a 0 to 5 point system and rounded to the nearest half-star. |'
  prefs: []
  type: TYPE_TB
- en: '| `Metacritic_norm_round` | Metacritic critic score for the film, normalized
    to a 0 to 5 point system and rounded to the nearest half-star. |'
  prefs: []
  type: TYPE_TB
- en: '| `Metacritic_user_norm_round` | Metacritic user score for the film, normalized
    to a 0 to 5 point system and rounded to the nearest half-star. |'
  prefs: []
  type: TYPE_TB
- en: '| `IMDB_norm_round` | IMDb user score for the film, normalized to a 0 to 5
    point system and rounded to the nearest half-star. |'
  prefs: []
  type: TYPE_TB
- en: '| `Metacritic_user_vote_count` | A number of user votes the film had on Metacritic.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `IMDB_user_vote_count` | A number of user votes the film had on IMDb. |'
  prefs: []
  type: TYPE_TB
- en: '| `Fandango_votes` | A number of user votes the film had on Fandango. |'
  prefs: []
  type: TYPE_TB
- en: '| `Fandango_Difference` | Difference between the presented `Fandango_Stars`
    and the actual `Fandango_Ratingvalue`. |'
  prefs: []
  type: TYPE_TB
- en: 'We have already seen that a typical linear regression problem using TensorFlow
    has the following workflow that updates the parameters to minimize the given cost
    function of Fandango''s lopsided rating curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Linear Regression for Movie Rating Prediction](img/02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The learning algorithm using linear regression in TensorFlow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to follow the preceding figure and reproduce the same for the
    linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the dataset and create a Panda DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Linear Regression for Movie Rating Prediction](img/02_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5: A snap of the dataset showing a typo in the Metacritic_user_nom'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So, if you look at the preceding DataFrame carefully, there is a typo that
    could cause a disaster. From our intuition, it is clear that `Metacritic_user_nom`
    should have actually been `Metacritic_user_norm`. Let''s rename it to avoid further
    confusion:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Moreover, according to a statistical analysis at [https://fivethirtyeight.com/features/fandango-movies-ratings](https://fivethirtyeight.com/features/fandango-movies-ratings)/,
    all the variables don''t contribute equally; the following columns have more importance
    in ranking the movies:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can check the correlation coefficients between variables before build
    the LR model. First, let''s create a ranking list for that:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following function computes the `Pearson` correlation coefficients and
    builds a full correlation matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s call the preceding method to plot the matrix as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Pearson correlation coefficients**: A measure of the strength of the linear
    relationship between two variables. If the relationship between the variables
    is not linear, then the correlation coefficient cannot accurately and adequately
    represent the strength of the relationship between those two variables. It is
    often represented as "ρ" when measured on population and "r" when measured on
    a sample. Statistically, the range is -1 to 1, where -1 indicates a perfect negative
    linear relationship, an r of 0 indicates no linear relationship, and an r of 1
    indicates a perfect positive linear relationship between variables.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following correlation matrix shows correlation between considered features
    using the Pearson correlation coefficients:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Linear Regression for Movie Rating Prediction](img/02_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6: The correlation matrix on the ranking list movies'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So, the correlation between Fandango and Metacritic is still positive. Now,
    let''s do another study by considering only the movies for which RT has provided
    at least a 4-star rating:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is the correlation matrix on the ranked movies and RT movies having
    ratings of at least 4 showing a correlation between considered features using
    the Pearson correlation coefficients:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Linear Regression for Movie Rating Prediction](img/02_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7: The correlation matrix on the ranked movies and RT movies having
    ratings at least 4'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This time, we have obtained anticorrelation (that is, negative correlation)
    between Fandango and Metacritic, with the correlation coefficient-0.23\. This
    means that the correlation of Metacritic in terms of Fandango is significantly
    biased toward high ratings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Therefore, we can train our model without considering Fandango's rating, but
    before that let's build the LR model using this first. Later on, we will decide
    which option would produce a better result eventually.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Preparing the training and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s create a feature matrix `X` by selecting two DataFrame columns:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, I have used only the selected column as features and now we need to create
    a response vector `y`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are assuming that the IMDB is the most reliable and the baseline source of
    ratings. Our ultimate target is to predict the rating of each movie and compare
    the predicted ratings with the response column `IMDB_norm`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the features and the response columns, it''s time to split
    data into training and testing sets:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you want to change the `random_state`, it helps you generate pseudo-random
    numbers for a random sampling value to obtain differentfinal results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Random state**: As the name sounds can be used for initializing the internal
    random number generator, which will decide the splitting of data into train and
    test indices. This also signifies that every time you run it without specifying
    `random_state`, you will get a different result, this is expected behavior. So,
    we can have the following three options:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `random_state` is None (or `np.random`), a randomly-initialized `RandomState`
    object is returned
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `random_state` is an integer, it is used to seed a new `RandomState` object
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `random_state` is a `RandomState` object, it is passed through
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we need to have the dimension of the dataset to be passed through the
    tensors:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to include an extra dimension for independent coefficient:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And so we need to create an extra column for the independent coefficient in
    the training set and test feature set as well:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So far, we have used and utilized the panda DataFrames but converting it into
    tensors is troublesome so instead let''s convert them into a NumPy array:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Creating a place holder for TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have all the training and test sets, before initializing these
    variables, we have to create the place holder for TensorFlow to feed the training
    sets across the tensors:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s add some bias to differing from the value in the case where both types
    are quantized as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Creating an optimizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s create an optimizer for the objective function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initializing global variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training the LR model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here we are iterating the training 50,000 times and tracking several parameters,
    such as means square error that signifies how good the training is; we are keeping
    the cost history for future visualization, and so on:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we evaluate the `mse` to get the scalar value out of the training
    evaluation on the test set. Now, let''s compute the `mse` and `rmse` values, as
    follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also change the feature column, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we are not considering the Fandango''s stars, I experienced the following
    result of `mse` and `rmse` respectively:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Observing the training cost throughout iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Linear Regression for Movie Rating Prediction](img/02_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8: The training and training cost become saturated after 10000 iterations'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding graph shows that the training cost becomes saturated after 10,000
    iterations. This also means that, even if you iterate the model more than 10,000
    times, the cost is not going to experience a significant decrease.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluating the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following shows the predicted versus actual rating using LR:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that the prediction is a continuous value. Now it''s time to see
    how well the LR model generalizes and fits to the regression line:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using Linear Regression for Movie Rating Prediction](img/02_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 9: Prediction made by the LR model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The graph does not tell us that the prediction made by the LR model is good
    or bad. But we can still improve the performance of such models using layer architectures
    such as deep neural networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next example is about applying other supervised learning algorithms such
    as logistic regression, support vector machines, and random forest for predictive
    analytics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From Disaster to Decision – Titanic Example Revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Lesson 1](ch01.html "Chapter 1. From Data to Decisions – Getting Started
    with TensorFlow"), *From Data to Decisions – Getting Started with TensorFlow*,
    we have seen a minimal data analysis of the Titanic dataset. Now it's our turn
    to do some analytics on top of the data. Let's look at what kinds of people survived
    the disaster.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have enough data, but how could we do the predictive modeling so that
    we can draw some fairly straightforward conclusions from this data? For example,
    being a woman, being in first class, and being a child were all factors that could
    boost a passengers chances of survival during this disaster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the brute-force approach such as if-else statements with some sort of
    weighted scoring system, you could write a program to predict whether a given
    passenger would survive the disaster. However, writing such a program in Python
    does not make much sense. Naturally, it would be very tedious to write, difficult
    to generalize, and would require extensive fine-tuning for each variable and samples
    (that is, each passenger):'
  prefs: []
  type: TYPE_NORMAL
- en: '![From Disaster to Decision – Titanic Example Revisited](img/02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A regression algorithm is meant to produce continuous output'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you might have confusion in your mind about what the basic difference
    between a classification and a regression problem is. Well, a regression algorithm
    is meant to produce continuous output. The input is allowed to be either discrete
    or continuous. In contrast, a classification algorithm is meant to produce discrete
    output from an input from a set of discrete or continuous values. This distinction
    is important to know because discrete-valued outputs are handled better by classification,
    which will be discussed in upcoming sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![From Disaster to Decision – Titanic Example Revisited](img/02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: A classification algorithm is meant to produce discrete output'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will see how we could develop several predictive models
    for Titanic survival prediction and do some analytics using them. In particular,
    we will discuss logistic regression, random forest, and linear SVM. We start with
    logistic regression. Then we go with SVM since the number of features is not that
    large. Finally, we will see how we could improve the performance using Random
    Forests. However, before diving in too deeply, a short exploratory analysis of
    the dataset is required.
  prefs: []
  type: TYPE_NORMAL
- en: An Exploratory Analysis of the Titanic Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will see how the variables contribute to survival. At first, we need to
    import the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s load the data and check what the features available to us are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the training dataset has `12` columns and `891` rows altogether. Also,
    the `Age`, `Cabin`, and `Embarked` columns have null or missing values. We will
    take care of the null values in the feature engineering section, but for the time
    being, let''s see how many have survived:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How many have survived?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'So, approximately 61% died and only 39% of passengers managed to survive as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An Exploratory Analysis of the Titanic Dataset](img/02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Survived versus dead from the Titanic training set'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what is the relationship between the class and the rate of survival? At
    first we should see the counts for each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may remember from the movie (that is, Titanic 1997), people from higher
    classes had better chances of surviving. So, you may assume that the title could
    be an important factor in survival, too. Another funny thing is that people with
    longer names have a higher probability of survival. This happens due to most of
    the people with longer names being married ladies whose husband or family members
    probably helped them to survive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Women and children had a higher chance to survive, since they are the first
    to evacuate the shipwreck:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Cabin has the most nulls (almost 700), but we can still extract information
    from it, like the first letter of each cabin. Therefore, we can see that most
    of the cabin letters are associated with survival rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it also seems that people who embarked at Cherbourg had a 20% higher
    survival rate than those embarked at other embarking locations. This is very likely
    due to the high percentage of upper-class passengers from that location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Graphically, the preceding result can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An Exploratory Analysis of the Titanic Dataset](img/02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Survived by embarked'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, there were several important factors to people's survival. This means
    we need to consider these facts while developing our predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: We will train several binary classifiers since this is a binary classification
    problem having two predictors, that is, 0 and 1 using the training set and will
    use the test set for making survival predictions.
  prefs: []
  type: TYPE_NORMAL
- en: But, before we even do that, let's do some feature engineering since you have
    seen that there are some missing or null values. We will either impute them or
    drop the entry from the training and test set. Moreover, we cannot use our datasets
    directly, but need to prepare them such that they could feed our machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we are considering the length of the passenger''s name as an important
    feature, it would be better to remove the name itself and compute the corresponding
    length and also we extract only the title:'
  prefs: []
  type: TYPE_NORMAL
- en: 'def create_name_feat(train, test):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As there are 177 null values for Age, and those ones have a 10% lower survival
    rate than the non-nulls. Therefore, before imputing values for the nulls, we are
    including an Age_null flag, just to make sure we can account for this characteristic
    of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We are imputing the null age values with the mean of that column. This will
    add some extra bias in the dataset. But, for the betterment of our predictive
    model, we will have to sacrifice something.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we combine the `SibSp` and `Parch` columns to create get family size and
    break it into three levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to extract the first letter of the `Cabin` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Fill the null values in the `Embarked` column with the most commonly occurring
    value, which is `''S''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to convert our categorical columns. So far, we have considered
    it important for the predictive models that we will be creating to have numerical
    values for string variables. The `dummies()` function below does a one-hot encoding
    to the string variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the numerical features, finally, we need to create a separate column
    for the predicted values or targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We have seen the data and its characteristics and done some feature engineering
    to construct the best features for the linear models. The next task is to build
    the predictive models and make a prediction on the test set. Let's start with
    the logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression for Survival Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is one of the most widely used classifiers to predict a
    binary response. It is a linear machine learning method The `loss` function in
    the formulation given by the logistic loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic Regression for Survival Prediction](img/02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the logistic regression model, the loss function is the logistic loss.
    For a binary classification problem, the algorithm outputs a binary logistic regression
    model such that, for a given new data point, denoted by **x**, the model makes
    predictions by applying the logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic Regression for Survival Prediction](img/02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![Logistic Regression for Survival Prediction](img/02_21.jpg)
    **and** if ![Logistic Regression for Survival Prediction](img/02_22.jpg), the
    outcome is positive; otherwise, it is negative. Note that the raw output of the
    logistic regression model, **f (z)**, has a probabilistic interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Well, if you now compare logistic regression with its predecessor linear regression,
    the former provides you with a higher accuracy of the classification result. Moreover,
    it is a flexible way to regularize a model for custom adjustment and overall the
    model responses are measures of probability. And, most importantly, whereas linear
    regression can predict only continuous values, logistic regression can be generalized
    enough to make it predict discrete values. From now on, we will often be using
    the TensorFlow contrib API. So let's have a quick look at it.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorFlow Contrib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The contrib is a high level API for learning with TensorFlow. It supports the
    following Estimators:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.BaseEstimator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.Estimator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.Trainable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.Evaluable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.KMeansClustering`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.ModeKeys`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.ModelFnOps`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.MetricSpec`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.PredictionKey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.DNNClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.DNNRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.DNNLinearCombinedRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.DNNLinearCombinedClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.LinearClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.LinearRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.contrib.learn.LogisticRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, without developing the logistic regression, from scratch, we will use
    the estimator from the TensorFlow contrib package. When we are creating our own
    estimator from scratch, the constructor still accepts two high-level parameters
    for model configuration, `model_fn` and `params`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'To instantiate an Estimator we need to provide two parameters such as `model_fn`
    and the `model_params` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: It is to be noted that the `model_fn()` function contains all the above mentioned
    TensorFlow logic to support the training, evaluation, and prediction. Thus, you
    only need to implement the functionality that could use it efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, upon invoking the `main()` method, `model_params` containing the learning
    rate, instantiates the Estimator. You can define the `model_params` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the TensorFlow contrib, interested readers can refer
    to this URL at [https://www.tensorflow.org/extend/estimators](https://www.tensorflow.org/extend/estimators)
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, so far we have acquired enough background knowledge to create an LR model
    with TensorFlow with our dataset. It''s time to implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import required packages and modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Loading and preparing the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At first, we load both the datasets:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s do some feature engineering. We will invoke the function we defined
    in the feature engineering section, but will be provided as separate Python script
    with name `feature.py`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is to be noted that the sequence of the above invocation is important to
    make the training and test set consistent. Now, we also need to create numerical
    values for categorical variables using the `dummies()` function from sklearn:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to prepare the training and test set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then convert the training and test set into a NumPy array since so far we
    have kept them in Pandas DataFrame format:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s prepare the target column for prediction:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to know the feature count to build the LR estimator:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Preparing the LR estimator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We build the LR estimator. We will utilize the `LinearClassfier` estimator
    for it. Since this is a binary classification problem, we provide two classes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we train the above LR estimator for `10,000` iterations. The `fit()`
    method does the trick and the `predict()` method computes the prediction on the
    training set containing the feature, that is, `X_train` and the label, that is,
    `y_train`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Model evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will evaluate the model seeing several classification performance metrics
    such as precision, recall, f1 score, and confusion matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since we trained the LR model with NumPy data, we now need to convert it back
    to a Panda DataFrame for confusion matrix creation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s see the count:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since seeing the count graphically is awesome, let''s draw it:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using TensorFlow Contrib](img/02_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14: Survival prediction using logistic regression with TensorFlow'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, the accuracy we achieved with the LR model is 86% which is not that bad
    at all. But it can still be improved with better predictive models. In the next
    section, we will try to do that using linear **SVM** for survival prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Linear SVM for Survival Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The linear **SVM** is one of the most widely used and standard methods for
    large-scale classification tasks. Both the multiclass and binary classification
    problem can be solved using SVM with the loss function in the formulation given
    by the hinge loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear SVM for Survival Prediction](img/02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Usually, linear SVMs are trained with L2 regularization. Eventually, the linear
    SVM algorithm outputs an SVM model that can be used to predict the label of unknown
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have an unknown data point, **x**, the SVM model makes predictions
    based on the value of ![Linear SVM for Survival Prediction](img/02_23.jpg). The
    outcome can be either positive or negative. More specifically, if ![Linear SVM
    for Survival Prediction](img/02_24.jpg), then the predicted value is positive;
    otherwise, it is negative.
  prefs: []
  type: TYPE_NORMAL
- en: The current version of the TensorFlow contrib package supports only the linear
    SVM. TensorFlow uses SDCAOptimizer for the underlying optimization. Now, the thing
    is that if you want to build an SVM model of your own, you need to consider the
    performance and convergence tuning issues. Fortunately, you can pass the `num_loss_partitions`
    parameter to the SDCAOptimizer function. But you need to set the **X** such that
    it converges to the concurrent train ops per worker.
  prefs: []
  type: TYPE_NORMAL
- en: If you set the `num_loss_partitions` larger than or equal to this value, convergence
    is guaranteed, but this makes the overall training slower with the increase of
    `num_loss_partitions`. On the other hand, if you set its value to a smaller one,
    the optimizer is more aggressive in reducing the global loss, but convergence
    is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more on the implemented contrib packages, interested readers should refer
    to this URL at [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn/estimators](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn/estimators).
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, so far we have acquired enough background knowledge for creating an SVM
    model, now it''s time to implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages and modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Dataset preparation for building SVM model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, the data preparation for building an SVM model is more or less the same
    as an LR model, except that we need to convert the `PassengerId` to string which
    is required for the SVM:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Creating a dictionary for SVM for continuous feature column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: To feed the data to the SVM model, we further need to create a dictionary mapping
    from each continuous feature column name (k) to the values of that column stored
    in a constant Tensor. For more information on this issue, refer to this issue
    on TensorFlow GitHub repository at [https://github.com/tensorflow/tensorflow/issues/9505](https://github.com/tensorflow/tensorflow/issues/9505).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'I have written two functions for both the feature and labels. Let''s see what
    the first one looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding function creates a dictionary mapping from each continuous feature
    column and then another for the `passengerId` column. Then I merged them into
    one. Since we want to target the 'Survived' column as the labels, I converted
    the label column into constant tensor. Finally, through this function, I returned
    both the feature column and the label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, the second method does almost the same trick except that it returns only
    the feature columns as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training the SVM model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we will iterate the training 10,000 times over the real valued column only.
    Finally, it creates a prediction list containing all the prediction values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluation of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Thus using SVM, the accuracy is only 79%, which is lower than that of an LR
    model. Well, similar to an LR model, draw and observe the confusion matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, let''s draw the count plot to see the ratio visually:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Linear SVM for Survival Prediction](img/02_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 15: Survival prediction using linear SVM with TensorFlow'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, the count:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Ensemble Method for Survival Prediction – Random Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most widely used machine learning techniques is using the ensemble
    methods, which are learning algorithms that construct a set of classifiers. It
    can then be used to classify new data points by taking a weighted vote of their
    predictions. In this section, we will mainly focus on the random forest that can
    be built by combining 100s of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision trees** (**DTs**) is a technique which is used in supervised learning
    for solving classification and regression tasks. Where a DT model learns simple
    decision rules that are inferred from the data features by utilizing a tree-like
    graph to demonstrate the course of actions. Each branch of a decision tree represents
    a possible decision, occurrence or reaction in terms of statistical probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ensemble Method for Survival Prediction – Random Forest](img/02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: A sample decision tree on the admission test dataset using the rattle
    package of R'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to LR or SVM, the DTs are far more robust classification algorithms.
    The tree infers predicted labels or classes after splitting available features
    to the training data based to produce a good generalization. Most interestingly,
    the algorithm can handle both the binary as well as multiclass classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the decision trees in figure 16 learn from the admission data
    to approximate a sine curve with a set of `if...else` decision rules. The dataset
    contains the record of each student who applied for admission, say to an American
    university. Each record contains the graduate record exam score, CGPA score and
    the rank of the column. Now we will have to predict who is competent based on
    these three features (variables).
  prefs: []
  type: TYPE_NORMAL
- en: DTs can be utilized to solve this kind of problem after training the DT model
    and pruning the unwanted branch of the tree. In general, a deeper tree signifies
    more complex decision rules and a better-fitted model. Therefore, the deeper the
    tree, the more complex the decision rules, and the more fitted the model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you would like to draw the above figure, just use my R script and execute
    on RStudio and feed the admission dataset. The script and the dataset can be found
    in my GitHub repository at [https://github.com/rezacsedu/AdmissionUsingDecisionTree](https://github.com/rezacsedu/AdmissionUsingDecisionTree).
  prefs: []
  type: TYPE_NORMAL
- en: Well, so far we have acquired enough background knowledge for creating a Random
    Forest (RF) model, now it's time to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages and modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Dataset preparation for building an RF model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, the data preparation for building an RF model is more or less the same
    as an LR model. So please refer to the logistic regression section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Building a random forest estimator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following function builds a random forest estimator. It creates 1,000 trees
    with maximum 1,000 nodes and 10-fold cross-validation. Since it''s a binary classification
    problem, I put number of classes as 2:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training the RF model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we train the above RF estimator. Once the `fit()` method does the trick
    and the `predict()` method computes the prediction on the training set containing
    the feature, that is, `x_train` and the label, that is, `y_train`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Evaluating the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now let''s evaluate the performance of the RF model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Thus, using RF, the accuracy is 87% which is higher than that of the LR and
    SVM models. Well, similar to the LR and SVM model, we''ll draw and observe the
    confusion matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, let''s draw the count plot to see the ratio visually:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Ensemble Method for Survival Prediction – Random Forest](img/02_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 17: Titanic survival prediction using random forest with TensorFlow'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, the count for each one:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A Comparative Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the classification reports, we can see that random forest has the best
    overall performance. The reason for this may be that it works better with categorical
    features than the other two methods. Also, since it uses implicit feature selection,
    overfitting was reduced significantly. Using logistic regression is a convenient
    probability score for observations. However, it doesn't perform well when feature
    space is too large that is, doesn't handle a large number of categorical features/variables
    well. It also solely relies on transformations for non-linear features.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, using SVM we can handle a large feature space with non-linear feature
    interactions without relying on the entire dataset. However, it is not very well
    with a large number of observations. Nevertheless, it can be tricky to find an
    appropriate kernel sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this lesson, we have discussed supervised learning from the theoretical and
    practical perspective. In particular, we have revisited the linear regression
    model for regression analysis. We have seen how to use regression for predicting
    continuous values. Later in this lesson, we have discussed some other supervised
    learning algorithms for predictive analytics. We have seen how to use logistic
    regression, SVM, and random forests for survival prediction on the Titanic dataset.
    Finally, we have seen a comparative analysis between these classifiers. We have
    also seen that random forest, which is based on decision trees ensembles, outperforms
    logistic regression and linear SVM models.
  prefs: []
  type: TYPE_NORMAL
- en: In [Lesson 3](ch03.html "Chapter 3. Clustering Your Data – Unsupervised Learning
    for Predictive Analytics"), *Clustering Your Data – Unsupervised Learning for
    Predictive Analytics*, we will provide some practical examples of unsupervised
    learning. Particularly, the clustering technique using TensorFlow will be provided
    for neighborhood clustering and audio clustering from audio features.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, we will provide an exploratory analysis of the dataset then
    we will develop a cluster of the neighborhood using K-means, K-NN, and bisecting
    K-means with sufficient performance metrics such as cluster cost, accuracy, and
    so on. In the second part of the lesson, we will see how to do audio feature clustering.
    Finally, we will provide a comparative analysis of clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the nature of the learning feedback available, the machine learning
    process is typically classified into three broad categories. Name them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether the following statement is True or False: Using the brute-force
    approach such as if-else statements with some sort of weighted scoring system,
    you cannot write a program to predict whether a given passenger would survive
    the disaster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon invoking the main() method, model_params containing the learning rate instantiates
    the Estimator. How can you define the model_params as?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether the following statement is True or False: Each branch of a decision
    tree represents a possible decision, occurrence, or reaction in terms of statistical
    probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A predictive model based on supervised learning algorithms can make predictions
    based on a labeled _______ that maps inputs to outputs aligning with the real
    world.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataflow graph
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear graph
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Regression model
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
