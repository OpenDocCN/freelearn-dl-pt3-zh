<html><head></head><body>
  <div id="_idContainer1185">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-240" class="chapterTitle">Deep Q Network and Its Variants</h1>
    <p class="normal">In this chapter, let's get started with one of the most popular <strong class="keyword">Deep Reinforcement Learning </strong>(<strong class="keyword">DRL</strong>) algorithms called <strong class="keyword">Deep Q Network </strong>(<strong class="keyword">DQN</strong>). Understanding DQN is very important as many of the state-of-the-art DRL algorithms are based on DQN. The DQN algorithm was first proposed by researchers at Google's DeepMind in 2013 in the paper <em class="italic">Playing Atari with Deep Reinforcement Learning.</em> They described the DQN architecture and explained why it was so effective at playing Atari games with human-level accuracy. We begin the chapter by learning what exactly a deep Q network is, and how it is used in reinforcement learning. Next, we will deep dive into the algorithm of DQN. Then we will learn how to implement DQN to play Atari games.</p>
    <p class="normal">After learning about DQN, we will cover several variants of DQN, such as double DQN, DQN with prioritized experience replay, dueling DQN, and the deep recurrent Q network in detail.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bullet">What is DQN?</li>
      <li class="bullet">The DQN algorithm</li>
      <li class="bullet">Playing Atari games with DQN </li>
      <li class="bullet">Double DQN </li>
      <li class="bullet">DQN with prioritized experience replay</li>
      <li class="bullet">The dueling DQN </li>
      <li class="bullet">The deep recurrent Q network</li>
    </ul>
    <h1 id="_idParaDest-241" class="title">What is DQN?</h1>
    <p class="normal">The objective of reinforcement learning is to find the optimal policy, that is, the policy that gives us the <a id="_idIndexMarker857"/>maximum return (the sum of rewards of the episode). In order to compute the policy, first we compute the Q function. Once we have the Q function, then we extract the policy by selecting an action in each state that has the maximum Q value. For instance, let's suppose we have two states <strong class="keyword">A</strong> and <strong class="keyword">B</strong> and our action space consists of two actions; let the actions be <em class="italic">up</em> and <em class="italic">down</em>. So, in order to find which action to perform in state <strong class="keyword">A</strong> and <strong class="keyword">B</strong>, first we compute the Q value of all state-action pairs, as <em class="italic">Table 9.1</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_01.png" alt=""/></figure>
    <p class="packt_figref">Table 9.1: Q-value of state-action pairs</p>
    <p class="normal">Once we have the Q value of all state-action pairs, then we select the action in each state that has the maximum Q value. So, we select the action <em class="italic">up</em> in state <strong class="keyword">A</strong> and <em class="italic">down</em> in state <strong class="keyword">B</strong> as they have the maximum Q value. We improve the Q function on every iteration and once we have the optimal Q function, then we can extract the optimal policy from it.</p>
    <p class="normal">Now, let's revisit our grid world environment, as shown in <em class="italic">Figure 9.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.1: Grid world environment</p>
    <p class="normal">We learned that in the grid world environment, the goal of our agent is to reach state <strong class="keyword">I</strong> from state <strong class="keyword">A</strong> without visiting the shaded states, and in each state, the agent has to perform one of the four actions—<em class="italic">up</em>, <em class="italic">down</em>, <em class="italic">left</em>, <em class="italic">right</em>.</p>
    <p class="normal">To compute the policy, first we compute the Q values of all state-action pairs. Here, the number of states is 9 (<strong class="keyword">A</strong> to <strong class="keyword">I</strong>) and we have 4 actions in our action space, so our Q table will consist of 9 x 4 = 36 rows containing the Q values of all possible state-action pairs. Once we obtain the Q values, then <a id="_idIndexMarker858"/>we extract the policy by selecting the action in each state that has the maximum Q value. But is it a good approach to compute the Q value exhaustively for all state-action pairs? Let's explore this in more detail.</p>
    <p class="normal">Let's suppose we have an environment where we have 1,000 states and 50 possible actions in each state. In this case, our Q table will consist of 1,000 x 50 = 50,000 rows containing the Q values of all possible state-action pairs. In cases like this, where our environment consists of a large number of states and actions, it will be very expensive to compute the Q values of all possible state-action pairs in an exhaustive fashion.</p>
    <p class="normal">Instead of computing Q values in this way, can we approximate them using any function approximator, such as a neural network? Yes! We can parameterize our Q function by a parameter <img src="../Images/B15558_09_001.png" alt="" style="height: 1.11em;"/> and compute the Q value where the parameter <img src="../Images/B15558_09_002.png" alt="" style="height: 1.11em;"/> is just the parameter of our neural network. So, we just feed the state of the environment to a neural network and it will return the Q value of all possible actions in that state. Once we obtain the Q values, then we can select the best action as the one that has the maximum Q value.</p>
    <p class="normal">For example, let's consider our grid world environment. As <em class="italic">Figure 9.2 </em>shows, we just feed state <strong class="keyword">D</strong> as an input to the network and it returns the Q value of all actions in state <strong class="keyword">D</strong>, which are <em class="italic">up</em>, <em class="italic">down</em>, <em class="italic">left</em>, and <em class="italic">right</em>, as output. Then, we select the action that has the maximum Q value. Since action <em class="italic">right</em> has a maximum Q value, we select action <em class="italic">right</em> in the state <strong class="keyword">D</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.2: Deep Q network</p>
    <p class="normal">Since we are using a neural network to approximate the Q value, the neural network is called the Q network, and if we <a id="_idIndexMarker859"/>use a deep neural network to approximate the Q value, then the deep <a id="_idIndexMarker860"/>neural network is called a <strong class="keyword">deep Q network</strong> (<strong class="keyword">DQN</strong>).</p>
    <p class="normal">We can denote our Q function by <img src="../Images/B15558_09_003.png" alt="" style="height: 1.11em;"/>, where the parameter <img src="../Images/B15558_09_004.png" alt="" style="height: 1.11em;"/> in subscript indicates that our Q function is parameterized by <img src="../Images/B15558_09_004.png" alt="" style="height: 1.11em;"/>, and <img src="../Images/B15558_09_006.png" alt="" style="height: 1.11em;"/> is just the parameter of our neural network.</p>
    <p class="normal">We initialize the network parameter <img src="../Images/B15558_09_004.png" alt="" style="height: 1.11em;"/> with random values and approximate the Q function (Q values), but since we initialized <img src="../Images/B15558_09_008.png" alt="" style="height: 1.11em;"/> with random values, the approximated Q function will not be optimal. So, we train the network for several iterations by finding the optimal parameter <img src="../Images/B15558_09_002.png" alt="" style="height: 1.11em;"/>. Once we find the optimal <img src="../Images/B15558_09_004.png" alt="" style="height: 1.11em;"/>, we will have the optimal Q function. Then we can extract the optimal policy from the optimal Q function.</p>
    <p class="normal">Okay, but how can we train our network? What about the training data and the loss function? Is it a <a id="_idIndexMarker861"/>classification or regression task? Now that we have a basic understanding of how DQN works, in the next section, we will get into the details and address all these questions.</p>
    <h2 id="_idParaDest-242" class="title">Understanding DQN</h2>
    <p class="normal">In this section, we will understand how exactly DQN works. We learned that we use DQN to approximate the Q value of all the actions in the given input state. The Q value is just a <a id="_idIndexMarker862"/>continuous number, so we are essentially using our DQN to perform a regression task.</p>
    <p class="normal">Okay, what about the training data? We use a buffer called a replay buffer to collect the agent's experience and, based on this experience, we train our network. Let's explore the replay buffer in detail. </p>
    <h3 id="_idParaDest-243" class="title">Replay buffer</h3>
    <p class="normal">We know that the agent makes a transition from a state <em class="italic">s</em> to the next state <img src="../Images/B15558_09_011.png" alt="" style="height: 1.2em;"/> by performing <a id="_idIndexMarker863"/>some action <em class="italic">a</em>, and then receives a reward <em class="italic">r</em>. We can save this transition information <img src="../Images/B15558_09_012.png" alt="" style="height: 1.2em;"/> in a buffer called a replay buffer or <a id="_idIndexMarker864"/>experience replay. The replay buffer is usually denoted by <img src="../Images/B15558_09_013.png" alt="" style="height: 1.11em;"/>. This transition information is basically the agent's experience. We store the agent's experience over several episodes in the replay buffer. The key idea of using the replay buffer to store the agent's experience is that we can train our DQN with experience (transition) sampled from the buffer. A replay buffer is shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.3: Replay buffer</p>
    <p class="normal">The following steps help us to understand how we store the transition information in the replay buffer <img src="../Images/B15558_09_014.png" alt="" style="height: 1.11em;"/>:</p>
    <ol>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_09_015.png" alt="" style="height: 1.11em;"/>.</li>
      <li class="numbered">For <a id="_idIndexMarker865"/>each episode perform <em class="italic">step 3</em>.</li>
      <li class="numbered">For each step in the episode:<ol>
          <li class="numbered-l2">Make a <a id="_idIndexMarker866"/>transition, that is, perform an action <em class="italic">a</em> in the state <em class="italic">s</em>, move to the next state <img src="../Images/B15558_09_016.png" alt="" style="height: 1.2em;"/>, and receive the reward <em class="italic">r</em>.</li>
          <li class="numbered-l2">Store the transition information <img src="../Images/B15558_09_017.png" alt="" style="height: 1.2em;"/> in the replay buffer <img src="../Images/B15558_09_018.png" alt="" style="height: 1.11em;"/>.</li>
        </ol>
      </li>
    </ol>
    <p class="normal">As explained in the preceding steps, we collect the agent's transition information over many episodes and save it in the replay buffer. To understand this clearly, let's consider our favorite grid world <a id="_idIndexMarker867"/>environment. Let's suppose <a id="_idIndexMarker868"/>we have the following two episodes/trajectories:</p>
    <p class="normal"><strong class="keyword">Episode 1</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.4: Trajectory 1</p>
    <p class="normal"><strong class="keyword">Episode 2</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.5: Trajectory 2</p>
    <p class="normal">Now, this information will be stored in the replay buffer, as <em class="italic">Figure 9.6 </em>shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.6: Replay buffer</p>
    <p class="normal">As <em class="italic">Figure 9.6 </em>shows, we store the transition information by stacking it sequentially one after another. We train the network by sampling a minibatch of transitions from the replay buffer. Wait! There is a small issue here. Since we are stacking up the agent's experience (transition) one after <a id="_idIndexMarker869"/>another sequentially, the agent's experience will be highly correlated. For example, as shown in the preceding figure, transitions will be correlated with the <a id="_idIndexMarker870"/>rows above and below. If we train our network with this correlated experience then our neural network will easily overfit. So, to combat this, we sample a random minibatch of transitions from the replay buffer and train the network. </p>
    <p class="normal">Note that the replay buffer is of limited size, that is, a replay buffer will store only a fixed amount of the agent's experience. So, when the buffer is full we replace the old experience with new experience. A replay buffer is usually implemented as a queue structure (first in first out) rather than a list. So, if the buffer is full when new experience comes in, we remove the old experience and add the new experience into the buffer.</p>
    <p class="normal">We have learned that we train our network by randomly sampling a minibatch of experience from the buffer. But how exactly does the training happen? How does our network learn to approximate the optimal Q function using this minibatch of samples? This is exactly what we discuss in the next section.</p>
    <h3 id="_idParaDest-244" class="title">Loss function</h3>
    <p class="normal">We learned that in DQN, our <a id="_idIndexMarker871"/>goal is to predict <a id="_idIndexMarker872"/>the Q value, which is just a continuous value. Thus, in DQN we basically perform a regression task. We generally use the <strong class="keyword">mean squared error</strong> (<strong class="keyword">MSE</strong>) as the loss function <a id="_idIndexMarker873"/>for the regression task. MSE can be defined as the average squared difference between the target value and the predicted value, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_019.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Where <em class="italic">y</em> is the target value, <img src="../Images/B15558_09_020.png" alt="" style="height: 1.11em;"/> is the predicted value, and <em class="italic">K</em> is the number of training samples.</p>
    <p class="normal">Now, let's learn how to use MSE in the DQN and train the network. We can train our network by minimizing the MSE between the target Q value and predicted Q value. First, how can we obtain the target Q value? Our target Q value should be the optimal Q value so that we can train our network by minimizing the error between the optimal Q value and predicted Q value. But how can we compute the optimal Q value? This is where the Bellman equation helps us. In <em class="chapterRef">Chapter 3</em>,<em class="italic"> The Bellman Equation and Dynamic Programming</em>, we learned that the optimal Q value can be obtained using the Bellman optimality equation:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_021.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">where <img src="../Images/B15558_09_022.png" alt="" style="height: 1.2em;"/> represents the immediate reward <em class="italic">r</em> that we obtain while performing an action <em class="italic">a</em> in state <em class="italic">s</em> and moving to the next state <img src="../Images/B15558_09_016.png" alt="" style="height: 1.2em;"/>, so we can just denote <img src="../Images/B15558_09_024.png" alt="" style="height: 1.2em;"/> by <em class="italic">r</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_025.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">In the preceding equation, we can remove the expectation. We will approximate the expectation by sampling <em class="italic">K</em> number of transitions from the replay buffer and taking the average value; we will learn more about this in a while. </p>
    <p class="normal">Thus, according to the Bellman optimality equation, the optimal Q value is just the sum of the reward and the discounted maximum Q value of the next state-action pair, that is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_026.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">So, we can define <a id="_idIndexMarker874"/>our loss as the difference between the target value (the optimal Q value) and the predicted value (the Q value predicted by the DQN) and <a id="_idIndexMarker875"/>express the loss function <em class="italic">L</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_027.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Substituting equation (1) in the preceding equation, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_028.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">We know that we compute the predicted Q value using the network parameterized by <img src="../Images/B15558_09_029.png" alt="" style="height: 1.11em;"/>. How can we compute the target value? That is, we learned that the target value is the sum of the reward and the discounted maximum Q value of the next state-action pair. How do we compute the Q value of the next state-action pair?</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_19.png" alt="" style="height: 8em;"/></figure>
    <p class="normal">Similar to the predicted Q value, we can compute the Q value of the next state-action pair in the target using the same DQN parameterized by <img src="../Images/B15558_09_001.png" alt="" style="height: 1.11em;"/>. So, we can rewrite our loss function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_031.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">As shown, both the target value and the predicted Q value are parameterized by <img src="../Images/B15558_09_006.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Instead of computing the loss as just the difference between the target Q value and the predicted Q value, we <a id="_idIndexMarker876"/>use MSE as our loss function. We learned that we store the agent's experience in a buffer called a replay buffer. So, we <a id="_idIndexMarker877"/>randomly sample a minibatch of <em class="italic">K</em> number of transitions <img src="../Images/B15558_09_012.png" alt="" style="height: 1.2em;"/> from the replay buffer and train the network by minimizing the MSE, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.7: Loss function of DQN</p>
    <p class="normal">Thus, our loss function can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_034.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">For simplicity of notation, we can denote the target value by <em class="italic">y</em> and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_035.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_09_036.png" alt="" style="height: 1.58em;"/>. We have learned that the target value is just the sum of the <a id="_idIndexMarker878"/>reward and the discounted maximum Q value of the next state-action pair. But what if the next state <img src="../Images/B15558_03_034.png" alt="" style="height: 1.2em;"/> is a <a id="_idIndexMarker879"/>terminal state? If the next state <img src="../Images/B15558_03_034.png" alt="" style="height: 1.2em;"/> is terminal then we cannot compute the Q value as we don't take any action in the terminal state, so in that case, the target value will be just the reward, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_039.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Thus, our loss function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_035.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">We train our network by minimizing the loss function. We can minimize the loss function by finding the optimal parameter <img src="../Images/B15558_09_002.png" alt="" style="height: 1.11em;"/>. So, we use gradient descent to find the optimal parameter <img src="../Images/B15558_09_042.png" alt="" style="height: 1.11em;"/>. We compute the gradient of our loss function <img src="../Images/B15558_09_043.png" alt="" style="height: 1.11em;"/> and update our network parameter <img src="../Images/B15558_09_044.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_045.png" alt="" style="height: 1.11em;"/></figure>
    <h3 id="_idParaDest-245" class="title">Target network</h3>
    <p class="normal">In the last section, we learned <a id="_idIndexMarker880"/>that we train the network by minimizing the loss function, which is the MSE between the target value and the <a id="_idIndexMarker881"/>predicted value, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_034.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">However, there is a small issue with our loss function. We have learned that the target value is just the sum of the reward and the discounted maximum Q value of the next state-action pair. We compute this Q value of the next state-action pair in the target and predicted Q values using the same network parameterized by <img src="../Images/B15558_09_001.png" alt="" style="height: 1.11em;"/>, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_20.png" alt="" style="height: 6em;"/></figure>
    <p class="normal">The problem is since the target and predicted value depend on the same parameter <img src="../Images/B15558_09_048.png" alt="" style="height: 1.11em;"/>, this will cause instability in the MSE and the network learns poorly. It also causes a lot of divergence during training.</p>
    <p class="normal">Let's understand this with a simple example. We will take arbitrary numbers to make it easier to understand. We know that we try to minimize the difference between the target value and the predicted value. So, on every iteration, we compute the gradient of loss and update our network parameter <img src="../Images/B15558_09_001.png" alt="" style="height: 1.11em;"/> so that we can make our predicted value the same as the target value.</p>
    <p class="normal">Let's suppose in iteration 1, the target value is 13 and the predicted value is 11. So, we update our parameter <img src="../Images/B15558_09_002.png" alt="" style="height: 1.11em;"/> to match the predicted value to the target value, which is 13. But in the next iteration, the target value changes to 15 and the predicted value becomes 13 since we <a id="_idIndexMarker882"/>updated our network parameter <img src="../Images/B15558_09_002.png" alt="" style="height: 1.11em;"/>. So, again we update our parameter <img src="../Images/B15558_09_002.png" alt="" style="height: 1.11em;"/> to match the predicted value to the target value, which is <a id="_idIndexMarker883"/>now 15. But in the next iteration, the target value changes to 17 and the predicted value becomes 15 since we updated our network parameter <img src="../Images/B15558_09_053.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">As <em class="italic">Table 9.2</em> shows, on every iteration, the predicted value tries to be the same as the target value, which keeps on changing:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_09.png" alt=""/></figure>
    <p class="packt_figref">Table 9.2: Target and predicted value</p>
    <p class="normal">This is because the predicted and target values both depend on the same parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>. If we update <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>, then both the target and predicted values change. Thus, the predicted value keeps on trying to be the same as the target value, but the target value keeps on changing due to the update on the network parameter <img src="../Images/B15558_09_056.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">How can we avoid this? Can we freeze the target value for a while and compute only the predicted value so that our predicted value matches the target value? Yes! To do this, we introduce another neural network called a target network for computing the Q value of the next state-action pair in the target. The parameter of the target network is represented by <img src="../Images/B15558_09_057.png" alt="" style="height: 1.2em;"/>. So, our <a id="_idIndexMarker884"/>main deep Q network, which is used for predicting Q values, learns the optimal parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> using gradient descent. The <a id="_idIndexMarker885"/>target network is frozen for a while and then the target network parameter <img src="../Images/B15558_09_059.png" alt="" style="height: 1.2em;"/> is updated by just copying the main deep Q network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>. Freezing the target network for a while and then updating its parameter <img src="../Images/B15558_09_061.png" alt="" style="height: 1.2em;"/> with the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> stabilizes the training.</p>
    <p class="normal">So, now our loss function can be rewritten as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_063.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Thus, the Q value of the next state-action pair in the target is computed by the target network parameterized by <img src="../Images/B15558_09_064.png" alt="" style="height: 1.2em;"/>, and the predicted Q value is computed by our main network parameterized by <img src="../Images/B15558_09_065.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_21.png" alt="" style="height: 7em;"/></figure>
    <p class="normal">For notation simplicity, we <a id="_idIndexMarker886"/>can represent our target value <a id="_idIndexMarker887"/>by <em class="italic">y</em> and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_035.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_09_067.png" alt="" style="height: 1.58em;"/>.</p>
    <p class="normal">We have learned about several concepts concerning DQN, including the experience replay, the loss function, and the target network. In the next section, we will put all these concepts together and see how DQN works. </p>
    <h2 id="_idParaDest-246" class="title">Putting it all together</h2>
    <p class="normal">First, we initialize the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> with random values. We learned that the target network <a id="_idIndexMarker888"/>parameter is just a copy of the main network. So, we initialize the target network parameter <img src="../Images/B15558_09_069.png" alt="" style="height: 1.2em;"/> by just copying the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>. We also initialize the replay buffer <img src="../Images/B15558_09_071.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, for each step in the episode, we feed the state of the environment to our network and it outputs the Q values of all possible actions in that state. Then, we select the action that has the maximum Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_072.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">If we only select the action that has the highest Q value, then we will not explore any new actions. So, to avoid this, we select actions using the epsilon-greedy policy. With the epsilon-greedy policy, we select a random action with probability epsilon and with probability 1-epsilon, we select the best action that has the maximum Q value.</p>
    <p class="normal">Note that, since we initialized our network parameter <img src="../Images/B15558_09_056.png" alt="" style="height: 1.11em;"/> with random values, the action we select by taking the maximum Q value will not be the optimal action. But that's okay, we simply perform the <a id="_idIndexMarker889"/>selected action, move to the next state, and obtain the reward. If the action is good then we will receive a positive reward, and if it is bad then the reward will be negative. We store all this transition information <img src="../Images/B15558_09_074.png" alt="" style="height: 1.2em;"/> in the replay buffer <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Next, we randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer and compute the loss. We have learned that our loss function is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_035.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Where <em class="italic">y</em><sub class="" style="font-style: italic;">i</sub> is the target value, that is, <img src="../Images/B15558_09_067.png" alt="" style="height: 1.58em;"/>.</p>
    <p class="normal">In the initial iterations, the loss will be very high since our network parameter <img src="../Images/B15558_09_008.png" alt="" style="height: 1.11em;"/> is just random values. To minimize the loss, we compute the gradients of the loss and update our network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_045.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">We don't update the target network parameter <img src="../Images/B15558_09_059.png" alt="" style="height: 1.2em;"/> in every time step. We freeze the target network <a id="_idIndexMarker890"/>parameter <img src="../Images/B15558_09_059.png" alt="" style="height: 1.2em;"/> for several time steps and then we copy the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> to the target network parameter <img src="../Images/B15558_09_084.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">We keep repeating the preceding steps for several episodes to approximate the optimal Q value. Once we have the optimal Q value, we extract the optimal policy from them. To give us a more detailed understanding, the algorithm of DQN is given in the next section.</p>
    <h3 id="_idParaDest-247" class="title">The DQN algorithm</h3>
    <p class="normal">The DQN algorithm is <a id="_idIndexMarker891"/>given in the following steps:</p>
    <ol>
      <li class="numbered" value="1">Initialize the main network parameter <img src="../Images/B15558_09_056.png" alt="" style="height: 1.11em;"/> with random values</li>
      <li class="numbered">Initialize the target network parameter <img src="../Images/B15558_09_086.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, perform <em class="italic">step 5</em></li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0, . . ., <em class="italic">T</em>-1:<ol>
          <li class="numbered-l2">Observe the state <em class="italic">s</em> and select an action using the epsilon-greedy policy, that is, with probability epsilon, select random action <em class="italic">a</em> and with probability 1-epsilon, select the action <img src="../Images/B15558_09_072.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">Perform the selected action and move to the next state <img src="../Images/B15558_03_021.png" alt="" style="height: 1.2em;"/> and obtain the reward <em class="italic">r</em></li>
          <li class="numbered-l2">Store the transition information in the replay buffer <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_09_092.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the target value, that is, <img src="../Images/B15558_09_067.png" alt="" style="height: 1.58em;"/></li>
          <li class="numbered-l2">Compute the loss, <img src="../Images/B15558_09_035.png" alt="" style="height: 3.33em;"/></li>
          <li class="numbered-l2">Compute the gradients of the loss and update the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> using <a id="_idIndexMarker892"/>gradient descent: <img src="../Images/B15558_09_096.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Freeze the target network parameter <img src="../Images/B15558_09_084.png" alt="" style="height: 1.2em;"/> for several time steps and then update it by just copying the main network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/></li>
        </ol>
      </li>
    </ol>
    <p class="normal">Now that we have understood how DQN works, in this next section, we will learn how to implement it.</p>
    <h1 id="_idParaDest-248" class="title">Playing Atari games using DQN</h1>
    <p class="normal">The Atari 2600 is a <a id="_idIndexMarker893"/>popular video game console from a <a id="_idIndexMarker894"/>game company called Atari. The Atari game console provides several popular games, such as <a id="_idIndexMarker895"/>Pong, Space Invaders, Ms. Pac-Man, Breakout, Centipede, and many more. In this section, we will learn how to build a DQN for playing the Atari games. First, let's explore the architecture of the DQN for playing the Atari games.</p>
    <h2 id="_idParaDest-249" class="title">Architecture of the DQN</h2>
    <p class="normal">In the Atari environment, the <a id="_idIndexMarker896"/>image of the game screen is the state of the environment. So, we just feed the image of the game screen as input to the DQN and it returns the Q values of all the actions in the state. Since we are dealing with images, instead of using a vanilla <a id="_idIndexMarker897"/>deep neural network for approximating the Q value, we can use a <strong class="keyword">convolutional neural network (CNN) </strong>since it is very effective for handling images.</p>
    <p class="normal">Thus, now our DQN is a CNN. We feed the image of the game screen (the game state) as input to the CNN, and it outputs the Q values of all the actions in the state.</p>
    <p class="normal">As <em class="italic">Figure 9.8</em> shows, given the image of the game screen, the convolutional layers extract features from the image and produce a feature map. Next, we flatten the feature map and feed the flattened feature map as input to the feedforward network. The feedforward network takes this flattened feature map as input and returns the Q values of all the actions in the state:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.8: Architecture of a DQN</p>
    <p class="normal">Note that we don't perform a pooling operation. A pooling operation is useful when we perform tasks such as object detection, image classification, and so on where we don't consider the position of the object in the image and we just want to know whether the desired object is present in the image. For example, if we want to identify whether there is a dog in an image, we only look for whether a dog is present in the image and we don't check the position of the dog in the image. Thus, in this case, a pooling operation is used to identify whether there is a dog in the image irrespective of the position of the dog.</p>
    <p class="normal">But in our setting, a pooling operation should not be performed because to understand the current game state, the position is very important. For example, in the Pong, we just don't want to <a id="_idIndexMarker898"/>classify if there is a ball on the game screen. We want to know the position of the ball so that we can make a better action. Thus, we don't include the pooling operation in our DQN architecture.</p>
    <p class="normal">Now that we have understood the architecture of the DQN to play Atari games, in the next section, we will start implementing it.</p>
    <h2 id="_idParaDest-250" class="title">Getting hands-on with the DQN</h2>
    <p class="normal">Let's implement <a id="_idIndexMarker899"/>the DQN to play the Ms Pacman game. First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense, Activation, Flatten, Conv2D, MaxPooling2D
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam
</code></pre>
    <p class="normal">Now, let's create the Ms Pacman game environment using Gym:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"MsPacman-v0"</span>)
</code></pre>
    <p class="normal">Set the state size:</p>
    <pre class="programlisting code"><code class="hljs-code">state_size = (<span class="hljs-number">88</span>, <span class="hljs-number">80</span>, <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Get the number of actions:</p>
    <pre class="programlisting code"><code class="hljs-code">action_size = env.action_space.n
</code></pre>
    <h3 id="_idParaDest-251" class="title">Preprocess the game screen</h3>
    <p class="normal">We learned that we feed the game state (an image of the game screen) as input to the DQN, which is a CNN, and it <a id="_idIndexMarker900"/>outputs the Q values of all the actions in the state. However, directly feeding the raw game screen image is not efficient, since the raw game screen size will be 210 x 160 x 3. This will be computationally expensive.</p>
    <p class="normal">To avoid this, we preprocess the game screen and then feed the preprocessed game screen to the DQN. First, we crop and resize the game screen image, convert the image to grayscale, normalize it, and then reshape the image to 88 x 80 x 1. Next, we feed this preprocessed game screen image as input to the CNN, which returns the Q values.</p>
    <p class="normal">Now, let's define a function called <code class="Code-In-Text--PACKT-">preprocess_state</code>, which takes the game state (image of the game screen) as an input and returns the preprocessed game state:</p>
    <pre class="programlisting code"><code class="hljs-code">color = np.array([<span class="hljs-number">210</span>, <span class="hljs-number">164</span>, <span class="hljs-number">74</span>]).mean()
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">preprocess_state</span><span class="hljs-function">(</span><span class="hljs-params">state</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Crop and resize the image:</p>
    <pre class="programlisting code"><code class="hljs-code">    image = state[<span class="hljs-number">1</span>:<span class="hljs-number">176</span>:<span class="hljs-number">2</span>, ::<span class="hljs-number">2</span>]
</code></pre>
    <p class="normal">Convert the image to grayscale:</p>
    <pre class="programlisting code"><code class="hljs-code">    image = image.mean(axis=<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Improve the image contrast:</p>
    <pre class="programlisting code"><code class="hljs-code">    image[image==color] = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">Normalize the image:</p>
    <pre class="programlisting code"><code class="hljs-code">    image = (image - <span class="hljs-number">128</span>) / <span class="hljs-number">128</span> - <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Reshape and return the image:</p>
    <pre class="programlisting code"><code class="hljs-code">    image = np.expand_dims(image.reshape(<span class="hljs-number">88</span>, <span class="hljs-number">80</span>, <span class="hljs-number">1</span>), axis=<span class="hljs-number">0</span>)
    <span class="hljs-keyword">return</span> image
</code></pre>
    <h3 id="_idParaDest-252" class="title">Defining the DQN class</h3>
    <p class="normal">Let's define the <a id="_idIndexMarker901"/>class called DQN where we will implement the DQN algorithm. For a clear understanding, let's look into the code line by line. You <a id="_idIndexMarker902"/>can also access the complete code from the GitHub repository of the book:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">DQN</span><span class="hljs-class">:</span>
</code></pre>
    <h4 class="title">Defining the init method</h4>
    <p class="normal">First, let's <a id="_idIndexMarker903"/>define the <code class="Code-In-Text--PACKT-">init</code> method</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, state_size, action_size</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Define the state size:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.state_size = state_size
</code></pre>
    <p class="normal">Define the action size:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.action_size = action_size
</code></pre>
    <p class="normal">Define the replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.replay_buffer = deque(maxlen=<span class="hljs-number">5000</span>)
</code></pre>
    <p class="normal">Define the discount factor:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.gamma = <span class="hljs-number">0.9</span>
</code></pre>
    <p class="normal">Define the epsilon value:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.epsilon = <span class="hljs-number">0.8</span>
</code></pre>
    <p class="normal">Define the update rate at which we want to update the target network:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.update_rate = <span class="hljs-number">1000</span>
</code></pre>
    <p class="normal">Define the main network:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.main_network = self.build_network()
</code></pre>
    <p class="normal">Define the target network:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.target_network = self.build_network()
</code></pre>
    <p class="normal">Copy the weights of the main network to the target network:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.target_network.set_weights(self.main_network.get_weights())
</code></pre>
    <h4 class="title">Building the DQN</h4>
    <p class="normal">Now, let's build the DQN. We have learned that to play Atari games, we use a CNN as the DQN, which takes the <a id="_idIndexMarker904"/>image of the game screen as an input and returns the Q values. We define the DQN with three convolutional layers. The convolutional layers extract the features from the image and output the feature maps, and then we flatten the feature map obtained by the convolutional layers and feed the flattened feature maps to the feedforward network (the fully connected layer), which returns the Q values:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_network</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Define the first convolutional layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        model = Sequential()
        model.add(Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">8</span>, <span class="hljs-number">8</span>), strides=<span class="hljs-number">4</span>, padding=<span class="hljs-string">'same'</span>, input_shape=self.state_size))
        model.add(Activation(<span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">Define the second convolutional layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        model.add(Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">4</span>), strides=<span class="hljs-number">2</span>, padding=<span class="hljs-string">'same'</span>))
        model.add(Activation(<span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">Define the third convolutional layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        model.add(Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=<span class="hljs-number">1</span>, padding=<span class="hljs-string">'same'</span>))
        model.add(Activation(<span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">Flatten the feature maps obtained as a result of the third convolutional layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        model.add(Flatten())
</code></pre>
    <p class="normal">Feed the flattened maps to the fully connected layer:</p>
    <pre class="programlisting code"><code class="hljs-code">        model.add(Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'relu'</span>))
        model.add(Dense(self.action_size, activation=<span class="hljs-string">'linear'</span>))
</code></pre>
    <p class="normal">Compile the model with loss as MSE:</p>
    <pre class="programlisting code"><code class="hljs-code">        model.compile(loss=<span class="hljs-string">'mse'</span>, optimizer=Adam())
</code></pre>
    <p class="normal">Return the model:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">return</span> model
</code></pre>
    <h4 class="title">Storing the transition</h4>
    <p class="normal">We have learned that we train the DQN by randomly sampling a minibatch of transitions from the <a id="_idIndexMarker905"/>replay buffer. So, we define a function called <code class="Code-In-Text--PACKT-">store_transition</code>, which stores the transition information in the replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">store_transistion</span><span class="hljs-function">(</span><span class="hljs-params">self, state, action,</span>
<span class="hljs-params">                          reward, next_state, done</span><span class="hljs-function">):</span>
        self.replay_buffer.append((state, action,
                                   reward, next_state, done))
</code></pre>
    <h4 class="title">Defining the epsilon-greedy policy</h4>
    <p class="normal">We <a id="_idIndexMarker906"/>learned that in DQN, to take care of exploration-exploitation trade-off, we select action using the epsilon-greedy policy. So, now we define the function called <code class="Code-In-Text--PACKT-">epsilon_greedy</code> for selecting an action using the epsilon-greedy policy:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">epsilon_greedy</span><span class="hljs-function">(</span><span class="hljs-params">self, state</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">if</span> random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>) &lt; self.epsilon:
            <span class="hljs-keyword">return</span> np.random.randint(self.action_size)
        Q_values = self.main_network.predict(state)
        <span class="hljs-keyword">return</span> np.argmax(Q_values[<span class="hljs-number">0</span>])
</code></pre>
    <h4 class="title">Define the training</h4>
    <p class="normal">Now let's define a function <a id="_idIndexMarker907"/>called <code class="Code-In-Text--PACKT-">train</code> for the training network:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">train</span><span class="hljs-function">(</span><span class="hljs-params">self, batch_size</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Sample a minibatch of transitions from the replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code">        minibatch = random.sample(self.replay_buffer, batch_size)
</code></pre>
    <p class="normal">Compute the target value using the target network:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">for</span> state, action, reward, next_state, done <span class="hljs-keyword">in</span> minibatch:
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> done:
                target_Q = (reward + self.gamma * np.amax(
                    self.target_network.predict(next_state)))
            <span class="hljs-keyword">else</span>:
                target_Q = reward
</code></pre>
    <p class="normal">Compute the predicted value using the main network and store the predicted value in the <code class="Code-In-Text--PACKT-">Q_values</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">            Q_values = self.main_network.predict(state)
</code></pre>
    <p class="normal">Update the target value:</p>
    <pre class="programlisting code"><code class="hljs-code">            Q_values[<span class="hljs-number">0</span>][action] = target_Q
</code></pre>
    <p class="normal">Train the <a id="_idIndexMarker908"/>main network:</p>
    <pre class="programlisting code"><code class="hljs-code">            self.main_network.fit(state, Q_values, epochs=<span class="hljs-number">1</span>, 
                                  verbose=<span class="hljs-number">0</span>)
</code></pre>
    <h4 class="title">Updating the target network</h4>
    <p class="normal">Now, let's define the <a id="_idIndexMarker909"/>function called <code class="Code-In-Text--PACKT-">update_target_network</code> for updating the target network weights by copying from the main network:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">update_target_network</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>
        self.target_network.set_weights(self.main_network.get_weights())
</code></pre>
    <h3 id="_idParaDest-253" class="title">Training the DQN</h3>
    <p class="normal">Now, let's train the <a id="_idIndexMarker910"/>network. First, let's set the number of episodes we want to train the network for:</p>
    <pre class="programlisting code"><code class="hljs-code">num_episodes = <span class="hljs-number">500</span>
</code></pre>
    <p class="normal">Define the number of time steps:</p>
    <pre class="programlisting code"><code class="hljs-code">num_timesteps = <span class="hljs-number">20000</span>
</code></pre>
    <p class="normal">Define the batch size:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">8</span>
</code></pre>
    <p class="normal">Set the number of past game screens we want to consider:</p>
    <pre class="programlisting code"><code class="hljs-code">num_screens = <span class="hljs-number">4</span>
</code></pre>
    <p class="normal">Instantiate the DQN class:</p>
    <pre class="programlisting code"><code class="hljs-code">dqn = DQN(state_size, action_size)
</code></pre>
    <p class="normal">Set done to <code class="Code-In-Text--PACKT-">False</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">done = <span class="hljs-literal">False</span>
</code></pre>
    <p class="normal">Initialize the <code class="Code-In-Text--PACKT-">time_step</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">time_step = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">For each episode:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_episodes):
</code></pre>
    <p class="normal">Set <code class="Code-In-Text--PACKT-">Return</code> to <code class="Code-In-Text--PACKT-">0</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    Return = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">Preprocess the game screen:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = preprocess_state(env.reset())
</code></pre>
    <p class="normal">For each step in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
</code></pre>
    <p class="normal">Render <a id="_idIndexMarker911"/>the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        env.render()
</code></pre>
    <p class="normal">Update the time step:</p>
    <pre class="programlisting code"><code class="hljs-code">        time_step += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Update the target network:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> time_step % dqn.update_rate == <span class="hljs-number">0</span>:
            dqn.update_target_network()
</code></pre>
    <p class="normal">Select the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = dqn.epsilon_greedy(state)
</code></pre>
    <p class="normal">Perform the selected action:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state, reward, done, _ = env.step(action)
</code></pre>
    <p class="normal">Preprocess the next state:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state = preprocess_state(next_state)
</code></pre>
    <p class="normal">Store the transition information:</p>
    <pre class="programlisting code"><code class="hljs-code">        dqn.store_transistion(state, action, reward, next_state, done)
</code></pre>
    <p class="normal">Update the current state to the next state:</p>
    <pre class="programlisting code"><code class="hljs-code">        state = next_state
</code></pre>
    <p class="normal">Update the return value:</p>
    <pre class="programlisting code"><code class="hljs-code">        Return += reward
</code></pre>
    <p class="normal">If the <a id="_idIndexMarker912"/>episode is done, then print the return:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> done:
            print(<span class="hljs-string">'Episode: '</span>,i, <span class="hljs-string">','</span> <span class="hljs-string">'Return'</span>, Return)
            <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">If the number of transitions in the replay buffer is greater than the batch size, then train the network:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> len(dqn.replay_buffer) &gt; batch_size:
            dqn.train(batch_size)
</code></pre>
    <p class="normal">By rendering the environment, we can also observe how the agent learns to play the game over a series of episodes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.9: DQN agent learning to play</p>
    <p class="normal">Now that we have learned how DQNs work and how to build a DQN to play Atari games, in the next section, we will learn an interesting variant of DQN called the double DQN. </p>
    <h1 id="_idParaDest-254" class="title">The double DQN</h1>
    <p class="normal">We have learned that in DQN, the <a id="_idIndexMarker913"/>target value is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_099.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">One of the problems with a DQN is that it tends to overestimate the Q value of the next state-action pair in the target: </p>
    <figure class="mediaobject"><img src="../Images/B15558_09_22.png" alt=""/></figure>
    <p class="normal">This overestimation is due to the presence of the max operator. Let's see how this overestimation happens with an example. Suppose we are in a state <img src="../Images/B15558_09_100.png" alt="" style="height: 1.2em;"/> and we have three actions <em class="italic">a</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub>, and <em class="italic">a</em><sub class="Subscript--PACKT-">3</sub>. Assume <em class="italic">a</em><sub class="Subscript--PACKT-">3</sub> is the optimal action in the state <img src="../Images/B15558_09_101.png" alt="" style="height: 1.2em;"/>. When we estimate the Q values of all the actions in state <img src="../Images/B15558_03_004.png" alt="" style="height: 1.2em;"/>, the estimated Q value will have some noise and differ from the actual value. Say, due to the noise, action <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub> will get a higher Q value than the optimal action <em class="italic">a</em><sub class="Subscript--PACKT-">3</sub>.</p>
    <p class="normal">We know that the target value is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_103.png" alt="" style="height: 1.76em;"/></figure>
    <p class="normal">Now, if we select the best action as the one that has the maximum value then we will end up selecting the action <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub> instead of optimal action <em class="italic">a</em><sub class="Subscript--PACKT-">3</sub>, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_104.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">So, how can we get rid of this overestimation? We can get rid of this overestimation by just modifying our target value computation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_105.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">As we can observe, now we have two Q functions in our target value computation. One Q function parameterized by the main network parameter <img src="../Images/B15558_09_106.png" alt="" style="height: 1.11em;"/> is used for action selection, and the other Q function parameterized by the target network parameter <img src="../Images/B15558_09_107.png" alt="" style="height: 1.2em;"/> is used for Q value computation.</p>
    <p class="normal">Let's <a id="_idIndexMarker914"/>understand the preceding equation by breaking it down into two steps:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Action selection</strong>: First, we compute the Q values of all the next state-action pairs using the main network parameterized by <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>, and then we select action <img src="../Images/B15558_09_109.png" alt="" style="height: 1.2em;"/>, which has the maximum Q value:<figure class="mediaobject"><img src="../Images/B15558_09_23.png" alt="" style="height: 8em;"/></figure>
      </li>
      <li class="bullet"><strong class="keyword">Q value computation</strong>: Once we have selected action <img src="../Images/B15558_09_110.png" alt="" style="height: 1.2em;"/>, then we compute the Q value using the target network parameterized by <img src="../Images/B15558_09_111.png" alt="" style="height: 1.2em;"/> for the selected action <img src="../Images/B15558_09_112.png" alt="" style="height: 1.2em;"/>:<figure class="mediaobject"><img src="../Images/B15558_09_24.png" alt="" style="height: 8em;"/></figure>
      </li>
    </ul>
    <p class="normal">Let's understand this with an example. Let's suppose state <img src="../Images/B15558_03_021.png" alt="" style="height: 1.2em;"/> is <em class="italic">E</em>, then we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_114.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">First, we compute the Q values of all actions in state <em class="italic">E</em> using the main network parameterized by <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>, and <a id="_idIndexMarker915"/>then we select the action that has the maximum Q value. Let's suppose the action that has the maximum Q value is <em class="italic">right</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_25.png" alt="" style="height: 6em;"/></figure>
    <p class="normal">Now, we can compute the Q value using the target network parameterized by <img src="../Images/B15558_09_061.png" alt="" style="height: 1.2em;"/> with the action selected by the main network, which is <em class="italic">right</em>. Thus, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_117.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Still not clear? The difference between how we compute the target value in DQN and double DQN is shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.10: Difference between a DQN and double DQN</p>
    <p class="normal">Thus, we learned that in a double DQN, we compute the target value using two Q functions. One Q function parameterized by the main network parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/> used for selecting the action that has the maximum Q value, and the other Q function parameterized by target network parameter <img src="../Images/B15558_09_059.png" alt="" style="height: 1.2em;"/> computes the Q value using the action selected by the main network:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_105.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Apart from target value computation, double DQN works exactly the same as DQN. To give us more clarity, the algorithm of double DQN is given in the next section.</p>
    <h2 id="_idParaDest-255" class="title">The double DQN algorithm</h2>
    <p class="normal">The algorithm of double DQN is <a id="_idIndexMarker916"/>shown here. As we can see, except the target value computation (the bold step), the rest of the steps are exactly the same as in the DQN: </p>
    <ol>
      <li class="numbered" value="1">Initialize the main network parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/> with random values</li>
      <li class="numbered">Initialize the target network parameter <img src="../Images/B15558_09_086.png" alt="" style="height: 1.2em;"/> by copying the main network parameter <img src="../Images/B15558_09_123.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes repeat <em class="italic">step 5</em></li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0, . . . , <em class="italic">T</em>-1:<ol>
          <li class="numbered-l2">Observe the state <em class="italic">s</em> and select an action using the epsilon-greedy policy, that is, with probability epsilon, select a random action <em class="italic">a</em> with probability 1-epsilon; select the action: <img src="../Images/B15558_09_072.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">Perform the selected action, move to the next state <img src="../Images/B15558_09_126.png" alt="" style="height: 1.2em;"/>, and obtain the reward <em class="italic">r</em></li>
          <li class="numbered-l2">Store the transition information in the replay buffer <img src="../Images/B15558_09_075.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_09_128.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2"><strong class="keyword">Compute the target value, that is, </strong><img src="../Images/B15558_09_129.png" alt="" style="height: 1.58em;"/></li>
          <li class="numbered-l2">Compute the loss, <img src="../Images/B15558_09_035.png" alt="" style="height: 3.33em;"/></li>
          <li class="numbered-l2">Compute the gradients of the loss and update the main network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> using gradient descent: <img src="../Images/B15558_09_132.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Freeze the target network parameter <img src="../Images/B15558_09_133.png" alt="" style="height: 1.2em;"/> for several time steps and then update it by just copying the main network parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/></li>
        </ol>
      </li>
    </ol>
    <p class="normal">Now that we have learned how the double DQN works, in the next section, we will learn about an interesting variant of DQN called DQN with prioritized experience replay.</p>
    <h1 id="_idParaDest-256" class="title">DQN with prioritized experience replay</h1>
    <p class="normal">We learned that in DQN, we randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <a id="_idIndexMarker917"/>and train the network. Instead of doing this, can we assign some priority to each transition in the replay buffer and <a id="_idIndexMarker918"/>sample the transitions that had high priority for learning? </p>
    <p class="normal">Yes, but first, why do we need to assign priority for the transition, and how can we decide which transition should be given more priority than the others? Let's explore this more in detail. </p>
    <p class="normal">The TD error <img src="../Images/B15558_09_135.png" alt="" style="height: 1.11em;"/> is the difference between the target value and the predicted value, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_136.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">A transition that has a high TD error implies that the transition is not correct, and so we need to learn more about that transition to minimize the error. A transition that has a low TD error implies that the transition is already good. We can always learn more from our mistakes rather than only focusing on what we are already good at, right? Similarly, we can learn more from the transitions that have a high TD error than those that have a low TD error. Thus, we can assign more priority to the transitions that have a high TD error and less priority to transitions that have a low TD error.</p>
    <p class="normal">We know that the transition information consists of <img src="../Images/B15558_09_137.png" alt="" style="height: 1.2em;"/>, and along with this information, we <a id="_idIndexMarker919"/>also add priority <em class="italic">p</em> and store the transition with the priority in our replay buffer as <img src="../Images/B15558_09_138.png" alt="" style="height: 1.2em;"/>. The <a id="_idIndexMarker920"/>following figure shows the replay buffer containing transitions along with the priority:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.11: Prioritized replay buffer</p>
    <p class="normal">In the next section, we will learn how to prioritize our transitions using the TD error based on two different types of prioritization methods. </p>
    <h2 id="_idParaDest-257" class="title">Types of prioritization</h2>
    <p class="normal">We can prioritize our transition using the following two methods:</p>
    <ul>
      <li class="bullet">Proportional prioritization</li>
      <li class="bullet">Rank-based prioritization</li>
    </ul>
    <h3 id="_idParaDest-258" class="title">Proportional prioritization</h3>
    <p class="normal">We learned that the <a id="_idIndexMarker921"/>transition can be prioritized <a id="_idIndexMarker922"/>using the TD error, so the priority <em class="italic">p</em> of the transition <em class="italic">i</em> will be just its TD error:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_139.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Note that we take the absolute value of the TD error as a priority to keep the priority positive. Okay, what about a transition that has a TD error of zero? Say we have a transition <em class="italic">i</em> and its TD error is 0, then the priority of the transition <em class="italic">i</em> will just be 0:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_140.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">But setting the priority of the transition to zero is not desirable, and if we set the priority of a transition to zero then that particular transition will not be used in our training at all. So, to avoid this issue, we will add a small value called epsilon to our TD error. So, even if the TD error is zero, we will still have a small priority due to the epsilon. To be more precise, adding an epsilon to the TD error guarantees that there will be no transition with zero priority. Thus, our priority can be modified as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_141.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Instead of having the priority as a raw number, we can convert it into a probability so that we will have priorities ranging from 0 to 1. We can convert the priority to a probability as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_142.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">The preceding equation calculates the probability <em class="italic">P</em> of the transition <em class="italic">i</em>.</p>
    <p class="normal">Can we also control the amount of prioritization? That is, instead of sampling only the prioritized transition, can we also take a random transition? Yes! To do this, we introduce a new parameter called <img src="../Images/B15558_09_143.png" alt="" style="height: 0.93em;"/> and rewrite our equation as follows. When the value of <img src="../Images/B15558_07_025.png" alt="" style="height: 0.93em;"/> is high, say 1, then we sample only the transitions that have high priority and when the value of <img src="../Images/B15558_07_025.png" alt="" style="height: 0.93em;"/> is low, say 0, then we sample a random transition:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_146.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">Thus, we have learned <a id="_idIndexMarker923"/>how to assign priority to a transition using the proportional prioritization method. In the next section, we will <a id="_idIndexMarker924"/>learn another prioritization method called rank-based prioritization.</p>
    <h3 id="_idParaDest-259" class="title">Rank-based prioritization</h3>
    <p class="normal">Rank-based prioritization is the simplest type of prioritization. Here, we assign priority based on the rank of a <a id="_idIndexMarker925"/>transition. What is the rank of a transition? The rank of a transition <em class="italic">i</em> can be defined as the location of the transition <a id="_idIndexMarker926"/>in the replay buffer where the transitions are sorted from high TD error to low TD error.</p>
    <p class="normal">Thus, we can define the priority of the transition <em class="italic">i</em> using rank as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_147.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">Just as we learned in the previous section, we convert the priority into probability:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_142.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Similar to what we learned in the previous section, we can add a parameter  to control the amount of prioritization and express our final equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_146.png" alt="" style="height: 2.51em;"/></figure>
    <h2 id="_idParaDest-260" class="title">Correcting the bias</h2>
    <p class="normal">We have learned how to prioritize the transitions using two methods—proportional prioritization and rank-based prioritization. But the problem with these methods is that we will <a id="_idIndexMarker927"/>be highly biased towards the samples that have high priority. That is, when we give more importance to samples that have a high TD error, it essentially means that we are learning only from a subset of samples that have a high TD error.</p>
    <p class="normal">Okay, what's the issue with this? It will lead to the problem of overfitting, and our agent will be highly biased to those transitions that have a high TD error. To combat this, we use importance weights <em class="italic">w</em>. The importance weights help us to reduce the weights of transitions that have occurred many times. The importance weight <em class="italic">w</em> of the transition <em class="italic">i</em> can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_150.png" alt="" style="height: 2.78em;"/></figure>
    <p class="normal">In the preceding expression, <em class="italic">N</em> denotes the length of our replay buffer and <em class="italic">P</em>(<em class="italic">i</em>) denotes the probability of the transition <em class="italic">i</em>. Okay, what's that parameter <img src="../Images/B15558_09_151.png" alt="" style="height: 1.11em;"/>? It controls the importance weight. We start off with small values of <img src="../Images/B15558_09_152.png" alt="" style="height: 1.11em;"/>, from 0.4 and anneal it toward 1.</p>
    <p class="normal">Thus, in this section, we have learned how to prioritize transitions in DQN with prioritized experience replay. In the next section, we will learn about another interesting variant of DQN called dueling DQN.</p>
    <h1 id="_idParaDest-261" class="title">The dueling DQN</h1>
    <p class="normal">Before going <a id="_idIndexMarker928"/>ahead, let's learn about one of the most important functions in reinforcement learning, called <a id="_idIndexMarker929"/>the advantage function. The advantage function is defined as the difference between the Q function and the value function, and it is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_153.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Okay, but what's the use of an advantage function? What does it signify? First, let's recall the Q function and the value function:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Q function</strong>: The Q function gives the expected return an agent would obtain starting from state <em class="italic">s</em>, performing action <em class="italic">a</em>, and following the policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/>.</li>
      <li class="bullet"><strong class="keyword">Value function</strong>: The value function gives the expected return an agent would obtain starting from state <em class="italic">s</em> and following the policy <img src="../Images/B15558_04_032.png" alt="" style="height: 0.84em;"/>.</li>
    </ul>
    <p class="normal">Now if we think intuitively, what's the difference between the Q function and the value function? The Q function gives us the value of a state-action pair, while the value function gives the value of a state irrespective of the action. Now, the difference between the Q function and the value function tells us how good the action <em class="italic">a</em> is compared to the average actions in state <em class="italic">s</em>.</p>
    <p class="normal">Thus, the advantage function tells us that in state <em class="italic">s</em>, how good the action <em class="italic">a</em> is compared to the average actions. Now <a id="_idIndexMarker930"/>that we have understood what the advantage function is, let's see why and how we can make use of it in the DQN.</p>
    <h2 id="_idParaDest-262" class="title">Understanding the dueling DQN</h2>
    <p class="normal">We have learned that in a DQN, we feed the state as input and our network computes the Q value for all <a id="_idIndexMarker931"/>actions in that state. Instead of computing the Q values in this way, can we compute the Q values using the advantage function? We have learned that the advantage function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_153.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">We can rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_157.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">As we can see from the preceding equation, we can compute the Q value just by adding the value function and the advantage function together. Wait! Why do we have to do this? What's wrong with computing the Q value directly?</p>
    <p class="normal">Let's suppose we are in some state <em class="italic">s</em> and we have 20 possible actions to perform in this state. Computing the Q values of all these 20 actions in state <em class="italic">s</em> is not going to be useful because most of the actions will not have any effect on the state, and also most of the actions will have a similar Q value. What do we mean by that? Let's understand this with the grid world environment shown in <em class="italic">Figure 9.12</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.12: Grid world environment</p>
    <p class="normal">As we can see, the agent is in state <strong class="keyword">A</strong>. In this case, what is the use of computing the Q value of the action <em class="italic">up</em> in state <strong class="keyword">A</strong>? Moving <em class="italic">up</em> will have no effect in state <strong class="keyword">A</strong>, and it's not going to take the agent <a id="_idIndexMarker932"/>anywhere. Similarly, think of an environment where our action space is huge, say 100. In this case, most of the actions will not have any effect in the given state. Also, when the action space is large, most of the actions will have a similar Q value.</p>
    <p class="normal">Now, let's talk about the value of a state. Note that not all the states are important for an agent. There could be a state that always gives a bad reward no matter what action we perform. In that case, it is not useful to compute the Q value of all possible actions in the state if we know that the state is always going to give us a bad reward. </p>
    <p class="normal">Thus, to solve this we can compute the Q function as the sum of the value function and the advantage function. That is, with the value function, we can understand whether a state is valuable or not without computing the values of all actions in the state. And with the advantage function, we can understand whether an action is really good or it just gives us the same value as all the other actions.</p>
    <p class="normal">Now that we have a basic idea of dueling DQN, let's explore the architecture of dueling DQN in the next section.</p>
    <h2 id="_idParaDest-263" class="title">The architecture of a dueling DQN</h2>
    <p class="normal">We have <a id="_idIndexMarker933"/>learned that in a dueling DQN, the Q values can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_157.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">How can we design our neural network to emit Q values in this way? We can break the final layer of our network into two streams. The first stream computes the value function and the second stream computes the advantage function. Given any state as input, the value stream gives the value of a state, while the advantage stream gives the advantage of all possible actions in the given state. For instance, as <em class="italic">Figure 9.13 </em>shows, we feed the game state (game screen) as an input to the network. The value stream computes the value of a state while the advantage stream computes the advantage values of all actions in the state:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.13: Architecture of a dueling DQN</p>
    <p class="normal">We learned that we compute the Q value by adding the state value and the advantage value together, so we combine the value stream and the advantage stream using another layer called the aggregate layer, and compute the Q value as <em class="italic">Figure 9.14</em> shows. Thus, the value stream computes the state value, the advantage stream computes the advantage value, and the aggregate layer sums these streams and computes the Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.14: Architecture of a dueling DQN including an aggregate layer</p>
    <p class="normal">But there is a small issue here. Just summing the state value and advantage value in the aggregate layer and computing the Q value leads us to a problem of identifiability. </p>
    <p class="normal">So, to combat this problem, we make the advantage function to have zero advantage for the selected action. We can achieve <a id="_idIndexMarker934"/>this by subtracting the average advantage value, that is, the average advantage of all actions in the action space, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_159.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_09_160.png" alt="" style="height: 1.11em;"/> denotes the length of the action space.</p>
    <p class="normal">Thus, we can write our final equation for computing the Q value with parameters as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_161.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">In the preceding equation, <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> is the parameter of the convolutional network, <img src="../Images/B15558_06_016.png" alt="" style="height: 1.11em;"/> is the parameter of the value stream, and <img src="../Images/B15558_09_143.png" alt="" style="height: 0.93em;"/> is the parameter of the advantage stream. After computing the Q value, we can select the action:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_165.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Thus, the only difference between a dueling DQN and DQN is that in a dueling DQN, instead of computing the Q <a id="_idIndexMarker935"/>values directly, we compute them by combining the state value and the advantage value.</p>
    <p class="normal">In the next section, we will explore another variant of DQN called deep recurrent Q network.</p>
    <h1 id="_idParaDest-264" class="title">The deep recurrent Q network</h1>
    <p class="normal"><strong class="keyword">The deep recurrent Q network</strong> (<strong class="keyword">DRQN</strong>) is just the same as a DQN but with recurrent layers. But <a id="_idIndexMarker936"/>what's the use of recurrent layers in DQN? To answer this question, first, let's understand the problem called <strong class="keyword">Partially Observable Markov Decision Process </strong>(<strong class="keyword">POMDP</strong>).</p>
    <p class="normal">An <a id="_idIndexMarker937"/>environment is called a POMDP when we have a limited set of information available about the environment. So far, in the previous chapters, we have seen a fully observable MDP where we know all possible actions and states—although we might be unaware of transition and reward probabilities, we had complete knowledge of the environment. For example, in the frozen lake environment, we had complete knowledge of all the states and actions of the environment.</p>
    <p class="normal">But most real-world environments are only partially observable; we cannot see all the states. For instance, consider an agent learning to walk in a real-world environment. In this case, the agent will not have complete knowledge of the environment (the real world); it will have no information outside its view. </p>
    <p class="normal">Thus, in a POMDP, states provide only partial information, but keeping the information about past states in the memory will help the agent to understand more about the nature of the environment and find the optimal policy. Thus, in POMDP, we need to retain information about past states in order to take the optimal action.</p>
    <p class="normal">So, can we take advantage of recurrent neural networks to understand and retain information about the past states as long as it is required? Yes, the <strong class="keyword">Long Short-Term Memory Recurrent Neural Network</strong> (<strong class="keyword">LSTM RNN</strong>) is very useful for retaining, forgetting, and updating the information as required. So, we can use the LSTM layer in the DQN to retain <a id="_idIndexMarker938"/>information about the past states as long as it is required. Retaining information about the past states helps when we have the problem of POMDP. </p>
    <p class="normal">Now that we have a basic <a id="_idIndexMarker939"/>understanding of why we need DRQN and how it solves the problem of POMDP, in the next section, we will look into the architecture of DRQN.</p>
    <h2 id="_idParaDest-265" class="title">The architecture of a DRQN</h2>
    <p class="normal"><em class="italic">Figure 9.15</em> shows the <a id="_idIndexMarker940"/>architecture of a DRQN. As we can see, it is similar to the DQN architecture except that it has an LSTM layer:</p>
    <figure class="mediaobject"><img src="../Images/B15558_09_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.15: Architecture of a DRQN</p>
    <p class="normal">We pass the game screen as an input to the convolutional layer. The convolutional layer convolves the image and produces a feature map. The resulting feature map is then passed to the LSTM layer. The LSTM layer has memory to hold information. So, it retains information about important previous game states and updates its memory over time steps as required. Then, we feed the hidden state from the LSTM layer to the fully connected layer, which outputs the Q value.</p>
    <p class="normal"><em class="italic">Figure 9.16</em> helps us to understand how exactly DRQN works. Let's suppose we need to compute the Q value for the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> and the action <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>. Unlike DQN, we don't just compute the Q value as Q(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) directly. As we can see, along with the current state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> we also use the hidden state <em class="italic">h</em><sub class="" style="font-style: italic;">t</sub> to compute the Q value. The reason for using the hidden state is that it holds information about the past game states in memory. </p>
    <p class="normal">Since we are using the LSTM cells, the hidden state <em class="italic">h</em><sub class="" style="font-style: italic;">t</sub> will consist of information about the past game states in the memory as long as it is required: </p>
    <figure class="mediaobject"><img src="../Images/B15558_09_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.16: Architecture of DRQN</p>
    <p class="normal">Except for this change, DRQN works just like DQN. Wait. What about the replay buffer? In DQN, we learned that we store the transition information in the replay buffer and train our <a id="_idIndexMarker941"/>network by sampling a minibatch of experience. We also learned that the transition information is placed sequentially in the replay buffer one after another, so to avoid the correlated experience, we randomly sample a minibatch of experience from the replay buffer and train the network.</p>
    <p class="normal">But in the case of a DRQN, we need sequential information so that our network can retain information from past game states. Thus we need sequential information but also we don't want to overfit the network by training with correlated experience. How can we achieve this?</p>
    <p class="normal">To achieve this, in a DRQN, we randomly sample a minibatch of episodes rather than a random minibatch of transitions. That is, we know that in the episode, we will have transition information that follows sequentially, so we take a random minibatch of episodes and in each episode we will have the transition information that is placed sequentially. So, in this way, we can accommodate both randomization and also the transition information <a id="_idIndexMarker942"/>that follows one another. This is called<strong class="keyword"> bootstrapped sequential updates</strong>.</p>
    <p class="normal">After <a id="_idIndexMarker943"/>sampling the minibatch of episodes randomly, then we can train the DRQN just like we trained the DQN network by minimizing the MSE loss. To learn more, you can refer to the DRQN paper given in the <em class="italic">Further reading</em> section.</p>
    <h1 id="_idParaDest-266" class="title">Summary</h1>
    <p class="normal">We started the chapter by learning what deep Q networks are and how they are used to approximate the Q value. We learned that in a DQN, we use a buffer called the replay buffer to store the agent's experience. Then, we randomly sample a minibatch of experience from the replay buffer and train the network by minimizing the MSE. Moving on, we looked at the algorithm of DQN in more detail, and then we learned how to implement DQN to play Atari games.</p>
    <p class="normal">Following this, we learned that the DQN overestimates the target value due to the max operator. So, we used double DQN, where we have two Q functions in our target value computation. One Q function parameterized by the main network parameter <img src="../Images/B15558_09_042.png" alt="" style="height: 1.11em;"/> is used for action selection, and the other Q function parameterized by the target network parameter <img src="../Images/B15558_09_059.png" alt="" style="height: 1.2em;"/> is used for Q value computation.</p>
    <p class="normal">Going ahead, we learned about the DQN with prioritized experience replay, where the transition is prioritized based on the TD error. We explored two different types of prioritization methods called proportional prioritization and rank-based prioritization. </p>
    <p class="normal">Next, we learned about another interesting variant of DQN called dueling DQN. In dueling DQN, instead of computing Q values directly, we compute them using two streams called the value stream and the advantage stream.</p>
    <p class="normal">At the end of the chapter, we learned about DRQN and how they solve the problem of partially observable Markov decision processes.</p>
    <p class="normal">In the next chapter, we will learn about another popular algorithm called policy gradient.</p>
    <h1 id="_idParaDest-267" class="title">Questions</h1>
    <p class="normal">Let's evaluate our understanding of DQN and its variants by answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">Why do we need a DQN?</li>
      <li class="numbered">What is the replay buffer?</li>
      <li class="numbered">Why do we need the target network?</li>
      <li class="numbered">How does a double DQN differ from a DQN?</li>
      <li class="numbered">Why do we have to prioritize the transitions? </li>
      <li class="numbered">What is the advantage function?</li>
      <li class="numbered">Why do we need LSTM layers in a DRQN?</li>
    </ol>
    <h1 id="_idParaDest-268" class="title">Further reading</h1>
    <p class="normal">For more information, we can refer to the following papers:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Playing Atari with Deep Reinforcement Learning</strong> by <em class="italic">Volodymyr Mnih, et al.</em>, <a href="https://arxiv.org/pdf/1312.5602.pdf"><span class="url">https://arxiv.org/pdf/1312.5602.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Deep Reinforcement Learning with Double Q-learning</strong> by <em class="italic">Hado van Hasselt, Arthur Guez, David Silver</em>, <a href="https://arxiv.org/pdf/1509.06461.pdf"><span class="url">https://arxiv.org/pdf/1509.06461.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Prioritized Experience Replay</strong> by <em class="italic">Tom Schaul, John Quan, Ioannis Antonoglou and David Silver</em>, <a href="https://arxiv.org/pdf/1511.05952.pdf"><span class="url">https://arxiv.org/pdf/1511.05952.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Dueling Network Architectures for Deep Reinforcement Learning</strong> by <em class="italic">Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas</em>, <a href="https://arxiv.org/pdf/1511.06581.pdf"><span class="url">https://arxiv.org/pdf/1511.06581.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Deep Recurrent Q-Learning for Partially Observable MDPs</strong> by <em class="italic">Matthew Hausknecht and Peter Stone</em>, <a href="https://arxiv.org/pdf/1507.06527.pdf"><span class="url">https://arxiv.org/pdf/1507.06527.pdf</span></a></li>
    </ul>
  </div>
</body></html>