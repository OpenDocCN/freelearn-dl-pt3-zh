- en: '*Chapter 7*: Captioning Images with CNNs and RNNs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Equipping neural networks with the ability to describe visual scenes in a human-readable
    fashion has to be one of the most interesting yet challenging applications of
    deep learning. The main difficulty arises from the fact that this problem combines
    two major subfields of artificial intelligence: **Computer Vision** (**CV**) and
    **Natural Language Processing** (**NLP**).'
  prefs: []
  type: TYPE_NORMAL
- en: The architectures of most image captioning networks use a **Convolutional Neural
    Network** (**CNN**) to encode images in a numeric format so that they're suitable
    for the consumption of the decoder, which is typically a **Recurrent Neural Network**
    (**RNN**). This is a kind of network specialized in learning from sequential data,
    such as time series, video, and text.
  prefs: []
  type: TYPE_NORMAL
- en: As we'll see in this chapter, the challenges of building a system with these
    capabilities start with preparing the data, which we'll cover in the first recipe.
    Then, we'll implement an image captioning solution from scratch. In the third
    recipe, we'll use this model to generate captions for our own pictures. Finally,
    in the fourth recipe, we'll learn how to include an attention mechanism in our
    architecture so that we can understand what parts of the image the network is
    looking at when generating each word in the output caption.
  prefs: []
  type: TYPE_NORMAL
- en: Pretty interesting, don't you agree?
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we''ll cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a reusable image caption feature extractor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an image captioning network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating captions for your own photos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an image captioning network on COCO with attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Image captioning is a problem that requires vast amounts of resources in terms
    of memory, storage, and computing power. My recommendation is that you use a cloud-based
    solution such as AWS or FloydHub to run the recipes in this chapter unless you
    have sufficiently capable hardware. As expected, a GPU is of paramount importance
    to complete the recipes in this chapter. In the *Getting ready* section of each
    recipe, you''ll find what you''ll need to prepare. The code of this chapter is
    available here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3qmpVme](https://bit.ly/3qmpVme).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a reusable image caption feature extractor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step of creating an image captioning, deep learning-based solution
    is to transform the data into a format that can be used by certain networks. This
    means we must encode images as vectors, or tensors, and the text as embeddings,
    which are vectorial representations of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement a customizable and reusable component that
    will allow us to preprocess the data we'll need to implement an image captioner
    beforehand, thus saving us tons of time later on in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dependencies we need are `tqdm` (to display a nice progress bar) and `Pillow`
    (to load and manipulate images using TensorFlow''s built-in functions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will use the `Flickr8k` dataset, which is available on `~/.keras/datasets/flickr8k`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sample images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Sample images from Flickr8k'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_07_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Sample images from Flickr8k
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are good to go!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create a reusable feature extractor for image captioning
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `ImageCaptionFeatureExtractor` class and its constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must receive the path where the outputs will be stored, along with
    the tokens that we''ll use to delimit the start and end of a text sequence. We
    must also take the input shape of the feature extractor as an argument. Next,
    let''s store these values as members:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we don''t receive any `feature_extractor`, we''ll use `VGG16` by default.
    Next, define a public method that will extract the features from an image, given
    its path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to clean the captions, we must get rid of all the punctuation characters
    and single-letter words (such as *a*). The `_clean_captions()` method performs
    this task, and also adds special tokens; that is, `self.start_token` and `self.end_token`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to compute the length of the longest caption, which we can do
    with the `_get_max_seq_length()` method. This is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a public method, `extract_features()`, which receives a list of image
    paths and captions and uses them to extract features from both the images and
    text sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that both lists must be of the same size. The next step is to clean the
    captions, compute the maximum sequence length, and fit a tokenizer to all the
    captions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll iterate over each image path and caption pair, extracting the features
    from the image. Then, we''ll save an entry in our `data_mapping` `dict`, associating
    the image ID (present in `image_path`) with the corresponding visual features
    and clean caption:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll save this `data_mapping` to disk, in pickle format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll complete this method by creating and storing the sequences that''ll
    be inputted to an image captioning network in the future:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following method creates the input and output sequences that will be used
    to train an image captioning model (see the *How it works…* section for a deeper
    explanation). We will start by determining the number of output classes, which
    is the vocabulary size plus one (to account for out-of-vocabulary tokens). We
    must also define the lists where we''ll store the sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll iterate over each features-caption pair. We will transform the
    caption from a string into a sequence of numbers that represents the words in
    the sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll generate as many input sequences as there are words in a caption.
    Each input sequence will be used to generate the next word in the sequence. Therefore,
    for a given index, `i`, the input sequence will be all the elements up to `i-1`,
    while the corresponding output sequence, or label, will be the one-hot encoded
    element at `i` (the next word). To ensure all the input sequences are the same
    length, we must pad them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then add the visual feature vector, the input sequence, and the output sequence
    to the corresponding lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must write the sequences to disk, in pickle format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the paths to the `Flickr8k` images and captions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of the feature extractor class we just implemented:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'List all the image files in the `Flickr8k` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the contents of the captions file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must create a map that will associate each image with multiple captions.
    The key is the image ID, while the value is a list of all captions associated
    with such an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will only keep one caption per image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must use our extractor to produce the data mapping and corresponding
    input sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This process may take a while. After several minutes, we should see the following
    files in the output path:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We'll see how this all works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned that one of the keys to creating a good image captioning
    system is to put the data in a suitable format. This allows the network to learn
    how to describe, with text, what's happening in a visual scenario.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to frame an image captioning problem, but the most popular
    and effective way is to use each word to generate the next word in the caption.
    This way, we'll construct the sentence, word by word, passing each intermediate
    output as the input to the next cycle. (This is how **RNNs** work. To read more
    about them, refer to the *See also* section.)
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering how we pass the visual information to the network. This
    is where the feature extraction step is crucial, because we convert each image
    in our dataset into a numeric vector that summarizes the spatial information in
    each picture. Then, we pass the same feature vector along each input sequence
    when training the network. This way, the network will learn to associate all the
    words in a caption with the same image.
  prefs: []
  type: TYPE_NORMAL
- en: If we're not careful, we could get trapped in an endless loop of word generation.
    How can we prevent this? By using a special token to signal the end of a sequence
    (this means the network should stop producing words when it encounters such a
    token). In our case, this token is, by default, `endsequence`.
  prefs: []
  type: TYPE_NORMAL
- en: A similar problem is how to start a sequence. Which word should we use? In this
    case, we must also resort to a special token (our default is `beginsequence`).
    This acts as a seed that the network will use to start producing captions.
  prefs: []
  type: TYPE_NORMAL
- en: All of this might sound confusing now, and that's because we've only focused
    on the data preprocessing stage. In the remaining recipes of this chapter, we'll
    leverage the work we've done here to train many different image captioners, and
    all the pieces will fall into place!
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s a great explanation of how **RNNs** work: [https://www.youtube.com/watch?v=UNmqTiOnRfg](https://www.youtube.com/watch?v=UNmqTiOnRfg).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an image captioning network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An image captioning architecture is comprised of an encoder and a decoder. The
    encoder is a **CNN** (typically a pre-trained one), which converts input images
    into numeric vectors. These vectors are then passed, along with text sequences,
    to the decoder, which is an **RNN**, that will learn, based on these values, how
    to iteratively generate each word in the corresponding caption.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll implement an image captioner that's been trained on the
    `Flickr8k` dataset. We'll leverage the feature extractor we implemented in the
    *Implementing a reusable image caption feature extractor* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The external dependencies we''ll be using in this recipe are `Pillow`, `nltk`,
    and `tqdm`. You can install them all at once with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will use the `Flickr8k` dataset, which you can get from `~/.keras/datasets/flickr8k`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some sample images from the `Flickr8k` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Sample images from Flickr8k'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_07_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Sample images from Flickr8k
  prefs: []
  type: TYPE_NORMAL
- en: Let's head over to the next section to start this recipe's implementation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to implement a deep learning-based image captioning system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import all of the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the paths to the images and captions, as well as the output path, which
    is where we''ll store the artifacts that will be created in this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load a list of image paths and their corresponding
    captions. This implementation is similar to *Steps* *20* through *22* of the *Implementing
    a reusable image caption feature extractor* recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile all the captions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will build the architecture of the network, which receives
    the vocabulary size, the maximum sequence length, and the encoder''s input shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first part of the network receives the feature vectors and passes them
    through a fully connected `ReLU` activated layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second part of the layer receives the text sequences, transformed into
    numeric vectors, and trains an embedding of 256 elements. Then, it passes that
    embedding to an `LSTM` layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We concatenate the outputs of these two parts and pass the concatenation through
    a fully connected network, with an output layer with as many units as there are
    words in our vocabulary. By `Softmax` activating this output, we get a one-hot
    encoded vector that corresponds to a word in the vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we build the model, passing the image features and text sequences
    as inputs, and outputting the one-hot encoded vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will convert an integer index into a word by using the
    tokenizer''s internal mapping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will produce a caption. It will start by feeding the
    `beginsequence` token to the network, which will iteratively construct the sentence
    until the maximum sequence length is reached, or the `endsequence` token is encountered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will evaluate the model''s performance. First, we''ll
    produce a caption for each feature corresponding to an image in the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll compute the **BLEU** score using different weights. Although the
    **BLEU** score is outside the scope of this recipe, you can find an excellent
    article that explains it in depth in the *See also* section. All you need to know
    is that it''s used to measure how well a generated caption compares to a set of
    reference captions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the image paths and captions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the image extractor model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the image caption feature extractor (passing the regular image extractor
    we created in *Step 15*) and use it to extract the sequences from the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the pickled input and output sequences we created in *Step 16*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use 80% of the data for training and 20% for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate and compile the model. Because, in the end, this is a multi-class
    classification problem, we''ll use `categorical_crossentropy` as our loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because the training process is so resource-intensive and the network tends
    to give the best results early on, let''s create a `ModelCheckpoint` callback
    that will store the model with the lowest validation loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model over 30 epochs. Notice that we must pass two set of inputs or
    features, but only a set of labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the best model. This may vary from run to run, but in this recipe, it''s
    stored in the `model-ep003-loss3.847-val_loss4.328.h5` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data mapping, which contains all the features paired with the ground
    truth captions. Extract the features and mappings into separate collections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This step might take a while. In the end, you''ll see an output similar to
    this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training an image captioner is not an easy task. However, by executing the proper
    steps, in the correct order, we were able to create a fairly capable one that
    performed well on the test set, based on the **BLEU** score shown in the preceding
    code block. Head over to the next section to see how it all works!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented an image captioning network from scratch. Although
    this might seem complicated at first, we must remember it is a variation of an
    encoder-decoder architecture, similar to the ones we studied in [*Chapter 5*](B14768_05_Final_JM_ePub.xhtml#_idTextAnchor177),
    *Reducing Noise with Autoencoders*, and [*Chapter 6*](B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214),
    *Generative Models and Adversarial Attacks*.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the encoder is just a fully connected and shallow network that
    maps the features we extracted from the pre-trained model on ImageNet, to a vector
    of 256 elements.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the decoder, instead of using transposed convolutions, uses
    an **RNN** that receives both text sequences (mapped to numeric vectors) and image
    features, concatenated into a long sequence of 512 elements.
  prefs: []
  type: TYPE_NORMAL
- en: The network is trained so that it learns to predict the next word in a sentence,
    given all the words it generated in previous time steps. Note that in each cycle,
    we pass the same feature vector that corresponds to the image, so the network
    learns to map certain words, in a particular order, to describe the visual data
    encoded in such a vector.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the network is one-hot encoded, which means that only the position
    its corresponding to the words the network believes should come next in the sentence
    contains a 1, while the remaining positions contain a 0.
  prefs: []
  type: TYPE_NORMAL
- en: "To generate captions, we follow a similar process. Of course, we somehow need\
    \ to tell the model to start producing words. With this in mind, we pass the `beginsequence`\
    \ token to the network and iterate until we reach the maximum sequence length,\
    \ or the model outputs an `endsequence` token. Remember, we take the output of\
    \ each iteration and \Luse it as input for the next cycle."
  prefs: []
  type: TYPE_NORMAL
- en: This might seem confusing and cumbersome at first, but you now have the building
    blocks you need to tackle any image captioning problem!
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s an excellent read if you wish to fully understand the **BLEU** score:
    [https://machinelearningmastery.com/calculate-bleu-score-for-text-python/](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).'
  prefs: []
  type: TYPE_NORMAL
- en: Generating captions for your own photos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a good image captioning system is only one part of the equation. To
    actually use it, we must perform a series of steps, akin to the ones we executed
    during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll use a trained image captioning network to produce textual
    descriptions of new images.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we don't need external dependencies for this particular recipe, we
    need access to a trained image captioning network, along with the cleaned captions
    that will be used to fit it. I highly recommend that you complete the *Implementing
    a reusable image caption feature extractor* and *Implementing an image captioning
    network* recipes before tackling this one.
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready? Let's start captioning!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow this series of steps to produce captions for your own images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, let''s begin by importing the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will translate an integer index into the corresponding
    word using the tokenizer''s mapping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `produce_caption()` function, which takes the captioning model,
    the tokenizer, an image to describe, and the maximum sequence length to generate
    a textual description of the input visual scene:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we must keep generating words until we either encounter the `endsequence`
    token or we reach the maximum sequence length.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a pre-trained **VGG16** network, which we''ll use as our image feature
    extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the image extractor to an instance of `ImageCaptionFeatureExtractor()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the cleaned captions we used to train the model. We need them to fit the
    tokenizer in *Step 7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `Tokenizer()` and fit it to all the captions. Also, compute the
    maximum sequence length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the trained network (in this case, the name of the network is `model-ep003-loss3.847-val_loss4.328.h5`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate over all the test images in the current location, extracting the corresponding
    numeric features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Produce the caption and remove the `beginsequence` and `endsequence` special
    tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open the image, add the generated caption as its title, and save it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s an image where the network does a very good job of generating a proper
    caption:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.3 – We can see that the caption is very close to what''s actually
    happening'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_07_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – We can see that the caption is very close to what's actually happening
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s another example where the network is technically correct, although
    it could be more precise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – A football player in a red uniform is, indeed, in the air, but
    there is more going on'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_07_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – A football player in a red uniform is, indeed, in the air, but
    there is more going on
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, here''s an instance where the network is clueless:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – The network couldn''t describe this scene'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_07_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – The network couldn't describe this scene
  prefs: []
  type: TYPE_NORMAL
- en: With that, we've seen that our model does well on some images, but still has
    room for improvement. We'll dive a bit deeper in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we learned that image captioning is a difficult problem that
    heavily depends on many factors. Some of these factors are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A well-trained **CNN** to extract high-quality visual features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rich set of descriptive captions for each image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings with enough capacity to encode the expressiveness of the vocabulary
    with minimal loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A powerful **RNN** to learn how to put all of this together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Despite these clear challenges, in this recipe, we used a trained network on
    the `Flickr8k` dataset to generate captions for new images. The process we followed
    is similar to the one we implemented to train the system in that, first, we must
    go from an image to a feature vector. Then, we must fit a tokenizer to our vocabulary
    to get a proper mechanism so that we can go from sequences to human-readable words.
    Finally, we assemble the captions one word at a time, passing the image features
    along with the sequence we''ve built so far. How do we know when to stop, though?
    We have two stopping criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: The caption reached the maximum sequence length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network encountered the `endsequence` token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we tested our solution on several images, with varied results. In some
    instances, the network is capable of producing very precise descriptions, while
    on other occasions, it generates somewhat vague captions. It also missed the mark
    completely in the last example, which is a clear indication of how much room for
    improvement there is.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to take a look at other captioned images, consult the official
    repository: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7/recipe3](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7/recipe3).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an image captioning network on COCO with attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A great way to understand how an image captioning network generates its descriptions
    is by adding an attention component to the architecture. This lets us appreciate
    what parts of the photo a network was looking at when it generated each word.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll train an end-to-end image captioning system on the more
    challenging **Common Objects in Context** (**COCO**) dataset. We'll also equip
    our network with an attention mechanism to improve its performance and to help
    us understand its inner reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: This is a long and advanced recipe, but don't panic! We'll go step by step.
    If you want to dive deeper into the theory that supports this implementation,
    take a look at the *See also* section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although we''ll be using the `COCO` dataset, you don''t need to do anything
    beforehand, because we''ll download it as part of the recipe (however, you can
    read more about this seminal dataset here: https://cocodataset.org/#home).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample from the `COCO` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Sample images from COCO'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_07_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Sample images from COCO
  prefs: []
  type: TYPE_NORMAL
- en: Let's get to work!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load an image. It must return both the image and
    its path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will get the maximum sequence length. This will be useful
    later on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define for the image captioning network a function that will load an image
    from disk (stored in `NumPy` format):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement **Bahdanau''s Attention** using model subclassing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous block defined the network layers. Now, let''s define the forward
    pass inside the `call()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the image encoder. This is just a `ReLU`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the decoder. This is an `GRU` and attention to learn how to produce
    captions from the visual feature vectors and the text input sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we''ve defined the layers in the **RNN** architecture, let''s implement
    the forward pass. First, we must pass the inputs through the attention sub-network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must pass the input sequence (`x`) through the embedding layer and
    concatenate it with the context vector we received from the attention mechanism:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must pass the merged tensor to the `GRU` layer, and then through the
    dense layers. This returns the output sequence, the state, and the attention weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must define a method that will reset the hidden state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `ImageCaptionerClass`. The constructor instantiates the basic components,
    which are the encoder, the decoder, the tokenizer, and the optimizer and loss
    functions needed to train the whole system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a method that will compute the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define a function that will perform a single training step. We will start
    by creating the hidden state and the input, which is just a batch of singleton
    sequences containing the index of the `<start>` token, a special element used
    to signal the beginning of a sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must encode the image tensor. Then, we''ll iteratively pass the resulting
    features to the decoder, along with the outputted sequence so far, and the hidden
    state. For a deeper explanation on how **RNNs** work, head to the *See also* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice in the previous block that we computed the loss at each time step. To
    get the total loss, we must calculate the average. For the network to actually
    learn, we must backpropagate the total loss by computing the gradients and applying
    them via the optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last method in this class is in charge of training the system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Every 100 epochs, we''ll print the loss. At the end of each epoch, we will
    also print the epoch loss and elapsed time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download and unzip the `COCO` dataset''s annotation files. If they''re already
    in the system, just store the file path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download and unzip the `COCO` dataset''s image files. If they''re already in
    the system, just store the file path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the image paths and the captions. We must add the special `<start>` and
    `<end>` tokens to each caption so that they''re in our vocabulary. These special
    tokens let us specify where a sequence begins and ends, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: "Because `COCO` is massive, and it would take ages to train a model on it, we'll\
    \ select \La random sample of 30,000 images, along with their captions:"
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s use a pre-trained instance of `InceptionV3` as our image feature extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `tf.data.Dataset` that maps image paths to tensors. Use it to go over
    all the images in our sample, convert them into feature vectors, and save them
    as `NumPy` arrays. This will allow us to save memory in the future:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train a tokenizer on the top 5,000 words in our captions. Then, convert each
    text into a numeric sequence and pad them so that they are all the same size.
    Also, compute the maximum sequence length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll use 20% of the data to test our model and the remaining 80% to train
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll load batches of 64 images (along with their captions) at a time. Notice
    that we''re using the `load_image_and_caption()` function, defined in *Step 5*,
    which reads the feature vector corresponding to the images, stored in `NumPy`
    format. Moreover, because this function works at the `NumPy` level, we must wrap
    it with `tf.numpy_function` so that it can be used as a valid TensorFlow function
    within the `map()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s instantiate an `ImageCaptioner`. The embeddings will have 256 elements,
    and the number of units for the decoder and the attention model will be 512\.
    The vocabulary size is 5,001\. Finally, we must pass the fitted tokenizer from
    *Step 27*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will evaluate the image captioner on an image. It must
    receive the encoder, the decoder, the tokenizer, the image to caption, the maximum
    sequence length, and the shape of the attention vector. We will start by creating
    a placeholder array, which is where we''ll store the subplots that comprise the
    attention plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must initialize the hidden state, extract the features from the input
    image, and pass them to the encoder. We must also initialize the decoder input
    by creating a singleton sequence with the `<start>` token index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s build the caption until we reach the maximum sequence length or
    encounter the `<end>` token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that for each word, we update `attention_plot` with the weights returned
    by the decoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s define a function that will plot the attention the network pays to each
    word in the caption. It receives the image, a list of the individual words that
    comprise the caption (`result`), `attention_plot` returned by `evaluate()`, and
    the output path where we''ll store the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll iterate over each word to create a subplot of the corresponding attention
    graph, titled with the specific word it''s linked to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can save the full plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the network on a random image from the validation set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build and clean the actual (ground truth) caption:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the caption for the validation image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build and clean the predicted caption:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the ground truth and generated captions, and then save the attention
    plot to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following code block, we can appreciate the similarity between the real
    caption and the one outputted by our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s take a look at the attention plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Attention plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_07_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Attention plot
  prefs: []
  type: TYPE_NORMAL
- en: Take note of the areas the network looked at when generating each word in the
    caption. Lighter squares mean that more attention was paid to those pixels. For
    instance, to produce the word *giraffe*, the network looked at the surroundings
    of the giraffe in the photo. Also, we can see that when the network generated
    the word *grass*, it looked at the giraffe legs, which have a grass portion behind
    them. Isn't that amazing?
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at this in more detail in the *How it works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we implemented a more complete image captioning system, this
    time on the considerably more challenging `COCO` dataset, which is not only several
    orders of magnitude bigger than `Flickr8k`, but much more varied and, therefore,
    harder for the network to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, we gave our network an advantage by providing it with an attention
    mechanism, inspired by the impressive breakthrough proposed by Dzmitry Bahdanau
    (take a look at the *See also* section for more details). This capability gives
    the model the power to perform a soft search for parts of the source caption that
    are relevant to predicting a target word or simply put, producing the best next
    word in the output sentence. Such an attention mechanism works as an advantage
    over the traditional approach, which consists of using a fixed-length vector (as
    we did in the *Implementing an image captioning network* recipe) from which the
    decoder generates the output sentence. The problem with such a representation
    is that it tends to act as a bottleneck when it comes to improving performance.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the attention mechanism allows us to understand how the network thinks
    to produce captions in a more intuitive way.
  prefs: []
  type: TYPE_NORMAL
- en: Because neural networks are complex pieces of software (often akin to a black
    box), using visual techniques to inspect their inner workings is a great tool
    at our disposal that can aid us in the training, fine-tuning, and optimization
    process.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we implemented our architecture using the Model Subclassing
    pattern, which you can read more about here: [https://www.tensorflow.org/guide/keras/custom_layers_and_models](https://www.tensorflow.org/guide/keras/custom_layers_and_models).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following link for a great refresher on **RNNs**: [https://www.youtube.com/watch?v=UNmqTiOnRfg](https://www.youtube.com/watch?v=UNmqTiOnRfg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I highly encourage you to read Dzmitry Bahdanau''s paper about the
    attention mechanism we just implemented and used: [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).'
  prefs: []
  type: TYPE_NORMAL
