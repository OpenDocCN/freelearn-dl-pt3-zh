- en: Indoor Localization in IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many IoT applications, such as indoor navigation and location-aware marketing
    by retailers, smart homes, smart campuses, and hospitals, rely on indoor localization.
    The input data generated from such applications generally comes from numerous
    sources such as infrared, ultrasound, Wi-Fi, RFID, ultrawideband, Bluetooth, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The communication fingerprint of those devices and technologies, such as Wi-Fi
    fingerprinting data, can be analyzed using DL models to predict the location of
    the device or user in indoor environments. In this chapter, we will discuss how
    DL techniques can be used for indoor localization in IoT applications in general
    with a hands-on example. Furthermore, we will discuss some deployment settings
    for indoor localization services in IoT environments. The following topics will
    be briefly covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing indoor localization in IoT applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning** (**DL**) for indoor localization in IoT applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example – indoor localization in Wi-Fi fingerprinting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different deployment options for DL-based indoor localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of indoor localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the rapid development of mobile internet, **Location-Based** **S****ervices**
    (**LBS**) in large public indoor places is becoming increasingly popular. In such
    an indoor location, the **Received Signal Strength Indicator** (**RSSI**) is often
    used as an estimated measure of the power level that an IoT device is receiving
    from **Wireless Access Points** (**WAPs**). However, when the distance from the
    source is increased, the signal gets weaker and the wireless data rates get slower,
    leading to a lower overall data throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for indoor localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several indoor localization technologies have been proposed to date based on
    measuring techniques such as ultrasound, infrared, image, light, magnetic field,
    and wireless signals. For example, **Bluetooth low energy** (**BLE**)-based indoor
    localization has been attracting increasing interest because it is low-cost, low-power
    consumption, and has ubiquitous availability on almost every mobile device. On
    the other hand, the Wi-Fi localization system is based on the **Channel State
    Information** (**CSI**) of Wi-Fi signals.
  prefs: []
  type: TYPE_NORMAL
- en: Lately, DL approaches have been proposed in which DL models are used to learn
    the fingerprint patterns of high-dimensional CSI signals. Although, each Wi-Fi
    scan contains the signal strength measurements for APs available in its vicinity,
    only a subset of a total number of networks in the environment are observed.
  prefs: []
  type: TYPE_NORMAL
- en: Also, since those devices are low-end with very small processing power, the
    unpredictable weakening or strengthening combination used in those approaches
    affect the multi-path signals, which will break the relationship between the RSSI
    and the transmission distance, and consequently prove to be less effective. On
    the other hand, the fingerprinting approach doesn't rely on the recovery of distances
    but instead uses the measured RSSIs as spatial patterns only. It is thus less
    vulnerable to the multi-path effect.
  prefs: []
  type: TYPE_NORMAL
- en: Fingerprinting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The fingerprinting approaches used commonly have two phases: an offline phase
    and an online phase.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One phase uses a fingerprint database to construct position-dependent parameters,
    which are extracted from measured RSSIs'' reference locations, known as **offline
    phases**. In a localization phase, which is also known as an **online phase**,
    the mapping of RSSI measurements is done to a reference location using the most
    relevant RSSI fingerprint from the database, which can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b86e0d50-48d9-41ad-bcad-3f123875d7ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, ![](img/bab27896-3a38-49f3-8ec0-1677e6fc1bb6.png)
    is the total number of reference locations in the database, ![](img/242d15f7-deb3-407d-b33c-ca60da767fea.png)
    denotes the fingerprint pattern of the ![](img/b68d16c9-827a-41a5-a8cf-380580a3429b.png)
    reference location, and ![](img/449d1379-8d2b-43d3-aab5-ecca314065e9.png) is the
    spatial coordinates of that reference location. The fingerprint pattern, ![](img/ab5fac86-a4b7-43f4-92b9-8b545d15ff87.png),
    can be raw RSSI values from multiple beacon stations, or any other feature vectors
    extracted from the RSSIs, which can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6372765-89d2-45b0-8de8-c747c53e1a14.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the raw RSSI values are used as spatial patterns in existing fingerprinting
    systems. In the preceding equation, *m* is the total number of BLE beacon stations
    or Wi-Fi APs, and ![](img/44ce7c44-7826-44b9-85f5-cf7b91f88441.png) represents
    the measured RSSI value of the ![](img/01e38a2a-24f4-455d-9388-1dc08b082661.png)
    station.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we roughly know what indoor localization is. In the next section, we'll
    see how machine learning and DL algorithms can be used to develop such an indoor
    localization system.
  prefs: []
  type: TYPE_NORMAL
- en: DL-based indoor localization for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, if we want to develop a DL application and deploy low-end devices, such
    IoT devices won't be able to process them. In particular, handling very high-dimensional
    data would be a bottleneck. So, an outdoor localization problem can be solved
    with reasonable accuracy using a machine learning algorithm such as **k-nearest
    neighbors** (**k-NNs**) because the inclusion of GPS sensors in mobile devices
    means we now have more data at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, indoor localization is still an open research problem, mainly due
    to the loss of GPS signals in indoor environments, despite advanced indoor positioning
    technologies. Fortunately, by using DL techniques, we can solve this problem with
    reasonable accuracy, especially since using **Autoencoders** (**AEs**) and their
    representation capabilities can be a pretty good workaround and a viable option.
    In such a setting, we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a fully connected layer and a softmax layer in front of the AE network,
    which will act as an end-to-end classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use any other classification algorithms, such as logistic regression, k-NN,
    Random Forest, or a support vector machine for the location estimation (that is,
    classification), as shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0c323e9d-9947-4845-b736-6e662bc90b1f.png)'
  prefs: []
  type: TYPE_IMG
- en: The idea is to use AEs for the representation learning so that the network can
    learn the features well. Then, the output of the encoder part can be used to initialize
    the weight of the classifier part. In the following section, we will discuss k-NN
    and AEs and see how they can be used to solve the indoor localization problem.
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbor (k-NN) classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-NN algorithm is a non-parametric method that can be trained using the
    fingerprinting data coming from IoT devices. This tries to classify the collected
    RSSI values from the gateways to one of the reference points and not to the coordinates.
    The input consists of the k-closest RSSI values and the output would be a class
    membership. An input sample is then classified by a plurality vote of its neighbors,
    with the object being assigned to the class most common among its k-nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, if the fingerprinting database consists of (*X, y*)—with *X* being
    the RSSI values and *y* being the set of already known locations—then k-NN first
    computes the distance ![](img/b6016adb-5765-47b1-8f24-9983adfe5dfb.png), where
    *x* is the unknown sample. Then, it computes a set, ![](img/90cfbfa6-1377-48ef-af6e-d3b0bad19a33.png),
    containing indices for the *k* smallest distances from ![](img/d6f38274-2e83-4fe5-8ee9-d2f0c4d3b6e7.png).
    Then, the majority label for ![](img/18562ef7-7264-488e-b212-421a0c37a3e7.png)
    is returned, where ![](img/d3ee66fb-7bd8-4e35-b3f9-2e7feaf938a4.png). In other
    words, using k-NN, the classification is performed by computing the similarity
    between the observed data and records in the training RSSI samples in the database.
    Ultimately, the grid cell with the highest occurrence in the first *k* most similar
    records is the estimated location, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7c21824-f316-41f0-99ef-9b1ceca414e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Localization of IoT enabled devices using k-NN algorithm
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, for k=4, the Wi-Fi packet trace is classified as being
    in the grid c (green triangles) record, while it is classified as being in grid
    a (red rectangle) when *k=6*. So, k-NN can be thought of as a lazy learning approach,
    where the function is only approximated locally and all computation is deferred
    until classification occurs. The good thing about the k-NN algorithm is that it
    is robust against noisy data. In particular, the inverse square of the weighted
    distance is used as the distance measure. Nevertheless, it performs well if it's
    already trained on a large amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: There are possible drawbacks as well. For example, we need to determine the
    *K* parameter value, which is the number of nearest neighbors. It performs quite
    differently based on the distance measure used. The computation cost using the
    k-NN algorithm is quite high since it is required to compute the distance of each
    sample in the training data. This becomes even worse in the case of very high-dimensional
    data. In the next section, we will use k-NN as an end-to-end classifier rather
    than using a neural network setting to provide a comparative analysis between
    an AE-based classifier and k-NN classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: AE classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As described in [Chapter 2](7626c72a-c3b8-4707-96a5-88d524d9f3f7.xhtml), *Deep
    Learning Architectures for IoT*, AEs are special types of neural networks that
    learn automatically from the input data. AEs consists of two components: an encoder
    and a decoder. An encoder compresses the input into a latent-space representation.
    Then, the decoder part, tries to reconstruct the original input data from that
    representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: Encodes or compresses the input into a latent-space representation
    using a function known as ![](img/b5bf675d-1f97-4cdd-a9c5-c5de096126fd.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: Decodes or reconstructs the input from the latent space representation
    using a function known as ![](img/643771fa-fed6-457b-b4c6-beb20bfbcf5d.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, an AE can be described by a function of ![](img/5455f554-5bcc-42bd-8b3b-ec1070db6e05.png),
    where we want *0* to be as close as the original input of *x*. AEs are very useful
    for data denoising and dimensionality reduction for data visualization. AEs can
    learn data projections, called **representations**, more effectively than PCA.
    The following diagram shows the architecture of a denoising AE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f40044e9-4968-443e-8ca5-1771240ccac6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, once we have a fingerprinting database to hand, AEs can be trained with
    the raw RSSI measurements and the trained network itself is used as the fingerprint
    pattern for a specific reference location. Since a deep network can be represented
    by the weight of each layer, the fingerprint pattern can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/468c9efa-9469-4607-a4a8-2b8f4f4defe4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, *l* is the number of encoding hidden layers of an
    AE, and ![](img/5a91f9bf-9c2b-44bb-857a-a1e0f676c199.png) and ![](img/35489ecc-25a5-4e0a-b4ae-4378f7665645.png)
    represent the weights of the ![](img/ef1ef057-8bed-40e9-87b9-8de470f3daae.png)
    encoding hidden layer and its decoding mirror layer, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b2950f2-58fb-4dff-ad2f-7fa884d78a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we can use the output of the central hidden layers of the AE as the input
    to the fully connected softmax layer to predict the location, as shown in the
    preceding diagram. Now that we know how indoor localization works in a neural
    network or machine learning setting, we can now start a hands-on example using
    Wi-Fi fingerprinting.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Indoor localization with Wi-Fi fingerprinting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will use a **multi-building, multi-floor indoor localization**
    database and stacked AEs to localize Wi-Fi fingerprinting. With some minimal effort,
    this application can be deployed to mobile robots to use Wi-Fi localization subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: Describing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `UJIIndoorLoc` dataset is a multi-building, multi-floor indoor localization
    database designed to test an indoor positioning system relying on Wi-Fi fingerprinting.
    Automatic user localization consists of estimating the position of the user, such
    as the latitude, longitude, and altitude, collected from a mobile phone. The `UJIIndoorLoc`
    database covers three buildings of Universitat Jaume I with 4 or more floors and
    almost 110,000 square meters, measured in 2013 by means of more than 20 different
    users and 25 Android devices. The database consists of two CSV files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`trainingData.csv`: 19,937 training/reference records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validationData.csv`: 1,111 validation/test records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The 529 attributes contain Wi-Fi fingerprints and the coordinates where they
    were taken. Each Wi-Fi fingerprint can be characterized by the detected WAPs and
    the corresponding RSSI. The intensity values are represented as negative integer
    values ranging from 1,04 dBm (extremely poor signal) to 0 dBm. The positive 100
    value is used to denote when a WAP was not detected. During the database creation,
    520 different WAPs were detected. Thus, the Wi-Fi fingerprint is composed of 520
    intensity values. The coordinates'' latitude, longitude, floor, and **BuildingID** information
    are the attributes to be predicted. The following list gives a quick summary of
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Attribute 001 to 520 (that is, WAP001 to WAP520)**: These are the intensity
    measurement values for the access points in which values are in—104 to 0 and +100\.
    The 100 value signifies that WAP001 was not detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 521 (Longitude)**: Negative real values from 7,695.9,387,549,299,299,000
    to -7299.786516730871000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 522 (Latitude)**: Positive real values from 4,864,745.7,450,159,714
    to 4,865,017.3,646,842,018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 523 (Floor)**: Altitude in floors inside the building. Integer
    values from 0 to 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 524 (BuildingID)**: ID to identify the building provided as categorical
    integer values from 0 to 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 525 (SpaceID)**: Internal ID number to identify the space, such
    as the office, the corridor, or the classroom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 526 (RelativePosition)**: Relative position with respect to the
    space (1—inside, 2—outside, in front of the door).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 527 (UserID)**: User identifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 528 (PhoneID)**: Android device identifier (see the following).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute 529 (Timestamp)**: UNIX time when the capture was taken.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The AE classifier we will be using will have an AE part consisting of an encoder
    and a decoder. The following AE architecture is used to determine the floor and
    building location where the Wi-Fi is located. The input to the AE are signal strengths
    detected in a scan. Then, one value for each visible network is considered an
    RSSI record. The output of a decoder is the reconstructed input from the reduced
    representation, as shown in the following diagram (source: *Low-effort place recognition
    with Wi-Fi fingerprints using deep learning*, Michał N. et al., arXiv:1611.02049v1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/104e89b7-6a30-4173-bc33-ed0cbacab1e5.png)'
  prefs: []
  type: TYPE_IMG
- en: The AE architecture for the feature space representation
  prefs: []
  type: TYPE_NORMAL
- en: 'The classifier part consists of two hidden layers; depending on the complexity
    of the problem, the number of neurons needs to be selected. When the unsupervised
    learning of the weights of AE is finished, the decoder part of the network is
    disconnected. Then fully-connected layers are typically placed after the output
    of the encoder by turning the whole network into a classifier. In the following
    diagram, the pre-trained encoder part is connected to the fully connected softmax
    layer (source: *Low-effort Place Recognition with Wi-Fi Fingerprints Using Deep
    Learning*, Michał N. et al., arXiv:1611.02049v1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7efbdf3f-e1a6-4412-9f42-0003d051578a.png)'
  prefs: []
  type: TYPE_IMG
- en: The architecture of an AE classifier for classifying a building and its floor
    based on Wi-Fi scan input
  prefs: []
  type: TYPE_NORMAL
- en: The final output layer is a softmax layer that outputs the probabilities of
    the current sample belonging to the analyzed classes. Now, without any further
    delay, let's start implementing the preceding networks.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use Keras to wrap up this conceptualization. First, let''s import the
    necessary packages and libraries, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once we have imported all the necessary packages, we can proceed to prepare
    the training set and test set, which can be used to train and evaluate the model,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The exploratory analysis of data using the **Python pandas** library provides
    many powerful features–no doubt. However, using `df.describe()`, `df.dtypes`,
    or using `df.isnull().sum()` and plotting them separately is always time-consuming.
    Sometimes, you won''t even get the required information in a sophisticated way.
    In fact, you''ll have to write extra lines of code to convert them into a presentable
    format. However, to make your life easier, you can now start using the `pandas_profiling`
    library (see [https://github.com/pandas-profiling/pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)).
    Just one line of code will give the information you need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Surely, it would be worth using `pandas_profiling` to get a quick understanding
    of your data. Let''s try it out! First, we read the training data by explicitly
    passing `header=0` to be able to replace the existing names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To retrieve the list of variables that are rejected due to high correlation,
    you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce a report showing information on the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f81ce1d6-dc12-4b22-a14c-4b38ff6d3eff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the first few lines of the report. As we can see, we don''t
    have any null values and all the variables are numeric, which is great. However,
    some features are less significant, being highly correlated with other variables
    (for example, 74 variables were rejected) and some of the variables are very skewed,
    giving a very wide distribution. Even our training dataset has 637 duplicate rows.
    Rejected variables would not help the model learn well. Consequently, those can
    be dropped from the training data (this is optional, though). The list of such
    rejected variables can be collected using the `get_rejected_variables` method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to generate a HTML report file, save the profile to an object and
    use the `to_file` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will generate an `HTML` report containing the necessary information. Now
    that we know the data and variables, let's focus on the feature engineering steps
    by which we'll prepare the data required for training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing training and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we scale the data to center to the mean. Then, we perform component-wise
    scaling to unit variance. This will help our model to converge the training more
    quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we construct the true labels. We convert all the building IDs and building
    floors to strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s try to create two variables: `train_x` and `train_y`. This will
    help to avoid confusion during the training evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, similar to the training set, we prepare the test set as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Once we have the training and the test sets ready, we can now proceed with creating
    an AE.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create separate encoder and decoder functions since you will be using
    encoder weights later on for classification purposes. First, we define some parameters,
    such as the number of epochs and the batch size. Also, we compute the shape of
    the input data and the number of classes that will be required to construct and
    train the AE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create the encoder part of the AE, which has three hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the decoder part of the AE, which has three hidden layers,
    followed by the `compile()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we stack them together to construct an AE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the structure and a summary of the AE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d99b49b5-34e4-402d-8d44-9f1def2179d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then train the AE with the training data for 100 iterations, where 10%
    of the training data is to be used for validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we set the `verbose =1` in the preceding code block, during training,
    you''ll see the following logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we take the output of the encoder network for both the training set and
    the test set as the latent features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Creating an AE classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will re-train the `auto_encoder` model by making the first three layers
    trainable as `True` instead of keeping them as `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can pop off the first three layers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we add fully connected layers in front, with the `BatchNormalization`
    layer is followed by the first dense layer. Then, we add another dense layer,
    followed by the `BatchNormalization` and `Dropout` layers. Then, we place another
    dense layer, followed by a `GaussionNoise` layer and a `Dropout` layer, before
    we finally have the softmax layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we get the full AE classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we compile the model before starting the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we start fine-tuning the network in a supervised way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we set `verbose =1` in the preceding code block, during training, you''ll
    experience the following logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s take a look at the training loss versus validation loss, which will
    help us to understand how the training went. This will also help us to establish
    whether our neural network has issues such as overfitting and underfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block will plot the training loss and validation losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2372d36-7447-4aca-9c0e-86565eeefeaf.png)'
  prefs: []
  type: TYPE_IMG
- en: As seen in the preceding graph, the training losses across epochs are higher
    than the validation loss, which is a sign of overfitting. We don't have enough
    training samples to train the neural network well. Some samples were even repeated
    in the dataset, which literally turned out to be trivial and redundant in the
    network. This was probably the reason adding the **Dropout** and **Gaussian**
    noise layers didn't help much. Anyway, we can also save the trained model for
    future reuse, which we'll discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the trained model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the AE classifier fully trained, we can save it so that we
    can restore it from disk later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will evaluate the trained model on the test set, which
    we will discuss in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our model is fully trained, we can evaluate its performance on unseen
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding lines of code will show the accuracy score, something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s compute the performance metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block will show the following output, giving an F1-score
    of 88%, approximately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we can print the classification report to know the class-specific
    localization as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2f80ecc-424d-4628-9cd6-b442280fcb78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, we will plot the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code will produce the following confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91024568-524d-40ff-9690-1f36e7a1242b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As seen in the preceding confusion matrix, our AE classifier was mostly confused
    for class 11 and predicted as many as 39 samples to be classified in grid 12\.
    However, we have still managed to get very good accuracy. Possible suggestions
    for improvements could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Training the network after removing the rejected variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the network on more epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing hyperparameter tuning using grid search and cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more layers to the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you find the optimized model trained on more data, giving stable, improved
    performance, it can be deployed in on IoT enabled device. We will discuss some
    possible deployment options in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we argued earlier, each Wi-Fi scan contains the signal strength measurements
    for APs available in its vicinity, but only a subset of the total number of networks
    in the environment are observed. Many IoT devices, such as a mobile phone or a
    Raspberry Pi, are low-end with very little processing power. So, deploying such
    a DL model would be a challenging task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many solution providers and technology companies provide smart positioning
    services commercially. Using Wi-Fi fingerprinting from indoor and outdoor location
    data, the accurate tracking of devices is now possible. In most of these companies,
    the RSSI fingerprint positioning is used as the core technology. In such a setting,
    signals or messages that bear different sensitivity levels across RSSI values
    (which is of course subject to the proximity) can be picked up by gateways. Then,
    if there are ![](img/c189cb14-792d-4335-8822-38f5e338c167.png) gateways in a network,
    the RSSI values acquired from a particular indoor or outdoor location will form
    the RSSI fingerprint having *n* entries at that location, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f080db6c-cb77-425d-bedb-97be4ebdc152.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram corresponds to the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95c8924c-4289-48ba-887d-109ac1944279.png)'
  prefs: []
  type: TYPE_IMG
- en: However, in cases with a large number of gateways (> 4), the fingerprint could
    be distinctly unique within a certain range. One deployment technique could be
    using the trained model serving at the backend and serving it as an Android or
    iOS mobile application. The application then monitors the signals from the IoT
    devices already deployed in the indoor location, inserts them as RSSI values in
    the SQLite database and, based on the RSSI values, prepares the test set and sends
    a query to the pre-trained model to get the location.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a schematic architecture outlining all the steps
    required for such a deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/187ba3bf-40ca-4c00-8ed9-aef4555073f5.png)'
  prefs: []
  type: TYPE_IMG
- en: In such a case, the trained model will serve as the transfer learning. Nevertheless,
    the trained model can be served as a web application using Flask or the DJango
    Python framework. Then, the RSSI values and signals from the IoT devices can be
    stored in a database to enrich the historical data. The location can subsequently
    be tracked using an Android or iOS application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed how indoor localization works for IoT enabled
    devices. In particular, we have seen how DL techniques can be used for indoor
    localization in IoT applications employing that data in general with a hands-on
    example. Furthermore, we have looked at some deployment settings of indoor localization
    services in IoT environments.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](958a0ed2-1d5f-4df1-8bfa-55d3c870d733.xhtml), *Physiological and
    Psychological State Detection in IoT*, we will discuss DL-based human physiological
    and psychological state detection techniques for IoT applications in general.
    Considering a real-world scenario, we will look at two IoT applications based
    on physiological and psychological state detection.
  prefs: []
  type: TYPE_NORMAL
