["```\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.neural_network import MLPClassifier\n\npwd_data = pd.read_csv(\"https://www.cs.cmu.edu/~keystroke/DSL-StrongPasswordData.csv\", header = 0)\n\n# Average Keystroke Latency per Subject\n\nDD = [dd for dd in pwd_data.columns if dd.startswith('DD')]\nplot = pwd_data[DD]\nplot['subject'] = pwd_data['subject'].values\nplot = plot.groupby('subject').mean()\nplot.iloc[:6].T.plot(figsize=(8, 6), title='Average Keystroke Latency per Subject')\n```", "```\ndata_train, data_test = train_test_split(pwd_data, test_size = 0.2, random_state=0)\n\nX_train = data_train[pwd_data.columns[2:]]\ny_train = data_train['subject']\n\nX_test = data_test[pwd_data.columns[2:]]\ny_test = data_test['subject']\n\n# K-Nearest Neighbor Classifier\nknc = KNeighborsClassifier()\nknc.fit(X_train, y_train)\n\ny_pred = knc.predict(X_test)\n\nknc_accuracy = metrics.accuracy_score(y_test, y_pred)\nprint('K-Nearest Neighbor Classifier Accuracy:', knc_accuracy)\nK-Nearest Neighbor Classifier Accuracy: 0.3730392156862745\n\n# Support Vector Linear Classifier\nsvc = svm.SVC(kernel='linear') \nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\n\nsvc_accuracy = metrics.accuracy_score(y_test, y_pred)\nprint('Support Vector Linear Classifier Accuracy:', svc_accuracy)\nSupport Vector Linear Classifier Accuracy: 0.7629901960784313\n\n# Multi Layer Perceptron Classifier\nmlpc = MLPClassifier()\nmlpc.fit(X_train,y_train)\n\ny_pred = mlpc.predict(X_test)\nmlpc_accuracy = metrics.accuracy_score(y_test, y_pred)\nprint('Multi Layer Perceptron Classifier Accuracy:', mlpc_accuracy)\nMulti Linear Perceptron Classifier Accuracy: 0.9115196078431372\n```", "```\n# Drawing confusion matrix for Multi Layer Perceptron results\nfrom sklearn.metrics import confusion_matrix\n\nlabels = list(pwd_data['subject'].unique())\ncm = confusion_matrix(y_test, y_pred, labels) \n\nfigure = plt.figure()\naxes = figure.add_subplot(111)\nfigure.colorbar(axes.matshow(cm))\naxes.set_xticklabels([''] + labels)\naxes.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\n```", "```\nimport numpy as np\n\nX = np.array([\n [3, 0.1, -2.4],\n [3.1, 0.3, -2.6],\n [3.4, 0.2, -1.9],\n])\n\nprint(np.cov(X).T)\n```", "```\nimport numpy as np\neigenvalues, eigenvectors = np.linalg.eig(np.array([[2, -4], [4, -6]]))\n```", "```\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.decomposition import PCA\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nlfw = fetch_lfw_people(min_faces_per_person=150)\n\nX_data = lfw.data\ny_target = lfw.target\nnames = lfw.target_names\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3)\n\npca = PCA(n_components=150, whiten=True)\npca.fit(X_train)\n\npca_train = pca.transform(X_train)\npca_test = pca.transform(X_test)\n\nmlpc = MLPClassifier()\nmlpc.fit(pca_train, y_train)\n\ny_pred = mlpc.predict(pca_test)\nprint(classification_report(y_test, y_pred, target_names=names))\n```", "```\n\n                precision    recall  f1-score   support\n\n  Colin Powell       0.92      0.89      0.90        79\n George W Bush       0.94      0.96      0.95       151\n\n     micro avg       0.93      0.93      0.93       230\n     macro avg       0.93      0.92      0.93       230\n  weighted avg       0.93      0.93      0.93       230\n```"]