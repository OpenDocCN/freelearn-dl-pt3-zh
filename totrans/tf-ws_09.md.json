["```\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n    ```", "```\n    import io\n    data = pd.read_csv('NVDA.csv')\n    ```", "```\n    data.head()\n    ```", "```\n    data_training = data[data['Date']<'2019-01-01'].copy()\n    ```", "```\n    data_test = data[data['Date']>='2019-01-01'].copy()\n    ```", "```\n    training_data = data_training.drop\\\n                    (['Date', 'Adj Close'], axis = 1)\n    training_data.head()\n    ```", "```\n    scaler = MinMaxScaler()\n    training_data = scaler.fit_transform(training_data)\n    training_data\n    ```", "```\n    X_train = []\n    y_train = []\n    ```", "```\n    training_data.shape[0]\n    ```", "```\n    868\n    ```", "```\n    for i in range(60, training_data.shape[0]):\n        X_train.append(training_data[i-60:i])\n        y_train.append(training_data[i, 0])\n    ```", "```\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    ```", "```\n    X_train.shape, y_train.shape\n    ```", "```\n    ((808, 60, 5), (808,))\n    ```", "```\n    X_old_shape = X_train.shape\n    X_train = X_train.reshape(X_old_shape[0], \\\n                              X_old_shape[1]*X_old_shape[2]) \n    X_train.shape\n    ```", "```\n    (808, 300)\n    ```", "```\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.layers import Input, Dense, Dropout\n    ```", "```\n    regressor_ann = Sequential()\n    ```", "```\n    regressor_ann.add(Input(shape = (300,)))\n    ```", "```\n    regressor_ann.add(Dense(units = 512, activation = 'relu'))\n    regressor_ann.add(Dropout(0.2))\n    ```", "```\n    regressor_ann.add(Dense(units = 128, activation = 'relu'))\n    regressor_ann.add(Dropout(0.3))\n    ```", "```\n    regressor_ann.add(Dense(units = 64, activation = 'relu'))\n    regressor_ann.add(Dropout(0.4))\n    ```", "```\n    regressor_ann.add(Dense(units = 16, activation = 'relu'))\n    regressor_ann.add(Dropout(0.5))\n    ```", "```\n    regressor_ann.add(Dense(units = 1))\n    ```", "```\n    regressor_ann.summary()\n    ```", "```\n    regressor_ann.compile(optimizer='adam', \\\n                          loss = 'mean_squared_error')\n    ```", "```\n    regressor_ann.fit(X_train, y_train, epochs=10, batch_size=32)\n    ```", "```\n    data_test.head()\n    ```", "```\n    past_60_days = data_training.tail(60)\n    df = past_60_days.append(data_test, ignore_index = True)\n    ```", "```\n    df = df.drop(['Date', 'Adj Close'], axis = 1)\n    inputs = scaler.transform(df) \n    X_test = []\n    y_test = []\n    for i in range(60, inputs.shape[0]):\n        X_test.append(inputs[i-60:i])\n        y_test.append(inputs[i, 0])\n    X_test, y_test = np.array(X_test), np.array(y_test)\n    X_old_shape = X_test.shape\n    X_test = X_test.reshape(X_old_shape[0], \\\n                            X_old_shape[1] * X_old_shape[2])\n    X_test.shape, y_test.shape\n    ```", "```\n    ((391, 300), (391,))\n    ```", "```\n    y_pred = regressor_ann.predict(X_test)\n    ```", "```\n    scaler.scale_\n    ```", "```\n    scale = 1/3.70274364e-03\n    scale \n    ```", "```\n    270.0700067909643\n    ```", "```\n    y_pred = y_pred*scale\n    y_test = y_test*scale\n    ```", "```\n    plt.figure(figsize=(14,5))\n    plt.plot(y_test, color = 'black', label = \"Real NVDA Stock Price\")\n    plt.plot(y_pred, color = 'gray',\\\n             label = 'Predicted NVDA Stock Price')\n    plt.title('NVDA Stock Price Prediction')\n    plt.xlabel('time')\n    plt.ylabel('NVDA Stock Price')\n    plt.legend()\n    plt.show()\n    ```", "```\nmodel = keras.models.Sequential([\n                                 keras.layers.SimpleRNN\\\n                                 (1, input_shape=[None, 1]) \n])\n```", "```\nmodel = keras.models.Sequential\\\n        ([Keras.layers.SimpleRNN\\\n          (20, return_sequences=True, input_shape=[None, 1]), \\\n          Keras.layers.SimpleRNN(20, return_sequences=True), \\\n          Keras.layers.SimpleRNN(1)])\n```", "```\nregressor = Sequential()\nregressor.add(LSTM(units= 50, activation = 'relu', \\\n                   return_sequences = True, \\\n                   input_shape = (X_train.shape[1], 5)))\nregressor.add(Dropout(0.2))\nregressor.add(LSTM(units= 60, activation = 'relu', \\\n                   return_sequences = True))\nregressor.add(Dropout(0.3))\nregressor.add(LSTM(units= 80, activation = 'relu', \\\n                   return_sequences = True))\nregressor.add(Dropout(0.4))\nregressor.add(LSTM(units= 120, activation = 'relu'))\nregressor.add(Dropout(0.5))\nregressor.add(Dense(units = 1))\n```", "```\nregressor = Sequential()\n```", "```\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.layers import Dense, LSTM, Dropout\n    ```", "```\n    regressor = Sequential()\n    regressor.add(LSTM(units= 50, activation = 'relu',\\\n                       return_sequences = True,\\\n                       input_shape = (X_train.shape[1], 5)))\n    regressor.add(Dropout(0.2))\n    regressor.add(LSTM(units= 60, activation = 'relu', \\\n                  return_sequences = True))\n    regressor.add(Dropout(0.3))\n    regressor.add(LSTM(units= 80, activation = 'relu', \\\n                  return_sequences = True))\n    regressor.add(Dropout(0.4))\n    regressor.add(LSTM(units= 120, activation = 'relu'))\n    regressor.add(Dropout(0.5))\n    regressor.add(Dense(units = 1))\n    ```", "```\n    regressor.summary()\n    ```", "```\n    regressor.compile(optimizer='adam', loss = 'mean_squared_error')\n    ```", "```\n    regressor.fit(X_train, y_train, epochs=10, batch_size=32)\n    ```", "```\n    data_test.head()\n    ```", "```\n    data_training.tail(60)\n    ```", "```\n    past_60_days = data_training.tail(60)\n    ```", "```\n    df = past_60_days.append(data_test, ignore_index = True)\n    df = df.drop(['Date', 'Adj Close'], axis = 1)\n    ```", "```\n    df.head()\n    ```", "```\n    inputs = scaler.transform(df)\n    inputs\n    ```", "```\n    X_test = []\n    y_test = []\n    for i in range(60, inputs.shape[0]):\n        X_test.append(inputs[i-60:i])\n        y_test.append(inputs[i, 0])\n    ```", "```\n    X_test, y_test = np.array(X_test), np.array(y_test)\n    X_test.shape, y_test.shape\n    ```", "```\n    ((391, 60, 5), (391,))\n    ```", "```\n    y_pred = regressor.predict(X_test)\n    ```", "```\n    scaler.scale_\n    ```", "```\n    scale = 1/3.70274364e-03\n    scale\n    ```", "```\n    270.0700067909643\n    ```", "```\n    y_pred = y_pred*scale\n    y_test = y_test*scale\n    ```", "```\n    y_pred\n    ```", "```\n    plt.figure(figsize=(14,5))\n    plt.plot(y_test, color = 'black', label = \"Real NVDA Stock Price\")\n    plt.plot(y_pred, color = 'gray',\\\n             label = 'Predicted NVDA Stock Price')\n    plt.title('NVDA Stock Price Prediction')\n    plt.xlabel('time')\n    plt.ylabel('NVDA Stock Price')\n    plt.legend()\n    plt.show()\n    ```", "```\ndef clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation)\\\n            .lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \ncorpus = [clean_text(x) for x in all_headlines]\n```", "```\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer()\n```", "```\ntokenizer.fit_on_texts(corpus)\n```", "```\ntokenizer.word_index\n```", "```\ntokenizer.texts_to_sequences([sentence])\n```", "```\ndef get_seq_of_tokens(corpus):\n    tokenizer.fit_on_texts(corpus)\n    all_words = len(tokenizer.word_index) + 1\n\n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, all_words\ninp_sequences, all_words = get_seq_of_tokens(corpus)\ninp_sequences[:10]\n```", "```\nfrom keras.preprocessing.sequence import pad_sequences\n```", "```\ndef generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences\\\n                               (input_sequences, \\\n                                maxlen=max_sequence_len, \\\n                                padding='pre'))\n    predictors, label = input_sequences[:,:-1], \\\n                        input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=all_words)\n    return predictors, label, max_sequence_len\npredictors, label, max_sequence_len = generate_padded_sequences\\\n                                      (inp_sequences)\n```", "```\n    from keras.preprocessing.sequence import pad_sequences\n    from keras.layers import Embedding, LSTM, Dense, Dropout\n    from keras.preprocessing.text import Tokenizer\n    from keras.callbacks import EarlyStopping\n    from keras.models import Sequential\n    import keras.utils as ku \n    import pandas as pd\n    import numpy as np\n    import string, os \n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    warnings.simplefilter(action='ignore', category=FutureWarning)\n    ```", "```\n    Using TensorFlow backend.\n    ```", "```\n    curr_dir = '/content/'\n    all_headlines = []\n    for filename in os.listdir(curr_dir):\n        if 'Articles' in filename:\n            article_df = pd.read_csv(curr_dir + filename)\n            all_headlines.extend(list(article_df.headline.values))\n            break\n    all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n    len(all_headlines)\n    ```", "```\n    831\n    ```", "```\n    def clean_text(txt):\n        txt = \"\".join(v for v in txt \\\n                      if v not in string.punctuation).lower()\n        txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n        return txt \n    corpus = [clean_text(x) for x in all_headlines]\n    corpus[:10]\n    ```", "```\n    tokenizer = Tokenizer()\n    def get_seq_of_tokens(corpus):\n        tokenizer.fit_on_texts(corpus)\n        all_words = len(tokenizer.word_index) + 1\n        input_sequences = []\n        for line in corpus:\n            token_list = tokenizer.texts_to_sequences([line])[0]\n            for i in range(1, len(token_list)):\n                n_gram_sequence = token_list[:i+1]\n                input_sequences.append(n_gram_sequence)\n        return input_sequences, all_words\n    inp_sequences, all_words = get_seq_of_tokens(corpus)\n    inp_sequences[:10]\n    ```", "```\n    def generate_padded_sequences(input_sequences):\n        max_sequence_len = max([len(x) for x in input_sequences])\n        input_sequences = np.array\\\n                          (pad_sequences(input_sequences, \\\n                                         maxlen=max_sequence_len, \\\n                                         padding='pre'))\n        predictors, label = input_sequences[:,:-1], \\\n                            input_sequences[:,-1]\n        label = ku.to_categorical(label, num_classes=all_words)\n        return predictors, label, max_sequence_len\n    predictors, label, max_sequence_len = generate_padded_sequences\\\n                                          (inp_sequences)\n    ```", "```\n    def create_model(max_sequence_len, all_words):\n        input_len = max_sequence_len - 1\n        model = Sequential()\n\n        model.add(Embedding(all_words, 10, input_length=input_len))\n\n        model.add(LSTM(100))\n        model.add(Dropout(0.1))\n\n        model.add(Dense(all_words, activation='softmax'))\n        model.compile(loss='categorical_crossentropy', \\\n                      optimizer='adam')\n        return model\n    model = create_model(max_sequence_len, all_words)\n    model.summary()\n    ```", "```\n    model.fit(predictors, label, epochs=100, verbose=5)\n    ```", "```\n    def generate_text(seed_text, next_words, \\\n                      model, max_sequence_len):\n        for _ in range(next_words):\n            token_list = tokenizer.texts_to_sequences\\\n                         ([seed_text])[0]\n            token_list = pad_sequences([token_list], \\\n                                       maxlen=max_sequence_len-1,\\\n                                       padding='pre')\n            predicted = model.predict_classes(token_list, verbose=0)\n            output_word = \"\"\n            for word,index in tokenizer.word_index.items():\n                if index == predicted:\n                    output_word = word\n                    break\n            seed_text += \" \"+output_word\n        return seed_text.title()\n    ```", "```\n    print (generate_text(\"the hottest new\", 5, model,\\\n                         max_sequence_len))\n    print (generate_text(\"the stock market\", 4, model,\\\n                         max_sequence_len))\n    print (generate_text(\"russia wants to\", 3, model,\\\n                         max_sequence_len))\n    print (generate_text(\"french citizen\", 4, model,\\\n                         max_sequence_len))\n    print (generate_text(\"the one thing\", 15, model,\\\n                         max_sequence_len))\n    print (generate_text(\"the coronavirus\", 5, model,\\\n                         max_sequence_len))\n    ```"]