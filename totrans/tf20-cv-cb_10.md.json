["```\n$> pip install opencv-contrib-python imutils\n```", "```\n    import csv\n    import glob\n    import pathlib\n    import cv2\n    import imutils\n    import numpy as np\n    from tensorflow.keras.callbacks import ModelCheckpoint\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.models import *\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.preprocessing.image import *\n    from tensorflow.keras.utils import to_categorical\n    ```", "```\n    EMOTIONS = ['angry', 'scared', 'happy', 'sad', \n              'surprised','neutral']\n    COLORS = {'angry': (0, 0, 255),\n        'scared': (0, 128, 255),\n        'happy': (0, 255, 255),\n        'sad': (255, 0, 0),\n        'surprised': (178, 255, 102),\n        'neutral': (160, 160, 160)\n    }\n    ```", "```\n    def build_network(input_shape, classes):\n        input = Input(shape=input_shape)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same',\n                   kernel_initializer='he_normal')(input)\n        x = ELU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   kernel_initializer='he_normal',\n                   padding='same')(x)\n        x = ELU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x) \n    ```", "```\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   kernel_initializer='he_normal',\n                   padding='same')(x)\n        x = ELU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   kernel_initializer='he_normal',\n                   padding='same')(x)\n        x = ELU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Conv2D(filters=128,\n                   kernel_size=(3, 3),\n                   kernel_initializer='he_normal',\n                   padding='same')(x)\n        x = ELU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=128,\n                   kernel_size=(3, 3),\n                   kernel_initializer='he_normal',\n                   padding='same')(x)\n        x = ELU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=64,\n                  kernel_initializer='he_normal')(x)\n        x = ELU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.5)(x)\n        x = Dense(units=64,\n                  kernel_initializer='he_normal')(x)\n        x = ELU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.5)(x)\n    ```", "```\n        x = Dense(units=classes,\n                  kernel_initializer='he_normal')(x)\n        output = Softmax()(x)\n        return Model(input, output)\n    ```", "```\n    def load_dataset(dataset_path, classes):\n        train_images = []\n        train_labels = []\n        val_images = []\n        val_labels = []\n        test_images = []\n        test_labels = []\n    ```", "```\n        with open(dataset_path, 'r') as f:\n            reader = csv.DictReader(f)\n            for line in reader:\n                label = int(line['emotion'])\n                if label <= 1:\n                  label = 0  # This merges classes 1 and 0.\n                if label > 0:\n                  label -= 1  # All classes start from 0.\n    ```", "```\n                image = np.array(line['pixels'].split\n                                        (' '),\n                                 dtype='uint8')\n                image = image.reshape((48, 48))\n                image = img_to_array(image)\n    ```", "```\n                if line['Usage'] == 'Training':\n                    train_images.append(image)\n                    train_labels.append(label)\n                elif line['Usage'] == 'PrivateTest':\n                    val_images.append(image)\n                    val_labels.append(label)\n                else:\n                    test_images.append(image)\n                    test_labels.append(label)\n    ```", "```\n        train_images = np.array(train_images)\n        val_images = np.array(val_images)\n        test_images = np.array(test_images)\n    ```", "```\n        train_labels = \n        to_categorical(np.array(train_labels),\n                                      classes)\n        val_labels = to_categorical(np.array(val_labels), \n                                     classes)\n        test_labels = to_categorical(np.array(test_labels),\n                                     classes)\n    ```", "```\n        return (train_images, train_labels), \\\n               (val_images, val_labels), \\\n               (test_images, test_labels)\n    ```", "```\n    def rectangle_area(r):\n        return (r[2] - r[0]) * (r[3] - r[1])\n    ```", "```\n    def plot_emotion(emotions_plot, emotion, probability, \n                     index):\n        w = int(probability * emotions_plot.shape[1])\n        cv2.rectangle(emotions_plot,\n                      (5, (index * 35) + 5),\n                      (w, (index * 35) + 35),\n                      color=COLORS[emotion],\n                      thickness=-1)\n        white = (255, 255, 255)\n        text = f'{emotion}: {probability * 100:.2f}%'\n        cv2.putText(emotions_plot,\n                    text,\n                    (10, (index * 35) + 23),\n                    fontFace=cv2.FONT_HERSHEY_COMPLEX,\n                    fontScale=0.45,\n                    color=white,\n                    thickness=2)\n        return emotions_plot\n    ```", "```\n    def plot_face(image, emotion, detection):\n        frame_x, frame_y, frame_width, frame_height = detection\n        cv2.rectangle(image,\n                      (frame_x, frame_y),\n                      (frame_x + frame_width,\n                       frame_y + frame_height),\n                      color=COLORS[emotion],\n                      thickness=2)\n        cv2.putText(image,\n                    emotion,\n                    (frame_x, frame_y - 10),\n                    fontFace=cv2.FONT_HERSHEY_COMPLEX,\n                    fontScale=0.45,\n                    color=COLORS[emotion],\n                    thickness=2)\n        return image\n    ```", "```\n    def predict_emotion(model, roi):\n        roi = cv2.resize(roi, (48, 48))\n        roi = roi.astype('float') / 255.0\n        roi = img_to_array(roi)\n        roi = np.expand_dims(roi, axis=0)\n        predictions = model.predict(roi)[0]\n        return predictions\n    ```", "```\n    checkpoints = sorted(list(glob.glob('./*.h5')), reverse=True)\n    if len(checkpoints) > 0:\n        model = load_model(checkpoints[0])\n    ```", "```\n    else:\n        base_path = (pathlib.Path.home() / '.keras' / \n                     'datasets' /\n                     'emotion_recognition' / 'fer2013')\n        input_path = str(base_path / 'fer2013.csv')\n        classes = len(EMOTIONS)\n    ```", "```\n        (train_images, train_labels), \\\n        (val_images, val_labels), \\\n        (test_images, test_labels) = load_dataset(input_path,\n                                                  classes)\n    ```", "```\n        model = build_network((48, 48, 1), classes)\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=Adam(lr=0.003),\n                      metrics=['accuracy'])\n        checkpoint_pattern = ('model-ep{epoch:03d}-\n                              loss{loss:.3f}'\n                              '-val_loss{val_loss:.3f}.h5')\n        checkpoint = ModelCheckpoint(checkpoint_pattern,\n                                     monitor='val_loss',\n                                     verbose=1,\n                                     save_best_only=True,\n                                     mode='min')\n    ```", "```\n        BATCH_SIZE = 128\n        train_augmenter = ImageDataGenerator(rotation_\n                                range=10,zoom_range=0.1,\n                                  horizontal_flip=True,\n                                        rescale=1\\. / 255.,\n                                    fill_mode='nearest')\n        train_gen = train_augmenter.flow(train_images,\n                                         train_labels,\n                                     batch_size=BATCH_SIZE)\n        train_steps = len(train_images) // BATCH_SIZE\n        val_augmenter = ImageDataGenerator(rescale=1\\. / 255.)\n        val_gen = val_augmenter.flow(val_images,val_labels,\n                             batch_size=BATCH_SIZE)\n    ```", "```\n        EPOCHS = 300\n        model.fit(train_gen,\n                  steps_per_epoch=train_steps,\n                  validation_data=val_gen,\n                  epochs=EPOCHS,\n                  verbose=1,\n                  callbacks=[checkpoint])\n       test_augmenter = ImageDataGenerator(rescale=1\\. / 255.)\n        test_gen = test_augmenter.flow(test_images,\n                                       test_labels,\n                                       batch_size=BATCH_SIZE)\n        test_steps = len(test_images) // BATCH_SIZE\n        _, accuracy = model.evaluate(test_gen, \n                                     steps=test_steps)\n        print(f'Accuracy: {accuracy * 100}%')\n    ```", "```\n    video_path = 'emotions.mp4'\n    camera = cv2.VideoCapture(video_path)  # Pass 0 to use webcam\n    ```", "```\n    cascade_file = 'resources/haarcascade_frontalface_default.xml'\n    det = cv2.CascadeClassifier(cascade_file)\n    ```", "```\n    while True:\n        frame_exists, frame = camera.read()\n        if not frame_exists:\n            break\n    ```", "```\n        frame = imutils.resize(frame, width=380)\n        emotions_plot = np.zeros_like(frame, \n                                      dtype='uint8')\n        copy = frame.copy()\n    ```", "```\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        detections = \\\n            det.detectMultiScale(gray,scaleFactor=1.1,\n                                 minNeighbors=5,\n                                 minSize=(35, 35),\n\n                            flags=cv2.CASCADE_SCALE_IMAGE)\n    ```", "```\n        if len(detections) > 0:\n            detections = sorted(detections,\n                                key=rectangle_area)\n            best_detection = detections[-1]\n    ```", "```\n            (frame_x, frame_y,\n             frame_width, frame_height) = best_detection\n            roi = gray[frame_y:frame_y + frame_height,\n                       frame_x:frame_x + frame_width]\n            predictions = predict_emotion(model, roi)\n            label = EMOTIONS[predictions.argmax()]\n    ```", "```\n            for i, (emotion, probability) in \\\n                    enumerate(zip(EMOTIONS, predictions)):\n                emotions_plot = plot_emotion(emotions_plot,\n                                             emotion,\n                                             probability,\n                                             i)\n    ```", "```\n            clone = plot_face(copy, label, best_detection)\n    ```", "```\n        cv2.imshow('Face & emotions',\n                   np.hstack([copy, emotions_plot]))\n    ```", "```\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    ```", "```\n    camera.release()\n    cv2.destroyAllWindows()\n    ```", "```\n$> pip install opencv-contrib-python tensorflow-hub imageio\n```", "```\n    import os\n    import random\n    import re\n    import ssl\n    import tempfile\n    from urllib import request\n    import cv2\n    import imageio\n    import numpy as np\n    import tensorflow as tf\n    import tensorflow_hub as tfhub\n    from tensorflow_docs.vis import embed\n    ```", "```\n    UCF_ROOT = 'https://www.crcv.ucf.edu/THUMOS14/UCF101/UCF101/'\n    ```", "```\n    KINETICS_URL = ('https://raw.githubusercontent.com/deepmind/'\n                    'kinetics-i3d/master/data/label_map.txt')\n    ```", "```\n    CACHE_DIR = tempfile.mkdtemp()\n    ```", "```\n    UNVERIFIED_CONTEXT = ssl._create_unverified_context()\n    ```", "```\n    def fetch_ucf_videos():\n        index = \\\n            (request\n             .urlopen(UCF_ROOT, \n                      context=UNVERIFIED_CONTEXT)\n             .read()\n             .decode('utf-8'))\n        videos = re.findall('(v_[\\w]+\\.avi)', index)\n        return sorted(set(videos))\n    ```", "```\n    def fetch_kinetics_labels():\n        with request.urlopen(KINETICS_URL) as f:\n            labels = [line.decode('utf-8').strip()\n                      for line in f.readlines()]\n        return labels\n    ```", "```\n    def fetch_random_video(videos_list):\n        video_name = random.choice(videos_list)\n        cache_path = os.path.join(CACHE_DIR, video_name)\n        if not os.path.exists(cache_path):\n            url = request.urljoin(UCF_ROOT, video_name)\n            response = (request\n                        .urlopen(url,\n\n                         context=UNVERIFIED_CONTEXT)\n                        .read())\n            with open(cache_path, 'wb') as f:\n                f.write(response)\n        return cache_path\n    ```", "```\n    def crop_center(frame):\n        height, width = frame.shape[:2]\n        smallest_dimension = min(width, height)\n        x_start = (width // 2) - (smallest_dimension // 2)\n        x_end = x_start + smallest_dimension\n        y_start = (height // 2) - (smallest_dimension // 2)\n        y_end = y_start + smallest_dimension\n        roi = frame[y_start:y_end, x_start:x_end]\n        return roi\n    ```", "```\n    def read_video(path, max_frames=32, resize=(224, 224)):\n        capture = cv2.VideoCapture(path)\n        frames = []\n        while len(frames) <= max_frames:\n            frame_read, frame = capture.read()\n            if not frame_read:\n                break\n            frame = crop_center(frame)\n            frame = cv2.resize(frame, resize)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        capture.release()\n        frames = np.array(frames)\n        return frames / 255.\n    ```", "```\n    def predict(model, labels, sample_video):\n        model_input = tf.constant(sample_video,\n                                  dtype=tf.float32)\n        model_input = model_input[tf.newaxis, ...]\n        logits = model(model_input)['default'][0]\n        probabilities = tf.nn.softmax(logits)\n        print('Top 5 actions:')\n        for i in np.argsort(probabilities)[::-1][:5]:\n            print(f'{labels[i]}:  {probabilities[i] * 100:5.2f}%')\n    ```", "```\n    def save_as_gif(images, video_name):\n        converted_images = np.clip(images * 255, 0, 255)\n        converted_images = converted_images.astype(np.uint8)\n        imageio.mimsave(f'./{video_name}.gif',\n                        converted_images,\n                        fps=25)\n    ```", "```\n    VIDEO_LIST = fetch_ucf_videos()\n    LABELS = fetch_kinetics_labels()\n    ```", "```\n    video_path = fetch_random_video(VIDEO_LIST)\n    sample_video = read_video(video_path)\n    ```", "```\n    model_path = 'https://tfhub.dev/deepmind/i3d-kinetics-400/1'\n    model = tfhub.load(model_path)\n    model = model.signatures['default']\n    ```", "```\n    predict(model, LABELS, sample_video)\n    video_name = video_path.rsplit('/', maxsplit=1)[1][:-4]\n    save_as_gif(sample_video, video_name)\n    ```", "```\nTop 5 actions:\nmopping floor:  75.29%\ncleaning floor:  21.11%\nsanding floor:   0.85%\nspraying:   0.69%\nsweeping floor:   0.64%\n```", "```\n$> pip install tensorflow-hub tensorflow-datasets\n```", "```\n$> wget -nv https://storage.googleapis.com/download.tensorflow.org/data/bair_test_traj_0_to_255.tfrecords -O ~/.keras/datasets/bair_robot_pushing/traj_0_to_255.tfrecords\n```", "```\n    import pathlib\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import tensorflow as tf\n    import tensorflow_hub as tfhub\n    from tensorflow_datasets.core import SplitGenerator\n    from tensorflow_datasets.video.bair_robot_pushing import \\\n        BairRobotPushingSmall\n    ```", "```\n    def plot_first_and_last_for_sample(frames, batch_size):\n        for i in range(4):\n            plt.subplot(batch_size, 2, 1 + 2 * i)\n            plt.imshow(frames[i, 0] / 255.)\n            plt.title(f'Video {i}: first frame')\n            plt.axis('off')\n            plt.subplot(batch_size, 2, 2 + 2 * i)\n            plt.imshow(frames[i, 1] / 255.)\n            plt.title(f'Video {i}: last frame')\n            plt.axis('off')\n    ```", "```\n    def plot_generated_frames_for_sample(gen_videos):\n        for video_id in range(4):\n            fig = plt.figure(figsize=(10 * 2, 2))\n            for frame_id in range(1, 16):\n                ax = fig.add_axes(\n                    [frame_id / 16., 0, (frame_id + 1) / \n                           16., 1],\n                    xmargin=0, ymargin=0)\n                ax.imshow(gen_videos[video_id, frame_id])\n                ax.axis('off')\n    ```", "```\n    def split_gen_func(data_path):\n        return [SplitGenerator(name='test',\n                               gen_kwargs={'filedir': \n                                           data_path})]\n    ```", "```\n    DATA_PATH = str(pathlib.Path.home() / '.keras' / \n                       'datasets' /\n                    'bair_robot_pushing')\n    ```", "```\n    builder = BairRobotPushingSmall()\n    builder._split_generators = lambda _:split_gen_func(DATA_PATH)\n    builder.download_and_prepare()\n    ```", "```\n    BATCH_SIZE = 16\n    dataset = builder.as_dataset(split='test')\n    test_videos = dataset.batch(BATCH_SIZE)\n    for video in test_videos:\n        first_batch = video\n        break\n    ```", "```\n    input_frames = first_batch['image_aux1'][:, ::15]\n    input_frames = tf.cast(input_frames, tf.float32)\n    ```", "```\n    model_path = 'https://tfhub.dev/google/tweening_conv3d_bair/1'\n    model = tfhub.load(model_path)\n    model = model.signatures['default']\n    ```", "```\n    middle_frames = model(input_frames)['default']\n    middle_frames = middle_frames / 255.0\n    ```", "```\n    generated_videos = np.concatenate(\n        [input_frames[:, :1] / 255.0,  # All first frames\n         middle_frames,  # All inbetween frames\n         input_frames[:, 1:] / 255.0],  # All last frames\n        axis=1)\n    ```", "```\n    plt.figure(figsize=(4, 2 * BATCH_SIZE))\n    plot_first_and_last_for_sample(input_frames, \n                                    BATCH_SIZE)\n    plot_generated_frames_for_sample(generated_videos)\n    plt.show()\n    ```", "```\n$> pip install opencv-contrib-python tensorflow-hub\n```", "```\n    import math\n    import os\n    import uuid\n    import cv2\n    import numpy as np\n    import tensorflow as tf\n    import tensorflow_hub as tfhub\n    from tensorflow.keras.utils import get_file\n    ```", "```\n    def produce_embeddings(model, input_frames, input_words):\n        frames = tf.cast(input_frames, dtype=tf.float32)\n        frames = tf.constant(frames)\n        video_model = model.signatures['video']\n        video_embedding = video_model(frames)\n        video_embedding = video_embedding['video_embedding']\n        words = tf.constant(input_words)\n        text_model = model.signatures['text']\n        text_embedding = text_model(words)\n        text_embedding = text_embedding['text_embedding']\n        return video_embedding, text_embedding\n    ```", "```\n    def crop_center(frame):\n        height, width = frame.shape[:2]\n        smallest_dimension = min(width, height)\n        x_start = (width // 2) - (smallest_dimension // 2)\n        x_end = x_start + smallest_dimension\n        y_start = (height // 2) - (smallest_dimension // \n                                        2)\n        y_end = y_start + smallest_dimension\n        roi = frame[y_start:y_end, x_start:x_end]\n        return roi\n    ```", "```\n    def fetch_and_read_video(video_url,\n                             max_frames=32,\n                             resize=(224, 224)):\n        extension = video_url.rsplit(os.path.sep,\n                                     maxsplit=1)[-1]\n        path = get_file(f'{str(uuid.uuid4())}.{extension}',\n                        video_url,\n                        cache_dir='.',\n                        cache_subdir='.')\n    ```", "```\n        capture = cv2.VideoCapture(path)\n        frames = []\n        while len(frames) <= max_frames:\n            frame_read, frame = capture.read()\n            if not frame_read:\n                break\n            frame = crop_center(frame)\n            frame = cv2.resize(frame, resize)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        capture.release()\n        frames = np.array(frames)\n    ```", "```\n        if len(frames) < max_frames:\n            repetitions = math.ceil(float(max_frames) /        \n                                    len(frames))\n            repetitions = int(repetitions)\n            frames = frames.repeat(repetitions, axis=0)\n    ```", "```\n        frames = frames[:max_frames]\n        return frames / 255.0\n    ```", "```\n    URLS = [\n        ('https://media.giphy.com/media/'\n         'WWYSFIZo4fsLC/source.gif'),\n        ('https://media.giphy.com/media/'\n         'fwhIy2QQtu5vObfjrs/source.gif'),\n        ('https://media.giphy.com/media/'\n         'W307DdkjIsRHVWvoFE/source.gif'),\n        ('https://media.giphy.com/media/'\n         'FOcbaDiNEaqqY/source.gif'),\n        ('https://media.giphy.com/media/'\n         'VJwck53yG6y8s2H3Og/source.gif')]\n    ```", "```\n    VIDEOS = [fetch_and_read_video(url) for url in URLS]\n    ```", "```\n    QUERIES = ['beach', 'playing drums', 'airplane taking \n                  off',\n               'biking', 'dog catching frisbee']\n    ```", "```\n    model = tfhub.load\n    ('https://tfhub.dev/deepmind/mil-nce/s3d/1')\n    ```", "```\n    video_emb, text_emb = produce_embeddings(model,\n                                  np.stack(VIDEOS, axis=0),\n                                             np.array(QUERIES))\n    ```", "```\n    scores = np.dot(text_emb, tf.transpose(video_emb))\n    ```", "```\n    first_frames = [v[0] for v in VIDEOS]\n    first_frames = [cv2.cvtColor((f * 255.0).astype('uint8'),\n                                 cv2.COLOR_RGB2BGR) for f \n                                    in  first_frames]\n    ```", "```\n    for query, video, query_scores in zip(QUERIES,VIDEOS,scores):\n        sorted_results = sorted(list(zip(QUERIES,\n                                         first_frames,\n                                         query_scores)),\n                                key=lambda p: p[-1],\n                                reverse=True)\n        annotated_frames = []\n        for i, (q, f, s) in enumerate(sorted_results, \n                                     start=1):\n            frame = f.copy()\n            cv2.putText(frame,\n                        f'#{i} - Score: {s:.2f}',\n                        (8, 15),\n                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale=0.6,\n                        color=(0, 0, 255),\n                        thickness=2)\n            annotated_frames.append(frame)\n        cv2.imshow(f'Results for query “{query}”',\n                   np.hstack(annotated_frames))\n        cv2.waitKey(0)\n    ```"]