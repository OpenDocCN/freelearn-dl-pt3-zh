- en: Overview of TensorFlow and Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a popular library for implementing machine learning-based solutions.
    It includes a low-level API known as TensorFlow core and many high-level APIs,
    including two of the most popular ones, known as TensorFlow Estimators and Keras.
    In this chapter, we will learn about the basics of TensorFlow and build a machine
    learning model using logistic regression to classify handwritten digits as an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow core:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors in TensorFlow core
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Constants
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Placeholders
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Operations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors from Python objects
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Variables
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors from library functions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computation graphs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lazy loading and execution order
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphs on multiple devices – CPU and GPGPU
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with multiple graphs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning, classification, and logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression examples in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression examples in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can follow the code examples in this chapter by using the Jupyter Notebook
    named `ch-01_Overview_of_TensorFlow_and_Machine_Learning.ipynb` that's included
    in the code bundle.
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow** is a popular open source library that''s used for implementing
    machine learning and deep learning. It was initially built at Google for internal
    consumption and was released publicly on November 9, 2015\. Since then, TensorFlow
    has been extensively used to develop machine learning and deep learning models
    in several business domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use TensorFlow in our projects, we need to learn how to program using the
    TensorFlow API. TensorFlow has multiple APIs that can be used to interact with
    the library. The TensorFlow APIs are divided into two levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-level API**: The API known as TensorFlow core provides fine-grained lower
    level functionality. Because of this, this low-level API offers complete control
    while being used on models. We will cover TensorFlow core in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-level API**: These APIs provide high-level functionalities that have
    been built on TensorFlow core and are comparatively easier to learn and implement.
    Some high-level APIs include Estimators, Keras, TFLearn, TFSlim, and Sonnet. We
    will also cover Keras in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow core
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **TensorFlow core** is the lower-level API on which the higher-level TensorFlow
    modules are built. In this section, we will go over a quick overview of TensorFlow
    core and learn about the basic elements of TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Tensors** are the basic components in TensorFlow. A tensor is a multidimensional
    collection of data elements. It is generally identified by shape, type, and rank. **Rank** refers
    to the number of dimensions of a tensor, while **shape** refers to the size of each
    dimension. You may have seen several examples of tensors before, such as in a
    zero-dimensional collection (also known as a scalar), a one-dimensional collection (also
    known as a vector), and a two-dimensional collection (also known as a matrix).'
  prefs: []
  type: TYPE_NORMAL
- en: A scalar value is a tensor of rank 0 and shape []. A vector, or a one-dimensional
    array, is a tensor of rank 1 and shape [`number_of_columns`] or [`number_of_rows`].
    A matrix, or a two-dimensional array, is a tensor of rank 2 and shape [`number_of_rows`,
    `number_of_columns`]. A three-dimensional array is a tensor of rank 3\. In the
    same way, an n-dimensional array is a tensor of rank n.
  prefs: []
  type: TYPE_NORMAL
- en: A tensor can store data of one type in all of its dimensions, and the data type
    of a tensor is the same as the data type of its elements.
  prefs: []
  type: TYPE_NORMAL
- en: The data types that can be found in the TensorFlow library are described at
    the following link: [https://www.tensorflow.org/api_docs/python/tf/DType](https://www.tensorflow.org/api_docs/python/tf/DType).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the most commonly used data types in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **TensorFlow Python API data type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.float16` | 16-bit floating point (half-precision) |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.float32` | 32-bit floating point (single-precision) |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.float64` | 64-bit floating point (double-precision) |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.int8` | 8-bit integer (signed) |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.int16` | 16-bit integer (signed) |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.int32` | 32-bit integer (signed) |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.int64` | 64-bit integer (signed) |'
  prefs: []
  type: TYPE_TB
- en: Use TensorFlow data types for defining tensors instead of native data types
    from Python or data types from NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensors can be created in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By defining constants, operations, and variables, and passing the values to
    their constructor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By defining placeholders and passing the values to `session.run()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By converting Python objects, such as scalar values, lists, NumPy arrays, and
    pandas DataFrames, with the `tf.convert_to_tensor()` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explore different ways of creating Tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Constants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The constant valued tensors are created using the `tf.constant()` function,
    and has the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create some constants with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the preceding code in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line of code defines a constant tensor, `const1`, stores a value of `34`,
    and names it `x1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second line of code defines a constant tensor, `const2`, stores a value
    of `59.0`, and names it `y1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third line of code defines the data type as `tf.float16` for `const3`. Use
    the `dtype` parameter or place the data type as the second argument to denote
    the data type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s print the constants `const1`, `const2`, and `const3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When we print these constants, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Upon printing the previously defined tensors, we can see that the data types
    of `const1` and `const2` are automatically deduced by TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'To print the values of these constants, we can execute them in a TensorFlow
    session with the `tfs.run()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TensorFlow library contains several built-in operations that can be applied on
    tensors. An operation node can be defined by passing input values and saving the
    output in another tensor. To understand this better, let''s define two operations, `op1` and `op2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print `op1` and `op2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows, and shows that `op1` and `op2` are defined as tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To print the output from executing these operations, the `op1` and `op2` tensors
    have to be executed in a TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Some of the built-in operations of TensorFlow include arithmetic operations,
    math functions, and complex number operations.
  prefs: []
  type: TYPE_NORMAL
- en: Placeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While constants store the value at the time of defining the tensor, placeholders
    allow you to create empty tensors so that the values can be provided at runtime.
    The TensorFlow library provides the `tf.placeholder()` function with the following
    signature to create placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example, let''s create two placeholders and print them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that each placeholder has been created as a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define an operation using these placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In TensorFlow, shorthand symbols can be used for various operations. In the
    preceding code, `p1 * p2` is shorthand for `tf.multiply(p1,p2)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command runs `mult_op` in the TensorFlow session and feeds the
    values dictionary (the second argument to the `run()` operation) with the values
    for `p1` and `p2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also specify the values dictionary by using the `feed_dict` parameter
    in the `run()` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at one final example, which is of a vector being fed to the same
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The elements of the two input vectors are multiplied in an element-wise fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors from Python objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tensors can be created from Python objects such as lists, NumPy arrays, and
    pandas DataFrames. To create tensors from Python objects, use the `tf.convert_to_tensor()`
    function with the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s practice doing this by creating some tensors and printing their definitions
    and values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a 0-D tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a 1-D tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a 2-D tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a 3-D tensor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we learned how to define tensor objects of different
    types, such as constants, operations, and placeholders. The values of parameters
    need to be held in an updatable memory location while building and training models
    with TensorFlow. Such updatable memory locations for tensors are known as variables
    in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize this, TensorFlow variables are tensor objects in that their values
    can be modified during the execution of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although `tf.Variable` seems to be similar to `tf.placeholder`, they have certain
    differences. These are listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `tf.placeholder` | `tf.Variable` |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.placeholder` defines the input data that does not get updated over time
    | `tf.Variable` defines values that get updated over time |'
  prefs: []
  type: TYPE_TB
- en: '| `tf.placeholder` does not need to be provided with an initial value at the
    time of definition | `tf.Variable` needs an initial value to be provided at the
    time of definition |'
  prefs: []
  type: TYPE_TB
- en: 'In TensorFlow, a variable can be created with the API function `tf.Variable()`.
    Let''s look at an example of using placeholders and variables and create the following
    model in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/276d3b44-1c56-4249-8e14-0b3625e49ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Define the model parameters `w` and `b` as variables with the initial values
    `[.3]` and `[-0.3]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input placeholder `x` and the output operation node `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the variables and placeholders `w`, `v`, `x`, and `y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output depicts the type of nodes as `Variable`, `Placeholder`, or operation
    node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output indicates that `x` is a `Placeholder` tensor, `y` is an
    operation tensor, and that `w` and `b` are variables with a shape of `(1,)` and
    a data type of `float32`.
  prefs: []
  type: TYPE_NORMAL
- en: The variables in a TensorFlow session have to be initialized before they can
    be used. We can either initialize a single variable by running its initializer
    operation or we can initialize all or a group of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to initialize the `w` variable, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'TensorFlow provides a convenient function that can initialize all of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow also provides the `tf.variables_initializer()` function so that you
    can initialize a specific set of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The global convenience function for initializing these variables can be executed
    in an alternative way. Instead of executing inside the `run()` function of a session
    object, the run function of the object returned by the initializer function itself
    can be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After the variables have been initialized, execute the model to get the output
    for the input values of `x = [1,2,3,4]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Tensors generated from library functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow provides various functions to generate tensors with pre-populated
    values. The generated values from these functions can be stored in a constant
    or variable tensor. Such generated values can also be provided to the tensor constructor
    at the time of initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s generate a 1-D tensor that''s been pre-populated with
    `100` zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the TensorFlow library functions that populate these tensors with different
    values at the time of their definition are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Populating all of the elements of a tensor with similar values: `tf.ones_like()`, `tf.ones()`,` tf.fill()`, `tf.zeros()`,
    and`tf.zeros_like()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Populating tensors with sequences: `tf.range()`,and `tf.lin_space()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Populating tensors with a probability distribution: `tf.random_uniform()`, `tf.random_normal()`, `tf.random_gamma()`,and `tf.truncated_normal()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtaining variables with the tf.get_variable()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a variable is defined with a name that has already been used for another
    variable, then an exception is thrown by TensorFlow. The `tf.get_variable()` function
    makes it convenient and safe to create a variable in place of using the `tf.Variable()` function.
    The `tf.get_variable()` function returns a variable that has been defined with
    a given name. If the variable with the given name does not exist, then it will
    create the variable with the specified initializer and shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The initializer can either be a list of values or another tensor. An initializer
    can also be one of the built-in initializers. Some of these are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.ones_initializer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.constant_initializer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.zeros_initializer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.truncated_normal_initializer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.random_normal_initializer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.random_uniform_initializer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.uniform_unit_scaling_initializer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.orthogonal_initializer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tf.get_variable()` function only returns the global variables when the
    code is run across multiple machines in distributed TensorFlow. The local variables
    can be retrieved by using the `tf.get_local_variable()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sharing or reusing variables**: Getting variables that have already been
    defined promotes reuse. However, an exception will be thrown if the reuse flags
    are not set by using `tf.variable_scope.reuse_variable()` or `tf.variable.scope(reuse=True)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have learned how to define tensors, constants, operations, placeholders,
    and variables, let''s learn about the next level of abstraction in TensorFlow
    that combines these basic elements to form a basic unit of computation: the computation
    graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Computation graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **computation graph** is the basic unit of computation in TensorFlow. A computation
    graph consists of nodes and edges. Each node represents an instance of `tf.Operation`,
    while each edge represents an instance of `tf.Tensor` that gets transferred between
    the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: A model in TensorFlow contains a computation graph. First, you must create the
    graph with the nodes representing variables, constants, placeholders, and operations,
    and then provide the graph to the TensorFlow execution engine. The TensorFlow
    execution engine finds the first set of nodes that it can execute. The execution
    of these nodes starts the execution of the nodes that follow the sequence of the
    computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, TensorFlow-based programs are made up of performing two types of activities
    on computation graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the computation graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing the computation graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A TensorFlow program starts execution with a default graph. Unless another
    graph is explicitly specified, a new node gets implicitly added to the default
    graph. Explicit access to the default graph can be obtained using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the following computation graph represents the addition of three
    inputs to produce the output, that is, ![](img/14e21277-f407-42a8-a84f-64349368adf2.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fdf5a9c-c960-445b-9059-3863ee9eb841.png)'
  prefs: []
  type: TYPE_IMG
- en: In TensorFlow, the add operation node in the preceding diagram would correspond
    to the code `y = tf.add( x1 + x2 + x3 )`.
  prefs: []
  type: TYPE_NORMAL
- en: The variables, constants, and placeholders get added to the graph as and when
    they are created. After defining the computation graph, a session object is instantiated
    that *executes* the operation objects and *evaluates* the tensor objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define and execute a computation graph to calculate ![](img/b44e70e9-36d0-4a12-9e1d-42496c5e860d.png),
    just like we saw in the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Creating and using a session in the `with` block ensures that the session is
    automatically closed when the block is finished. Otherwise, the session has to
    be explicitly closed with the `tfs.close()` command, where `tfs` is the session
    name.
  prefs: []
  type: TYPE_NORMAL
- en: The order of execution and lazy loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The nodes in a computation graph are executed in their order of dependency.
    If node *x* depends on node *y*, then *x* is executed before *y *when the execution
    of *y* is requested. A node is only executed if either the node itself or another
    node depending on it is invoked for execution. This execution philosophy is known
    as lazy loading. As the name implies, the node objects are not instantiated and
    initialized until they are actually required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, it is necessary to control the order of the execution of the nodes in
    a computation graph. This can be done with the `tf.Graph.control_dependencies()` function.
    For example, if the graph has the nodes `l`*,* `m`*,* `n`*,* and `o`, and we want
    to execute `n` and `o` before `l` and `m`, then we would use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This makes sure that any node in the preceding `with` block is executed after
    nodes `n` and `o` have been executed.
  prefs: []
  type: TYPE_NORMAL
- en: Executing graphs across compute devices – CPU and GPGPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A graph can be partitioned into several parts, and each part can be placed
    and executed on different devices, such as a CPU or GPU. All of the devices that
    are available for graph execution can be listed with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is listed as follows (the output for your machine will be different
    because this will depend on the available compute devices in your specific system):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The devices in TensorFlow are identified with the string `/device:<device_type>:<device_idx>`.
    In the last output, `CPU` and `GPU` denote the device type, and `0` denotes the
    device index.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note about the last output is that it shows only one CPU, whereas
    our computer has 8 CPUs. The reason for this is that TensorFlow implicitly distributes
    the code across the CPU units and thus, by default, `CPU:0` denotes all of the
    CPUs available to TensorFlow. When TensorFlow starts executing graphs, it runs
    the independent paths within each graph in a separate thread, with each thread
    running on a separate CPU. We can restrict the number of threads used for this
    purpose by changing the number of `inter_op_parallelism_threads`. Similarly, if,
    within an independent path, an operation is capable of running on multiple threads,
    TensorFlow will launch that specific operation on multiple threads. The number
    of threads in this pool can be changed by setting the number of `intra_op_parallelism_threads`.
  prefs: []
  type: TYPE_NORMAL
- en: Placing graph nodes on specific compute devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To enable the logging of variable placement by defining a config object, set
    the `log_device_placement` property to `true`, and then pass this `config` object
    to the session as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the console window of the Jupyter Notebook is listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, by default, TensorFlow creates the variable and operations nodes on a
    device so that it can get the highest performance. These variables and operations
    can be placed on specific devices by using the `tf.device()` function. Let''s
    place the graph on the CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'In the Jupyter console, we can see that the variables have been placed on the
    CPU and that execution also takes place on the CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Simple placement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow follows the following rules for placing the variables on devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic placement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tf.device()` function can be provided with a function name in place of
    a device string. If a function name is provided, then the function has to return
    the device string. This way of providing a device string through a custom function
    allows complex algorithms to be used for placing the variables on different devices.
    For example, TensorFlow provides a round robin device setter function in `tf.train.replica_device_setter()`.
  prefs: []
  type: TYPE_NORMAL
- en: Soft placement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If a TensorFlow operation is placed on the GPU, then the execution engine must
    have the GPU implementation of that operation, known as the **kernel**. If the
    kernel is not present, then the placement results in a runtime error. Also, if
    the requested GPU device does not exist, then a runtime error is raised. The best
    way to handle such errors is to allow the operation to be placed on the CPU if
    requesting the GPU device results in an error. This can be achieved by setting
    the following `config` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: GPU memory handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the start of the TensorFlow session, by default, a session grabs all of
    the GPU memory, even if the operations and variables are placed only on one GPU
    in a multi-GPU system. If another session starts execution at the same time, it
    will receive an out-of-memory error. This can be solved in multiple ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For multi-GPU systems, set the environment variable `CUDA_VISIBLE_DEVICES=<list
    of device idx>`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The code that's executed after this setting will be able to grab all of the
    memory of the visible GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'For letting the session grab a part of the memory of the GPU, use the config
    option `per_process_gpu_memory_fraction` to allocate a percentage of the memory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This will allocate 50% of the memory in all of the GPU devices.
  prefs: []
  type: TYPE_NORMAL
- en: By combining both of the preceding strategies, you can make only a certain percentage,
    alongside just some of the GPU, visible to the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Limit the TensorFlow process to grab only the minimum required memory at the
    start of the process. As the process executes further, set a config option to
    allow for the growth of this memory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This option only allows for the allocated memory to grow, so the memory is never
    released back.
  prefs: []
  type: TYPE_NORMAL
- en: To find out more about learning techniques for distributing computation across
    multiple compute devices, refer to our book, *Mastering TensorFlow*.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can create our own graphs, which are separate from the default graph, and
    execute them in a session. However, creating and executing multiple graphs is
    not recommended, because of the following disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating and using multiple graphs in the same program would require multiple
    TensorFlow sessions, and each session would consume heavy resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data cannot be directly passed in-between graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hence, the recommended approach is to have multiple subgraphs in a single graph.
    In case we wish to use our own graph instead of the default graph, we can do so
    with the `tf.graph()` command. In the following example, we create our own graph, `g`,
    and execute it as the default graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's put this learning into practice and implement the classification
    of handwritten digital images with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning, classification, and logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now learn about machine learning, classification, and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning refers to the application of algorithms to make computers learn
    from data. The models that are learned by computers are used to make predictions
    and forecasts. Machine learning has been successfully applied in a variety of
    areas, such as natural language processing, self-driving vehicles, image and speech
    recognition, chatbots, and computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning algorithms are broadly categorized into three types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: In supervised learning, the machine learns the model
    from a training dataset that consists of features and labels. The supervised learning
    problems are generally of two types: *regression* and *classification*. Regression
    refers to predicting future values based on the model, while classification refers
    to predicting the categories of the input values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: In unsupervised learning, the machine learns the
    model from a training dataset that consists of features only. One of the most
    common types of unsupervised learning is known as **clustering**. Clustering refers
    to dividing the input data into multiple groups, thus producing clusters or segments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: In reinforcement learning, the agent starts with
    an initial model and then continuously learns the model based on the feedback
    from the environment. A reinforcement learning agent learns or updates the model
    by applying supervised or unsupervised learning techniques as part of the reinforcement
    learning algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These machine learning problems are abstracted to the following equation in
    one form or another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce510bf8-62f2-44b1-8114-679eae1cbd33.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* represents the *target* and *x* represents the *feature*. If *x* is
    a collection of features, it is also called a feature vector and denoted with *X*.
    The model is the function *f* that maps features to targets. Once the computer
    learns *f*, it can use the new values of *x* to predict the values of *y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding simple equation can be rewritten in the context of linear models
    for machine learning as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cca6472f-8449-4fc9-a9ef-16ff8a8f76d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w* is known as the weight and *b* is known as the bias. Thus, the machine
    learning problem now can be stated as a problem of finding w and *b* from the
    current values of *X* so that the equation can now be used to predict the values
    of *y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regression analysis or regression modeling refers to the methods and techniques
    used to estimate relationships among variables. The variables that are used as
    input for regression models are called independent variables, predictors, or features,
    and the output variables from regression models are called dependent variables
    or targets. Regression models are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f835c01-2187-4d76-aa73-58a00674429d.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *Y* is the target variable, *X* is a vector of features, and *β* is a
    vector of parameters (*w*,*b* in the preceding equation).
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification is one of the classical problems in machine learning. Data under consideration
    could belong to one class or another, for example, if the images provided are
    data, they could be pictures of cats or dogs. Thus, the classes, in this case,
    are cats and dogs. Classification means identifying the label or class of the
    objects under consideration. Classification falls under the umbrella of supervised
    machine learning. In classification problems, a training dataset is provided that
    has features or inputs and their corresponding outputs or labels. Using this training
    dataset, a model is trained; in other words, the parameters of the model are computed.
    The trained model is then used on new data to find its correct labels.
  prefs: []
  type: TYPE_NORMAL
- en: Classification problems can be of two types: **binary class** or **multiclass**.
    Binary class means that the data is to be classified into two distinct and discrete
    labels; for example, the patient has cancer or the patient does not have cancer,
    and the images are of cats or dogs and so on. Multiclass means that the data is
    to be classified among multiple classes, for example, an email classification problem
    will divide emails into social media emails, work-related emails, personal emails,
    family-related emails, spam emails, shopping offer emails, and so on. Another
    example would be of pictures of digits; each picture could be labeled between
    0 and 9, depending on what digit the picture represents. In this chapter, we will
    look at examples of both kinds of classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular method for classification is logistic regression. Logistic regression is
    a probabilistic and linear classifier. The probability that the vector of input
    features belongs to a specific class can be described mathematically by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a303e0f-ce6b-4910-9193-b20ff0d71ff8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y* represents the output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*i* represents one of the classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* represents the inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w* represents the weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* represents the biases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*z* represents the regression equation ![](img/c31c0e53-eceb-4e8b-99aa-4ba0c19db77b.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ϕ* represents the smoothing function (or model, in our case)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *ϕ(z)* function represents the probability that *x* belongs to class *i* when *w* and *b* are
    given. Thus, the model has to be trained to maximize the value of this probability.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression for binary classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For binary classification, the model function *ϕ(z)* is defined asthe sigmoid
    function, which can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcf29118-cfc3-4624-bc9b-5350dd509600.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sigmoid function transforms the *y* value to be between the range [0,1].
    Thus, the value of *y=ϕ(z)* can be used to predict the class: if *y* > 0.5, then the
    object belongs to 1, otherwise the object belongs to 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model training means to search for the parameters that minimize the loss
    function, which can either be the sum of squared errors or the sum of mean squared
    errors. For logistic regression, the likelihood is maximized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cab647bc-44e9-466c-bfda-61edd8576e9d.png)'
  prefs: []
  type: TYPE_IMG
- en: However, as it is easier to maximize the log-likelihood, we use the log-likelihood
    (*l(w)*) as the cost function. The loss function (*J(w)*) is written as *-l(w)*,
    and can be minimized by using optimization algorithms such as gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function for binary logistic regression is written mathematically
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d3c322a-f6bc-4f4c-b2d7-778782788a8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ϕ(z)* is the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression for multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When more than two classes are involved, logistic regression is known as multinomial
    logistic regression. In multinomial logistic regression, instead of sigmoid, use the
    softmax function, which can be described mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/825ae940-aa27-4241-ad2d-7443037331a7.png)'
  prefs: []
  type: TYPE_IMG
- en: The softmax function produces the probabilities for each class so that the probabilities
    vector adds up to *1*. At the time of inference, the class with the highest softmax
    value becomes the output or predicted class. The loss function, as we discussed
    earlier, is the negative log-likelihood function, *-l(w)*, that can be minimized
    by the optimizers, such as gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function for multinomial logistic regression is written formally as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/632dd8f0-9c91-453c-a5c0-ed267c5d80b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ϕ(z)* is the softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: We will implement this loss function in the next section. In the following section,
    we will dig into our example for multiclass classification with logistic regression
    in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most popular examples regarding multiclass classification is to
    label the images of handwritten digits. The classes, or labels, in this example
    are *{0,1,2,3,4,5,6,7,8,9}*. The dataset that we are going to use is popularly
    known as MNIST and is available from the following link: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
    The MNIST dataset has 60,000 images for training and 10,000 images for testing.
    The images in the dataset appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ce1802-2f03-4e6a-ac4b-a4289bc12086.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we must import `datasetslib`, a library that was written by us to help
    with examples in this book (available as a submodule of this book''s GitHub repository):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the path to the `datasets` folder in our home directory, which is where
    we want all of the `datasets` to be stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the MNIST data using our `datasetslib` and print the shapes to ensure that
    the data is loaded properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the hyperparameters for training the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholders and parameters for our simple model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the model with `logits` and `y_hat`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `loss` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `optimizer` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the function to check the accuracy of the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `training` loop for each epoch in a TensorFlow session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the evaluation function for each epoch with the test data in the same TensorFlow
    session that was created previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: There you go. We just trained our very first logistic regression model using
    TensorFlow for classifying handwritten digit images and got 74.3% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how writing the same model in Keras makes this process even easier.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Keras** is a high-level library that is available as part of TensorFlow.
    In this section, we will rebuild the same model we built earlier with TensorFlow
    core with Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras takes data in a different format, and so we must first reformat the data
    using `datasetslib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are loading the training images in memory before both
    the training and test images are scaled, which we do by dividing them by `255`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we build the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model with the `sgd` optimizer. Set the categorical entropy as
    the `loss` function and the accuracy as a metric to test the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model for `5` epochs with the training set of images and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model with the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following evaluation scores as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Wow! Using Keras, we can achieve higher accuracy. We achieved approximately
    90% accuracy. This is because Keras internally sets many optimal values for us
    so that we can quickly start building models.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about Keras and to look at more examples, refer to the book *Mastering
    TensorFlow,* from Packt Publications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly covered the TensorFlow library. We covered the TensorFlow
    data model elements, such as constants, variables, and placeholders, and how they
    can be used to build TensorFlow computation graphs. We learned how to create tensors
    from Python objects. Tensor objects can also be generated as specific values,
    sequences, or random valued distributions from various TensorFlow library functions.
  prefs: []
  type: TYPE_NORMAL
- en: We covered the TensorFlow programming model, which includes defining and executing
    computation graphs. These computation graphs have nodes and edges. The nodes represent
    operations and edges represent tensors that transfer data from one node to another.
    We covered how to create and execute graphs, the order of execution, and how to
    execute graphs on multiple compute devices, such as CPU and GPU.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about machine learning and implemented a classification algorithm
    to identify the handwritten digits dataset. The algorithm we implemented is known
    as multinomial logistic regression. We used both TensorFlow core and Keras to
    implement the logistic regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the next chapter, we will look at many projects that will be implemented
    using TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Enhance your understanding by practicing the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the logistic regression model that was given in this chapter so that
    you can use different training rates and observe how it impacts training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use different optimizer functions and observe the impact of different functions
    on training time and accuracy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We suggest the reader learn more by reading the following materials:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mastering TensorFlow* by Armando Fandango.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow tutorials at [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow 1.x Deep Learning Cookbook* by Antonio Gulli and Amita Kapoor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
