<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">A General Production Framework for Deep Learning-Enabled Websites</h1>
                </header>
            
            <article>
                
<p>We have covered decent ground on using industry-grade cloud <strong>Deep Learning</strong> <span>(</span><strong>DL</strong><span>)</span> APIs in our applications in previous chapters and we have learned about their use through practical examples. In this chapter, we will cover a general outline for developing DL-enabled websites. This will require us to bring together all the things that we have learned so far so that we can put them to use in real-life use cases. In this chapter, we will learn how to structure a DL web application for production by first preparing the dataset. We will then train a DL model in Python and then wrap the DL models in APIs using Flask.</p>
<p>The following is a high-level summary of this chapter:</p>
<ul>
<li style="font-weight: 400">Defining our problem statement</li>
<li style="font-weight: 400">Breaking the problem into several components</li>
<li style="font-weight: 400">Building a mental model to bind the project components</li>
<li style="font-weight: 400">How we should be collecting the data</li>
<li style="font-weight: 400">Following a directory structure for our project</li>
<li style="font-weight: 400">Building the project from scratch</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can access the code used in this chapter at <a href="https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9">https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9</a>.<a href="https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9"/></p>
<p>To run the code used in this chapter, you'll need the following software:</p>
<ul>
<li>Python 3.6+</li>
<li>The Python PIL library</li>
<li>NumPy</li>
<li>Pandas</li>
<li>The <strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>)</li>
<li>Flask 1.1.0+ and compatible versions of the following:
<ul>
<li>FlaskForm</li>
<li>wtforms</li>
<li>flask_restful</li>
<li>flask_jsonpify</li>
</ul>
</li>
</ul>
<p>All other installations will be described during the course of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the problem statement</h1>
                </header>
            
            <article>
                
<p>Any project should start with a well-defined problem statement or the project development is bound to suffer. The problem statement governs all the major steps involved in an overall project development pipeline, starting from project planning to project cost.</p>
<p>In a DL-based web project, for example, the problem statement will direct us to the following:</p>
<ul>
<li style="font-weight: 400">Determine what kind of data we would need.</li>
<li style="font-weight: 400">How much complexity there would be in terms of code, planning, and other resources.</li>
<li style="font-weight: 400">What kind of user interface we would develop.</li>
<li style="font-weight: 400">How much human involvement there would be so that an estimate can be prepared on the project’s manpower and so on.</li>
</ul>
<p>Hence, a well-defined problem statement is really required in order for us to get started with further project development.</p>
<p>Imagine being a DL engineer at a company that is planning to build a recommendation system to suggest products from a product listing based on some user-provided criteria. Your boss has asked you to develop a <strong>Proof of Concept</strong> (<strong>PoC</strong>) based on this. So, how should we go about it? As mentioned previously, let’s start by defining the problem statement first.</p>
<p>The main entity that provides inputs to the final recommendation system is a user. Based on the user’s preferences (let’s call the input features preferences for now), the system would provide a list of products that best match their preference. So, long story short, the problem statement can be written as follows:</p>
<p><em>Given a set of input features (user preferences), our task is to suggest a list of products.</em></p>
<p>Now that we have a well-defined problem statement to proceed, let’s go ahead and build up the next steps in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a mental model of the project</h1>
                </header>
            
            <article>
                
<p>Looking at the problem statement, you might feel tempted to open a browser and start searching for some datasets. But when it comes to properly develop a project, definite planning is required to structure it piece by piece. A project without a structure is nothing more than a rudderless ship. So, we will be cautious about this from the start. We will discuss the modules that are going to play a very essential role in our project. This includes several mental considerations as well. I like to call this phase building a mental model of the project.</p>
<p>Let’s take some time to discuss the problem statement further, so as to figure out the essential modules we would need to develop.</p>
<p>Our project concerns recommending products to users based on their preferences. So, in order to perform this recommendation, we would need a system that knows how to understand the set of preferences a user is providing to it. To be able to make sense of these preferences, the system would need some kind of training that we would be implementing <span>DL</span>. But what about preferences? How would they look like? You will often encounter these questions in real-world project situations that need humans in the loop.</p>
<p>Now, think for a second and try to think of the aspects you typically look for while choosing a product to buy. Let’s list them here:</p>
<ul>
<li style="font-weight: 400">What are the specifications of the product? If I want a large size T-shirt, I should not be recommended a small size T-shirt.</li>
<li style="font-weight: 400">What is the cost of the product? Users have a limited amount of money is this recommendation good for their wallet?</li>
<li style="font-weight: 400">What brand is this product? Users often have brand preferences for similar products manufactured by several companies.</li>
</ul>
<p>Note that the preceding pointers are not in any particular order.</p>
<p>So, from the preceding section, we are starting to get a sense of what we would need, which is an interface (which will essentially be a web page, in our case) for a user to provide their preferences. Taking these preferences into account, our system would predict a set of products that it found to be the most appropriate ones. This is where the <span>DL</span> part comes into play. As we will recollect from earlier chapters, for a <span>DL</span> model to work on a given problem, it needs to be trained on some data that represents the problem as closely as possible. Let’s now discuss the data part of our system.</p>
<p class="mce-root">We have a readily available dataset for our project—the Amazon Fine Food Reviews dataset provided by Amazon and created by the Stanford Network Analysis Project team. While the dataset is large in size, we will not be using the full dataset when creating the demonstration in this chapter. An immediate question that might get triggered here is how would the dataset look? We need to formulate a rough plan to decide the following:</p>
<ul>
<li>What features we would be choosing to construct the dataset</li>
<li style="font-weight: 400">Where we would be looking to collect the data</li>
</ul>
<p>Let’s add a bit of enhancement to the original problem statement before proceeding further from here. Here’s the original problem statement:</p>
<p><em>Given a set of input features (user preferences), our task is to suggest a list of products.</em></p>
<p>Users will not like our system if it recommends them substandard products. So, we would modify the problem statement a bit, as follows:</p>
<p><em>Given a set of input features (user preferences), our task is to suggest a list of the best possible products to buy.</em></p>
<p class="mce-root"/>
<p>For our system to recommend a list of the best possible products with respect to a given criterion, it first needs to know the average ratings of the products. Along with the average ratings, it would be useful to have the following information about a particular product, apart from its name:</p>
<ul>
<li style="font-weight: 400">Specifications</li>
<li style="font-weight: 400">Category of product</li>
<li style="font-weight: 400">Seller name</li>
<li style="font-weight: 400">Average price</li>
<li style="font-weight: 400">Expected delivery time</li>
</ul>
<p>While preparing the data, we would look for the previous pointers about a particular product. Now comes the question of where we would be collecting the data from. The answer is Amazon! Amazon is known for its services in the e-commerce industry in providing us with various products and information about them, such as their ratings, product specifications, the price of the items, and so on. But say Amazon does not allow you to download this data directly as a zipped file. In order to get the data from Amazon in the desired form, we would have to resort to web scraping.</p>
<p>Up to this point in the discussion, we are certain on two broad areas of the project:</p>
<ul>
<li style="font-weight: 400">An interface to receive preferences from the user</li>
<li style="font-weight: 400">Data that would represent the problem statement we are dealing with</li>
</ul>
<p>For <span>DL</span> modeling, we will be starting with simple, fully-connected, neural network-based architecture. It’s often useful to start with a simple model and gradually increase the complexity because it makes the code base easier to debug.</p>
<p>So, from this, it is safe enough to say that the following three modules are going to play an essential role in this project:</p>
<ul>
<li style="font-weight: 400">An interface</li>
<li style="font-weight: 400">Data</li>
<li style="font-weight: 400">A <span>DL</span> model</li>
</ul>
<p>Hopefully, you now have a fair idea about approaching the development of a project in the first place. What questions you should be asking at this stage and what considerations you may have to make can be worked out from the involved framework you now have.</p>
<p>We would not want our recommendation system to be biased toward anything. There can be many types of biases hidden in the data and naturally enough, it can cause the <span>DL</span> system that uses it to inherit that bias.</p>
<p>To find out more about different types of biases in machine learning systems, you are encouraged to refer to this article at <a href="https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias">https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias</a>. In our case, a staggering example of bias would be a situation where a male visitor gets product recommendations that are averaged out. The recommendations might only come on the basis of his gender but not based on any other visitor-browsing pattern. This can be erroneous and may have been done mistakenly. But instances like this can make our model very inappropriate. In the next section, we will be discussing a few points to learn how can we avoid bias on the data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Avoiding the chances of getting erroneous data in the first place</h1>
                </header>
            
            <article>
                
<p>What is erroneous data? Are we only talking about data with wrong values? The answer is no. Besides data having wrong or missing values, erroneous data can have subtle but grave errors that may lead to poor training of the model or even bias. So, it is important to identify such erroneous data and remove it before training our model. There are five main ways of identifying these errors:</p>
<ul>
<li>Look for missing values.</li>
<li>Look for values that seem out of scale or possibility—in other words, outliers.</li>
<li>Do not include any features in the dataset that might cause data leakage.</li>
<li>Ensure that all categories of evaluation have a similar number of samples in the dataset.</li>
<li>Ensure that your design of the solution to the problem itself does not introduce a bias.</li>
</ul>
<p>Once we are clear on these points, we are ready to move on to the more specific areas that we need to be careful about during the collection of data. It is important that during data collection a proper plan is prepared to keep in mind all the properties of the data source and the requirements of the problem statement.</p>
<p>Suppose you are scraping data for products from US-based outlets on Amazon and end up searching for products on the Indian version of Amazon instead. The scraper might give you data from India-based outlets, which may not be suitable for recommendation to US-based residents.</p>
<p>Further, since Amazon—and similar services, such as Flipkart—takes the help of recommender systems to target the <em>most suitable</em> products for their customers, during data collection, the scraper should not become prey to such recommendations. It is important that the scraper clears its context every now and then and avoids getting biased results due to the AI put in place by Amazon.</p>
<p>Let's take an example from the Amazon Fine Food Reviews dataset. Though on the first look the dataset looks pretty balanced, we can uncover a lot of bias in the dataset. Consider the length of the text that the customers write for their reviews of products. Let's plot them in a graph against the score they were rated. <span>The following graphs show the plot for products rated 1 and 2 stars:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1231 image-border" src="assets/29b500df-2cd7-4404-b9da-a9cbc49b7fe0.png" style="width:32.92em;height:15.08em;"/></p>
<p>The following graphs show the plot for products rated 3 and 4 stars:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1232 image-border" src="assets/8325444b-269a-4dca-9f6d-b1cb9cb77b8a.png" style="width:34.33em;height:15.58em;"/></p>
<p><span>The following graph shows the plot for products rated 5 stars:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1233 image-border" src="assets/4b4555fa-698d-4e04-bf56-b8a2ba2c4cb1.png" style="width:24.83em;height:20.83em;"/></p>
<p>Notice how more positive reviews have more written text in them. This would directly convert into most of the words in the dataset, leading to a higher rating from the user. Now, consider a scenario where a user writes a lengthy review with a low rating and a generally negative opinion about the product. Since our model is trained to associate larger lengths of reviews to positive ratings, it would mark the negative review as positive.</p>
<p>The bottom line here is that real-world data can contain many edge cases, as shown, and if they are not handled in a proper manner, you will most likely get an erroneous model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How not to build an AI backend</h1>
                </header>
            
            <article>
                
<p>Considering the vastness that web applications can grow to and the strong dependence of nearly every other platform on a backend that runs as a web-based service, it is important for the backend to be well thought of and properly executed. AI-based applications, even in a PoC stage, are often not blazingly fast in responding or take a lot of time to train on the new samples.</p>
<p class="mce-root">While we will be discussing tips and tricks to make a backend that does not choke under pressure due to bottlenecks, we need to lay down a few pointers that need to be avoided in the best possible way when developing an AI-integrated backend for a website.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Expecting the AI part of the website to be real time</h1>
                </header>
            
            <article>
                
<p class="mce-root">AI is computationally expensive and needless to say, this is undesirable for a website that aims to serve its clients in the quickest time possible. While smaller models or using browser AI (such as TensorFlow.js or other libraries) can provide the experience of real-time AI responses, even they suffer issues where the client is in a slow network area or using a low-end device. So, both the methods of in-browser AI models or lightweight AI models replying near instantaneously are subject to device configuration and network bandwidth. Hence, the backend of the website, which is supposed to make quick responses to the client, should ideally be separated from the part that handles the AI model responses. Both, working in parallel, should maintain a common data storage and a proper method of interaction between the two, such that the backend code responsible for responding to the clients has less dependence on the AI model part.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assuming the incoming data from a website is ideal</h1>
                </header>
            
            <article>
                
<p>Even though the website or app corresponding to the project might resemble an ideal method of data collection, the data coming from it must not be assumed to be free of errors. Bad network requests, malicious connections, or simply garbage input provided by users can lead to data that is unfit for training. A non-malicious user may have network issues and refresh the same page 10 to 20 times in a short time frame, which should not add to the viewing-based importance of that page. All data collected from the website must be subject to cleanup and filtering based on the requirements of the model. It must be kept in mind that the challenges faced by websites will almost certainly affect the quality of data collected.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A sample end-to-end AI-integrated web application</h1>
                </header>
            
            <article>
                
<p>Now that we have discussed an overview and the pitfalls to avoid when creating an AI-powered website backend, let's move on to creating one—albeit a rather simple one—that demonstrates the general overview of the solution.</p>
<p>We will cover the following steps, as stated previously:</p>
<ul>
<li>The collection of data as per the problem statement</li>
<li>Cleaning and preprocessing the data</li>
<li>Building the AI model</li>
<li>Creating an interface</li>
<li>Using the AI model on the interface</li>
</ul>
<p>While we have previously discussed the pitfalls of collecting the data, we will briefly discuss here the tools and methods that can be employed to complete the task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data collection and cleanup</h1>
                </header>
            
            <article>
                
<p>For the purpose of collecting data, from a general perspective, there could be several data sources. You could scrape data off websites or simply download some prepared dataset. Other methods could also be employed, such as the following:</p>
<ul>
<li>Generating data on the fly within the runtime of applications/websites</li>
<li>Logging from applications or smart devices</li>
<li>Collecting data directly from users via systematic forms (such as quizzes or surveys)</li>
<li>Collecting data from survey agencies</li>
<li>Observational data measured by specific methods (scientific data) and other ways</li>
</ul>
<p><kbd>beautifulsoup</kbd> is a library <span>commonly used</span> <span>to perform web scraping.</span> <kbd>Scrapy</kbd> <span>is yet another popular tool and can be used very rapidly.</span></p>
<p>The data cleaning would entirely depend on the form of data collected by you and has been discussed in previous chapters of the book. We will assume that you are able to convert your data into a format that is suitable for how you wish to proceed with the model-building part. For the further topics in this section, we will use a prepared dataset titled Amazon Fine Food Reviews, which can be downloaded from <a href="https://www.kaggle.com/snap/amazon-fine-food-reviews">https://www.kaggle.com/snap/amazon-fine-food-reviews.</a> Once you extract the downloaded ZIP file, you'll get the dataset as a file called <kbd>Reviews.csv</kbd>.</p>
<p>A good starting point to observe how to perform web scraping and prepare a clean dataset is <a href="https://github.com/Nilabhra/kolkata_nlp_workshop_2019">https://github.com/Nilabhra/kolkata_nlp_workshop_2019</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the AI model</h1>
                </header>
            
            <article>
                
<p>Now, we will prepare the AI model, which will recommend products based on the user's query. To do so, let's create a new Jupyter notebook.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making the necessary imports</h1>
                </header>
            
            <article>
                
<p>We begin by importing the required Python modules to the project:</p>
<pre>import numpy as np<br/>import pandas as pd<br/>import nltk<br/>from nltk.corpus import stopwords <br/>from nltk.tokenize import WordPunctTokenizer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/><br/># Comment below line if you already have stopwords installed<br/>nltk.download('stopwords')</pre>
<p>We import <kbd>TfidfVectorizer</kbd> to help us create <span><strong>Term Frequency-Inverse Document Frequency</strong> (</span><strong>TF-IDF</strong>) vectors for performing natural language processing. TF-IDF is a numerical measure of how important a word in a single document is, given a number of documents that may or may not contain the words. Numerically, it increases the importance value when a single word occurs frequently in a single document but not in other documents. TF-IDF is so popular that over 80% of the world's natural language-based recommender systems currently use it.</p>
<p>We are also importing <kbd>WordPunctTokenizer</kbd>. A tokenizer performs the function of breaking down a text into elemental tokens. For example, a large paragraph may be broken down into sentences and further into words.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading the dataset and preparing cleaning functions</h1>
                </header>
            
            <article>
                
<p>We will read the Amazon Fine Food Reviews dataset with the <kbd>ISO-8859-1</kbd> encoding. This is only to ensure that we do not lose out on any special symbols used in the text of the review:</p>
<pre>df = pd.read_csv('Reviews.csv', encoding = "ISO-8859-1")<br/>df = df.head(10000)</pre>
<p>Since the dataset is very large, we've restricted our work in this chapter to the first 10,000 rows in the dataset.</p>
<p>We would need to remove stop words from the text and filter out symbols such as brackets and other symbols not natural to written text. We will create a function named <kbd>cleanText()</kbd>, which will perform the filtering and removal of stop words:</p>
<pre>import string<br/>import re<br/><br/>stopwordSet = set(stopwords.words("english"))<br/><br/>def cleanText(line):<br/>    global stopwordSet<br/>    <br/>    line = line.translate(string.punctuation)<br/>    line = line.lower().split()<br/>    <br/>    line = [word for word in line if not word in stopwordSet and len(word) &gt;= 3]<br/>    line = " ".join(line)<br/>    <br/>    return re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", line) </pre>
<p>Using the preceding function, we have removed the stop words and any words shorter than three characters from the text. We have filtered out punctuation and are only keeping the relevant characters from the text.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Slicing out the required data</h1>
                </header>
            
            <article>
                
<p>The dataset contains more data than is useful to us for the demo at hand. We will extract the <kbd>ProductId</kbd>, <kbd>UserId</kbd>, <kbd>Score</kbd>, and <kbd>Text</kbd> columns to prepare our demo. The names of the products are encrypted for privacy reasons, just as the names of the users are encrypted:</p>
<pre>data = df[['ProductId', 'UserId', 'Score', 'Text']]</pre>
<p><span>Keeping data encrypted and free of personal information is a challenge in data science. It is important to remove parts from the dataset that would make it possible to identify the private entities that are a part of the dataset. For example, you would need to remove people and organization names from the text of the review to stop the products and users from being identified, despite them having encrypted product and user IDs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying text cleaning</h1>
                </header>
            
            <article>
                
<p>We will now apply the text filtering and stop word removal function to clean the text in the dataset:</p>
<pre>%%time<br/>data['Text'] = data['Text'].apply(cleanText)</pre>
<p>The time taken for the task is displayed.</p>
<div class="packt_infobox">Note that the preceding code block will only work in Jupyter Notebook and not in normal Python scripts. <span>To run it on normal Python scripts, remove the</span> <kbd>%%time</kbd> <span>command.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Splitting the dataset into train and test parts</h1>
                </header>
            
            <article>
                
<p>Since we have a single dataset, we will break it into two parts, with the feature and label parts separated:</p>
<pre>X_train, X_valid, y_train, y_valid = train_test_split(data['Text'], df['ProductId'], test_size = 0.2) </pre>
<p><span>We will use the <kbd>train_test_split()</kbd> method from the <kbd>sklearn</kbd> module to split the dataset into 80% for training and</span> <span>20% for</span> <span>testing.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Aggregating text about products and users</h1>
                </header>
            
            <article>
                
<p>We will now aggregate the dataset's reviews by users and product IDs. We'll need the reviews for each product to determine what that product would be a good choice for:</p>
<pre>user_df = data[['UserId','Text']]<br/>product_df = data[['ProductId', 'Text']]<br/>user_df = user_df.groupby('UserId').agg({'Text': ' '.join})<br/>product_df = product_df.groupby('ProductId').agg({'Text': ' '.join})</pre>
<p><span>Similarly, reviews aggregated by users will help us determine what a user likes.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating TF-IDF vectorizers of users and products</h1>
                </header>
            
            <article>
                
<p>We will now create two different vectorizers one is for users and the other for products. We will need these vectorizers in place to determine the similarity between the requirements of the users and what the reviews tell us about any given product. First, we will create the vectorizer for users and display its shape:</p>
<pre>user_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=1000)<br/>user_vectors = user_vectorizer.fit_transform(user_df['Text'])<br/>user_vectors.shape</pre>
<p>Then, we will create the vectorizer for products:</p>
<pre>product_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=1000)<br/>product_vectors = product_vectorizer.fit_transform(product_df['Text'])<br/>product_vectors.shape</pre>
<p><span>We use <kbd>WordPunctTokenizer</kbd> to break down the text and use the <kbd>fit_transform</kbd> method of the <kbd>TfidfVectorizer</kbd> object to prepare the vectors, which map the word dictionary to their importance in documents.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an index of users and products by the ratings provided</h1>
                </header>
            
            <article>
                
<p>We use the <kbd>pivot_table</kbd> method of the <kbd>pandas</kbd> module to create a matrix of user ratings against products. We will use this matrix to perform matrix factorization to determine the products that a user likes:</p>
<pre>userRatings = pd.pivot_table(data, values='Score', index=['UserId'], columns=['ProductId'])<br/>userRatings.shape</pre>
<p><span>We will also convert the <kbd>TfidfVectorizer</kbd> vectors for users and products</span> <span>in</span><span>to matrices suitable for matrix factorization</span><span>:</span></p>
<pre>P = pd.DataFrame(user_vectors.toarray(), index=user_df.index, columns=user_vectorizer.get_feature_names())<br/>Q = pd.DataFrame(product_vectors.toarray(), index=product_df.index, columns=product_vectorizer.get_feature_names())</pre>
<p>We can now create the matrix factorization function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the matrix factorization function</h1>
                </header>
            
            <article>
                
<p>We will now create a function to perform matrix factorization. Matrix factorization became a popular family of algorithms used for recommender systems during the Netflix Prize challenge in 2006. It is a family of algorithms that decomposes a user-item matrix into a set of two lower-dimension rectangular matrices that can be multiplied to restore the original higher-order matrix:</p>
<pre>def matrix_factorization(R, P, Q, steps=1, gamma=0.001,lamda=0.02):<br/>    for step in range(steps):<br/>        for i in R.index:<br/>            for j in R.columns:<br/>                if R.loc[i,j]&gt;0:<br/>                    eij=R.loc[i,j]-np.dot(P.loc[i],Q.loc[j])<br/>                    P.loc[i]=P.loc[i]+gamma*(eij*Q.loc[j]-lamda*P.loc[i])<br/>                    Q.loc[j]=Q.loc[j]+gamma*(eij*P.loc[i]-lamda*Q.loc[j])<br/>        e=0<br/>        for i in R.index:<br/>            for j in R.columns:<br/>                if R.loc[i,j]&gt;0:<br/>                    e= e + pow(R.loc[i,j]-np.dot(P.loc[i],Q.loc[j]),2)+lamda*(pow(np.linalg.norm(P.loc[i]),2)+pow(np.linalg.norm(Q.loc[j]),2))<br/>        if e&lt;0.001:<br/>            break<br/>        <br/>    return P,Q</pre>
<p>We then perform the matrix factorization and log the time taken:</p>
<pre>%%time<br/>P, Q = matrix_factorization(userRatings, P, Q, steps=1, gamma=0.001,lamda=0.02)</pre>
<p>After this, we need to save the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving the model as pickle</h1>
                </header>
            
            <article>
                
<p>Now, create a folder called <kbd>api</kbd> in the <kbd>root</kbd> directory of your project. Then, save the trained model, which is the lower-order matrices obtained after factorization of the user-products rating matrix:</p>
<pre>import pickle<br/>output = open('api/model.pkl', 'wb')<br/>pickle.dump(P,output)<br/>pickle.dump(Q,output)<br/>pickle.dump(user_vectorizer,output)<br/>output.close()</pre>
<p>Saving the models as binary pickle files allows us to quickly load them back into the memory during deployment of the model on the backend of the website.</p>
<p>Now that we are done developing the predictive model, we will move on to building an interface for the application to work on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an interface</h1>
                </header>
            
            <article>
                
<p>To build an interface for the web application, we need to think about how we would want our users to interact with the system. In our case, we are expecting the user to be presented with suggestions based on what they search for in a search bar the moment they submit the search query. This means we need the system to respond in real time and generate suggestions on the fly. To build this system, we will create an API that will respond to the search query.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an API to answer search queries</h1>
                </header>
            
            <article>
                
<p>We will create an API that accepts queries in the form of HTTP requests and replies with suggestions of products based on the search query entered by the user. To do so, follow these steps:</p>
<ol>
<li>We will begin by importing the required modules for the API. We discussed these imported modules in the previous section:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import pandas as pd<br/>from nltk.corpus import stopwords<br/>from nltk.tokenize import WordPunctTokenizer<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from flask import Flask, request, render_template, make_response<br/>from flask_wtf import FlaskForm<br/>from wtforms import StringField, validators<br/>import io<br/>from flask_restful import Resource, Api<br/>import string<br/>import re<br/>import pickle<br/>from flask_jsonpify import jsonpify</pre>
<ol start="2">
<li>We will also import the <kbd>Flask</kbd> module to create a quick HTTP server that can serve on a defined route in the form of an API. We will instantiate the <kbd>Flask</kbd> app object as shown:</li>
</ol>
<pre style="padding-left: 60px">DEBUG = True<br/>app = Flask(__name__)<br/>app.config['SECRET_KEY'] = 'abcdefgh'<br/>api = Api(app)</pre>
<p style="padding-left: 60px">The value of <kbd>SECRET_KEY</kbd> in the app configuration is up to you.</p>
<ol start="3">
<li>We will then create a <kbd>class</kbd> function to handle the text input that we receive in the form of a search query from the user:</li>
</ol>
<pre style="padding-left: 60px">class TextFieldForm(FlaskForm):<br/>    text = StringField('Document Content', validators=[validators.data_required()])</pre>
<ol start="4">
<li>To encapsulate the API methods, we wrap them in a <kbd>Flask_Work</kbd> class:</li>
</ol>
<pre style="padding-left: 60px">class Flask_Work(Resource):<br/>    def __init__(self):<br/>        self.stopwordSet = set(stopwords.words("english"))<br/>        pass</pre>
<ol start="5">
<li>The <kbd>cleanText()</kbd> method we used during model creation is again required. It will be used to clean and filter out the search query entered by the user:</li>
</ol>
<pre>    def cleanText(self, line): <br/>        line = line.translate(string.punctuation)<br/>        line = line.lower().split()<br/>        <br/>        line = [word for word in line if not word in self.stopwordSet and len(word) &gt;= 3]<br/>        line = " ".join(line)<br/>        <br/>        return re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", line) </pre>
<ol start="6">
<li>We define a home page for the application, which will be loaded from the <kbd>index.html</kbd> file that we create in the templates later:</li>
</ol>
<pre>    def get(self):<br/>        headers = {'Content-Type': 'text/html'}<br/>        return make_response(render_template('index.html'), 200, headers)</pre>
<ol start="7">
<li>We create the <kbd>post</kbd> method-based prediction route, which will respond with the product suggestions upon receiving the user's search query:</li>
</ol>
<pre>    def post(self):<br/>        f = open('model.pkl', 'rb')<br/>        P, Q, userid_vectorizer = pickle.load(f), pickle.load(f), pickle.load(f)<br/>        sentence = request.form['search']<br/>        test_data = pd.DataFrame([sentence], columns=['Text'])<br/>        test_data['Text'] = test_data['Text'].apply(self.cleanText)<br/>        test_vectors = userid_vectorizer.transform(test_data['Text'])<br/>        test_v_df = pd.DataFrame(test_vectors.toarray(), index=test_data.index,<br/>                                 columns=userid_vectorizer.get_feature_names())<br/><br/>        predicted_ratings = pd.DataFrame(np.dot(test_v_df.loc[0], Q.T), index=Q.index, columns=['Rating'])<br/>        predictions = pd.DataFrame.sort_values(predicted_ratings, ['Rating'], ascending=[0])[:10]<br/><br/>        JSONP_data = jsonpify(predictions.to_json())<br/>        return JSONP_data</pre>
<ol start="8">
<li>We attach the <kbd>Flask_Work</kbd> class to the <kbd>Flask</kbd> server. This completes the script on running. We have put an API in place that suggests products based on the user's search query:</li>
</ol>
<pre style="padding-left: 60px">api.add_resource(Flask_Work, '/')<br/><br/><br/>if __name__ == '__main__':<br/>    app.run(host='127.0.0.1', port=4000, debug=True)</pre>
<p style="padding-left: 60px">Save this file as <kbd>main.py</kbd>. With the API script created, we need to host the server.</p>
<ol start="9">
<li>To do so on a local machine, run the following command in the terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>python main.py</strong></pre>
<p style="padding-left: 60px">This will start the server on the computer on port <kbd>4000</kbd>, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1234 image-border" src="assets/5f7e2b07-8b1d-4ba9-a2ac-c4002fc9a22a.png" style="width:31.67em;height:8.50em;"/></p>
<p>However, we still need to prepare a user interface to use this API. We will do so in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an interface to use the API</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>We will now create a simple, minimal UI to use the API we created. In essence, we will create a single search bar where the user enters the product or product specification that they want and the API returns recommendations based on the user's query. We will not be discussing the code for building the UI, but we have included it in the GitHub repository, which can be found at</span> <a href="http://tiny.cc/DL4WebCh9">http://tiny.cc/DL4WebCh9</a><span>.</span></p>
<p>This UI will be visible at <kbd>http://127.0.0.1:4000</kbd> once you start the server, as shown in step 9 of the <em>Creating an API to answer search queries</em> <span>section</span><span>.</span></p>
<p class="mce-root"><span>The interface we created looks like this:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1235 image-border" src="assets/65ce8f08-b037-4d00-a8b5-a94172b2d2bd.png" style="width:32.58em;height:9.92em;"/></p>
<p>The user enters a search query and gets recommendations, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1236 image-border" src="assets/e161cee5-2998-42d1-a4ef-4ed108ad54ad.png" style="width:25.17em;height:21.50em;"/></p>
<p>Our application does not have the benefit of saving user sessions. Also, it does not have parameters for the expected budget of the user, which is often a deciding factor in whether the product is a good fit for the user. It is easy to add these features to web applications and leverage their benefits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>As a general overview, web applications that hone the power of <span>DL</span> have a few set methods to do so via APIs, in-browser JavaScript, or by silently embedding <span>DL</span> models in the backend of the application. In this chapter, we saw how to use the most common of these methods—an API-based <span>DL</span> web application—while at the same time, we saw a rough overview of how to design similar solutions. We covered the thought process that goes into the identification of the problem statement and a subsequent solution, along with the pitfalls and pain points to avoid during the design of a web application that integrates <span>DL</span> models.</p>
<p>In the next chapter, we will cover an end to end project that integrates <span>DL</span> on web applications for security purposes. We will see how <span>DL</span> can help us recognize suspicious activity and block spam users.</p>


            </article>

            
        </section>
    </body></html>