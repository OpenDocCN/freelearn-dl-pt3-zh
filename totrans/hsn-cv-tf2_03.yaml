- en: TensorFlow Basics and Training a Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow** is a numerical processing library used by researchers and machine
    learning practitioners. While you can perform any numerical operation with TensorFlow,
    it is mostly used to train and run deep neural networks. This chapter will introduce
    you to the core concepts of TensorFlow 2 and walk you through a simple example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with TensorFlow 2 and Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and training a simple computer vision model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow and Keras core concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we will use TensorFlow 2\. You can find detailed installation
    instructions for the different platforms at [https://www.tensorflow.org/install](https://www.tensorflow.org/install/).
  prefs: []
  type: TYPE_NORMAL
- en: If you plan on using your machine's GPU, make sure you install the corresponding
    version, `tensorflow-gpu`. It must be installed along with the CUDA Toolkit, a
    library provided by NVIDIA ([https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone)).
  prefs: []
  type: TYPE_NORMAL
- en: Installation instructions are also available in the README on GitHub at [https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter02).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with TensorFlow 2 and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before detailing the core concepts of TensorFlow, we will start with a brief
    introduction of the framework and a basic example.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow was originally developed at Google to allow researchers and developers
    to conduct machine learning research. It was originally defined as *an interface
    for expressing machine learning algorithms, and an implementation for executing
    such algorithms.*
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow primarily offers to simplify the deployment of machine learning solutions
    on various platforms—computer CPUs, computer GPUs, mobile devices, and, more recently,
    in the browser. On top of that, TensorFlow offers many useful functions for creating
    machine learning models and running them at scale. In 2019, TensorFlow 2 was released
    with a focus on ease of use while maintaining good performance.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to TensorFlow 1.0's concepts is available in [Appendix](59767fa2-b254-47a4-a39a-3f8c826490fa.xhtml), *Migrating
    from TensorFlow 1 to TensorFlow 2* of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The library was open sourced in November 2015\. Since then, it has been improved
    and used by users all around the world. It is considered one of the platforms
    of choice for research. It is also one of the most active deep learning frameworks
    in terms of GitHub activity.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow can be used by beginners as well as experts. The TensorFlow API has
    different levels of complexity, allowing newcomers to start with a simple API
    and experts to create very complex models at the same time. Let's explore those
    different levels.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow's main architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow''s architecture has several levels of abstraction. Let''s first
    introduce the lowest layer and find our way to the uppermost layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/554cef8f-62a0-4b4c-abb8-9ab79075c69f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Diagram of the TensorFlow architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Most deep learning computations are coded in C++. To run operations on the GPU,
    TensorFlow uses a library developed by NVIDIA called **CUDA**. This is the reason
    you need to install CUDA if you want to exploit GPU capabilities and why you cannot
    use GPUs from another hardware manufacturer.
  prefs: []
  type: TYPE_NORMAL
- en: The Python **low-level** **API** then wraps the C++ sources. When you call a
    Python method in TensorFlow, it usually invokes C++ code behind the scenes. This
    wrapper layer allows users to work more quickly because Python is considered easier
    to use than C++ and does not require compilation. This Python wrapper makes it
    possible to perform extremely basic operations such as matrix multiplication and
    addition.
  prefs: []
  type: TYPE_NORMAL
- en: At the top sits the **high-level API**, made of two components—Keras and the
    Estimator API. **Keras** is a user-friendly, modular, and extensible wrapper for
    TensorFlow. We will introduce it in the next section. The **Estimator API** contains
    several pre-made components that allow you to build your machine learning model
    easily. You can consider them building blocks or templates.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, a **model** usually refers to a neural network that was trained
    on data. A model is composed of an architecture, matrix weights, and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First released in 2015, Keras was designed as an interface to enable fast experimentation
    with neural networks. As such, it relied on TensorFlow or **Theano** (another
    deep learning framework, now deprecated) to run deep learning operations. Known
    for its user-friendliness, it was the library of choice for beginners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since 2017, TensorFlow has integrated Keras fully, meaning that you can use
    it without installing anything other than TensorFlow. Throughout this book, we
    will rely on `tf.keras` instead of the standalone version of Keras. There are
    a few minor differences between the two versions, such as compatibility with TensorFlow''s
    other modules and the way models are saved. For this reason, readers must make
    sure to use the correct version, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In your code, import `tf.keras` and not `keras`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go through the `tf.keras` documentation on TensorFlow's website and not the *keras.io*
    documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using external Keras libraries, make sure they are compatible with `tf.keras`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some saved models might not be compatible between different versions of Keras.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two versions will continue to co-exist for the foreseeable future, and `tf.keras`
    will become more and more integrated with TensorFlow. To illustrate the power
    and simplicity of Keras, we will now use it to implement a simple neural network.
  prefs: []
  type: TYPE_NORMAL
- en: A simple computer vision model using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we delve into the core concepts of TensorFlow, let's start with a classical
    example of computer vision—digit recognition with the **Modified National Institute
    of Standards and Technology** (**MNIST**) dataset. The dataset was introduced
    in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml), *Computer Vision and
    Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the data. It is made up of 60,000 images for the training
    set and 10,000 images for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is common practice to import TensorFlow with the alias `tf` for faster reading
    and typing. It is also common to use `x` to denote input data, and `y` to represent
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.keras.datasets` module provides quick access to download and instantiate
    a number of classical datasets. After importing the data using `load_data`, notice
    that we divide the array by `255.0` to get a number in the range [*0, 1*] instead
    of [*0, 255*]. It is common practice to normalize data, either in the [*0, 1*]
    range or in the [*-1, 1*] range.
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now move on to building the actual model. We will use a very simple
    architecture composed of two **fully connected** (also called **dense**) layers.
    Before we explore the architecture, let''s have a look at the code. As you can
    see, Keras code is very concise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our model is a linear stack of layers, we start by calling the `Sequential`
    function. We then add each layer one after the other. Our model is composed of
    two fully connected layers. We build it layer by layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flatten**: This will take the 2D matrix representing the image pixels and
    turn it into a 1D array. We need to do this before adding a fully connected layer.
    The *28* × *28* images are turned into a vector of size *784*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dense** of size *128*: This will turn the *784* pixel values into 128 activations
    using a weight matrix of size *128* × *784* and a bias matrix of size *128*. In
    total, this means *100,480* parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dense** of size *10*: This will turn the *128* activations into our final
    prediction. Notice that because we want probabilities to sum to *1*, we will use
    the `softmax` activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `softmax` function takes the output of a layer and returns probabilities
    that sum up to `1`. It is the activation of choice for the last layer of a classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you can get a description of the model, the outputs, and their weights
    using `model.summary()`. Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With its architecture set and weights initialized, the model is now ready to
    be trained for the chosen task.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras makes training extremely simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `.compile()` on the model we just created is a mandatory step. A few
    arguments must be specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '`optimizer`: This is the component that will perform the gradient descent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss`: This is the metric we will optimize. In our case, we choose cross-entropy,
    just like in the previous chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metrics`: These are additional metric functions evaluated during training
    to provide further visibility of the model''s performance (unlike `loss`, they
    are not used in the optimization process).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras `loss` named `sparse_categorical_crossentropy` performs the same cross-entropy
    operation as `categorical_crossentropy`, but the former directly takes the ground
    truth labels as inputs, while the latter requires the ground truth labels to be
    *one-hot* encoded already before hand. Using the `sparse_...` loss thus saves
    us from manually having to transform the labels.
  prefs: []
  type: TYPE_NORMAL
- en: Passing `'sgd'` to Keras is equivalent to passing `tf.keras.optimizers.SGD()`.
    The former option is easier to read, while the latter makes it possible to specify
    parameters such as a custom learning rate. The same goes for the loss, metrics,
    and most arguments passed to Keras methods.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we call the `.fit()` method. It is very similar to the interface used
    in **scikit-learn**, another popular machine learning library. We will train for
    five epochs, meaning that we will iterate over the whole train dataset five times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we set `verbose` to `1`. This will allow us to get a progress bar
    with the metrics we chose earlier, the loss, and the **Estimated Time of Arrival**
    (**ETA**). The ETA is an estimate of the remaining time before the end of the
    epoch. Here is what the progress bar looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59872f2a-0192-4997-98c9-8bb6fd4c1b1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Screenshot of the progress bar displayed by Keras in verbose mode'
  prefs: []
  type: TYPE_NORMAL
- en: Model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml), *Computer
    Vision and Neural Networks*, you will notice that our model is overfitting—training
    accuracy is greater than test accuracy. If we train the model for five epochs,
    we end up with an accuracy of 97% on the test set. This is about 2% better than
    in the previous chapter, where we achieved 95%. State-of-the-art algorithms attain
    99.79% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We followed three main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading the data**: In this case, the dataset was already available. During
    future projects, you may need additional steps to gather and clean the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Creating the model**: This step was made easy by using Keras—we defined the
    architecture of the model by adding sequential layers. Then, we selected a loss,
    an optimizer, and a metric to monitor.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training the model**: Our model worked pretty well the first time. On more
    complex datasets, you will usually need to fine-tune parameters during training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The whole process was extremely simple thanks to Keras, the high-level API of
    TensorFlow. Behind this simple API, the library hides a lot of the complexity.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 2 and Keras in detail
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have introduced the general architecture of TensorFlow and trained our first
    model using Keras. Let's now walk through the main concepts of TensorFlow 2\.
    We will explain several core concepts of TensorFlow that feature in this book,
    followed by some advanced notions. While we may not employ all of them in the
    remainder of the book, you might find it useful to understand some open source
    models that are available on GitHub or to get a deeper understanding of the library.
  prefs: []
  type: TYPE_NORMAL
- en: Core concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Released in spring 2019, the new version of the framework is focused on simplicity
    and ease of use. In this section, we will introduce the concepts that TensorFlow
    relies on and cover how they evolved from version 1 to version 2.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow takes its name from a mathematical object called a **tensor**. You
    can imagine tensors as N-dimensional arrays. A tensor could be a scalar, a vector,
    a 3D matrix, or an N-dimensional matrix.
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental component of TensorFlow, the `Tensor` object is used to store
    mathematical values. It can contain fixed values (created using `tf.constant`)
    or changing values (created using `tf.Variable`).
  prefs: []
  type: TYPE_NORMAL
- en: In this book, *tensor* denotes the mathematical concept, while *Tensor* (with
    a capital *T*) corresponds to the TensorFlow object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each `Tensor` object has the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Type**: `string`, `float32`, `float16`, or `int8`, among others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shape**: The dimensions of the data. For instance, the shape would be `()`
    for a scalar, `(n)` for a vector of size *n*, and `(n, m)` for a 2D matrix of
    size *n* × *m*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rank**: The number of dimensions, *0* for a scalar, `1` for a vector, and *2*
    for a 2D matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some tensors can have partially unknown shapes. For instance, a model accepting
    images of variable sizes could have an input shape of `(None, None, 3)`. Since
    the height and the width of the images are not known in advance, the first two
    dimensions are set to `None`. However, the number of channels (`3`, corresponding
    to red, blue, and green) is known and is therefore set.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow uses tensors as inputs as well as outputs. A component that transforms
    input into output is called an **operation**. A computer vision model is therefore
    composed of multiple operations.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow represents these operations using a **directed acyclic graph** (**DAC**),
    also referred to as a **graph**. In TensorFlow 2, graph operations have disappeared
    under the hood to make the framework easier to use. Nevertheless, the graph concept
    remains important to understand how TensorFlow really works.
  prefs: []
  type: TYPE_NORMAL
- en: 'When building the previous example using Keras, TensorFlow actually built a
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12472b05-6c87-4dd4-a778-1c3344dd07f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3: A simplified graph corresponding to our model. In practice, each
    node is composed of smaller operations (such as matrix multiplications and additions)
  prefs: []
  type: TYPE_NORMAL
- en: 'While very simple, this graph represents the different layers of our model
    in the form of operations. Relying on graphs has many advantages, allowing TensorFlow
    to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Run part of the operations on the CPU and another part on the GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run different parts of the graph on different machines in the case of a distributed
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the graph to avoid unnecessary operations, leading to better computational
    performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, the graph concept allows TensorFlow models to be portable. A single
    graph definition can be run on any kind of device.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow 2, graph creation is no longer handled by the user. While managing
    graphs used to be a complex task in TensorFlow 1, the new version greatly improves
    usability while still maintaining performance. In the next section, we will peek
    into the inner workings of TensorFlow and briefly explore how graphs are created.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing lazy execution to eager execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main change in TensorFlow 2 is **eager execution**. Historically, TensorFlow
    1 always used **lazy execution** by default. It is called *lazy* because operations
    are not run by the framework until asked specifically to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a very simple example to illustrate the difference between
    lazy and eager execution, summing the values of two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that `tf.add(a, b)` could be replaced by `a + b` since TensorFlow overloads
    many Python operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the previous code depends on the TensorFlow version. With TensorFlow
    1 (where lazy execution is the default mode), the output would be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, with TensorFlow 2 (where eager execution is the default mode), you
    would get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, the output is a Tensor. In the second case, the operation has
    been run eagerly and we can observe directly that the Tensor contains the result
    (`[1 2 4]`). In the first case, the Tensor contains information about the addition
    operation (`Add:0`), but not the result of the operation.
  prefs: []
  type: TYPE_NORMAL
- en: In eager mode, you can access the value of a Tensor by calling the `.numpy()`
    method. In our example, calling `c.numpy()` returns `[1 2 4]` (as a NumPy array).
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow 1, more code would be needed to compute the result, making the
    development process more complex. Eager execution makes code easier to debug (as
    developers can peak at the value of a Tensor at any time) and easier to develop.
    In the next section, we will detail the inner workings of TensorFlow and look
    at how it builds graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Creating graphs in TensorFlow 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll start with a simple example to illustrate graph creation and optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Assuming `a`, `b`, and `c` are Tensor matrices, this code computes two new values: `d` and
    `e`. Using eager execution, TensorFlow would compute the value for `d` and then
    compute the value for `e`.
  prefs: []
  type: TYPE_NORMAL
- en: Using lazy execution, TensorFlow would create a graph of operations. Before
    running the graph to get the result, a **graph optimizer** would be run. To avoid
    computing `a * b` twice, the optimizer would **cache** the result and reuse it
    when necessary. For more complex operations, the optimizer could enable **parallelism**
    to make computation faster. Both techniques are important when running large and
    complex models.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw, running in eager mode implies that every operation is run when defined.
    Therefore, such optimizations cannot be applied. Thankfully, TensorFlow includes
    a module to work around this—TensorFlow **AutoGraph**.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorFlow AutoGraph and tf.function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TensorFlow AutoGraph module makes it easy to turn eager code into a graph,
    allowing automatic optimization. To do so, the easiest way is to add the `tf.function`
    decorator on top of your function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: A **Python decorator** is a concept that allows functions to be wrapped, adding
    functionalities or altering them. Decorators start with an `@` (the "at" symbol).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we call the `compute` function for the first time, TensorFlow will transparently
    create the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da22bed4-9aa1-4c0d-b5f3-18addb45a933.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: The graph automatically generated by TensorFlow when calling the
    compute function for the first time'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow AutoGraph can convert most Python statements, such as `for` loops,
    `while` loops, `if` statements, and iterations. Thanks to graph optimizations,
    graph execution can sometimes be faster than eager code. More generally, AutoGraph
    should be used in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: When the model needs to be exported to other devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When performance is paramount and graph optimizations can lead to speed improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another advantage of graphs is their **automatic differentiation**. Knowing
    the full list of operations, TensorFlow can easily compute the gradient for each
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in order to compute the gradient, the operations need to be **differentiable**.
    Some of them, such as `tf.math.argmax`, are not. Using them in a `loss` function
    will most likely cause the automatic differentiation to fail. It is up to the
    user to make sure that the loss is differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: However, since, in eager mode, each operation is independent from one another,
    automatic differentiation is not possible by default. Thankfully, TensorFlow 2
    provides a way to perform automatic differentiation while still using eager mode—the
    **gradient tape**.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagating errors using the gradient tape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The gradient tape allows easy backpropagation in eager mode. To illustrate this,
    we will use a simple example. Let's assume that we want to solve the equation
    *A* × *X = B*, where *A* and *B* are constants. We want to find the value of *X*
    to solve the equation. To do so, we will try to minimize a simple loss, *abs(A
    × X - B)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, this translates to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to update the value of *X*, we would like to compute the gradient of the
    loss with respect to *X*. However, when printing the content of the loss, we obtain
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In eager mode, TensorFlow computed the result of the operation instead of storing
    the operation! With no information on the operation and its inputs, it would be
    impossible to automatically differentiate the `loss` operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is where the gradient tape comes in handy. By running our loss computation
    in the context of `tf.GradientTape`, TensorFlow will automatically record all
    operations and allow us to replay them backward afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code defines a single training step. Every time `train_step` is
    called, the loss is computed in the context of the gradient tape. The context
    is then used to compute the gradient. The *X* variable is then updated. Indeed,
    we can see *X* converging toward the value that solves the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that in the very first example of this chapter, we did not make
    use of the gradient tape. This is because Keras models encapsulate training inside
    the `.fit()` function—there's no need to update the variables manually. Nevertheless,
    for innovative models or when experimenting, the gradient tape is a powerful tool
    that allows automatic differentiation without much effort. Readers can find a
    more practical use of the gradient tape in the regularization notebook of [Chapter
    3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Keras models and layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first section of this chapter, we built a simple Keras Sequential model.
    The resulting `Model` object contains numerous useful methods and properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.inputs` and `.outputs`: Provide access to the inputs and outputs of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.layers`: Lists the model''s layers as well as their shape.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.summary()`: Prints the architecture of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.save()`: Saves the model, its architecture, and the current state of training.
    It is very useful for resuming training later on. Models can be instantiated from
    a file using `tf.keras.models.load_model()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.save_weights()`: Only saves the weights of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While there is only one type of Keras model object, they can be built in a variety
    of ways.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential and functional APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead of employing the Sequential API, like at the beginning of this chapter,
    you can instead use the functional API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the code is slightly longer than it previously was. Nevertheless,
    the functional API is much more versatile and expressive than the Sequential API.
    The former allows for branching models (that is, for building architectures with
    multiple parallel layers for instance), while the latter can only be used for
    linear models. For even more flexibility, Keras also offers the possibility to
    subclass the `Model` class, as described in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml),
    *Modern Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of how a `Model` object is built, it is composed of layers. A layer
    can be seen as a node that accepts one or several inputs and returns one or several
    outputs, similar to a TensorFlow operation. Its weights can be accessed using
    `.get_weights()` and set using `.set_weights()`. Keras provides pre-made layers
    for the most common deep learning operations. For more innovative or complex models,
    `tf.keras.layers.Layer` can also be subclassed.
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Keras callbacks** are utility functions that you can pass to a Keras model''s
    `.fit()` method to add functionality to its default behavior. Multiple callbacks
    can be defined, which will be called by Keras either before or after each batch
    iteration, each epoch, or the whole training procedure. Predefined Keras callbacks
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CSVLogger`: Logs training information in a CSV file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EarlyStopping`: Stops training if the loss or a metric stops improving. It
    can be useful in avoiding overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LearningRateScheduler`: Changes the learning rate on each epoch according
    to a schedule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReduceLROnPlateau`: Automatically reduces the learning rate when the loss
    or a metric stops improving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also possible to create custom callbacks by subclassing `tf.keras.callbacks.Callback`,
    as demonstrated in later chapters and their code samples.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, the AutoGraph module, the `tf.function` decorator, and the gradient
    tape context make graph creation and management very simple—if not invisible.
    However, a lot of the complexity is hidden from the user. In this section, we
    will explore the inner workings of these modules.
  prefs: []
  type: TYPE_NORMAL
- en: This section presents advanced concepts that are not required throughout the
    book, but it may be useful for you to understand more complex TensorFlow code.
    More impatient readers can skip this part and come back to it later.
  prefs: []
  type: TYPE_NORMAL
- en: How tf.function works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, when calling a function decorated with `tf.function` for
    the first time, TensorFlow will create a graph corresponding to the function's
    operations. TensorFlow will then cache the graph so that the next time the function
    is called, graph creation will not be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, let''s create a simple `identity` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This function will print a message every time TensorFlow creates a graph corresponding
    to its operation. In this case, since TensorFlow is caching the graph, it will
    print something only the first time it is run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'However, note that if we change the input type, TensorFlow will recreate a
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This behavior is explained by the fact that TensorFlow graphs are defined by
    their operations and the shapes and types of the tensors they receive as inputs.
    Therefore, when the input type changes, a new graph needs to be created. In TensorFlow
    vocabulary, when a `tf.function` function has defined input types, it becomes
    a **concrete function**.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, every time a decorated function is run for the first time, TensorFlow
    caches the graph corresponding to the input types and input shapes. If the function
    is run with inputs of a different type, TensorFlow will create a new graph and
    cache it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, it might be useful to log information every time a concrete function
    is run and not just the first time. To do so, use `tf.print`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Instead of printing information only the first time, this function will print
    `Running identity` every single time it is run.
  prefs: []
  type: TYPE_NORMAL
- en: Variables in TensorFlow 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To hold the model weights, TensorFlow uses `Variable` instances. In our Keras
    example, we can list the content of the model by accessing `model.variables`.
    It will return the list of all variables contained in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, variable management (including naming) has been entirely handled
    by Keras. As we saw earlier, it is also possible to create our own variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that for large projects, it is recommended to name variables to clarify
    the code and ease debugging. To change the value of a variable, use the `Variable.assign`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Failing to use the `.assign()` method would create a new `Tensor` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Finally, deleting the Python reference to a variable will remove the object
    itself from the active memory, releasing space for other variables to be created.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We trained a simple model on a very small dataset. When using larger models
    and datasets, more computing power is necessary—this often implies multiple servers.
    The `tf.distribute.Strategy` API defines how multiple machines communicate together
    to train a model efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the strategies defined by TensorFlow are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MirroredStrategy`: For training on multiple GPUs on a single machine. Model
    weights are kept in sync between each device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultiWorkerMirroredStrategy`: Similar to `MirroredStategy`, but for training
    on multiple machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ParameterServerStrategy`: For training on multiple machines. Instead of syncing
    the weights on each device, they are kept on a parameter server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TPUStrategy`: For training on Google''s **Tensor Processing Unit** (**TPU**)
    chip.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TPU is a custom chip made by Google, similar to a GPU, designed specifically
    to run neural network computations. It is available through Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a distribution strategy, create and compile your model in its scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that you will probably have to increase the batch size, as each device
    will now receive a small subset of each batch. Depending on your model, you may
    also have to change the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Estimator API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in the first part of this chapter that the Estimator API is a high-level
    alternative to the Keras API. Estimators simplify training, evaluation, prediction,
    and serving.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of Estimators. Pre-made Estimators are very simple models
    provided by TensorFlow, allowing you to quickly try out machine learning architectures.
    The second type is custom Estimators, which can be created using any model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Estimators handle all the small details of a model's life cycle—data queues,
    exception handling, recovering from failure, periodic checkpoints, and many more.
    While using Estimators was considered best practice in TensorFlow 1, in version
    2, it is recommended to use the Keras API.
  prefs: []
  type: TYPE_NORMAL
- en: Available pre-made Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, the available pre-made Estimators are `DNNClassifier`,
    `DNNRegressor`, `LinearClassifier`, and `LinearRegressor`. Here, DNN stands for
    **deep neural network**. Combined Estimators based on both architectures are also
    available—`DNNLinearCombinedClassifier` and `DNNLinearCombinedRegressor`.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, classification is the process of predicting a discrete
    category, while regression is the process of predicting a continuous number.
  prefs: []
  type: TYPE_NORMAL
- en: '**Combined Estimators**, also called **deep-n-wide models**, make use of a
    linear model (for memorization) and a deep model (for generalization). They are
    mostly used for recommendation or ranking models.'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-made Estimators are suitable for some machine learning problems. However,
    they are not suitable for computer vision problems, as there are no pre-made Estimators
    with convolutions, a powerful type of layer described in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Training a custom Estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way to create an Estimator is to convert a Keras model. After the
    model has been compiled, call `tf.keras.estimator.model_to_estimator()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `model_dir` argument allows you to specify a location where the checkpoints
    of the model will be saved. As mentioned earlier, Estimators will automatically
    save checkpoints for our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training an Estimator requires the use of an **input function**—a function
    that returns data in a specific format. One of the accepted formats is a TensorFlow
    dataset. The dataset API is described in depth in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml),
    *Training on Complex and Scarce Datasets*. For now, we''ll define the following
    function, which returns the dataset defined in the first part of this chapter
    in the correct format, in batches of *32* samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this function is defined, we can launch the training with the Estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Just like Keras, the training part is very simple, as Estimators handle the
    heavy lifting.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the main library, TensorFlow offers numerous tools that are useful
    for machine learning. While some of them are shipped with TensorFlow, others are
    grouped under **TensorFlow Extended** (**TFX**) and **TensorFlow Addons**. We
    will now introduce the most commonly used tools.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the progress bar we used in the first example of this chapter displayed
    useful information, we might want to access more detailed graphs. TensorFlow provides
    a powerful tool for monitoring—**TensorBoard**. Installed by default with TensorFlow,
    it is also very easy to use when combined with Keras''s callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In this updated code, we pass the TensorBoard callback to the `model.fit()`
    method. By default, TensorFlow will automatically write the loss and the metrics
    to the folder we specified. We can then launch TensorBoard from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This command outputs a URL that we can then open to display the TensorBoard
    interface. In the Scalars tab, we can find graphs displaying the loss and the
    accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49b87d26-016f-4802-ae6b-13175e0ce6f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Two graphs displayed by TensorBoard during training'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will see in this book, training a deep learning model requires a lot
    of fine-tuning. Therefore, it is essential to monitor how your model is performing.
    TensorBoard allows you to do precisely this. The most common use case is to monitor
    the evolution of the loss of your model over time. But you can also do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Plot any metric (such as accuracy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display input and output images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the execution time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draw your model's graph representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorBoard is very versatile, and there are many ways to use it. Each piece
    of information is stored in `tf.summary`—this can be scalars, images, histograms,
    or text. For instance, to log a scalar you might first create a summary writer
    and log information using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we specify the step—it could be the epoch number, the
    batch number, or custom information. It will correspond to the *x *axis in TensorBoard
    figures. TensorFlow also provides tools for generating aggregates. To manually
    log accuracy, you could use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Other metrics are available, such as `Mean`, `Recall`, and `TruePositives`.
    While setting up the logging of metrics in TensorBoard may seem a bit complicated
    and time-consuming, it is an essential part of the TensorFlow toolkit. It will
    save you countless hours of debugging and manual logging.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Addons and TensorFlow Extended
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow Addons** is a collection of extra functionalities gathered into
    a single repository ([https://github.com/tensorflow/addons](https://github.com/tensorflow/addons)).
    It hosts some of the newer advancements in deep learning that are too unstable
    or not used by enough people to justify adding them to the main TensorFlow library.
    It also acts as a replacement for `tf.contrib`, which was removed from TensorFlow
    1.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow Extended** is an end-to-end machine learning platform for TensorFlow.
    It offers several useful tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow Data Validation**: A library for exploring and validating machine
    learning data. You can use it before even building your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow Transform**: A library for preprocessing data. It allows you to
    make sure training and evaluation data are processed the same way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow Model Analysis**: A library for evaluating TensorFlow models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow Serving**: A serving system for machine learning models. Serving
    is the process of delivering predictions from a model, usually through a REST
    API:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/009f5850-47f5-4c19-af52-9861f8716370.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: End-to-end process of creating and using a deep learning model'
  prefs: []
  type: TYPE_NORMAL
- en: As seen in *Figure 2.6*, these tools fulfill the goal of being end to end, covering
    every step of the process of building and using a deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Lite and TensorFlow.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main version of TensorFlow is designed for Windows, Linux, and Mac computers.
    To operate on other devices, a different version of TensorFlow is necessary. **TensorFlow
    Lite** is designed to run model predictions (inference) on mobile phones and embedded
    devices. It is composed of a converter transforming TensorFlow models to the required
    `.tflite` format and an interpreter that can be installed on mobile devices to
    run inferences.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, **TensorFlow.js** (also referred to as **tfjs**) was developed
    to empower almost any web browser with deep learning. It does not require any
    installation from the user and can sometimes make use of the device's GPU acceleration.
    We detail the use of TensorFlow Lite and TensorFlow.js in [Chapter 9](e8935e55-c3b5-419e-a86a-43eba3ff4dad.xhtml),
    *Optimizing Models and Deploying on Mobile Devices*.
  prefs: []
  type: TYPE_NORMAL
- en: Where to run your model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As computer vision models process large amounts of data, they take a long time
    to train. Because of this, training on your local computer can take a considerable
    amount of time. You will also notice that creating efficient models requires a
    lot of iterations. Those two insights will drive your decision regarding where
    to train and run your models. In this section, we will compare the different options
    available to train and use your model.
  prefs: []
  type: TYPE_NORMAL
- en: On a local machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coding your model on your computer is often the fastest way to get started.
    As you have access to a familiar environment, you can easily change your code
    as often as needed. However, personal computers, especially laptops, lack the
    computing power to train a computer vision model. Training on a GPU may be between
    10 and 100 times faster than using a CPU. This is why it is recommended to use
    a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Even if your computer has a GPU, only very specific models can run TensorFlow.
    Your GPU must be compatible with CUDA, NVIDIA's computing library. At the time
    of writing, the latest version of TensorFlow requires a CUDA compute capability
    of 3.5 or higher.
  prefs: []
  type: TYPE_NORMAL
- en: Some laptops are compatible with external GPU enclosures, but this defeats the
    purpose of a portable computer. Instead, a practical way is to run your model
    on a remote computer that has a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: On a remote machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, you can rent powerful machines with GPUs by the hour. Pricing varies,
    depending on the GPU power and the provider. It usually costs around $1 per hour
    for a single GPU machine, with the price going down every day. If you commit to
    renting the machine for the month, you can get good computing power for around
    $100 per month. Considering the time you will save waiting for the model to train,
    it often makes economic sense to rent a remote machine.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to build your own deep learning server. Note that this requires
    investment and assembly, and that GPUs consume large amounts of electricity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have secured access to a remote machine, you have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Run Jupyter Notebook on the remote server. Jupyter Lab or Jupyter Notebook will
    then be accessible using your browser, anywhere on the planet. It is a very convenient
    way of performing deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sync your local development folder and run your code remotely. Most IDEs have
    a feature to sync your local code with a remote server. This allows you to code
    in your favorite IDE while still enjoying a powerful machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Colab, based on Jupyter notebooks, allows you to run notebooks in the
    cloud for *free*. You can even enable GPU mode. Colab has limited storage space
    and a limit of 8 hours of consecutive running time. While it is the perfect tool
    for getting started or experimenting, it is not convenient for larger models.
  prefs: []
  type: TYPE_NORMAL
- en: On Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To run TensorFlow on a remote machine, you will need to manage it yourself—installing
    the correct software, making sure it is up to date, and turning the server on
    and off. While it is still possible to do so for one machine, and you sometimes
    need to distribute the training among numerous GPUs, using Google Cloud ML to
    run TensorFlow allows you to focus on your model and not on operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find that Google Cloud ML is useful for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Training your model quickly thanks to elastic resources in the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking for the best model parameters in the shortest amount of time possible
    using parallelization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once your model is ready, serving predictions without having to run your own
    prediction server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the details for packaging, sending, and running your model are available
    in the Google Cloud ML documentation ([https://cloud.google.com/ml-engine/docs/](https://cloud.google.com/ml-engine/docs/)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by training a basic computer vision model using
    the Keras API. We introduced the main concepts behind TensorFlow 2—tensors, graphs,
    AutoGraph, eager execution, and the gradient tape. We also detailed some of the
    more advanced concepts of the framework. We went through the main tools surrounding
    the use of deep learning with the library, from TensorBoard for monitoring, to
    TFX for preprocessing and model analysis. Finally, we covered where to run your
    model depending on your needs.
  prefs: []
  type: TYPE_NORMAL
- en: With these powerful tools in hand, you are now ready to discover modern computer
    vision models in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Keras in relation to TensorFlow, and what is its purpose?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does TensorFlow use graphs, and how do you create them manually?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between eager execution mode and lazy execution mode?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you log information in TensorBoard, and how do you display it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main differences between TensorFlow 1 and TensorFlow 2?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
