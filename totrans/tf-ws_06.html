<html><head></head><body>
		<div>
			<div id="_idContainer228" class="Content">
			</div>
		</div>
		<div id="_idContainer229" class="Content">
			<h1 id="_idParaDest-113"><a id="_idTextAnchor125"/>6. Regularization and Hyperparameter Tuning</h1>
		</div>
		<div id="_idContainer254" class="Content">
			<p class="callout-heading"><a id="_idTextAnchor126"/>Overview</p>
			<p class="callout">In this chapter, you will be introduced to hyperparameter tuning. You will get hands-on experience in using TensorFlow to perform regularization on deep learning models to reduce overfitting. You will explore concepts such as L1, L2, and dropout regularization. Finally, you will look at the Keras Tuner package for performing automatic hyperparameter tuning.</p>
			<p class="callout">By the end of the chapter, you will be able to apply regularization and tune hyperparameters in order to reduce the risk of overfitting your model and improve its performance.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor127"/>Introduction</h1>
			<p>In the previous chapter, you learned how classification models can solve problems when the response variable is discrete. You also saw different metrics used to assess the performance of such classifiers. You got hands-on experience in building and training binary, multi-class, and multi-label classifiers with TensorFlow.</p>
			<p>When evaluating a model, you will face three different situations: model overfitting, model underfitting, and model performing. The last one is the ideal scenario, in which a model is accurately predicting the right outcome and is generalizing to unseen data well. </p>
			<p>If a model is underfitting, it means it is neither achieving satisfactory performance nor accurately predicting the target variable. In this case, a data scientist can try tuning different hyperparameters and finding the best combination that will boost the accuracy of the model. Another possibility is to improve the input dataset by handling issues such as the cleanliness of the data or feature engineering.</p>
			<p>A model is overfitting when it can only achieve high performance on the training set and performs poorly on the test set. In this case, the model has only learned patterns from the data relevant to the data used for training. Regularization helps to lower the risk of overfitting.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor128"/>Regularization Techniques </h1>
			<p>The main goal of a data scientist is to train a model that achieves high performance and generalizes to unseen data well. The model should be able to predict the right outcome on both data used during the training process and new data. This is the reason why a model is always assessed on the test set. This set of data serves as a proxy to evaluate the ability of the model to output correct results while in production. </p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/B16341_06_01.jpg" alt="Figure 6.1: Model not overfitting or underfitting&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1: Model not overfitting or underfitting</p>
			<p>In <em class="italic">Figure 6.1</em>, the linear model (line) seems to predict relatively accurate results for both the training (circles) and test (triangles) sets.</p>
			<p>But sometimes a model fails to generalize well and will overfit the training set. In this case, the performance of the model will be very different between the training and test sets.</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/B16341_06_02.jpg" alt="Figure 6.2: Model overfitting&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2: Model overfitting</p>
			<p><em class="italic">Figure 6.2</em> shows the model (line) has only learned to predict accurately for the training set (circles) and is performing badly on the test set (triangles). This model is clearly overfitting.</p>
			<p>Fortunately, there are <strong class="bold">regularization techniques</strong> that a data scientist can use to reduce and prevent overfitting, defined in the following sections.</p>
			<h2 id="_idParaDest-116">L1<a id="_idTextAnchor129"/> Regularization</h2>
			<p>For deep learning models, overfitting happens when some of the features have higher weights than they should. The model puts too much emphasis on these features as it believes they are extremely important for predicting the training set. Unfortunately, these features are less relevant for the test set or any new unseen data. Regularization techniques try to penalize such weights and reduce their importance to the model predictions.</p>
			<p>There are multiple ways to perform regularization. One of them is to add a regularization component to the cost function:</p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="image/B16341_06_03.jpg" alt="Figure 6.3: Adding a regularization component to the cost function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3: Adding a regularization component to the cost function</p>
			<p>The addition of this regularization component will lead the weights of the model to be smaller as neural networks try to reduce the cost function while performing forward and backward propagations.</p>
			<p>One very popular regularization component is L1. Its formula is as follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="image/B16341_06_04.jpg" alt="Figure 6.4: L1 regularization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4: L1 regularization</p>
			<p><img src="image/B16341_06_04a.png" alt="Formula"/> is a hyperparameter that defines the level of penalization of the L1 regularization. <strong class="source-inline">W</strong> is the weight of the model. With L1 regularization, you add the sum of the absolute value of the weights to the model loss.</p>
			<p>L1 regularization is sometimes referred to as <strong class="bold">feature selection</strong> as it tends to push the weights of non-relevant features to <strong class="source-inline">0</strong>. Therefore, only the relevant features are used for making predictions.</p>
			<p>In TensorFlow, you can define L1 regularization with the following code snippet:</p>
			<p class="source-code">from tensorflow.keras.regularizers import l1</p>
			<p class="source-code">l1_reg = l1(l=0.01)</p>
			<p>The <strong class="source-inline">l</strong> parameter corresponds to the <img src="image/B16341_06_04b.png" alt="Formula 2"/> hyperparameter. The instantiated L1 regularization can then be added to any layer from TensorFlow Keras:</p>
			<p class="source-code">from tensorflow.keras.layers import Dense</p>
			<p class="source-code">Dense(10, kernel_regularizer=l1_reg)</p>
			<p>In the preceding example, you added the L1 regularizer that you defined earlier to a fully connected layer of <strong class="source-inline">10</strong> units.</p>
			<h2 id="_idParaDest-117">L2 Reg<a id="_idTextAnchor130"/>ularization</h2>
			<p><em class="italic">L2</em> regularization is similar to <em class="italic">L1</em> in that it adds a regularization component to the cost function, but its formula is different: </p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/B16341_06_05.jpg" alt="Figure 6.5: L2 regularization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5: L2 regularization</p>
			<p>L2 regularization tends to decrease the weights of the non-relevant features. They will be close to <strong class="source-inline">0</strong>, but not exactly <strong class="source-inline">0</strong>. So, it reduces the impact of these features but does not disable them as L1 does.</p>
			<p>In TensorFlow, you can define L2 regularization as follows:</p>
			<p class="source-code">from tensorflow.keras.regularizers import l2</p>
			<p class="source-code">from tensorflow.keras.layers import Dense</p>
			<p class="source-code">l2_reg = l2(l=0.01)</p>
			<p class="source-code">Dense(20, kernel_regularizer=l2_reg)</p>
			<p>In the preceding example, you defined an L2 regularizer and added it to a fully connected layer of <strong class="source-inline">20</strong> units.</p>
			<p>TensorFlow provides another regularizer class that combines both L1 and L2 regularizers. You can instantiate it with the following code snippet:</p>
			<p class="source-code">from tensorflow.keras.regularizers </p>
			<p class="source-code">import l1_l2</p>
			<p class="source-code">l1_l2_reg = l1_l2(l1=0.01, l2=0.001)</p>
			<p>In the preceding example, you instantiated L1 and L2 regularizers and specified the factors for L1 and L2 as <strong class="source-inline">0.01</strong> and <strong class="source-inline">0.001</strong>, respectively. You can observe that more weights are put on the L1 regularization compared to L2. These values are hyperparameters that can be fine-tuned depending on the dataset.</p>
			<p>In the next exercise, you will put this into practice as you apply L2 regularization to a model.</p>
			<h2 id="_idParaDest-118">Exercis<a id="_idTextAnchor131"/>e 6.01: Predicting a Connect-4 Game Outcome Using the L2 Regularizer</h2>
			<p>In this exercise, you will build and train two multi-class models in TensorFlow that will predict the class outcome for player one in the game Connect-4. </p>
			<p>Each observation of this dataset contains different situations of the game with different positions. For each of these situations, the model tries to predict the outcome for the first player: win, loss, or draw. The first model will not have any regularization, while the second will have L2 regularization:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be accessed here: <a href="https://packt.link/xysRc">https://packt.link/xysRc</a>.</p>
			<p class="callout">The original dataset can be found here: <a href="http://archive.ics.uci.edu/ml/datasets/Connect-4">http://archive.ics.uci.edu/ml/datasets/Connect-4</a>.</p>
			<ol>
				<li>Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> that contains the URL to the dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">          '/The-TensorFlow-Workshop/master/Chapter06/dataset'\</p><p class="source-code">          '/connect-4.csv'</p></li>
				<li>Load the dataset into a DataFrame called <strong class="source-inline">data</strong> using the <strong class="source-inline">read_csv()</strong> function and provide the URL to the CSV file. Print the first five rows using the <strong class="source-inline">head()</strong> function:<p class="source-code">data = pd.read_csv(file_url)</p><p class="source-code">data.head()</p><p>The expected output will be as follows:</p><div id="_idContainer237" class="IMG---Figure"><img src="image/B16341_06_06.jpg" alt="Figure 6.6: First five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.6: First five rows of the dataset</p><p>The preceding figure shows the first five rows of the dataset.</p></li>
				<li>Extract the target variable (the <strong class="source-inline">class</strong> column) using the <strong class="source-inline">pop()</strong> method and save it in a variable named <strong class="source-inline">target</strong>:<p class="source-code">target = data.pop('class')</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias. Then, import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Set the seed as <strong class="source-inline">8</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong> and store it in a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function and the input shape as <strong class="source-inline">(42,)</strong>, which corresponds to the number of features from the dataset. Save it in a variable called <strong class="source-inline">fc1</strong>:<p class="source-code">fc1 = Dense(512, input_shape=(42,), activation='relu')</p></li>
				<li>Create three fully connected layers of <strong class="source-inline">512</strong>, <strong class="source-inline">128</strong>, and <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save them in three variables, called <strong class="source-inline">fc2</strong>, <strong class="source-inline">fc3</strong>, and <strong class="source-inline">fc4</strong>, respectively:<p class="source-code">fc2 = Dense(512, activation='relu')</p><p class="source-code">fc3 = Dense(128, activation='relu')</p><p class="source-code">fc4 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of three units (corresponding to the number of classes) with <strong class="source-inline">Dense()</strong> and specify softmax as the activation function. Save it in a variable called <strong class="source-inline">fc5</strong>:<p class="source-code">fc5 = Dense(3, activation='softmax')</p></li>
				<li>Sequentially add all five fully connected layers to the model using the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(fc2)</p><p class="source-code">model.add(fc3)</p><p class="source-code">model.add(fc4)</p><p class="source-code">model.add(fc5)</p></li>
				<li>Print the summary of the model using the <strong class="source-inline">summary()</strong> method:<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer238" class="IMG---Figure"><img src="image/B16341_06_07.jpg" alt="Figure 6.7: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 6.7: Summary of the model architecture</p></li>
				<li>Instantiate a <strong class="source-inline">SparseCategoricalCrossentropy()</strong> function from <strong class="source-inline">tf.keras.losses</strong> and save it in a variable called <strong class="source-inline">loss</strong>:<p class="source-code">loss = tf.keras.losses.SparseCategoricalCrossentropy()</p></li>
				<li>Instantiate <strong class="source-inline">Adam()</strong> from <strong class="source-inline">tf.keras.optimizers</strong> with <strong class="source-inline">0.001</strong> as the learning rate and save it in a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the model using the <strong class="source-inline">compile()</strong> method, and specify the optimizer and loss you created in <em class="italic">steps 14</em> and <em class="italic">15</em> and <strong class="source-inline">accuracy</strong> as the metric to be displayed:<p class="source-code">model.compile(optimizer=optimizer, loss=loss, \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method for five epochs and split the data into a validation set with 20% of the data:<p class="source-code">model.fit(data, target, epochs=5, validation_split=0.2)</p><p>The expected output will be as follows:</p><div id="_idContainer239" class="IMG---Figure"><img src="image/B16341_06_08.jpg" alt="Figure 6.8: Logs of the training process&#13;&#10;"/></div><p class="figure-caption">Figure 6.8: Logs of the training process</p><p>The preceding output reveals that the model is overfitting. It achieved an accuracy score of <strong class="source-inline">0.85</strong> on the training set and only <strong class="source-inline">0.58</strong> on the validation set. Now, train another model with L2 regularization.</p></li>
				<li>Create five fully connected layers similar to the previous model's and specify the L2 regularizer for the <strong class="source-inline">kernel_regularizer</strong> parameters. Use the value <strong class="source-inline">0.001</strong> for the regularizer factor. Save the layers in five variables, called <strong class="source-inline">reg_fc1</strong>, <strong class="source-inline">reg_fc2</strong>, <strong class="source-inline">reg_fc3</strong>, <strong class="source-inline">reg_fc4</strong>, and <strong class="source-inline">reg_fc5</strong>:<p class="source-code">reg_fc1 = Dense(512, input_shape=(42,), activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l2(l=0.1))</p><p class="source-code">reg_fc2 = Dense(512, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l2(l=0.1))</p><p class="source-code">reg_fc3 = Dense(128, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l2(l=0.1))</p><p class="source-code">reg_fc4 = Dense(128, activation='relu', \</p><p class="source-code">                kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                     .l2(l=0.1))</p><p class="source-code">reg_fc5 = Dense(3, activation='softmax')</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong>, store it in a variable called <strong class="source-inline">model2</strong>, and add sequentially all five fully connected layers to the model using the <strong class="source-inline">add()</strong> method:<p class="source-code">model2 = tf.keras.Sequential()</p><p class="source-code">model2.add(reg_fc1)</p><p class="source-code">model2.add(reg_fc2)</p><p class="source-code">model2.add(reg_fc3)</p><p class="source-code">model2.add(reg_fc4)</p><p class="source-code">model2.add(reg_fc5)</p></li>
				<li>Print the summary of the model:<p class="source-code">model2.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer240" class="IMG---Figure"><img src="image/B16341_06_09.jpg" alt="Figure 6.9: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 6.9: Summary of the model architecture</p></li>
				<li>Compile the model using the <strong class="source-inline">compile()</strong> method, and specify the optimizer and loss you created in <em class="italic">steps 14</em> and <em class="italic">15</em> and <strong class="source-inline">accuracy</strong> as the metric to be displayed:<p class="source-code">model2.compile(optimizer=optimizer, loss=loss, \</p><p class="source-code">               metrics=['accuracy'])</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method for five epochs and split the data into a validation set with 20% of the data:<p class="source-code">model2.fit(data, target, epochs=5, validation_split=0.2)</p><p>The expected output will be as follows:</p><div id="_idContainer241" class="IMG---Figure"><img src="image/B16341_06_10.jpg" alt="Figure 6.10: Logs of the training process&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.10: Logs of the training process</p>
			<p>With the addition of L2 regularization, the model now has similar accuracy scores between the training (<strong class="source-inline">0.68</strong>) and test (<strong class="source-inline">0.58</strong>) sets. The model is not overfitting as much as before, but its performance is not great. </p>
			<p>Now that you know how to apply L1 and L2 regularization to neural networks, the next section will introduce another regularization technique, called <strong class="bold">dropout</strong>. </p>
			<h2 id="_idParaDest-119">Dropout Regu<a id="_idTextAnchor132"/>larization</h2>
			<p>Unlike L1 and L2 regularization, dropout is a regularization technique specific to neural networks. The logic behind it is very simple: the networks will randomly change the weights of some features to <strong class="source-inline">0</strong>. This will force the model to rely on other features that would have been ignored and, therefore, bump up their weights.</p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/B16341_06_11.jpg" alt="Figure 6.11: Dropout of neural networks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11: Dropout of neural networks</p>
			<p>The preceding example shows an architecture with a dropout of 50%. This means that 50% of the units of the model are turned off at each iteration. The following code snippet shows you how to create a dropout layer of 50% in TensorFlow:</p>
			<p class="source-code">from tensorflow.keras.layers import Dropout</p>
			<p class="source-code">do = Dropout(0.5)</p>
			<p>In the next e<a id="_idTextAnchor133"/>xercise, you will extend the previous model by applying dropout.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor134"/>Exercise 6.02: Predicting a Connect-4 Game Outcome Using Dropout</h2>
			<p>In this exercise, you will be using the same dataset as for <em class="italic">Exercise 6.01</em>, <em class="italic">Predicting a Connect-4 Game Outcome Using the L2 Regularizer</em>. You will build and train a multi-class model in TensorFlow that will predict the class outcome for player 1 in the game Connect-4 using the dropout technique as a regularizer:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be accessed here: <a href="https://packt.link/0Bo1B">https://packt.link/0Bo1B</a>.</p>
			<p class="callout">The original dataset can be found here: <a href="http://archive.ics.uci.edu/ml/datasets/Connect-4">http://archive.ics.uci.edu/ml/datasets/Connect-4</a>.</p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable, <strong class="source-inline">file_url</strong>, to store the URL of the dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">           '/The-TensorFlow-Workshop/master/Chapter06/dataset'\</p><p class="source-code">           '/connect-4.csv'</p></li>
				<li>Load the dataset into a DataFrame, <strong class="source-inline">data</strong>, using the <strong class="source-inline">read_csv()</strong> function and provide the URL of the CSV file. Print the first five rows using the <strong class="source-inline">head()</strong> function:<p class="source-code">data = pd.read_csv(file_url)</p><p class="source-code">data.head()</p><p>The expected output will be as follows:</p><div id="_idContainer243" class="IMG---Figure"><img src="image/B16341_06_12.jpg" alt="Figure 6.12: First five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.12: First five rows of the dataset</p></li>
				<li>Extract the target variable (the column called <strong class="source-inline">class</strong>) using the <strong class="source-inline">pop()</strong> method, and save it in a variable called <strong class="source-inline">target</strong>:<p class="source-code">target = data.pop('class')</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias. Then, import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Set the seed as <strong class="source-inline">8</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a sequential model using <strong class="source-inline">tf.keras.Sequential()</strong> and store it in a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Create a fully connected layer of <strong class="source-inline">512</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function and the input shape as <strong class="source-inline">(42,)</strong>, which corresponds to the number of features from the dataset. Save it in a variable called <strong class="source-inline">fc1</strong>:<p class="source-code">fc1 = Dense(512, input_shape=(42,), activation='relu')</p></li>
				<li>Create three fully connected layers of <strong class="source-inline">512</strong>, <strong class="source-inline">128</strong>, and <strong class="source-inline">128</strong> units with <strong class="source-inline">Dense()</strong> and specify ReLu as the activation function. Save them in three variables, called <strong class="source-inline">fc2</strong>, <strong class="source-inline">fc3</strong>, and <strong class="source-inline">fc4</strong>, respectively:<p class="source-code">fc2 = Dense(512, activation='relu')</p><p class="source-code">fc3 = Dense(128, activation='relu')</p><p class="source-code">fc4 = Dense(128, activation='relu')</p></li>
				<li>Create a fully connected layer of three units (corresponding to the number of classes) with <strong class="source-inline">Dense()</strong> and specify softmax as the activation function. Save it in a variable called <strong class="source-inline">fc5</strong>:<p class="source-code">fc5 = Dense(3, activation='softmax')</p></li>
				<li>Sequentially add all five fully connected layers to the model with a dropout layer of <strong class="source-inline">0.75</strong> in between each of them using the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(Dropout(0.75))</p><p class="source-code">model.add(fc2)</p><p class="source-code">model.add(Dropout(0.75))</p><p class="source-code">model.add(fc3)</p><p class="source-code">model.add(Dropout(0.75))</p><p class="source-code">model.add(fc4)</p><p class="source-code">model.add(Dropout(0.75))</p><p class="source-code">model.add(fc5)</p></li>
				<li>Print the summary of the model:<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer244" class="IMG---Figure"><img src="image/B16341_06_13.jpg" alt="Figure 6.13: Summary of the model architecture&#13;&#10;"/></div><p class="figure-caption">Figure 6.13: Summary of the model architecture</p></li>
				<li>Instantiate a <strong class="source-inline">SparseCategoricalCrossentropy()</strong> function from <strong class="source-inline">tf.keras.losses</strong> and save it in a variable called <strong class="source-inline">loss</strong>:<p class="source-code">loss = tf.keras.losses.SparseCategoricalCrossentropy()</p></li>
				<li>Instantiate <strong class="source-inline">Adam()</strong> from <strong class="source-inline">tf.keras.optimizers</strong> with <strong class="source-inline">0.001</strong> as the learning rate and save it in a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the model using the <strong class="source-inline">compile()</strong> method, specify the optimizer and loss, and set <strong class="source-inline">accuracy</strong> as the metric to be displayed:<p class="source-code">model.compile(optimizer=optimizer, loss=loss, \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method for five epochs and split the data into a validation set with 20% of the data:<p class="source-code">model.fit(data, target, epochs=5, validation_split=0.2)</p><p>The output will be as follows:</p><div id="_idContainer245" class="IMG---Figure"><img src="image/B16341_06_14.jpg" alt="Figure 6.14: Logs of the training process&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.14: Logs of the training process</p>
			<p>With the addition of dropout, the model now has similar accuracy scores between the training (<strong class="source-inline">0.69</strong>) and test (<strong class="source-inline">0.59</strong>) sets. The model is not overfitting as much as before, but its performance is still less than ideal.</p>
			<p>You have now seen how to apply L1, L2, or dropout as regularizers for a model. In deep learning, there is another very simple technique that you can apply to avoid overfitting—that is, early stopping. </p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor135"/>Early Stopping</h2>
			<p>A<a id="_idTextAnchor136"/>nother reason why neural networks overfit is due to the training process. The more you train the model, the more it will try to improve its performance. By training the model for a longer duration (more epochs), it will at some point start finding patterns that are only relevant to the training set. In such a case, the difference between the scores of the training and test (or validation) sets will start increasing after a certain number of epochs. </p>
			<p>To prevent this situation, you can stop the model training when the difference between the two sets starts to increase. This technique is called <strong class="bold">early stopping</strong>.</p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/B16341_06_15.jpg" alt="Figure 6.15: Early stopping to prevent overfitting&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15: Early stopping to prevent overfitting</p>
			<p>The preceding graph shows the loss value of a model on the training and test (or validation) sets according to the number of epochs. In early epochs, the loss value is quite different between the two sets. As the training goes on, the models start learning the relevant patterns for making predictions and both losses converge. But after a while, they start diverging. The loss of the training set keeps decreasing while the one for the test (or validation) set is increasing. You can observe that the model is overfitting and is optimizing only for the training set. Stopping the training at the point when the difference between the two losses starts to increase prevents the model from overfitting.</p>
			<p>In TensorFlow, you can achieve this by setting up callbacks that analyze the performance of the models at each epoch and compare its score between the training and test sets. To define an early stopping callback, you will do the following:</p>
			<p class="source-code">from tensorflow.keras.callbacks import EarlyStopping</p>
			<p class="source-code">EarlyStopping(monitor='val_accuracy', patience=5)</p>
			<p>The preceding code shows you how to instantiate an <strong class="source-inline">EarlyStopping</strong> class that will monitor the accuracy score of the validation set and wait for five successive epochs with no improvement before stopping the training process.</p>
			<p>In the next activity, you will practice applying both L1 and L2 regularization to a model.</p>
			<h2 id="_idParaDest-122">Activity 6.01: Pr<a id="_idTextAnchor137"/>edicting Income with L1 and L2 Regularizers</h2>
			<p>The <strong class="source-inline">census-income-train.csv</strong> dataset contains weighted census data extracted from the 1994 and 1995 current population surveys conducted by the US Census Bureau. The dataset is the subset of the original dataset shared by the US Census Bureau. In this activity, you are tasked with building and training a regressor to predict the income of a person based on their census data. The dataset can be accessed here: <a href="https://packt.link/G8xFd">https://packt.link/G8xFd</a>.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the required libraries.</li>
				<li>Create a list called <strong class="source-inline">usecols</strong> containing the column names <strong class="source-inline">AAGE</strong>, <strong class="source-inline">ADTIND</strong>, <strong class="source-inline">ADTOCC</strong>, <strong class="source-inline">SEOTR</strong>, <strong class="source-inline">WKSWORK</strong>, and <strong class="source-inline">PTOTVAL</strong>.</li>
				<li>Load the data using the <strong class="source-inline">read_csv()</strong> method.</li>
				<li>Split the data into training (the first 15,000 rows) and test (the last 5,000 rows) sets.</li>
				<li>Build the multi-class classifier with five fully connected layers of, respectively, <strong class="source-inline">512</strong>, <strong class="source-inline">512</strong>, <strong class="source-inline">128</strong>, <strong class="source-inline">128</strong>, and <strong class="source-inline">26</strong> units.</li>
				<li>Train the model on the training set. <p>The expected output will be as follows:</p><div id="_idContainer247" class="IMG---Figure"><img src="image/B16341_06_16.jpg" alt="Figure 6.16: Logs of the training process&#13;&#10;"/></div><p> </p></li>
			</ol>
			<p class="figure-caption">Figure 6.16: Logs of the training process</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor269">this link</a>.</p>
			<p>In the section ahe<a id="_idTextAnchor138"/>ad, you will see how to tune hyperparameters to achieve better results.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor139"/>Hyperparameter Tuning</h1>
			<p>Previously, you saw how to deal with a model that is overfitting by using different regularization techniques. These techniques help the model to better generalize to unseen data but, as you have seen, they can also lead to inferior performance and make the model underfit.</p>
			<p>With neural networks, data scientists have access to different hyperparameters they can tune to improve the performance of a model. For example, you can try different learning rates and see whether one leads to better results, you can try different numbers of units for each hidden layer of a network, or you can test to see whether different ratios of dropout can achieve a better trade-off between overfitting and underfitting.</p>
			<p>However, the choice of one hyperparameter can impact the effect of another one. So, as the number of hyperparameters and values you want to tune grows, the number of combinations to be tested will increase exponentially. It will also take a lot of time to train models for all these combinations—especially if you have to do it manually. There are some packages that can automatically scan the hyperparameter search space you defined and find the best combination overall for you. In the section ahead, you will see how to use one of them: Keras Tuner.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor140"/>Keras Tuner </h2>
			<p>Unfor<a id="_idTextAnchor141"/>tunately, this package is not included in TensorFlow. You will need to install it manually by running the following command:</p>
			<p class="source-code">pip install keras-tuner</p>
			<p>This package is very simple to use. There are two concepts to understand: <strong class="bold">hyperparameters</strong> and <strong class="bold">tuners</strong>.</p>
			<p>Hyperparameters are the classes used to define a parameter that will be assessed by the tuner. You can use different types of hyperparameters. The main ones are the following:</p>
			<ul>
				<li><strong class="source-inline">hp.Boolean</strong>: A choice between <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong></li>
				<li><strong class="source-inline">hp.Int</strong>: A choice with a range of integers</li>
				<li><strong class="source-inline">hp.Float</strong>: A choice with a range of decimals</li>
				<li><strong class="source-inline">hp.Choice</strong>: A choice within a list of possible values</li>
			</ul>
			<p>The following code snippet shows you how to define a hyperparameter called <strong class="source-inline">learning_rate</strong> that can only take one of four values—<strong class="source-inline">0.1</strong>, <strong class="source-inline">0.01</strong>, <strong class="source-inline">0.001</strong>, or <strong class="source-inline">0.0001</strong>:</p>
			<p class="source-code">hp.Choice('learning_rate', values = [0.1, 0.01, 0.001, 0.0001])</p>
			<p>A tuner in the Keras Tuner package is an algorithm that will look at the hyperparameter search space, test some combinations, and find the one that gives the best result. The Keras Tuner package provides different tuners, and in the section ahead, you will look at three of them: <strong class="bold">random search</strong>, <strong class="bold">Hyperband</strong>, and <strong class="bold">Bayesian optimization</strong>.</p>
			<p>Once defined with the algorithm of your choice, you can call the <strong class="source-inline">search()</strong> method to start the hyperparameter tuning process on the training and test sets, as follows:</p>
			<p class="source-code">tuner.search(X_train, y_train, validation_data=(X_test, y_test))</p>
			<p>Once the search is complete, you can access the best combination with <strong class="source-inline">get_best_hyperparameters()</strong> and then look specifically at one of the hyperparameters you defined:</p>
			<p class="source-code">best_hps = tuner.get_best_hyperparameters()[0]</p>
			<p class="source-code">best_hps.get('learning_rate')</p>
			<p>Finally, the <strong class="source-inline">hypermodel.build()</strong> method will instantiate a TensorFlow Keras model with the best hyperparameters found:</p>
			<p class="source-code">model = tuner.hypermodel.build(best_hps)</p>
			<p>It's as simple as that. Now, let's have a look at the random search tuner.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor142"/>Random Search</h2>
			<p>Rand<a id="_idTextAnchor143"/>om search is one of the available algorithms in this package. As its name implies, it randomly defines the combinations to be tested by sampling through the search space. Even though this algorithm doesn't test every single possible combination, random search provides very good results.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The algorithm that tests every single combination of the search space is called grid search.</p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="image/B16341_06_17.jpg" alt="Figure 6.17: Comparison between grid search and random search&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17: Comparison between grid search and random search</p>
			<p>The preceding figure shows an example of the difference between grid search and random search. You can see that grid search splits the search space into a grid and tests each of the combinations, but some may lead to the same loss value, which makes it less efficient. On the other side, random search covers the search space more efficiently and helps find the optimal solution.</p>
			<p>In Keras Tuner, before instantiating a tuner, you need to define a model-building function that will define the architecture of the TensorFlow Keras model to be trained with the hyperparameters you want to test. Here is an example of such a function:</p>
			<p class="source-code">def model_builder(hp):</p>
			<p class="source-code">    model = tf.keras.Sequential()</p>
			<p class="source-code">    hp_lr = hp.Choice('learning_rate', \</p>
			<p class="source-code">                      values = [0.1, 0.01, 0.001, 0.0001])</p>
			<p class="source-code">    model.add(Dense(512, input_shape=(100,), activation='relu'))</p>
			<p class="source-code">    model.add(Dense(128, activation='relu'))</p>
			<p class="source-code">    model.add(Dense(10, activation='softmax'))</p>
			<p class="source-code">    loss = tf.keras.losses.SparseCategoricalCrossentropy()</p>
			<p class="source-code">    optimizer = tf.keras.optimizers.Adam(hp_lr)</p>
			<p class="source-code">    model.compile(optimizer=optimizer, loss=loss, \</p>
			<p class="source-code">                  metrics=['accuracy'])</p>
			<p class="source-code">    return model</p>
			<p>In the preceding code snippet, you created a model composed of three fully connected layers of <strong class="source-inline">512</strong>, <strong class="source-inline">128</strong>, and <strong class="source-inline">10</strong> units that will be trained with a categorical cross-entropy loss function and the Adam optimizer. You defined the <strong class="source-inline">learning_rate</strong> hyperparameter that will be assessed by Keras Tuner. </p>
			<p>Once the model-building function is defined, you can instantiate a random search tuner like the following:</p>
			<p class="source-code">import kerastuner as kt</p>
			<p class="source-code">tuner = kt.RandomSearch(model_builder, objective='val_accuracy', \</p>
			<p class="source-code">                        max_trials=10)</p>
			<p>In the preceding code, you instantiated a <strong class="source-inline">RandomSearch</strong> tuner that will look at the model and hyperparameters defined in the <strong class="source-inline">model_builder</strong> function using the validation accuracy as the <strong class="source-inline">objective</strong> metric and will run for a maximum of <strong class="source-inline">10</strong> trials.</p>
			<p>In the next exercise, you will use random search to find the best set of hyperparameters for a model.</p>
			<h2 id="_idParaDest-126">Exercise 6.03: Pred<a id="_idTextAnchor144"/>icting a Connect-4 Game Outcome Using Random Search from Keras Tuner </h2>
			<p>In this exercise, you will be using the same dataset as for <em class="italic">Exercise 6.01</em>, <em class="italic">Predicting a Connect-4 Game Outcome Using the L2 Regularizer</em>. You will build and train a multi-class model in TensorFlow that will predict the class outcome for player 1 in the game Connect-4 using the Keras Tuner package to find the best regularization factor for L2 regularization through random search:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be accessed here: <a href="https://packt.link/aTSbC">https://packt.link/aTSbC</a>.</p>
			<p class="callout">The original dataset can be found here: <a href="http://archive.ics.uci.edu/ml/datasets/Connect-4">http://archive.ics.uci.edu/ml/datasets/Connect-4</a>.</p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> that contains the URL to the dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">          '/The-TensorFlow-Workshop/master/Chapter06/dataset'\</p><p class="source-code">          '/connect-4.csv'</p></li>
				<li>Load the dataset into a DataFrame called <strong class="source-inline">data</strong> using the <strong class="source-inline">read_csv()</strong> method and provide the URL to the CSV file. Print the first five rows using the <strong class="source-inline">head()</strong> method:<p class="source-code">data = pd.read_csv(file_url)</p><p class="source-code">data.head()</p><p>The output will be as follows:</p><div id="_idContainer249" class="IMG---Figure"><img src="image/B16341_06_18.jpg" alt="Figure 6.18: First five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.18: First five rows of the dataset</p></li>
				<li>Extract the target variable (the column called <strong class="source-inline">class</strong>) using the <strong class="source-inline">pop()</strong> method and save it in a variable called <strong class="source-inline">target</strong>:<p class="source-code">target = data.pop('class')</p></li>
				<li>Import <strong class="source-inline">train_test_split</strong> from <strong class="source-inline">sklearn.model_selection</strong>:<p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li>Split the data into training and test sets using <strong class="source-inline">train_test_split()</strong>, with 20% of the data for testing and <strong class="source-inline">42</strong> for <strong class="source-inline">random_state</strong>:<p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (data, target, \</p><p class="source-code">                                    test_size=0.2, \</p><p class="source-code">                                    random_state=42)</p></li>
				<li>Install the <strong class="source-inline">kerastuner</strong> package and then import it and assign it the <strong class="source-inline">kt</strong> alias: <p class="source-code">!pip install keras-tuner</p><p class="source-code">import kerastuner as kt</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias. Then, import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Set the seed as <strong class="source-inline">8</strong> using <strong class="source-inline">tf.random.set_seed()</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Define a function called <strong class="source-inline">model_builder</strong> that will create a sequential model with the same architecture as <em class="italic">Exercise 6.02</em>, <em class="italic">Predicting a Connect-4 Game Outcome Using Dropout</em>, with L2 regularization, but this time, provide an <strong class="source-inline">hp.Choice</strong> hyperparameter for the regularization factor: <p class="source-code">def model_builder(hp):</p><p class="source-code">    model = tf.keras.Sequential()</p><p class="source-code">    p_l2 = hp.Choice('l2', values = [0.1, 0.01, 0.001, 0.0001])</p><p class="source-code">    reg_fc1 = Dense(512, input_shape=(42,), activation='relu', \</p><p class="source-code">                    kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                         .l2(l=hp_l2))</p><p class="source-code">    reg_fc2 = Dense(512, activation='relu', \</p><p class="source-code">                    kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                         .l2(l=hp_l2))</p><p class="source-code">    reg_fc3 = Dense(128, activation='relu', \</p><p class="source-code">                    kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                         .l2(l=hp_l2))</p><p class="source-code">    reg_fc4 = Dense(128, activation='relu', \</p><p class="source-code">                    kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                         .l2(l=hp_l2))</p><p class="source-code">    reg_fc5 = Dense(3, activation='softmax')</p><p class="source-code">  </p><p class="source-code">    model.add(reg_fc1)</p><p class="source-code">    model.add(reg_fc2)</p><p class="source-code">    model.add(reg_fc3)</p><p class="source-code">    model.add(reg_fc4)</p><p class="source-code">    model.add(reg_fc5)</p><p class="source-code">    loss = tf.keras.losses.SparseCategoricalCrossentropy()</p><p class="source-code">    optimizer = tf.keras.optimizers.Adam(0.001)</p><p class="source-code">    model.compile(optimizer = optimizer, loss = loss, \</p><p class="source-code">                  metrics = ['accuracy'])</p><p class="source-code">    return model</p></li>
				<li>Instantiate a <strong class="source-inline">RandomSearch</strong> tuner and assign <strong class="source-inline">val_accuracy</strong> to <strong class="source-inline">objective</strong> and <strong class="source-inline">10</strong> to <strong class="source-inline">max_trials</strong>:<p class="source-code">tuner = kt.RandomSearch(model_builder, objective='val_accuracy', \</p><p class="source-code">                        max_trials=10)</p></li>
				<li>Launch the hyperparameter search with the <strong class="source-inline">search()</strong> method on the training and test sets:<p class="source-code">tuner.search(X_train, y_train, validation_data=(X_test, y_test))</p></li>
				<li>Extract the best hyperparameter combination (index <strong class="source-inline">0</strong>) with <strong class="source-inline">get_best_hyperparameters()</strong> and save it in a variable called <strong class="source-inline">best_hps</strong>:<p class="source-code">best_hps = tuner.get_best_hyperparameters()[0]</p></li>
				<li>Extract the best value for the <strong class="source-inline">l2</strong> regularization hyperparameter, save it in a variable called <strong class="source-inline">best_l2</strong>, and print its value:<p class="source-code">best_l2 = best_hps.get('l2')</p><p class="source-code">best_l2</p><p>You should get the following result:</p><p class="source-code">0.0001</p><p>The best value for the <strong class="source-inline">l2</strong> hyperparameter found by random search is <strong class="source-inline">0.0001</strong>.</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method for five epochs and use the test set for <strong class="source-inline">validation_data</strong>:<p class="source-code">model = tuner.hypermodel.build(best_hps)</p><p class="source-code">model.fit(X_train, y_train, epochs=5, \</p><p class="source-code">          validation_data=(X_test, y_test))</p><p>You will get the following output:</p><div id="_idContainer250" class="IMG---Figure"><img src="image/B16341_06_19.jpg" alt="Figure 6.19: Logs of the training process&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.19: Logs of the training process</p>
			<p>Using a random search tuner, you found the best value for L2 regularization (<strong class="source-inline">0.0001</strong>), which helped the model to achieve an accuracy of <strong class="source-inline">0.83</strong> on the training set and <strong class="source-inline">0.81</strong> on the test set. These scores are quite an improvement on those from <em class="italic">Exercise 6.01</em>, <em class="italic">Predicting a Connect-4 Game Outcome Using the L2 Regularizer</em> (<strong class="source-inline">0.69</strong> for the training set and <strong class="source-inline">0.59</strong> for the test set).</p>
			<p>In the next section, you will use another Keras tuner, called Hyperband. </p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor145"/>Hyperband</h2>
			<p>Hyperband i<a id="_idTextAnchor146"/>s another tuner available in the Keras Tuner package. Like random search, it randomly picks candidates from the search space, but more efficiently. The idea behind it is to test a set of combinations for just one or two iterations, keeping only the best performers and training them for longer. So, the algorithm doesn't waste time in training non-performing combinations as with random search. Instead, it simply discards them from the next run. Only the ones that achieve higher performance are kept for longer training. To instantiate a Hyperband tuner, execute the following command:</p>
			<p class="source-code">tuner = kt.Hyperband(model_builder, objective='val_accuracy', \</p>
			<p class="source-code">                     max_epochs=5)</p>
			<p>This tuner takes a model-building function and an objective metric as input parameters, as for random search. But it requires an additional one, <strong class="source-inline">max_epochs</strong>, corresponding to the maximum number of epochs a model is allowed to train for during the hyperparameter search.</p>
			<h2 id="_idParaDest-128">Exercise 6.04: Predic<a id="_idTextAnchor147"/>ting a Connect-4 Game Outcome Using Hyperband from Keras Tuner </h2>
			<p>In this exercise, you will be using the same dataset as for <em class="italic">Exercise 6.01</em>, <em class="italic">Predicting a Connect-4 Game Outcome Using the L2 Regularizer</em>. You will build and train a multi-class model in TensorFlow that will predict the class outcome for player 1 in the game Connect-4 using the Keras Tuner package to find the best learning rate and the number of units for the input layer through Hyperband:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be accessed here: <a href="https://packt.link/WLgen">https://packt.link/WLgen</a>.</p>
			<p class="callout">The original dataset can be found here: <a href="http://archive.ics.uci.edu/ml/datasets/Connect-4">http://archive.ics.uci.edu/ml/datasets/Connect-4</a>.</p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the pandas library and use <strong class="source-inline">pd</strong> as the alias:<p class="source-code">import pandas as pd</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> that contains the URL to the dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/PacktWorkshops'\</p><p class="source-code">           '/The-TensorFlow-Workshop/master/Chapter06/dataset'\</p><p class="source-code">           '/connect-4.csv'</p></li>
				<li>Load the dataset into a DataFrame called <strong class="source-inline">data</strong> using the <strong class="source-inline">read_csv()</strong> method and provide the URL to the CSV file. Print the first five rows using the <strong class="source-inline">head()</strong>  method:<p class="source-code">data = pd.read_csv(file_url)</p><p class="source-code">data.head()</p><p>The output will be as follows:</p><div id="_idContainer251" class="IMG---Figure"><img src="image/B16341_06_20.jpg" alt="Figure 6.20: First five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.20: First five rows of the dataset</p></li>
				<li>Extract the target variable (<strong class="source-inline">class</strong>) using the <strong class="source-inline">pop()</strong> method, and save it in a variable called <strong class="source-inline">target</strong>:<p class="source-code">target = data.pop('class')</p></li>
				<li>Import <strong class="source-inline">train_test_split</strong> from <strong class="source-inline">sklearn.model_selection</strong>:<p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li>Split the data into training and test sets using <strong class="source-inline">train_test_split()</strong>, with 20% of the data for testing and <strong class="source-inline">42</strong> for <strong class="source-inline">random_state</strong>:<p class="source-code">X_train, X_test, y_train, y_test = train_test_split\</p><p class="source-code">                                   (data, target, \</p><p class="source-code">                                    test_size=0.2, \</p><p class="source-code">                                    random_state=42)</p></li>
				<li>Install the <strong class="source-inline">keras-tuner</strong> package, and then import it and assign it the <strong class="source-inline">kt</strong> alias:<p class="source-code">!pip install keras-tuner</p><p class="source-code">import kerastuner as kt</p></li>
				<li>Import the TensorFlow library and use <strong class="source-inline">tf</strong> as the alias, and then import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Set the seed as <strong class="source-inline">8</strong> using <strong class="source-inline">tf.random.set_seed()</strong> to get reproducible results:<p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Define a function called <strong class="source-inline">model_builder</strong> to create a sequential model with the same architecture as <em class="italic">Exercise 6.02</em>, <em class="italic">Predicting a Connect-4 Game Outcome Using Dropout</em>, with L2 regularization and a <strong class="source-inline">0.0001</strong> regularization factor. But, this time, provide a hyperparameter, <strong class="source-inline">hp.Choice</strong>, for the learning rate (<strong class="source-inline">0.01</strong>, <strong class="source-inline">0.001</strong>, or <strong class="source-inline">0.0001</strong>) and an <strong class="source-inline">hp.Int</strong> function for the number of units (between <strong class="source-inline">128</strong> and <strong class="source-inline">512</strong> with a step of <strong class="source-inline">64</strong>) for the input fully connected layer:<p class="source-code">def model_builder(hp):</p><p class="source-code">    model = tf.keras.Sequential()</p><p class="source-code">    hp_units = hp.Int('units', min_value=128, max_value=512, \</p><p class="source-code">                      step=64)</p><p class="source-code">    reg_fc1 = Dense(hp_units, input_shape=(42,), \</p><p class="source-code">                    activation='relu', \</p><p class="source-code">                    kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                         .l2(l=0.0001))</p><p class="source-code">    reg_fc2 = Dense(512, activation='relu', \</p><p class="source-code">                    kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                         .l2(l=0.0001))</p><p class="source-code">    reg_fc3 = Dense(128, activation='relu', \</p><p class="source-code">                    kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                         .l2(l=0.0001))</p><p class="source-code">    reg_fc4 = Dense(128, activation='relu', \</p><p class="source-code">                    kernel_regularizer=tf.keras.regularizers\</p><p class="source-code">                                         .l2(l=0.0001))</p><p class="source-code">    reg_fc5 = Dense(3, activation='softmax')</p><p class="source-code">    model.add(reg_fc1)</p><p class="source-code">    model.add(reg_fc2)</p><p class="source-code">    model.add(reg_fc3)</p><p class="source-code">    model.add(reg_fc4)</p><p class="source-code">    model.add(reg_fc5)</p><p class="source-code">    loss = tf.keras.losses.SparseCategoricalCrossentropy()</p><p class="source-code">    hp_learning_rate = hp.Choice('learning_rate', \</p><p class="source-code">                                 values = [0.01, 0.001, 0.0001])</p><p class="source-code">    optimizer = tf.keras.optimizers.Adam(hp_learning_rate)</p><p class="source-code">    model.compile(optimizer = optimizer, loss = loss, \</p><p class="source-code">                  metrics = ['accuracy'])</p><p class="source-code">    return model</p></li>
				<li>Instantiate a Hyperband tuner, and assign <strong class="source-inline">val_accuracy</strong> to the <strong class="source-inline">objective</strong> metric and <strong class="source-inline">5</strong> to <strong class="source-inline">max_epochs</strong>:<p class="source-code">tuner = kt.Hyperband(model_builder, objective='val_accuracy', \</p><p class="source-code">                     max_epochs=5)</p></li>
				<li>Launch the hyperparameter search with <strong class="source-inline">search()</strong> on the training and test sets:<p class="source-code">tuner.search(X_train, y_train, validation_data=(X_test, y_test))</p></li>
				<li>Extract the best hyperparameter combination (index <strong class="source-inline">0</strong>) with <strong class="source-inline">get_best_hyperparameters()</strong> and save it in a variable called <strong class="source-inline">best_hps</strong>:<p class="source-code">best_hps = tuner.get_best_hyperparameters()[0]</p></li>
				<li>Extract the best value for the number of units for the input layer, save it in a variable called <strong class="source-inline">best_units</strong>, and print its value:<p class="source-code">best_units = best_hps.get('units')</p><p class="source-code">best_units</p><p>You will get the following output:</p><p class="source-code">192</p><p>The best value for the number of units of the input layer found by Hyperband is <strong class="source-inline">192</strong>.</p></li>
				<li>Extract the best value for the learning rate, save it in a variable called <strong class="source-inline">best_lr</strong>, and print its value:<p class="source-code">best_lr = best_hps.get('learning_rate')</p><p class="source-code">best_lr</p></li>
				<li>The output will be the following:<p class="source-code">0.001</p><p>The best value for the learning rate hyperparameter found by Hyperband is <strong class="source-inline">0.001</strong>.</p></li>
				<li>Start the model training process using the <strong class="source-inline">fit()</strong> method for five epochs and use the test set for <strong class="source-inline">validation_data</strong>:<p class="source-code">model.fit(X_train, y_train, epochs=5, \</p><p class="source-code">          validation_data=(X_test, y_test))</p><p>You will get the following output:</p><div id="_idContainer252" class="IMG---Figure"><img src="image/B16341_06_21.jpg" alt="Figure 6.21: Logs of the training process&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.21: Logs of the training process</p>
			<p>Using Hyperband as the tuner, you found the best number of units for the input layer (<strong class="source-inline">192</strong>) and learning rate (<strong class="source-inline">0.001</strong>). With these hyperparameters, the final model achieved an accuracy of <strong class="source-inline">0.81</strong> on both the training and test sets. It is not overfitting much and achieved a satisfactory accuracy score.</p>
			<p>Another very popular tuner is Bayesian optimization, which you will learn about in the following section.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor148"/>Bayesian Optimization</h2>
			<p>B<a id="_idTextAnchor149"/>ayesian optimization is another very popular algorithm used for automatic hyperparameter tuning. It uses probabilities to determine the best combination of hyperparameters. The objective is to iteratively build a probability model that optimizes the objective function from a set of hyperparameters. At each iteration, the probability model is updated from the results obtained. Therefore, unlike random search and Hyperband, Bayesian optimization takes past results into account to improve new ones. The following code snippet will show you how to instantiate a Bayesian optimizer in Keras Tuner:</p>
			<p class="source-code">tuner = kt.BayesianOptimization(model_builder, \</p>
			<p class="source-code">                                objective='val_accuracy', \</p>
			<p class="source-code">                                max_trials=10)</p>
			<p>The expected parameters are similar to random search, including the model-building function, the <strong class="source-inline">objective</strong> metric, and the maximum number of trials.</p>
			<p>In the following activity, you will use Bayesian optimization to predict the income of a person. </p>
			<h2 id="_idParaDest-130">Activity 6.02: Predicti<a id="_idTextAnchor150"/>ng Income with Bayesian Optimization from Keras Tuner</h2>
			<p>In this activity, you will use the same dataset as used in <em class="italic">Activity 6.01</em>, <em class="italic">Predicting Income with L1 and L2 Regularizers</em>. You are tasked with building and training a regressor to predict the income of a person based on their census data. You will perform automatic hyperparameter tuning with Keras Tuner and find the best combination of hyperparameters for the learning rate, the number of units for the input layer, and L2 regularization with Bayesian optimization. </p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Load the data with <strong class="source-inline">read_csv()</strong> from pandas. </li>
				<li>Extract the target variable with the <strong class="source-inline">pop()</strong> method.</li>
				<li>Split the data into training (the first 15,000 rows) and test (the last 5,000 rows) sets.</li>
				<li>Create the model-building function multi-class classifier with five fully connected layers of <strong class="source-inline">512</strong>, <strong class="source-inline">512</strong>, <strong class="source-inline">128</strong>, <strong class="source-inline">128</strong>, and <strong class="source-inline">26</strong> units and the three different hyperparameters to be tuned: the learning rate (between <strong class="source-inline">0.01</strong> and <strong class="source-inline">0.001</strong>), the number of units for the input layer (between <strong class="source-inline">128</strong> and <strong class="source-inline">512</strong> and a step of <strong class="source-inline">64</strong>), and L2 regularization (between <strong class="source-inline">0.1</strong>, <strong class="source-inline">0.01</strong>, and <strong class="source-inline">0.001</strong>).</li>
				<li>Find the best combination of hyperparameters with Bayesian optimization.</li>
				<li>Train the model on the training set with the best hyperparameters found.<p>The expected output will be as follows:</p><div id="_idContainer253" class="IMG---Figure"><img src="image/B16341_06_22.jpg" alt="Figure 6.22: Logs of the training process&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.22: Logs of the training process</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor270">this link</a>.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor151"/>Summary</h1>
			<p>You started your<a id="_idTextAnchor152"/> journey in this chapter with an introduction to the different scenarios of training a model. A model is overfitting when its performance is much better on the training set than the test set. An underfitting model is one that can achieve good results only after training. Finally, a good model achieves good performance on both the training and test sets.</p>
			<p>Then, you encountered several regularization techniques that can help prevent a model from overfitting. You first looked at the L1 and L2 regularizations, which add a penalty component to the cost function. This additional penalty helps to simplify the model by reducing the weights of some features. Then, you went through two different techniques specific to neural networks: dropout and early stopping. Dropout randomly drops some units in the model architecture and forces it to consider other features to make predictions. Early stopping is a mechanism that automatically stops the training of a model once the performance of the test set starts to deteriorate.</p>
			<p>After this, you learned how to use the Keras Tuner package for automatic hyperparameter tuning. You considered three specific types of tuners: random search, Hyperband, and Bayesian optimization. You saw how to instantiate them, perform a hyperparameter search, and extract the best values and model. This process helped you to achieve better performance on the models trained for the exercises and activities.</p>
			<p>In the next chapter, you will learn more about <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>). Such architecture has led to groundbreaking results in computer vision in the past few years. The following chapter will show you how to use this architecture to recognize objects in images.</p>
		</div>
		<div>
			<div id="_idContainer255" class="Content">
			</div>
		</div>
	</body></html>