- en: Object Detection at a Large Scale with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recent breakthroughs in the field of **Artificial Intelligence** (**AI**)
    have brought deep learning to the forefront. Today, even more organizations are
    employing deep learning technologies for analyzing their data, which is often
    voluminous in nature. Hence, it's imperative that deep learning frameworks such
    as TensorFlow can be combined with big data platforms and pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: The 2017 Facebook paper regarding training ImageNet in one hour using 256 GPUs
    spread over 32 servers ([https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf](https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf))
    and a recent paper by Hong Kong Baptist University where they train ImageNet in
    four minutes using 2,048 GPUs ([https://arxiv.org/pdf/1807.11205.pdf](https://arxiv.org/pdf/1807.11205.pdf))
    prove that distributed AI can be a viable solution.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind distributed AI is that the task can be divided into different
    processing clusters. A large number of frameworks have been proposed for distributed
    AI. We can use either distributed TensorFlow or TensorFlowOnSpark, two popular
    choices for distributed AI. Both have their own sets of pros and cons, as we'll
    learn in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Applying computationally expensive deep learning applications at a large scale
    can be an enormous challenge. Using TensorFlowOnSpark, we can distribute these
    computationally expensive processes in the cluster, enabling us to perform computations
    at a larger scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll explore Yahoo''s TensorFlowOnSpark framework for distributed
    deep learning on Spark clusters. Then, we''ll apply TensorFlowOnSpark on a large
    scale dataset of images and train the network to detect objects. In this chapter,
    we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The need for distributed AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to the Apache Spark platform for big data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlowOnSpark – a Python framework to run TensorFlow on Spark clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing object detection using TensorFlowOnSpark and the Sparkdl API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For big data, Spark is the de facto choice, so we''ll start with an introduction
    to Spark. Then, we''ll explore the two popular choices: distributed TensorFlow,
    and TensorFlowOnSpark.'
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Projects/tree/master/Chapter12](https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Projects/tree/master/Chapter12).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have worked in big data, there is a high probability that you already
    know what Apache Spark is, and you can skip this section. But if you don't, don't
    worry—we'll go through the basics.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a powerful, fast, and scalable real-time data analytics engine for
    large scale data processing. It's an open source framework that was developed
    initially by the UC Berkeley AMPLab around the year 2009\. Around 2013, AMPLab
    contributed Spark to the Apache Software Foundation, with Apache Spark Community
    releasing Spark 1.0 in 2014.
  prefs: []
  type: TYPE_NORMAL
- en: The community continues to make regular releases and brings new features into
    the project. At the time of writing this book, we have the Apache Spark 2.4.0
    release and active community on GitHub. It's a real-time data analytics engine
    that allows you to distribute programs across a cluster of machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The beauty of Spark lays in the fact that it''s **scalable**: it runs on top
    of a cluster manager, allowing you to use the scripts written in Python (Java
    or Scala, too) with minimal change. Spark is made up of many components. At the
    heart, we have the Spark core, which distributes the processing of data and the
    mapping and reducing of large datasets. There are several libraries that run on
    top of it. Here are some of the important components of the Spark API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resilient Distributed Dataset (RDD)**:RDD is the base element of the Spark
    API. It''s a fault-tolerant collection of elements that can be operated on in
    parallel, which means that the elements in RDD can be accessed and operated upon
    by the workers in the cluster at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformations and actions**: On the Spark RDD, we can perform two types
    of operations, transformations and actions. Transformations take RDDs as their
    argument and return another RDD. Actions take an RDD as an argument and return
    the local results. All transformations in Spark are lazy, which means that the
    results are not computed right away. Instead, they are computed only when an action
    requires a result to be returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataFrames**: These are very similar to pandas DataFrames. Like pandas, we
    can read from various file formats in the DataFrame (JSON, Parquet, Hive, and
    so on) and perform an operation on the entire DataFrame with single command functions.
    They are distributed across the cluster. Spark uses an engine called Catalyst
    to optimize their usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark uses a master/worker architecture. It has a master node/process and many
    worker nodes/processes. The driver, SparkContext, is the heart of Spark Application.
    It''s the main entry point and the master of the Spark application. It sets up
    the internal services and establishes a connection with the Spark execution environment.
    The following diagram shows Spark''s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81a07ebd-ff60-49e1-b2ed-6af95112f289.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So far, we have provided an introduction to Apache Spark. It''s a big and vast
    subject, and we would recommend readers to refer to the Apache documentation for
    more information: [https://spark.apache.org/documentation.html](https://spark.apache.org/documentation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding distributed TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow also supports distributed computing, allowing us to partition a graph
    and compute it on different processes. Distributed TensorFlow works like a client-server
    model, or to be more specific, a master-workers model. In TensorFlow, we first
    create a cluster of workers, with one being the master-worker. The master coordinates
    the distribution of tasks to different workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do when you have to work with many machines (or processors)
    is to define their name and job type, that is, make a cluster of machines (or
    processors). Each machine in the cluster is assigned a unique address (for example,
    `worker0.example.com:2222`), and they have a specific job, such as `type: master`
    (parameter server), or worker. Later, the TensorFlow server assigns a specific
    task to each worker. To create a cluster, we first need to define cluster specification.
    This is a dictionary that maps worker processes and jobs. The following code creates
    a cluster with the job name `work` and two worker processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can start the process by using the `Server` class and specifying the
    task and task index. The following code will start the `worker` job on `worker1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll need to define a `Server` class for each worker in the cluster. This
    will start all of the workers, making us ready to distribute. To place TensorFlow
    operations on a particular task, we''ll use `tf.device` to specify which tasks
    run on a particular worker. Consider the following code, which distributes the
    task between two workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates two workers on the same machine. In this case, the
    work is divided between the two workers via the `tf.device` function. The variables
    are created on the respective workers; TensorFlow inserts the appropriate data
    transfers between the jobs/workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by creating a `GrpcServer`, which is created with the target, `grpc://localhost:2222`.
    This server knows how to talk to the tasks in the same job via `GrpcChannels`.
    In the following screenshot, you can see the output of the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f35bd435-a9c4-48ed-82b2-197f09604d2e.png)'
  prefs: []
  type: TYPE_IMG
- en: The code for this chapter is located in the repository under the `Chapter12/distributed.py` directory.
  prefs: []
  type: TYPE_NORMAL
- en: This looked easy, right? But what if we want to extend this to our deep learning
    pipeline?
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning through distributed TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the heart of any deep learning algorithm is the stochastic gradient descent
    optimizer. This is what makes the model learn and, at the same time, makes learning
    computationally expensive. Distributing the computation to different nodes on
    the cluster should reduce the training time. TensorFlow allows us to split the
    computational graph, describes the model to different nodes in the cluster, and
    finally merges the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is achieved in TensorFlow with the help of master nodes, worker nodes,
    and parameter nodes. The actual computation is done by the worker nodes; the computed
    parameters are kept by the parameter nodes and shared with worker nodes. The master
    node is responsible for coordinating the workload among different worker nodes.
    There are two popular approaches that are employed for distributed computing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronous approach**: In this case, the mini-batches are divided among
    the workers. Each worker has a replica of the model and calculates the gradients
    separately for the mini-batches allocated to it. Later, the gradients are combined
    at the master and updates are applied to the parameters at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asynchronous approach**: Here, the updates to the model parameters are applied
    asynchronously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These two approaches are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2342b586-d056-42f5-8109-ef177e503ca4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at how we can incorporate distributed TensorFlow in a deep
    learning pipeline. The following code is based upon the following Medium post, [https://medium.com/@ntenenz/distributed-tensorflow-2bf94f0205c3](https://medium.com/@ntenenz/distributed-tensorflow-2bf94f0205c3):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules. Here, we are importing only the necessary ones
    to demonstrate the changes needed to convert existing deep learning code to distributed
    TensorFlow code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the cluster. We''ll create it with one master at the address and two
    workers. In our case, the machine we want to make master has an IP address assigned
    to it, that is, `192.168.1.3`, and we specify port `2222`. You can modify them
    with the addresses of your machines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The same code executes on each machine, so we need to parse the command-line
    arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the TensorFlow server for each worker and the master so that the nodes
    in the cluster can communicate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that the variables are allocated on the same worker device. TensorFlow''s `tf.train.replica_device_setter()` function
    helps us to automatically assign devices to `Operation` objects as they are constructed.
    At the same time, we want the parameter server to wait until the server shuts
    down. This is achieved by using the `server.join()` method at the parameter server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can access this script from GitHub or from the `Chapter12/tensorflow_distributed_dl.py` directory.
    Remember that the same script needs to be executed on each machine in the cluster,
    but with different command-line arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same script now needs to be executed on the parameter server and the four
    workers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code to execute the script on the parameter server (`192.168.1.3:2222`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to execute the script on worker 0 (`192.168.1.4:2222`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to execute the script on `worker 1` (`192.168.1.5:2222`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to execute the script on `worker 2` (`192.168.1.6:2222`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to execute the script on `worker 3` (`192.168.1.6:2222`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The major disadvantage of distributed TensorFlow is that we need to specify
    the IP addresses and ports of all of the nodes in the cluster at startup. This
    puts a limitation on the scalability of distributed TensorFlow. In the next section,
    you will learn about TensorFlowOnSpark, an API built by Yahoo. It provides a simplified
    API to run deep learning models on the distributed Spark platform.
  prefs: []
  type: TYPE_NORMAL
- en: To find out more about distributed TensorFlow, we suggest that you read the
    paper *TensorFlow:* *Large Scale Machine Learning on Heterogeneous Distributed
    Systems* by Google REsearch teamNIPS, 2012 ([http://download.tensorflow.org/paper/whitepaper2015.pdf](http://download.tensorflow.org/paper/whitepaper2015.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Learning about TensorFlowOnSpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the year 2016, Yahoo open sourced TensorFlowOnSpark, a Python framework for
    performing TensorFlow-based distributed deep learning on Spark clusters. Since
    then, it has undergone a lot of developmental changes and is one of the most active
    repositories regarding the distributed deep learning framework.
  prefs: []
  type: TYPE_NORMAL
- en: The **TensorFlowOnSpark** (**TFoS**) framework allows you to run distributed
    TensorFlow applications from within Spark programs. It runs on the existing Spark
    and Hadoop clusters. It can use existing Spark libraries such as SparkSQL or MLlib
    (the Spark machine learning library).
  prefs: []
  type: TYPE_NORMAL
- en: TFoS is automatic, so we do not need to define the nodes as PS nodes, nor do
    we need to upload the same code to all of the nodes in the cluster. By just performing
    a few modifications, we can run our existing TensorFlow code. It allows us to
    scale up the existing TensorFlow apps with minimal changes. It supports all of
    the existing TensorFlow functionality such as synchronous/asynchronous training,
    data parallelism, and TensorBoard. Basically, it's a PySpark wrapper for the TensorFlow
    code. It launches distributed TensorFlow clusters using Spark executors. To support
    TensorFlow data ingestion, it adds `feed_dict` and `queue_runner`, allowing direct
    HDFS access from TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture of TensorFlowOnSpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram depicts the architecture of TFoS. We can see that TFoS
    does not involve Spark drivers in tensor communication, giving the same scalability
    as standalone TensorFlow clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb60c0c1-2ed6-45ef-aaf7-d1f78d082151.png)'
  prefs: []
  type: TYPE_IMG
- en: 'TFoS provides two input modes to take in data for training and inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark RDD**: Spark RDD data is fed to each Spark executor. The executor,
    in turn, feeds the data to the TensorFlow graph using `feed_dict`. However, in
    this mode, TensorFlow worker failures stay hidden from Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow QueueRunners**: Here, the TensorFlow worker runs in the foreground.
    TFoS takes advantage of the TensorFlow file readers and QueueRunners to read data
    directly from HDFS files. TensorFlow worker failures are retired as Spark Tasks,
    and it restores them from the checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep delving inside the TFoS API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The use of TFoS can be divided into three basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the TensorFlow cluster. We can launch the cluster using `TFCluster.run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Feed the data into the TensorFlow app. The data is given for both training
    and inference. To train, we use the `train` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We perform the inference with the help of `cluster.inference(dataRDD)`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, shut down the TensorFlow cluster with `cluster.shutdown()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can modify any TensorFlow program to work with TFoS. In the following section,
    we'll look at how we can train a model to recognize handwritten digits using TFoS.
  prefs: []
  type: TYPE_NORMAL
- en: Handwritten digits using TFoS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll look at how to convert our TensorFlow code to run on
    TFoS. To do this, first, we need to build an EC2 cluster on Amazon AWS. One of
    the easy ways to do this is to use Flintrock, a CLI tool for launching Apache
    Spark clusters from your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the prerequisites that you''ll need to complete this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flintrock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlowOnSpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s see how we can do this. We''re using the MNIST dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
    The following code is taken from the TensorFlowOnSpark GitHub. The repository
    contains the links to documentation and more examples ([https://github.com/yahoo/TensorFlowOnSpark](https://github.com/yahoo/TensorFlowOnSpark)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model architecture and training in the `main(argv, ctx)` function,
    where the `argv` parameter contains the arguments supplied at the command line,
    and `ctx` contains the node metadata such as `job` and `task_idx`. The `cnn_model_fn`
    model function is the CNN model that''s defined as a function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `if  __name__=="__main__"` block, add the following imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the Spark Driver and initiate the TensorFlowOnSpark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `TFCluster.run` to manage the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training is over, shut down the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The complete code is available in the GitHub repository in the `Chapter12/mnist_TFoS.py` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute the code on the EC2 cluster, you''ll need to submit it to Spark
    cluster using `spark-submit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The model learned in 6.6 minutes on the EC2 cluster with two workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57532b58-b342-4bf2-9abf-24125871d75f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use TensorBoard to visualize the model architecture. Once we run the
    code successfully, the event file is created and it can be viewed on the TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we visualize loss, we can see that the loss decreases as the network learns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a95a92b5-e3a7-4608-8ceb-14f0b745b946.png)'
  prefs: []
  type: TYPE_IMG
- en: The model provides 75% accuracy on the test data set on only 1,000 steps, with
    a very basic CNN model. We can further optimize the result by using a better model
    architecture and tuning hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection using TensorFlowOnSpark and Sparkdl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark has a higher level API Sparkdl for scalable deep learning in Python.
    In this section, we'll use the Sparkdl API. In this section, you will learn how
    to build a model over the pre-trained Inception v3 model to detect cars and buses.
    This technique of using a pre-trained model is called **transfer learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning in humans is a continuous process—whatever we learn today is built
    upon the learning we have had in the past. For example, if you know how to drive
    a bicycle, you can extend the same knowledge to drive a motorcycle, or drive a
    car. The driving rule remains the same—the only thing that changes is the control
    panel and actuators. However, in deep learning, we often start afresh. Is it possible
    to use the knowledge the model has gained in solving a problem in one domain,
    to solve the problem in another related domain?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, it's indeed possible, and it's called transfer learning. Though a lot of
    research is still going on in the field, a great deal of success has been achieved
    in applying transfer learning in the area of computer vision. This is due to the
    fact that for computer vision tasks **Convolutional Neural Networks** (**CNNs**)
    are preferred since they are good in extracting features from the image (features
    such as lines, circles, and squares, at lower layers, and higher abstract features
    such as ears and nose at the higher layers). Hence, the features extracted by
    convolutional layers while learning one type of image dataset can be reused in
    other similar domain images. This can help in reducing the training time.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll use Inception v3 ([https://arxiv.org/pdf/1512.00567v1.pdf](https://arxiv.org/pdf/1512.00567v1.pdf)),
    a state-of-the-art CNN trained on the ImageNet dataset. ImageNet ([http://image-net.org/](http://image-net.org/))
    contains over 14 million labelled high-resolution hand-annotated images that have
    been classified into 22,000 categories. Inception v3 was trained on a subset of
    it consisting of about 1.3 million images with 1,000 categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the transfer learning approach, you keep the feature extractor CNN layers
    but replace the classifier layers with a new classifier. This new classifier is
    then trained on the new images. Two approaches are generally followed: either
    we only train the new classifier or we fine-tune the entire network. In the first
    case, we extract the features from our new dataset, called **bottleneck features**,
    by feeding the new dataset into CNN layers. The extracted bottleneck features
    are then used to train the final classifier. In the second case, we train the
    entire network, the original CNN, along with the new classifier on the training
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Sparkdl interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To access Spark functionality in the deep learning pipeline, we need to use
    a Spark driver program. From Spark 2.0.0, we have a single point entry using `SparkSession`.
    The simplest way to do this is by using `builder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This can allow us to get an existing session or create a new session. At the
    time of instantiation, we can use the `.config()`, `.master()`, and `.appName()`
    methods to set configuration options, set the Spark master, and set the application
    name.
  prefs: []
  type: TYPE_NORMAL
- en: To read and manipulate images, Sparkdl provides the `ImageSchema` class. Out
    of its many methods, we'll be using the `readImages` method to read the directory
    of images. It returns a Spark DataFrame with a single column – `image`, of images.
  prefs: []
  type: TYPE_NORMAL
- en: We can add or remove column/rows from the Spark DataFrames using transformations.
    The example code in this section uses the `withColumn` transformation to add a
    column named `label` and assign label classes to our dataset. Just like with a
    pandas Dataframe, we can view the rows of the Spark DataFrame with the help of
    the `show()` method. The Spark DataFrames can also be split or combined together.
  prefs: []
  type: TYPE_NORMAL
- en: The Sparkdl API has methods to enable fast transfer learning. It provides the
    `DeepImageFeaturizer` class, which automatically peels the classifier layer from
    the pre-trained model and uses the features (bottleneck features) from the pre-trained
    CNN layers as an input to the new classifier.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of working with Sparkdl is that we can access all of the Spark
    APIs—even its machine learning API MLlib from the same `SparkSession` instance.
    Using MLlib, we can easily combine multiple algorithms into a single a pipeline. The
    Spark machine learning API MLlib also provides support for various classification
    and regression methods.
  prefs: []
  type: TYPE_NORMAL
- en: Building an object detection model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll now make some code by using TFoS and Sparkdl. The dataset consists of
    images of buses and cars that have been curated from a Google image search. The
    aim is to train a model so that it can differentiate between cars and buses. The following is
    a list of prerequisites that you will need for this code to work:'
  prefs: []
  type: TYPE_NORMAL
- en: PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlowOnSpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pillow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FindSpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: py4j
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, let''s explore our dataset. Inception v3 was trained on ImageNet data
    with 1,000 categories. These included images of various vehicles as well. We have
    49 images for buses and 41 images of cars. Here, you can see the sample images
    from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1e926c3-d5ac-4d48-834b-a05f458bcda5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/f7acc5e6-ee73-440b-b004-434ac416c2a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s build the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we''ll not be using `spark-submit`. Instead, we''ll run the code
    like any standard Python code. Therefore, we''ll define the location of spark
    driver and the Spark deep learning package in the code itself and create a Spark
    session using PySpark''s `SparkSession` builder. One thing to keep in mind here
    is the memory allocated to the heap: Spark executor and Spark driver. The values
    should be based on your machine''s specifications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The images are loaded in the Spark DataFrame using PySpark''s `ImageSchema` class.
    The bus and cars images are loaded in different Spark DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the top five rows of the Spark DataFrame here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70221402-30db-4f84-8719-ac9fb0414f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We split the dataset into the training-test datasets, with a ratio of 60% training
    and 40% test. Remember that these values are random and you can vary them accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The training dataset for buses and cars is combined. The same is done for the
    test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the Sparkdl API to get the pre-trained Inception v3 model and on top
    of the CNN layers of Inception, we add a logistic regressor. Now, we''ll train
    the model on our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how the trained model fairs on the test dataset. Let''s use a perfect
    confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ebcccea-cfec-43ab-910f-ec6742d7112a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the test dataset, the model gives 100% accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Our model is giving such a good performance because the Inception v3 model that
    we have used as the base model for transfer learning has already been trained
    on a lot of vehicle images. A word of caution, however—100% accuracy doesn't mean
    it's the best model, just that it does well on the present test images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Developed by DataBricks, Sparkdl is part of the Deep Learning Pipelines. They
    provide high-level APIs for scalable deep learning in Python with Apache Spark.
    You can learn more about its features and how to use it here: [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning models provide better performance when the training dataset is
    large (big data). Training models for big data is computationally expensive. This
    problem can be handled using the divide and conquer approach: we divide the extensive
    computation part to many machines in a cluster, in other words, distributed AI.'
  prefs: []
  type: TYPE_NORMAL
- en: One way of achieving this is by using Google's distributed TensorFlow, the API
    that helps in distributing the model training among different worker machines
    in the cluster. You need to specify the address of each worker machine and the
    parameter server. This makes the task of scaling the model difficult and cumbersome.
  prefs: []
  type: TYPE_NORMAL
- en: This problem can be solved by using the TensorFlowOnSpark API. By making minimal
    changes to the preexisting TensorFlow code, we can make it run on the cluster.
    The Spark framework handles the distribution among executor machines and the master,
    shielding the user from the details and giving better scalability.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the TensorFlowOnSpark API was used to train a model to recognize
    handwritten digits. This solved the problem of scalability, but we still had to
    process data so that it's available in the right format for training. Unless you
    are well-versed with the Spark infrastructure, especially Hadoop, this can be
    a difficult task.
  prefs: []
  type: TYPE_NORMAL
- en: To ease the difficulty, we can make use of another API, Sparkdl, which provides
    the complete deep learning pipeline on Spark for training using Spark DataFrames.
    Finally, this chapter used the Sparkdl API for object detection. A model was built
    over the pre-trained Inception v3 model to classify images of buses and cars.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to generate book scripts using RNN.
    Who knows—it may win the Booker Prize!
  prefs: []
  type: TYPE_NORMAL
