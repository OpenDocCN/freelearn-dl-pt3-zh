<html><head></head><body>
		<div>
			<div id="_idContainer048" class="Content">
			</div>
		</div>
		<div id="_idContainer049" class="Content">
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>1. Introduction to Machine Learning with TensorFlow</h1>
		</div>
		<div id="_idContainer103" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will learn how to create, utilize, and apply linear transformations to the fundamental building blocks of programming with TensorFlow: tensors. You will then utilize tensors to understand the complex concepts associated with neural networks, including tensor reshaping, transposition, and multiplication. </p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Introduction</h1>
			<p><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) has permeated various aspects of daily life that are unknown to many. From the recommendations of your daily social feeds to the results of your online searches, they are all powered by machine learning algorithms. These algorithms began in research environments solving niche problems, but as their accessibility broadened, so too have their applications for broader use cases. Researchers and businesses of all types recognize the value of using models to optimize every aspect of their respective operations. Doctors can use machine learning to decide diagnosis and treatment options, retailers can use ML to get the right products to their stores at the right time, and entertainment companies can use ML to provide personalized recommendations to their customers.</p>
			<p>In the age of data, machine learning models have proven to be valuable assets to any data-driven company. The large quantities of data available allow powerful and accurate models to be created to complete a variety of tasks, from regression to classification, recommendations to time series analysis, and even generative art, many of which will be covered in this workshop. And all can be built, trained, and deployed with TensorFlow.</p>
			<p>The TensorFlow API has a huge amount of functionality that has made it popular among all machine learning practitioners building machine learning models or working with tensors, which are multidimensional numerical arrays. For researchers, TensorFlow is an appropriate choice to create new machine learning applications due to its advanced customization and flexibility. For developers, TensorFlow is an excellent choice of machine learning library due to its ease in terms of deploying models from development to production environments. Combined, TensorFlow's flexibility and ease of deployment make the library a smart choice for many practitioners looking to build performant machine learning models using a variety of different data sources and to replicate the results of that learning in production environments.</p>
			<p>This chapter provides a practical introduction to TensorFlow's API. You will learn how to perform mathematical operations pertinent to machine learning that will give you a firm foundation for building performant ML models using TensorFlow. You will first learn basic operations such as how to create variables with the API. Following that, you will learn how to perform linear transformations such as addition before moving on to more advanced tasks, including tensor multiplication.</p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Implementing Artificial Neural Networks in TensorFlow</h1>
			<p>The advanced flexibility that TensorFlow offers lends itself well to creating <strong class="bold">artificial neural networks</strong> (<strong class="bold">ANNs</strong>). ANNs are algorithms that are inspired by the connectivity of neurons in the brain and are intended to replicate the process in which humans learn. They consist of layers through which information propagates from the input to the output. </p>
			<p><em class="italic">Figure 1.1</em> shows a visual representation of an ANN. An input layer is on the left-hand side, which, in this example, has two features (<strong class="source-inline">X</strong><span class="subscript">1</span> and <strong class="source-inline">X</strong><span class="subscript">2</span>). The input layer is connected to the first hidden layer, which has three units. All the data from the previous layer gets passed to each unit in the first hidden layer. The data is then passed to the second hidden layer, which also has three units. Again, the information from each unit of the prior layer is passed to each unit of the second hidden layer. Finally, all the information from the second hidden layer is passed to the output layer, which has one unit, representing a single number for each set of input features.</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B16341_01_01.jpg" alt="Figure 1.1: A visual representation of an ANN with two hidden layers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1: A visual representation of an ANN with two hidden layers</p>
			<p>ANNs have proven to be successful in learning complex and nonlinear relationships with large, unstructured datasets, such as audio, images, and text data. While the results can be impressive, there is a lot of variability in how ANNs can be configured. For example, the number of layers, the size of each layer, and which nonlinear function should be used are some of the factors that determine the configuration of ANNs. Not only are the classes and functions that TensorFlow provides well-suited to building and training ANNs, but the library also supplies a suite of tools to help visualize and debug ANNs during the training process.</p>
			<p>Compared with traditional machine learning algorithms, such as linear and logistic regression, ANNs can outperform them when provided with large amounts of data. ANNs are advantageous since they can be fed unstructured data and feature engineering is not necessarily required. Data pre-processing can be a time-intensive process. Therefore, many practitioners prefer ANNs if there is a large amount of data.</p>
			<p>Many companies from all sectors utilize TensorFlow to build ANNs for their applications. Since TensorFlow is backed by Google, the company utilizes the library for much of its research, development, and production of machine learning applications. However, there are many other companies that also use the library. Companies such as Airbnb, Coca-Cola, Uber, and GE Healthcare all utilize the library for a variety of tasks. The use of ANNs is particularly appealing since they can achieve remarkable accuracy if provided with sufficient data and trained appropriately. For example, GE Healthcare uses TensorFlow to build ANNs to identify specific anatomy regardless of orientation from magnetic resonance images to improve speed and accuracy. By using ANNs, they can achieve over 99% accuracy in identifying anatomy in seconds, regardless of head rotation, which would otherwise take a trained professional much more time.</p>
			<p>While the number of companies utilizing ANNs is vast, ANNs may not be the most appropriate choice for solving all business problems. In such an environment, you must answer the following questions to determine whether ANNs are the most appropriate choice:</p>
			<ul>
				<li><strong class="bold">Does the problem have a numerical solution?</strong> Machine learning algorithms, ANNs included, generate predicted numerical results based on input data. For example, machine learning algorithms may predict a given number, such as the temperature of a city given the location and previous weather conditions, or the stock price given previous stock prices, or label images into a given number of categories. In each of these examples, a numerical output is generated based on the data provided and, given enough labeled data, models can perform well. However, when the desired result is more abstract, or creativity is needed, such as creating a new song, then machine learning algorithms may not be the most appropriate choice, since a well-defined numerical solution may not be available.</li>
				<li><strong class="bold">Is there enough appropriately labeled data to train a model?</strong> For a supervised learning task, you must have at least some labeled data to train a model. For example, if you want to build a model to predict financial stock data for a given company, you will first need historical training data. If the company in question has not been public for very long, there may not be adequate training data. ANNs can often require a lot of data. When working with images, ANNs often need millions of training examples to develop accurate, robust models. This may be a determining factor for consideration when deciding which algorithm is appropriate for a given task.</li>
			</ul>
			<p>Now that you are aware of what TensorFlow is, consider the following advantages and disadvantages of TensorFlow.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Advantages of TensorFlow</h2>
			<p>The following are a few of the main advantages of using TensorFlow that many practitioners consider when deciding whether to pursue the library for machine learning purposes:</p>
			<ul>
				<li><strong class="bold">Library Management</strong>: There is a large community of practitioners that maintain the TensorFlow library to keep it up to date with frequent new releases to help fix bugs, add new functions and classes to reflect current advances in the field, and add support for multiple programming languages.</li>
				<li><strong class="bold">Pipelining</strong>: TensorFlow supports end-to-end model production, from model development in highly parallelizable environments that support GPU processing to a suite of model deployment tools. Also, there are lightweight libraries in TensorFlow that are used for deploying trained TensorFlow models on mobile and embedded devices, such as <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) devices.</li>
				<li><strong class="bold">Community Support</strong>: The community of practitioners that use and support the library is vast and they support each other, because of which those practitioners who are new to the library achieve the results they are looking for easily.</li>
				<li><strong class="bold">Open Source</strong>: TensorFlow is an open source library, and its code base is available for anyone to use and modify for their own applications.</li>
				<li><strong class="bold">Works with Multiple Languages</strong>: While the library is natively designed for Python, models can now be trained and deployed in JavaScript.</li>
			</ul>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Disadvantages of TensorFlow</h2>
			<p>The following are a few of the disadvantages of using TensorFlow:</p>
			<ul>
				<li><strong class="bold">Computational Speed</strong>: Since the primary programming language of TensorFlow is Python, the library is not as computationally fast as it could be if it were native to other languages, such as C++.</li>
				<li><strong class="bold">Steep Learning Curve</strong>: Compared to other machine learning libraries, such as Keras, the learning curve is steeper, and this can make it challenging for new practitioners to create their own models outside of given example code.</li>
			</ul>
			<p>Now that you have understood what TensorFlow is, the next section will demonstrate how to use the TensorFlow library using Python.</p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>The TensorFlow Library in Python</h1>
			<p>TensorFlow can be used in Python by importing certain libraries. You can import libraries in Python using the <strong class="source-inline">import</strong> statement:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p>In the preceding command, you have imported the TensorFlow library and used the shorthand <strong class="source-inline">tf</strong>.</p>
			<p>In the next exercise, you will learn how to import the TensorFlow library and check its version so that you can utilize the classes and functions supplied by the library, which is an important and necessary first step when utilizing the library.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Exercise 1.01: Verifying Your Version of TensorFlow</h2>
			<p>In this exercise, you will load TensorFlow and check which version is installed on your system.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li>Open a Jupyter notebook to implement this exercise by typing <strong class="source-inline">jupyter notebook</strong> in the terminal.</li>
				<li>Import the TensorFlow library by entering the following code in the Jupyter cell: <p class="source-code">import tensorflow as tf</p></li>
				<li>Verify the version of TensorFlow using the following command:<p class="source-code">tf.__version__</p><p>This will result in the following output:</p><p class="source-code">'2.6.0'</p><p>As you can see from the preceding output, the version of TensorFlow is <strong class="source-inline">2.6.0</strong>.</p><p class="callout-heading">Note</p><p class="callout">The version may vary on your system if you have not set up the environment using the steps provided in <em class="italic">Preface</em>.</p></li>
			</ol>
			<p>In this exercise, you successfully imported TensorFlow. You have also checked which version of TensorFlow is installed on your system. </p>
			<p>This task can be done for any imported library in Python and is useful for debugging and referencing documentation.</p>
			<p>The potential applications of using TensorFlow are numerous, and it has already achieved impressive results, as evidenced by the results from companies such as Airbnb, which uses TensorFlow to classify images on their platform, to GE Healthcare, which uses TensorFlow to identify anatomy on MRIs of the brain. To learn how to create powerful models for your own applications, you first must learn the basic mathematical principles and operations that make up the machine learning models that can be achieved in TensorFlow. The mathematical operations can be intimidating to new users, but a comprehensive understanding of how they operate is key to making performant models. </p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Introduction to Tensors</h1>
			<p>Tensors can be thought of as the core components of ANNs—the input data, output predictions, and weights that are learned throughout the training process are all tensors. Information propagates through a series of linear and nonlinear transformations to turn the input data into predictions. This section demonstrates how to apply linear transformations such as additions, transpositions, and multiplications to tensors. Other linear transformations, such as rotations, reflections, and shears, also exist. However, their applications as they pertain to ANNs are less common.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Scalars, Vectors, Matrices, and Tensors</h2>
			<p>Tensors can be represented as multi-dimensional arrays. The number of dimensions a tensor spans is known as the tensor's rank. Tensors with ranks <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, and <strong class="source-inline">2</strong> are used often and have their own names, which are <strong class="bold">scalars</strong>, <strong class="bold">vectors</strong>, and <strong class="bold">matrices</strong>, respectively, although the term <em class="italic">tensors</em> can be used to describe each of them. <em class="italic">Figure 1.2</em> shows some examples of tensors of various ranks. From left to right are a scalar, vector, matrix, and a 3-dimensional tensor, where each element represents a different number, and the subscript represents the location of the element in the tensor:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B16341_01_02.jpg" alt="Figure 1.2: A visual representation of a scalar, vector, matrix, and tensor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2: A visual representation of a scalar, vector, matrix, and tensor</p>
			<p>The formal definitions of a scalar, vector, matrix, and tensor are as follows:</p>
			<ul>
				<li><strong class="bold">Scalar</strong>: A scalar consists of a single number, making it a zero-dimensional array. It is an example of zero-order tensors. Scalars do not have any axes. For instance, the width of an object is a scalar. </li>
				<li><strong class="bold">Vector</strong>: Vectors are one-dimensional arrays and are an example of first-order tensors. They can be considered lists of values. Vectors have one axis. The size of a given object denoted by the width, height, and depth is an example of a vector field. </li>
				<li><strong class="bold">Matrix</strong>: Matrices are two-dimensional arrays with two axes. They are an example of second-order tensors. Matrices might be used to store the size of several objects. Each dimension of the matrix comprises the size of each object (width, height, depth) and the other matrix dimension is used to differentiate between objects.</li>
				<li><strong class="bold">Tensor</strong>: Tensors are the general entities that encapsulate scalars, vectors, and matrices, although the name is generally reserved for tensors of rank <strong class="source-inline">3</strong> or more. A tensor can be used to store the size of many objects and their locations over time. The first dimension of the matrix comprises the size of each object (width, height, depth), the second dimension is used to differentiate between the objects, and the third dimension describes the location of these objects over time. </li>
			</ul>
			<p>Tensors can be created using the <strong class="source-inline">Variable</strong> class present in the TensorFlow library and passing in a value representing the tensor. A float or integer can be passed for scalars, a list of floats or integers can be passed for vectors, a nested list of floats or integers for matrices, and so on. The following command demonstrates the use of the <strong class="source-inline">Variable</strong> class where a list of the intended values for the tensor as well as any other attributes that are required to be explicitly defined are passed:</p>
			<p class="source-code">tensor1 = tf.Variable([1,2,3], dtype=tf.int32, \</p>
			<p class="source-code">                      name='my_tensor', trainable=True)</p>
			<p>The resultant <strong class="source-inline">Variable</strong> object has several attributes that may be commonly called, and these are as follows:</p>
			<ul>
				<li><strong class="source-inline">dtype</strong>: The datatype of the <strong class="source-inline">Variable</strong> object (for the tensor defined above, the datatype is <strong class="source-inline">tf.int32</strong>). The default value for this attribute is determined from the values passed.</li>
				<li><strong class="source-inline">shape</strong>: The number of dimensions and length of each dimension of the <strong class="source-inline">Variable</strong> object (for the tensor defined above, the shape is <strong class="source-inline">[3]</strong>). The default value for this attribute is also determined from the values passed.</li>
				<li><strong class="source-inline">name</strong>: The name of the <strong class="source-inline">Variable</strong> object (for the tensor defined above, the name of the tensor is defined as <strong class="source-inline">'my_tensor'</strong>). The default for this attribute is <strong class="source-inline">Variable</strong>.</li>
				<li><strong class="source-inline">trainable</strong>: This attribute indicates whether the <strong class="source-inline">Variable</strong> object can be updated during model training (for the tensor defined above, the <strong class="source-inline">trainable</strong> parameter is set to <strong class="source-inline">true</strong>). The default for this attribute is <strong class="source-inline">true</strong>.<p class="callout-heading">Note</p><p class="callout">You can read more about the attributes of the <strong class="source-inline">Variable</strong> object here: <a href="https://www.tensorflow.org/api_docs/python/tf/Variable">https://www.tensorflow.org/api_docs/python/tf/Variable</a>.</p></li>
			</ul>
			<p>The <strong class="source-inline">shape</strong> attribute of the <strong class="source-inline">Variable</strong> object can be called as follows:</p>
			<p class="source-code">tensor1.shape</p>
			<p>The <strong class="source-inline">shape</strong> attribute gives the shape of the tensor, that is, is it a scalar, vector, matrix, and so on. The output of the preceding command will be <strong class="source-inline">[3]</strong> since the tensor has a single dimension with three values along that dimension.</p>
			<p>The rank of a tensor can be determined in TensorFlow using the <strong class="source-inline">rank</strong> function. It can be used by passing the tensor as the single argument to the function and the result will be an integer value:</p>
			<p class="source-code">tf.rank(tensor1)</p>
			<p>The output of the following command will be a zero-dimensional integer tensor representing the rank of the input. In this case, the rank of <strong class="source-inline">tensor1</strong> will be <strong class="source-inline">1</strong> as the tensor has only one dimension.</p>
			<p>In the following exercise, you will learn how to create tensors of various ranks using TensorFlow's <strong class="source-inline">Variable</strong> class.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Exercise 1.02: Creating Scalars, Vectors, Matrices, and Tensors in TensorFlow</h2>
			<p>The votes cast for different candidates of three different political parties in districts A and B are as follows:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B16341_01_03.jpg" alt="Figure 1.3: Votes cast for different candidates of three different political &#13;&#10;parties in districts A and B"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3: Votes cast for different candidates of three different political parties in districts A and B</p>
			<p>You are required to do the following:</p>
			<ul>
				<li>Create a scalar to store the votes cast for <strong class="source-inline">Candidate 1</strong> of political party <strong class="source-inline">X</strong> in district <strong class="source-inline">A</strong>, that is, <strong class="source-inline">4113</strong>, and check its shape and rank.</li>
				<li>Create a vector to represent the proportion of votes cast for three different candidates of political party <strong class="source-inline">X</strong> in district <strong class="source-inline">A</strong> and check its shape and rank.</li>
				<li>Create a matrix to represent the votes cast for three different candidates of political parties <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> and check its shape and rank.</li>
				<li>Create a tensor to represent the votes cast for three different candidates in two different districts, for three political parties, and check its shape and rank.</li>
			</ul>
			<p>Perform the following steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the TensorFlow library: <p class="source-code">import tensorflow as tf</p></li>
				<li>Create an integer variable using TensorFlow's <strong class="source-inline">Variable</strong> class and pass <strong class="source-inline">4113</strong> to represent the number of votes cast for a particular candidate. Also, pass <strong class="source-inline">tf.int16</strong> as a second argument to ensure that the input number is an integer datatype. Print the result:<p class="callout-heading">Note</p><p class="callout">The datatype does not have to be explicitly defined. If one is not defined, the datatype will be determined by TensorFlow's <strong class="source-inline">convert_to_tensor</strong> function.</p><p class="source-code">int_variable = tf.Variable(4113, tf.int16)</p><p class="source-code">int_variable</p><p>This will result in the following output:</p><p class="source-code">&lt;tf.Variable 'Variable:0' shape=() dtype=int32, numpy=4113&gt;</p><p>Here, you can see the attributes of the variable created, including the name, <strong class="source-inline">Variable:0</strong>, the shape, datatype, and the NumPy representation of the tensor. </p></li>
				<li>Use TensorFlow's <strong class="source-inline">rank</strong> function to print the rank of the variable created:<p class="source-code">tf.rank(int_variable)</p><p>This will result in the following output:</p><p class="source-code">&lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;</p><p>You can see that the rank of the integer variable that was created is <strong class="source-inline">0</strong> from the NumPy representation of the tensor.</p></li>
				<li>Access the integer variable of the rank by calling the <strong class="source-inline">numpy</strong> attribute:<p class="source-code">tf.rank(int_variable).numpy()</p><p>This will result in the following output:</p><p class="source-code">0</p><p>The rank of the scalar is <strong class="source-inline">0</strong>.</p><p class="callout-heading">Note</p><p class="callout">All attributes of the result of the <strong class="source-inline">rank</strong> function can be called, including the <strong class="source-inline">shape</strong> and <strong class="source-inline">dtype</strong> attributes.</p></li>
				<li>Call the <strong class="source-inline">shape</strong> attribute of the integer to find the shape of the tensor:<p class="source-code">int_variable.shape</p><p>This will result in the following output:</p><p class="source-code">TensorShape([])</p><p>The preceding output signifies that the shape of the tensor has no size, which is representative of a scalar.</p></li>
				<li>Print the <strong class="source-inline">shape</strong> of the scalar variable as a Python list:<p class="source-code">int_variable.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[]</p></li>
				<li>Create a <strong class="source-inline">vector</strong> variable using TensorFlow's <strong class="source-inline">Variable</strong> class. Pass a list for the vector to represent the proportion of votes cast for three different candidates, and pass in a second argument for the datatype as <strong class="source-inline">tf.float32</strong> to ensure that it is a <strong class="source-inline">float</strong> datatype. Print the result:<p class="source-code">vector_variable = tf.Variable([0.23, 0.42, 0.35], \</p><p class="source-code">                              tf.float32)</p><p class="source-code">vector_variable</p><p>This will result in the following output:</p><p class="source-code">&lt;tf.Variable 'Variable:0' shape(3,) dtype=float32, </p><p class="source-code">numpy=array([0.23, 0.42, 0.35], dtype=float32)&gt;</p><p>You can see that the shape and NumPy attributes are different from the scalar variable created earlier. The shape is now <strong class="source-inline">(3,)</strong>, indicating that the tensor is one-dimensional with three elements along that dimension.</p></li>
				<li>Print the rank of the <strong class="source-inline">vector</strong> variable using TensorFlow's <strong class="source-inline">rank</strong> function as a NumPy variable:<p class="source-code">tf.rank(vector_variable).numpy()</p><p>This will result in the following output:</p><p class="source-code">1</p><p>Here, you can see that the rank of the vector variable is <strong class="source-inline">1</strong>, confirming that this variable is one-dimensional.</p></li>
				<li>Print the shape of the <strong class="source-inline">vector</strong> variable as a Python list:<p class="source-code">vector_variable.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[3]</p></li>
				<li>Create a matrix variable using TensorFlow's <strong class="source-inline">Variable</strong> class. Pass a list of lists of integers for the matrix to represent the votes cast for three different candidates in two different districts. This matrix will have three columns representing the candidates, and two rows representing the districts. Pass in a second argument for the datatype as <strong class="source-inline">tf.int32</strong> to ensure that it is an integer datatype. Print the result:<p class="source-code">matrix_variable = tf.Variable([[4113, 7511, 6259], \</p><p class="source-code">                               [3870, 6725, 6962]], \</p><p class="source-code">                              tf.int32)</p><p class="source-code">matrix_variable</p><p>This will result in the following output:</p><div id="_idContainer053" class="IMG---Figure"><img src="image/B16341_01_04.jpg" alt="Figure 1.4: The output of the TensorFlow variable&#13;&#10;"/></div><p class="figure-caption">Figure 1.4: The output of the TensorFlow variable</p></li>
				<li>Print the rank of the matrix variable as a NumPy variable:<p class="source-code">tf.rank(matrix_variable).numpy()</p><p>This will result in the following output:</p><p class="source-code">2</p><p>Here, you can see that the rank of the matrix variable is <strong class="source-inline">2</strong>, confirming that this variable is two-dimensional.</p></li>
				<li>Print the shape of the matrix variable as a Python list:<p class="source-code">matrix_variable.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[2, 3]</p></li>
				<li>Create a tensor variable using TensorFlow's <strong class="source-inline">Variable</strong> class. Pass in a triple nested list of integers for the tensor to represent the votes cast for three different candidates in two different districts, for three political parties. Print the result:<p class="source-code">tensor_variable = tf.Variable([[[4113, 7511, 6259], \</p><p class="source-code">                                [3870, 6725, 6962]], \</p><p class="source-code">                               [[5102, 7038, 6591], \</p><p class="source-code">                                [3661, 5901, 6235]], \</p><p class="source-code">                               [[951, 1208, 1098], \</p><p class="source-code">                                [870, 645, 948]]])</p><p class="source-code">tensor_variable</p><p>This will result in the following output:</p><div id="_idContainer054" class="IMG---Figure"><img src="image/B16341_01_05.jpg" alt="Figure 1.5: The output of the TensorFlow variable&#13;&#10;"/></div><p class="figure-caption">Figure 1.5: The output of the TensorFlow variable</p></li>
				<li>Print the rank of the tensor variable as a NumPy variable:<p class="source-code">tf.rank(tensor_variable).numpy()</p><p>This will result in the following output:</p><p class="source-code">3</p><p>Here, you can see that the rank of the tensor variable is <strong class="source-inline">3</strong>, confirming that this variable is three-dimensional.</p></li>
				<li>Print the shape of the tensor variable as a Python list:<p class="source-code">tensor_variable.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[3, 2, 3]</p><p>The result shows that the shape of the resulting tensor is a list object.</p></li>
			</ol>
			<p>In this exercise, you have successfully created tensors of various ranks from political voting data using TensorFlow's <strong class="source-inline">Variable</strong> class. First, you created scalars, which are tensors that have a rank of <strong class="source-inline">0</strong>. Next, you created vectors, which are tensors with a rank of <strong class="source-inline">1</strong>. Matrices were then created, which are tensors of rank <strong class="source-inline">2</strong>. Finally, tensors were created that have rank <strong class="source-inline">3</strong> or more. You confirmed the rank of the tensors you created by using TensorFlow's <strong class="source-inline">rank</strong> function and verified their shape by calling the tensor's <strong class="source-inline">shape</strong> attribute. </p>
			<p>In the next section, you will combine tensors to create new tensors using tensor addition.</p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Tensor Addition</h1>
			<p>Tensors can be added together to create new tensors. You will use the example of matrices in this chapter, but the concept can be extended to tensors with any rank. Matrices may be added to scalars, vectors, and other matrices under certain conditions in a process known as broadcasting. Broadcasting refers to the process of array arithmetic on tensors of different shapes.</p>
			<p>Two matrices may be added (or subtracted) together if they have the same shape. For such matrix-matrix addition, the resultant matrix is determined by the element-wise addition of the input matrices. The resultant matrix will therefore have the same shape as the two input matrices. You can define the matrix <strong class="source-inline">Z = [Z</strong><span class="subscript">ij</span><strong class="source-inline">]</strong> as the matrix sum <strong class="source-inline">Z = X + Y</strong>, where <strong class="source-inline">z</strong><span class="subscript">ij</span> = <strong class="source-inline">x</strong><span class="subscript">ij</span> <strong class="source-inline">+</strong> <strong class="source-inline">y</strong><span class="subscript">ij</span> and each element in <strong class="source-inline">Z</strong> is the sum of the same element in <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>. </p>
			<p>Matrix addition is commutative, which means that the order of <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> does not matter, that is, <strong class="source-inline">X + Y = Y + X</strong>. Matrix addition is also associative, which means that the same result is achieved even when the order of additions is different or even if the operation is applied more than once, that is, <strong class="source-inline">X + (Y + Z) = (X + Y) + Z</strong>.</p>
			<p>The same matrix addition principles apply to scalars, vectors, and tensors. An example is shown in the following figure: </p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B16341_01_06.jpg" alt="Figure 1.6: A visual example of matrix-matrix addition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6: A visual example of matrix-matrix addition</p>
			<p>Scalars can also be added to matrices. Here, each element of the matrix is added to the scalar individually, as shown in <em class="italic">Figure 1.7</em>:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B16341_01_07.jpg" alt="Figure 1.7: A visual example of matrix-scalar addition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7: A visual example of matrix-scalar addition</p>
			<p>Addition is an important transformation that can be applied to tensors since the transformation occurs so frequently. For example, a common transformation in developing ANNs is to add a bias to a layer. This is when a constant tensor array of the same size of the ANN layer is added to that layer. Therefore, it is important to know how and when this seemingly simple transformation can be applied to tensors.</p>
			<p>Tensor addition can be performed in TensorFlow by using the <strong class="source-inline">add</strong> function and passing in the tensors as arguments, or simply by using the <strong class="source-inline">+</strong> operator as follows:</p>
			<p class="source-code">tensor1 = tf.Variable([1,2,3])</p>
			<p class="source-code">tensor2 = tf.Variable([4,5,6])</p>
			<p class="source-code">tensor_add1 = tf.add(tensor1, tensor2)</p>
			<p class="source-code">tensor_add2 = tensor1 + tensor2</p>
			<p>In the following exercise, you will perform tensor addition on scalars, vectors, and matrices in TensorFlow.</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Exercise 1.03: Performing Tensor Addition in TensorFlow</h2>
			<p>The votes cast for different candidates of three different political parties in districts A and B are as follows:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B16341_01_08.jpg" alt="Figure 1.8: Votes cast for different candidates of three different political &#13;&#10;parties in districts A and B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8: Votes cast for different candidates of three different political parties in districts A and B</p>
			<p>Your requisite tasks are as follows:</p>
			<ul>
				<li>Store the total number of votes cast for political party X in district A.</li>
				<li>Store the total number of votes cast for each political party in district A.</li>
				<li>Store the total number of votes cast for each political party in both districts.</li>
			</ul>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the TensorFlow library: <p class="source-code">import tensorflow as tf</p></li>
				<li>Create three scalar variables using TensorFlow's <strong class="source-inline">Variable</strong> class to represent the votes cast for three candidates of political party X in district A:<p class="source-code">int1 = tf.Variable(4113, tf.int32)</p><p class="source-code">int2 = tf.Variable(7511, tf.int32)</p><p class="source-code">int3 = tf.Variable(6529, tf.int32)</p></li>
				<li>Create a new variable to store the total number of votes cast for political party X in district A:<p class="source-code">int_sum = int1+int2+int3</p></li>
				<li>Print the result of the sum of the two variables as a NumPy variable:<p class="source-code">int_sum.numpy()</p><p>This will result in the following output:</p><p class="source-code">18153</p></li>
				<li>Create three vectors to represent the number of votes cast for different political parties in district A, each with one row and three columns:<p class="source-code">vec1 = tf.Variable([4113, 3870, 5102], tf.int32)</p><p class="source-code">vec2 = tf.Variable([7511, 6725, 7038], tf.int32)</p><p class="source-code">vec3 = tf.Variable([6529, 6962, 6591], tf.int32)</p></li>
				<li>Create a new variable to store the total number of votes for each political party in district A:<p class="source-code">vec_sum = vec1 + vec2 + vec3</p></li>
				<li>Print the result of the sum of the two variables as a NumPy array:<p class="source-code">vec_sum.numpy()</p><p>This will result in the following output:</p><p class="source-code">array([18153, 17557, 18731])</p></li>
				<li>Verify that the vector addition is as expected by performing the addition of each element of the vector:<p class="source-code">print((vec1[0] + vec2[0] + vec3[0]).numpy())</p><p class="source-code">print((vec1[1] + vec2[1] + vec3[1]).numpy())</p><p class="source-code">print((vec1[2] + vec2[2] + vec3[2]).numpy())</p><p>This will result in the following output:</p><p class="source-code">18153</p><p class="source-code">17557</p><p class="source-code">18731</p><p>You can see that the <strong class="source-inline">+</strong> operation on three vectors is simply element-wise addition of the vectors.</p></li>
				<li>Create three matrices to store the votes cast for candidates of each political party in each district:<p class="source-code">matrix1 = tf.Variable([[4113, 3870, 5102], \</p><p class="source-code">                       [3611, 951, 870]], tf.int32)</p><p class="source-code">matrix2 = tf.Variable([[7511, 6725, 7038], \</p><p class="source-code">                       [5901, 1208, 645]], tf.int32)</p><p class="source-code">matrix3 = tf.Variable([[6529, 6962, 6591], \</p><p class="source-code">                       [6235, 1098, 948]], tf.int32)</p></li>
				<li>Verify that the three tensors have the same shape:<p class="source-code">matrix1.shape == matrix2.shape == matrix3.shape</p><p>This will result in the following output:</p><p class="source-code">True</p></li>
				<li>Create a new variable to store the total number of votes cast for each political party in both districts:<p class="source-code">matrix_sum = matrix1 + matrix2 + matrix3</p></li>
				<li>Print the result of the sum of the two variables as a NumPy array:<p class="source-code">matrix_sum.numpy()</p><p>This will result in the following output representing the total votes for each candidate and each party across districts:</p><div id="_idContainer058" class="IMG---Figure"><img src="image/B16341_01_09.jpg" alt="Figure 1.9: The output of the matrix summation as a NumPy variable&#13;&#10;"/></div><p class="figure-caption">Figure 1.9: The output of the matrix summation as a NumPy variable</p></li>
				<li>Verify that the tensor addition is as expected by performing the addition of each element of the vector:<p class="source-code">print((matrix1[0][0] + matrix2[0][0] + matrix3[0][0]).numpy())</p><p class="source-code">print((matrix1[0][1] + matrix2[0][1] + matrix3[0][1]).numpy())</p><p class="source-code">print((matrix1[0][2] + matrix2[0][2] + matrix3[0][2]).numpy())</p><p class="source-code">print((matrix1[1][0] + matrix2[1][0] + matrix3[1][0]).numpy())</p><p class="source-code">print((matrix1[1][1] + matrix2[1][1] + matrix3[1][1]).numpy())</p><p class="source-code">print((matrix1[1][2] + matrix2[1][2] + matrix3[1][2]).numpy())</p><p>This will result in the following output:</p><p class="source-code">18153</p><p class="source-code">17557</p><p class="source-code">18731</p><p class="source-code">15747</p><p class="source-code">3257</p><p class="source-code">2463</p><p>You can see that the <strong class="source-inline">+</strong> operation is equivalent to the element-wise addition of the three matrices created.</p></li>
			</ol>
			<p>In this exercise, you successfully performed tensor addition on data representing votes cast for political candidates. The transformation can be applied by using the <strong class="source-inline">+</strong> operation. You also verified that addition is performed element by element, and that one way to ensure that the transformation is valid is for the tensors to have the same rank and shape.</p>
			<p>In the following activity, you will further practice tensor addition in TensorFlow.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Activity 1.01: Performing Tensor Addition in TensorFlow</h2>
			<p>You work in a company that has three locations, each with two salespersons and each location sells three products. You are required to sum the tensors to represent the total revenue for each product across locations.</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B16341_01_10.jpg" alt="Figure 1.10: Number of different products sold by each salesperson at different locations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10: Number of different products sold by each salesperson at different locations</p>
			<p>The steps you will take are as follows:</p>
			<ol>
				<li value="1">Import the TensorFlow library.</li>
				<li>Create two scalars to represent the total revenue for <strong class="source-inline">Product A</strong> by all salespeople at <strong class="source-inline">Location X</strong> using TensorFlow's <strong class="source-inline">Variable</strong> class. The first variable will have a value of <strong class="source-inline">2706</strong> and the second will have a value of <strong class="source-inline">2386</strong>.</li>
				<li>Create a new variable as the sum of the scalars and print the result.<p>You should get the following output:</p><p class="source-code">5092</p></li>
				<li>Create a vector with values <strong class="source-inline">[2706, 2799, 5102]</strong> and a scalar with the value <strong class="source-inline">95</strong> using TensorFlow's <strong class="source-inline">Variable</strong> class.</li>
				<li>Create a new variable as the sum of the scalar with the vector to represent the sales goal for <strong class="source-inline">Salesperson 1</strong> at <strong class="source-inline">Location X</strong> and print the result.<p>You should get the following output:</p><div id="_idContainer060" class="IMG---Figure"><img src="image/B16341_01_11.jpg" alt="Figure 1.11: The output of the integer-vector summation as a NumPy variable&#13;&#10;"/></div><p class="figure-caption">Figure 1.11: The output of the integer-vector summation as a NumPy variable</p></li>
				<li>Create three tensors with a rank of 2 representing the revenue for each salesperson, product, and location using TensorFlow's <strong class="source-inline">Variable</strong> class. The first tensor will have the value <strong class="source-inline">[[2706, 2799, 5102], [2386, 4089, 5932]]</strong>, the second will have the value <strong class="source-inline">[[5901, 1208, 645], [6235, 1098, 948]]</strong>, and the third will have <strong class="source-inline">[[3908, 2339, 5520], [4544, 1978, 4729]]</strong>.</li>
				<li>Create a new variable as the sum of the matrices and print the result:<div id="_idContainer061" class="IMG---Figure"><img src="image/B16341_01_12.jpg" alt="Figure 1.12: The output of the matrix summation as a NumPy variable&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.12: The output of the matrix summation as a NumPy variable</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor250">this link</a>.</p>
			<p>In the following section, you will learn how to change a tensor's shape and rank.</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Reshaping</h1>
			<p>Some operations, such as addition, can only be applied to tensors if they meet certain conditions. Reshaping is one method for modifying the shape of tensors so that such operations can be performed. Reshaping takes the elements of a tensor and rearranges them into a tensor of a different size. A tensor of any size can be reshaped so long as the number of total elements remains the same. </p>
			<p>For example, a <strong class="source-inline">(4x3)</strong> matrix can be reshaped into a <strong class="source-inline">(6x2)</strong> matrix since they both have a total of <strong class="source-inline">12</strong> elements. The rank, or number, of dimensions, can also be changed in the reshaping process. For instance, a <strong class="source-inline">(4x3)</strong> matrix that has a rank equal to <strong class="source-inline">2</strong> can be reshaped into a <strong class="source-inline">(3x2x2)</strong> tensor that has a rank equal to <strong class="source-inline">3</strong>. The <strong class="source-inline">(4x3)</strong> matrix can also be reshaped into a <strong class="source-inline">(12x1)</strong> vector in which the rank has changed from <strong class="source-inline">2</strong> to <strong class="source-inline">1</strong>. </p>
			<p><em class="italic">Figure 1.13</em> illustrates tensor reshaping. On the left is a tensor with shape <strong class="source-inline">(3x2)</strong>, which can be reshaped to a tensor of shape equal to either <strong class="source-inline">(2x3)</strong>, <strong class="source-inline">(6)</strong>, or <strong class="source-inline">(6x1)</strong>. Here, the number of elements, that is, six, has remained constant, though the shape and rank of the tensor have changed:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B16341_01_13.jpg" alt="Figure 1.13: Visual representation of reshaping a (3x2) tensor to tensors of different shapes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13: Visual representation of reshaping a (3x2) tensor to tensors of different shapes</p>
			<p>Tensor reshaping can be performed in TensorFlow by using the <strong class="source-inline">reshape</strong> function and passing in the tensor and the desired shape of the new tensor as the arguments:</p>
			<p class="source-code">tensor1 = tf.Variable([1,2,3,4,5,6])</p>
			<p class="source-code">tensor_reshape = tf.reshape(tensor1, shape=[3,2])</p>
			<p>Here, a new tensor is created that has the same elements as the original; however, the shape is <strong class="source-inline">[3,2]</strong> instead of <strong class="source-inline">[6]</strong>.</p>
			<p>The next section introduces tensor transposition, which is another method for modifying the shape of a tensor.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Tensor Transposition</h2>
			<p>When a tensor is transposed, the elements in the tensor are rearranged in a specific order. The transpose operation is usually denoted as a <strong class="source-inline">T</strong> superscript on the tensor. The new position of each element in the tensor can be determined by <strong class="source-inline">(x</strong><span class="subscript">12…k</span><strong class="source-inline">)</strong><span class="superscript">T</span> = <strong class="source-inline">x</strong><span class="subscript">k…21</span>. For a matrix or tensor of rank equal to <strong class="source-inline">2</strong>, the rows become the columns and vice versa. An example of matrix transposition is shown in <em class="italic">Figure 1.14</em>. Tensors of any rank can be transposed, and often the shape changes as a result:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B16341_01_14.jpg" alt="Figure 1.14: A visual representation of tensor transposition on a (3x2) matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14: A visual representation of tensor transposition on a (3x2) matrix</p>
			<p>The following diagram shows the matrix transposition properties of matrices <strong class="source-inline">A</strong> and <strong class="source-inline">B</strong>:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B16341_01_15.jpg" alt="Figure 1.15: Tensor transposition properties where X and Y are tensors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.15: Tensor transposition properties where X and Y are tensors</p>
			<p>A tensor is said to be symmetrical if the transpose of a tensor is equivalent to the original tensor. </p>
			<p>Tensor transposition can be performed in TensorFlow by using its <strong class="source-inline">transpose</strong> function and passing in the tensor as the only argument:</p>
			<p class="source-code">tensor1 = tf.Variable([1,2,3,4,5,6])</p>
			<p class="source-code">tensor_transpose = tf.transpose(tensor1)</p>
			<p>When transposing a tensor, there is only one possible result; however, reshaping a tensor has multiple possible results depending on the desired shape of the output.</p>
			<p>In the following exercise, reshaping and transposition are demonstrated on tensors using TensorFlow.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Exercise 1.04: Performing Tensor Reshaping and Transposition in TensorFlow</h2>
			<p>In this exercise, you will learn how to perform tensor reshaping and transposition using the TensorFlow library.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Import the TensorFlow library and create a matrix with two rows and four columns using TensorFlow's <strong class="source-inline">Variable</strong> class: <p class="source-code">import tensorflow as tf</p><p class="source-code">matrix1 = tf.Variable([[1,2,3,4], [5,6,7,8]])</p></li>
				<li>Verify the shape of the matrix by calling the <strong class="source-inline">shape</strong> attribute of the matrix as a Python list:<p class="source-code">matrix1.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[2, 4]</p><p>You see that the shape of the matrix is <strong class="source-inline">[2,4]</strong>.</p></li>
				<li>Use TensorFlow's <strong class="source-inline">reshape</strong> function to change the matrix to a matrix with four rows and two columns by passing in the matrix and the desired new shape:<p class="source-code">reshape1 = tf.reshape(matrix1, shape=[4, 2])</p><p class="source-code">reshape1</p><p>You should get the following output:</p><div id="_idContainer065" class="IMG---Figure"><img src="image/B16341_01_16.jpg" alt="Figure 1.16: The reshaped matrix&#13;&#10;"/></div><p class="figure-caption">Figure 1.16: The reshaped matrix</p></li>
				<li>Verify the shape of the reshaped matrix by calling the <strong class="source-inline">shape</strong> attribute as a Python list:<p class="source-code">reshape1.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[4, 2]</p><p>Here, you can see that the shape of the matrix has changed to your desired shape, <strong class="source-inline">[4,2]</strong>.</p></li>
				<li>Use TensorFlow's <strong class="source-inline">reshape</strong> function to convert the matrix into a matrix with one row and eight columns. Pass the matrix and the desired new shape as parameters to the <strong class="source-inline">reshape</strong> function:<p class="source-code">reshape2 = tf.reshape(matrix1, shape=[1, 8])</p><p class="source-code">reshape2</p><p>You should get the following output:</p><p class="source-code">&lt;tf.Tensor: shape=(1, 8), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7, 8]])&gt;</p></li>
				<li>Verify the shape of the reshaped matrix by calling the <strong class="source-inline">shape</strong> attribute as a Python list:<p class="source-code">reshape2.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[1, 8]</p><p>The preceding output confirms the shape of the reshaped matrix as <strong class="source-inline">[1, 8]</strong>.</p></li>
				<li>Use TensorFlow's <strong class="source-inline">reshape</strong> function to convert the matrix into a matrix with eight rows and one column, passing the matrix and the desired new shape as parameters to the <strong class="source-inline">reshape</strong> function:<p class="source-code">reshape3 = tf.reshape(matrix1, shape=[8, 1])</p><p class="source-code">reshape3</p><p>You should get the following output:</p><div id="_idContainer066" class="IMG---Figure"><img src="image/B16341_01_17.jpg" alt="Figure 1.17: Reshaped matrix of shape (8, 1)&#13;&#10;"/></div><p class="figure-caption">Figure 1.17: Reshaped matrix of shape (8, 1)</p></li>
				<li>Verify the shape of the reshaped matrix by calling the <strong class="source-inline">shape</strong> attribute as a Python list:<p class="source-code">reshape3.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[8, 1]</p><p>The preceding output confirms the shape of the reshaped matrix as <strong class="source-inline">[8, 1]</strong>.</p></li>
				<li>Use TensorFlow's <strong class="source-inline">reshape</strong> function to convert the matrix to a tensor of size <strong class="source-inline">2x2x2</strong>. Pass the matrix and the desired new shape as parameters to the reshape function:<p class="source-code">reshape4 = tf.reshape(matrix1, shape=[2, 2, 2])</p><p class="source-code">reshape4</p><p>You should get the following output:</p><div id="_idContainer067" class="IMG---Figure"><img src="image/B16341_01_18.jpg" alt="Figure 1.18: Reshaped matrix of shape (2, 2, 2)&#13;&#10;"/></div><p class="figure-caption">Figure 1.18: Reshaped matrix of shape (2, 2, 2)</p></li>
				<li>Verify the shape of the reshaped matrix by calling the <strong class="source-inline">shape</strong> attribute as a Python list:<p class="source-code">reshape4.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[2, 2, 2]</p><p>The preceding output confirms the shape of the reshaped matrix as <strong class="source-inline">[2, 2, 2]</strong>.</p></li>
				<li>Verify that the rank has changed using TensorFlow's <strong class="source-inline">rank</strong> function and print the result as a NumPy variable:<p class="source-code">tf.rank(reshape4).numpy()</p><p>This will result in the following output:</p><p class="source-code">3</p></li>
				<li>Use TensorFlow's <strong class="source-inline">transpose</strong> function to convert the matrix of size <strong class="source-inline">2X4</strong> to a matrix of size <strong class="source-inline">4x2</strong>:<p class="source-code">transpose1 = tf.transpose(matrix1)</p><p class="source-code">transpose1</p><p>You should get the following output:</p><div id="_idContainer068" class="IMG---Figure"><img src="image/B16341_01_19.jpg" alt="Figure 1.19: Transposed matrix&#13;&#10;"/></div><p class="figure-caption">Figure 1.19: Transposed matrix</p></li>
				<li>Verify that the <strong class="source-inline">reshape</strong> function and the <strong class="source-inline">transpose</strong> function create different resulting matrices when applied to the given matrix:<p class="source-code">transpose1 == reshape1</p><div id="_idContainer069" class="IMG---Figure"><img src="image/B16341_01_20.jpg" alt="Figure 1.20: Verification that transposition and reshaping produce different results&#13;&#10;"/></div><p class="figure-caption">Figure 1.20: Verification that transposition and reshaping produce different results</p></li>
				<li>Use TensorFlow's <strong class="source-inline">transpose</strong> function to transpose the reshaped matrix in <em class="italic">step 9</em>:<p class="source-code">transpose2 = tf.transpose(reshape4)</p><p class="source-code">transpose2</p><p>This will result in the following output:</p><div id="_idContainer070" class="IMG---Figure"><img src="image/B16341_01_21.jpg" alt="Figure 1.21: The output of the transposition of the reshaped tensor&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.21: The output of the transposition of the reshaped tensor</p>
			<p>This result shows how the resulting tensor appears after reshaping and transposing a tensor.</p>
			<p>In this exercise, you have successfully modified the shape of a tensor either through reshaping or transposition. You studied how the shape and rank of the tensor changes following the reshaping and transposition operation.</p>
			<p>In the following activity, you will test your knowledge on how to reshape and transpose tensors using TensorFlow.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Activity 1.02: Performing Tensor Reshaping and Transposition in TensorFlow</h2>
			<p>In this activity, you are required to simulate the grouping of 24 school children for class projects. The dimensions of each resulting reshaped or transposed tensor will represent the size of each group.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Import the TensorFlow library.</li>
				<li>Create a one-dimensional tensor with 24 monotonically increasing elements using the <strong class="source-inline">Variable</strong> class to represent the IDs of the school children. Verify the shape of the matrix.<p>You should get the following output:</p><p class="source-code">[24]</p></li>
				<li>Reshape the matrix so that it has 12 rows and 2 columns using TensorFlow's <strong class="source-inline">reshape</strong> function representing 12 pairs of school children. Verify the shape of the new matrix.<p>You should get the following output:</p><p class="source-code">[12, 2]</p></li>
				<li>Reshape the original matrix so that it has a shape of <strong class="source-inline">3x4x2</strong> using TensorFlow's <strong class="source-inline">reshape</strong> function representing 3 groups of 4 sets of pairs of school children. Verify the shape of the new tensor.<p>You should get the following output:</p><p class="source-code">[3, 4, 2]</p></li>
				<li>Verify that the rank of this new tensor is <strong class="source-inline">3</strong>.</li>
				<li>Transpose the tensor created in <em class="italic">step 3</em> to represent 2 groups of 12 students using TensorFlow's <strong class="source-inline">transpose</strong> function. Verify the shape of the new tensor.<p>You should get the following output:</p><p class="source-code">[2, 12]</p><p class="callout-heading">Note</p><p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor252">this link</a>.</p></li>
			</ol>
			<p>In this section, you were introduced to some of the basic components of ANNs—tensors. You also learned about some basic manipulation of tensors, such as addition, transposition, and reshaping. You implemented these concepts by using functions in the TensorFlow library. </p>
			<p>In the next topic, you will extend your understanding of linear transformations by covering another important transformation related to ANNs—tensor multiplication.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Tensor Multiplication</h1>
			<p>Tensor multiplication is another fundamental operation that is used frequently in the process of building and training ANNs since information propagates through the network from the inputs to the result via a series of additions and multiplications. While the rules for addition are simple and intuitive, the rules for tensors are more complex. Tensor multiplication involves more than simple element-wise multiplication of the elements. Rather, a more complicated procedure is implemented that involves the dot product between the entire rows/columns of each of the tensors to calculate each element of the resulting tensor. This section will explain how multiplication works for two-dimensional tensors or matrices. However, tensors of higher orders can also be multiplied.</p>
			<p>Given a matrix, <strong class="source-inline">X = [x</strong><span class="subscript">ij</span><strong class="source-inline">]</strong><span class="subscript">m x n</span>, and another matrix, <strong class="source-inline">Y = [y</strong><span class="subscript">ij</span><strong class="source-inline">]</strong><span class="subscript">n x p</span>, the product of the two matrices is <strong class="source-inline">Z = XY = [z</strong><span class="subscript">ij</span><strong class="source-inline">]</strong><span class="subscript">m x p</span>, and each element, <strong class="source-inline">z</strong><span class="subscript">ij</span>, is defined element-wise as <img src="image/B16341_01_21a.png" alt="Formula"/>. The shape of the resultant matrix is the same as the outer dimensions of the matrix product, or the number of rows of the first matrix and the number of columns of the second matrix. For the multiplication to work, the inner dimensions of the matrix product must match, or the number of columns in the first matrix and the number of columns in the second matrix must correspond.</p>
			<p>The concept of inner and outer dimensions of matrix multiplication is shown in the following diagram, where <strong class="source-inline">X</strong> represents the first matrix and <strong class="source-inline">Y</strong> represents the second matrix:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B16341_01_22.jpg" alt="Figure 1.22: A visual representation of inner and outer dimensions in matrix multiplication&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.22: A visual representation of inner and outer dimensions in matrix multiplication</p>
			<p>Unlike matrix addition, matrix multiplication is not commutative, which means that the order of the matrices in the product matters:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B16341_01_23.jpg" alt="Figure 1.23: Matrix multiplication is non-commutative&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.23: Matrix multiplication is non-commutative</p>
			<p>For example, say you have the following two matrices:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B16341_01_24.jpg" alt="Figure 1.24: Two matrices, X and Y&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.24: Two matrices, X and Y</p>
			<p>One way to construct the product is to have matrix <strong class="source-inline">X</strong> first, multiplied by <strong class="source-inline">Y</strong>:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B16341_01_25.jpg" alt="Figure 1.25: Visual representation of matrix X multiplied by Y, X•Y &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.25: Visual representation of matrix X multiplied by Y, X•Y </p>
			<p>This results in a <strong class="source-inline">2x2</strong> matrix. Another way to construct the product is to have <strong class="source-inline">Y</strong> first, multiplied by <strong class="source-inline">X</strong>:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B16341_01_26.jpg" alt="Figure 1.26: Visual representation of matrix Y multiplied by X, Y•X&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.26: Visual representation of matrix Y multiplied by X, Y•X</p>
			<p>Here you can see that the matrix formed from the product <strong class="source-inline">YX</strong> is a <strong class="source-inline">3x3</strong> matrix and is very different from the matrix formed from the product <strong class="source-inline">XY</strong>.</p>
			<p>Tensor multiplication can be performed in TensorFlow by using the <strong class="source-inline">matmul</strong> function and passing in the tensors to be multiplied in the order in which they are to be multiplied as the arguments:</p>
			<p class="source-code">tensor1 = tf.Variable([[1,2,3]])</p>
			<p class="source-code">tensor2 = tf.Variable([[1],[2],[3]])</p>
			<p class="source-code">tensor_mult = tf.matmul(tensor1, tensor2)</p>
			<p>Tensor multiplication can also be achieved by using the <strong class="source-inline">@</strong> operator as follows:</p>
			<p class="source-code">tensor_mult = tensor1 @ tensor2</p>
			<p>Scalar-tensor multiplication is much more straightforward and is simply the product of every element in the tensor multiplied by the scalar so that <strong class="source-inline">λX = [λx</strong><span class="subscript">ij…k</span><strong class="source-inline">]</strong>, where <strong class="source-inline">λ</strong> is a scalar and <strong class="source-inline">X</strong> is a tensor.</p>
			<p>Scalar multiplication can be achieved in TensorFlow either by using the <strong class="source-inline">matmul</strong> function or by using the <strong class="source-inline">*</strong> operator:</p>
			<p class="source-code">tensor1 = tf.Variable([[1,2,3]])</p>
			<p class="source-code">scalar_mult = 5 * tensor1</p>
			<p>In the following exercise, you will perform tensor multiplication using the TensorFlow library. </p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Exercise 1.05: Performing Tensor Multiplication in TensorFlow</h2>
			<p>In this exercise, you will perform tensor multiplication in TensorFlow using TensorFlow's <strong class="source-inline">matmul</strong> function and the <strong class="source-inline">@</strong> operator. In this exercise, you will use the example of data from a sandwich retailer representing the ingredients of various sandwiches and the costs of different ingredients. You will use matrix multiplication to determine the costs of each sandwich.</p>
			<p><strong class="bold">Sandwich recipe</strong>:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B16341_01_27.jpg" alt="Figure 1.27: Sandwich recipe&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.27: Sandwich recipe</p>
			<p><strong class="bold">Ingredient details</strong>:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B16341_01_28.jpg" alt="Figure 1.28: Ingredient details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.28: Ingredient details</p>
			<p><strong class="bold">Sales projections</strong>:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B16341_01_29.jpg" alt="Figure 1.29: Sales projections&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.29: Sales projections</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a matrix representing the different sandwich recipes, with the rows representing the three different sandwich offerings and the columns representing the combination and number of the five different ingredients using the <strong class="source-inline">Variable</strong> class: <p class="source-code">matrix1 = tf.Variable([[1.0,0.0,3.0,1.0,2.0], \</p><p class="source-code">                       [0.0,1.0,1.0,1.0,1.0], \</p><p class="source-code">                       [2.0,1.0,0.0,2.0,0.0]], \</p><p class="source-code">                      tf.float32)</p><p class="source-code">matrix1</p><p>You should get the following output:</p><div id="_idContainer080" class="IMG---Figure"><img src="image/B16341_01_30.jpg" alt="Figure 1.30: Matrix representing the number of ingredients needed to make sandwiches&#13;&#10;"/></div><p class="figure-caption">Figure 1.30: Matrix representing the number of ingredients needed to make sandwiches</p></li>
				<li>Verify the shape of the matrix by calling the <strong class="source-inline">shape</strong> attribute of the matrix as a Python list:<p class="source-code">matrix1.shape.as_list()</p><p>This will result in the following output:</p><p class="source-code">[3, 5]</p></li>
				<li>Create a second matrix representing the cost and weight of each individual ingredient in which the rows represent the five ingredients, and the columns represent the cost and weight of the ingredients in grams:<p class="source-code">matrix2 = tf.Variable([[0.49, 103], \</p><p class="source-code">                       [0.18, 38], \</p><p class="source-code">                       [0.24, 69], \</p><p class="source-code">                       [1.02, 75], \</p><p class="source-code">                       [0.68, 78]])</p><p class="source-code">matrix2</p><p>You should get the following result:</p><div id="_idContainer081" class="IMG---Figure"><img src="image/B16341_01_31.jpg" alt="Figure 1.31: A matrix representing the cost and weight of each ingredient&#13;&#10;"/></div><p class="figure-caption">Figure 1.31: A matrix representing the cost and weight of each ingredient</p></li>
				<li>Use TensorFlow's <strong class="source-inline">matmul</strong> function to perform the matrix multiplication of <strong class="source-inline">matrix1</strong> and <strong class="source-inline">matrix2</strong>:<p class="source-code">matmul1 = tf.matmul(matrix1, matrix2)</p><p class="source-code">matmul1</p><p>This will result in the following output:</p><div id="_idContainer082" class="IMG---Figure"><img src="image/B16341_01_32.jpg" alt="Figure 1.32: The output of the matrix multiplication&#13;&#10;"/></div><p class="figure-caption">Figure 1.32: The output of the matrix multiplication</p></li>
				<li>Create a matrix to represent the sales projections of five different stores for each of the three sandwiches:<p class="source-code">matrix3 = tf.Variable([[120.0, 100.0, 90.0], \</p><p class="source-code">                       [30.0, 15.0, 20.0], \</p><p class="source-code">                       [220.0, 240.0, 185.0], \</p><p class="source-code">                       [145.0, 160.0, 155.0], \</p><p class="source-code">                       [330.0, 295.0, 290.0]])</p></li>
				<li>Multiply <strong class="source-inline">matrix3</strong> by the result of the matrix multiplication of <strong class="source-inline">matrix1</strong> and <strong class="source-inline">matrix2</strong> to give the expected cost and weight for each of the five stores:<p class="source-code">matmul3 = matrix3 @ matmul1</p><p class="source-code">matmul3</p><p>This will result in the following output:</p><div id="_idContainer083" class="IMG---Figure"><img src="image/B16341_01_33.jpg" alt="Figure 1.33: The output of matrix multiplication&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.33: The output of matrix multiplication</p>
			<p>The resulting tensor from the multiplication shows the expected cost of sandwiches and the expected weight of the total ingredients for each of the stores.</p>
			<p>In this exercise, you have successfully learned how to perform matrix multiplication in TensorFlow using several operators. You used TensorFlow's <strong class="source-inline">matmul</strong> function, as well as the shorthand <strong class="source-inline">@</strong> operator. Each will perform the multiplication; however, the <strong class="source-inline">matmul</strong> function has several different arguments that can be passed into the function that make it more flexible.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about the <strong class="source-inline">matmul</strong> function here: <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul">https://www.tensorflow.org/api_docs/python/tf/linalg/matmul</a>.</p>
			<p>In the next section, you will explore some other mathematical concepts that are related to ANNs. You will explore forward and backpropagation, as well as activation functions.</p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Optimization</h1>
			<p>In this section, you will learn about some optimization approaches that are fundamental to training machine learning models. Optimization is the process by which the weights of the layers of an ANN are updated such that the error between the predicted values of the ANN and the true values of the training data is minimized.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Forward Propagation</h2>
			<p>Forward propagation is the process by which information propagates through ANNs. Operations such as a series of tensor multiplications and additions occur at each layer of the network until the final output. Forward propagation is explained in <em class="italic">Figure 1.37</em>, showing a single hidden layer ANN. The input data has two features, while the output layer has a single value for each input record. </p>
			<p>The weights and biases for the hidden layer and output are shown as matrices and vectors with the appropriate indexes. For the hidden layer, the number of rows in the weight matrix is equal to the number of features of the input, and the number of columns is equal to the number of units in the hidden layer. Therefore, <strong class="source-inline">W1</strong> has two rows and three columns because the input, <strong class="source-inline">X</strong>, has two features. Likewise, <strong class="source-inline">W2</strong> has three rows and one column, the hidden layer has three units, and the output has the size one. The bias, however, is always a vector with a size equal to the number of nodes in that layer and is added to the product of the input and weight matrix.</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B16341_01_34.jpg" alt="Figure 1.34: A single-layer artificial neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.34: A single-layer artificial neural network</p>
			<p>The steps to perform forward propagation are as follows:</p>
			<ol>
				<li value="1"><strong class="source-inline">X</strong> is the input to the network and the input to the hidden layer. First, the input matrix, <strong class="source-inline">X</strong>, is multiplied by the weight matrix for the hidden layer, <strong class="source-inline">W1</strong>, and then the bias, <strong class="source-inline">b1</strong>, is added:<p><strong class="source-inline">z1 = X*W1 + b1</strong></p><p>Here is an example of what the shape of the resulting tensor will be after the operation. If the input is size <strong class="source-inline">nX2</strong>, where <strong class="source-inline">n</strong> is the number of input examples, <strong class="source-inline">W1</strong> is of size <strong class="source-inline">2X3</strong>, and <strong class="source-inline">b1</strong> is of size <strong class="source-inline">1X3</strong>, the resulting matrix, <strong class="source-inline">z1</strong>, will have a size of <strong class="source-inline">nX3</strong>.</p></li>
				<li><strong class="source-inline">z1</strong> is the output of the hidden layer, which is the <strong class="bold">input</strong> for the output layer. Next, the output of the hidden layer is the input matrix multiplied by the weight matrix for the output layer, <strong class="source-inline">W2</strong>, and the bias, <strong class="source-inline">b2</strong>, is added:<p><strong class="source-inline">Y = z1 * W2 + b2</strong></p><p>To understand the shape of the resulting tensor, consider the following example. If the input to the output layer, <strong class="source-inline">z1</strong>, is of size <strong class="source-inline">nX3</strong>, <strong class="source-inline">W2</strong> is of size <strong class="source-inline">3X1</strong>, and <strong class="source-inline">b1</strong> is of size <strong class="source-inline">1X1</strong>, the resulting matrix, <strong class="source-inline">Y</strong>, will have a size of <strong class="source-inline">nX1</strong>, representing one result for each training example.</p></li>
			</ol>
			<p>The total number of parameters in this model is equal to the sum of the number of elements in <strong class="source-inline">W1</strong>, <strong class="source-inline">W2</strong>, <strong class="source-inline">b1</strong>, and <strong class="source-inline">b2</strong>. Therefore, the number of parameters can be calculated by summing the elements in each of the parameters in weight matrices and biases, which is equal to <strong class="source-inline">6 + 3 + 3 + 1 = 13</strong>. These are the parameters that need to be learned in the process of training the ANN. </p>
			<p>Following the forward propagation step, you must evaluate your model and compare it to the real target values. This is achieved using a loss function. Mean squared error, that is, the mean value of the squared difference between true and predicted values, is one of the examples of the loss function of the regression task. Once the loss is calculated, the weights must be updated to reduce the loss, and the amount and direction that the weights should be updated are found using backpropagation.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Backpropagation</h2>
			<p><strong class="bold">Backpropagation</strong> is the process of determining the derivative of the loss with respect to the model parameter. The loss is calculated by applying the <strong class="source-inline">loss</strong> function to the predicted outputs as follows:</p>
			<p><strong class="source-inline">loss = L(y_predicted)</strong></p>
			<p>The derivative of the loss with respect to the model parameters will inform you if increasing or decreasing the model parameter will result in increasing or decreasing the loss. The process of backpropagation is achieved by applying the chain rule of calculus from the output layer to the input layer of a neural network, at each layer computing the derivatives of the <strong class="source-inline">loss</strong> function with respect to the model parameters.</p>
			<p>The chain rule of calculus is a technique used to compute the derivative of a composite function via intermediate functions. A generalized version of the function can be written as follows: </p>
			<p><strong class="source-inline">dz/dx = dz/dy * dy/dx</strong></p>
			<p>Here, <strong class="source-inline">dz/dx</strong> is the composite function and <strong class="source-inline">y</strong> is the intermediate function. In the case of ANNs, the composite function is the loss as a function of the model parameters and the intermediate functions represent the hidden layers. Therefore, the derivative of the loss with respect to the model parameters can be computed by multiplying the derivative of the loss with respect to the predicted output by the derivative of the predicted output with respect to the model parameters.</p>
			<p>In the next section, you will learn how the weight parameters are updated given the derivatives of the loss function with respect to each of the weights so that the loss is minimized.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Learning Optimal Parameters</h2>
			<p>In this section, you will see how optimal weights are iteratively chosen. You know that forward propagation transfers information through the network via a series of tensor additions and multiplications, and that backpropagation is the process of understanding the change in loss with respect to each model weight. The next step is to use the results from backpropagation to update the weights so that they reduce the error according to the loss function. This process is known as learning the parameters and is achieved using an optimization algorithm. A common optimization algorithm often utilized is called <strong class="bold">gradient descent</strong>.</p>
			<p>In learning the optimal parameters, you apply the optimization algorithm until a minimum in the loss function is reached. You usually stop after a given number of steps or when there is a negligible change in the loss function. If you plot the loss as a function of each model parameter, the shape of the loss function resembles a convex shape, having only one minimum, and it is the goal of the optimization function to find this minimum. </p>
			<p>The following figure shows the loss function of a particular feature: </p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B16341_01_35.jpg" alt="Figure 1.35: A visual representation of the gradient descent algorithm finding &#13;&#10;the optimal parameter to minimize the loss&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.35: A visual representation of the gradient descent algorithm finding the optimal parameter to minimize the loss</p>
			<p>This is achieved, first, by randomly setting parameters for each weight, indicated by <strong class="source-inline">p</strong><span class="subscript">1</span> in the diagram. The loss is then calculated for that model parameter, <strong class="source-inline">l</strong><span class="subscript">1</span>. The backpropagation step determines the derivative of the loss with respect to the model parameter and will determine in which direction the model should be updated. The next model parameter, <strong class="source-inline">p</strong><span class="subscript">2</span>, is equal to the current model parameter minus the learning rate (<strong class="source-inline">α</strong>) multiplied by the derivative value. The learning rate is a hyperparameter that is set before the model training process. By multiplying by the derivative value, larger steps will be taken when the parameter is far from the minimum where the absolute value for the derivative is larger. The loss, <strong class="source-inline">l</strong><span class="subscript">2</span>, is then calculated and the process continues until the minimum loss is reached, <strong class="source-inline">l</strong><span class="subscript">m</span>, with the optimal parameter, <strong class="source-inline">p</strong><span class="subscript">m</span>.</p>
			<p>To summarize, these are the iterative steps that the optimization algorithm performs to find the optimal parameters: </p>
			<ol>
				<li value="1">Use forward propagation and current parameters to predict the outputs for the entire dataset.</li>
				<li>Apply the loss function to compute the loss over all the examples from the predicted output.</li>
				<li>Use backpropagation to compute the derivatives of the loss with respect to the weights and biases at each layer.</li>
				<li>Update the weights and biases using the derivative values and the learning rate.</li>
			</ol>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Optimizers in TensorFlow</h2>
			<p>There are several different optimizers readily available within TensorFlow. Each is based on a different optimization algorithm that aims to reach a global minimum for the loss function. They are all based on the gradient descent algorithm, although they differ slightly in implementation. The available optimizers in TensorFlow include the following:</p>
			<ul>
				<li><strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>): The SGD algorithm applies gradient descent to small batches of training data. A momentum parameter is also available when using the optimizer in TensorFlow that applies exponential smoothing to the computed gradient to speed up training.</li>
				<li><strong class="bold">Adam</strong>: This optimization is an SGD method that is based on the continuous adaptive estimation of first and second-order moments.</li>
				<li><strong class="bold">Root Mean Squared Propagation</strong> (<strong class="bold">RMSProp</strong>): This is an unpublished, adaptive learning rate optimizer. RMSprop divides the learning rate by an average of the squared gradients when finding the loss minimum after each step, which results in a learning rate that exponentially decays.</li>
				<li><strong class="bold">Adagrad</strong>: This optimizer has parameter-specific learning rates that are updated depending on how frequently the parameter is updated during the training process. As the parameter receives more updates, each subsequent update is smaller in value.</li>
			</ul>
			<p>The choice of optimizer will affect training time and model performance. Each optimizer also has hyperparameters, such as the initial learning rate, that must be selected before training, and tuning of these hyperparameters will also affect training time and model performance. While other optimizers available in TensorFlow are not explicitly stated here (and can be found here: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers</a>), those stated above perform well both in terms of training time and model performance and are a safe first choice when selecting an optimizer for your model. The optimizers available in TensorFlow are located in the <strong class="source-inline">tf.optimizers</strong> module; for example, an Adam optimizer with a learning rate equal to <strong class="source-inline">0.001</strong> can be initialized as follows:</p>
			<p class="source-code">optimizer = tf.optimizer.adam(learning_rate=0.001)</p>
			<p>In this topic, you have seen the steps taken in achieving gradient descent to compute the optimal parameters for model training. In gradient descent, every single training example is used to learn the parameters. However, when working with large volume datasets, such as with images and audio, you will often work in batches and make updates after learning from each batch. When using gradient descent on batch data, the algorithm is known as SGD. The SGD optimizer, along with a suite of other performant optimizers, is readily available in TensorFlow, including the Adam, RMSProp, and Adagrad optimizers, and more. </p>
			<p>In the next section, you will explore different activation functions, which are generally applied to the output of each layer.</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Activation functions</h1>
			<p>Activation functions are mathematical functions that are generally applied to the outputs of ANN layers to limit or bound the values of the layer. The reason that values may want to be bounded is that without activation functions, the value and corresponding gradients can either explode or vanish, thereby making the results unusable. This is because the final value is the cumulative product of the values from each subsequent layer. As the number of layers increases, the likelihood of values and gradients exploding to infinity or vanishing to zero increases. This concept is known as the <strong class="bold">exploding and vanishing gradient problem</strong>. Deciding whether a node in a layer should be <em class="italic">activated</em> is another use of activation functions, hence their name. Common activation functions and their visual representation in <em class="italic">Figure 1.36</em> are as follows:</p>
			<ul>
				<li><strong class="bold">Step</strong> function: The value is non-zero if it is above a certain threshold, otherwise it is zero. This is shown in <em class="italic">Figure 1.36a</em>.</li>
				<li><strong class="bold">Linear</strong> function: <img src="image/B16341_01_35a.png" alt="Formula"/>, which is a scalar multiplication of the input value. This is shown in <em class="italic">Figure 1.36b</em>.</li>
				<li><strong class="bold">Sigmoid</strong> function: <img src="image/B16341_01_35b.png" alt="Formula"/>, like a smoothed-out step function with smooth gradients. This activation function is useful for classification since the values are bound from zero to one. This is shown in <em class="italic">Figure 1.36c</em>.</li>
				<li><strong class="bold">Tanh</strong> function: <img src="image/B16341_01_35c.png" alt="Formula"/>, which is a scaled version of the sigmoid with steeper gradients around <strong class="source-inline">x=0</strong>. This is shown in <em class="italic">Figure 1.36d</em>.</li>
				<li><strong class="bold">ReLU</strong> (<strong class="bold">Rectified Linear Unit</strong>) function: <img src="image/B16341_01_35d.png" alt="Formula"/>, otherwise <strong class="source-inline">0</strong>. This is shown in <em class="italic">Figure 1.36e</em>.</li>
				<li><strong class="bold">ELU</strong> (<strong class="bold">Exponential Linear Unit</strong>) function: <img src="image/B16341_01_35e.png" alt="Formula"/>, otherwise <img src="image/B16341_01_35f.png" alt="Formula"/>, where <img src="image/B16341_01_35g.png" alt="Formula"/> is a constant.</li>
				<li><strong class="bold">SELU</strong> (<strong class="bold">Scaled Exponential Linear Unit</strong>) function: <img src="image/B16341_01_35h.png" alt="Formula"/>, otherwise <img src="image/B16341_01_35i.png" alt="Formula"/>, where <img src="image/B16341_01_35j.png" alt="Formula"/>are constants. This is shown in <em class="italic">Figure 1.36f</em>.</li>
				<li><strong class="bold">Swish</strong> function: <img src="image/B16341_01_35k.png" alt="Formula"/>. This is shown in <em class="italic">Figure 1.36g</em>:</li>
			</ul>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B16341_01_36.jpg" alt="Figure 1.36: A visual representation of the common activation functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.36: A visual representation of the common activation functions</p>
			<p>An activation function can be applied to any tensor by utilizing the activation functions in the <strong class="source-inline">tf.keras.activations</strong> module. For example, a sigmoid activation function can be applied to a tensor as follows:</p>
			<p class="source-code">y=tf.keras.activations.sigmoid(x)</p>
			<p>Now, let's test the knowledge that you have gained so far in the following activity.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Activity 1.03: Applying Activation Functions </h2>
			<p>In this activity, you will recall many of the concepts used throughout the chapter as well as apply activation functions to tensors. You will use example data of car dealership sales, apply these concepts, show the sales records of various salespeople, and highlight those with net positive sales.</p>
			<p><strong class="bold">Sales records</strong>:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B16341_01_37.jpg" alt="Figure 1.37: Sales records&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.37: Sales records</p>
			<p><strong class="bold">Vehicle MSRPs</strong>:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B16341_01_38.jpg" alt="Figure 1.38: Vehicle MSRPs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.38: Vehicle MSRPs</p>
			<p><strong class="bold">Fixed costs</strong>:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B16341_01_39.jpg" alt="Figure 1.39: Fixed costs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.39: Fixed costs</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Import the TensorFlow library.</li>
				<li>Create a <strong class="source-inline">3x4</strong> tensor as an input with the values <strong class="source-inline">[[-0.013, 0.024, 0.06, 0.022], [0.001, -0.047, 0.039, 0.016], [0.018, 0.030, -0.021, -0.028]]</strong>. The rows in this tensor represent the sales of various sales representatives, the columns represent various vehicles available at the dealership, and values represent the average percentage difference from MSRP. The values are positive or negative depending on whether the salesperson was able to sell for more or less than the MSRP. </li>
				<li>Create a <strong class="source-inline">4x1</strong> weights tensor with the shape <strong class="source-inline">4x1</strong> with the values <strong class="source-inline">[[19995.95], [24995.50], [36745.50], [29995.95]]</strong> representing the MSRP of the cars.</li>
				<li>Create a bias tensor of size <strong class="source-inline">3x1</strong> with the values <strong class="source-inline">[[-2500.0], [-2500.0], [-2500.0]]</strong> representing the fixed costs associated with each salesperson.</li>
				<li>Matrix multiply the input by the weight to show the average deviation from the MSRP on all cars and add the bias to subtract the fixed costs of the salesperson. Print the result.<p>You should get the following result:</p><div id="_idContainer101" class="IMG---Figure"><img src="image/B16341_01_40.jpg" alt="Figure 1.40: The output of the matrix multiplication&#13;&#10;"/></div><p class="figure-caption">Figure 1.40: The output of the matrix multiplication</p></li>
				<li>Apply a ReLU activation function to highlight the net-positive salespeople and print the result.<p>You should get the following result:</p><div id="_idContainer102" class="IMG---Figure"><img src="image/B16341_01_41.jpg" alt="Figure 1.41: The output after applying the activation function&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.41: The output after applying the activation function</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor253">this link</a>.</p>
			<p>In subsequent chapters, you will see how to add activation functions to your ANNs, either between layers or applied directly after a layer when layers are defined. You will learn how to choose which activation functions are most appropriate, which is often by hyperparameter optimization techniques. The activation function is one example of a hyperparameter, a parameter set before the learning process begins, that can be tuned to find the optimal values for model performance.</p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Summary</h1>
			<p>In this chapter, you were introduced to the TensorFlow library. You learned how to use it in the Python programming language. You created the building blocks of ANNs (tensors) with various ranks and shapes, performed linear transformations on tensors using TensorFlow, and implemented addition, reshaping, transposition, and multiplication on tensors—all of which are fundamental for understanding the underlying mathematics of ANNs.</p>
			<p>In<a id="_idTextAnchor043"/> the next chapter, you will improve your understanding of tensors and learn how to load data of various types and pre-process it such that it is appropriate for training ANNs in TensorFlow. You will work with tabular, visual, and textual data, all of which must be pre-processed differently. By working with visual data (that is, images), you will also learn how to use training data in which the size of the training data cannot fit into memory.</p>
		</div>
		<div>
			<div id="_idContainer104" class="Content">
			</div>
		</div>
	</body></html>