["```\n$> pip install Pillow\n```", "```\n    import os\n    import pathlib\n    import glob\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from tensorflow.keras import Model\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    def load_images_and_labels(image_paths):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, target_size=(32,32), \n                             color_mode='grayscale')\n            image = img_to_array(image)\n            label = image_path.split(os.path.sep)[-2]\n            label = 'positive' in label\n            label = float(label)\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    def build_network():\n        input_layer = Input(shape=(32, 32, 1))\n        x = Conv2D(filters=20,\n                   kernel_size=(5, 5),\n                   padding='same',\n                   strides=(1, 1))(input_layer)\n        x = ELU()(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=(2, 2),\n                         strides=(2, 2))(x)\n        x = Dropout(0.4)(x)\n        x = Conv2D(filters=50,\n                   kernel_size=(5, 5),\n                   padding='same',\n                   strides=(1, 1))(x)\n        x = ELU()(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=(2, 2),\n                         strides=(2, 2))(x)\n        x = Dropout(0.4)(x)\n        x = Flatten()(x)\n        x = Dense(units=500)(x)\n        x = ELU()(x)\n        x = Dropout(0.4)(x)\n        output = Dense(1, activation='sigmoid')(x)\n        model = Model(inputs=input_layer, outputs=output)\n        return model\n    ```", "```\n    files_pattern = (pathlib.Path.home() / '.keras' / \n                     'datasets' /\n                     'SMILEsmileD-master' / 'SMILEs' / '*' \n                        / '*' / \n                     '*.jpg')\n    files_pattern = str(files_pattern)\n    dataset_paths = [*glob.glob(files_pattern)]\n    ```", "```\n    X, y = load_images_and_labels(dataset_paths)\n    ```", "```\n    X /= 255.0\n    total = len(y)\n    total_positive = np.sum(y)\n    total_negative = total - total_positive\n    ```", "```\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y,\n                                         test_size=0.2,\n                                         stratify=y,\n                                         random_state=999)\n    (X_train, X_val,\n     y_train, y_val) = train_test_split(X_train, y_train,\n                                        test_size=0.2,\n                                        stratify=y_train,\n                                        random_state=999)\n    ```", "```\n    model = build_network()\n    model.compile(loss='binary_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    ```", "```\n    BATCH_SIZE = 32\n    EPOCHS = 20\n    model.fit(X_train, y_train,\n              validation_data=(X_val, y_val),\n              epochs=EPOCHS,\n              batch_size=BATCH_SIZE,\n              class_weight={\n                  1.0: total / total_positive,\n                  0.0: total / total_negative\n              })\n    ```", "```\n    test_loss, test_accuracy = model.evaluate(X_test, \n                                              y_test)\n    ```", "```\n    import os\n    import pathlib\n    import glob\n    import numpy as np\n    import tensorflow as tf\n    from sklearn.model_selection import train_test_split\n    from tensorflow.keras import Model\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.losses import CategoricalCrossentropy\n    ```", "```\n    CLASSES = ['rock', 'paper', 'scissors']\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    ```", "```\n    def load_image_and_label(image_path, target_size=(32, 32)):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.rgb_to_grayscale(image)\n        image = tf.image.convert_image_dtype(image, \n                                             np.float32)\n        image = tf.image.resize(image, target_size)\n        label = tf.strings.split(image_path,os.path.sep)[-2]\n        label = (label == CLASSES)  # One-hot encode.\n        label = tf.dtypes.cast(label, tf.float32)\n        return image, label\n    ```", "```\n    def build_network():\n        input_layer = Input(shape=(32, 32, 1))\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same',\n                   strides=(1, 1))(input_layer)\n        x = ReLU()(x)\n        x = Dropout(rate=0.5)(x)\n        x = Flatten()(x)\n        x = Dense(units=3)(x)\n        output = Softmax()(x)\n        return Model(inputs=input_layer, outputs=output)\n    ```", "```\n    def prepare_dataset(dataset_path,\n                        buffer_size,\n                        batch_size,\n                        shuffle=True):\n        dataset = (tf.data.Dataset\n                   .from_tensor_slices(dataset_path)\n                   .map(load_image_and_label,\n                        num_parallel_calls=AUTOTUNE))\n        if shuffle:\n            dataset.shuffle(buffer_size=buffer_size)\n        dataset = (dataset\n                   .batch(batch_size=batch_size)\n                   .prefetch(buffer_size=buffer_size))\n        return dataset\n    ```", "```\n    file_patten = (pathlib.Path.home() / '.keras' / \n                   'datasets' /\n                   'rockpaperscissors' / 'rps-cv-images' / \n                     '*' /\n                   '*.png')\n    file_pattern = str(file_patten)\n    dataset_paths = [*glob.glob(file_pattern)]\n    ```", "```\n    train_paths, test_paths = train_test_split(dataset_paths,\n                                              test_size=0.2,\n                                            random_state=999)\n    train_paths, val_paths = train_test_split(train_paths,\n                                          test_size=0.2,\n                                         random_state=999)\n    ```", "```\n    BATCH_SIZE = 1024\n    BUFFER_SIZE = 1024\n    train_dataset = prepare_dataset(train_paths,\n                                  buffer_size=BUFFER_SIZE,\n                                    batch_size=BATCH_SIZE)\n    validation_dataset = prepare_dataset(val_paths,\n                                  buffer_size=BUFFER_SIZE,\n                                   batch_size=BATCH_SIZE,\n                                    shuffle=False)\n    test_dataset = prepare_dataset(test_paths,\n                                  buffer_size=BUFFER_SIZE,\n                                   batch_size=BATCH_SIZE,\n                                   shuffle=False)\n    ```", "```\n    model = build_network()\n    model.compile(loss=CategoricalCrossentropy\n                 (from_logits=True),\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    ```", "```\n    EPOCHS = 250\n    model.fit(train_dataset,\n              validation_data=validation_dataset,\n              epochs=EPOCHS)\n    ```", "```\n    test_loss, test_accuracy = model.evaluate(test_dataset)\n    ```", "```\n$> pip install Pillow\n```", "```\n    import os\n    import pathlib\n    from csv import DictReader\n    import glob\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import MultiLabelBinarizer\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    def build_network(width, height, depth, classes):\n        input_layer = Input(shape=(width, height, depth))\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(input_layer)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=512)(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.5)(x)\n        x = Dense(units=classes)(x)\n        output = Activation('sigmoid')(x)\n        return Model(input_layer, output)\n    ```", "```\n    def load_images_and_labels(image_paths, styles, \n                               target_size):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, \n                             target_size=target_size)\n            image = img_to_array(image)\n            image_id = image_path.split(os.path.sep)[-\n                                             1][:-4]\n            image_style = styles[image_id]\n            label = (image_style['gender'], \n                     image_style['usage'])\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    SEED = 999\n    np.random.seed(SEED)\n    ```", "```\n    base_path = (pathlib.Path.home() / '.keras' / \n                 'datasets' /\n                 'fashion-product-images-small')\n    styles_path = str(base_path / 'styles.csv')\n    images_path_pattern = str(base_path / 'images/*.jpg')\n    image_paths = glob.glob(images_path_pattern)\n    ```", "```\n    with open(styles_path, 'r') as f:\n        dict_reader = DictReader(f)\n        STYLES = [*dict_reader]\n        article_type = 'Watches'\n        genders = {'Men', 'Women'}\n        usages = {'Casual', 'Smart Casual', 'Formal'}\n        STYLES = {style['id']: style\n                  for style in STYLES\n                  if (style['articleType'] == article_type \n                                               and\n                      style['gender'] in genders and\n                      style['usage'] in usages)}\n    image_paths = [*filter(lambda p: \n                   p.split(os.path.sep)[-1][:-4]\n                                     in STYLES.keys(),\n                           image_paths)]\n    ```", "```\n    X, y = load_images_and_labels(image_paths, STYLES, \n                                  (64, 64))\n    ```", "```\n    X = X.astype('float') / 255.0\n    mlb = MultiLabelBinarizer()\n    y = mlb.fit_transform(y)\n    ```", "```\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y,\n                                         stratify=y,\n                                         test_size=0.2,\n\n                                        random_state=SEED)\n    (X_train, X_valid,\n     y_train, y_valid) = train_test_split(X_train, y_train,\n                                        stratify=y_train,\n                                          test_size=0.2,\n                                       random_state=SEED)\n    ```", "```\n    model = build_network(width=64,\n                          height=64,\n                          depth=3,\n                          classes=len(mlb.classes_))\n    model.compile(loss='binary_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    ```", "```\n    BATCH_SIZE = 64\n    EPOCHS = 20\n    model.fit(X_train, y_train,\n              validation_data=(X_valid, y_valid),\n              batch_size=BATCH_SIZE,\n              epochs=EPOCHS)\n    ```", "```\n    result = model.evaluate(X_test, y_test, \n                           batch_size=BATCH_SIZE)\n    print(f'Test accuracy: {result[1]}')\n    ```", "```\n    Test accuracy: 0.90233546\n    ```", "```\n    test_image = np.expand_dims(X_test[0], axis=0)\n    probabilities = model.predict(test_image)[0]\n    for label, p in zip(mlb.classes_, probabilities):\n        print(f'{label}: {p * 100:.2f}%')\n    ```", "```\n    Casual: 100.00%\n    Formal: 0.00%\n    Men: 1.08%\n    Smart Casual: 0.01%\n    Women: 99.16%\n    ```", "```\n    ground_truth_labels = np.expand_dims(y_test[0], \n                                         axis=0)\n    ground_truth_labels = mlb.inverse_transform(ground_truth_labels)\n    print(f'Ground truth labels: {ground_truth_labels}')\n    ```", "```\n    Ground truth labels: [('Casual', 'Women')]\n    ```", "```\n    import os\n    import numpy as np\n    import tarfile\n    import tensorflow as tf\n    from tensorflow.keras.callbacks import ModelCheckpoint\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.models import *\n    from tensorflow.keras.regularizers import l2\n    from tensorflow.keras.utils import get_file\n    ```", "```\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    ```", "```\n        if reduce:\n            shortcut = Conv2D(filters=filters,\n                              kernel_size=(1, 1),\n                              strides=stride,\n                              use_bias=False,\n                        kernel_regularizer=l2(reg))(act_1)\n    ```", "```\n        x = Add()([conv_3, shortcut])\n        return x\n    ```", "```\n    def build_resnet(input_shape,\n                     classes,\n                     stages,\n                     filters,\n                     reg=1e-3,\n                     bn_eps=2e-5,\n                     bn_momentum=0.9):\n        inputs = Input(shape=input_shape)\n        x = BatchNormalization(axis=-1,\n                               epsilon=bn_eps,\n\n                             momentum=bn_momentum)(inputs)\n        x = Conv2D(filters[0], (3, 3),\n                   use_bias=False,\n                   padding='same',\n                   kernel_regularizer=l2(reg))(x)\n        for i in range(len(stages)):\n            stride = (1, 1) if i == 0 else (2, 2)\n            x = residual_module(data=x,\n                                filters=filters[i + 1],\n                                stride=stride,\n                                reduce=True,\n                                bn_eps=bn_eps,\n                                bn_momentum=bn_momentum)\n            for j in range(stages[i] - 1):\n                x = residual_module(data=x,\n                                    filters=filters[i + \n                                                   1],\n                                    stride=(1, 1),\n                                    bn_eps=bn_eps,\n\n                                bn_momentum=bn_momentum)\n        x = BatchNormalization(axis=-1,\n                               epsilon=bn_eps,\n                               momentum=bn_momentum)(x)\n        x = ReLU()(x)\n        x = AveragePooling2D((8, 8))(x)\n        x = Flatten()(x)\n        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n        x = Softmax()(x)\n        return Model(inputs, x, name='resnet')\n    ```", "```\n    def load_image_and_label(image_path, target_size=(32, 32)):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_png(image, channels=3)\n        image = tf.image.convert_image_dtype(image, \n                                            np.float32)\n        image -= CINIC_MEAN_RGB  # Mean normalize\n        image = tf.image.resize(image, target_size)\n        label = tf.strings.split(image_path, os.path.sep)[-2]\n        label = (label == CINIC_10_CLASSES)  # One-hot encode.\n        label = tf.dtypes.cast(label, tf.float32)\n        return image, label\n    ```", "```\n    def prepare_dataset(data_pattern, shuffle=False):\n        dataset = (tf.data.Dataset\n                   .list_files(data_pattern)\n                   .map(load_image_and_label,\n                        num_parallel_calls=AUTOTUNE)\n                   .batch(BATCH_SIZE))\n\n        if shuffle:\n            dataset = dataset.shuffle(BUFFER_SIZE)\n\n        return dataset.prefetch(BATCH_SIZE)\n    ```", "```\n    CINIC_MEAN_RGB = np.array([0.47889522, 0.47227842, 0.43047404])\n    ```", "```\n    CINIC_10_CLASSES = ['airplane', 'automobile', 'bird', 'cat',\n                        'deer', 'dog', 'frog', 'horse',    'ship',\n                        'truck']\n    ```", "```\n    DATASET_URL = ('https://datashare.is.ed.ac.uk/bitstream/handle/'\n                   '10283/3192/CINIC-10.tar.gz?'\n                   'sequence=4&isAllowed=y')\n    DATA_NAME = 'cinic10'\n    FILE_EXTENSION = 'tar.gz'\n    FILE_NAME = '.'.join([DATA_NAME, FILE_EXTENSION])\n    downloaded_file_location = get_file(origin=DATASET_URL,\n                                        fname=FILE_NAME,\n                                        extract=False)\n    data_directory, _ = (downloaded_file_location\n                         .rsplit(os.path.sep, maxsplit=1))\n    data_directory = os.path.sep.join([data_directory, \n                                      DATA_NAME])\n    tar = tarfile.open(downloaded_file_location)\n    if not os.path.exists(data_directory):\n        tar.extractall(data_directory)\n    ```", "```\n    train_pattern = os.path.sep.join(\n        [data_directory, 'train/*/*.png'])\n    test_pattern = os.path.sep.join(\n        [data_directory, 'test/*/*.png'])\n    valid_pattern = os.path.sep.join(\n        [data_directory, 'valid/*/*.png'])\n    ```", "```\n    BATCH_SIZE = 128\n    BUFFER_SIZE = 1024\n    train_dataset = prepare_dataset(train_pattern, \n                                    shuffle=True)\n    test_dataset = prepare_dataset(test_pattern)\n    valid_dataset = prepare_dataset(valid_pattern)\n    ```", "```\n    model = build_resnet(input_shape=(32, 32, 3),\n                         classes=10,\n                         stages=(9, 9, 9),\n                         filters=(64, 64, 128, 256),\n                         reg=5e-3)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    model_checkpoint_callback = ModelCheckpoint(\n        filepath='./model.{epoch:02d}-{val_accuracy:.2f}.hdf5',\n        save_weights_only=False,\n        monitor='val_accuracy')\n    EPOCHS = 100\n    model.fit(train_dataset,\n              validation_data=valid_dataset,\n              epochs=EPOCHS,\n              callbacks=[model_checkpoint_callback])\n    ```", "```\n    model = load_model('model.38-0.72.hdf5')\n    result = model.evaluate(test_dataset)\n    print(f'Test accuracy: {result[1]}')\n    ```", "```\n    Test accuracy: 0.71956664\n    ```", "```\n$> pip install Pillow\n```", "```\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from tensorflow.keras.applications import imagenet_utils\n    from tensorflow.keras.applications.inception_v3 import *\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    model = InceptionV3(weights='imagenet')\n    ```", "```\n    image = load_img('dog.jpg', target_size=(299, 299))\n    ```", "```\n    image = img_to_array(image)\n    image = np.expand_dims(image, axis=0)\n    ```", "```\n    image = preprocess_input(image)\n    ```", "```\n    predictions = model.predict(image)\n    prediction_matrix = (imagenet_utils\n                         .decode_predictions(predictions))\n    ```", "```\n    for i in range(5):\n        _, label, probability = prediction_matrix[0][i]\n        print(f'{i + 1}. {label}: {probability * 100:.3f}%')\n    ```", "```\n    1\\. pug: 85.538%\n    2\\. French_bulldog: 0.585%\n    3\\. Brabancon_griffon: 0.543%\n    4\\. Boston_bull: 0.218%\n    5\\. bull_mastiff: 0.125%\n    ```", "```\n    _, label, _ = prediction_matrix[0][0]\n    plt.figure()\n    plt.title(f'Label: {label}.')\n    original = load_img('dog.jpg')\n    original = img_to_array(original)\n    plt.imshow(original / 255.0)\n    plt.show()\n    ```", "```\n$> pip install tensorflow-hub Pillow\n```", "```\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import tensorflow_hub as hub\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.preprocessing.image import *\n    from tensorflow.keras.utils import get_file\n    ```", "```\n    classifier_url = ('https://tfhub.dev/google/imagenet/'\n                      'resnet_v2_152/classification/4')\n    ```", "```\n    model = Sequential([\n        hub.KerasLayer(classifier_url, input_shape=(224, \n                                                  224, 3))])\n    ```", "```\n    image = load_img('beetle.jpg', target_size=(224, 224))\n    image = img_to_array(image)\n    image = image / 255.0\n    image = np.expand_dims(image, axis=0)\n    ```", "```\n    predictions = model.predict(image)\n    ```", "```\n    predicted_index = np.argmax(predictions[0], axis=-1)\n    ```", "```\n    file_name = 'ImageNetLabels.txt'\n    file_url = ('https://storage.googleapis.com/'\n        'download.tensorflow.org/data/ImageNetLabels.txt')\n             labels_path = get_file(file_name, file_url)\n    ```", "```\n    with open(labels_path) as f:\n        imagenet_labels = np.array(f.read().splitlines())\n    ```", "```\n    predicted_class = imagenet_labels[predicted_index]\n    ```", "```\n    plt.figure()\n    plt.title(f'Label: {predicted_class}.')\n    original = load_img('beetle.jpg')\n    original = img_to_array(original)\n    plt.imshow(original / 255.0)\n    plt.show()\n    ```", "```\n$> pip install Pillow git+https://github.com/tensorflow/docs\n```", "```\n    import os\n    import pathlib\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import tensorflow_docs as tfdocs\n    import tensorflow_docs.plots\n    from glob import glob\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelBinarizer\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.preprocessing.image import *\n    ```", "```\n    def load_images_and_labels(image_paths, target_size=(64, 64)):\n        images = []\n        labels = []\n        for image_path in image_paths:\n            image = load_img(image_path, \n                             target_size=target_size)\n            image = img_to_array(image)\n            label = image_path.split(os.path.sep)[-2]\n            images.append(image)\n            labels.append(label)\n        return np.array(images), np.array(labels)\n    ```", "```\n    def build_network(width, height, depth, classes):\n        input_layer = Input(shape=(width, height, depth))\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(input_layer)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n        x = Flatten()(x)\n        x = Dense(units=512)(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.25)(x)\n        x = Dense(units=classes)(x)\n        output = Softmax()(x)\n        return Model(input_layer, output)\n    ```", "```\n    def plot_model_history(model_history, metric, \n                           plot_name):\n        plt.style.use('seaborn-darkgrid')\n        plotter = tfdocs.plots.HistoryPlotter()\n        plotter.plot({'Model': model_history}, \n                      metric=metric)\n        plt.title(f'{metric.upper()}')\n        plt.ylim([0, 1])\n        plt.savefig(f'{plot_name}.png')\n        plt.close()\n    ```", "```\n    SEED = 999\n    np.random.seed(SEED)\n    ```", "```\n    base_path = (pathlib.Path.home() / '.keras' / \n                 'datasets' /\n                 '101_ObjectCategories')\n    images_pattern = str(base_path / '*' / '*.jpg')\n    image_paths = [*glob(images_pattern)]\n    image_paths = [p for p in image_paths if\n                   p.split(os.path.sep)[-2] !=\n                  'BACKGROUND_Google']\n    ```", "```\n    classes = {p.split(os.path.sep)[-2] for p in \n              image_paths}\n    ```", "```\n    X, y = load_images_and_labels(image_paths)\n    X = X.astype('float') / 255.0\n    y = LabelBinarizer().fit_transform(y)\n    ```", "```\n    (X_train, X_test,\n     y_train, y_test) = train_test_split(X, y,\n                                         test_size=0.2,\n                                        random_state=SEED)\n    ```", "```\n    EPOCHS = 40\n    BATCH_SIZE = 64\n    model = build_network(64, 64, 3, len(classes))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    history = model.fit(X_train, y_train,\n                        validation_data=(X_test, y_test),\n                        epochs=EPOCHS,\n                        batch_size=BATCH_SIZE)\n    result = model.evaluate(X_test, y_test)\n    print(f'Test accuracy: {result[1]}')\n    plot_model_history(history, 'accuracy', 'normal')\n    ```", "```\n    Test accuracy: 0.61347926\n    ```", "```\n    model = build_network(64, 64, 3, len(classes))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    augmenter = ImageDataGenerator(horizontal_flip=True,\n                                   rotation_range=30,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   fill_mode='nearest')\n    train_generator = augmenter.flow(X_train, y_train, \n                                      BATCH_SIZE)\n    hist = model.fit(train_generator,\n                     steps_per_epoch=len(X_train) // \n                     BATCH_SIZE,\n                     validation_data=(X_test, y_test),\n                     epochs=EPOCHS)\n    result = model.evaluate(X_test, y_test)\n    print(f'Test accuracy: {result[1]}')\n    plot_model_history(hist, 'accuracy', 'augmented')\n    ```", "```\n    Test accuracy: 0.65207374\n    ```", "```\n$> pip install git+https://github.com/tensorflow/docs\n```", "```\n    import os\n    import pathlib\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import tensorflow as tf\n    import tensorflow_docs as tfdocs\n    import tensorflow_docs.plots\n    from glob import glob\n    from sklearn.model_selection import train_test_split\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.models import Model\n    ```", "```\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    ```", "```\n    def build_network(width, height, depth, classes):\n        input_layer = Input(shape=(width, height, depth))\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(input_layer)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=32,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Conv2D(filters=64,\n                   kernel_size=(3, 3),\n                   padding='same')(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(rate=0.25)(x)\n    ```", "```\n        x = Flatten()(x)\n        x = Dense(units=512)(x)\n        x = ReLU()(x)\n        x = BatchNormalization(axis=-1)(x)\n        x = Dropout(rate=0.5)(x)\n        x = Dense(units=classes)(x)\n        output = Softmax()(x)\n        return Model(input_layer, output)\n    ```", "```\n    def plot_model_history(model_history, metric, \n                           plot_name):\n        plt.style.use('seaborn-darkgrid')\n        plotter = tfdocs.plots.HistoryPlotter()\n        plotter.plot({'Model': model_history}, \n                      metric=metric)\n        plt.title(f'{metric.upper()}')\n        plt.ylim([0, 1])\n        plt.savefig(f'{plot_name}.png')\n        plt.close()\n    ```", "```\n    def load_image_and_label(image_path, target_size=(64, \n                                                     64)):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.convert_image_dtype(image, \n                                             np.float32)\n        image = tf.image.resize(image, target_size)\n        label = tf.strings.split(image_path, os.path.sep)[-2]\n        label = (label == CLASSES)  # One-hot encode.\n        label = tf.dtypes.cast(label, tf.float32)\n        return image, label\n    ```", "```\n    def augment(image, label):\n        image = tf.image.resize_with_crop_or_pad(image, \n                                                 74, 74)\n        image = tf.image.random_crop(image, size=(64, 64, 3))\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_brightness(image, 0.2)\n        return image, label\n    ```", "```\n    def prepare_dataset(data_pattern):\n        return (tf.data.Dataset\n                .from_tensor_slices(data_pattern)\n                .map(load_image_and_label,\n                     num_parallel_calls=AUTOTUNE))\n    ```", "```\n    SEED = 999\n    np.random.seed(SEED)\n    ```", "```\n    base_path = (pathlib.Path.home() / '.keras' / \n                 'datasets' /\n                 '101_ObjectCategories')\n    images_pattern = str(base_path / '*' / '*.jpg')\n    image_paths = [*glob(images_pattern)]\n    image_paths = [p for p in image_paths if\n                   p.split(os.path.sep)[-2] !=\n                  'BACKGROUND_Google']\n    ```", "```\n    CLASSES = np.unique([p.split(os.path.sep)[-2]\n                         for p in image_paths])\n    ```", "```\n    train_paths, test_paths = train_test_split(image_paths,\n                                              test_size=0.2,\n                                          random_state=SEED)\n    ```", "```\n    BATCH_SIZE = 64\n    BUFFER_SIZE = 1024\n    train_dataset = (prepare_dataset(train_paths)\n                     .batch(BATCH_SIZE)\n                     .shuffle(buffer_size=BUFFER_SIZE)\n                     .prefetch(buffer_size=BUFFER_SIZE))\n    test_dataset = (prepare_dataset(test_paths)\n                    .batch(BATCH_SIZE)\n                    .prefetch(buffer_size=BUFFER_SIZE))\n    ```", "```\n    EPOCHS = 40\n    model = build_network(64, 64, 3, len(CLASSES))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    history = model.fit(train_dataset,\n                        validation_data=test_dataset,\n                        epochs=EPOCHS)\n    result = model.evaluate(test_dataset)\n    print(f'Test accuracy: {result[1]}')\n    plot_model_history(history, 'accuracy', 'normal')\n    ```", "```\n    Test accuracy: 0.6532258\n    ```", "```\n    train_dataset = (prepare_dataset(train_paths)\n                     .map(augment, \n                         num_parallel_calls=AUTOTUNE)\n                     .batch(BATCH_SIZE)\n                     .shuffle(buffer_size=BUFFER_SIZE)\n                     .prefetch(buffer_size=BUFFER_SIZE))\n    test_dataset = (prepare_dataset(test_paths)\n                    .batch(BATCH_SIZE)\n                    .prefetch(buffer_size=BUFFER_SIZE))\n    ```", "```\n    model = build_network(64, 64, 3, len(CLASSES))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    history = model.fit(train_dataset,\n                        validation_data=test_dataset,\n                        epochs=EPOCHS)\n    result = model.evaluate(test_dataset)\n    print(f'Test accuracy: {result[1]}')\n    plot_model_history(history, 'accuracy', 'augmented')\n    ```", "```\n    Test accuracy: 0.74711984\n    ```"]