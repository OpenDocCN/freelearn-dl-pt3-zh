- en: '2\. Real-World Deep Learning: Predicting the Price of Bitcoin'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will help you to prepare data for a deep learning model, choose
    the right model architecture, use Keras—the default API of TensorFlow 2.0, and
    make predictions with the trained model. By the end of this chapter, you will
    have prepared a model to make predictions which we will explore in the upcoming
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on fundamental concepts from *Chapter 1*, *Introduction to Neural Networks
    and Deep Learning*, let's now move on to a real-world scenario and identify whether
    we can build a deep learning model that predicts Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn the principles of preparing data for a deep learning model, and
    how to choose the right model architecture. We will use Keras—the default API
    of TensorFlow 2.0 and make predictions with the trained model. We will conclude
    this chapter by putting all these components together and building a bare bones,
    yet complete, first version of a deep learning application.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a field that is undergoing intense research activity. Among
    other things, researchers are devoted to inventing new neural network architectures
    that can either tackle new problems or increase the performance of previously
    implemented architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will study both old and new architectures. Older architectures
    have been used to solve a large array of problems and are generally considered
    the right choice when starting a new project. Newer architectures have shown great
    success in specific problems but are harder to generalize. The latter are interesting
    as references of what to explore next but are hardly a good choice when starting
    a project.
  prefs: []
  type: TYPE_NORMAL
- en: The following topic discusses the details of these architectures and how to
    determine the best one for a particular problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Model Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Considering the available architecture possibilities, there are two popular
    architectures that are often used as starting points for several applications:
    **Convolutional Neural Networks** (**CNNs**) and **Recurrent Neural Networks**
    (**RNNs**). These are foundational networks and should be considered starting
    points for most projects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also include descriptions of another three networks, due to their relevance
    in the field: **Long Short-Term Memory** (**LSTM**) networks (an RNN variant);
    **Generative Adversarial Networks** (**GANs**); and **Deep Reinforcement Learning**
    (**DRL**). These latter architectures have shown great success in solving contemporary
    problems, however, they are slightly difficult to use. The next section will cover
    the use of different types of architecture in different problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNNs have gained notoriety for working with problems that have a grid-like structure.
    They were originally created to classify images, but have been used in several
    other areas, ranging from speech recognition to self-driving vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN''s essential insight is to use closely related data as an element of
    the training process, instead of only individual data inputs. This idea is particularly
    effective in the context of images, where a pixel located at the center is related
    to the ones located to its right and left. The name **convolution** is given to
    the mathematical representation of the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Illustration of the convolution process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: Illustration of the convolution process'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: Volodymyr Mnih, *et al.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find this image at: [https://packt.live/3fivWLB](https://packt.live/3fivWLB)'
  prefs: []
  type: TYPE_NORMAL
- en: For more information about deep reinforcement learning, refer to *Human-level
    Control through Deep Reinforcement Learning*. *February 2015*, *Nature*, available
    at [https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A CNN works with a set of inputs that keeps altering the weights and biases
    of the network's respective layers and nodes. A known limitation of this approach
    is that its architecture ignores the sequence of these inputs when determining
    the changes to the network's weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs were created precisely to address that problem. They are designed to work
    with sequential data. This means that at every epoch, layers can be influenced
    by the output of previous layers. The memory of previous observations in each
    sequence plays an important role in the evaluation of posterior observations.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs have had successful applications in speech recognition due to the sequential
    nature of that problem. Also, they are used for translation problems. Google Translate's
    current algorithm—Transformer, uses an RNN to translate text from one language
    to another. In late 2018, Google introduced another algorithm based on the Transformer
    algorithm called **Bidirectional Encoder Representations from Transformers** (**BERT**),
    which is currently state of the art in **Natural Language Processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information RNN applications, refer to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Transformer: A Novel Neural Network Architecture for Language Understanding*,
    *Jakob Uszkoreit*, *Google Research Blog*, *August 2017*, available at [https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '*BERT: Open Sourcing BERT: State-of-the-Art Pre-Training for Natural Language
    Processing*, available at [https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how words in English are linked to words
    in French, based on where they appear in a sentence. RNNs are very popular in
    language translation problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Illustration from distill.pub linking words in English and French'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: Illustration from distill.pub linking words in English and French'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/)'
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory (LSTM) Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LSTM** networks are RNN variants created to address the vanishing gradient
    problem. This problem is caused by memory components that are too distant from
    the current step that receive lower weights due to their distance. LSTMs are a
    variant of RNNs that contain a memory component called a **forget gate**. This
    component can be used to evaluate how both recent and old elements affect the
    weights and biases, depending on where the observation is placed in a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM architecture was first introduced by Sepp Hochreiter and Jürgen Schmidhuber
    in 1997\. Current implementations have had several modifications. For a detailed
    mathematical explanation of how each component of an LSTM works, refer to the
    article *Understanding LSTM Networks*, by Christopher Olah, August 2015, available
    at [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**) were invented in 2014 by Ian
    Goodfellow and his colleagues at the University of Montreal. GANs work based on
    the approach that, instead of having one neural network that optimizes weights
    and biases with the objective to minimize its errors, there should be two neural
    networks that compete against each other for that purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on GANs, refer to *Generative Adversarial Networks*, *Ian
    Goodfellow, et al.*, *arXiv*. *June 10, 2014*, available at [https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs generate new data (*fake* data) and a network that evaluates the likelihood
    of the data generated by the first network being *real* or *fake*. They compete
    because both learn: one learns how to better generate *fake* data, and the other
    learns how to distinguish whether the data presented is real. They iterate on
    every epoch until convergence. That is the point when the network that evaluates
    generated data cannot distinguish between *fake* and *real* data any further.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have been successfully used in fields where data has a clear topological
    structure. Originally, GANs were used to create synthetic images of objects, people''s
    faces, and animals that were similar to real images of those things. You will
    see in the following image, used in the **StarGAN** project, that the expressions
    on the face change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Changes in people''s faces based on emotion, using GAN algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.3: Changes in people''s faces based on emotion, using GAN algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: This domain of image creation is where GANs are used the most frequently, but
    applications in other domains occasionally appear in research papers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: StarGAN project, available at [https://github.com/yunjey/StarGAN](https://github.com/yunjey/StarGAN).'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning (DRL)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original DRL architecture was championed by DeepMind, a Google-owned artificial
    intelligence research organization based in the UK. The key idea of DRL networks
    is that they are unsupervised in nature and that they learn from trial and error,
    only optimizing for a reward function.
  prefs: []
  type: TYPE_NORMAL
- en: That is, unlike other networks (which use a supervised approach to optimize
    incorrect predictions as compared to what are known to be correct ones), DRL networks
    do not know of a correct way of approaching a problem. They are simply given the
    rules of a system and are then rewarded every time they perform a function correctly.
    This process, which takes a very large number of iterations, eventually trains
    networks to excel in several tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about DRL, refer to *Human-Level Control through Deep
    Reinforcement Learning*, *Volodymyr Mnih et al.*, *February 2015*, *Nature*, available
    at: [https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: DRL models gained popularity after DeepMind created AlphaGo—a system that plays
    the game Go better than professional players. DeepMind also created DRL networks
    that learn how to play video games at a superhuman level, entirely on their own.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information about DQN, look up the DQN that was created by DeepMind
    to beat Atari games. The algorithm uses a DRL solution to continuously increase
    its reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [https://keon.io/deep-q-learning/](https://keon.io/deep-q-learning/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a summary of neural network architectures and their applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: Different neural network architectures, data structures, and
    their successful applications'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.4: Different neural network architectures, data structures, and their
    successful applications'
  prefs: []
  type: TYPE_NORMAL
- en: Data Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before building a deep learning model, data normalization is an important step.
    Data normalization is a common practice in machine learning systems. For neural
    networks, researchers have proposed that normalization is an essential technique
    for training RNNs (and LSTMs), mainly because it decreases the network's training
    time and increases the network's overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, refer to *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*, *Sergey Ioffe et al.*, *arXiv*,
    March 2015, available at [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Which normalization technique works best depends on the data and the problem
    at hand. A few commonly used techniques are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Z-Score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When data is normally distributed (that is, Gaussian), you can compute the
    distance between each observation as a standard deviation from its mean. This
    normalization is useful when identifying how distant the data points are from
    more likely occurrences in the distribution. The Z-score is defined by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Formula for Z-Score'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.5: Formula for Z-Score'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *x*i is the *i*th observation, ![1](img/B15911_02_Formula_01.png) is the
    mean, and ![2](img/B15911_02_Formula_02.png) is the standard deviation of the
    series.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, refer to the standard score article (*Z-Score: Definition,
    Formula, and Calculation*), available at [https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/).'
  prefs: []
  type: TYPE_NORMAL
- en: Point-Relative Normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This normalization computes the difference in a given observation in relation
    to the first observation of the series. This kind of normalization is useful for
    identifying trends in relation to a starting point. The point-relative normalization
    is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: Formula for point-relative normalization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.6: Formula for point-relative normalization'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *o*i is the *i*th observation, and *o*o is the first observation of the
    series.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on making predictions, watch *How to Predict Stock Prices
    Easily – Intro to Deep Learning #7*, *Siraj Raval*, available on YouTube at [https://www.youtube.com/watch?v=ftMq5ps503w](https://www.youtube.com/watch?v=ftMq5ps503w).'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum and Minimum Normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This type of normalization computes the distance between a given observation
    and the maximum and minimum values of the series. This is useful when working
    with series in which the maximum and minimum values are not outliers and are important
    for future predictions. This normalization technique can be applied with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7: Formula for calculating normalization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.7: Formula for calculating normalization'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *O*i is the *i*th observation, *O* represents a vector with all *O* values,
    and the functions *min (O)* and *max (O)* represent the minimum and maximum values
    of the series, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: During *Exercise* *2.01*, *Exploring the Bitcoin Dataset and Preparing Data
    for a Model*, we will prepare available Bitcoin data to be used in our LSTM model.
    That includes selecting variables of interest, selecting a relevant period, and
    applying the preceding point-relative normalization technique.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring Your Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Compared to researchers, practitioners spend much less time determining which
    architecture to choose when starting a new deep learning project. Acquiring data
    that represents a given problem correctly is the most important factor to consider
    when developing these systems, followed by an understanding of the dataset''s
    inherent biases and limitations. When starting to develop a deep learning system,
    consider the following questions for reflection:'
  prefs: []
  type: TYPE_NORMAL
- en: Do I have the right data? This is the hardest challenge when training a deep
    learning model. First, define your problem with mathematical rules. Use precise
    definitions and organize the problem into either categories (classification problems)
    or a continuous scale (regression problems). Now, how can you collect data pertaining
    to those metrics?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do I have enough data? Typically, deep learning algorithms have shown to perform
    much better on large datasets than on smaller ones. Knowing how much data is necessary
    to train a high-performance algorithm depends on the kind of problem you are trying
    to address, but aim to collect as much data as you can.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I use a pre-trained model? If you are working on a problem that is a subset
    of a more general application, but within the same domain. Consider using a pre-trained
    model. Pre-trained models can give you a head start on tackling the specific patterns
    of your problem, instead of the more general characteristics of the domain at
    large. A good place to start is the official TensorFlow repository ([https://github.com/tensorflow/models](https://github.com/tensorflow/models)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When you structure your problem with such questions, you will have a sequential
    approach to any new deep learning project. The following is a representative flow
    chart of these questions and tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: Decision tree of key reflection questions to be asked at the
    beginning of a deep learning project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8: Decision tree of key reflection questions to be asked at the beginning
    of a deep learning project'
  prefs: []
  type: TYPE_NORMAL
- en: In certain circumstances, the data may simply not be available. Depending on
    the case, it may be possible to use a series of techniques to effectively create
    more data from your input data. This process is known as **data augmentation**
    and can be applied successfully when working with image recognition problems.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A good reference is the article *Classifying plankton with deep neural networks*,
    available at [https://benanne.github.io/2015/03/17/plankton.html](https://benanne.github.io/2015/03/17/plankton.html).
    The authors show a series of techniques for augmenting a small set of image data
    in order to increase the number of training samples the model has.
  prefs: []
  type: TYPE_NORMAL
- en: Once the problem is well-structured, you will be able to start preparing the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using Jupyter Notebook to code in this section. Jupyter Notebooks
    provide Python sessions via a web browser that allows you to work with data interactively.
    They are a popular tool for exploring datasets. They will be used in exercises
    throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.01: Exploring the Bitcoin Dataset and Preparing Data for a Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will prepare the data and then pass it to the model. The
    prepared data will then be useful in making predictions as we move ahead in the
    chapter. Before preparing the data, we will do some visual analysis on it, such
    as looking at when the value of Bitcoin was at its highest and when the decline
    started.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will be using a public dataset originally retrieved from the Yahoo Finance
    website ([https://finance.yahoo.com/quote/BTC-USD/history/](https://finance.yahoo.com/quote/BTC-USD/history/)).
    The dataset has been slightly modified, as it has been provided alongside this
    chapter, and will be used throughout the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be downloaded from: [https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your Terminal, navigate to the `Chapter02/Exercise2.01` directory. Activate
    the environment created in the previous chapter and execute the following command
    to start a Jupyter Notebook instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should automatically open the Jupyter lab server in your browser. From
    there you can start a Jupyter Notebook.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You should see the following output or similar:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.9: Terminal image after starting a Jupyter lab instance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.9: Terminal image after starting a Jupyter lab instance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select the `Exercise2.01_Exploring_Bitcoin_Dataset.ipynb` file. This is a Jupyter
    Notebook file that will open in a new browser tab. The application will automatically
    start a new Python interactive session for you:![Figure 2.10: Landing page of
    your Jupyter Notebook instance'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_02_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.10: Landing page of your Jupyter Notebook instance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click the Jupyter Notebook file:![Figure 2.11: Image of Jupyter Notebook'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_02_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.11: Image of Jupyter Notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Opening our Jupyter Notebook, consider the Bitcoin data made available with
    this chapter. The dataset `data/bitcoin_historical_prices.csv` ([https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r))
    contains the details of Bitcoin prices since early 2013\. It contains eight variables,
    two of which (`date` and `week`) describe a time period of the data. These can
    be used as indices—and six others (`open`, `high`, `low`, `close`, `volume`, and
    `market_capitalization`) can be used to understand changes in the price and value
    of Bitcoin over time:![Figure 2.12: Available variables (that is, columns) in
    the Bitcoin historical prices dataset'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_02_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.12: Available variables (that is, columns) in the Bitcoin historical
    prices dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the open Jupyter Notebook instance, consider the time series of two of
    those variables: `close` and `volume`. Start with those time series to explore
    price fluctuation patterns, that is, how the price of Bitcoin varied at different
    times in the historical data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the open instance of the Jupyter Notebook, `Exercise2.01_Exploring_Bitcoin_Dataset.ipynb`.
    Now, execute all cells under the `Introduction` header. This will import the required
    libraries and import the dataset into memory:![Figure 2.13: Output from the first
    cells of the notebook time-series plot'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: of the closing price for Bitcoin from the close variable](img/B15911_02_13.jpg)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.13: Output from the first cells of the notebook time-series plot of
    the closing price for Bitcoin from the close variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After the dataset has been imported into memory, move to the `Exploration`
    section. You will find a snippet of code that generates a time series plot for
    the `close` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output looks like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.14: Time series plot of the closing price for Bitcoin from the close
    variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.14: Time series plot of the closing price for Bitcoin from the close
    variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Reproduce this plot but using the `volume` variable in a new cell below this
    one. You will have most certainly noticed that price variables surge in 2017 and
    then the downfall starts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.15: The total daily volume of Bitcoin coins'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.15: The total daily volume of Bitcoin coins'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 2.15* shows that since 2017, Bitcoin transactions have significantly
    increased in the market. The total daily volume varies much more than daily closing
    prices.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Execute the remaining cells in the Exploration section to explore the range
    from 2017 to 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fluctuations in Bitcoin prices have been increasingly commonplace in recent
    years. While those periods could be used by a neural network to understand certain
    patterns, we will be excluding older observations, given that we are interested
    in predicting future prices for not-too-distant periods. Filter the data after
    2016 only. Navigate to the `Preparing Dataset for a Model` section. Use the pandas
    API to filter the data. Pandas provides an intuitive API for performing this operation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract recent data and save it into a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `bitcoin_recent` variable now has a copy of our original Bitcoin dataset,
    but filtered to the observations that are newer or equal to January 4, 2016.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Normalize the data using the point-relative normalization technique described
    in the *Data Normalization* section in the Jupyter Notebook. You will only normalize
    two variables—`close` and `volume`—because those are the variables that we are
    working to predict.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the next cell in the notebook to ensure that we only keep the close and
    volume variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the same directory containing this chapter, we have placed a script called
    `normalizations.py`. That script contains the three normalization techniques described
    in this chapter. We import that script into our Jupyter Notebook and apply the
    functions to our series.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Navigate to the `Preparing Dataset for a Model` section. Now, use the `iso_week`
    variable to group daily observations from a given week using the pandas `groupby()`
    method. We can now apply the normalization function, `normalizations.point_relative_normalization()`,
    directly to the series within that week. We can store the normalization output
    as a new variable in the same pandas DataFrame using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `close_point_relative_normalization` variable now contains the normalized
    data for the `close` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.16: Image of the Jupyter Notebook focusing on the section where
    the normalization function is applied'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.16: Image of the Jupyter Notebook focusing on the section where the
    normalization function is applied'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Do the same with the `volume` variable (`volume_point_relative_normalization`).
    The normalized `close` variable contains an interesting variance pattern every
    week. We will be using that variable to train our LSTM model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Your output should be as follows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.17: Plot that displays the series from the normalized variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.17: Plot that displays the series from the normalized variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to evaluate how well the model performs, you need to test its accuracy
    versus some other data. Do this by creating two datasets: a training set and a
    test set. You will use 90 percent of the dataset to train the LSTM model and 10
    percent to evaluate its performance. Given that the data is continuous and in
    the form of a time series, use the last 10 percent of available weeks as a test
    set and the first 90 percent as a training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.18: Output of the test set weeks'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.18: Output of the test set weeks'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.19: Using weeks to create a training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.19: Using weeks to create a training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the separate datasets for each operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, navigate to the `Storing Output` section and save the filtered variable
    to disk, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3ehbgCi](https://packt.live/3ehbgCi).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2ZdGq9s](https://packt.live/2ZdGq9s).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we explored the Bitcoin dataset and prepared it for a deep
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that in 2017, the price of Bitcoin skyrocketed. This phenomenon took
    a long time to take place and may have been influenced by a number of external
    factors that this data alone doesn't explain (for instance, the emergence of other
    cryptocurrencies). After the great surge of 2017, we saw a great fall in the value
    of Bitcoin in 2018.
  prefs: []
  type: TYPE_NORMAL
- en: We also used the point-relative normalization technique to process the Bitcoin
    dataset in weekly chunks. We do this to train an LSTM network to learn the weekly
    patterns of Bitcoin price changes so that it can predict a full week into the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: However, Bitcoin statistics show significant fluctuations on a weekly basis.
    Can we predict the price of Bitcoin in the future? What will the price be seven
    days from now? We will build a deep learning model to explore these questions
    in our next section using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Using Keras as a TensorFlow Interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are using Keras because it simplifies the TensorFlow interface into general
    abstractions and, in TensorFlow 2.0, this is the default API in this version.
    In the backend, the computations are still performed in TensorFlow, but we spend
    less time worrying about individual components, such as variables and operations,
    and spend more time building the network as a computational unit. Keras makes
    it easy to experiment with different architectures and hyperparameters, moving
    more quickly toward a performant solution.
  prefs: []
  type: TYPE_NORMAL
- en: As of TensorFlow 2.0.0, Keras is now officially distributed with TensorFlow
    as `tf.keras`. This suggests that Keras is now tightly integrated with TensorFlow
    and will likely continue to be developed as an open source tool for a long period
    of time. Components are an integral part when building models. Let's deep dive
    into this concept now.
  prefs: []
  type: TYPE_NORMAL
- en: Model Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in *Chapter 1*, *Introduction to Neural Networks and Deep Learning*,
    LSTM networks also have input, hidden, and output layers. Each hidden layer has
    an activation function that evaluates that layer's associated weights and biases.
    As expected, the network moves data sequentially from one layer to another and
    evaluates the results by the output at every iteration (that is, an epoch).
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras provides intuitive classes that represent each one of the components
    listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.20: Description of key components from the Keras API'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.20: Description of key components from the Keras API'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using these components to build a deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Keras' `keras.models.Sequential()` component represents a whole sequential neural
    network. This Python class can be instantiated on its own and have other components
    added to it subsequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are interested in building an LSTM network because those networks perform
    well with sequential data—and a time series is a kind of sequential data. Using
    Keras, the complete LSTM network would be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation will be further optimized in *Chapter 3*, *Real-World Deep
    Learning with TensorFlow and Keras: Evaluating the Bitcoin Model*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras abstraction allows you to focus on the key elements that make a deep
    learning system more performant: determining the right sequence of components,
    how many layers and nodes to include, and which activation function to use. All
    of these choices are determined by either the order in which components are added
    to the instantiated `keras.models``.Sequential()` class or by parameters passed
    to each component instantiation (that is, `Activation("linear")`). The final `model.compile()`
    step builds the neural network using TensorFlow components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the network is built, we train our network using the `model.fit()` method.
    This will yield a trained model that can be used to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `X_train` and `Y_train` variables are, respectively, a set used for training
    and a smaller set used for evaluating the loss function (that is, testing how
    well the network predicts data). Finally, we can make predictions using the `model.predict()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding steps cover the Keras paradigm for working with neural networks.
    Despite the fact that different architectures can be dealt with in very different
    ways, Keras simplifies the interface for working with different architectures
    by using three components – `Network Architecture`, `Fit`, and `Predict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21: The Keras neural network paradigm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.21: The Keras neural network paradigm'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Keras neural network diagram comprises the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A neural network architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network (or **Fit**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras allows much greater control within each of these steps. However, its focus
    is to make it as easy as possible for users to create neural networks in as little
    time as possible. That means that we can start with a simple model, and then add
    complexity to each one of the preceding steps to make that initial model perform
    better.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take advantage of that paradigm during our upcoming exercise and chapters.
    In the next exercise, we will create the simplest LSTM network possible. Then,
    in *Chapter 3*, *Real-World Deep Learning: Evaluating the Bitcoin Model*, we will
    continuously evaluate and alter that network to make it more robust and performant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.02: Creating a TensorFlow Model Using Keras'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this notebook, we design and compile a deep learning model using Keras as
    an interface to TensorFlow. We will continue to modify this model in our next
    chapters and exercises by experimenting with different optimization techniques.
    However, the essential components of the model are designed entirely in this notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook and import the following libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our dataset contains daily observations and each observation influences a future
    observation. Also, we are interested in predicting a week—that is, 7 days—of Bitcoin
    prices in the future:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have calculated `number_of_observations` based on available weeks in our
    dataset. Given that we will be using last week to test the LSTM network on every
    epoch, we will use 208 – 21 – 1\. You''ll get:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the LSTM model using Keras. We have the batch size as one because we
    are passing the whole data in a single iteration. If data is big, then we can
    pass the data with multiple batches, That''s why we used batch_input_shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should return a compiled Keras model that can be trained and stored in
    disk.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s store the model on the output of the model to a disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the `bitcoin_lstm_v0.h5` model hasn''t been trained yet. When saving
    a model without prior training, you effectively only save the architecture of
    the model. That same model can later be loaded by using Keras'' `load_model()`
    function, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/38KQI3Y](https://packt.live/38KQI3Y).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3fhEL89](https://packt.live/3fhEL89).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This concludes the creation of our Keras model, which we can now use to make
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You may encounter the following warning when loading the Keras library:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Using TensorFlow backend`'
  prefs: []
  type: TYPE_NORMAL
- en: Keras can be configured to use another backend instead of TensorFlow (that is,
    Theano). In order to avoid this message, you can create a file called `keras.json`
    and configure its backend there. The correct configuration of that file depends
    on your system. Hence, it is recommended that you visit Keras' official documentation
    on the topic at [https://keras.io/backend/](https://keras.io/backend/).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to build a deep learning model using Keras—an
    interface for TensorFlow. We studied core components of Keras and used those components
    to build the first version of our Bitcoin price-predicting system based on an
    LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we will discuss how to put all the components from this
    chapter together into a (nearly complete) deep learning system. That system will
    yield our very first predictions, serving as a starting point for future improvements.
  prefs: []
  type: TYPE_NORMAL
- en: From Data Preparation to Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section focuses on the implementation aspects of a deep learning system.
    We will use the Bitcoin data from the *Choosing the Right Model Architecture*
    section, and the Keras knowledge from the preceding section, *Using Keras as a
    TensorFlow Interface*, to put both of these components together. This section
    concludes the chapter by building a system that reads data from a disk and feeds
    it into a model as a single piece of software.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural networks can take long periods of time to train. Many factors affect
    how long that process may take. Among them, three factors are commonly considered
    the most important:'
  prefs: []
  type: TYPE_NORMAL
- en: The network's architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many layers and neurons the network has
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much data there is to be used in the training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other factors may also greatly impact how long a network takes to train, but
    most of the optimization that a neural network can have when addressing a business
    problem comes from exploring those three.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the normalized data from our previous section. Recall that
    we have stored the training data in a file called `train_dataset.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the training data by visiting this link: [https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will load that dataset into memory using the `pandas` library for easy exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you change the path (highlighted) based on where you have downloaded
    or saved the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the output in a tabular form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.22: Table showing the first five rows of the training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.22: Table showing the first five rows of the training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the series from the `close_point_relative_normalization` variable,
    which is a normalized series of the Bitcoin closing prices—from the `close` variable—since
    the beginning of 2016.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `close_point_relative_normalization` variable has been normalized on a
    weekly basis. Each observation from the week''s period is made relative to the
    difference from the closing prices on the first day of the period. This normalization
    step is important and will help our network train faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.23: Plot that displays the series from the normalized variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.23: Plot that displays the series from the normalized variable.'
  prefs: []
  type: TYPE_NORMAL
- en: This variable will be used to train our LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping Time Series Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks typically work with vectors and tensors—both mathematical objects
    that organize data in a number of dimensions. Each neural network implemented
    in Keras will have either a vector or a tensor that is organized according to
    a specification as input.
  prefs: []
  type: TYPE_NORMAL
- en: At first, understanding how to reshape the data into the format expected by
    a given layer can be confusing. To avoid confusion, it is advisable to start with
    a network with as few components as possible, and then add components gradually.
    Keras' official documentation (under the `Layers` section) is essential for learning
    about the requirements for each kind of layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The Keras official documentation is available at [https://keras.io/layers/core/](https://keras.io/layers/core/).
    That link takes you directly to the `Layers` section.
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy** is a popular Python library used for performing numerical computations.
    It is used by the deep learning community to manipulate vectors and tensors and
    prepare them for deep learning systems.'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the `numpy.reshape()` method is very important when adapting
    data for deep learning models. That model allows for the manipulation of NumPy
    arrays, which are Python objects analogous to vectors and tensors.
  prefs: []
  type: TYPE_NORMAL
- en: We'll now organize the prices from the `close_point_relative_normalization`
    variable using the weeks after 2016\. We will create distinct groups containing
    7 observations each (one for each day of the week) for a total of 208 complete
    weeks. We do that because we are interested in predicting the prices of a week's
    worth of trading.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We use the ISO standard to determine the beginning and the end of a week. Other
    kinds of organizations are entirely possible. This one is simple and intuitive
    to follow, but there is room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM networks work with three-dimensional tensors. Each one of those dimensions
    represents an important property for the network. These dimensions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Period length**: The period length, that is, how many observations there
    are for a period'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of periods**: How many periods are available in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of features**: The number of features available in the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our data from the `close_point_relative_normalization` variable is currently
    a one-dimensional vector. We need to reshape it to match those three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using a period of a week. So, our period length is 7 days (period
    length = 7). We have 208 complete weeks available in our data. We will be using
    the very last of those weeks to test our model against during its training period.
    That leaves us with 187 distinct weeks. Finally, we will be using a single feature
    in this network (number of features = 1); we will include more features in future
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reshape the data to match those dimensions, we will be using a combination
    of base Python properties and the `reshape()` method from the `numpy` library.
    First, we create the 186 distinct week groups with 7 days, each using pure Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This piece of code creates distinct week groups. The resulting variable data
    is a variable that contains all the right dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Each Keras layer will expect its input to be organized in specific ways. However,
    Keras will reshape data accordingly, in most cases. Always refer to the Keras
    documentation on layers ([https://keras.io/layers/core/](https://keras.io/layers/core/))
    before adding a new layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Keras LSTM layer expects these dimensions to be organized in a specific
    order: the number of features, the number of observations, and the period length.
    Reshape the dataset to match that format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding snippet also selects the very last week of our set as a validation
    set (via `data[-1]`). We will be attempting to predict the very last week in our
    dataset by using the preceding 76 weeks. The next step is to use those variables
    to fit our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'LSTMs are computationally expensive models. They may take up to 5 minutes to
    train with our dataset on a modern computer. Most of that time is spent at the
    beginning of the computation when the algorithm creates the full computation graph.
    The process gains speed after it starts training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.24: Graph that shows the results of the loss function evaluated
    at each epoch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.24: Graph that shows the results of the loss function evaluated at
    each epoch'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This compares what the model predicted at each epoch, and then compares that
    with the real data using a technique called mean-squared error. This plot shows
    those results.
  prefs: []
  type: TYPE_NORMAL
- en: At a glance, our network seems to perform very well; it starts with a very small
    error rate that continuously decreases. Now that we have lowered the error rate,
    let's move on to make some predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Making Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After our network has been trained, we can proceed to make predictions. We will
    be making predictions for a future week beyond our time period.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have trained our model with the `model.fit()` method, making predictions
    is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We use the same data for making predictions as the data used for training (the
    `X_train` variable). If we have more data available, we can use that instead—given
    that we reshape it to the format the LSTM requires.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When a neural network overfits a validation set, this means that it learns
    patterns present in the training set but is unable to generalize it to unseen
    data (for instance, the test set). In the next chapter, we will learn how to avoid
    overfitting and create a system for both evaluating our network and increasing
    its performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.25: Graph showing the weekly performance of Bitcoin'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.25: Graph showing the weekly performance of Bitcoin'
  prefs: []
  type: TYPE_NORMAL
- en: In the plot shown above, the horizontal axis represents the week number and
    the vertical axis represents the predicted performance of Bitcoin. Now that we
    have explored the data, prepared a model, and learned how to make predictions,
    let's put this knowledge into practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2.01: Assembling a Deep Learning System'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we''ll bring together all the essential pieces for building
    a basic deep learning system – the data, a model, and predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: Start a Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the training dataset into memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspect the training set to see whether it is in the form period length, number
    of periods, or number of features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the training set if it is not in the required format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the previously trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model using your training dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a prediction on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Denormalize the values and save the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The final output will look as follows with the horizontal axis representing
    the number of days and the vertical axis represents the price of Bitcoin:'
  prefs: []
  type: TYPE_NORMAL
- en: '6'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.26: Expected output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_02_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.26: Expected output'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 136.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have assembled a complete deep learning system, from data
    to prediction. The model created in this activity requires a number of improvements
    before it can be considered useful. However, it serves as a great starting point
    from which we will continuously improve.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will explore techniques for measuring the performance of our
    model and will continue to make modifications until we reach a model that is both
    useful and robust.
  prefs: []
  type: TYPE_NORMAL
