<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Migrating from TensorFlow 1 to TensorFlow 2</h1>
                </header>
            
            <article>
                
<p>Since TensorFlow 2 has only been released very recently, most of the projects that are available online are still built for TensorFlow 1. While this first version was already packed with useful features, such as AutoGraph and the Keras API, it is recommended that you migrate to the latest version of TensorFlow so as to avoid any technical debt. Thankfully, TensorFlow 2 comes with an automatic migration tool that is able to convert most projects to its latest version. It requires little effort and outputs functional code. However, migrating to idiomatic TensorFlow 2 code requires some diligence and knowledge of both versions. In this section, we will introduce the migration tool and compare TensorFlow 1 concepts with their TensorFlow 2 counterparts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automatic migration</h1>
                </header>
            
            <article>
                
<p>After installing TensorFlow 2, the migration tool is available from the command line. To convert a project directory, run the following command:</p>
<pre><strong>$ tf_upgrade_v2 --intree ./project_directory --outtree ./project_directory_updated</strong></pre>
<p>Here is a sample of the command's logs on an example project:</p>
<pre class="mce-root"><strong>INFO line 1111:10: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'</strong><br/><strong> INFO line 1112:10: Renamed 'tf.layers.dense' to 'tf.compat.v1.layers.dense'</strong><br/><strong> TensorFlow 2.0 Upgrade Script</strong><br/><strong> -----------------------------</strong><br/><strong> Converted 21 files</strong><br/><strong> Detected 1 issues that require attention</strong><br/><strong> ----------------------------------------------------------------------</strong><br/><strong> ----------------------------------------------------------------------</strong><br/><strong> File: project_directory/test_tf_converter.py</strong><br/><strong> ----------------------------------------------------------------------</strong><br/><strong> project_directory/test_tf_converter.py:806:10: WARNING: tf.image.resize_bilinear called with align_corners argument requires manual check: align_corners is not supported by tf.image.resize, the new default transformation is close to what v1 provided. If you require exactly the same transformation as before, use compat.v1.image.resize_bilinear.</strong><br/> <strong> Make sure to read the detailed log 'report.txt'</strong></pre>
<p>The conversion tool details all the changes it made to the files. In the rare case when it detects a code line that requires manual attention, it outputs a warning with instructions to <span>update</span>.</p>
<p>Most of the outdated calls are moved to <kbd>tf.compat.v1</kbd>. Indeed, despite the deprecation of many concepts, TensorFlow 2 still provides access to the old API through this module. However, be aware that calls to <kbd>tf.contrib</kbd> will cause the conversion tool to fail and generate an error:</p>
<pre class="mce-root"><strong>ERROR: Using member tf.contrib.copy_graph.copy_op_to_graph in deprecated module tf.contrib. tf.contrib.copy_graph.copy_op_to_graph cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository, or fork the required code.</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Migrating TensorFlow 1 code</h1>
                </header>
            
            <article>
                
<p>If the tool runs without any error, the code can be used as is. However, the <kbd>tf.compat.v1</kbd> module used by the migration tool is deemed to be deprecated. Calling this module already outputs deprecation warnings, and its content will not be further updated by the community. For this reason, it is recommended that you refactor the code in order to make it more idiomatic. In the following sections, we will introduce TensorFlow 1 concepts and explain how to migrate them to TensorFlow 2. In the following examples, <kbd>tf1</kbd> will be used instead of <kbd>tf</kbd> to denote the use of TensorFlow 1.13.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sessions</h1>
                </header>
            
            <article>
                
<p>Since TensorFlow 1 does not use eager execution by default, the results of the operations are not directly available. For instance, when summing two constants, the output object is an operation:</p>
<pre>import tensorflow as tf1 # TensorFlow 1.13<br/><br/>a = tf1.constant([1,2,3])<br/>b = tf1.constant([1,2,3])<br/>c = a + b<br/>print(c) # Prints &lt;tf.Tensor 'add:0' shape=(3,) dtype=int32</pre>
<p>To compute a result, you need to manually create <kbd>tf1.Session</kbd>. A session takes care of the following:</p>
<ul>
<li>Managing the memory</li>
<li>Running operations on CPU or GPU</li>
<li>Running on several machines if necessary</li>
</ul>
<p>The most common way of using a session is through the <kbd>with</kbd> statement in Python. As with other unmanaged resources, the <kbd>with</kbd> statement guarantees that the session is properly closed after we use it. If the session is not closed, it may keep using memory. Sessions in TensorFlow 1 are, therefore, typically instantiated and used as follows:</p>
<pre><span class="k">with</span> <span class="n">tf1</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
 <span class="n">result</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">c</span><span class="p">)<br/>print(result) # Prints array([2, 4, 6], dtype=int32)</span></pre>
<p>You can also explicitly close a session, but it is not recommended:</p>
<pre>sess = tf1.Session()<br/>result = sess.run(c)<br/>sess.close()</pre>
<p>In TensorFlow 2, session management happens behind the scenes. As the new version uses eager execution, there is no need for this superfluous code to compute results. Calls to <kbd>tf1.Session()</kbd> can, therefore, be removed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placeholders</h1>
                </header>
            
            <article>
                
<p>In the previous example, we computed the sum of two vectors. However, we defined the value of those vectors when creating the graph. If we wanted to use variables instead, we could have used <kbd>tf1.placeholder</kbd>:</p>
<pre>a = tf1.placeholder(dtype=tf.int32, shape=(None,)) <br/>b = tf1.placeholder(dtype=tf.int32, shape=(None,))
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p"> + </span><span class="n">b<br/><br/></span>with tf1.Session() as sess:<br/>  result = sess.run(c, feed_dict={<br/>      a: [1, 2, 3],<br/>      b: [1, 1, 1]<br/>    })</pre>
<p>In TensorFlow 1, placeholders are mostly used to provide input data. Their type and shape have to be defined. In our example, the shape is <kbd>(None,)</kbd> because we may want to run the operation on vectors of any size. When running the graph, we have to provide specific values for our placeholders. This is why we use the <kbd>feed_dict</kbd> argument in <kbd>sess.run</kbd>, passing the content of variables as a dictionary, with the placeholders as keys. Failing to provide a value for all placeholders would cause an exception.</p>
<p>Before TensorFlow 2, placeholders were used to provide input data, as well as layers' parameters. The former use case can be replaced with <kbd>tf.keras.Input</kbd>, while the latter can be addressed using <kbd>tf.keras.layers.Layer</kbd> parameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variable management</h1>
                </header>
            
            <article>
                
<p>In TensorFlow 1, variables were created globally. Each variable had a unique name and the best practice in terms of creating them was to use <kbd>tf1.get_variable()</kbd>:</p>
<pre>weights = tf1.get_variable(name='W', initializer=[3])</pre>
<p>Here, we created a global variable named <kbd>W</kbd>. Deleting the Python <kbd>weights</kbd> variable (using the Python <kbd>del weights</kbd> command, for instance) would have no effect on TensorFlow memory. In fact, if we try to create the same variable again, we would end up with an error:</p>
<pre>Variable W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?</pre>
<p>While <kbd>tf1.get_variable()</kbd> allows you to reuse variables, its default behavior is to throw an error if a variable with the chosen name already exists, preventing you from mistakenly overriding variables. To avoid this error, we can update our call to <kbd>tf1.variable_scope(...)</kbd> and employ the <kbd>reuse</kbd> argument:</p>
<pre>with tf1.variable_scope("conv1", reuse=True):<br/>    weights = tf1.get_variable(name='W', initializer=[3])</pre>
<div class="packt_infobox">The <kbd>variable_scope</kbd> context manager was used to manage variable creation. On top of handling variable reuse, it was useful to group variables together by appending a prefix to their name. In the previous example, the variable would be named <kbd>conv1/W</kbd>.</div>
<p>In this case, setting reuse to <kbd>True</kbd> means that if TensorFlow encounters a variable called <kbd>conv1/W</kbd>, it will not throw an error as it did before. Instead, it will reuse the existing variable, including its content. However, if you try calling the preceding code and the variable named <kbd>conv1/W</kbd> does not exist, you will encounter the following error:</p>
<pre>Variable conv1/W does not exist</pre>
<p>Indeed, <kbd>reuse=True</kbd> can only be specified when reusing an existing variable. If you want to create a variable if it does not exist, and reuse it when it does exist, you can pass <kbd>reuse=tf.AUTO_REUSE</kbd>.</p>
<p>In TensorFlow 2, the behavior is different. While variable scope still exists to make naming and debugging easier, variables are no longer global. They are handled at the Python level. As long as you can access the Python reference (the <kbd>weights</kbd> variable, in our example), you can modify the variable. To delete the variable, you need to delete its reference, for instance, by running the following command:</p>
<pre>del weights</pre>
<p>Previously, variables could be accessed and modified globally, and could potentially be overridden by other pieces of code. The deprecation of global variables makes TensorFlow code more readable and less prone to errors. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Layers and models</h1>
                </header>
            
            <article>
                
<p>TensorFlow models were originally defined using <kbd>tf1.layers</kbd>. As this module has been deprecated in TensorFlow 2, the replacement of choice is <kbd>tf.keras.layers</kbd>. To train a model using TensorFlow 1, a <em>train operation</em> has to be defined using an optimizer and a loss. For instance, if <kbd>y</kbd> is the output of a fully connected layer, we would define the training operation using the following command:</p>
<pre>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=output, logits=y))<br/>train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)<br/><br/></pre>
<p>Every time we call this operation, a batch of images will be fed to the network and a single step of backpropagation will happen. We then run a loop to compute multiple training steps:</p>
<pre>num_steps = 10**7<br/><br/>with tf1.Session() as sess:<br/>    sess.run(tf1.global_variables_initializer())<br/><br/>    for i in range(num_steps):<br/>        batch_x, batch_y = next(batch_generator)<br/>        sess.run(train_step, feed_dict={x: batch_x, y: batch_y})</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>When opening the session, a call to <kbd>tf1.global_variables_initializer()</kbd> is necessary so that layers are initialized with the correct weights. A failure to do so would throw an exception. In TensorFlow 2, the initialization of variables is handled automatically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other concepts</h1>
                </header>
            
            <article>
                
<p>We detailed the most common TensorFlow 1 concepts that were deprecated in the new version. Many smaller modules and paradigms were also redesigned in TensorFlow 2. When migrating a project, we recommend having a thorough look at the documentation of both versions. To ensure that a migration went well and the TensorFlow 2 version works as expected, we recommend that you log both inference metrics (such as latency, accuracy, or average precision) and training metrics (such as the number of iterations before convergence), and compare their values between the old and new versions.</p>
<p>As it is open source and backed by an active community, TensorFlow is constantly evolving—integrating new features, optimizing others, improving the developer experience, and more. While this may sometimes require some additional effort, upgrading to the latest version as soon as possible will provide you with the best environment to develop more performant recognition applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>This section lists the scientific papers and other web resources mentioned in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 1: Computer Vision and Neural Networks</h1>
                </header>
            
            <article>
                
<ul>
<li>Angeli, A., Filliat, D., Doncieux, S., Meyer, J.-A., 2008. <em>A Fast and Incremental Method for Loop-Closure Detection Using Bags of Visual Words</em>. <em>IEEE Transactions on Robotics 1027–1037</em>.</li>
<li>Bradski, G., Kaehler, A., 2000. OpenCV. <em>Dr. Dobb’s Journal of Software Tools 3</em>.</li>
<li>Cortes, C., Vapnik, V., 1995. <em>Support-Vector Networks</em>. <em>Machine Learning 20, 273–297</em>.</li>
<li>Drucker, H., Burges, C.J., Kaufman, L., Smola, A.J., Vapnik, V., 1997. <em>Support Vector Regression Machines. In: Advances in Neural Information Processing Systems, pp. 155–161</em>.</li>
<li>Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. <em>ImageNet Classification with Deep Convolutional Neural Networks</em>. <em>In: Advances in Neural Information Processing Systems, pp. 1097–1105</em>.</li>
<li>Lawrence, S., Giles, C.L., Tsoi, A.C., Back, A.D., 1997. <em>Face Recognition: A Convolutional Neural-Network Approach</em>. <em>IEEE Transactions on Neural Networks 8, 98–113</em>.</li>
<li>LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.E., Jackel, L.D., 1990. <em>Handwritten Digit Recognition with a Back-Propagation Network</em>. <em>In: Advances in Neural Information Processing Systems, pp. 396–404</em>.</li>
<li>LeCun, Y., Cortes, C., Burges, C., 2010. <em>MNIST Handwritten Digit Database. AT&amp;T Labs [Online]</em>. Available at <a href="http://yann.lecun.com/exdb/mnist">http://yann.lecun.com/exdb/mnist</a> 2, 18.</li>
<li>Lowe, D.G., 2004. <em>Distinctive Image Features from Scale-Invariant Keypoints</em>. <em>International Journal of Computer Vision 60, 91–110</em>.</li>
<li>Minsky, M., 1961. <em>Steps Toward Artificial Intelligence</em>. <em>Proceedings of the IRE 49, 8–30</em>.</li>
<li>Minsky, M., Papert, S.A., 2017. <em>Perceptrons: An Introduction to Computational Geometry. MIT press</em>.</li>
<li>Moravec, H., 1984. <em>Locomotion, Vision, and Intelligence</em>.</li>
<li>Papert, S.A., 1966. <em>The Summer Vision Project</em>.</li>
<li>Plaut, D.C., et al., 1986. <em>Experiments on Learning by Back Propagation</em>.</li>
<li>Rosenblatt, F., 1958. <em>The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</em>. <em>Psychological Review 65, 386</em>.</li>
<li>Turk, M., Pentland, A., 1991. <em>Eigenfaces for Recognition. Journal of Cognitive Neuroscience 3, 71–86</em>.</li>
<li>Wold, S., Esbensen, K., Geladi, P., 1987. <em>Principal Component Analysis. Chemometrics and Intelligent Laboratory Systems 2, 37–52</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 2: TensorFlow Basics and Training a Model</h1>
                </header>
            
            <article>
                
<ul>
<li class="csl-entry">Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, et al. <em>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems 19</em>.</li>
<li class="csl-entry"><em>API Documentation [WWW Document], n.d. TensorFlow</em>. URL: <a href="https://www.tensorflow.org/api_docs/">https://www.tensorflow.org/api_docs/</a> (accessed December 14, 2018).</li>
<li class="csl-entry">Chollet, F., 2018. TensorFlow is the platform of choice for deep learning in the research community. There are deep learning framework mentions on arXiv over the past three months, <em>pic.twitter.com/v6ZEi63hzP. @fchollet</em>.</li>
<li class="csl-entry">Goldsborough, P., 2016. <em>A Tour of TensorFlow. arXiv:1610.01178 [cs]</em>.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 3: Modern Neural Networks</h1>
                </header>
            
            <article>
                
<ul>
<li>Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al., 2016. <em>Tensorflow: A System for Large-Scale Machine Learning. In: OSDI, pp. 265–283</em>.</li>
<li><em>API Documentation, URL</em>: <a href="https://www.tensorflow.org/api_docs/">https://www.tensorflow.org/api_docs/</a> (accessed <span>December 14, 2018</span>).</li>
<li>Bottou, L., 2010. <em>Large-Scale Machine Learning with Stochastic Gradient Descent. In: Proceedings of COMPSTAT'2010</em>. <em>Springer, pp. 177–186</em>.</li>
<li>Bottou, L., Curtis, F.E., Nocedal, J., 2018. <em>Optimization Methods for Large-Scale Machine Learning. SIAM Review 60, 223–311</em>.</li>
<li>Dozat, T., 2016. <em>Incorporating Nesterov Momentum into Adam</em>.</li>
<li>Duchi, J., Hazan, E., Singer, Y., 2011. <em>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research 12, 2121–2159</em>.</li>
<li>Gardner, W.A., 1984. <em>Learning Characteristics of Stochastic Gradient Descent Algorithms: A General Study, Analysis, and Critique. Signal Processing 6, 113–133</em>.</li>
<li>Girosi, F., Jones, M., Poggio, T., 1995. <em>Regularization Theory and Neural Networks Architectures</em>. <em>Neural Computation 7, 219–269</em>.</li>
<li>Ioffe, S., Szegedy, C., 2015. <em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.</em> <em>arXiv preprint arXiv:1502.03167</em>.</li>
<li>Karpathy, A., n.d. <em>Stanford University CS231n: Convolutional Neural Networks for Visual Recognition [WWW Document]</em>. URL: <a href="http://cs231n.stanford.edu/">http://cs231n.stanford.edu/</a> (accessed <span>December 14, 2018</span>).</li>
<li>Kingma, D.P., Ba, J., 2014. <em>Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980</em>.</li>
<li>Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. <em>ImageNet Classification with Deep Convolutional Neural Networks</em>. <em>In: Advances in Neural Information Processing Systems, pp. 1097–1105</em>.</li>
<li>Lawrence, S., Giles, C.L., Tsoi, A.C., Back, A.D., 1997. <em>Face Recognition: A Convolutional Neural Network Approach</em>. <em>IEEE Transactions on Neural Networks 8, 98–113</em>.</li>
<li>Le and Borji – 2017 – <em>What are the Receptive, Effective Receptive, and P.pdf, n.d</em>.</li>
<li>Le, H., Borji, A., 2017. <em>What are the Receptive, Effective Receptive, and Projective Fields of Neurons in Convolutional Neural Networks? arXiv:1705.07049 [cs]</em>.</li>
<li>LeCun, Y., Cortes, C., Burges, C., 2010. <em>MNIST Handwritten Digit Database. AT&amp;T Labs [Online]</em>. Available at <a href="http://yann.lecun.com/exdb/mnist">http://yann.lecun.com/exdb/mnist</a> 2.</li>
<li>LeCun, Y., et al., 2015. LeNet-5, <em>Convolutional Neural Networks</em>. URL: <a href="http://yann.lecun.com/exdb/lenet">http://yann.lecun.com/exdb/lenet</a> 20.</li>
<li>Lenail, A., <em>n.d. NN SVG [WWW Document]</em>. URL: <a href="http://alexlenail.me/NN-SVG/">http://alexlenail.me/NN-SVG/</a> (accessed <span>December 14, 2018</span>).</li>
<li>Luo, W., Li, Y., Urtasun, R., Zemel, R., n.d. <em>Understanding the Effective Receptive Field in Deep Convolutional Neural Networks 9</em>.</li>
<li>Nesterov, Y., 1998. <em>Introductory Lectures on Convex Programming Volume I: Basic Course. Lecture notes</em>.</li>
<li>Perkins, E.S., Davson, H., n.d. <em>Human Eye | Definition, Structure, &amp; Function [WWW Document]</em>. <em>Encyclopedia Britannica</em>. URL: <a href="https://www.britannica.com/science/human-eye">https://www.britannica.com/science/human-eye</a> (accessed <span>December 14, 2018</span>).</li>
<li>Perone, C.S., n.d. <em>The effective receptive field on CNNs | Terra Incognita. Terra Incognita</em>.</li>
<li>Polyak, B.T., 1964. <em>Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics 4, 1–17</em>.</li>
<li>Raj, D., 2018. <em>A Short Note on Gradient Descent Optimization Algorithms</em>. <em>Medium</em>.</li>
<li>Simard, P.Y., Steinkraus, D., Platt, J.C., 2003. <em>Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis. In: Null, p. 958</em>.</li>
<li>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., 2014. <em>Dropout: A Simple Way to Prevent Neural Networks from Overfitting. The Journal of Machine Learning Research 15, 1929–1958</em>.</li>
<li>Sutskever, I., Martens, J., Dahl, G., Hinton, G., 2013. <em>On the importance of initialization and momentum in deep learning. In: International Conference on Machine Learning, pp. 1139–1147</em>.</li>
<li>Tieleman, T., Hinton, G., 2012. <em>Lecture 6.5-rmsprop:</em> <em>Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning 4, 26–31</em>.</li>
<li>Walia, A.S., 2017. <em>Types of Optimization Algorithms Used in Neural Networks and Ways to Optimize Gradient Descent [WWW Document]. Towards Data Science</em>. URL: <a href="https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f">https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f</a> (accessed <span>December 14, 2018</span>).</li>
<li>Zeiler, M.D., 2012. <em>ADADELTA: An Adaptive Learning Rate Method. arXiv preprint arXiv:1212.5701</em>.</li>
<li>Zhang, T., 2004. <em>Solving large-scale linear prediction problems using stochastic gradient descent algorithms</em>. <em>In: Proceedings of the Twenty-first International Conference on Machine Learning, p. 116</em>.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 4: Influential Classification Tools</h1>
                </header>
            
            <article>
                
<ul>
<li><em>API Documentation [WWW Document], n.d. TensorFlow</em>. URL: <a href="https://www.tensorflow.org/api_docs/">https://www.tensorflow.org/api_docs/</a> (accessed December 14, 2018).</li>
<li>Goodfellow, I., Bengio, Y., Courville, A., 2016. <em>Deep Learning. MIT Press</em>.</li>
<li>He, K., Zhang, X., Ren, S., Sun, J., 2015. <em>Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs]</em>.</li>
<li>Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H., 2017. <em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv:1704.04861 [cs]</em>.</li>
<li>Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q., 2016. <em>Densely Connected Convolutional Networks. arXiv:1608.06993 [cs]</em>.</li>
<li>Karpathy, A., n.d. <em>Stanford University CS231n: Convolutional Neural Networks for Visual Recognition [WWW Document]</em>. URL: <a href="http://cs231n.stanford.edu/">http://cs231n.stanford.edu/</a> (accessed December 14, 2018).</li>
<li>Karpathy, A. <em>What I learned from competing against a ConvNet on ImageNet [WWW Document], n.d</em>. URL: <a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/</a> (accessed January 4, 2019).</li>
<li>Lin, M., Chen, Q., Yan, S., 2013. <em>Network In Network. arXiv:1312.4400 [cs]</em>.</li>
<li>Pan, S.J., Yang, Q., 2010. <em>A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering 22, 1345–1359</em>.</li>
<li>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L., 2014. <em>ImageNet Large-Scale Visual Recognition Challenge. arXiv:1409.0575 [cs]</em>.</li>
<li>Sarkar, D. (DJ), 2018. <em>A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning [WWW Document]. Towards Data Science</em>. URL: <a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a</a> (accessed <span>January 15, 2019</span>).</li>
<li>shu-yusa, 2018. <em>Using Inception-v3 from TensorFlow Hub for Transfer Learning. Medium.</em></li>
<li><em>Simonyan, K., Zisserman, A., 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv:1409.1556 [cs]</em>.</li>
<li>Srivastava, R.K., Greff, K., Schmidhuber, J., 2015. <em>Highway Networks. arXiv:1505.00387 [cs]</em>.</li>
<li>Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A., 2016. <em>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. arXiv:1602.07261 [cs]</em>.</li>
<li>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2014. <em>Going Deeper with Convolutions. arXiv:1409.4842 [cs]</em>.</li>
<li>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., 2015. <em>Rethinking the Inception Architecture for Computer Vision. arXiv:1512.00567 [cs]</em>.</li>
<li>Thrun, S., Pratt, L., 1998. <em>Learning to Learn</em>.</li>
<li>Zeiler, Matthew D., Fergus, R., 2014. <em>Visualizing and Understanding Convolutional Networks</em>. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (Eds.), <em>Computer Vision – ECCV 2014. Springer International Publishing, Cham, pp. 818–833</em>.</li>
<li>Zeiler, Matthew D, Fergus, R., 2014. <em>Visualizing and Understanding Convolutional Networks. In: European Conference on Computer Vision, pp. 818–833</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 5: Object Detection Models</h1>
                </header>
            
            <article>
                
<ul>
<li class="csl-entry">Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., 2015. <em>The Pascal Visual Object Classes Challenge: A Retrospective. International Journal of Computer Vision 111, 98–136</em>.</li>
<li class="csl-entry">Girshick, R., 2015. <em>Fast R-CNN. arXiv:1504.08083 [cs]</em>.</li>
<li class="csl-entry">Girshick, R., Donahue, J., Darrell, T., Malik, J., 2013. <em>Rich feature hierarchies for accurate object detection and semantic segmentation. arXiv:1311.2524 [cs]</em>.</li>
<li class="csl-entry">Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2015. <em>You Only Look Once: Unified, Real-Time Object Detection. arXiv:1506.02640 [cs]</em>.</li>
<li class="csl-entry">Redmon, J., Farhadi, A., 2016. YOLO9000: <em>Better, Faster, Stronger. arXiv:1612.08242 [cs]</em>.</li>
<li class="csl-entry">Redmon, J., Farhadi, A., 2018. YOLOv3: <em>An Incremental Improvement. arXiv:1804.02767 [cs]</em>.</li>
<li class="csl-entry">Ren, S., He, K., Girshick, R., Sun, J., 2015. <em>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. arXiv:1506.01497 [cs]</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 6: Enhancing and Segmenting Images</h1>
                </header>
            
            <article>
                
<ul>
<li>Bai, M., Urtasun, R., 2016. <em>Deep Watershed Transform for Instance Segmentation. arXiv:1611.08303 [cs]</em>.</li>
<li>Beyer, L., 2019. <em>Python wrapper to Philipp Krähenbühl's dense (fully connected) CRFs with gaussian edge potentials: lucasb-eyer/pydensecr</em><em>f</em>.</li>
<li><em>Building Autoencoders in Keras [WWW Document]</em>, n.d. URL: <a href="https://blog.keras.io/building-autoencoders-in-keras.html">https://blog.keras.io/building-autoencoders-in-keras.html</a> (accessed January 18, 2019).</li>
<li>Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B., 2016. <em>The Cityscapes Dataset for Semantic Urban Scene Understanding. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. <em>Presented at the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, <em>IEEE, Las Vegas, NV, USA, pp. 3213–3223</em>.</li>
<li>Dice, L.R., 1945. <em>Measures of the Amount of Ecologic Association Between Species. Ecology 26, 297–302</em>.</li>
<li>Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., Pal, C., 2016. <em>The Importance of Skip Connections in Biomedical Image Segmentation. arXiv:1608.04117 [cs]</em>.</li>
<li>Dumoulin, V., Visin, F., 2016. <em>A Guide to Convolution Arithmetic for Deep Learning. arXiv:1603.07285 [cs, stat]</em>.</li>
<li>Guan, S., Khan, A., Sikdar, S., Chitnis, P.V., n.d. <em>Fully Dense UNet for 2D Sparse Photoacoustic Tomography Artifact Removal 8</em>.</li>
<li>He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. <em>Mask R-CNN. arXiv:1703.06870 [cs]</em>.</li>
<li><em>Kaggle. 2018 Data Science Bowl [WWW Document]</em>, n.d. URL: <a href="https://kaggle.com/c/data-science-bowl-2018">https://kaggle.com/c/data-science-bowl-2018</a> (accessed February 8, 2019).</li>
<li>Krähenbühl, P., Koltun, V., n.d. <em>Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials 9</em>.</li>
<li>Lan, T., Li, Y., Murugi, J.K., Ding, Y., Qin, Z., 2018. <em>RUN: Residual U-Net for Computer-Aided Detection of Pulmonary Nodules without Candidate Selection. arXiv:1805.11856 [cs]</em>.</li>
<li>Li, X., Chen, H., Qi, X., Dou, Q., Fu, C.-W., Heng, P.A., 2017. <em>H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes. arXiv:1709.07330 [cs]</em>.</li>
<li>Lin, T.-Y., Goyal, P., Girshick, R., He, K., Dollár, P., 2017. <em>Focal Loss for Dense Object Detection. arXiv:1708.02002 [cs]</em>.</li>
<li>Milletari, F., Navab, N., Ahmadi, S.-A., 2016. <em>V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</em>. <em>In: 2016 Fourth International Conference on 3D Vision (3DV)</em>. <em>Presented at the 2016 Fourth International Conference on 3D Vision (3DV), IEEE, Stanford, CA, USA, pp. 565–571</em>.</li>
<li>Noh, H., Hong, S., Han, B., 2015. <em>Learning Deconvolution Network for Semantic Segmentation</em>. In: 2015 <em>IEEE International Conference on Computer Vision (ICCV)</em>. <em>Presented at the 2015 ICCV, IEEE, Santiago, Chile, pp. 1520–1528</em>.</li>
<li>Odena, A., Dumoulin, V., Olah, C., 2016. <em>Deconvolution and Checkerboard Artifacts. Distill 1, e3</em>.</li>
<li>Ronneberger, O., Fischer, P., Brox, T., 2015. <em>U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv:1505.04597 [cs]</em>.</li>
<li>Shelhamer, E., Long, J., Darrell, T., 2017. <em>Fully Convolutional Networks for Semantic Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 39, 640–651</em>.</li>
<li>Sørensen, T., 1948. <em>A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons. Biol. Skr. 5, 1–34</em>.</li>
<li><em>Unsupervised Feature Learning and Deep Learning Tutorial [WWW Document]</em>, n.d. URL: <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/">http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/</a> (accessed January 17, 2019).</li>
<li>Zeiler, M.D., Fergus, R., 2013. <em>Visualizing and Understanding Convolutional Networks. arXiv:1311.2901 [cs]</em>.</li>
<li>Zhang, Z., Liu, Q., Wang, Y., 2018. <em>Road Extraction by Deep Residual U-Net. IEEE Geoscience and Remote Sensing Letters 15, 749–753</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 7: Training on Complex and Scarce Datasets</h1>
                </header>
            
            <article>
                
<ul>
<li>Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D., 2017a. <em>Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</em>. <em>In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Honolulu</em>, <em>HI, pp. 95–104</em>.</li>
<li>Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D., 2017b. <em>Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3722–3731</em>.</li>
<li>Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti, L., Strub, F., Rouat, J., Larochelle, H., Courville, A., 2017. <em>HoME: a Household Multimodal Environment. arXiv:1711.11017 [cs, eess]</em>.</li>
<li>Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F., 2015. ShapeNet: <em>An Information-Rich 3D Model Repository (No. arXiv:1512.03012 [cs.GR]). Stanford University – Princeton University – Toyota Technological Institute at Chicago</em>.</li>
<li>Chen, Y., Li, W., Sakaridis, C., Dai, D., Van Gool, L., 2018. <em>Domain Adaptive Faster R-CNN for Object Detection in the Wild. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. <em>Presented at the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Salt Lake City, UT, USA, pp. 3339–3348</em>.</li>
<li>Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B., 2016. <em>The Cityscapes Dataset for Semantic Urban Scene Understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3213–3223</em>.</li>
<li>Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V., 2017. <em>Domain-Adversarial Training of Neural Networks. In: Csurka, G. (Ed.), Domain Adaptation in Computer Vision Applications. Springer International Publishing, Cham, pp. 189–209</em>.</li>
<li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. <em>Generative Adversarial Nets. In: Advances in Neural Information Processing Systems, pp. 2672–2680</em>.</li>
<li>Gschwandtner, M., Kwitt, R., Uhl, A., Pree, W., 2011. <em>BlenSor: Blender Sensor Simulation Toolbox. In: International Symposium on Visual Computing, pp. 199–208</em>.</li>
<li>Hernandez-Juarez, D., Schneider, L., Espinosa, A., Vázquez, D., López, A.M., Franke, U., Pollefeys, M., Moure, J.C., 2017. <em>Slanted Stixels: Representing San Francisco's Steepest Streets. arXiv:1707.05397 [cs]</em>.</li>
<li>Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A.A., Darrell, T., 2017. <em>CyCADA: Cycle-Consistent Adversarial Domain Adaptation. arXiv:1711.03213 [cs]</em>.</li>
<li>Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A., 2017. <em>Image-to-Image Translation with Conditional Adversarial Networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1125–1134</em>.</li>
<li>Kingma, D.P., Welling, M., 2013. <em>Auto-encoding Variational Bayes. arXiv preprint arXiv:1312.6114</em>.</li>
<li>Long, M., Cao, Y., Wang, J., Jordan, M.I., n.d. <em>Learning Transferable Features with Deep Adaptation Networks 9</em>.</li>
<li>Planche, B., Wu, Z., Ma, K., Sun, S., Kluckner, S., Lehmann, O., Chen, T., Hutter, A., Zakharov, S., Kosch, H., et al., 2017. <em>Depthsynth: Real-Time Realistic Synthetic Data Generation from CAD Models for 2.5D Recognition. In: 2017 International Conference on 3D Vision (3DV), pp</em>. 1–10.</li>
<li>Planche, B., Zakharov, S., Wu, Z., Hutter, A., Kosch, H., Ilic, S., 2018. <em>Seeing Beyond Appearance—Mapping Real Images into Geometrical Domains for Unsupervised CAD-based Recognition. arXiv preprint arXiv:1810.04158</em>.</li>
<li><em>Protocol Buffers [WWW Document], n.d. Google Developers</em>. URL: <a href="https://developers.google.com/protocol-buffers/">https://developers.google.com/protocol-buffers/</a> (accessed February 23, 2019).</li>
<li>Radford, A., Metz, L., Chintala, S., 2015. <em>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arXiv:1511.06434 [cs]</em>.</li>
<li>Richter, S.R., Vineet, V., Roth, S., Koltun, V., 2016. <em>Playing for Data: Ground Truth from Computer Games. In: European Conference on Computer Vision, pp. 102–118</em>.</li>
<li>Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M., 2016. <em>The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Presented at the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Las Vegas, NV, USA, pp. 3234–3243</em>.</li>
<li>Rozantsev, A., Lepetit, V., Fua, P., 2015. <em>On Rendering Synthetic Images for Training an Object Detector. Computer Vision and Image Understanding 137, 24–37</em>.</li>
<li>Tremblay, J., Prakash, A., Acuna, D., Brophy, M., Jampani, V., Anil, C., To, T., Cameracci, E., Boochoon, S., Birchfield, S., 2018. <em>Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>. <em>Presented at the 2018 IEEE/CVF CVPRW, IEEE, Salt Lake City, UT, pp. 1082–10828</em>.</li>
<li>Tzeng, E., Hoffman, J., Saenko, K., Darrell, T., 2017. <em>Adversarial Discriminative Domain Adaptation. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. <em>Presented at the 2017 IEEE CVPR, IEEE, Honolulu, HI, pp. 2962–2971</em>.</li>
<li>Zhu, J.-Y., Park, T., Isola, P., Efros, A.A., 2017. <em>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2223–2232</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 8: Video and Recurrent Neural Networks</h1>
                </header>
            
            <article>
                
<ul>
<li class="csl-entry">Britz, D., 2015. <em>Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients. WildML</em>.</li>
<li class="csl-entry">Brown, C., 2019. <em>repo for learning neural nets and related material: go2carter/nn-learn.</em></li>
<li class="csl-entry"><em>Chung, J., Gulcehre, C., Cho, K., Bengio, Y., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv:1412.3555 [cs]</em>.</li>
<li class="csl-entry">Hochreiter, S., Schmidhuber, J., 1997. <em>Long Short-Term Memory. Neural Computation 9, 1735–1780</em>.</li>
<li class="csl-entry">Lipton, Z.C., Berkowitz, J., Elkan, C., 2015. <em>A Critical Review of Recurrent Neural Networks for Sequence Learning. arXiv:1506.00019 [cs]</em>.</li>
<li class="csl-entry">Soomro, K., Zamir, A.R., Shah, M., 2012. <em>UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild. arXiv:1212.0402 [cs]</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 9: Optimizing Models and Deploying on Mobile Devices</h1>
                </header>
            
            <article>
                
<ul>
<li>Goodfellow, I.J., Erhan, D., Carrier, P.L., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler, D., Lee, D.-H., Zhou, Y., Ramaiah, C., Feng, F., Li, R., Wang, X., Athanasakis, D., Shawe-Taylor, J., Milakov, M., Park, J., Ionescu, R., Popescu, M., Grozea, C., Bergstra, J., Xie, J., Romaszko, L., Xu, B., Chuang, Z., Bengio, Y., 2013. <em>Challenges in Representation Learning: A Report on Three Machine Learning Contests. arXiv:1307.0414 [cs, stat]</em>.</li>
<li class="csl-entry">Hinton, G., Vinyals, O., Dean, J., 2015. <em>Distilling the Knowledge in a Neural Network. arXiv:1503.02531 [cs, stat]</em>.</li>
<li class="csl-entry">Hoff, T., n.d. <em>The Technology Behind Apple Photos and the Future of Deep Learning and Privacy – High Scalability</em>.</li>
<li class="csl-entry"><em>Tencent, n.d. Tencent/PocketFlow: An Automatic Model Compression (AutoMC) Framework for Developing Smaller and Faster AI Applications</em>.</li>
</ul>


            </article>

            
        </section>
    </body></html>