["```\npip install tensorflow==1.13.1 \n```", "```\nimport tensorflow as tf\nhello = tf.constant(\"Hello TensorFlow!\")\nsess = tf.Session()\nprint(sess.run(hello)) \n```", "```\na = tf.multiply(8,5)\nb = tf.multiply(a,1) \n```", "```\na = tf.multiply(8,5)\nb = tf.multiply(4,3) \n```", "```\ngraph = tf.Graph()\nwith graph.as_default():\n     z = tf.add(x, y, name='Add') \n```", "```\nsess = tf.Session() \n```", "```\na = tf.multiply(3,3)\nprint(a) \n```", "```\na = tf.multiply(3,3)\nwith tf.Session as sess:\n    print(sess.run(a)) \n```", "```\nx = tf.Variable(13) \n```", "```\nW = tf.Variable(tf.random_normal([500, 111], stddev=0.35), name=\"weights\") \n```", "```\nx = tf.Variable(1212)\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n  sess.run(init) \n  print(sess.run(x)) \n```", "```\n x = tf.constant(13) \n```", "```\n x = tf.placeholder(\"float\", shape=None) \n```", "```\nx = tf.placeholder(\"float\", None)\ny = x+3\nwith tf.Session() as sess:\n    result = sess.run(y)\n    print(result) \n```", "```\nwith tf.Session() as sess:\n    result = sess.run(y, feed_dict={x: 5})\n    print(result) \n```", "```\nx = tf.constant(1,name='x')\ny = tf.constant(1,name='y')\na = tf.constant(3,name='a')\nb = tf.constant(3,name='b') \n```", "```\nprod1 = tf.multiply(x,y,name='prod1')\nprod2 = tf.multiply(a,b,name='prod2') \n```", "```\nsum = tf.add(prod1,prod2,name='sum') \n```", "```\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(logdir='./graphs',graph=sess.graph)\n    print(sess.run(sum)) \n```", "```\ntensorboard --logdir=graphs --port=8000 \n```", "```\nwith tf.name_scope(\"Product\"):\n    with tf.name_scope(\"prod1\"):\n        prod1 = tf.multiply(x,y,name='prod1')\n\n    with tf.name_scope(\"prod2\"):\n        prod2 = tf.multiply(a,b,name='prod2') \n```", "```\nwith tf.name_scope(\"sum\"):\n    sum = tf.add(prod1,prod2,name='sum') \n```", "```\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter('./graphs', sess.graph)\n    print(sess.run(sum)) \n```", "```\ntensorboard --logdir=graphs --port=8000 \n```", "```\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\nimport matplotlib.pyplot as plt\n%matplotlib inline \n```", "```\nmnist = input_data.read_data_sets(\"data/mnist\", one_hot=True) \n```", "```\nprint(\"No of images in training set {}\".format(mnist.train.images.shape))\nprint(\"No of labels in training set {}\".format(mnist.train.labels.shape))\nprint(\"No of images in test set {}\".format(mnist.test.images.shape))\nprint(\"No of labels in test set {}\".format(mnist.test.labels.shape))\nNo of images in training set (55000, 784)\nNo of labels in training set (55000, 10)\nNo of images in test set (10000, 784)\nNo of labels in test set (10000, 10) \n```", "```\nimg1 = mnist.train.images[0].reshape(28,28)\nplt.imshow(img1, cmap='Greys') \n```", "```\n#number of neurons in input layer\nnum_input = 784\n#num of neurons in hidden layer 1\nnum_hidden1 = 512\n#num of neurons in hidden layer 2\nnum_hidden2 = 256\n#num of neurons in hidden layer 3\nnum_hidden_3 = 128\n#num of neurons in output layer\nnum_output = 10 \n```", "```\nwith tf.name_scope('input'):\n    X = tf.placeholder(\"float\", [None, num_input])\nwith tf.name_scope('output'):\n    Y = tf.placeholder(\"float\", [None, num_output]) \n```", "```\nwith tf.name_scope('weights'):\n\n weights = {\n 'w1': tf.Variable(tf.truncated_normal([num_input, num_hidden1], stddev=0.1),name='weight_1'),\n 'w2': tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=0.1),name='weight_2'),\n 'w3': tf.Variable(tf.truncated_normal([num_hidden2, num_hidden_3], stddev=0.1),name='weight_3'),\n 'out': tf.Variable(tf.truncated_normal([num_hidden_3, num_output], stddev=0.1),name='weight_4'),\n } \n```", "```\nwith tf.name_scope('biases'):\n    biases = {\n        'b1': tf.Variable(tf.constant(0.1, shape=[num_hidden1]),name='bias_1'),\n        'b2': tf.Variable(tf.constant(0.1, shape=[num_hidden2]),name='bias_2'),\n        'b3': tf.Variable(tf.constant(0.1, shape=[num_hidden_3]),name='bias_3'),\n        'out': tf.Variable(tf.constant(0.1, shape=[num_output]),name='bias_4')\n    } \n```", "```\nwith tf.name_scope('Model'):\n\n    with tf.name_scope('layer1'):\n        layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['w1']), biases['b1']) )\n\n    with tf.name_scope('layer2'):\n        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']))\n\n    with tf.name_scope('layer3'):\n        layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['w3']), biases['b3']))\n\n    with tf.name_scope('output_layer'):\n         y_hat = tf.nn.sigmoid(tf.matmul(layer_3, weights['out']) + biases['out']) \n```", "```\nwith tf.name_scope('Loss'):\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat,labels=Y)) \n```", "```\nlearning_rate = 1e-4\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss) \n```", "```\nwith tf.name_scope('Accuracy'):\n\n    predicted_digit = tf.argmax(y_hat, 1)\n    actual_digit = tf.argmax(Y, 1)\n\n    correct_pred = tf.equal(predicted_digit,actual_digit)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) \n```", "```\ntf.summary.scalar(\"Accuracy\", accuracy)\ntf.summary.scalar(\"Loss\", loss) \n```", "```\nmerge_summary = tf.summary.merge_all() \n```", "```\ninit = tf.global_variables_initializer() \n```", "```\nlearning_rate = 1e-4\nnum_iterations = 1000\nbatch_size = 128 \n```", "```\nwith tf.Session() as sess: \n```", "```\n sess.run(init) \n```", "```\n summary_writer = tf.summary.FileWriter('./graphs', graph=tf.get_default_graph() \n```", "```\n for i in range(num_iterations): \n```", "```\n batch_x, batch_y = mnist.train.next_batch(batch_size) \n```", "```\n sess.run(optimizer, feed_dict={ X: batch_x, Y: batch_y}) \n```", "```\n if i % 100 == 0:\n            batch_loss, batch_accuracy,summary = sess.run(\n                [loss, accuracy, merge_summary],\n                feed_dict={X: batch_x, Y: batch_y}\n                )\n            #store all the summaries    \n            summary_writer.add_summary(summary, i)\n            print('Iteration: {}, Loss: {}, Accuracy: {}'.format(i,batch_loss,batch_accuracy)) \n```", "```\nIteration: 0, Loss: 2.30789709091, Accuracy: 0.1171875\nIteration: 100, Loss: 1.76062202454, Accuracy: 0.859375\nIteration: 200, Loss: 1.60075569153, Accuracy: 0.9375\nIteration: 300, Loss: 1.60388696194, Accuracy: 0.890625\nIteration: 400, Loss: 1.59523034096, Accuracy: 0.921875\nIteration: 500, Loss: 1.58489584923, Accuracy: 0.859375\nIteration: 600, Loss: 1.51407408714, Accuracy: 0.953125\nIteration: 700, Loss: 1.53311181068, Accuracy: 0.9296875\nIteration: 800, Loss: 1.57677125931, Accuracy: 0.875\nIteration: 900, Loss: 1.52060437202, Accuracy: 0.9453125 \n```", "```\nx = tf.constant(11)\ny = tf.constant(11)\nz = x*y\nwith tf.Session() as sess:\n    print(sess.run(z)) \n```", "```\nx = tf.constant(11)\ny = tf.constant(11)\nz = x*y\nprint(z) \n```", "```\n<tf.Tensor: id=789, shape=(), dtype=int32, numpy=121> \n```", "```\nz.numpy()\n121 \n```", "```\nx = tf.constant([1., 2., 3.])\ny = tf.constant([3., 2., 1.]) \n```", "```\nsum = tf.add(x,y)\nsum.numpy()\narray([4., 4., 4.], dtype=float32) \n```", "```\ndifference = tf.subtract(x,y)\ndifference.numpy()\narray([-2.,  0.,  2.], dtype=float32) \n```", "```\nproduct = tf.multiply(x,y)\nproduct.numpy()\narray([3., 4., 3.], dtype=float32) \n```", "```\ndivision = tf.divide(x,y)\ndivision.numpy()\narray([0.33333334, 1\\.        , 3\\.        ], dtype=float32) \n```", "```\ndot_product = tf.reduce_sum(tf.multiply(x, y))\ndot_product.numpy()\n10.0 \n```", "```\nx = tf.constant([10, 0, 13, 9]) \n```", "```\ntf.argmin(x).numpy()\n1 \n```", "```\ntf.argmax(x).numpy()\n2 \n```", "```\nx = tf.Variable([1,3,5,7,11])\ny = tf.Variable([1])\ntf.math.squared_difference(x,y).numpy()\n[  0,   4,  16,  36, 100] \n```", "```\nprint(x.dtype)\ntf.int32 \n```", "```\nx = tf.cast(x, dtype=tf.float32) \n```", "```\nprint(x.dtype)\ntf.float32 \n```", "```\nx = [[3,6,9], [7,7,7]]\ny = [[4,5,6], [5,5,5]] \n```", "```\ntf.concat([x, y], 0).numpy()\narray([[3, 6, 9],\n       [7, 7, 7],\n       [4, 5, 6],\n       [5, 5, 5]], dtype=int32) \n```", "```\ntf.concat([x, y], 1).numpy()\narray([[3, 6, 9, 4, 5, 6],\n       [7, 7, 7, 5, 5, 5]], dtype=int32) \n```", "```\ntf.stack(x, axis=1).numpy()\narray([[3, 7],\n       [6, 7],\n       [9, 7]], dtype=int32) \n```", "```\nx = tf.Variable([[1.0, 5.0], [2.0, 3.0]])\nx.numpy()\narray([[1., 5.],\n       [2., 3.]] \n```", "```\ntf.reduce_mean(input_tensor=x).numpy()\n2.75 \n```", "```\ntf.reduce_mean(input_tensor=x, axis=0).numpy()\narray([1.5, 4\\. ], dtype=float32) \n```", "```\ntf.reduce_mean(input_tensor=x, axis=1, keepdims=True).numpy()\narray([[3\\. ],\n       [2.5]], dtype=float32) \n```", "```\ntf.random.normal(shape=(3,2), mean=10.0, stddev=2.0).numpy()\ntf.random.uniform(shape = (3,2), minval=0, maxval=None, dtype=tf.float32,).numpy() \n```", "```\nx = tf.constant([7., 2., 5.])\ntf.nn.softmax(x).numpy()\narray([0.8756006 , 0.00589975, 0.11849965], dtype=float32) \n```", "```\ndef square(x):\n  return tf.multiply(x, x) \n```", "```\nwith tf.GradientTape(persistent=True) as tape:\n     print(square(6.).numpy())\n36.0 \n```", "```\npip install tensorflow==2.0.0-alpha0 \n```", "```\nfrom keras.models import Sequential\nfrom keras.layers import Dense \n```", "```\nmodel = Sequential() \n```", "```\nmodel.add(Dense(13, input_dim=7, activation='relu')) \n```", "```\nmodel.add(Dense(7, activation='relu')) \n```", "```\nmodel.add(Dense(1, activation='sigmoid')) \n```", "```\nmodel = Sequential()\nmodel.add(Dense(13, input_dim=7, activation='relu'))\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid')) \n```", "```\ninput = Input(shape=(2,)) \n```", "```\nlayer1 = Dense(10, activation='relu') \n```", "```\nlayer1 = Dense(10, activation='relu')(input) \n```", "```\nlayer2 = Dense(10, activation='relu')(layer1) \n```", "```\noutput = Dense(1, activation='sigmoid')(layer2) \n```", "```\nmodel = Model(inputs=input, outputs=output) \n```", "```\ninput = Input(shape=(2,))\nlayer1 = Dense(10, activation='relu')(input)\nlayer2 = Dense(10, activation='relu')(layer1)\noutput = Dense(1, activation='sigmoid')(layer2)\nmodel = Model(inputs=input, outputs=output) \n```", "```\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy']) \n```", "```\nmodel.fit(x=data, y=labels, epochs=100, batch_size=10) \n```", "```\nmodel.evaluate(x=data_test,y=labels_test) \n```", "```\nmodel.evaluate(x=data,y=labels) \n```", "```\nmnist = tf.keras.datasets.mnist \n```", "```\n(x_train,y_train), (x_test, y_test) = mnist.load_data() \n```", "```\nx_train, x_test = tf.cast(x_train/255.0, tf.float32), tf.cast(x_test/255.0, tf.float32)\ny_train, y_test = tf.cast(y_train,tf.int64),tf.cast(y_test,tf.int64) \n```", "```\nmodel = tf.keras.models.Sequential() \n```", "```\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(256, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(128, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(10, activation=\"softmax\")) \n```", "```\nmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n```", "```\nmodel.fit(x_train, y_train, batch_size=32, epochs=10) \n```", "```\nmodel.evaluate(x_test, y_test) \n```"]