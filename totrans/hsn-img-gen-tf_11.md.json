["```\nclass SpectralNorm(tf.keras.constraints.Constraint):\n    def __init__(self, n_iter=5):\n        self.n_iter = n_iter\n    def call(self, input_weights):\n        w = tf.reshape(input_weights, (-1, \t\t\t\t \t\tinput_weights.shape[-1]))\n        u = tf.random.normal((w.shape[0], 1))\n        for _ in range(self.n_iter):\n            v = tf.matmul(w, u, transpose_a=True)\n            v /= tf.norm(v)\n            u = tf.matmul(w, v)\n            u /= tf.norm(u)\n        spec_norm = tf.matmul(u, tf.matmul(w, v), \t\t\t\t\t   transpose_a=True)\n        return input_weights/spec_norm\n```", "```\nclass SelfAttention(Layer):\n    def __init__(self):\n        super(SelfAttention, self).__init__()\n    def build(self, input_shape):\n        n, h, w, c = input_shape\n        self.n_feats = h * w\n        self.conv_theta = Conv2D(c//8, 1, padding='same', \t\t\t\t     kernel_constraint=SpectralNorm(), \t\t\t\t     name='Conv_Theta')\n        self.conv_phi = Conv2D(c//8, 1, padding='same', \t\t\t\t    kernel_constraint=SpectralNorm(), \t\t\t\t    name='Conv_Phi')\n        self.conv_g = Conv2D(c//2, 1, padding='same',  \t\t\t\t    kernel_constraint=SpectralNorm(), \t\t\t\t    name='Conv_G')\n        self.conv_attn_g = Conv2D(c, 1, padding='same', \t\t\t\t    kernel_constraint=SpectralNorm(), \t\t\t\t    name='Conv_AttnG')\n        self.sigma = self.add_weight(shape=[1], \t\t\t\t\t\t    initializer='zeros', \t\t\t\t\t\t    trainable=True,  \t\t\t\t\t\t    name='sigma')\n```", "```\ndef call(self, x):\n        n, h, w, c = x.shape\n        theta = self.conv_theta(x)\n        theta = tf.reshape(theta, (-1, self.n_feats, \t\t\t\t\t\t  theta.shape[-1]))        \n        phi = self.conv_phi(x)\n        phi = tf.nn.max_pool2d(phi, ksize=2, strides=2, \t\t\t\t\t    padding='VALID')\n        phi = tf.reshape(phi, (-1, self.n_feats//4, \t\t\t\t   phi.shape[-1]))   \n        g = self.conv_g(x)\n        g = tf.nn.max_pool2d(g, ksize=2, strides=2, \t\t\t\t\t  padding='VALID')\n        g = tf.reshape(g, (-1, self.n_feats//4,  \t\t\t\t\tg.shape[-1]))\n```", "```\n        attn = tf.matmul(theta, phi, transpose_b=True)        attn = tf.nn.softmax(attn)\n```", "```\n        attn_g = tf.matmul(attn, g)\n        attn_g = tf.reshape(attn_g, (-1, h, w, \t\t\t\t\t attn_g.shape[-1]))\n        attn_g = self.conv_attn_g(attn_g)        \n        output = x + self.sigma * attn_g        \n        return output\n```", "```\ndef build_generator(z_dim, n_class):\n    DIM = 64\n    z = layers.Input(shape=(z_dim))\n    labels = layers.Input(shape=(1), dtype='int32')\n    x = Dense(4*4*4*DIM)(z)\n    x = layers.Reshape((4, 4, 4*DIM))(x)   \n    x = layers.UpSampling2D((2,2))(x)\n    x = Resblock(4*DIM, n_class)(x, labels)\n    x = layers.UpSampling2D((2,2))(x)\n    x = Resblock(2*DIM, n_class)(x, labels)\n    x = SelfAttention()(x)\n    x = layers.UpSampling2D((2,2))(x)\n    x = Resblock(DIM, n_class)(x, labels)\n    output_image = tanh(Conv2D(3, 3, padding='same')(x))\n    return Model([z, labels], output_image,  \t\t\t name='generator')  \n```", "```\nclass ConditionBatchNorm(Layer):\n    def build(self, input_shape):\n        self.input_size = input_shape\n        n, h, w, c = input_shape        \n        self.gamma = self.add_weight( \t\t\t\t\t\t  shape=[self.n_class, c], \t\t\t\t\t\t  initializer='ones', \t\t\t\t\t\t  trainable=True, \t\t\t\t\t\t  name='gamma')        \n        self.beta = self.add_weight( \t\t\t\t\t\t shape=[self.n_class, c], \t\t\t\t\t\t initializer='zeros', \t\t\t\t\t\t trainable=True, \t\t\t\t\t\t name='beta')             \n        self.moving_mean = self.add_weight(shape=[1, 1,  \t\t\t\t\t  1, c], initializer='zeros', \t\t\t\t\t  trainable=False, \t\t\t\t\t  name='moving_mean')    \n        self.moving_var = self.add_weight(shape=[1, 1,  \t\t\t\t\t  1, c], initializer='ones',\n \t\t\t\t\t  trainable=False, \t\t\t\t\t  name='moving_var')\n```", "```\ndef call(self, x, labels, training=False):\n    beta = tf.gather(self.beta, labels)\n    beta = tf.expand_dims(beta, 1)\n    gamma = tf.gather(self.gamma, labels)\n    gamma = tf.expand_dims(gamma, 1)\n```", "```\nclass Resblock(Layer):\n    def build(self, input_shape):\n        input_filter = input_shape[-1]\n        self.conv_1 = Conv2D(self.filters, 3, \t\t\t\t\t  padding='same', \t\t\t\t\t  name='conv2d_1')\n        self.conv_2 = Conv2D(self.filters, 3, \t\t\t\t\t  padding='same', \t\t\t\t\t  name='conv2d_2')\n        self.cbn_1 = ConditionBatchNorm(self.n_class)\n        self.cbn_2 = ConditionBatchNorm(self.n_class)\n        self.learned_skip = False\n        if self.filters != input_filter:\n            self.learned_skip = True\n            self.conv_3 = Conv2D(self.filters, 1, \t\t\t\t\t\tpadding='same', \t\t\t\t\t\tname='conv2d_3')       \n            self.cbn_3 = ConditionBatchNorm(self.n_class)\n```", "```\n    def call(self, input_tensor, labels):\n        x = self.conv_1(input_tensor)\n        x = self.cbn_1(x, labels)\n        x = tf.nn.leaky_relu(x, 0.2)\n        x = self.conv_2(x)\n        x = self.cbn_2(x, labels)\n        x = tf.nn.leaky_relu(x, 0.2)\n        if self.learned_skip:\n            skip = self.conv_3(input_tensor)\n            skip = self.cbn_3(skip, labels)\n            skip = tf.nn.leaky_relu(skip, 0.2)            \n        else:\n            skip = input_tensor\n        output = skip + x\n        return output\n```", "```\ndef build_discriminator(n_class):\n    DIM = 64\n    input_image = Input(shape=IMAGE_SHAPE)\n    input_labels = Input(shape=(1))\n    embedding = Embedding(n_class, 4*DIM)(input_labels)\n    embedding = Flatten()(embedding)\n    x = ResblockDown(DIM)(input_image) # 16\n    x = SelfAttention()(x)\n    x = ResblockDown(2*DIM)(x) # 8\n    x = ResblockDown(4*DIM)(x) # 4\n    x = ResblockDown(4*DIM, False)(x) # 4\n    x = tf.reduce_sum(x, (1, 2))\n    embedded_x  = tf.reduce_sum(x * embedding,  \t\t\t\t\t    axis=1, keepdims=True)\n    output = Dense(1)(x)\n    output += embedded_x\n    return Model([input_image, input_labels],  \t\t\t  output, name='discriminator')\n```", "```\nz_input = layers.Input(shape=(z_dim))\nz = tf.split(z_input, 4, axis=1) \nlabels = layers.Input(shape=(1), dtype='int32')\ny = Embedding(n_class, y_dim)(tf.squeeze(labels, [1]))\nx = Dense(4*4*4*DIM, **g_kernel_cfg)(z[0])\nx = layers.Reshape((4, 4, 4*DIM))(x)\nx = layers.UpSampling2D((2,2))(x)\ny_z = tf.concat((y, z[1]), axis=-1)\nx = Resblock(4*DIM, n_class)(x, y_z)\n```", "```\nclass ConditionBatchNorm(Layer):\n    def build(self, input_shape):\n        c = input_shape[-1]\n        self.dense_beta = Dense(c, **g_kernel_cfg,)\n        self.dense_gamma = Dense(c, **g_kernel_cfg,)\n        self.moving_mean = self.add_weight(shape=[1, 1, 1, c],     \t                                           initializer='zeros',                                           trainable=False,                                           name='moving_mean')\n        self.moving_var = self.add_weight(shape=[1, 1, 1, c], \t                                          initializer='ones',                                          trainable=False,                                          name='moving_var')\n```", "```\n    def call(self, x, z_y, training=False):\n        beta = self.dense_beta(z_y)\n        gamma = self.dense_gamma(z_y)\n        for _ in range(2):\n            beta = tf.expand_dims(beta, 1)\n            gamma = tf.expand_dims(gamma, 1)\n```", "```\nclass OrthogonalReguralizer( \t\t\ttf.keras.regularizers.Regularizer):\n    def __init__(self, beta=1e-4):\n        self.beta = beta   \n    def __call__(self, input_tensor):\n        c = input_tensor.shape[-1]\n        w = tf.reshape(input_tensor, (-1, c)) \n        ortho_loss = tf.matmul(w, w, transpose_a=True) *\\ \t\t\t\t\t   (1 -tf.eye(c))\n        return self.beta * tf.norm(ortho_loss)\n    def get_config(self):\n        return {'beta': self.beta}\n```", "```\ng_kernel_cfg={\n    'kernel_initializer' : \\ \t\t\t\ttf.keras.initializers.Orthogonal(),\n    'kernel_constraint' : SpectralNorm(),\n    'kernel_regularizer' : OrthogonalReguralizer()\n}\nConv2D(1, 1, padding='same', **g_kernel_cfg)\n```"]