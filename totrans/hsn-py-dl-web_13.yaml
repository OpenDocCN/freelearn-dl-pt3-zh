- en: A General Production Framework for Deep Learning-Enabled Websites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered decent ground on using industry-grade cloud **Deep Learning**
    (**DL**) APIs in our applications in previous chapters and we have learned about
    their use through practical examples. In this chapter, we will cover a general
    outline for developing DL-enabled websites. This will require us to bring together
    all the things that we have learned so far so that we can put them to use in real-life
    use cases. In this chapter, we will learn how to structure a DL web application
    for production by first preparing the dataset. We will then train a DL model in
    Python and then wrap the DL models in APIs using Flask.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a high-level summary of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining our problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking the problem into several components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a mental model to bind the project components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How we should be collecting the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following a directory structure for our project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the project from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can access the code used in this chapter at [https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9](https://github.com/PacktPublishing/Hands-On-Python-Deep-Learning-for-Web/tree/master/Chapter9).
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the code used in this chapter, you''ll need the following software:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.6+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Python PIL library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Natural Language Toolkit** (**NLTK**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flask 1.1.0+ and compatible versions of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FlaskForm
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: wtforms
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: flask_restful
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: flask_jsonpify
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All other installations will be described during the course of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any project should start with a well-defined problem statement or the project
    development is bound to suffer. The problem statement governs all the major steps
    involved in an overall project development pipeline, starting from project planning
    to project cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a DL-based web project, for example, the problem statement will direct us
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine what kind of data we would need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much complexity there would be in terms of code, planning, and other resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of user interface we would develop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much human involvement there would be so that an estimate can be prepared
    on the project’s manpower and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, a well-defined problem statement is really required in order for us to
    get started with further project development.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine being a DL engineer at a company that is planning to build a recommendation
    system to suggest products from a product listing based on some user-provided
    criteria. Your boss has asked you to develop a **Proof of Concept** (**PoC**)
    based on this. So, how should we go about it? As mentioned previously, let’s start
    by defining the problem statement first.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main entity that provides inputs to the final recommendation system is
    a user. Based on the user’s preferences (let’s call the input features preferences
    for now), the system would provide a list of products that best match their preference.
    So, long story short, the problem statement can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Given a set of input features (user preferences), our task is to suggest a
    list of products.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a well-defined problem statement to proceed, let’s go ahead
    and build up the next steps in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a mental model of the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at the problem statement, you might feel tempted to open a browser and
    start searching for some datasets. But when it comes to properly develop a project,
    definite planning is required to structure it piece by piece. A project without
    a structure is nothing more than a rudderless ship. So, we will be cautious about
    this from the start. We will discuss the modules that are going to play a very
    essential role in our project. This includes several mental considerations as
    well. I like to call this phase building a mental model of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take some time to discuss the problem statement further, so as to figure
    out the essential modules we would need to develop.
  prefs: []
  type: TYPE_NORMAL
- en: Our project concerns recommending products to users based on their preferences.
    So, in order to perform this recommendation, we would need a system that knows
    how to understand the set of preferences a user is providing to it. To be able
    to make sense of these preferences, the system would need some kind of training
    that we would be implementing DL. But what about preferences? How would they look
    like? You will often encounter these questions in real-world project situations
    that need humans in the loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, think for a second and try to think of the aspects you typically look
    for while choosing a product to buy. Let’s list them here:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the specifications of the product? If I want a large size T-shirt,
    I should not be recommended a small size T-shirt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the cost of the product? Users have a limited amount of money is this
    recommendation good for their wallet?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What brand is this product? Users often have brand preferences for similar products
    manufactured by several companies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the preceding pointers are not in any particular order.
  prefs: []
  type: TYPE_NORMAL
- en: So, from the preceding section, we are starting to get a sense of what we would
    need, which is an interface (which will essentially be a web page, in our case)
    for a user to provide their preferences. Taking these preferences into account,
    our system would predict a set of products that it found to be the most appropriate
    ones. This is where the DL part comes into play. As we will recollect from earlier
    chapters, for a DL model to work on a given problem, it needs to be trained on
    some data that represents the problem as closely as possible. Let’s now discuss
    the data part of our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a readily available dataset for our project—the Amazon Fine Food Reviews
    dataset provided by Amazon and created by the Stanford Network Analysis Project
    team. While the dataset is large in size, we will not be using the full dataset
    when creating the demonstration in this chapter. An immediate question that might
    get triggered here is how would the dataset look? We need to formulate a rough
    plan to decide the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What features we would be choosing to construct the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where we would be looking to collect the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s add a bit of enhancement to the original problem statement before proceeding
    further from here. Here’s the original problem statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Given a set of input features (user preferences), our task is to suggest a
    list of products.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Users will not like our system if it recommends them substandard products.
    So, we would modify the problem statement a bit, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Given a set of input features (user preferences), our task is to suggest a
    list of the best possible products to buy.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our system to recommend a list of the best possible products with respect
    to a given criterion, it first needs to know the average ratings of the products.
    Along with the average ratings, it would be useful to have the following information
    about a particular product, apart from its name:'
  prefs: []
  type: TYPE_NORMAL
- en: Specifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Category of product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seller name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expected delivery time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While preparing the data, we would look for the previous pointers about a particular
    product. Now comes the question of where we would be collecting the data from.
    The answer is Amazon! Amazon is known for its services in the e-commerce industry
    in providing us with various products and information about them, such as their
    ratings, product specifications, the price of the items, and so on. But say Amazon
    does not allow you to download this data directly as a zipped file. In order to
    get the data from Amazon in the desired form, we would have to resort to web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to this point in the discussion, we are certain on two broad areas of the
    project:'
  prefs: []
  type: TYPE_NORMAL
- en: An interface to receive preferences from the user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data that would represent the problem statement we are dealing with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For DL modeling, we will be starting with simple, fully-connected, neural network-based
    architecture. It’s often useful to start with a simple model and gradually increase
    the complexity because it makes the code base easier to debug.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, from this, it is safe enough to say that the following three modules are
    going to play an essential role in this project:'
  prefs: []
  type: TYPE_NORMAL
- en: An interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A DL model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully, you now have a fair idea about approaching the development of a project
    in the first place. What questions you should be asking at this stage and what
    considerations you may have to make can be worked out from the involved framework
    you now have.
  prefs: []
  type: TYPE_NORMAL
- en: We would not want our recommendation system to be biased toward anything. There
    can be many types of biases hidden in the data and naturally enough, it can cause
    the DL system that uses it to inherit that bias.
  prefs: []
  type: TYPE_NORMAL
- en: To find out more about different types of biases in machine learning systems,
    you are encouraged to refer to this article at [https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias](https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias).
    In our case, a staggering example of bias would be a situation where a male visitor
    gets product recommendations that are averaged out. The recommendations might
    only come on the basis of his gender but not based on any other visitor-browsing
    pattern. This can be erroneous and may have been done mistakenly. But instances
    like this can make our model very inappropriate. In the next section, we will
    be discussing a few points to learn how can we avoid bias on the data.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding the chances of getting erroneous data in the first place
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What is erroneous data? Are we only talking about data with wrong values? The
    answer is no. Besides data having wrong or missing values, erroneous data can
    have subtle but grave errors that may lead to poor training of the model or even
    bias. So, it is important to identify such erroneous data and remove it before
    training our model. There are five main ways of identifying these errors:'
  prefs: []
  type: TYPE_NORMAL
- en: Look for missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look for values that seem out of scale or possibility—in other words, outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not include any features in the dataset that might cause data leakage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that all categories of evaluation have a similar number of samples in
    the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that your design of the solution to the problem itself does not introduce
    a bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we are clear on these points, we are ready to move on to the more specific
    areas that we need to be careful about during the collection of data. It is important
    that during data collection a proper plan is prepared to keep in mind all the
    properties of the data source and the requirements of the problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you are scraping data for products from US-based outlets on Amazon and
    end up searching for products on the Indian version of Amazon instead. The scraper
    might give you data from India-based outlets, which may not be suitable for recommendation
    to US-based residents.
  prefs: []
  type: TYPE_NORMAL
- en: Further, since Amazon—and similar services, such as Flipkart—takes the help
    of recommender systems to target the *most suitable* products for their customers,
    during data collection, the scraper should not become prey to such recommendations.
    It is important that the scraper clears its context every now and then and avoids
    getting biased results due to the AI put in place by Amazon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example from the Amazon Fine Food Reviews dataset. Though on
    the first look the dataset looks pretty balanced, we can uncover a lot of bias
    in the dataset. Consider the length of the text that the customers write for their
    reviews of products. Let''s plot them in a graph against the score they were rated.
    The following graphs show the plot for products rated 1 and 2 stars:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29b500df-2cd7-4404-b9da-a9cbc49b7fe0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graphs show the plot for products rated 3 and 4 stars:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8325444b-269a-4dca-9f6d-b1cb9cb77b8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph shows the plot for products rated 5 stars:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b4555fa-698d-4e04-bf56-b8a2ba2c4cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how more positive reviews have more written text in them. This would
    directly convert into most of the words in the dataset, leading to a higher rating
    from the user. Now, consider a scenario where a user writes a lengthy review with
    a low rating and a generally negative opinion about the product. Since our model
    is trained to associate larger lengths of reviews to positive ratings, it would
    mark the negative review as positive.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line here is that real-world data can contain many edge cases, as
    shown, and if they are not handled in a proper manner, you will most likely get
    an erroneous model.
  prefs: []
  type: TYPE_NORMAL
- en: How not to build an AI backend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Considering the vastness that web applications can grow to and the strong dependence
    of nearly every other platform on a backend that runs as a web-based service,
    it is important for the backend to be well thought of and properly executed. AI-based
    applications, even in a PoC stage, are often not blazingly fast in responding
    or take a lot of time to train on the new samples.
  prefs: []
  type: TYPE_NORMAL
- en: While we will be discussing tips and tricks to make a backend that does not
    choke under pressure due to bottlenecks, we need to lay down a few pointers that
    need to be avoided in the best possible way when developing an AI-integrated backend
    for a website.
  prefs: []
  type: TYPE_NORMAL
- en: Expecting the AI part of the website to be real time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI is computationally expensive and needless to say, this is undesirable for
    a website that aims to serve its clients in the quickest time possible. While
    smaller models or using browser AI (such as TensorFlow.js or other libraries)
    can provide the experience of real-time AI responses, even they suffer issues
    where the client is in a slow network area or using a low-end device. So, both
    the methods of in-browser AI models or lightweight AI models replying near instantaneously
    are subject to device configuration and network bandwidth. Hence, the backend
    of the website, which is supposed to make quick responses to the client, should
    ideally be separated from the part that handles the AI model responses. Both,
    working in parallel, should maintain a common data storage and a proper method
    of interaction between the two, such that the backend code responsible for responding
    to the clients has less dependence on the AI model part.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming the incoming data from a website is ideal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though the website or app corresponding to the project might resemble an
    ideal method of data collection, the data coming from it must not be assumed to
    be free of errors. Bad network requests, malicious connections, or simply garbage
    input provided by users can lead to data that is unfit for training. A non-malicious
    user may have network issues and refresh the same page 10 to 20 times in a short
    time frame, which should not add to the viewing-based importance of that page.
    All data collected from the website must be subject to cleanup and filtering based
    on the requirements of the model. It must be kept in mind that the challenges
    faced by websites will almost certainly affect the quality of data collected.
  prefs: []
  type: TYPE_NORMAL
- en: A sample end-to-end AI-integrated web application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have discussed an overview and the pitfalls to avoid when creating
    an AI-powered website backend, let's move on to creating one—albeit a rather simple
    one—that demonstrates the general overview of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following steps, as stated previously:'
  prefs: []
  type: TYPE_NORMAL
- en: The collection of data as per the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning and preprocessing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the AI model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the AI model on the interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While we have previously discussed the pitfalls of collecting the data, we will
    briefly discuss here the tools and methods that can be employed to complete the
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection and cleanup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the purpose of collecting data, from a general perspective, there could
    be several data sources. You could scrape data off websites or simply download
    some prepared dataset. Other methods could also be employed, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating data on the fly within the runtime of applications/websites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging from applications or smart devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting data directly from users via systematic forms (such as quizzes or
    surveys)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting data from survey agencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observational data measured by specific methods (scientific data) and other
    ways
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beautifulsoup` is a library commonly used to perform web scraping. `Scrapy`
    is yet another popular tool and can be used very rapidly.'
  prefs: []
  type: TYPE_NORMAL
- en: The data cleaning would entirely depend on the form of data collected by you
    and has been discussed in previous chapters of the book. We will assume that you
    are able to convert your data into a format that is suitable for how you wish
    to proceed with the model-building part. For the further topics in this section,
    we will use a prepared dataset titled Amazon Fine Food Reviews, which can be downloaded
    from [https://www.kaggle.com/snap/amazon-fine-food-reviews.](https://www.kaggle.com/snap/amazon-fine-food-reviews)
    Once you extract the downloaded ZIP file, you'll get the dataset as a file called
    `Reviews.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: A good starting point to observe how to perform web scraping and prepare a clean
    dataset is [https://github.com/Nilabhra/kolkata_nlp_workshop_2019](https://github.com/Nilabhra/kolkata_nlp_workshop_2019).
  prefs: []
  type: TYPE_NORMAL
- en: Building the AI model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will prepare the AI model, which will recommend products based on the
    user's query. To do so, let's create a new Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Making the necessary imports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin by importing the required Python modules to the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We import `TfidfVectorizer` to help us create **Term Frequency-Inverse Document
    Frequency** (**TF-IDF**) vectors for performing natural language processing. TF-IDF
    is a numerical measure of how important a word in a single document is, given
    a number of documents that may or may not contain the words. Numerically, it increases
    the importance value when a single word occurs frequently in a single document
    but not in other documents. TF-IDF is so popular that over 80% of the world's
    natural language-based recommender systems currently use it.
  prefs: []
  type: TYPE_NORMAL
- en: We are also importing `WordPunctTokenizer`. A tokenizer performs the function
    of breaking down a text into elemental tokens. For example, a large paragraph
    may be broken down into sentences and further into words.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the dataset and preparing cleaning functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will read the Amazon Fine Food Reviews dataset with the `ISO-8859-1` encoding.
    This is only to ensure that we do not lose out on any special symbols used in
    the text of the review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since the dataset is very large, we've restricted our work in this chapter to
    the first 10,000 rows in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would need to remove stop words from the text and filter out symbols such
    as brackets and other symbols not natural to written text. We will create a function
    named `cleanText()`, which will perform the filtering and removal of stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using the preceding function, we have removed the stop words and any words shorter
    than three characters from the text. We have filtered out punctuation and are
    only keeping the relevant characters from the text.
  prefs: []
  type: TYPE_NORMAL
- en: Slicing out the required data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset contains more data than is useful to us for the demo at hand. We
    will extract the `ProductId`, `UserId`, `Score`, and `Text` columns to prepare
    our demo. The names of the products are encrypted for privacy reasons, just as
    the names of the users are encrypted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Keeping data encrypted and free of personal information is a challenge in data
    science. It is important to remove parts from the dataset that would make it possible
    to identify the private entities that are a part of the dataset. For example,
    you would need to remove people and organization names from the text of the review
    to stop the products and users from being identified, despite them having encrypted
    product and user IDs.
  prefs: []
  type: TYPE_NORMAL
- en: Applying text cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now apply the text filtering and stop word removal function to clean
    the text in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The time taken for the task is displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the preceding code block will only work in Jupyter Notebook and not
    in normal Python scripts. To run it on normal Python scripts, remove the `%%time`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset into train and test parts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we have a single dataset, we will break it into two parts, with the feature
    and label parts separated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We will use the `train_test_split()` method from the `sklearn` module to split
    the dataset into 80% for training and 20% for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating text about products and users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now aggregate the dataset''s reviews by users and product IDs. We''ll
    need the reviews for each product to determine what that product would be a good
    choice for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, reviews aggregated by users will help us determine what a user likes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating TF-IDF vectorizers of users and products
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now create two different vectorizers one is for users and the other
    for products. We will need these vectorizers in place to determine the similarity
    between the requirements of the users and what the reviews tell us about any given
    product. First, we will create the vectorizer for users and display its shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will create the vectorizer for products:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We use `WordPunctTokenizer` to break down the text and use the `fit_transform`
    method of the `TfidfVectorizer` object to prepare the vectors, which map the word
    dictionary to their importance in documents.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an index of users and products by the ratings provided
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the `pivot_table` method of the `pandas` module to create a matrix of
    user ratings against products. We will use this matrix to perform matrix factorization
    to determine the products that a user likes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also convert the `TfidfVectorizer` vectors for users and products into
    matrices suitable for matrix factorization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can now create the matrix factorization function.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the matrix factorization function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now create a function to perform matrix factorization. Matrix factorization
    became a popular family of algorithms used for recommender systems during the
    Netflix Prize challenge in 2006\. It is a family of algorithms that decomposes
    a user-item matrix into a set of two lower-dimension rectangular matrices that
    can be multiplied to restore the original higher-order matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We then perform the matrix factorization and log the time taken:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: After this, we need to save the model.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the model as pickle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, create a folder called `api` in the `root` directory of your project.
    Then, save the trained model, which is the lower-order matrices obtained after
    factorization of the user-products rating matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Saving the models as binary pickle files allows us to quickly load them back
    into the memory during deployment of the model on the backend of the website.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are done developing the predictive model, we will move on to building
    an interface for the application to work on.
  prefs: []
  type: TYPE_NORMAL
- en: Building an interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build an interface for the web application, we need to think about how we
    would want our users to interact with the system. In our case, we are expecting
    the user to be presented with suggestions based on what they search for in a search
    bar the moment they submit the search query. This means we need the system to
    respond in real time and generate suggestions on the fly. To build this system,
    we will create an API that will respond to the search query.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an API to answer search queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will create an API that accepts queries in the form of HTTP requests and
    replies with suggestions of products based on the search query entered by the
    user. To do so, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by importing the required modules for the API. We discussed these
    imported modules in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also import the `Flask` module to create a quick HTTP server that can
    serve on a defined route in the form of an API. We will instantiate the `Flask`
    app object as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The value of `SECRET_KEY` in the app configuration is up to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then create a `class` function to handle the text input that we receive
    in the form of a search query from the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To encapsulate the API methods, we wrap them in a `Flask_Work` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cleanText()` method we used during model creation is again required. It
    will be used to clean and filter out the search query entered by the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a home page for the application, which will be loaded from the `index.html`
    file that we create in the templates later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `post` method-based prediction route, which will respond with
    the product suggestions upon receiving the user''s search query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We attach the `Flask_Work` class to the `Flask` server. This completes the
    script on running. We have put an API in place that suggests products based on
    the user''s search query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Save this file as `main.py`. With the API script created, we need to host the
    server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so on a local machine, run the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will start the server on the computer on port `4000`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f7e2b07-8b1d-4ba9-a2ac-c4002fc9a22a.png)'
  prefs: []
  type: TYPE_IMG
- en: However, we still need to prepare a user interface to use this API. We will
    do so in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an interface to use the API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now create a simple, minimal UI to use the API we created. In essence,
    we will create a single search bar where the user enters the product or product
    specification that they want and the API returns recommendations based on the
    user's query. We will not be discussing the code for building the UI, but we have
    included it in the GitHub repository, which can be found at [http://tiny.cc/DL4WebCh9](http://tiny.cc/DL4WebCh9).
  prefs: []
  type: TYPE_NORMAL
- en: This UI will be visible at `http://127.0.0.1:4000` once you start the server,
    as shown in step 9 of the *Creating an API to answer search queries* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface we created looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65ce8f08-b037-4d00-a8b5-a94172b2d2bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The user enters a search query and gets recommendations, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e161cee5-2998-42d1-a4ef-4ed108ad54ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Our application does not have the benefit of saving user sessions. Also, it
    does not have parameters for the expected budget of the user, which is often a
    deciding factor in whether the product is a good fit for the user. It is easy
    to add these features to web applications and leverage their benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a general overview, web applications that hone the power of DL have a few
    set methods to do so via APIs, in-browser JavaScript, or by silently embedding
    DL models in the backend of the application. In this chapter, we saw how to use
    the most common of these methods—an API-based DL web application—while at the
    same time, we saw a rough overview of how to design similar solutions. We covered
    the thought process that goes into the identification of the problem statement
    and a subsequent solution, along with the pitfalls and pain points to avoid during
    the design of a web application that integrates DL models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover an end to end project that integrates DL
    on web applications for security purposes. We will see how DL can help us recognize
    suspicious activity and block spam users.
  prefs: []
  type: TYPE_NORMAL
