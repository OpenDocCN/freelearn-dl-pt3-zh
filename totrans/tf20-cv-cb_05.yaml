- en: '*Chapter 5*: Reducing Noise with Autoencoders'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Among the most interesting families of deep neural networks is the autoencoder
    family. As their name suggests, their sole purpose is to digest their input, and
    then reconstruct it back into its original shape. In other words, an autoencoder
    learns to copy its input to its output. Why? Because the side effect of this process
    is what we are after: not to produce a tag or classification, but to learn an
    efficient, high-quality representation of the images that have been passed to
    the autoencoder. The name of such a representation is **encoding**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How do they achieve this? By training two networks in tandem: an **encoder**,
    which takes images and produces the encoding, and a **decoder**, which takes the
    encoding and tries to reconstruct the input from its information.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover the basics, starting with a simple fully connected
    implementation of an autoencoder. Later, we'll create a more common and versatile
    convolutional autoencoder. We will also learn how to apply autoencoders in more
    practical contexts, such as denoising images, detecting outliers in a dataset,
    and creating an inverse image search index. Sound interesting?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple fully connected autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a convolutional autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denoising images with autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spotting outliers using autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an inverse image search index with deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a variational autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although using a GPU is always a good idea, some of these recipes (especially
    *Creating a simple fully connected autoencoder*) work well with a mid-tier CPU,
    such as an Intel i5 or i7\. If any particular recipe depends on external resources
    or requires preparatory steps, you''ll find specific preparation instructions
    in the *Getting ready* section. You can promptly access all the code for this
    chapter here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3qrHYaF](https://bit.ly/3qrHYaF).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple fully connected autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Autoencoders** are unusual in their design, as well as in terms of their
    functionality. That''s why it''s a great idea to master the basics of implementing,
    perhaps, the simplest version of an autoencoder: a fully connected one.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll implement a fully connected autoencoder to reconstruct
    the images in `Fashion-MNIST`, a standard dataset that requires minimal preprocessing,
    allowing us to focus on the autoencoder itself.
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready? Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fortunately, `Fashion-MNIST` comes bundled with TensorFlow, so we don't need
    to download it on our own.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use `OpenCV`, a famous computer vision library, to create a mosaic so
    that we can compare the original images with the ones reconstructed by the autoencoder.
    You can install `OpenCV` effortlessly with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that all the preparations have been handled, let's take a look at the recipe!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps, to implement a simple yet capable autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages to implement the fully connected autoencoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will build the autoencoder''s architecture. By default,
    the encoding or latent vector dimension is *128*, but *16*, *32*, and *64* are
    good values too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will plot a sample of general images against their original
    counterparts, in order to visually assess the autoencoder''s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous block selects 15 random indices, which we''ll use to pick the
    same sample images from the `original` and `generated` batches. Next, let''s define
    an inner function so that we can stack a sample of 15 images in a 3x5 grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define another inner function so that we can add text on top of an image.
    This will be useful for distinguishing the generated images from the originals,
    as we''ll see shortly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wrap up this function by selecting the same images from the original and generated
    groups. Then, stack both groups together to form a mosaic, resize it so that it''s
    860x860 in size, label the original and generated tiles in the mosaic using `add_text()`,
    and display the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download (or load, if cached) `Fashion-MNIST`. Because this is not a classification
    problem, we are only keeping the images, not the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the images into vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the autoencoder and compile it. We''ll use `''adam''` as the optimizer
    and mean squared Error (`''mse''`) as the loss function. Why? We''re not interested
    in getting the classification right but reconstructing the input as closely as
    possible, which translates into minimizing the overall error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the autoencoder over 300 epochs, a figure high enough to allow the network
    to learn a good representation of the input. To speed up the training process
    a bit, we''ll pass batches of `1024` vectors at a time (feel free to change the
    batch size based on your hardware capabilities). Notice how the input features
    are also the labels or targets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make predictions on the test set (basically, generate copies of the test vectors):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the predictions and test vectors back to grayscale images of dimensions
    28x28x1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a comparative plot of the original images against the ones produced
    by the autoencoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Mosaic of the original images (top three rows) compared with
    the'
  prefs: []
  type: TYPE_NORMAL
- en: generated ones (bottom three rows)](img/B14768_05_001.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Mosaic of the original images (top three rows) compared with the
    generated ones (bottom three rows)
  prefs: []
  type: TYPE_NORMAL
- en: Judging by the results, our autoencoder did a pretty decent job. In all cases,
    the shape of the clothing items is well-preserved. However, it isn't as accurate
    at reconstructing the inner details, as shown by the T-shirt in the sixth row,
    fourth column, where the horizontal stripe in the original is missing in the produced
    copy.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we learned that autoencoders work by combining two networks
    into one: the encoder and the decoder. In the `build_autoencoder()` function,
    we implemented a fully connected autoencoding architecture, where the encoder
    portion takes a 784-element vector and outputs an encoding of 128 numbers. Then,
    the decoder picks up this encoding and expands it through several stacked dense
    (fully connected) layers, where the last one creates a 784-element vector (the
    same dimensions that the input contains).'
  prefs: []
  type: TYPE_NORMAL
- en: The training process thus consists of minimizing the distance or error between
    the input the encoder receives and the output the decoder produces. The only way
    to achieve this is to learn encodings that minimize the information loss when
    compressing the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the loss function (in this case, `MSE`) is a good measure to see if
    the autoencoder is progressing in its learning, with these particular networks,
    visual verification is just as relevant, if not more. That''s why we implemented
    the `plot_original_vs_generated()` function: to check that the copies look like
    their original counterparts.'
  prefs: []
  type: TYPE_NORMAL
- en: Why don't you try changing the encoding size? How does it affect the quality
    of the copies?
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you''re wondering why `Fashion-MNIST` exists at all, take a look at the
    official repository here: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a convolutional autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with regular neural networks, when it comes to images, using convolutions
    is usually the way to go. In the case of autoencoders, this is no different. In
    this recipe, we'll implement a convolutional autoencoder to reproduce images from
    `Fashion-MNIST`.
  prefs: []
  type: TYPE_NORMAL
- en: The distinguishing factor is that in the decoder, we'll use reverse or transposed
    convolutions, which upscale volumes instead of downscaling them. This is what
    happens in traditional convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: This is an interesting recipe. Are you ready to begin?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because there are convenience functions in TensorFlow for downloading `Fashion-MNIST`,
    we don''t need to do any manual preparations on the data side. However, we must
    install `OpenCV` so that we can visualize the outputs of the autoencoder. This
    can be done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Without further ado, let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to implement a fully functional convolutional autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `build_autoencoder()` function, which internally builds the autoencoder
    architecture and returns the encoder, the decoder, and the autoencoder itself.
    Start defining the input and the first set of 32 convolutional filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the second set of convolutions (64 this time):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the output layers of the encoder:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In *Step 2*, we defined the encoder model, which is a regular convolutional
    neural network. The next block defines the decoder model, starting with the input
    and 64 transposed convolution filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the second set of transposed convolutions (32 this time):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the output layer of the decoder:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The decoder uses `Conv2DTranspose` layers, which expand their inputs to generate
    larger output volumes. Notice that the further we go into the decoder, the fewer
    filters the `Conv2DTranspose` layers use. Finally, define the autoencoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The autoencoder is the end-to-end architecture. This starts with the input layer,
    which goes into the encoder, and ends with an output layer, which is the result
    of passing the encoder's output through the decoder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function that will plot a sample of general images against their original
    counterparts. This will help us visually assess the autoencoder''s performance.
    (This is the same function we defined in the previous recipe. For a more complete
    explanation, refer to the *Creating a simple fully connected autoencoder* recipe
    of this chapter.) Take a look at the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an inner helper function in order to stack a sample of images in a 3x5
    grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define a function that will put text on an image in a given position:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, create a mosaic containing both the original and generated images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download (or load, if cached) `Fashion-MNIST`. We are only interested in the
    images; therefore, we can drop the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the images and add a channel dimension to them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we are only interested in the autoencoder, so we''ll ignore the other
    two return values of the `build_autoencoder()` function. However, in different
    circumstances, we could want to keep them. We''ll train the model using `''adam''`
    and use `''mse''` as the loss function since we want to reduce the error, not
    optimize for classification accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the autoencoder over 300 epochs, in batches of 512 images at a time.
    Notice how the input images are also the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make copies of the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape both the predictions and the test images back to 28x28 (no channel
    dimension):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a comparative mosaic of the original images and the copies outputted
    by the autoencoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Mosaic of the original images (top three rows), compared with'
  prefs: []
  type: TYPE_NORMAL
- en: those produced by the convolutional autoencoder (bottom three rows)
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_05_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Mosaic of the original images (top three rows), compared with those
    produced by the convolutional autoencoder (bottom three rows)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the autoencoder has learned a good encoding, which allowed it
    to reconstruct the input images with minimal detail loss. Let's head over to the
    next section to understand how it works!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we learned that a convolutional autoencoder is one of the most
    common yet powerful members of this family of neural networks. The encoder portion
    of the architecture is a regular convolutional neural network that relies on convolutions
    and dense layers to downsize the output and produce a vector representation. The
    decoder is the interesting part because it has to deal with the converse problem:
    to reconstruct the input based on the synthesized feature vector, also known as
    an encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: How does it do this? By using a transposed convolution (`Conv2DTranspose`).
    Unlike traditional `Conv2D` layers, these produce shallower volumes (fewer filters),
    but they are wider and taller. The result is an output layer with only one filter,
    and 28x28 dimensions, which is the same shape as the input. Fascinating, isn't
    it?
  prefs: []
  type: TYPE_NORMAL
- en: The training process consists of minimizing the error between the output (the
    generated copies) and the input (the original images). Therefore, MSE is a fitting
    loss function because it provides us with this very information.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we assessed the performance of the autoencoder by visually inspecting
    a sample of test images, along with their synthetic counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In an autoencoder, the size of the encoding is crucial to guarantee the decoder
    has enough information to reconstruct the input.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s a great explanation of transposed convolutions: [https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba](https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba).'
  prefs: []
  type: TYPE_NORMAL
- en: Denoising images with autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using images to reconstruct their input is great, but are there more useful
    ways to apply autoencoders? Of course there are! One of them is image denoising.
    As the name suggests, this is the act of restoring damaged images by replacing
    the corrupted pixels and regions with sensible values.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll purposely damage the images in `Fashion-MNIST`, and then
    train an autoencoder to denoise them.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Fashion-MNIST` can easily be accessed using the convenience functions TensorFlow
    provides, so we don''t need to manually download the dataset. On the other hand,
    because we''ll be creating some visualizations using `OpenCV`, we must install
    it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to implement a convolutional autoencoder capable of restoring
    damaged images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `build_autoencoder()` function, which creates the corresponding
    neural architecture. Notice that this is the same architecture we implemented
    in the previous recipe; therefore, we won''t go into too much detail here. For
    an in-depth explanation, please refer to the *Creating a convolutional autoencoder*
    recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we''ve created the encoder model, let''s create the decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, define the autoencoder itself and return the three models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `plot_original_vs_generated()` function, which creates a comparative
    mosaic of the original and generated images. We''ll use this function later to
    show the noisy images and their restored counterparts. Similar to `build_autoencoder()`,
    this function works in the same way we defined it in the *Creating a simple fully
    connected autoencoder* recipe, so if you want a detailed explanation, please review
    that recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an inner helper function that will stack a sample of images in a 3x5
    grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will put custom text on top of an image, in a certain
    location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the mosaic with both the original and the generated images, label each
    sub-grid, and display the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load `Fashion-MNIST` using TensorFlow''s handy function. We will only keep
    the images since the labels are unnecessary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the images and add a single color channel to them using `np.expand_dims()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate two tensors with the same dimensions as `X_train` and `X_test`, respectively.
    These will correspond to random `0.5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Purposely damage both `X_train` and `X_test` by adding `train_noise` and `test_noise`,
    respectively. Make sure that the values remain between `0` and `1` using `np.clip()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the autoencoder and compile it. We''ll use `''adam''` as our optimizer
    and `''mse''` as our loss function, given that we''re interested in reducing the
    error instead of improving accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model for `300` epochs, on batches of `1024` noisy images at a time.
    Notice that the features are the noisy images, while the labels or targets are
    the original ones, prior to being damaged:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make predictions with the trained model. Reshape both the noisy and generated
    images back to 28x28, and scale them up to the [0, 255] range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, display the mosaic of noisy versus restored images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Mosaic of noisy images (top) versus the ones restored by the
    network (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_05_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Mosaic of noisy images (top) versus the ones restored by the network
    (bottom)
  prefs: []
  type: TYPE_NORMAL
- en: Look how damaged the images at the top are! The good news is that, in most instances,
    the autoencoder did a good job of restoring them. However, it couldn't denoise
    the images closer to the edges of the mosaic properly, which is a sign that more
    experimentation can be done to improve their performance (to be fair, these bad
    examples are hard to discern, even for humans).
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The novelty in this recipe is the practical use of the convolutional autoencoder.
    Both the network and other building blocks have been covered in depth in the last
    two recipes, so let's focus on the denoising problem itself.
  prefs: []
  type: TYPE_NORMAL
- en: To recreate a real-life scenario of damaged images, we added a heavy amount
    of Gaussian noise to both the training and test sets in the `Fashion-MNIST` dataset.
    This kind of noise is known as salt and pepper because the damaged image looks
    as though it had these seasonings spilled all over it.
  prefs: []
  type: TYPE_NORMAL
- en: To teach our autoencoder how the images once looked, we used the noisy ones
    as the features and the originals as the target or labels. This way, after 300
    epochs, the network learned an encoding capable of, on many occasions, mapping
    salt and peppered instances to satisfyingly restored versions of them.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, the model is not perfect, as we saw in the mosaic, where the network
    was unable to restore the images at the edges of the grid. This is a demonstration
    of how difficult repairing a damaged image can be.
  prefs: []
  type: TYPE_NORMAL
- en: Spotting outliers using autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another great application of autoencoders is outlier detection. The idea behind
    this use case is that the autoencoder will learn an encoding with a very small
    error for the most common classes in a dataset, while its ability to reproduce
    scarcely represented categories (outliers) will be much more error-prone.
  prefs: []
  type: TYPE_NORMAL
- en: With this premise in mind, in this recipe, we'll rely on a convolutional autoencoder
    to detect outliers in a subsample of `Fashion-MNIST`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install `OpenCV`, use the following `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We'll rely on TensorFlow's built-in convenience functions to load the `Fashion-MNIST`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set a random seed to guarantee reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will build the autoencoder architecture. This function
    follows the same structure we studied in the *Creating a convolutional autoencoder*
    recipe, so if you want a deeper explanation, please go back to that recipe. Let''s
    start by creating the encoder model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, build the decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, build the autoencoder and return the three models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define a function that will contrive a dataset of two classes, where
    one of them represents an anomaly or outlier. Start by selecting the instances
    corresponding to the two classes of interest, and then shuffle them to break any
    possible ordering bias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, from the anomalous category, select a number of instances proportional
    to `corruption_proportion`. Finally, create the final dataset by merging the regular
    instances with the outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load `Fashion-MNIST`. Merge both the train and test sets into a single dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the regular and anomalous labels, and then create the anomalous dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a channel dimension to the dataset, normalize it, and divide it into 80%
    for training and 20% for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the autoencoder and compile it. We''ll use `''adam''` as the optimizer
    and `''mse''` as the loss function since this gives us a good measure of the model''s
    error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the autoencoder for 300 epochs, on batches of `1024` images at a time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make predictions on the data to find the outliers. We''ll compute the mean
    squared error between the original image and the one produced by the autoencoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the indices of the images with errors greater than the 99.9% quantile.
    These will be our outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save a comparative image of the original and generated images for each outlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s an example of an outlier:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Left: Original outlier. Right: Reconstructed image.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_05_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.4 – Left: Original outlier. Right: Reconstructed image.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we can harness the knowledge stored in the encoding learned by
    the autoencoder to easily detect anomalous or uncommon images in a dataset. We'll
    look at this in more detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea behind this recipe is very simple: outliers, by definition, are rare
    occurrences of an event or class within a dataset. Therefore, when we train an
    autoencoder on a dataset that contains outliers, it won''t have sufficient time
    nor examples to learn a proper representation of them.'
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the low confidence (in other words, the high error) the network
    will display when reconstructing anomalous images (in this example, T-shirts),
    we can select the worst copies in order to spot outliers.
  prefs: []
  type: TYPE_NORMAL
- en: However, for this technique to work, the autoencoder must be great at reconstructing
    the regular classes (for instance, sandals); otherwise, the false positive rate
    will be too high.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an inverse image search index with deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because the whole point of an autoencoder is to learn an encoding or a low-dimensional
    representation of a set of images, they make for great feature extractors. Furthermore,
    we can use them as the perfect building blocks of image search indices, as we'll
    discover in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s install `OpenCV` with `pip`. We''ll use it to visualize the outputs
    of our autoencoder, in order to visually assess the effectiveness of the image
    search index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We'll start implementing the recipe in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create your own image search index:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `build_autoencoder()`, which instantiates the autoencoder. First, let''s
    assemble the encoder part:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to define the decoder portion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, build the autoencoder and return it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will compute the Euclidean distance between two vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `search()` function, which uses the search index (a dictionary of
    feature vectors paired with their corresponding images) to retrieve the most similar
    results to a query vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `Fashion-MNIST` dataset. Keep only the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the images and add a color channel dimension:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the autoencoder and compile it. We''ll use `''adam''` as the optimizer
    and `''mse''` as the loss function since this gives us a good measure of the model''s
    error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the autoencoder for 10 epochs, on batches of `512` images at a time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new model, which we''ll use as a feature extractor. It''ll receive
    the same inputs as the autoencoder and will output the encoding learned by the
    autoencoder. In essence, we are using the encoder part of the autoencoder to turn
    images into vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the search index, comprised of the feature vectors of `X_train`, along
    with the original images (which must be reshaped back to 28x28 and rescaled to
    the range [0, 255]):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the feature vectors of `X_test`, which we will use as our sample of
    query images. Also, reshape `X_test` to 28x28 and rescale its values to the range
    [0, 255]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select 16 random test images (with their corresponding feature vectors) to
    use as queries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform a search for each of the images in the test sample and save a side-to-side
    visual comparison of the test query, along with the results fetched from the index
    (which, remember, is comprised of the train data):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s an example of a search result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Left: Query image of a shoe. Right: The best 16 search results,
    all of which contain shoes too'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_05_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.5 – Left: Query image of a shoe. Right: The best 16 search results,
    all of which contain shoes too'
  prefs: []
  type: TYPE_NORMAL
- en: As the preceding image demonstrates, our image search index is a success! We'll
    see how it works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned how to leverage the distinguishing trait of an autoencoder,
    which is to learn an encoding that greatly compresses the information in the input
    images, resulting in minimal loss of information. Then, we used the encoder part
    of a convolutional autoencoder to extract the features of fashion item photos
    and construct an image search index.
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, using this index as a search engine is as easy as computing the
    Euclidean distance between a query vector (corresponding to a query image) and
    all the images in the index, selecting only those that are closest to the query.
  prefs: []
  type: TYPE_NORMAL
- en: The most important aspect in our solution is to train an autoencoder that is
    good enough to produce high-quality vectors, since they make or break the search
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The implementation is based on the great work of Dong *et al.*, whose paper
    can be read here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5/recipe5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5/recipe5).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a variational autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of the most modern and complex use cases of autoencoders are **Variational
    Autoencoders** (**VAEs**). They differ from the rest of the autoencoders in that,
    instead of learning an arbitrary function, they learn a probability distribution
    of the input images. We can then sample this distribution to produce new, unseen
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: A **VAE** is, in fact, a generative model, and in this recipe, we'll implement
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We don't need any special preparation for this recipe, so let's get started
    right away!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to learn how to implement and train a **VAE**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because we''ll be using the `tf.function` annotation soon, we must tell TensorFlow
    to run functions eagerly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a class that will encapsulate our implementation of the `self.z_log_var`
    and `self.z_mean` are the parameters of the latent Gaussian distribution that
    we''ll learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define some members that will store the inputs and outputs of the `encoder`,
    `decoder`, and `vae`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `build_vae()` method, which builds the variational autoencoder architecture
    (notice that we are using dense layers instead of convolutions):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice that the encoder is just a fully connected network that produces three
    outputs: `self.z_mean`, which is the mean of the Gaussian distribution we are
    training to model, `self.z_log_var`, which is the logarithmic variance of this
    distribution, and `z`, a sample point in that probability space. In order to generate
    the `z` simple, we must wrap a custom function, `sampling()` (implemented in *Step
    5*), in a `Lambda` layer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, define the decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The decoder is just another fully connected network. The decoder will take
    samples from the latent dimension in order to reconstruct the inputs. Finally,
    connect the encoder and decoder to create the **VAE** model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `train()` method, which trains the variational autoencoder. Therefore,
    it receives the train and test data, as well as the number of epochs and the batch
    size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the reconstruction loss as the MSE between the inputs and outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`kl_loss` is the `reconstruction_loss`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configure the `self.vae` model so that it uses `vae_loss` and `Adam()` as the
    optimizer (with a learning rate of 0.003). Then, fit the network over the specified
    number of epochs. Finally, return the three models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will generate a random sample or point from the latent
    space, given the two relevant parameters (passed in the `arguments` array); that
    is, `z_mean` and `z_log_var`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that `epsilon` is a random Gaussian vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function that will generate and plot images generated from the latent
    space. This will give us an idea of the **shapes** that are closer to the distribution,
    and the ones that are nearer to the tails of the curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a range of values that span from -4 to 4 in both the X and Y axes. We''ll
    use these to generate and visualize samples at each location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the decoder to generate a new sample for each combination of `z_mean` and
    `z_log_var`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the sample and place it in the corresponding cell in the grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the ticks and axes labels, and then display the plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `Fashion-MNIST` dataset. Normalize the images and add a color channel
    to them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate and build the **variational autoencoder**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the models for 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the decoder to generate new images and plot the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Visualization of the latent space learned by the VAE'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_05_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Visualization of the latent space learned by the VAE
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the collection of points that comprise the latent space and
    the corresponding clothing item for each of these points. This is a representation
    of the probability distribution the network learned, in which the item at the
    center of such a distribution resembles a T-shirt, while the ones at the edges
    look more like pants, sweaters, and shoes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned that a **variational autoencoder** is an advanced,
    more complex type of autoencoder that, instead of learning an arbitrary, vanilla
    function to map inputs to outputs, learns a probability distribution of the inputs.
    This gives it the ability to generate new, unseen images that make it a precursor
    of more modern generative models, such as **Generative Adversarial Networks**
    (**GANs**).
  prefs: []
  type: TYPE_NORMAL
- en: The architecture is not that different from the others autoencoder we studied
    in this chapter. The key to understanding the power of a `z`, which we generate
    using the `sampling()` function, within a Lambda layer.
  prefs: []
  type: TYPE_NORMAL
- en: This means that in each iteration, the whole network is optimizing the `z_mean`
    and `z_log_var` parameters so that it closely resembles the probability distribution
    of the inputs. It does this because it's the only way the random samples (`z`)
    are going to be of such high quality that the decoder will be able to generate
    better, more realistic outputs.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A key component we can use to tune the **VAE** is the **Kullback-Leibler**
    divergence, which you can read more about here: [https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that **VAE**s are the perfect runway to generative models, which we'll
    cover in depth in the next chapter!
  prefs: []
  type: TYPE_NORMAL
