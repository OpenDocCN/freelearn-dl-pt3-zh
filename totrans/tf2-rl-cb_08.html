<html><head></head><body>
		<div id="_idContainer101">
			<h1 id="_idParaDest-192"><em class="italic"><a id="_idTextAnchor221"/>Chapter 8</em>: <a id="_idTextAnchor222"/>Distributed Training for Accelerated Development of Deep RL Agents</h1>
			<p>Training Deep RL agents to solve a task takes enormous wall-clock time due to the high sample complexity. For real-world applications, iterating over agent training and testing cycles at a faster pace plays a crucial role in the market readiness of a Deep RL application. The recipes in this chapter provide instructions on how to speed up Deep RL agent development using the distributed training of deep neural network models by leveraging TensorFlow 2.x’s capabilities. Strategies for utilizing multiple CPUs and GPUs both on a single machine and across a cluster of machines are discussed. Multiple recipes for training distributed <strong class="bold">Deep Reinforcement Learning</strong> (<strong class="bold">Deep RL</strong>) agents using the <strong class="bold">Ray</strong>, <strong class="bold">Tune</strong>, and <strong class="bold">RLLib</strong> frameworks are also provided. </p>
			<p>Specifically, the following recipes are a part of this chapter:</p>
			<ul>
				<li>Building distributed deep learning models using TensorFlow 2.x – Multi-GPU training</li>
				<li>Scaling up and out – Multi-machine, multi-GPU training</li>
				<li>Training Deep RL agents at scale – Multi-GPU PPO agent</li>
				<li>Building blocks for distributed Deep Reinforcement Learning for accelerated training</li>
				<li>Large-scale Deep RL agent training using Ray, Tune, and RLLib</li>
			</ul>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor223"/>Technical requirements</h1>
			<p>The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04 and should work with later versions of Ubuntu if Python 3.6+ is available. With Python 3.6+ installed along with the necessary Python packages, as listed before the start of each of the recipes, the code should run fine on Windows and Mac OSX too. It is advised to create and use a Python virtual environment named <strong class="source-inline">tf2rl-cookbook</strong> to install the packages and run the code in this book. Miniconda or Anaconda installation for Python virtual environment management is recommended.</p>
			<p>The complete code for each recipe in each chapter will be available here: <a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook</a>.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor224"/>Distributed deep learning models using TensorFlow 2.x – Multi-GPU training</h1>
			<p>Deep RL utilizes a <a id="_idIndexMarker814"/>deep neural network <a id="_idIndexMarker815"/>for policy, value-function, or model representations. For higher-dimensional observation/state spaces, for example, in the case of image or image-like observations, it is typical to use <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) architectures. While CNNs are powerful and enable training Deep RL policies for vision-based control tasks, training deep CNNs requires a lot of time, especially in the RL setting. This recipe will help you understand how we can leverage TensorFlow 2.x’s distributed training APIs to train deep <strong class="bold">residual networks</strong> (<strong class="bold">ResNets</strong>) using multiple GPUs. The recipe comes with configurable building blocks that you can use to build Deep RL components like deep policy networks or value networks.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor225"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repo. Having access to a (local or cloud) machine with one or more GPUs will be beneficial for this recipe. We will <a id="_idIndexMarker816"/>be using the <strong class="bold">TensorFlow Datasets</strong> library in this recipe, which is available as a Python package named <strong class="source-inline">tensorflow_datasets</strong>. This should be already installed if you used <strong class="source-inline">tfrl-cookbook.yml</strong> to set up/update your conda environment.</p>
			<p>Now, let’s begin!</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor226"/>How to do it...</h2>
			<p>The implementation in this recipe is based on the latest official TensorFlow documentation/tutorial. The following steps will help you get a good command over TensorFlow 2.x’s distributed execution capabilities. We will be using a ResNet model as an example of a large <a id="_idIndexMarker817"/>model that will benefit from <a id="_idIndexMarker818"/>being trained in a distributed fashion, utilizing multiple GPUs to speed up training. We will discuss the code snippets for the main components for building a ResNet. Please refer to the <strong class="source-inline">resnet.py</strong> file in the cookbook’s code repository for the full and complete implementation. Let’s get started:</p>
			<ol>
				<li>Let’s jump right into the template for building residual neural networks:<p class="source-code">def resnet_block(</p><p class="source-code">    input_tensor, size, kernel_size, filters, stage, \</p><p class="source-code">     conv_strides=(2, 2), training=None</p><p class="source-code">):</p><p class="source-code">    x = conv_building_block(</p><p class="source-code">        input_tensor,</p><p class="source-code">        kernel_size,</p><p class="source-code">        filters,</p><p class="source-code">        stage=stage,</p><p class="source-code">        strides=conv_strides,</p><p class="source-code">        block="block_0",</p><p class="source-code">        training=training,</p><p class="source-code">    )</p><p class="source-code">    for i in range(size - 1):</p><p class="source-code">        x = identity_building_block(</p><p class="source-code">            x,</p><p class="source-code">            kernel_size,</p><p class="source-code">            filters,</p><p class="source-code">            stage=stage,</p><p class="source-code">            block="block_%d" % (i + 1),</p><p class="source-code">            training=training,</p><p class="source-code">        )</p><p class="source-code">    return x</p></li>
				<li>With the above template for a ResNet block, we can quickly build ResNets with multiple ResNet blocks. We will implement a ResNet with one ResNet block here in the book, <a id="_idIndexMarker819"/>and you will <a id="_idIndexMarker820"/>find the ResNet implemented with multiple configurable numbers and sizes of ResNet blocks in the code repository. Let’s get started and complete the ResNet implementation in the following several steps, focusing on one important concept at a time. First, let’s define the function signature:<p class="source-code">def resnet(num_blocks, img_input=None, classes=10, training=None):</p><p class="source-code">    """Builds the ResNet architecture using provided </p><p class="source-code">       config"""</p></li>
				<li>Next, let’s handle the channel ordering in the input image data representation. The most common ordering of the dimensions is either: <strong class="source-inline">batch_size</strong> x <strong class="source-inline">channels</strong> x <strong class="source-inline">width</strong> x <strong class="source-inline">height</strong> or <strong class="source-inline">batch_size</strong> x <strong class="source-inline">width</strong> x <strong class="source-inline">height</strong> x <strong class="source-inline">channels</strong>. We will handle these two cases:<p class="source-code">    if backend.image_data_format() == "channels_first":</p><p class="source-code">        x = layers.Lambda(</p><p class="source-code">            lambda x: backend.permute_dimensions(x, \</p><p class="source-code">                (0, 3, 1, 2)), name="transpose"</p><p class="source-code">        )(img_input)</p><p class="source-code">        bn_axis = 1</p><p class="source-code">    else:  # channel_last</p><p class="source-code">        x = img_input</p><p class="source-code">        bn_axis = 3</p></li>
				<li>Now, let’s <a id="_idIndexMarker821"/>apply zero padding <a id="_idIndexMarker822"/>to the input and apply initial layers to start processing:<p class="source-code">    x = tf.keras.layers.ZeroPadding2D(padding=(1, 1), \</p><p class="source-code">                                     name="conv1_pad")(x)</p><p class="source-code">    x = tf.keras.layers.Conv2D(16,(3, 3),strides=(1, 1),</p><p class="source-code">                         padding="valid",</p><p class="source-code">                         kernel_initializer="he_normal",</p><p class="source-code">                         kernel_regularizer= \</p><p class="source-code">                            tf.keras.regularizers.l2(</p><p class="source-code">                                 L2_WEIGHT_DECAY), </p><p class="source-code">                         bias_regularizer= \</p><p class="source-code">                             tf.keras.regularizers.l2(</p><p class="source-code">                                 L2_WEIGHT_DECAY), </p><p class="source-code">                                                        name="conv1",)(x)</p><p class="source-code">    x = tf.keras.layers.BatchNormalization(axis=bn_axis,</p><p class="source-code">             name="bn_conv1", momentum=BATCH_NORM_DECAY,</p><p class="source-code">             epsilon=BATCH_NORM_EPSILON,)\</p><p class="source-code">                  (x, training=training)</p><p class="source-code">    x = tf.keras.layers.Activation("relu")(x)</p></li>
				<li>It’s time to add the ResNet blocks using the <strong class="source-inline">resnet_block</strong> function we created:<p class="source-code">    x = resnet_block(x, size=num_blocks, kernel_size=3,</p><p class="source-code">        filters=[16, 16], stage=2, conv_strides=(1, 1),</p><p class="source-code">        training=training,)</p><p class="source-code">    x = resnet_block(x, size=num_blocks, kernel_size=3,</p><p class="source-code">        filters=[32, 32], stage=3, conv_strides=(2, 2),</p><p class="source-code">        training=training)</p><p class="source-code">    x = resnet_block(x, size=num_blocks, kernel_size=3,</p><p class="source-code">        filters=[64, 64], stage=4, conv_strides=(2, 2),</p><p class="source-code">        training=training,)</p></li>
				<li>As the final layer, we want to add a <strong class="source-inline">softmax</strong> activated <strong class="source-inline">Dense</strong> (fully connected) layer with <a id="_idIndexMarker823"/>the number of <a id="_idIndexMarker824"/>nodes equal to the number of output classes needed for the task:<p class="source-code">x = tf.keras.layers.GlobalAveragePooling2D(</p><p class="source-code">                                     name="avg_pool")(x)</p><p class="source-code">    x = tf.keras.layers.Dense(classes,</p><p class="source-code">        activation="softmax",</p><p class="source-code">        kernel_initializer="he_normal",</p><p class="source-code">        kernel_regularizer=tf.keras.regularizers.l2(</p><p class="source-code">             L2_WEIGHT_DECAY), </p><p class="source-code">        bias_regularizer=tf.keras.regularizers.l2(</p><p class="source-code">             L2_WEIGHT_DECAY), </p><p class="source-code">        name="fc10",)(x)</p></li>
				<li>The last step in the ResNet model building function is to wrap the layers as a TensorFlow 2.x Keras model and return the output:<p class="source-code">    inputs = img_input</p><p class="source-code">    # Create model.</p><p class="source-code">    model = tf.keras.models.Model(inputs, x, name=f"resnet{6 * num_blocks + 2}")</p><p class="source-code">    return model</p></li>
				<li>Using the ResNet function that we just discussed, it becomes quite easy to build deep residual networks of varying layer depths by simply changing the number of blocks. For example, the following is possible:<p class="source-code">resnet_mini = functools.partial(resnet, num_blocks=1)</p><p class="source-code">resnet20 = functools.partial(resnet, num_blocks=3)</p><p class="source-code">resnet32 = functools.partial(resnet, num_blocks=5)</p><p class="source-code">resnet44 = functools.partial(resnet, num_blocks=7)</p><p class="source-code">resnet56 = functools.partial(resnet, num_blocks=9)</p></li>
				<li>With our model defined, we can jump to the multi-GPU training code. The remaining steps in this recipe will guide you through the implementation that will allow you <a id="_idIndexMarker825"/>to speed up <a id="_idIndexMarker826"/>training the ResNet using all the available GPUs on a machine. Let’s start by importing the <strong class="source-inline">ResNet</strong> module that we built along with the <strong class="source-inline">tensorflow_datasets</strong> module:<p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">if "." not in sys.path:</p><p class="source-code">    sys.path.insert(0, ".")</p><p class="source-code">import resnet</p></li>
				<li>We can now choose which dataset we want to use to exercise our distributed training pipeline. For this recipe, we will use the <strong class="source-inline">dmlab</strong> dataset that contains images typically observed by RL agents acting in the DeepMind Lab environment. Depending on the compute capabilities of the GPUs, RAM, and CPUs on your training machine, you may want to use a smaller dataset such as <strong class="source-inline">CIFAR10</strong>:<p class="source-code">dataset_name = "dmlab"  # "cifar10" or "cifar100"; See tensorflow.org/datasets/catalog for complete list</p><p class="source-code"># NOTE: dmlab is large in size; Download bandwidth and # GPU memory to be considered</p><p class="source-code">datasets, info = tfds.load(name="dmlab", with_info=True,</p><p class="source-code">                           as_supervised=True)</p><p class="source-code">dataset_train, dataset_test = datasets["train"], \</p><p class="source-code">                              datasets["test"]</p><p class="source-code">input_shape = info.features["image"].shape</p><p class="source-code">num_classes = info.features["label"].num_classes</p></li>
				<li>The next step needs your full attention! We are going to choose the distributed execution strategy. TensorFlow 2.x has wrapped a lot of functionality into a simple API <a id="_idIndexMarker827"/>call like the one listed here:<p class="source-code">strategy = tf.distribute.MirroredStrategy()</p><p class="source-code">print(f"Number of devices: {</p><p class="source-code">           strategy.num_replicas_in_sync}")</p></li>
				<li>We will <a id="_idIndexMarker828"/>declare the key hyperparameters in this step that you can adjust depending on your machine’s hardware (such as RAM and GPU memory):<p class="source-code">num_train_examples = info.splits["train"].num_examples</p><p class="source-code">num_test_examples = info.splits["test"].num_examples</p><p class="source-code">BUFFER_SIZE = 1000  # Increase as per available memory</p><p class="source-code">BATCH_SIZE_PER_REPLICA = 64</p><p class="source-code">BATCH_SIZE = BATCH_SIZE_PER_REPLICA * \</p><p class="source-code">                  strategy.num_replicas_in_sync</p></li>
				<li>Before we start preparing the datasets, let’s implement a preprocessing function that performs operations before we pass the images to the neural network. You can add your own custom preprocessing operations. In this recipe, we will only need to cast the image data to <strong class="source-inline">float32</strong> first and then convert the image pixel value ranges to be [0, 1] rather than the typical interval of [0, 255]:<p class="source-code">def preprocess(image, label):</p><p class="source-code">    image = tf.cast(image, tf.float32)</p><p class="source-code">    image /= 255</p><p class="source-code">    return image, label</p></li>
				<li>We are <a id="_idIndexMarker829"/>ready to create the dataset splits for training and validation/testing:<p class="source-code">train_dataset = (</p><p class="source-code">    dataset_train.map(preprocess).cache().\</p><p class="source-code">        shuffle(BUFFER_SIZE).batch(BATCH_SIZE)</p><p class="source-code">)</p><p class="source-code">eval_dataset = dataset_test.map(preprocess).batch(</p><p class="source-code">                                             BATCH_SIZE)</p></li>
				<li>We are <a id="_idIndexMarker830"/>at the crucial step of this recipe! Let’s instantiate and compile our model within the scope of the distributed strategy:<p class="source-code">with strategy.scope():</p><p class="source-code">    # model = create_model()</p><p class="source-code">    model = create_model("resnet_mini")</p><p class="source-code">    tf.keras.utils.plot_model(model, </p><p class="source-code">                             to_file="./slim_resnet.png", </p><p class="source-code">                             show_shapes=True)</p><p class="source-code">    model.compile(</p><p class="source-code">        loss=\</p><p class="source-code">          tf.keras.losses.SparseCategoricalCrossentropy(</p><p class="source-code">              from_logits=True),</p><p class="source-code">        optimizer=tf.keras.optimizers.Adam(),</p><p class="source-code">        metrics=["accuracy"],</p><p class="source-code">    )</p></li>
				<li>Let’s also <a id="_idIndexMarker831"/>create callbacks <a id="_idIndexMarker832"/>for logging to TensorBoard and checkpointing our model parameters during training:<p class="source-code">checkpoint_dir = "./training_checkpoints"</p><p class="source-code">checkpoint_prefix = os.path.join(checkpoint_dir, </p><p class="source-code">                                 "ckpt_{epoch}")</p><p class="source-code">callbacks = [</p><p class="source-code">    tf.keras.callbacks.TensorBoard(</p><p class="source-code">        log_dir="./logs", write_images=True, \</p><p class="source-code">        update_freq="batch"</p><p class="source-code">    ),</p><p class="source-code">    tf.keras.callbacks.ModelCheckpoint(</p><p class="source-code">        filepath=checkpoint_prefix, \</p><p class="source-code">        save_weights_only=True</p><p class="source-code">    ),</p><p class="source-code">]</p></li>
				<li>With that, we have everything needed to train our model using the distributed strategy. With Keras’s user-friendly <strong class="source-inline">fit()</strong> API, it is as simple as the following:<p class="source-code">model.fit(train_dataset, epochs=12, callbacks=callbacks)</p></li>
				<li>When the preceding line is executed, the training process will start. We can also manually save the model using the following lines:<p class="source-code">path = "saved_model/"</p><p class="source-code">model.save(path, save_format="tf")</p></li>
				<li>Once we have a saved checkpoint, it is easy to load the weights and start evaluating the model:<p class="source-code">model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))</p><p class="source-code">eval_loss, eval_acc = model.evaluate(eval_dataset)</p><p class="source-code">print("Eval loss: {}, Eval Accuracy: {}".format(eval_loss, eval_acc))</p></li>
				<li>To verify that the trained model using the distributed strategy works with and without <a id="_idIndexMarker833"/>replication, we will <a id="_idIndexMarker834"/>load it using two different methods in the following steps and evaluate. First, let’s load the model without replicating using the (same) strategy we used to train the model:<p class="source-code">unreplicated_model = tf.keras.models.load_model(path)</p><p class="source-code">unreplicated_model.compile(</p><p class="source-code">    loss=tf.keras.losses.\</p><p class="source-code">         SparseCategoricalCrossentropy(from_logits=True),</p><p class="source-code">    optimizer=tf.keras.optimizers.Adam(),</p><p class="source-code">    metrics=["accuracy"],</p><p class="source-code">)</p><p class="source-code">eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)</p><p class="source-code">print("Eval loss: {}, Eval Accuracy: {}".format(eval_loss, eval_acc))</p></li>
				<li>Next, let’s load the model within the distributed execution strategy’s scope, which would create replicas and evaluate the model:<p class="source-code">with strategy.scope():</p><p class="source-code">    replicated_model = tf.keras.models.load_model(path)</p><p class="source-code">    replicated_model.compile(</p><p class="source-code">        loss=tf.keras.losses.\</p><p class="source-code">         SparseCategoricalCrossentropy(from_logits=True),</p><p class="source-code">        optimizer=tf.keras.optimizers.Adam(),</p><p class="source-code">        metrics=["accuracy"],</p><p class="source-code">    )</p><p class="source-code">    eval_loss, eval_acc = \</p><p class="source-code">        replicated_model.evaluate(eval_dataset)</p><p class="source-code">    print("Eval loss: {}, \</p><p class="source-code">          Eval Accuracy: {}".format(eval_loss, eval_acc))</p><p>When <a id="_idIndexMarker835"/>you execute the preceding <a id="_idIndexMarker836"/>two code blocks, you will notice that both the methods result in the same evaluation accuracy, which is a good sign and signifies that we can use the model for prediction without any constraints on the execution strategy!</p></li>
				<li>That completes our recipe. Let’s recap and look at how the recipe works.</li>
			</ol>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor227"/>How it works...</h2>
			<p>A residual block in a neural network architecture applies convolution filters followed by multiple identity blocks. Specifically, a convolution block is applied once, followed by (size - 1) identity blocks where size is an integer representing the number of constituent convolutional-identity blocks. The identity block implements the shortcut or skip connections for the inputs to go through without being filtered through convolution operators. The convolutional block implements convolution layers followed by batch-normalization activation, followed by one or more sets of convolution-batchnorm-activation layers. The <strong class="source-inline">resnet</strong> module we built uses these convolution and identity building blocks to build a full ResNet with varying sizes that can be configured by simply changing the number of blocks. The size of the network is calculated as <strong class="source-inline">6 * num_blocks + 2</strong>.</p>
			<p>Once our ResNet model was ready, we used the <strong class="source-inline">tensorflow_datasets</strong> module to generate training and validation datasets. The TensorFlow Datasets module offers several popular datasets, such as CIFAR10, CIFAR100, and DMLAB, that have images and the associated <a id="_idIndexMarker837"/>labels for classification tasks. The list of all the available datasets can be found here: <a href="https://tensorflow.org/datasets/catalog">https://tensorflow.org/datasets/catalog</a>.</p>
			<p>In this recipe, we used the Mirrored Strategy for distributed execution using <strong class="source-inline">tf.distribute.MirroredStrategy</strong>, which enables synchronous distributed training using multiple replicas on one machine. Even with distributed execution with multiple replicas, we saw that the usual logging and checkpointing using callbacks worked as expected. We also verified that loading a saved model and running inference for evaluation works both with and without replication, making it portable without any added constraints just because the training utilized a distributed execution strategy!</p>
			<p>It’s time to advance to the next recipe!</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor228"/>Scaling up and out – Multi-machine, multi-GPU training</h1>
			<p>To reach the highest scale in terms of the distributed training of deep learning-based models, we need <a id="_idIndexMarker838"/>the capability to leverage compute resources across <a id="_idIndexMarker839"/>GPUs and across machines. This can significantly reduce the time it takes to iterate over or develop new models and architectures for the problem <a id="_idIndexMarker840"/>you are trying to solve. With easy access to <a id="_idIndexMarker841"/>cloud computing services such as Microsoft Azure, Amazon AWS, and Google’s GCP, renting multiple GPU-equipped machines for an hourly rate has become easier and much more common. It is also more economical than setting up and maintaining your own multi-GPU multi-machine node. This recipe will provide a quick walk-through of training deep models using TensorFlow 2.x’s multi-worker mirrored distributed execution strategy based on the official documentation, which you can use and easily customize for your use cases. For the multi-machine, multi-GPU distributed training example in this recipe, we will train a deep residual network (ResNet or resnet) for typical image classification tasks. The same network architecture can be used by RL agents for their policy or value-function representation with a slight modification to the output layer, as we will see in the later recipes of this chapter.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor229"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repo. To exercise the <a id="_idIndexMarker842"/>distributed training pipeline, it is recommended to set up a cluster <a id="_idIndexMarker843"/>with two or more machines equipped with GPUs either locally or on a cloud instance such as Azure, AWS, or GCP. While the training script we will implement can utilize multiple machines in a cluster, it is not absolutely necessary to have a cluster set up, although it is encouraged.</p>
			<p>Now, let’s begin!</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor230"/>How to do it...</h2>
			<p>Since this <a id="_idIndexMarker844"/>distributed training setup involves multiple machines, we <a id="_idIndexMarker845"/>need a communication interface between the machines and a way to address the individual machines. This is typically done using the existing network infrastructure and IP address:</p>
			<ol>
				<li value="1">Let’s begin by setting up a configuration parameter describing the cluster where we would like to train the models. The following code block is commented out so that you can edit and uncomment based on your cluster setup or leave it commented if you want to simply try it out on a single machine setup:<p class="source-code"># Uncomment the following lines and fill worker details </p><p class="source-code"># based on your cluster configuration</p><p class="source-code"># tf_config = {</p><p class="source-code">#    "cluster": {"worker": ["1.2.3.4:1111", </p><p class="source-code">                 "localhost:2222"]},</p><p class="source-code">#    "task": {"index": 0, "type": "worker"},</p><p class="source-code"># }</p><p class="source-code"># os.environ["TF_CONFIG"] = json.dumps(tf_config)</p></li>
				<li>To leverage multi-machine setups, we will use TensorFlow 2.x’s <strong class="source-inline">MultiWorkerMirroredStrategy</strong>:<p class="source-code">strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()</p></li>
				<li>Next, let’s <a id="_idIndexMarker846"/>declare the basic hyperparameters for <a id="_idIndexMarker847"/>the training. Feel free to adjust the batch <a id="_idIndexMarker848"/>sizes and the <strong class="source-inline">NUM_GPUS</strong> values as per your <a id="_idIndexMarker849"/>cluster/computer configuration:<p class="source-code">NUM_GPUS = 2</p><p class="source-code">BS_PER_GPU = 128</p><p class="source-code">NUM_EPOCHS = 60</p><p class="source-code">HEIGHT = 32</p><p class="source-code">WIDTH = 32</p><p class="source-code">NUM_CHANNELS = 3</p><p class="source-code">NUM_CLASSES = 10</p><p class="source-code">NUM_TRAIN_SAMPLES = 50000</p><p class="source-code">BASE_LEARNING_RATE = 0.1</p></li>
				<li>To prepare the dataset, let’s implement two quick functions for normalizing and augmenting the input images:<p class="source-code">def normalize(x, y):</p><p class="source-code">    x = tf.image.per_image_standardization(x)</p><p class="source-code">    return x, y</p><p class="source-code">def augmentation(x, y):</p><p class="source-code">    x = tf.image.resize_with_crop_or_pad(x, HEIGHT + 8, </p><p class="source-code">                                         WIDTH + 8)</p><p class="source-code">    x = tf.image.random_crop(x, [HEIGHT, WIDTH, </p><p class="source-code">                                 NUM_CHANNELS])</p><p class="source-code">    x = tf.image.random_flip_left_right(x)</p><p class="source-code">    return x, y</p></li>
				<li>For the sake <a id="_idIndexMarker850"/>of simplicity and faster convergence, we will <a id="_idIndexMarker851"/>stick with the CIFAR10 dataset as per the official <a id="_idIndexMarker852"/>TensorFlow 2.x sample for training, but feel <a id="_idIndexMarker853"/>free to choose a different dataset when you explore. Once you choose the dataset, we can generate the training and the testing sets:<p class="source-code">(x, y), (x_test, y_test) = \</p><p class="source-code">      keras.datasets.cifar10.load_data()</p><p class="source-code">train_dataset = tf.data.Dataset.from_tensor_slices((x,y))</p><p class="source-code">test_dataset = \</p><p class="source-code">    tf.data.Dataset.from_tensor_slices((x_test, y_test))</p></li>
				<li>To make the training results reproducible, we will use a fixed random seed to shuffle the dataset:<p class="source-code">tf.random.set_seed(22)</p></li>
				<li>We are not ready to generate the training and validation/testing dataset. We will shuffle the dataset using the known and fixed random seed declared in the previous step and apply the augmentation to the training set:<p class="source-code">train_dataset = (</p><p class="source-code">    train_dataset.map(augmentation)</p><p class="source-code">    .map(normalize)</p><p class="source-code">    .shuffle(NUM_TRAIN_SAMPLES)</p><p class="source-code">    .batch(BS_PER_GPU * NUM_GPUS, drop_remainder=True)</p><p class="source-code">)</p></li>
				<li>Similarly, we will prepare the test dataset but we do not want to apply random cropping to <a id="_idIndexMarker854"/>the test images! So, we will skip the augmentation <a id="_idIndexMarker855"/>and use the normalization step for preprocessing:<p class="source-code">test_dataset = test_dataset.map(normalize).batch(</p><p class="source-code">    BS_PER_GPU * NUM_GPUS, drop_remainder=True</p><p class="source-code">)</p></li>
				<li>Before we <a id="_idIndexMarker856"/>can start training, we need to create an instance of <a id="_idIndexMarker857"/>an optimizer and also prepare the input layer. Feel free to use a different optimizer, such as Adam, as per the needs of your task:<p class="source-code">opt = keras.optimizers.SGD(learning_rate=0.1, </p><p class="source-code">                           momentum=0.9)</p><p class="source-code">input_shape = (HEIGHT, WIDTH, NUM_CHANNELS)</p><p class="source-code">img_input = tf.keras.layers.Input(shape=input_shape)</p></li>
				<li>Finally, we are ready to construct the model instance within the scope of the <strong class="source-inline">MultiMachineMirroredStrategy</strong>:<p class="source-code">with strategy.scope():</p><p class="source-code">    model = resnet.resnet56(img_input=img_input, </p><p class="source-code">                            classes=NUM_CLASSES)</p><p class="source-code">    model.compile(</p><p class="source-code">        optimizer=opt,</p><p class="source-code">        loss="sparse_categorical_crossentropy",</p><p class="source-code">        metrics=["sparse_categorical_accuracy"],</p><p class="source-code">    )</p></li>
				<li>To train <a id="_idIndexMarker858"/>the <a id="_idIndexMarker859"/>model, we use the simple but powerful Keras API:<p class="source-code">model.fit(train_dataset, epochs=NUM_EPOCHS)</p></li>
				<li>Once <a id="_idIndexMarker860"/>the model <a id="_idIndexMarker861"/>is trained, we easily save, load, and evaluate:<p class="source-code">
# 12.1 Save</p><p class="source-code">model.save(path, save_format="tf")</p><p class="source-code"># 12.2 Load</p><p class="source-code">loaded_model = tf.keras.models.load_model(path)</p><p class="source-code">loaded_model.compile(</p><p class="source-code">    loss=tf.keras.losses.\</p><p class="source-code">        SparseCategoricalCrossentropy(from_logits=True),</p><p class="source-code">    optimizer=tf.keras.optimizers.Adam(),</p><p class="source-code">    metrics=["accuracy"],</p><p class="source-code">)</p><p class="source-code"># 12.3 Evaluate</p><p class="source-code">eval_loss, eval_acc = loaded_model.evaluate(eval_dataset)</p></li>
			</ol>
			<p>That completes our recipe implementation! Let’s summarize what we implemented and how it works in the next section. </p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor231"/>How it works...</h2>
			<p>For any distributed training runs with TensorFlow 2.x, the <strong class="source-inline">TF_CONFIG</strong> environment variable needs <a id="_idIndexMarker862"/>to be set on each of the (virtual) machines on <a id="_idIndexMarker863"/>your cluster. These configuration values inform each of the machines about the role and the training information each of the nodes will need to perform its job. You can read more about the details of <strong class="bold">TF_CONFIG</strong> configurations used by TensorFlow 2.x’s distributed training here: <a href="https://cloud.google.com/ai-platform/training/docs/distributed-training-details">https://cloud.google.com/ai-platform/training/docs/distributed-training-details</a>.</p>
			<p>We used TensorFlow 2.x’s <strong class="source-inline">MultiWorkerMirroredStrategy</strong>, which is a strategy similar to the Mirrored <a id="_idIndexMarker864"/>Strategy we used in the previous recipe in this chapter. This <a id="_idIndexMarker865"/>strategy is useful for synchronous training across machines with each machine potentially having one or more GPUs. All the variables and computations required for training the model are replicated on each of the worker nodes as in the Mirrored Strategy and, additionally, a distributed collection routine (such as all-reduce) is used to collate results from multiple distributed nodes. The remaining workflow for training, saving the model, loading the model, and evaluating the model remains the same as in our previous recipe.</p>
			<p>Ready for the next recipe? Let’s do it.</p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor232"/>Training Deep RL agents at scale – Multi-GPU PPO agent</h1>
			<p>RL agents in <a id="_idIndexMarker866"/>general require a large number of samples and gradient steps to be trained depending on the complexity of the state, action, and the problem space. With Deep RL, the computational complexity also increases drastically as the deep neural network used by the agent (for Q/value-function representation, for policy representation, or for both) has a lot more operations and parameters that need to be executed and updated, respectively. To speed up the training process, we need the capability to scale our Deep RL agent training to leverage the available compute resources, such as GPUs. This recipe will help you leverage multiple GPUs to train a PPO agent with a deep convolutional neural network policy in a distributed fashion in one of the <a id="_idIndexMarker867"/>procedurally generated RL environments using <strong class="bold">OpenAI’s procgen</strong> library.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor233"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment <a id="_idIndexMarker868"/>specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repo. Although not required, it is recommended to use a machine with two or more GPUs to execute this recipe.</p>
			<p>Now, let’s begin!</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor234"/>How to do it...</h2>
			<p>We will implement a complete recipe to allow configurable training of a PPO agent with a deep convolutional neural network policy in a distributed fashion. Let’s start implementing it step by step:</p>
			<ol>
				<li value="1">We will begin by importing the necessary modules for our recipe:<p class="source-code">import argparse</p><p class="source-code">import os</p><p class="source-code">from datetime import datetime</p><p class="source-code">import gym</p><p class="source-code">import gym.wrappers</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers import (</p><p class="source-code">    Conv2D,</p><p class="source-code">    Dense,</p><p class="source-code">    Dropout,</p><p class="source-code">    Flatten,</p><p class="source-code">    Input,</p><p class="source-code">    MaxPool2D,</p><p class="source-code">)</p></li>
				<li>We will be <a id="_idIndexMarker869"/>using the <strong class="source-inline">procgen</strong> environments from OpenAI. Let’s import that as well:<p class="source-code">import procgen  # Import &amp; register procgen Gym envs</p></li>
				<li>In order to make this recipe easy to configure and run, let’s add support for command-line arguments with useful configuration flags:<p class="source-code">parser = argparse.ArgumentParser(prog="TFRL-Cookbook-Ch9-Distributed-RL-Agent")</p><p class="source-code">parser.add_argument("--env", default="procgen:procgen-coinrun-v0")</p><p class="source-code">parser.add_argument("--update-freq", type=int, default=16)</p><p class="source-code">parser.add_argument("--epochs", type=int, default=3)</p><p class="source-code">parser.add_argument("--actor-lr", type=float, default=1e-4)</p><p class="source-code">parser.add_argument("--critic-lr", type=float, default=1e-4)</p><p class="source-code">parser.add_argument("--clip-ratio", type=float, default=0.1)</p><p class="source-code">parser.add_argument("--gae-lambda", type=float, default=0.95)</p><p class="source-code">parser.add_argument("--gamma", type=float, default=0.99)</p><p class="source-code">parser.add_argument("--logdir", default="logs")</p><p class="source-code">args = parser.parse_args()</p></li>
				<li>Let’s use <a id="_idIndexMarker870"/>a TensorBoard summary writer for logging:<p class="source-code">logdir = os.path.join(</p><p class="source-code">    args.logdir, parser.prog, args.env, \</p><p class="source-code">    datetime.now().strftime("%Y%m%d-%H%M%S")</p><p class="source-code">)</p><p class="source-code">print(f"Saving training logs to:{logdir}")</p><p class="source-code">writer = tf.summary.create_file_writer(logdir)</p></li>
				<li>We will first implement the <strong class="source-inline">Actor</strong> class in the following several steps, starting with the <strong class="source-inline">__init__</strong> method. You will notice that we need to instantiate the models within the context of the execution strategy:<p class="source-code">class Actor:</p><p class="source-code">    def __init__(self, state_dim, action_dim, </p><p class="source-code">    execution_strategy):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">        self.execution_strategy = execution_strategy</p><p class="source-code">        with self.execution_strategy.scope():</p><p class="source-code">            self.weight_initializer = \</p><p class="source-code">                tf.keras.initializers.he_normal()</p><p class="source-code">            self.model = self.nn_model()</p><p class="source-code">            self.model.summary()  # Print a summary of</p><p class="source-code">            # the Actor model</p><p class="source-code">            self.opt = \</p><p class="source-code">                tf.keras.optimizers.Nadam(args.actor_lr)</p></li>
				<li>For the Actor’s policy network model, we will implement a deep convolutional neural network <a id="_idIndexMarker871"/>comprising of multiple <strong class="source-inline">Conv2D</strong> and <strong class="source-inline">MaxPool2D</strong> layers. We will start the implementation in this step and finish in the following few steps:<p class="source-code">    def nn_model(self):</p><p class="source-code">        obs_input = Input(self.state_dim)</p><p class="source-code">        conv1 = Conv2D(</p><p class="source-code">            filters=64,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="same",</p><p class="source-code">            input_shape=self.state_dim,</p><p class="source-code">            data_format="channels_last",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(obs_input)</p><p class="source-code">        pool1 = MaxPool2D(pool_size=(3, 3), \</p><p class="source-code">                          strides=1)(conv1)</p></li>
				<li>We will add more Conv2D – Pool2D layers to stack up the processing layers depending on the needs for the task. In this recipe, we will be training policies for the procgen environment, which is somewhat visually rich, so we will stack a few more layers:<p class="source-code">       conv2 = Conv2D(</p><p class="source-code">            filters=32,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool1)</p><p class="source-code">        pool2 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                    (conv2)</p><p class="source-code">        conv3 = Conv2D(</p><p class="source-code">            filters=16,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool2)</p><p class="source-code">        pool3 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                    (conv3)</p><p class="source-code">        conv4 = Conv2D(</p><p class="source-code">            filters=8,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(pool3)</p><p class="source-code">        pool4 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                    (conv4)</p></li>
				<li>Now, we can use a flattening layer and prepare the output heads for the policy network:<p class="source-code">       flat = Flatten()(pool4)</p><p class="source-code">        dense1 = Dense(</p><p class="source-code">            16, activation="relu", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(flat)</p><p class="source-code">        dropout1 = Dropout(0.3)(dense1)</p><p class="source-code">        dense2 = Dense(</p><p class="source-code">            8, activation="relu", \</p><p class="source-code">               kernel_initializer=self.weight_initializer</p><p class="source-code">        )(dropout1)</p><p class="source-code">        dropout2 = Dropout(0.3)(dense2)</p></li>
				<li>As the final <a id="_idIndexMarker872"/>step for building the neural model for the policy network, we will create the output layer and return a Keras model:<p class="source-code">        output_discrete_action = Dense(</p><p class="source-code">            self.action_dim,</p><p class="source-code">            activation="softmax",</p><p class="source-code">            kernel_initializer=self.weight_initializer,</p><p class="source-code">        )(dropout2)</p><p class="source-code">        return tf.keras.models.Model(</p><p class="source-code">            inputs=obs_input, </p><p class="source-code">            outputs = output_discrete_action, </p><p class="source-code">            name="Actor")</p></li>
				<li>With the model we have defined in the previous steps, we can start processing a state/observation image input and produce the logits (unnormalized probabilities) and the action that the Actor would take. Let’s implement a method to do that:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        # Convert [Image] to np.array(np.adarray)</p><p class="source-code">        state_np = np.array([np.array(s) for s in state])</p><p class="source-code">        if len(state_np.shape) == 3:</p><p class="source-code">            # Convert (w, h, c) to (1, w, h, c)</p><p class="source-code">            state_np = np.expand_dims(state_np, 0)</p><p class="source-code">        logits = self.model.predict(state_np)  </p><p class="source-code">        # shape: (batch_size, self.action_dim)</p><p class="source-code">        action = np.random.choice(self.action_dim, </p><p class="source-code">                                  p=logits[0])</p><p class="source-code">        # 1 Action per instance of env; Env expects:</p><p class="source-code">        # (num_instances, actions)</p><p class="source-code">        # action = (action,)</p><p class="source-code">        return logits, action</p></li>
				<li>Next, to compute <a id="_idIndexMarker873"/>the surrogate loss to drive the learning, we will implement the <strong class="source-inline">compute_loss</strong> method:<p class="source-code">    def compute_loss(self, old_policy, new_policy, </p><p class="source-code">    actions, gaes):</p><p class="source-code">        log_old_policy = tf.math.log(tf.reduce_sum(</p><p class="source-code">                                   old_policy * actions))</p><p class="source-code">        log_old_policy = tf.stop_gradient(log_old_policy)</p><p class="source-code">        log_new_policy = tf.math.log(tf.reduce_sum(</p><p class="source-code">                                   new_policy * actions))</p><p class="source-code">        # Avoid INF in exp by setting 80 as the upper </p><p class="source-code">        # bound since,</p><p class="source-code">        # tf.exp(x) for x&gt;88 yeilds NaN (float32)</p><p class="source-code">        ratio = tf.exp(</p><p class="source-code">            tf.minimum(log_new_policy - \</p><p class="source-code">                       tf.stop_gradient(log_old_policy),\</p><p class="source-code">                       80)</p><p class="source-code">        )</p><p class="source-code">        clipped_ratio = tf.clip_by_value(</p><p class="source-code">            ratio, 1.0 - args.clip_ratio, 1.0 + \</p><p class="source-code">            args.clip_ratio</p><p class="source-code">        )</p><p class="source-code">        gaes = tf.stop_gradient(gaes)</p><p class="source-code">        surrogate = -tf.minimum(ratio * gaes, \</p><p class="source-code">                                clipped_ratio * gaes)</p><p class="source-code">        return tf.reduce_mean(surrogate)</p></li>
				<li>Next up is a core method that ties all the methods together to perform the training. Note that <a id="_idIndexMarker874"/>this is the train method per replica, and we will use it in our distributed training method, which will follow in the next steps:<p class="source-code">    def train(self, old_policy, states, actions, gaes):</p><p class="source-code">        actions = tf.one_hot(actions, self.action_dim)  </p><p class="source-code">        # One-hot encoding</p><p class="source-code">        actions = tf.reshape(actions, [-1, \</p><p class="source-code">                             self.action_dim])  </p><p class="source-code">        # Add batch dimension</p><p class="source-code">        actions = tf.cast(actions, tf.float64)</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            logits = self.model(states, training=True)</p><p class="source-code">            loss = self.compute_loss(old_policy, logits, </p><p class="source-code">                                     actions, gaes)</p><p class="source-code">        grads = tape.gradient(loss, </p><p class="source-code">                          self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                         self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>To implement the distributed training method, we will make use of the <strong class="source-inline">tf.function</strong> decorator to implement a TensorFlow 2.x function:<p class="source-code">    @tf.function</p><p class="source-code">    def train_distributed(self, old_policy, states,</p><p class="source-code">                          actions, gaes):</p><p class="source-code">        per_replica_losses = self.execution_strategy.run(</p><p class="source-code">            self.train, args=(old_policy, states, </p><p class="source-code">                              actions, gaes))</p><p class="source-code">        return self.execution_strategy.reduce(</p><p class="source-code">            tf.distribute.ReduceOp.SUM, \</p><p class="source-code">                per_replica_losses, axis=None)</p></li>
				<li>That completes <a id="_idIndexMarker875"/>our <strong class="source-inline">Actor</strong> class implementation, and we will now start our implementation of the <strong class="source-inline">Critic</strong> class:<p class="source-code">class Critic:</p><p class="source-code">    def __init__(self, state_dim, execution_strategy):</p><p class="source-code">        self.state_dim = state_dim</p><p class="source-code">        self.execution_strategy = execution_strategy</p><p class="source-code">        with self.execution_strategy.scope():</p><p class="source-code">            self.weight_initializer = \</p><p class="source-code">                tf.keras.initializers.he_normal()</p><p class="source-code">            self.model = self.nn_model()</p><p class="source-code">            self.model.summary()  </p><p class="source-code">            # Print a summary of the Critic model</p><p class="source-code">            self.opt = \</p><p class="source-code">                tf.keras.optimizers.Nadam(args.critic_lr)</p></li>
				<li>You must have noticed that we are creating the Critic’s value-function model instance within the scope of the execution strategy to support distributed training. We will now start implementing the Critic’s neural network model in the following few steps:<p class="source-code">    def nn_model(self):</p><p class="source-code">        obs_input = Input(self.state_dim)</p><p class="source-code">        conv1 = Conv2D(</p><p class="source-code">            filters=64,</p><p class="source-code">            kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="same",</p><p class="source-code">            input_shape=self.state_dim,</p><p class="source-code">            data_format="channels_last",</p><p class="source-code">            activation="relu",</p><p class="source-code">        )(obs_input)</p><p class="source-code">        pool1 = MaxPool2D(pool_size=(3, 3), strides=2)\</p><p class="source-code">                    (conv1)</p></li>
				<li>Like our Actor’s model, we will have similar layering of Conv2D-MaxPool2D layers followed <a id="_idIndexMarker876"/>by flattening layers with dropout:<p class="source-code">        conv2 = Conv2D(filters=32, kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1),</p><p class="source-code">            padding="valid", activation="relu",)(pool1)</p><p class="source-code">        pool2 = MaxPool2D(pool_size=(3, 3), strides=2)\</p><p class="source-code">                    (conv2)</p><p class="source-code">        conv3 = Conv2D(filters=16,</p><p class="source-code">            kernel_size=(3, 3), strides=(1, 1),</p><p class="source-code">            padding="valid", activation="relu",)(pool2)</p><p class="source-code">        pool3 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                    (conv3)</p><p class="source-code">        conv4 = Conv2D(filters=8, kernel_size=(3, 3),</p><p class="source-code">            strides=(1, 1), padding="valid",</p><p class="source-code">            activation="relu",)(pool3)</p><p class="source-code">        pool4 = MaxPool2D(pool_size=(3, 3), strides=1)\</p><p class="source-code">                    (conv4)</p><p class="source-code">        flat = Flatten()(pool4)</p><p class="source-code">        dense1 = Dense(16, activation="relu", </p><p class="source-code">                       kernel_initializer =\</p><p class="source-code">                           self.weight_initializer)\</p><p class="source-code">                       (flat)</p><p class="source-code">        dropout1 = Dropout(0.3)(dense1)</p><p class="source-code">        dense2 = Dense(8, activation="relu", </p><p class="source-code">                       kernel_initializer = \</p><p class="source-code">                           self.weight_initializer)\</p><p class="source-code">                       (dropout1)</p><p class="source-code">        dropout2 = Dropout(0.3)(dense2)</p></li>
				<li>We will add <a id="_idIndexMarker877"/>the value output head and return the model as a Keras model to complete our Critic’s neural network model:<p class="source-code">        value = Dense(</p><p class="source-code">            1, activation="linear", </p><p class="source-code">            kernel_initializer=self.weight_initializer)\</p><p class="source-code">            (dropout2)</p><p class="source-code">        return tf.keras.models.Model(inputs=obs_input, \</p><p class="source-code">                                     outputs=value, \</p><p class="source-code">                                     name="Critic")</p></li>
				<li>As you may recall, the Critic’s loss is the mean squared error between the predicted temporal-difference target and the actual temporal-difference targets. Let’s implement a method to compute the loss:<p class="source-code">    def compute_loss(self, v_pred, td_targets):</p><p class="source-code">        mse = tf.keras.losses.MeanSquaredError(</p><p class="source-code">                 reduction=tf.keras.losses.Reduction.SUM)</p><p class="source-code">        return mse(td_targets, v_pred)</p></li>
				<li>Similar to our Actor implementation, we will implement a per-replica <strong class="source-inline">train</strong> method and then use it in a later step for distributed training:<p class="source-code">    def train(self, states, td_targets):</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            v_pred = self.model(states, training=True)</p><p class="source-code">            # assert v_pred.shape == td_targets.shape</p><p class="source-code">            loss = self.compute_loss(v_pred, \</p><p class="source-code">                           tf.stop_gradient(td_targets))</p><p class="source-code">        grads = tape.gradient(loss, \</p><p class="source-code">                       self.model.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, \</p><p class="source-code">                       self.model.trainable_variables))</p><p class="source-code">        return loss</p></li>
				<li>We will <a id="_idIndexMarker878"/>now finalize our <strong class="source-inline">Critic</strong> class implementation by implementing the <strong class="source-inline">train_distributed</strong> method that enables distributed training:<p class="source-code">    @tf.function</p><p class="source-code">    def train_distributed(self, states, td_targets):</p><p class="source-code">        per_replica_losses = self.execution_strategy.run(</p><p class="source-code">            self.train, args=(states, td_targets)</p><p class="source-code">        )</p><p class="source-code">        return self.execution_strategy.reduce(</p><p class="source-code">            tf.distribute.ReduceOp.SUM, \</p><p class="source-code">            per_replica_losses, axis=None</p><p class="source-code">        )</p></li>
				<li>With our <strong class="source-inline">Actor</strong> and <strong class="source-inline">Critic</strong> classes implemented, we can start our distributed <strong class="source-inline">PPOAgent</strong> implementation. We will implement the <strong class="source-inline">PPOAgent</strong> class in the following several steps. Let’s begin with the <strong class="source-inline">__init__</strong> method:<p class="source-code">class PPOAgent:</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        """Distributed PPO Agent for image observations </p><p class="source-code">        and discrete action-space Gym envs</p><p class="source-code">        Args:</p><p class="source-code">            env (gym.Env): OpenAI Gym I/O compatible RL </p><p class="source-code">            environment with discrete action space</p><p class="source-code">        """</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_dim = self.env.observation_space.shape</p><p class="source-code">        self.action_dim = self.env.action_space.n</p><p class="source-code">        # Create a Distributed execution strategy</p><p class="source-code">        self.distributed_execution_strategy = \</p><p class="source-code">                     tf.distribute.MirroredStrategy()</p><p class="source-code">        print(f"Number of devices: {self.\</p><p class="source-code">                distributed_execution_strategy.\</p><p class="source-code">                num_replicas_in_sync}")</p><p class="source-code">        # Create Actor &amp; Critic networks under the </p><p class="source-code">        # distributed execution strategy scope</p><p class="source-code">        with self.distributed_execution_strategy.scope():</p><p class="source-code">            self.actor = Actor(self.state_dim, </p><p class="source-code">                            self.action_dim, </p><p class="source-code">                            tf.distribute.get_strategy())</p><p class="source-code">            self.critic = Critic(self.state_dim, </p><p class="source-code">                            tf.distribute.get_strategy())</p></li>
				<li>Next, we will <a id="_idIndexMarker879"/>implement a method to calculate the target for the <strong class="bold">generalized advantage estimate</strong> (<strong class="bold">GAE</strong>):<p class="source-code">    def gae_target(self, rewards, v_values, next_v_value,</p><p class="source-code">    done):</p><p class="source-code">        n_step_targets = np.zeros_like(rewards)</p><p class="source-code">        gae = np.zeros_like(rewards)</p><p class="source-code">        gae_cumulative = 0</p><p class="source-code">        forward_val = 0</p><p class="source-code">        if not done:</p><p class="source-code">            forward_val = next_v_value</p><p class="source-code">        for k in reversed(range(0, len(rewards))):</p><p class="source-code">            delta = rewards[k] + args.gamma * \</p><p class="source-code">              forward_val - v_values[k]</p><p class="source-code">            gae_cumulative = args.gamma * \</p><p class="source-code">              args.gae_lambda * gae_cumulative + delta</p><p class="source-code">            gae[k] = gae_cumulative</p><p class="source-code">            forward_val = v_values[k]</p><p class="source-code">            n_step_targets[k] = gae[k] + v_values[k]</p><p class="source-code">        return gae, n_step_targets</p></li>
				<li>We are all <a id="_idIndexMarker880"/>set to start our main <strong class="source-inline">train(…)</strong> method. We will split the implementation of this method into the following few steps. Let’s set up the scope, start the outer loop, and initialize varaibles:<p class="source-code">    def train(self, max_episodes=1000):</p><p class="source-code">        with self.distributed_execution_strategy.scope():</p><p class="source-code">            with writer.as_default():</p><p class="source-code">                for ep in range(max_episodes):</p><p class="source-code">                    state_batch = []</p><p class="source-code">                    action_batch = []</p><p class="source-code">                    reward_batch = []</p><p class="source-code">                    old_policy_batch = []</p><p class="source-code">                    episode_reward, done = 0, False</p><p class="source-code">                    state = self.env.reset()</p><p class="source-code">                    prev_state = state</p><p class="source-code">                    step_num = 0</p></li>
				<li>Now, we can <a id="_idIndexMarker881"/>start the loop that needs to be executed for each episode until the episode is done:<p class="source-code">                      while not done:</p><p class="source-code">                        self.env.render()</p><p class="source-code">                        logits, action = \</p><p class="source-code">                             self.actor.get_action(state)</p><p class="source-code">                        next_state, reward, dones, _ = \</p><p class="source-code">                                    self.env.step(action)</p><p class="source-code">                        step_num += 1</p><p class="source-code">                        print(f"ep#:{ep} step#:{step_num} </p><p class="source-code">                                step_rew:{reward} \</p><p class="source-code">                                action:{action} \</p><p class="source-code">                                dones:{dones}",end="\r",)</p><p class="source-code">                        done = np.all(dones)</p><p class="source-code">                        if done:</p><p class="source-code">                            next_state = prev_state</p><p class="source-code">                        else:</p><p class="source-code">                            prev_state = next_state</p><p class="source-code">                        state_batch.append(state)</p><p class="source-code">                        action_batch.append(action)</p><p class="source-code">                        reward_batch.append(</p><p class="source-code">                                        (reward + 8) / 8)</p><p class="source-code">                        old_policy_batch.append(logits)  </p></li>
				<li>Within each episode, if we have reached <strong class="source-inline">update_freq</strong> or just reached an end state, we need <a id="_idIndexMarker882"/>to compute the GAEs and TD targets. Let’s add the code for that:<p class="source-code">                         if len(state_batch) &gt;= \</p><p class="source-code">                         args.update_freq or done:</p><p class="source-code">                            states = np.array(</p><p class="source-code">                                [state.squeeze() for \</p><p class="source-code">                                 state in state_batch])</p><p class="source-code">                            actions = \</p><p class="source-code">                                np.array(action_batch)</p><p class="source-code">                            rewards = \</p><p class="source-code">                                np.array(reward_batch)</p><p class="source-code">                            old_policies = np.array(</p><p class="source-code">                                [old_pi.squeeze() for \</p><p class="source-code">                             old_pi in old_policy_batch])</p><p class="source-code">                            v_values = self.critic.\</p><p class="source-code">                                    model.predict(states)</p><p class="source-code">                            next_v_value = self.critic.\</p><p class="source-code">                               model.predict(</p><p class="source-code">                                   np.expand_dims(</p><p class="source-code">                                       next_state, 0))</p><p class="source-code">                            gaes, td_targets = \</p><p class="source-code">                                 self.gae_target(</p><p class="source-code">                                     rewards, v_values,</p><p class="source-code">                                     next_v_value, done)</p><p class="source-code">                            actor_losses, critic_losses=\</p><p class="source-code">                                                   [], []   </p></li>
				<li>Within the same execution context, we need to train the <strong class="source-inline">Actor</strong> and the <strong class="source-inline">Critic</strong>:<p class="source-code">                               for epoch in range(args.\</p><p class="source-code">                               epochs):</p><p class="source-code">                                actor_loss = self.actor.\</p><p class="source-code">                                  train_distributed(</p><p class="source-code">                                     old_policies,</p><p class="source-code">                                     states, actions,</p><p class="source-code">                                     gaes)</p><p class="source-code">                                actor_losses.\</p><p class="source-code">                                  append(actor_loss)</p><p class="source-code">                                critic_loss = self.\</p><p class="source-code">                                critic.train_distributed(</p><p class="source-code">                                   states, td_targets)</p><p class="source-code">                                critic_losses.\</p><p class="source-code">                                   append(critic_loss)</p><p class="source-code">                            # Plot mean actor &amp; critic </p><p class="source-code">                            # losses on every update</p><p class="source-code">                            tf.summary.scalar(</p><p class="source-code">                                "actor_loss", </p><p class="source-code">                                 np.mean(actor_losses), </p><p class="source-code">                                 step=ep)</p><p class="source-code">                            tf.summary.scalar(</p><p class="source-code">                                 "critic_loss", </p><p class="source-code">                                  np.mean(critic_losses), </p><p class="source-code">                                  step=ep) </p></li>
				<li>Finally, we need <a id="_idIndexMarker883"/>to reset the tracking variables and update our episode reward values:<p class="source-code">   </p><p class="source-code">                            state_batch = []</p><p class="source-code">                            action_batch = []</p><p class="source-code">                            reward_batch = []</p><p class="source-code">                            old_policy_batch = []</p><p class="source-code">                        episode_reward += reward</p><p class="source-code">                        state = next_state </p></li>
				<li>With that, our distributed <strong class="bold">PPOAgent</strong> implementation is complete! We will implement our <strong class="source-inline">main</strong> method to finalize our recipe:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env_name = "procgen:procgen-coinrun-v0"</p><p class="source-code">    env = gym.make(env_name, render_mode="rgb_array")</p><p class="source-code">    env = gym.wrappers.Monitor(env=env, </p><p class="source-code">                        directory="./videos", force=True)</p><p class="source-code">    agent = PPOAgent(env)</p><p class="source-code">    agent.train()</p><p>That’s it for the recipe! Hope you enjoyed cooking it up. You can execute the recipe and <a id="_idIndexMarker884"/>watch the progress using the TensorBoard logs to see the training speedup you get with a greater number of GPUs! </p></li>
			</ol>
			<p>Let’s recap what we accomplished and how the recipe works in the next section.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor235"/>How it works...</h2>
			<p>We implemented <strong class="source-inline">Actor</strong> and <strong class="source-inline">Critic</strong> classes where the Actor used a deep convolutional neural network for the policy representation and the Critic utilized a similar deep convolutional neural <a id="_idIndexMarker885"/>network for its value function representation. Both these models were instantiated under the scope of the distributed execution strategy using the <strong class="source-inline">self.execution_strategy.scope()</strong> construct.</p>
			<p>The procgen environments, such as coinrun, fruitbot, jumper, leaper, maze, and others, are visually (relatively) rich environments and therefore require convolutional layers that are relatively deep to process the visual observations. We therefore used a deep CNN model for the policy network of the Actor. For distributed training using multiple replicas on multiple GPUs, we first implemented a single-replica training method (train) and then used <strong class="source-inline">Tensorflow.function</strong> to run across replicas and reduce the results to arrive at the total loss.</p>
			<p>Finally, while training our PPO agent in the distributed setting, we performed all the training operations <a id="_idIndexMarker886"/>within the scope of the distributed execution strategy by using Python’s <strong class="source-inline">with</strong> statement for context management like this: <strong class="source-inline">with self.distributed_execution_strategy.scope()</strong>.</p>
			<p>It’s time to move on to the next recipe!</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor236"/>Building blocks for distributed Deep Reinforcement Learning for accelerated training</h1>
			<p>The previous recipes in this chapter discussed how you could scale your Deep RL training using <a id="_idIndexMarker887"/>TensorFlow 2.x’s distributed execution APIs. While it was straightforward after understanding the concepts and the implementation style, training Deep RL agents with more advanced architectures such as Impala and R2D2 requires RL building blocks such as distributed parameter servers and distributed experience replay. This chapter will walk through the implementation of such building blocks for distributed RL training. We will be using the Ray distributed computing framework to implement our building blocks.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor237"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repo. To test the building blocks we build in this recipe, we will be using the <strong class="source-inline">sac_agent_base</strong> module based on our SAC agent implemented in one of the book’s earlier recipes. If the following <strong class="source-inline">import</strong> statements run without issues, you are ready to start:</p>
			<p class="source-code">import pickle</p>
			<p class="source-code">import sys</p>
			<p class="source-code">import fire</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import ray</p>
			<p class="source-code">if "." not in sys.path:</p>
			<p class="source-code">    sys.path.insert(0, ".")</p>
			<p class="source-code">from sac_agent_base import SAC</p>
			<p>Now, let’s begin!</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor238"/>How to do it...</h2>
			<p>We <a id="_idIndexMarker888"/>will implement the building blocks one by one, starting with the distributed parameter server:</p>
			<ol>
				<li value="1">The <strong class="source-inline">ParameterServer</strong> class is a simple store for sharing the neural network parameters or weights between workers in a distributed training setting. We will implement the class as a Ray’s remote Actor:<p class="source-code">@ray.remote</p><p class="source-code">class ParameterServer(object):</p><p class="source-code">    def __init__(self, weights):</p><p class="source-code">        values = [value.copy() for value in weights]</p><p class="source-code">        self.weights = values</p><p class="source-code">    def push(self, weights):</p><p class="source-code">        values = [value.copy() for value in weights]</p><p class="source-code">        self.weights = values</p><p class="source-code">    def pull(self):</p><p class="source-code">        return self.weights</p><p class="source-code">    def get_weights(self):</p><p class="source-code">        return self.weights</p></li>
				<li>Let’s also add a method to save the weights to the disk:<p class="source-code">    # save weights to disk</p><p class="source-code">    def save_weights(self, name):</p><p class="source-code">        with open(name + "weights.pkl", "wb") as pkl:</p><p class="source-code">            pickle.dump(self.weights, pkl)</p><p class="source-code">        print(f"Weights saved to {name + </p><p class="source-code">                                  ‘weights.pkl’}.")</p></li>
				<li>As <a id="_idIndexMarker889"/>the next building block, we will implement the <strong class="source-inline">ReplayBuffer</strong>, which can be used by a distributed set of agents. We will start the implementation in this step and continue in the next several steps:<p class="source-code">@ray.remote</p><p class="source-code">class ReplayBuffer:</p><p class="source-code">    """</p><p class="source-code">    A simple FIFO experience replay buffer for RL Agents</p><p class="source-code">    """</p><p class="source-code">    def __init__(self, obs_shape, action_shape, size):</p><p class="source-code">        self.cur_states = np.zeros([size, obs_shape[0]],</p><p class="source-code">                                    dtype=np.float32)</p><p class="source-code">        self.actions = np.zeros([size, action_shape[0]],</p><p class="source-code">                                 dtype=np.float32)</p><p class="source-code">        self.rewards = np.zeros(size, dtype=np.float32)</p><p class="source-code">        self.next_states = np.zeros([size, obs_shape[0]],</p><p class="source-code">                                     dtype=np.float32)</p><p class="source-code">        self.dones = np.zeros(size, dtype=np.float32)</p><p class="source-code">        self.idx, self.size, self.max_size = 0, 0, size</p><p class="source-code">        self.rollout_steps = 0</p></li>
				<li>Next, we will implement a method to store new experiences in the replay buffer:<p class="source-code">    def store(self, obs, act, rew, next_obs, done):</p><p class="source-code">        self.cur_states[self.idx] = np.squeeze(obs)</p><p class="source-code">        self.actions[self.idx] = np.squeeze(act)</p><p class="source-code">        self.rewards[self.idx] = np.squeeze(rew)</p><p class="source-code">        self.next_states[self.idx] = np.squeeze(next_obs)</p><p class="source-code">        self.dones[self.idx] = done</p><p class="source-code">        self.idx = (self.idx + 1) % self.max_size</p><p class="source-code">        self.size = min(self.size + 1, self.max_size)</p><p class="source-code">        self.rollout_steps += 1</p></li>
				<li>To <a id="_idIndexMarker890"/>sample a batch of experience data from the replay buffer, we will implement a method that randomly samples from the replay buffer and returns a dictionary containing the sampled experience data:<p class="source-code">    def sample_batch(self, batch_size=32):</p><p class="source-code">        idxs = np.random.randint(0, self.size, </p><p class="source-code">                                 size=batch_size)</p><p class="source-code">        return dict(</p><p class="source-code">            cur_states=self.cur_states[idxs],</p><p class="source-code">            actions=self.actions[idxs],</p><p class="source-code">            rewards=self.rewards[idxs],</p><p class="source-code">            next_states=self.next_states[idxs],</p><p class="source-code">            dones=self.dones[idxs])</p></li>
				<li>That completes our <strong class="source-inline">ReplayBuffer</strong> class implementation. We will now start implementing a method to roll out, which essentially collects experiences in an RL environment using an exploration policy with parameters pulled from the distributed <a id="_idIndexMarker891"/>parameter server object and stores the collected experience in the distributed replay buffer. We will start our implementation in this step and complete the <strong class="source-inline">rollout</strong> method implementation in the following steps:<p class="source-code">@ray.remote</p><p class="source-code">def rollout(ps, replay_buffer, config):</p><p class="source-code">    """Collect experience using an exploration policy"""</p><p class="source-code">    env = gym.make(config["env"])</p><p class="source-code">    obs, reward, done, ep_ret, ep_len = env.reset(), 0, \</p><p class="source-code">                                          False, 0, 0</p><p class="source-code">    total_steps = config["steps_per_epoch"] * \</p><p class="source-code">                   config["epochs"]</p><p class="source-code">    agent = SAC(env.observation_space.shape, \</p><p class="source-code">                env.action_space)</p><p class="source-code">    weights = ray.get(ps.pull.remote())</p><p class="source-code">    target_weights = agent.actor.get_weights()</p><p class="source-code">    for i in range(len(target_weights)):  </p><p class="source-code">    # set tau% of target model to be new weights</p><p class="source-code">        target_weights[i] = weights[i]</p><p class="source-code">    agent.actor.set_weights(target_weights)</p></li>
				<li>With <a id="_idIndexMarker892"/>the agent intialized and loaded and the environment instance(s) ready, we can start our experience-gathering loop:<p class="source-code">    for step in range(total_steps):</p><p class="source-code">        if step &gt; config["random_exploration_steps"]:</p><p class="source-code">            # Use Agent’s policy for exploration after </p><p class="source-code">            `random_exploration_steps`</p><p class="source-code">            a = agent.act(obs)</p><p class="source-code">        else:  # Use a uniform random exploration policy</p><p class="source-code">            a = env.action_space.sample()</p><p class="source-code">        next_obs, reward, done, _ = env.step(a)</p><p class="source-code">        print(f"Step#:{step} reward:{reward} \</p><p class="source-code">                done:{done}")</p><p class="source-code">        ep_ret += reward</p><p class="source-code">        ep_len += 1</p></li>
				<li>Let’s handle the case when a <strong class="source-inline">max_ep_len</strong> is configured to indicate the maximum length of the episode and then store the collected experience in the distributed replay buffer:<p class="source-code">        done = False if ep_len == config["max_ep_len"]\</p><p class="source-code">                 else done</p><p class="source-code">        # Store experience to replay buffer</p><p class="source-code">        replay_buffer.store.remote(obs, a, reward, </p><p class="source-code">                                   next_obs, done)</p></li>
				<li>Finally, at the end of the episode, sync the weights of the behavior policy using the parameter server:<p class="source-code">        obs = next_obs</p><p class="source-code">        if done or (ep_len == config["max_ep_len"]):</p><p class="source-code">            """</p><p class="source-code">            Perform parameter sync at the end of the </p><p class="source-code">            trajectory.</p><p class="source-code">            """</p><p class="source-code">            obs, reward, done, ep_ret, ep_len = \</p><p class="source-code">                             env.reset(), 0, False, 0, 0</p><p class="source-code">            weights = ray.get(ps.pull.remote())</p><p class="source-code">            agent.actor.set_weights(weights)</p></li>
				<li>That <a id="_idIndexMarker893"/>completes the implementation of the <strong class="source-inline">rollout</strong> method and we can now implement a <strong class="source-inline">train</strong> method that runs the train loop:<p class="source-code">@ray.remote(num_gpus=1, max_calls=1)</p><p class="source-code">def train(ps, replay_buffer, config):</p><p class="source-code">    agent = SAC(config["obs_shape"], \</p><p class="source-code">                config["action_space"])</p><p class="source-code">    weights = ray.get(ps.pull.remote())</p><p class="source-code">    agent.actor.set_weights(weights)</p><p class="source-code">    train_step = 1</p><p class="source-code">    while True:</p><p class="source-code">        agent.train_with_distributed_replay_memory(</p><p class="source-code">            ray.get(replay_buffer.sample_batch.remote())</p><p class="source-code">        )</p><p class="source-code">        if train_step % config["worker_update_freq"]== 0:</p><p class="source-code">            weights = agent.actor.get_weights()</p><p class="source-code">            ps.push.remote(weights)</p><p class="source-code">        train_step += 1</p></li>
				<li>The final module in our recipe is the <strong class="source-inline">main</strong> function, which puts together all the building blocks we have built so far in this recipe and exercises them. We will begin <a id="_idIndexMarker894"/>the implementation in this step and finish it in the remaining steps. Let’s start with the <strong class="source-inline">main</strong> function argument list and capture the arguments in a config dictionary:<p class="source-code">def main(</p><p class="source-code">    env="MountainCarContinuous-v0",</p><p class="source-code">    epochs=1000,</p><p class="source-code">    steps_per_epoch=5000,</p><p class="source-code">    replay_size=100000,</p><p class="source-code">    random_exploration_steps=1000,</p><p class="source-code">    max_ep_len=1000,</p><p class="source-code">    num_workers=4,</p><p class="source-code">    num_learners=1,</p><p class="source-code">    worker_update_freq=500,</p><p class="source-code">):</p><p class="source-code">    config = {</p><p class="source-code">        "env": env,</p><p class="source-code">        "epochs": epochs,</p><p class="source-code">        "steps_per_epoch": steps_per_epoch,</p><p class="source-code">        "max_ep_len": max_ep_len,</p><p class="source-code">        "replay_size": replay_size,</p><p class="source-code">        "random_exploration_steps": \</p><p class="source-code">             random_exploration_steps,</p><p class="source-code">        "num_workers": num_workers,</p><p class="source-code">        "num_learners": num_learners,</p><p class="source-code">        "worker_update_freq": worker_update_freq,</p><p class="source-code">    }</p></li>
				<li>Next, let’s <a id="_idIndexMarker895"/>create an instance of the desired environment, obtain the state and observation space, initialize ray, and also initialize a Stochastic Actor-Critic agent. Note that we are initializing a single-node ray cluster but feel free to initialize ray with a cluster of nodes (local or in the cloud):<p class="source-code">    env = gym.make(config["env"])</p><p class="source-code">    config["obs_shape"] = env.observation_space.shape</p><p class="source-code">    config["action_space"] = env.action_space</p><p class="source-code">    ray.init()</p><p class="source-code">    agent = SAC(config["obs_shape"], \</p><p class="source-code">                config["action_space"])</p></li>
				<li>In this step, we will initialize an instance of the <strong class="source-inline">ParameterServer</strong> class and an instance of the <strong class="source-inline">ReplayBuffer</strong> class:<p class="source-code">    params_server = \</p><p class="source-code">        ParameterServer.remote(agent.actor.get_weights())</p><p class="source-code">    replay_buffer = ReplayBuffer.remote(</p><p class="source-code">        config["obs_shape"], \</p><p class="source-code">        config["action_space"].shape, \</p><p class="source-code">        config["replay_size"]</p><p class="source-code">    )</p></li>
				<li>We are now ready to exercise the building blocks we have built. We will first launch <a id="_idIndexMarker896"/>a series of rollout tasks based on the number of workers specified as a configuration argument that will launch the rollout process on the distributed ray cluster:<p class="source-code">    task_rollout = [</p><p class="source-code">        rollout.remote(params_server, replay_buffer, </p><p class="source-code">                       config)</p><p class="source-code">        for i in range(config["num_workers"])</p><p class="source-code">    ]</p><p>The rollout task will launch the remote tasks that will populate the replay buffer with the gathered experience. The above line will return immediately even though the rollout tasks will take time to complete because of the asynchronous function call.</p></li>
				<li>Next, we will launch a configurable number of learners that run the distributed training task on the ray cluster:<p class="source-code">    task_train = [</p><p class="source-code">        train.remote(params_server, replay_buffer, </p><p class="source-code">                     config)</p><p class="source-code">        for i in range(config["num_learners"])</p><p class="source-code">    ]</p><p>The above statement will launch the remote training process and return immediately even though the <strong class="source-inline">train</strong> function on the learners will take time to complete. </p><p class="source-code">We will wait for the tasks to complete on the main thread before exiting:</p><p class="source-code">    ray.wait(task_rollout)</p><p class="source-code">    ray.wait(task_train)</p></li>
				<li>Finally, let’s define our entry point. We will use the Python Fire library to expose our <a id="_idIndexMarker897"/><strong class="source-inline">main</strong> function, and its arguments to look like an executable supporting command-line argument:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    fire.Fire(main)</p><p>With the preceding entry point, the script can be configured and launched from the command line. An example is provided here for your reference:</p><p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@dev-cluster:~/tfrl-cookbook$python 4_building_blocks_for_distributed_rl_using_ray.py main --env="MountaincarContinuous-v0" --num_workers=8 --num_learners=3</strong></p></li>
			</ol>
			<p>That completes our implementation! Let’s briefly discuss how it works in the next section. </p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor239"/>How it works...</h2>
			<p>We built a distributed <strong class="source-inline">ParameterServer</strong>, <strong class="source-inline">ReplayBuffer</strong>, rollout worker, and learner processes. These building blocks are crucial for training distributed RL agents. We utilized Ray as the framework for distributed computing.</p>
			<p>After implementing the building blocks and the tasks, in the <strong class="source-inline">main</strong> function, we launched the two asynchronous, distributed tasks on the ray cluster. The <strong class="source-inline">task_rollout</strong> launched a (configurable) number of rollout workers and the <strong class="source-inline">task_train</strong> launched a (configurable) number of learners. Both the tasks run on the ray cluster asynchronously in a distributed manner. The <a id="_idIndexMarker898"/>rollout workers pull the latest weights from the parameter server and gather and store experiences in the replay memory buffer while, simultaneously, the learners train using batches of experiences sampled from the replay memory and push the updated (and potentially improved) set of parameters to the parameter server. </p>
			<p>It’s time to move on to the next and final recipe of this chapter!</p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor240"/>Large-scale Deep RL agent training using Ray, Tune, and RLLib</h1>
			<p>In the previous recipe, we got a flavor of how to implement distributed RL agent training routines <a id="_idIndexMarker899"/>from scratch. Since most of the components <a id="_idIndexMarker900"/>used as building blocks have <a id="_idIndexMarker901"/>become a standard way of building Deep RL training infrastructure, we can leverage an existing library that maintains <a id="_idIndexMarker902"/>a high-quality implementation <a id="_idIndexMarker903"/>of such building blocks. Fortunately, with our choice of ray as the framework for distributed computing, we are <a id="_idIndexMarker904"/>in a good place. Tune and RLLib are two libraries built on top of ray, and are available together with Ray, that provide highly scalable hyperparameter tuning (Tune) and RL training (RLLib). This recipe will provide a curated set of steps to get you acquainted with ray, Tune, and RLLib so that you can utilize them to scale your Deep RL training routines. In addition to the recipe discussed here in the text, the cookbook’s code repository for this chapter will have a handful of additional recipes for you.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor241"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repo. Ray, Tune, and RLLib will be installed in your <strong class="source-inline">tfrl-cookbook</strong> conda environment when you use the provided conda YAML spec for the environment. If you want to install Tune and RLLib in a different environment, the easiest way is to install it using the following command:</p>
			<p class="source-code"> pip install ray[tune,rllib]</p>
			<p>Now, let’s begin!</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor242"/>How to do it...</h2>
			<p>We will <a id="_idIndexMarker905"/>start with quick and basic <a id="_idIndexMarker906"/>commands and recipes to launch <a id="_idIndexMarker907"/>training on ray clusters using Tune and RLLib and progressively customize the training pipeline to provide you with a useful recipe:</p>
			<ol>
				<li value="1">Launching <a id="_idIndexMarker908"/>typical training of RL agents <a id="_idIndexMarker909"/>in OpenAI Gym environments <a id="_idIndexMarker910"/>is as easy as specifying the algorithm name and the environment name. For example, to train a PPO agent in the CartPole-v4 Gym environment, all you need to execute is the following command:<p class="source-code"><strong class="bold">(tfrl-cookbook) praveen@dev-cluster:~/tfrl-cookbook$rllib train –run PPO –env "CartPole-v4" --eager</strong></p><p>Note that the <strong class="source-inline">--eager</strong> flag is also specified, which forces RLLib to use eager execution (the default mode of execution in TensorFlow 2.x).</p></li>
				<li>Let’s try to train a PPO agent in the <strong class="source-inline">coinrun</strong> <strong class="source-inline">procgen</strong> environment, like in one of our previous recipes:<p class="source-code"><strong class="bold">(tfrl-cookbook) praveen@dev-cluster:~/tfrl-cookbook$rllib train --run PPO --env "procgen:procgen-coinrun-v0" --eager</strong></p><p>You will notice that the preceding command fails with the following (shortened) error:</p><p class="source-code"><strong class="bold">    ValueError: No default configuration for obs shape [64, 64, 3], you must specify `conv_filters` manually as a model option. Default configurations are only available for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use a custom model or preprocessor.</strong></p><p>This is because, as the error states, RLLib by default supports observations of shapes (42, 42, k) or (84, 84, k). Observations of other shapes will need a custom model <a id="_idIndexMarker911"/>or a preprocessor. In the <a id="_idIndexMarker912"/>next few steps, we <a id="_idIndexMarker913"/>will see how we can implement a custom neural network model implemented using the TensorFlow 2.x Keras API, which can be used with ray RLLib.</p></li>
				<li>We will <a id="_idIndexMarker914"/>start our custom model <a id="_idIndexMarker915"/>implementation (<strong class="source-inline">custom_model.py</strong>) in <a id="_idIndexMarker916"/>this step and complete it in the following few steps. In this step, let’s import the necessary modules and also implement a helper method to return a Conv2D layer with a certain filter depth:<p class="source-code">from ray.rllib.models.tf.tf_modelv2 import TFModelV2</p><p class="source-code">import tensorflow as tf</p><p class="source-code">def conv_layer(depth, name):</p><p class="source-code">    return tf.keras.layers.Conv2D(</p><p class="source-code">        filters=depth, kernel_size=3, strides=1, \</p><p class="source-code">        padding="same", name=name</p><p class="source-code">    )</p></li>
				<li>Next, let’s <a id="_idIndexMarker917"/>implement a helper <a id="_idIndexMarker918"/>method to build and return a <a id="_idIndexMarker919"/>simple residual block:<p class="source-code">def residual_block(x, depth, prefix):</p><p class="source-code">    inputs = x</p><p class="source-code">    assert inputs.get_shape()[-1].value == depth</p><p class="source-code">    x = tf.keras.layers.ReLU()(x)</p><p class="source-code">    x = conv_layer(depth, name=prefix + "_conv0")(x)</p><p class="source-code">    x = tf.keras.layers.ReLU()(x)</p><p class="source-code">    x = conv_layer(depth, name=prefix + "_conv1")(x)</p><p class="source-code">    return x + inputs</p></li>
				<li>Let’s <a id="_idIndexMarker920"/>implement another handy <a id="_idIndexMarker921"/>function to construct <a id="_idIndexMarker922"/>multiple residual block sequences:<p class="source-code">def conv_sequence(x, depth, prefix):</p><p class="source-code">    x = conv_layer(depth, prefix + "_conv")(x)</p><p class="source-code">    x = tf.keras.layers.MaxPool2D(pool_size=3, \</p><p class="source-code">                                  strides=2,\</p><p class="source-code">                                  padding="same")(x)</p><p class="source-code">    x = residual_block(x, depth, prefix=prefix + \</p><p class="source-code">                       "_block0")</p><p class="source-code">    x = residual_block(x, depth, prefix=prefix + \</p><p class="source-code">                       "_block1")</p><p class="source-code">    return x</p></li>
				<li>We can now start implementing the <strong class="source-inline">CustomModel</strong> class as a subclass of the TFModelV2 base class provided by RLLib to make it easy to integrate with RLLib:<p class="source-code">class CustomModel(TFModelV2):</p><p class="source-code">    """Deep residual network that produces logits for </p><p class="source-code">       policy and value for value-function;</p><p class="source-code">    Based on architecture used in IMPALA paper:https://</p><p class="source-code">       arxiv.org/abs/1802.01561"""</p><p class="source-code">    def __init__(self, obs_space, action_space, </p><p class="source-code">    num_outputs, model_config, name):</p><p class="source-code">        super().__init__(obs_space, action_space, \</p><p class="source-code">                         num_outputs, model_config, name)</p><p class="source-code">        depths = [16, 32, 32]</p><p class="source-code">        inputs = tf.keras.layers.Input(</p><p class="source-code">                        shape=obs_space.shape,</p><p class="source-code">                        name="observations")</p><p class="source-code">        scaled_inputs = tf.cast(inputs, </p><p class="source-code">                                tf.float32) / 255.0</p><p class="source-code">        x = scaled_inputs</p><p class="source-code">        for i, depth in enumerate(depths):</p><p class="source-code">            x = conv_sequence(x, depth, prefix=f"seq{i}")</p><p class="source-code">        x = tf.keras.layers.Flatten()(x)</p><p class="source-code">        x = tf.keras.layers.ReLU()(x)</p><p class="source-code">        x = tf.keras.layers.Dense(units=256,</p><p class="source-code">                                  activation="relu", </p><p class="source-code">                                  name="hidden")(x)</p><p class="source-code">        logits = tf.keras.layers.Dense(units=num_outputs,</p><p class="source-code">                                       name="pi")(x)</p><p class="source-code">        value = tf.keras.layers.Dense(units=1, </p><p class="source-code">                                      name="vf")(x)</p><p class="source-code">        self.base_model = tf.keras.Model(inputs, </p><p class="source-code">                                        [logits, value])</p><p class="source-code">        self.register_variables(</p><p class="source-code">                               self.base_model.variables)</p></li>
				<li>After the <strong class="source-inline">__init__</strong> method, we need to implement the <strong class="source-inline">forward</strong> method as it is not implemented by the base class (<strong class="source-inline">TFModelV2</strong>) but is necessary:<p class="source-code">    def forward(self, input_dict, state, seq_lens):</p><p class="source-code">        # explicit cast to float32 needed in eager</p><p class="source-code">        obs = tf.cast(input_dict["obs"], tf.float32)</p><p class="source-code">        logits, self._value = self.base_model(obs)</p><p class="source-code">        return logits, state</p></li>
				<li>We <a id="_idIndexMarker923"/>will also implement a <a id="_idIndexMarker924"/>one-line method to <a id="_idIndexMarker925"/>reshape the value function output:<p class="source-code">    def value_function(self):</p><p class="source-code">        return tf.reshape(self._value, [-1])</p><p>With that, our <strong class="source-inline">CustomModel</strong> implementation is complete and is ready to use!</p></li>
				<li>We will <a id="_idIndexMarker926"/>implement a solution (<strong class="source-inline">5.1_training_using_tune_run.py</strong>) using ray, Tune, and RLLib’s Python API so that <a id="_idIndexMarker927"/>you also utilize the model in addition to their command-line usage. Let’s split the implementation into two steps. In this step, we will import the necessary modules and initialize ray:<p class="source-code">import ray</p><p class="source-code">import sys</p><p class="source-code">from ray import tune</p><p class="source-code">from ray.rllib.models import ModelCatalog</p><p class="source-code">if not "." in sys.path:</p><p class="source-code">    sys.path.insert(0, ".")</p><p class="source-code">from custom_model import CustomModel</p><p class="source-code">ray.init()  # Can also initialize a cluster with multiple </p><p class="source-code">#nodes here using the cluster head node’s IP</p></li>
				<li>In this step, we will register our custom model in RLLib’s <strong class="source-inline">ModelCatlog</strong> and then use it <a id="_idIndexMarker928"/>to train a PPO agent <a id="_idIndexMarker929"/>with a custom set of <a id="_idIndexMarker930"/>parameters including <a id="_idIndexMarker931"/>the <strong class="source-inline">framework</strong> parameter that <a id="_idIndexMarker932"/>forces RLLib to use <a id="_idIndexMarker933"/>TensorFlow 2. We will also shut down ray at the end of the script:<p class="source-code"># Register custom-model in ModelCatalog</p><p class="source-code">ModelCatalog.register_custom_model("CustomCNN", </p><p class="source-code">                                    CustomModel)</p><p class="source-code">experiment_analysis = tune.run(</p><p class="source-code">    "PPO",</p><p class="source-code">    config={</p><p class="source-code">        "env": "procgen:procgen-coinrun-v0",</p><p class="source-code">        "num_gpus": 0,</p><p class="source-code">        "num_workers": 2,</p><p class="source-code">        "model": {"custom_model": "CustomCNN"},</p><p class="source-code">        "framework": "tf2",</p><p class="source-code">        "log_level": "INFO",</p><p class="source-code">    },</p><p class="source-code">    local_dir="ray_results",  # store experiment results</p><p class="source-code">    #  in this dir</p><p class="source-code">)</p><p class="source-code">ray.shutdown()</p></li>
				<li>We will look at another quick recipe (<strong class="source-inline">5_2_custom_training_using_tune.py</strong>) to customize <a id="_idIndexMarker934"/>the training loop. We <a id="_idIndexMarker935"/>will split the implementation <a id="_idIndexMarker936"/>into the <a id="_idIndexMarker937"/>following few steps to keep it <a id="_idIndexMarker938"/>simple. In this step, we will <a id="_idIndexMarker939"/>import the necessary libraries and initialize ray: <p class="source-code">import sys</p><p class="source-code">import ray</p><p class="source-code">import ray.rllib.agents.impala as impala</p><p class="source-code">from ray.tune.logger import pretty_print</p><p class="source-code">from ray.rllib.models import ModelCatalog</p><p class="source-code">if not "." in sys.path:</p><p class="source-code">    sys.path.insert(0, ".")</p><p class="source-code">from custom_model import CustomModel</p><p class="source-code">ray.init()  # You can also initialize a multi-node ray </p><p class="source-code"># cluster here</p></li>
				<li>Now, let’s <a id="_idIndexMarker940"/>register our custom model with RLLib’s <strong class="source-inline">ModelCatalog</strong> and configure the <strong class="bold">IMPALA agent</strong>. We could very well use any other RLLib support agents, such as PPO or SAC: <p class="source-code"># Register custom-model in ModelCatalog</p><p class="source-code">ModelCatalog.register_custom_model("CustomCNN", </p><p class="source-code">                                    CustomModel)</p><p class="source-code">config = impala.DEFAULT_CONFIG.copy()</p><p class="source-code">config["num_gpus"] = 0</p><p class="source-code">config["num_workers"] = 1</p><p class="source-code">config["model"]["custom_model"] = "CustomCNN"</p><p class="source-code">config["log_level"] = "INFO"</p><p class="source-code">config["framework"] = "tf2"</p><p class="source-code">trainer = impala.ImpalaTrainer(config=config,</p><p class="source-code">                        env="procgen:procgen-coinrun-v0")</p></li>
				<li>We can <a id="_idIndexMarker941"/>now implement <a id="_idIndexMarker942"/>our custom training loop <a id="_idIndexMarker943"/>and include any steps <a id="_idIndexMarker944"/>in the loop as we desire. We <a id="_idIndexMarker945"/>will keep the example <a id="_idIndexMarker946"/>loop simple by simply performing a training step and saving the agent’s model every n(100) epochs:<p class="source-code">for step in range(1000):</p><p class="source-code">    # Custom training loop</p><p class="source-code">    result = trainer.train()</p><p class="source-code">    print(pretty_print(result))</p><p class="source-code">    if step % 100 == 0:</p><p class="source-code">        checkpoint = trainer.save()</p><p class="source-code">        print("checkpoint saved at", checkpoint</p></li>
				<li>Note that we could continue to train the agent using the saved checkpoint and using <a id="_idIndexMarker947"/>the simpler ray tune’s run <a id="_idIndexMarker948"/>API as shown here <a id="_idIndexMarker949"/>as an example:<p class="source-code"># Restore agent from a checkpoint and start a new </p><p class="source-code"># training run with a different config</p><p class="source-code">config["lr"] =  ray.tune.grid_search([0.01, 0.001])"]</p><p class="source-code">ray.tune.run(trainer, config=config, restore=checkpoint)</p></li>
				<li>Finally, let’s <a id="_idIndexMarker950"/>shut down ray to free <a id="_idIndexMarker951"/>up system resources:<p class="source-code">ray.shutdown()</p></li>
			</ol>
			<p>That completes <a id="_idIndexMarker952"/>this recipe! In the next section, let’s recap what we discussed in this recipe.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor243"/>How it works...</h2>
			<p>We identified one of the common limitations with the simple but limited command-line interface of ray RLLib. We also discussed a solution to overcome the failure in step 2 where a custom model was needed to use RLLib’s PPO agent training and implemented it in steps 9 and 10.</p>
			<p>While the solution discussed in steps 9 and 10 looks elegant, it may not provide all the customization knobs you are looking for or are familiar with. For example, it abstracts away the basic RL loop that steps through the environment. We implemented another quick recipe starting from step 11 that allows the customization of the training loop. In step 12, we saw how we can register our custom model and use it with the IMPALA agent – which is a scalable, distributed Deep RL agent based on IMPortance-weighted Actor-Learner Architecture. IMPALA agent actors communicate sequences of states, actions, and rewards to a centralized learner where batch gradient updates take place, in contrast <a id="_idIndexMarker953"/>to the (asynchronous) Actor-Critic-based <a id="_idIndexMarker954"/>agents where the gradients <a id="_idIndexMarker955"/>are communicated to a centralized parameter server.</p>
			<p>For more <a id="_idIndexMarker956"/>information on Tune, you can <a id="_idIndexMarker957"/>refer to the Tune user guide and <a id="_idIndexMarker958"/>configuration documentation <a id="_idIndexMarker959"/>at <a href="https://docs.ray.io/en/master/tune/user-guide.html">https://docs.ray.io/en/master/tune/user-guide.html</a>.</p>
			<p>For more <a id="_idIndexMarker960"/>information on RLLib training APIs and configuration documentation, you can refer to <a href="https://docs.ray.io/en/master/rllib-training.html">https://docs.ray.io/en/master/rllib-training.html</a>.</p>
			<p>That completes the recipe and the chapter! Hope you feel empowered with the new skills and knowledge you have gained to speed up your Deep RL agent training. See you in the next chapter!</p>
		</div>
</body></html>