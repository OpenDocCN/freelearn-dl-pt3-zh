- en: '*Chapter 6*: Generative Models and Adversarial Attacks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Being able to differentiate between two or more classes is certainly impressive,
    and a healthy sign that deep neural networks do, in fact, learn.
  prefs: []
  type: TYPE_NORMAL
- en: But if traditional classification is impressive, then producing new content
    is staggering! That definitely requires a superior understanding of the domain.
    So, are there neural networks capable of such a feat? You bet there are!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll study one of the most captivating and promising types
    of neural networks: **Generative Adversarial Networks** (**GANs**). As the term
    implies, these networks are actually a system comprised of two sub-networks: the
    generator and the discriminator. The job of the generator is to produce images
    so good that they *could* come from the original distribution (but actually don''t;
    they''re generated from scratch), thereby fooling the discriminator, whose task
    is to discern between real and fake images.'
  prefs: []
  type: TYPE_NORMAL
- en: '**GANs** are the tip of the spear in areas such as semi-supervised learning
    and image-to-image translation, both topics that we will cover in this chapter.
    As a complement, the final recipe in this chapter teaches us how to perform an
    adversarial attack on a network using the **Fast Gradient Signed Method** (**FGSM**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The recipes that we will cover in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a deep convolutional GAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a DCGAN for semi-supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating images with Pix2Pix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating unpaired images with CycleGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an adversarial attack using the Fast Gradient Signed Method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GANs are great, but also extremely taxing in terms of computing power. Therefore,
    a GPU is a must-have in order to work on these recipes (and even then, most will
    run for several hours). In the *Getting ready* section, you''ll find the preparations
    that are necessary, if any, for each recipe. The code for this chapter is available
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/35Z8IYn](https://bit.ly/35Z8IYn).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a deep convolutional GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `seed`, which is just a vector of Gaussian noise.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll implement a `EMNIST`, a dataset that extends the well-known
    `MNIST` dataset with uppercase and lowercase handwritten letters on top of the
    digits from 0 to 9\.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll need to install `tensorflow-datasets` to access `EMNIST` more easily.
    Also, in order to display a nice progress bar during the training of our GAN,
    we'll use `tqdm`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both dependencies can be installed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We are good to go!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to implement a DCGAN on `EMNIST`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an alias for the `AUTOTUNE` setting, which we''ll use later to determine
    the number of parallel calls when processing the images in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a `DCGAN()` class to encapsulate our implementation. The constructor
    creates the discriminator, generator, loss function, and the respective optimizers
    for both sub-networks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a static method to create the generator network. It reconstructs a 28x28x1
    image from an input tensor of 100 elements. Notice the use of transposed convolutions
    (`Conv2DTranspose`) to expand the output volumes as we go deeper into the network.
    Also, notice the activation is `''tanh''`, which means the outputs will be in
    the range [-1, 1]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the first transposed convolution block, with 128 filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the second transposed convolution block, with 64 filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the last transposed convolution block, with only one filter, corresponding
    to the output of the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a static method to create the discriminator. This architecture is a
    regular CNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to calculate the discriminator''s loss, which is the sum of
    the real and fake losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to calculate the generator''s loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to perform a single training step. We''ll start by generating
    a vector of random Gaussian noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, pass the random noise to the generator to produce fake images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the real and fake images to the discriminator and compute the losses of
    both sub-networks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, apply the gradients using the respective optimizers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, define a method to train the whole architecture. Every 10 epochs,
    we will plot the images the generator produces in order to visually assess their
    quality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to produce new images, and then save a 4x4 mosaic of them
    to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to scale the images that come from the `EMNIST` dataset to
    the [-1, 1] interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `EMNIST` dataset using `tfds`. We''ll only use the `''train''` split,
    which contains more than 600,000 images. We will also make sure to scale each
    image to the `''tanh''` range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a test seed that will be used throughout the training of the DCGAN to
    generate images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, instantiate and train a `DCGAN()` instance for 200 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first image generated by the GAN will look similar to this, just a collection
    of shapeless blobs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Images generated at epoch 0'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_06_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – Images generated at epoch 0
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the training process, the results are much better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Images generated at epoch 200'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_06_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Images generated at epoch 200
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6.2*, we can distinguish familiar letters and numbers, including
    *A*, *d*, *9*, *X*, and *B*. However, in the first row, we notice a couple of
    ambiguous forms, which is a sign that the generator has room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how it all works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned that GANs work in tandem and, unlike autoencoders,
    they work against each other (hence the *adversarial* in the name) instead of
    cooperating. When our focus is on the generator, the discriminator is just a tool
    to train the latter, as is the case in this recipe. This means that after training,
    the discriminator is tossed out.
  prefs: []
  type: TYPE_NORMAL
- en: Our generator is actually a decoder that takes random Gaussian vectors of 100
    elements and produces 28x28x1 images that are then passed to the discriminator,
    a regular CNN, which has to guess whether they are real or fake.
  prefs: []
  type: TYPE_NORMAL
- en: Because our goal is to create the best generator possible, the classification
    problem the discriminator tries to solve has nothing to do with the actual classes
    in `EMNIST`. For this reason, we don't explicitly label the images as real or
    fake beforehand, but in the `discriminator_loss()` method, where we know that
    all images in `real` come from `EMNIST`, and therefore we compute the loss against
    a tensor of ones (`tf.ones_like(real)`) and, analogously, all images in `fake`
    are synthetic, and we compute the loss against a tensor of zeros (`tf.zeros_like(fake)`).
  prefs: []
  type: TYPE_NORMAL
- en: The generator, on the other hand, takes into consideration the feedback received
    from the discriminator when computing its loss to improve its outputs.
  prefs: []
  type: TYPE_NORMAL
- en: It must be noted that the goal here is to achieve an equilibrium, instead of
    minimizing the loss. Therefore, visual inspection is crucial, and the reason why
    we save the images the generator produces every 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we went from random, shapeless blobs at epoch 0 to recognizable
    digits and letters at epoch 200, although the network can be improved further.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about `EMNIST` here: [https://arxiv.org/abs/1702.05373v1](https://arxiv.org/abs/1702.05373v1).'
  prefs: []
  type: TYPE_NORMAL
- en: Using a DCGAN for semi-supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is the most important part of developing any deep learning model. However,
    good data is often scarce and expensive to acquire. The good news is that GANs
    can lend us a hand in these situations by artificially producing novel training
    examples, in a process known as **semi-supervised learning**.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll develop a special DCGAN architecture to train a classifier
    on a very small subset of `Fashion-MNIST` and still achieve a decent performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We won''t require anything extra to access `Fashion-MNIST` because it comes
    bundled with TensorFlow. In order to display a nice-looking progress bar, let''s
    install `tqdm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Let's now move on to the next section to start the recipe's implementation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `pick_supervised_subset()` function to pick a subset of the data.
    This will allow us to simulate a situation of scarce data, a perfect fit for semi-supervised
    learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define a function to select a random sample of data for classification.
    This means that we''ll use the labels from the original dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `pick_samples_for_discrimination()` function in order to select
    a random sample for discrimination. The main difference with the last function
    is that the labels here are all 1, indicating that all images are real, which
    clearly indicates that this sample is intended for the discriminator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `generate_fake_samples()` function to produce a batch of latent
    points or, put another way, a sample of random noise vectors that the generator
    will use to generate fake images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `generate_fake_samples()` function to generate fake data using the
    generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are ready to define our semi-supervised DCGAN, which we''ll encapsulate
    in the `SSGAN()` class defined here. We''ll start with the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After storing the arguments as members, let''s instantiate the discriminators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, compile both the classifier and discriminator models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the GAN and compile it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the private `_create_discriminators()` method to create the discriminators.
    The inner `custom_activation()` function is used to activate the outputs of the
    classifier model and generate a value between 0 and 1 that will be used to discern
    whether the image is real or fake:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the classifier architecture, which is just a regular softmax-activated
    CNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The discriminator shares weights with the classifier, but instead of softmax
    activating the outputs, it uses the `custom_activation()` function defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Return both the classifier and the discriminator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the private `_create_generator()` method to implement the generator
    architecture, which is just a decoder, as explained in the first recipe in this
    chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the private `_create_gan()` method to create the GAN itself, which is
    just the connection between the generator and the discriminator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, define `train()`, a function to train the whole system. We''ll start
    by selecting the subset of `Fashion-MNIST` that we''ll train on, and then we''ll
    define the number of batches and training steps required to fit the architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pick samples for classification, and use these to fit the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pick real samples for discrimination, and use these to fit the discriminator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the generator to produce fake data, and use this to fit the discriminator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate latent points, and use these to train the GAN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load `Fashion-MNIST` and normalize both the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate an `SSCGAN()` and train it for 30 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Report the accuracy of the classifier on both the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the training finishes, both the training and test accuracy should be around
    83%, which is pretty satisfying if we consider we only used 1,000 examples out
    of 50,000!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we implemented an architecture quite similar to the one implemented
    in the *Implementing a deep convolutional GAN* recipe that opened this chapter.
    The main difference resides in the fact that we have two discriminators: the first
    one is actually a classifier, which is trained on the small subset of labeled
    data at our disposal. The other is a regular discriminator, whose sole job is
    to not be fooled by the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: How does the classifier achieve such a respectable performance with so little
    data? The answer is shared weights. Both the classifier and the discriminator
    share the same feature extraction layers, differing only in the final output layer,
    which is activated with a plain old softmax function in the case of the classifier,
    and with a `Lambda()` layer that wraps our `custom_activation()` function in the
    case of the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: This means that these shared weights get updated each time the classifier trains
    on a batch of labeled data, and also when the discriminator trains on both real
    and fake images. In the end, we circumvent the data scarcity problem with the
    aid of the generator.
  prefs: []
  type: TYPE_NORMAL
- en: Pretty impressive, right?
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can consolidate your understanding of the semi-supervised training approach
    used in this recipe by reading the paper where it was first proposed: [https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498).'
  prefs: []
  type: TYPE_NORMAL
- en: Translating images with Pix2Pix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most interesting applications of GANs is image-to-image translation,
    which, as the name suggests, consists of translating the content from one image
    domain to another (for instance, sketches to photos, black and white images to
    RGB, and Google Maps to satellite views, among others).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll implement a fairly complex conditional adversarial network
    known as Pix2Pix. We'll focus solely on the practical aspects of the solution,
    but if you want to get familiar with the literature, check out the *See also*
    section at the end of the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll use the `cityscapes` dataset, which is available here: https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/cityscapes.tar.gz.
    Download it and decompress it in a location of your choosing. For the purposes
    of this tutorial, we will assume that it''s placed in the `~/.keras/datasets`
    directory, under the name `cityscapes`. To display a progress bar during training,
    install `tqdm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'By the end of this recipe, we''ll learn to generate the image on the left from
    the right one using Pix2Pix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – We will use the segmented images on the right to produce real-world
    images like the one on the left](img/B14768_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – We will use the segmented images on the right to produce real-world
    images like the one on the left
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After completing these steps, you'll have implemented Pix2Pix from scratch!
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define constants for TensorFlow''s autotuning and resizing options, as well
    as the dimensions. We will resize all the images in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each image in the dataset is comprised of both the input and target, so after
    processing it, we need to split them into separate images. The `load_image()`
    function does this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create the `resize()` function to resize both the input and target images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, implement the `random_crop()` function to perform random cropping on the
    images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, code up the `normalize()` function to normalize the images to the range
    [-1, 1]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `random_jitter()` function, which performs random jittering on the
    input images (notice that it uses the functions defined in *Step 4* and *Step
    5*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `load_training_image()` function to load and augment the training
    images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now implement the `load_test_image()` function, which, as its name indicates,
    will be used to load test images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s proceed to create the `generate_and_save_images()` function to
    store synthetic images created by the generator model. The resulting images will
    be a concatenation of `input`, `target`, and `prediction`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the `Pix2Pix()` class, which encapsulates this architecture implementation.
    Start with the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The constructor implemented in *Step 11* defines the loss function to be used
    (**binary cross-entropy**), the lambda value (used in *Step 18*), and instantiates
    the generator and the discriminator, as well as their respective optimizers. Our
    generator is a modified **U-Net**, which is a U-shaped network comprising downsampling
    and upsampling blocks. Let''s create a static method to produce a downsample block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A downsample block is a convolution, optionally batch normalized, and activated
    with `LeakyReLU()`. Let''s now implement a static method to create upsampling
    blocks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An upsampling block is a transposed convolution, optionally followed by dropout
    and with `ReLU()` activated. Let''s now use these two convenience methods to implement
    the U-Net generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After defining the downsampling stack, let''s do the same with the upsampling
    layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Thread the input through the down and up stacks, and also add skip connections
    to prevent the depth of the network from impeding its learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output layers are a transposed convolution with `''tanh''` activated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to compute the generator loss, as the authors of Pix2Pix recommend.
    Notice the use of the `self._lambda` constant:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The discriminator, defined in this step, receives two images; the input and
    the target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice that the last couple of layers are convolutions, instead of `Dense()`
    layers. This is because the discriminator works on patches of images at a time,
    and tells whether each patch is real or fake:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the discriminator loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to perform a single train step, named `train_step()`, consisting
    of taking the input image, passing through the generator, and then using the discriminator
    on the input image paired with the original target image, and then on the input
    imaged paired with the fake image output from the generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, the losses are computed, along with the gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the gradients to update the models through the respective optimizers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement `fit()`, a method to train the whole architecture. For each epoch,
    we''ll save to disk the images generated to visually assess the performance of
    the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assemble the path to the training and test splits of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the training and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `Pix2Pix()` and fit it over 150 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s a generated image at epoch 1:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.4 – At first, the generator only produces noise'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_06_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – At first, the generator only produces noise
  prefs: []
  type: TYPE_NORMAL
- en: 'And here''s one at epoch 150:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – At the end of its training run, the generator is capable of
    producing reasonable results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_06_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – At the end of its training run, the generator is capable of producing
    reasonable results
  prefs: []
  type: TYPE_NORMAL
- en: When the training ends, our Pix2Pix architecture can translate segmented images
    to real scenes, as demonstrated in *Figure 6.5*, where the first image is the
    input, the second is the target, and the rightmost is the generated one.
  prefs: []
  type: TYPE_NORMAL
- en: Let's connect the dots in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented an architecture which was a bit hard, but was
    based, but based on the same ideas as all GANs. The main difference is that this
    time, the discriminator works on patches, instead of whole images. More specifically,
    the discriminator looks at patches of the original and fake images at a time and
    decides whether those patches belong to real or synthetized images.
  prefs: []
  type: TYPE_NORMAL
- en: Because image-to-image translation is a form of image segmentation, our generator
    is a modified U-Net, a groundbreaking type of CNN first used for biomedical image
    segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Because Pix2Pix is such a complex and deep network, the training process takes
    several hours to complete, but in the end, we obtained very good results translating
    the content of segmented city landscapes to real-looking predictions. Impressive!
  prefs: []
  type: TYPE_NORMAL
- en: If you want to take a look at other produced images, as well as a graphical
    representation of the generator and discriminator, consult the official repository
    at [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe3](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe3).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I recommend you read the original paper by Phillip Isola, Jun-Yan Zhu, Tinghui
    Zhou, and Alexei A. Efros, the authors of **Pix2Pix**, here: [https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004).
    We used a U-Net as the generator, which you can read more about here: [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597).'
  prefs: []
  type: TYPE_NORMAL
- en: Translating unpaired images with CycleGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Translating images with Pix2Pix* recipe, we discovered how to transfer
    images from one domain to another. However, in the end, it's supervised learning
    that requires a pairing of input and target images in order for Pix2Pix to learn
    the correct mapping. Wouldn't it be great if we could bypass this pairing condition,
    and let the network figure out on its own how to translate the characteristics
    from one domain to another, while preserving image consistency?
  prefs: []
  type: TYPE_NORMAL
- en: Well, that's what **CycleGAN** does, and in this recipe, we'll implement one
    from scratch to convert pictures of Yosemite National Park taken during the summer
    into their winter counterparts!
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll use `OpenCV`, `tqdm`, and `tensorflow-datasets` in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install these simultaneously with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Through the TensorFlow datasets, we'll access the `cyclegan/summer2winter_yosemite`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sample images of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Left: Yosemite during summer; right: Yosemite during winter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_06_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.6 – Left: Yosemite during summer; right: Yosemite during winter'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of CycleGAN is very similar to Pix2Pix. Therefore, we won't
    explain most of it in detail. Instead, I encourage you to complete the *Translating
    images with Pix2Pix* recipe before tackling this one.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to perform the random cropping of an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to normalize images to the range [-1, 1]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to perform random jittering on an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to preprocess and augment training images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to preprocess test images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to generate and save images using the generator model. The
    resulting images will be a concatenation of the input and the prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a custom instance normalization layer, starting with the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define the `build()` method, which creates the inner components of the
    `InstanceNormalization()` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `call()` method, which implements the logic to instance-normalize
    the input tensor, `x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a class to encapsulate the CycleGAN implementation. Start with the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The main difference with Pix2Pix is that we have two generators (`gen_g` and
    `gen_f`) and two discriminators (`dis_x` and `dis_y`). `gen_g` learns how to transform
    image X to image Y, and `gen_f` learns how to transform image Y to image Y. Analogously,
    `dis_x` learns to differentiate between the real image X and the one generated
    by `gen_f`, while `dis_y` learns to differentiate between the real image Y and
    the one generated by `gen_g`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s create a static method to produce downsampling blocks (this is
    the same as in the last recipe, only this time we use instance instead of batch
    normalization):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, define a static method to produce upsampling blocks (this is the same
    as in the last recipe, only this time we use instance instead of batch normalization):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to build the generator. Start by creating the downsampling
    layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, create the upsampling layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Thread the input through the downsampling and upsampling layers. Add skip connections
    to avoid the vanishing gradient problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output layers are a `''tanh''` activated transposed convolution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to calculate the generator loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to create the discriminator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the last couple of layers, which are convolutional:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to compute the discriminator loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to compute the loss between the real and cycled images. This
    loss is in charge of quantifying the cycle consistency, which says that if you
    translate an image X to Y, and then Y to X, the result should be X, or close to
    X:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to compute the identity loss. This loss establishes that if
    you pass image Y through `gen_g`, we should obtain the real image Y or something
    close to it (the same applies to `gen_f`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to perform a single training step. It receives images X and
    Y from different domains. Then, it uses `gen_g` to translate X to Y, and `gen_f`
    to translate Y to X:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, pass X through `gen_f` and Y through `gen_y` to later compute the identity
    loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass real X and fake X to `dis_x`, and real Y, along with generated Y, to `dis_y`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the generators'' losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the cycle loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the identity loss and the total generator G loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat for generator F:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the discriminators'' losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the gradients for the generators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the gradients for the discriminators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the gradients to each generator using the respective optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the gradients to each discriminator using the respective optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to fit the whole architecture. It will save to disk the images
    produced by generator G after each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Unpack the training and test splits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the data processing pipelines for the training spit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the data processing pipelines for the test split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of `CycleGAN()` and train it for 40 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At epoch 1, we''ll notice that the network hasn''t learned much:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Left: original image during summer; right: translated image
    (winter)](img/B14768_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7 – Left: original image during summer; right: translated image (winter)'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, at epoch 40, the results are more promising:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Left: original image during summer; right: translated image
    (winter)](img/B14768_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8 – Left: original image during summer; right: translated image (winter)'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding image, our `CycleGAN()` added a little more white
    to certain parts of the trail and the trees to make the translated image seem
    like it was taken during winter. Of course, training for more epochs can potentially
    lead to better results, which I encourage you to do to solidify your understanding
    of CycleGANs!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned that CycleGANs work in a very similar fashion to
    Pix2Pix. However, the biggest advantage is that a CycleGAN doesn't require a dataset
    of paired images to achieve its goal. Instead, it relies on two sets of generators
    and discriminators, which, in fact, create a learning cycle, hence the name.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, CycleGANs work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A generator G must learn a mapping from an image X to an image Y.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A generator F must learn a mapping from an image Y to an image X.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discriminator D(X) must distinguish the real image X from the fake one generated
    by G.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discriminator D(Y) must distinguish the real image Y from the fake one generated
    by F.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two conditions that ensure that the translation preserves the meaning
    in both domains (very much like when we want to preserve the meaning of our words
    when we translate from English to Spanish, and vice versa):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cycle consistency: Going from X to Y and then from Y to X should produce the
    original X or something very similar to X. The same applies to Y.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identity consistency: Passing X to G should produce the same X or something
    very similar to X. The same applies to Y.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these four components, CycleGAN tries to preserve the cycle and identity
    consistency in the translation, which generates very satisfying results without
    the need for supervised, paired data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read the original paper on CycleGANs here: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593).
    Also, here is a very interesting thread to understand the difference between instance
    and batch normalization: [https://intellipaat.com/community/1869/instance-normalisation-vs-batch-normalisation](https://intellipaat.com/community/1869/instance-normalisation-vs-batch-normalisation).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an adversarial attack using the Fast Gradient Signed Method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often think of highly accurate deep neural networks as robust models, but
    the **Fast Gradient Signed Method** (**FGSM**), proposed by no other than the
    father of GANs himself, Ian Goodfellow, showed otherwise. In this recipe, we'll
    perform an FGSM attack on a pre-trained model to see how, by introducing seemingly
    imperceptible changes, we can completely fool a network.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's install `OpenCV` with `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use it to save the perturbed images using the FGSM method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After completing the following steps, you''ll have successfully performed an
    adversarial attack:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to preprocess an image, which entails resizing it and applying
    the same treatment as the pre-trained network we''ll use (in this case, `NASNetMobile`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to get the human-readable image from a set of probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to save an image. This will use the pre-trained model to
    get the proper label and will utilize it as part of the filename of the image,
    which also contains the prediction confidence percentage. Prior to storing the
    image on disk, it ensures that it''s in the expected [0, 255] range, as well as
    in BGR space, which is the one used by OpenCV:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to create the adversarial pattern that will be used later
    on to perform the actual FGSM attack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The pattern is pretty simple: It consists of a tensor with the sign of the
    gradient in each element. More specifically, `signed_gradient` will contain a
    `-1` for gradient values below `0`, `1` for values above `0`, and `0` if the gradient
    is, well, `0`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Instantiate the pre-trained `NASNetMobile()` model and freeze its weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the test image and pass it through the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'One-hot encode the ground truth label of the original image, and use it to
    generate the adversarial pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform a series of adversarial attacks using increasing, yet small, values
    of `epsilon`, which will be applied in the direction of the gradient, leveraging
    the pattern present in `disturbances`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For epsilon = 0 (no attack), the image looks like this, and the label is `pug`
    with an 80% confidence:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Original image. Label: pug (80.23% confidence)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_06_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.9 – Original image. Label: pug (80.23% confidence)'
  prefs: []
  type: TYPE_NORMAL
- en: 'When epsilon = 0.005 (a very small perturbation), the label changes to `Brabancon_griffon`,
    with a 43.03% confidence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Epsilon = 0.005 applied in the gradient direction. Label: Brabancon_gritton
    (43.03% ](img/B14768_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10 – Epsilon = 0.005 applied in the gradient direction. Label: Brabancon_gritton
    (43.03% confidence)'
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the preceding image, an imperceptible variation in the pixel
    values produced a drastically different response from the network. However, the
    situation worsens the more we increment the magnitude of epsilon. For a complete
    list of results, refer to [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe5).
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a fairly simple attack based on the FGSM proposed
    by Ian Goodfellow, which simply consists of determining the direction (sign) of
    the gradient at each location and using that information to create an adversarial
    pattern. The underlying principle is that this technique maximizes the loss at
    each pixel value.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use this pattern to either add or subtract a small perturbation to
    each pixel in the image that gets passed to the network.
  prefs: []
  type: TYPE_NORMAL
- en: Although these changes are often imperceptible to the human eye, they have the
    power to completely confuse a network, resulting in nonsensical predictions, as
    demonstrated in the last step of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fortunately, many defenses against this type of attack (and more sophisticated
    ones) have emerged. You can read a pretty interesting survey of adversarial attacks
    and defenses here: [https://arxiv.org/abs/1810.00069](https://arxiv.org/abs/1810.00069).'
  prefs: []
  type: TYPE_NORMAL
