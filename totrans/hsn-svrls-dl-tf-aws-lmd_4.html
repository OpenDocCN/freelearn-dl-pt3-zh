<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Working with TensorFlow on AWS Lambda</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn about the architecture of deploying TensorFlow with AWS, and we will deploy TensorFlow on AWS Lambda using the pre-existing pack and the serverless framework.We will also look into the various general issues with deploying the various Python frameworks on AWS Lambda and then cover all of the solutions to the same issues. </p>
<p>We will cover the following topics:</p>
<ul>
<li>Architecture of deploying TensorFlow with AWS Lambda</li>
<li>General issues with deploying Python frameworks on AWS Lambda</li>
<li>Deploying TensorFlow on AWS Lambda using pre-existing pack</li>
<li>Deploying TensorFlow using a serverless framework</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical Requirements</h1>
                </header>
            
            <article>
                
<ul>
<li>AWS subscription</li>
<li>Python 3.6</li>
<li>AWS CLI</li>
<li>Serverless framework</li>
<li>You can find all the codes at:<span> </span><a href="https://github.com/PacktPublishing/Hands-On-Serverless-Deep-Learning-with-TensorFlow-and-AWS-Lambda">https://github.com/PacktPublishing/Hands-On-Serverless-Deep-Learning-with-TensorFlow-and-AWS-Lambda</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture of the deploying TensorFlow with AWS Lambda</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn about the architecture of deploying TensorFlow with AWS Lambda. One of the critical questions of deployment is about where to keep the retrained model that will be used within AWS Lambda.</p>
<p>There are the following three possible options:</p>
<ul>
<li>Keep the model within the deployment package alongside the code and libraries</li>
<li>Keep the model on the S3 bucket and unload it in AWS Lambda during execution</li>
<li>Keep the model on the FTP or HTTP server and unload it into AWS Lambda during execution</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model within the deployment package</h1>
                </header>
            
            <article>
                
<p>This option means that the model is within the deployment package. The code will import it from a local filesystem. This option has its own pros and cons.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pros</h1>
                </header>
            
            <article>
                
<p>The advantages of the model within the deployment package are as follows:</p>
<ul>
<li><span class="MsoBookTitle">We will get a very good start speed for our deployment since there is no overhead on the loading model</span></li>
<li><span class="MsoBookTitle">We will have a single package to start with</span></li>
<li><span class="MsoBookTitle"> We won't need any outside servers or AWS services as part of our deployment</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cons</h1>
                </header>
            
            <article>
                
<p>The disadvantages of the model within the deployment package are as follows:</p>
<ul>
<li>There is considerable limitation on the package size and it limits the possible size of our model</li>
<li>In the case where you need to manage different versions of the model, it may be tricky to either keep them all in one package or deal with different versions of your package</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model on the S3 bucket</h1>
                </header>
            
            <article>
                
<p>This option means that we have to keep the model in the S3 bucket and unload it during AWS Lambda execution. This option is very limited in terms of package size.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pros</h1>
                </header>
            
            <article>
                
<p>The advantages of the model on the S3 bucket are as follows:</p>
<ul>
<li>At first glance, it will be limited to only 500 MB of usage, which is the maximum size of TMP folder on AWS Lambda, but it is actually possible to download the model directly into memory by passing this limit</li>
<li>It will be a lot easier to manage multiple models as you can use AWS Lambda environmental variables to procure equipment links to S3 bucket for each of the models that you want to use</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cons</h1>
                </header>
            
            <article>
                
<p>The disadvantages of the model on the S3 bucket are as follows:</p>
<ul>
<li>We will get a slower start than in previous cases, since Lambda will need to download the model first</li>
<li>It should be noted that, though it happens only during cold start, during warm start, the model will already be in memory</li>
<li>You will need to make the S3 bucket upload all of your models as a part of your deployment, and add logic for managing the different models within the code</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model on the HTTP/FTP server</h1>
                </header>
            
            <article>
                
<p>This option is mostly useful for the case where you want to limit the use of AWS services, memory or integrate with services outside of AWS. The AWS Lambda downloads the model from HTTP or FTP server during deployment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pros</h1>
                </header>
            
            <article>
                
<p>The advantages of the model on the HTTP/FTP server are as follows:</p>
<ul>
<li>You can use a number of publicly available services with models</li>
<li>You don't have to update your model on S3 bucket or within the package</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cons</h1>
                </header>
            
            <article>
                
<p>The disadvantages of the model on the HTTP/FTP server are as follows:</p>
<ul>
<li>It may be even slower than with the previous case, which is a downside of this model</li>
<li>Due to the slower time, you will need to make sure that the server is available from your location</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General issues with deploying Python frameworks on AWS Lambda</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn about AWS Lambda main limit, which is also known as the size of the package. The current limit for the Lambda deployment package is 50 MB. It is supposed to include libraries and code. There are two main libraries that we need to fit:</p>
<ul>
<li>TensorFlow</li>
<li>NumPy</li>
</ul>
<p>These libraries are used for matrix calculations. As you may know, the libraries by themselves are pretty big and they just wouldn't work on AWS Lambda. As you have already seen in the previous section on deployment that when we deploy them through S3, we don't have this limitation, and we only have 250 MB limitation for the unzipped package. In this case to make it work, we need to reduce the size of the package.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Solutions for issues with deploying Python frameworks on AWS Lambda</h1>
                </header>
            
            <article>
                
<p>There are a number of ways as to how we can reduce the package size. Here are the solutions for the issues in question:</p>
<ul>
<li>We can compress the shared libraries; this usually enables us to get the best reduction of size.</li>
<li>We can remove the <kbd>.pyc</kbd> files as they do not influence the library work.</li>
<li>Next, we can remove tests and visualization folders from the libraries as they are not useful in production.</li>
<li>Next, we can remove libraries that already exist on AWS Lambda.</li>
<li>Finally, we can check and remove the libraries that aren't used during execution, for example, wheel or PIP libraries.</li>
</ul>
<p>Now, in the following code, there is the part that finds and compresses all shared libraries. Then, we find and delete all <kbd>.pyc</kbd> files.</p>
<p>The following screenshot shows the commands for the preceding explanation:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/20c2e405-bef7-4cf5-a70a-bcf2add76128.png" style="width:22.75em;height:6.75em;"/></p>
<p>Next, we need to delete libraries that won't be used during execution, such as <kbd>.pip</kbd> and <kbd>wheel</kbd>. Finally, we can also delete some folders from TensorFlow library.</p>
<p>The following screenshot shows the different commands for the preceding explanation:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b707219e-0514-45d0-ba06-2ae2ab8777fa.png" style="width:21.58em;height:8.08em;"/></p>
<p>The whole process of preparing a package for AWS Lambda can be done through Docker. You don't need to use it for the project we will create, but it is good to keep in mind how to prepare this kind of package.</p>
<p>To install Docker, you just need to run three comments in your comment line:</p>
<ol>
<li>You need to get the latest Amazon Linux image on which we will run the script.</li>
<li>You need to start a Docker container with the managing output folder inside the container.</li>
</ol>
<ol start="3">
<li>You can run the script inside the container and it will assemble the package for you. The following screenshot displays all of the commands to install Docker:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5af485f9-ae76-479d-9a5a-3e4d5144bcf0.png" style="width:37.83em;height:11.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying TensorFlow on AWS Lambda using the pre-existing pack</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn to deploy TensorFlow on AWS Lambda using the pre-existing pack. In the project files, we have model files that are also known as the model itself and files that enable us to translate model response through labels. In <kbd>Inception</kbd> folder and Lambda package, which is also known as the code and libraries in <kbd>lambdapack</kbd> folder.</p>
<p>To run the code, we need to do the following:</p>
<ul>
<li>We'll create S3 bucket where we will keep the model and upload the model itself</li>
<li>Then,we'll modify code for the specific bucket and add the created bucket name</li>
<li>Lastly, we can package it and upload to add the AWS Lambda</li>
</ul>
<p>Now, we will create S3 bucket using the AWS Console and upload files there. We will open the code and add the bucket that we have just created. Then, let's package it and upload it to add AWS Lambda.</p>
<p>We will have to follow the given steps:</p>
<ol>
<li>We need to go to the S3 service and click <span class="packt_screen">Create bucket</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b5a2b678-1b9c-4697-adbc-1135d167ae79.png"/></p>
<ol start="2">
<li>Now, we can choose the bucket name:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/00a6f22d-931b-4cf4-b9ce-c845814d4794.png"/></p>
<ol start="3">
<li>Once we have the bucket in place, we can upload files there. You just need to click <span class="packt_screen">Upload</span> and then choose files. So, here we just upload the package with libraries, and it will start the upload process, along with the package itself. We will need to upload model files, which are present in the <kbd>Inception</kbd> folder:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f34cd6e4-a3e2-4a48-9398-4199e7a14df6.png" style="width:29.92em;height:15.00em;"/></p>
<div class="packt_figure CDPAlignCenter CDPAlign"/>
<ol start="4">
<li>You can see that now we have a package inside our S3 bucket:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/682c7e5f-5f9c-4f88-905f-b0ad32720507.png" style="width:50.25em;height:8.75em;"/></p>
<ol start="5">
<li>Now, we have to create the role for our AWS Lambda, which we can do from the IAM service:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ae17137a-3ad4-4bef-87e2-301f05dec8dc.png"/></p>
<ol start="6">
<li>We need to choose <span class="packt_screen">Lambda</span> and click on <span class="packt_screen">Next: Permissions</span>, which lies at the bottom-right of your screen:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/588d2e5d-3f17-47ca-9e68-57637d30be29.png"/></p>
<ol start="7">
<li>For simplicity, it is easier to choose administrator access and click on <span class="packt_screen">Next: Tags,</span> which lies at the bottom-right of your screen. This would allow our Lambda to access all services. Usually in production, the role is limited to accessing only specific services. We will cover this when we work with serverless frameworks:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5c6a709f-8ec2-48a7-9bf9-212c95d8eb07.png"/></p>
<ol start="8">
<li>Create a role name: <kbd>lambdaAdminRole</kbd>, which will create the role in Lambda:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-741 image-border" src="assets/193478df-a06a-41a1-93cd-c7d66ab776b9.png" style="width:82.67em;height:45.92em;"/></p>
<ol start="9">
<li>To create Lambda, navigate to Lambda function and create the function. Here, enter the name <kbd>testensorflolambda</kbd>, <span class="packt_screen">Runtime</span> as <span class="packt_screen">Python 3.6</span>. For <span class="packt_screen">Roles</span>, select <span class="packt_screen">Choose an existing role</span>, and in <span class="packt_screen">Existing role</span>, select <kbd>lambdaAdminRole</kbd>, then click on <span class="packt_screen">Create function</span> at the bottom-right corner:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/72181902-1d22-486f-9804-3251a600c875.png" style="width:51.33em;height:37.25em;"/></p>
<div class="packt_figure CDPAlignCenter CDPAlign"/>
<p style="padding-left: 60px">10. After the function is created, we need to change the <span class="packt_screen">Handler</span> to <kbd>index.handler</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b80f11de-8c3c-43c2-ba43-afb498cc4ae6.png" style="width:45.33em;height:8.08em;"/></p>
<ol start="11">
<li>On the same screen, scroll down and, in the <span class="packt_screen">Basic setting</span> tab, add enough resources as presented in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aac2594e-7e37-4cd4-be1b-676607b34a82.png"/></p>
<ol start="12">
<li>Pass the link with the URL of our package (S3 bucekt) and click on <span class="packt_screen">Save</span> in the top right corner:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ee4da3e6-39d0-4dc0-8790-13c716385262.png" style="width:47.08em;height:15.08em;"/></p>
<ol start="13">
<li>You can see the function is created. To test the function, click on the <span class="packt_screen">Test</span> on the top- right corner:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/53e3a850-3fe8-4ae4-b926-85058b791050.png" style="width:48.33em;height:15.08em;"/></p>
<ol start="14">
<li>After the function is tested, it will successfully produce the following result:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9da215f9-adda-42eb-afd4-bba15ac46475.png" style="width:46.33em;height:11.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying TensorFlow using a serverless framework</h1>
                </header>
            
            <article>
                
<p>First, we will look at the project files. We have model files in the <kbd>Inception</kbd> folder, and Lambda code with <kbd>Serverless.yml</kbd>, the configuration file in the <kbd>Lambdapack</kbd> folder.</p>
<p>The flow of deployment will be the same as in the previous section. One of the main differences will be that, instead of providing an AWS Lambda admin role, we will provide access to bucket by serverless CML file. The only thing we need to add is <kbd>bucketname</kbd>, and run the properties of access as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/410584be-c45d-4925-b33b-75144ce6eb88.png" style="width:25.92em;height:12.17em;"/></p>
<p>We will need to create an S3 bucket, upload files there, and then deploy AWS Lambda. We will create an S3 bucket and upload files from the command line: <kbd>aws s3 sync.s3://&lt;bucket&gt;/</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a bucket</h1>
                </header>
            
            <article>
                
<p>We will first need to create a bucket, then we will need to upload model files to the bucket, run serverless, and start the AWS Lambda.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Index.py</h1>
                </header>
            
            <article>
                
<p>Let's look at the available files. We will look at the <kbd>index.py</kbd> file as shown:</p>
<pre>import boto3<br/>import numpy as np<br/>import tensorflow as tf<br/>import os.path<br/>import re<br/>from urllib.request import urlretrieve<br/>import json<br/>SESSION = None<br/>strBucket = 'serverlessdeeplearning'<br/>def handler(event, context):<br/> global strBucket<br/> if not os.path.exists('/tmp/imagenet/'):<br/> os.makedirs('/tmp/imagenet/')<br/>strFile = '/tmp/imagenet/inputimage.jpg'</pre>
<p>The main difference is that we run the code inside the <kbd>handler</kbd> function and we need to download the model files and image file from the S3 bucket:</p>
<pre>if not os.path.exists('/tmp/imagenet/'):<br/> os.makedirs('/tmp/imagenet/')<br/>strFile = '/tmp/imagenet/inputimage.jpg'<br/>downloadFromS3(strBucket,'imagenet/inputimage.jpg',strFile)<br/>global SESSION<br/> if SESSION is None:<br/>downloadFromS3(strBucket,'imagenet/imagenet_2012_challenge_label_map_proto.pbtxt','/tmp/imagenet/imagenet_2012_challenge_label_map_proto.pbtxt')<br/>downloadFromS3(strBucket,'imagenet/imagenet_synset_to_human_label_map.txt','/tmp/imagenet/imagenet_synset_to_human_label_map.txt')<br/> image = os.path.join('/tmp/imagenet/', 'inputimage.jpg')<br/> strResult = run_inference_on_image(image)<br/>return strResult<br/>def run_inference_on_image(image):</pre>
<p>Also, we can use one of the advantages of AWS Lambda. We can save model files as global variables. Basically, we can define session as a global variable. With these, if we start Lambda right after the previous Lambda was executed, all model files will be in the RAM memory:</p>
<pre>global SESSION<br/> if SESSION is None:<br/> downloadFromS3(strBucket,'imagenet/imagenet_2012_challenge_label_map_proto.pbtxt','/tmp/imagenet/imagenet_2012_challenge_label_map_proto.pbtxt')<br/> downloadFromS3(strBucket,'imagenet/imagenet_synset_to_human_label_map.txt','/tmp/imagenet/imagenet_synset_to_human_label_map.txt')<br/> image = os.path.join('/tmp/imagenet/', 'inputimage.jpg')<br/> strResult = run_inference_on_image(image)<br/>return strResult<br/>def run_inference_on_image(image):<br/> image_data = tf.gfile.FastGFile(image, 'rb').read()<br/> global SESSION<br/> if SESSION is None:<br/> SESSION = tf.InteractiveSession()<br/> create_graph()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Serverless.yml</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">In the <kbd>Serverless.yml</kbd> file, we need to define access to the S3 bucket as that's where we will keep our model. Other than that, it will look exactly as the previously mentioned serverless CML files for other Lambdas:</p>
<pre>service: deeplearninglambda<br/>frameworkVersion: "&gt;=1.2.0 &lt;2.0.0"<br/>provider:<br/>  name: aws<br/>  region: us-east-1<br/>  runtime: python3.6<br/>  memorySize: 1536<br/>  timeout: 60<br/><strong>iamRoleStatements:</strong><br/><strong> - Effect: "Allow"</strong><br/><strong> Action:</strong><br/><strong> - "s3:ListBucket"</strong><br/><strong> Resource:</strong><br/><strong> - arn:aws:s3:::serverlessdeeplearning</strong><br/><strong> - Effect: "Allow"</strong><br/><strong> Action:</strong><br/><strong> - "s3:GetObject"</strong><br/><strong> Resource:</strong><br/><strong> - arn:aws:s3:::serverlessdeeplearning/*</strong><br/>functions:<br/> main:<br/> handler: index.handler</pre>
<p class="mce-root">Also, the we need <kbd>inputimage.jpg</kbd> image for the inception model.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Let's look at the files that we need to upload to S3 bucket:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9bfbd63b-4255-4328-bb0d-41f3e314ff40.png"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">There are two very convenient commands; one allows us to create a bucket, and another allows us to easily upload files into the bucket:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><kbd>aws s3api create-bucket --bucket serverlessdeeplearning</kbd></li>
<li><kbd>aws s3 sync . s3://serverlessdeeplearning/imagenet</kbd></li>
</ul>
<p class="mce-root CDPAlignLeft CDPAlign">Since we already have model files in this bucket, there is no need to hold it now, but you can use this command to upload to your bucket. Next, we can return back to the folder with our function and run <kbd>serverless deploy</kbd> command.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Now, we will invoke the function with the following command:</p>
<pre class="mce-root CDPAlignLeft CDPAlign"><strong>serverless invoke --function main</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign">As you can see, it successfully recognized the image. Also, if we invoke the function one more time after that, it will work faster:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/10ae6ce4-b240-4ed1-b05a-50de48fc9c4c.png" style="width:44.67em;height:11.92em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about the architecture of the deploying TensorFlow with AWS Lambda, in which we covered the possible options of deploying TensorFlow with AWS Lambda along with each of its pros and cons. We also discussed the general issues with deploying Python frameworks in AWS Lambda along with its solutions. Lastly, we deployed TensorFlow on AWS Lambda using the pre-existing pack and using a serverless framework.</p>
<p>In the next chapter, we'll  create deep learning API using AWS Lambda.</p>


            </article>

            
        </section>
    </body></html>