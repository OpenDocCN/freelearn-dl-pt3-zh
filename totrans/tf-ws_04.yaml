- en: 4\. Regression and Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to build regression and classification models
    using TensorFlow. You will build models with TensorFlow utilizing Keras layers,
    which are a simple approach to model building that offer a high-level API for
    building and training models. You will create models to solve regression and classification
    tasks, including the classification of the binding properties of various molecules.
    You will also use TensorBoard to visualize the architecture of TensorFlow models
    and view the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to use some TensorFlow resources to
    aid in development. These included TensorBoard (for visualizing computational
    graphs), TensorFlow Hub (an online repository for machine learning modules), and
    Google Colab (an online Python development environment for running code on Google
    servers). All these resources help machine learning practitioners develop models efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will explore how to create ANNs using TensorFlow. You will
    build ANNs with different architectures to solve regression and classification
    tasks. Regression tasks aim to predict continuous variables from the input training
    data, while classification tasks aim to classify the input data into two or more
    classes. For example, a model to predict whether or not it will rain on a given
    day is a classification task since the result of the model will be of two classes—rain
    or no rain. However, a model to predict the amount of rain on a given day would
    be an example of a regression task since the output of the model would be a continuous
    variable—the amount of rain.
  prefs: []
  type: TYPE_NORMAL
- en: Models that are used to tackle these tasks represent a large class of machine
    learning models, and a huge amount of machine learning problems fall into these
    two categories. This chapter will demonstrate how regression and classification
    models can be created, trained, and evaluated in TensorFlow. You will use much
    of the learning covered in the previous chapters (including using TensorBoard
    to monitor the model training process) to understand how to build performant models.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces the various parameters used to build ANNs (known as
    **hyperparameters**), which include activation functions, loss functions, and
    optimizers. Other hyperparameters to select in the model-fitting process include
    the number of epochs and batch size, which vary the number of times the entire
    dataset is used to update the weights and the number of data points for each update,
    respectively. You will also learn how to log variables during the model-fitting
    process so that they can be visualized in TensorBoard. This allows you to determine
    whether the model is under- or overfitting the training data. Finally, after building
    your model, you will learn how to evaluate it on the dataset to see how well it
    performs.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A sequential model is used to build regression and classification models. In
    sequential models, information propagates through the network from the input layer
    at the beginning to the output layer at the end. Layers are stacked in the model
    sequentially, with each layer having an input and an output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other types of ANN models exist, such as recurrent neural networks (in which
    the output feeds back into the input), which will be covered in later chapters.
    The difference between sequential and recurrent neural networks is shown in *Figure
    4.01*. In both the models, the information flows from the input layer through
    the hidden layers to the output layer, as indicated by the direction of the arrows.
    However, in recurrent architectures, the output of the hidden layers feeds back
    into the input of the hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: The architectures of sequential and recurrent ANNs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1: The architectures of sequential and recurrent ANNs'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, you will learn how to create sequential models in
    TensorFlow that form the basis of regression and classification models. You will
    utilize the Keras API, which is now included as part of the TensorFlow library
    for sequential models, since the high-level API provides a simple interface for
    creating these models. Using the API, you will find that adding more layers to
    a model is incredibly easy and is great for new practitioners learning the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sequential model can be initialized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once the model has been initialized, layers can be added to the model. In this
    section, you will also explore how to add Keras layers to the model.
  prefs: []
  type: TYPE_NORMAL
- en: Keras Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras layers are included in the TensorFlow package. Keras layers are a collection
    of commonly used layers that can be added easily to your sequential models.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check out all the possible options for Keras layers here: [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To add layers to a model of the `Sequential` class, you can use the model''s
    `add` method. One optional layer that can be added to the beginning of a sequential
    model is an **input layer** as an entry point to the network. Input layers can
    take the following common input arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_shape` (required): The shape of the input tensor, not including the
    batch axis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: An optional argument indicating the input batch size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: Optional name of the input layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input layers can be added to a model as follows. The following code snippet
    is used to add a layer, expecting inputs to have eight features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'By providing a `name` argument, you can label the layers, which will be useful
    when visualizing the model in TensorBoard. Another type of layer that is commonly
    used when building regression and classification models is the `input_shape` provided
    as an argument. The following are the common input arguments for layers of the
    `Dense` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`units` (required): This is a positive integer denoting the number of units
    in the layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_shape`: This is the shape of the input tensor but is not required unless
    it is the first layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation`: This is an optional argument indicating which activation function
    to apply to the output of the layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_bias`: This is a Boolean argument indicating whether to use bias in the
    layer. The default is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: This refers to the name of the layer. One will be generated if this
    argument is not provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_initializer`: This is the initializer for the kernel weights. The **Glorot
    uniform initializer**, which has a normal distribution centered on zero and a
    standard deviation that is dependent on the number of units in the layer, is used
    by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias_initializer`: This is the initializer for the bias. The default of this
    parameter is used to set the bias values to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_regularizer`: This is the regularizer to use on the kernel weights.
    There are none applied by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias_regularizer`: This is the regularizer to use on the bias. There are none
    applied by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example of adding a dense layer to a model with `12` units,
    adding a `sigmoid` activation function at the output of the layer, and naming
    the layer `Dense_layer_1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that you understand how to initialize sequential models and add layers to
    them, you will create a Keras sequential model using TensorFlow in the first exercise.
    You will initialize a model, add layers to the model, add activation functions
    to the output of the model, and pass data through the model to simulate creating
    a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.01: Creating an ANN with TensorFlow'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will create your first sequential ANN in TensorFlow. You
    will have an input layer, a hidden layer with four units and a ReLU activation
    function, and an output layer with one unit. Then, you will create some simulation
    data by generating random numbers and passing it through the model, using the
    model's `predict` method to simulate a prediction for each data example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and import the TensorFlow library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a Keras model of the sequential class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an input layer to the model using the model''s `add` method, and add the
    `input_shape` argument with size `(8,)` to represent input data with eight features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add two layers of the `Dense` class to the model. The first will represent
    your hidden layer with four units and a ReLU activation function, and the second
    will represent your output layer with one unit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'View the weights by calling the `variables` attribute of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2: The variables of the ANN'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_04_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.2: The variables of the ANN'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This output shows all the variables that compose the model; they include the
    values for all weights and biases in each layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a tensor of size `32x8`, which represents a tensor with 32 records and
    8 features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `predict` method of the model and pass in the sample data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3: The output of the ANN after random inputs have been applied'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_04_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.3: The output of the ANN after random inputs have been applied'
  prefs: []
  type: TYPE_NORMAL
- en: Calling the `predict()` method on the sample data will propagate the data through
    the network. In each layer, there will be a matrix multiplication of the data
    with the weights, and the bias will be added before the data is passed as input
    data to the next layer. This process continues until the final output layer.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you created a sequential model with multiple layers. You initialized
    a model, added an input layer to accept data with eight features, added a hidden
    layer with four units, and added an output layer with one unit. Before fitting
    a model to training data, you must first compile the model with an optimizer and
    choose a loss function to minimize the value it computes by updating weights in
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will explore how to compile models, then fit them to
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Model Fitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once a model has been initialized and layers have been added to the ANN, the
    model must be configured with an optimizer, losses, and any evaluation metrics
    through the compilation process. A model can be compiled using the model''s `compile`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimizers can be chosen by simply naming the optimizer as the argument. The
    following optimizers are available as default for Keras models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent** (**SGD**): This updates the weights for each
    example in the dataset. You can find more information about SGD here: [https://keras.io/api/optimizers/sgd/](https://keras.io/api/optimizers/sgd/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSprop**: This is an adaptive optimizer that varies the weights during training
    by using a decaying average of the gradients at each update. You can find more
    information about RMSprop here: [https://keras.io/api/optimizers/rmsprop/](https://keras.io/api/optimizers/rmsprop/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adam**: This is also an adaptive optimizer that implements the Adam algorithm,
    updating the learning rates based on the first- and second-order gradients. You
    can find more information about Adam here: [https://keras.io/api/optimizers/adam/](https://keras.io/api/optimizers/adam/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adagrad**: This adaptive gradient optimizer adapts the learning rate at each
    weight update. The learning rate is adapted for each feature using the prior gradients
    and observations. You can find more information about Adagrad here: [https://keras.io/api/optimizers/adagrad/](https://keras.io/api/optimizers/adagrad/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adadelta**: This is a more robust version of Adagrad that uses a sliding
    window of gradient updates to adapt the learning rate. You can find more information
    about Adadelta here: [https://keras.io/api/optimizers/adadelta/](https://keras.io/api/optimizers/adadelta/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adamax**: This is an adaptive optimizer that is a variant of the Adam optimizer. You
    can find more information about Adamax here: [https://keras.io/api/optimizers/adamax/](https://keras.io/api/optimizers/adamax/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nadam**: This is another adaptive optimizer that is a variant of the Adam
    optimizer with Nesterov momentum. You can find more information about Nadam here:
    [https://keras.io/api/optimizers/Nadam/](https://keras.io/api/optimizers/Nadam/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ftrl**: This is an optimizer that implements the FTRL algorithm. You can
    find more information about Ftrl here: [https://keras.io/api/optimizers/ftrl/](https://keras.io/api/optimizers/ftrl/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Custom optimizers can also be added to Keras models if the provided ones are
    not relevant. Selecting the most appropriate optimizer is often a matter of trying
    each and identifying which optimizer produces the lowest error. This process is
    known as **hyperparameter tuning** and will be covered in a later chapter. In
    the next section, you will uncover another option when compiling models: the loss
    function. The goal of training a model is to minimize the value computed by the
    loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: The Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss function is the measure of error between the predicted results and
    the true results. You use the loss function during the training process to determine
    whether varying any of the weights and biases will create a better model by minimizing
    the loss function's value through the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different types of loss functions that can be used, and the specific
    one will depend on the problem and goal. In general, regression and classification
    tasks will have different loss functions. Since regression models predict continuous
    variables, loss functions for regression models typically aim to summarize how
    far, on average, the predictions are from the true values. For classification
    models, loss functions aim to determine how the quantity of true positive, true
    negative, false positive, and false negative classifications of the predicted
    classes vary compared to the true classes.
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** are defined as correct predictions labeled positive by the
    classifier; similarly, **true negatives** are correct predictions labeled negative.
    **False positives** are predictions labeled positive where the true value is negative,
    and **false negatives** are predictions labeled negative that are actually positive.
    Loss functions that are directly available to use in Keras sequential models for
    regression include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(true value – predicted value)^2`, and returns the average across the entire
    dataset. This loss function is primarily used for regression problems, and the
    squaring of the difference between the two values ensures the loss function results
    in a positive number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`|true value – predicted value|`, and returns the average across the dataset.
    This method also ensures that the result is a positive value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`|(true value– predicted value) / true value|`, and returns the average across
    the dataset as a percentage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For classification, loss functions that are available include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0` and `1`, with values closer to `1` representing a greater number of true
    positive classifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0` and `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When compiling a model, other metrics can also be passed in as an argument
    to the method. They will be calculated after each epoch and saved during the training
    process. The metrics that are available to be calculated for Keras models include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: This is the proportion of correct results out of the total results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: This is the proportion of true positives out of the total positives predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: This is the proportion of true positives out of the actual positives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUC**: This metric represents the area under the ROC curve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These metrics can be incredibly valuable in understanding the performance of
    the model during the training process. All the metrics have values between `0`
    and `1`, with higher values representing better performance. Once the model has
    been compiled, it can be fit to the training data. This can be accomplished by
    calling the `fit` method and passing in the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: This is the feature data as a TensorFlow tensor or NumPy array.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y`: This is the target data as a TensorFlow tensor or NumPy array.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs`: This refers to the number of epochs to run the model for. An epoch
    is an iteration over the entire training dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: This is the number of training data samples to use per gradient update.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_split`: This is the proportion of the training data to be used
    for validation that is evaluated after each epoch. This proportion of data is
    not used in the weight update process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle`: This indicates whether to shuffle the training data before each
    epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To fit the model to the training data, the `fit` method can be applied to a
    model in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once the `fit` method has been called, the model will begin fitting to the training
    data. After each epoch, the loss is returned for the training. If a validation
    split is defined, then the loss is also evaluated on the validation split.
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once models are trained, they can be evaluated by utilizing the model''s `evaluate`
    method. The `evaluate` method assesses the performance of the model according
    to the loss function used to train the model and any metrics that were passed
    to the model. The method is best used when determining how the model will perform
    on new, unseen data by passing in a feature and target dataset that has not been
    used in the training process or out-of-sample dataset. The method can be called
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The result of the method is first the loss calculated on the input data, and
    then, if any metrics were passed in the model compilation process, they will also
    be calculated when the `evaluate` method is executed. Model evaluation is an important
    step in determining how well your model is performing. Since there is an enormous
    number of hyperparameters (such as the number of hidden layers, the number of
    units in each layer, and the choice of activation functions, to name a few), model
    evaluation is necessary to determine which combination of hyperparameters is optimal.
    Effective model evaluation can help provide an unbiased view on which model architecture
    will perform best overall.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, you will undertake the process of creating an ANN,
    compiling the model, fitting the model to training data, and finally, evaluating
    the model on the training data. You will recreate the linear regression algorithm
    with an ANN, which can be interpreted as an ANN with only one layer and one unit.
    Furthermore, you will view the architecture of the model and model training process
    in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.02: Creating a Linear Regression Model as an ANN with TensorFlow'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will create a linear regression model as an ANN using
    TensorFlow. The dataset, `Bias_correction_ucl.csv`, describes the bias correction
    of air temperature forecasts of Seoul, South Korea. The fields represent temperature
    measurements of the given date, the weather station at which the metrics were
    measured, model forecasts of weather-related metrics such as humidity, and projections
    for the temperature the following day. You are required to predict the next maximum
    and minimum temperature given measurements of the prior timepoints and attributes
    of the weather station.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Bias_correction_ucl.csv` file can be found here: [https://packt.link/khfeF](https://packt.link/khfeF).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook to implement this exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a new Jupyter Notebook cell, import the TensorFlow and pandas libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load in the dataset using the pandas `read_csv` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Make sure you change the path (highlighted) to the CSV file based on its location
    on your system. If you're running the Jupyter notebook from the same directory
    where the CSV file is stored, you can run the preceding code without any modification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Drop the `date` column and drop any rows that have null values since your model
    requires numerical values only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create target and feature datasets. The target dataset will contain the columns
    named `Next_Tmax` and `Next_Tmin`, while the feature dataset will contain all
    columns except those named `Next_Tmax` and `Next_Tmin`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rescale the feature dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a Keras model of the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an input layer to the model using the model''s `add` method, and set `input_shape`
    to be the number of columns in the feature dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the output layer of the `Dense` class to the model with a size of `2`,
    representing the two target variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model with an RMSprop optimizer and a mean squared error loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a callback for TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4: The output of the fitting process showing the epoch, train time
    per sample, and loss after each epoch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_04_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.4: The output of the fitting process showing the epoch, train time
    per sample, and loss after each epoch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluate the model on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'View the model architecture and model-fitting process on TensorBoard by calling
    the following on the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see its execution in a web browser by visiting the URL that is provided after
    launching TensorBoard. The default URL provided is `http://localhost:6006/`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.5: A visual representation of the model architecture in TensorBoard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_04_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.5: A visual representation of the model architecture in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function can be visualized as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: A visual representation of the loss as a function of an epoch
    in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.6: A visual representation of the loss as a function of an epoch in
    TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: You can see the architecture of the model in the `GRAPHS` tab. The architecture
    shows the input layer and output layer in the model, as well as the calculated
    loss. During the model-fitting process, the loss is calculated after each epoch
    and is displayed in TensorBoard in the `SCALARS` tab. The loss is that which is
    defined in the compilation process; so, in this case, the loss is the mean squared
    error. From TensorBoard, you can see that the mean squared error reduces after
    each epoch, indicating that the model is learning from the training data, updating
    the weights in order to reduce the total loss.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you have learned how to create, train, and evaluate an ANN
    with TensorFlow by using Keras layers. You recreated the linear regression algorithm
    by creating an ANN with an input layer and an output layer that has one unit for
    each output. Here, there were two outputs representing the maximum and minimum
    values of the temperature; thus, the output layer has two units.
  prefs: []
  type: TYPE_NORMAL
- en: In *Exercise 4.01*, *Creating an ANN with TensorFlow*, you created an ANN with
    only one layer containing weights and the output layer. This is an example of
    a **shallow neural network**. ANNs that have many hidden layers containing weights
    are called **deep neural networks**, and the process of training them is called
    **deep learning**. By increasing the number of layers and making the ANN deeper,
    the model becomes more flexible and will be able to model more complex functions.
    However, to gain this increase in flexibility, you need more training data and
    more computation power to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, you will create and train ANNs that have multiple hidden
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.03: Creating a Multi-Layer ANN with TensorFlow'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will create a multi-layer ANN using TensorFlow. This model
    will have four hidden layers. You will add multiple layers to the model and activation
    functions to the output of the layers. The first hidden layer will have `16` units,
    the second will have `8` units, and the third will have `4` units. The output
    layer will have `2` units. You will utilize the same dataset as in *Exercise 4.02*,
    *Creating a Linear Regression Model as an ANN with TensorFlow*, which describes
    the bias correction of air temperature forecasts for Seoul, South Korea. The exercise
    aims to predict the next maximum and minimum temperature given measurements of
    the prior timepoints and attributes of the weather station.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook to implement this exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a new Jupyter Notebook cell, import the TensorFlow and pandas libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load in the dataset using the pandas `read_csv` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Make sure you change the path (highlighted) to the CSV file based on its location
    on your system. If you're running the Jupyter notebook from the same directory
    where the CSV file is stored, you can run the preceding code without any modification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Drop the `Date` column and drop any rows that have null values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create target and feature datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rescale the feature dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a Keras model of the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an input layer to the model using the model''s `add` method, and set `input_shape`
    to the number of columns in the feature dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add three hidden layers and an output layer of the `Dense` class to the model.
    The first hidden layer will have `16` units, the second will have `8` units, and
    the third will have `4` units. Label the layers appropriately. The output layer
    will have two units to match the target variable that has two columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model with an RMSprop optimizer and mean squared error loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a callback for TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data for `50` epochs and add a validation split
    equal to 20%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.7: The output of the fitting process showing the epoch, training
    time per sample, and loss after each epoch](img/B16341_04_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.7: The output of the fitting process showing the epoch, training time
    per sample, and loss after each epoch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluate the model on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the following result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'View the model architecture and model-fitting process in TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8: A visual representation of the model architecture in TensorBoard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_04_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.8: A visual representation of the model architecture in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can visualize the loss function as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: A visual representation of the loss as a function of an epoch
    in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: on the training and validation split
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_04_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.9: A visual representation of the loss as a function of an epoch in
    TensorBoard on the training and validation split'
  prefs: []
  type: TYPE_NORMAL
- en: The network architecture shows the input layer and the four hidden layers of
    the model as well as the calculated loss at the end. During the model-fitting
    process, the loss is calculated after each epoch and is displayed in TensorBoard
    in the `SCALARS` tab. Here, the loss is the mean squared error. From TensorBoard,
    you can see that the mean squared error reduces on the training set (the orange
    line) and the validation set (the blue line), after each epoch, indicating that
    the model is learning effectively from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you have created an ANN with multiple hidden layers. The loss
    you obtained was lower than that achieved using linear regression, which demonstrates
    the power of ANNs. With some tuning to the hyperparameters (such as varying the
    number of layers, the number of units within each layer, adding activation functions,
    and changing the loss and optimizer), the loss could be even lower. In the next
    activity, you will put your model-building skills into action on a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.01: Creating a Multi-Layer ANN with TensorFlow'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The feature dataset, `superconductivity.csv`, contains the properties of superconductors
    including the atomic mass of the material and its density. Importantly, the dataset
    also contains the critical temperature of the material, which is the temperature
    at which the material exhibits superconductive properties. In this activity, you
    are tasked with finding the critical temperature of the material or the temperature
    at which the material gains superconductive properties.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The `superconductivity.csv` file can be found here: [https://packt.link/sOCPh](https://packt.link/sOCPh).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook to implement this activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the TensorFlow and pandas libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load in the `superconductivity.csv` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop any rows that have null values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the target as the `critical_temp` column and the feature dataset as the
    remaining columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rescale the feature dataset using a standard scaler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a model of the Keras `Sequential` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an input layer, four hidden layers of sizes `64`, `32`, `16`, and `8`, and
    an output layer of size `1` to the model. Add a ReLU activation function to the
    first hidden layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the model with an RMSprop optimizer with a learning rate equal to `0.001`
    and the mean squared error for the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a callback to write logs to TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to the training data for `100` epochs, with a batch size equal
    to `32` and a validation split equal to 20%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model on the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: View the model architecture in TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should get an output like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10: A visual representation of the model architecture in TensorBoard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.10: A visual representation of the model architecture in TensorBoard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the model-fitting process in TensorBoard. You should get the following output:![Figure
    4.11: A visual representation of the loss as a function of an epoch on the training
    and validation split in TensorBoard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16341_04_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.11: A visual representation of the loss as a function of an epoch
    on the training and validation split in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor262).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will explore classification models, which attempt to
    classify data into distinct classes. You will begin with binary classification
    models that classify data into just two classes. This is the simplest form of
    a classification model. Once binary classifiers are mastered, more complicated
    models can be tackled, such as multi-label and multi-class classification.
  prefs: []
  type: TYPE_NORMAL
- en: Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of classification models is to classify data into distinct classes.
    For example, a spam filter is a classification model that aims to classify emails
    into "spam" (referring to unsolicited and unwanted email) or "ham" (a legitimate
    email). Spam filters are an example of a binary classifier since there are two
    classes. The input to the filter may include the content of the email, the email
    address of the sender, and the subject line, among other features, and the output
    will be the predicted class, `spam` or `ham`. Classification models can classify
    data into more than two distinct classes (known as **multi-class classification**)
    or classify data with multiple positive labels (known as **multi-label classification**).
  prefs: []
  type: TYPE_NORMAL
- en: There are several different algorithms that can be used for classification tasks.
    Some popular ones include logistic regression, decision trees, and ANNs. ANNs
    are a great choice for classification models since they can learn complex relationships
    between the features and the target, and results can be achieved with the appropriate
    activation function on the output layer of the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: A common activation function to use for classification models is the sigmoid
    function, which is the same function used in logistic regression. In fact, a logistic
    regression model can be created by building an ANN with a single layer with one
    unit and a sigmoid activation function. The sigmoid function is a transformation
    in which the input is any real value, and the output is a number strictly between
    `0` and `1`. A visual representation is shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the sigmoid transformation can be interpreted as a probability
    of a value being in the positive class; a value closer to a value of `1` indicates
    a higher probability of being in the positive class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: A visual representation of the sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_04_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.12: A visual representation of the sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: After the sigmoid function has been applied, a threshold is applied, above which
    the data is classified as the positive class and below as the negative class.
    The default threshold for a sigmoid function is `0.5`, meaning that any value
    at or above `0.5` is classified as positive.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, you will create a logistic regression model with TensorFlow.
    You will achieve this by creating a single-layer ANN, the process of which is
    similar to that of the linear regression model in *Exercise 4.02*, *Creating a
    Linear Regression Model as an ANN with TensorFlow*. The difference is that you
    will add a sigmoid activation function to the output of the ANN. Another difference
    that separates the two exercises is the loss function that you will use to calculate
    the loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.04: Creating a Logistic Regression Model as an ANN with TensorFlow'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will create a logistic regression model as an ANN using
    TensorFlow. The dataset, `qsar_androgen_receptor.csv`, is used to develop classification
    models for the discrimination of binder/non-binder molecules given various attributes
    of the molecules. Here, the molecule attributes represent the features of your
    dataset, and their binding properties represent the target variable, in which
    a positive value represents a binding molecule, and a negative value represents
    a non-binding molecule. You will create a logistic regression model to predict
    the binding properties of the molecule given attributes of the molecule provided
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The `qsar_androgen_receptor.csv` file can be found here: [https://packt.link/hWvjc](https://packt.link/hWvjc).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook to implement this exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the TensorFlow and pandas libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load in the dataset using the pandas `read_csv` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Make sure you change the path (highlighted) to the CSV file based on its location
    on your system. If you're running the Jupyter notebook from the same directory
    where the CSV file is stored, you can run the preceding code without any modification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Drop any rows that have null values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create target and feature datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a Keras model of the `Sequential` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an input layer to the model using the model''s `add` method and set `input_shape`
    to be the number of columns in the feature dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the output layer of the `Dense` class to the model with a size of `1`,
    representing the target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model with an RMSprop optimizer and binary cross-entropy for the
    loss, and compute the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a TensorBoard callback:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data for `50` epochs, adding the TensorBoard
    callback with a validation split of 20%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output should be similar to the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.13: The output of the fitting process showing the epoch, training
    time per sample, and loss after each epoch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_04_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.13: The output of the fitting process showing the epoch, training
    time per sample, and loss after each epoch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluate the model on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get output something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the model-fitting process in TensorBoard by calling the following
    command on the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get a screen similar to the following in the browser:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14: A visual representation of the model architecture in TensorBoard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.14: A visual representation of the model architecture in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15: A visual representation of the loss and accuracy as a function
    of an epoch evaluated on the training and validation split in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_04_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.15: A visual representation of the loss and accuracy as a function
    of an epoch evaluated on the training and validation split in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: You can see from TensorBoard that, with the addition of the `metrics` argument
    that was added in the model compilation process, there is an additional node in
    the architecture for the calculation of the accuracy metric. There is also an
    additional chart in the `SCALARS` tab showing the accuracy metric as a function
    of the epoch for the training and validation split.
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the charts that, for the training set, the accuracy increases,
    and the loss decreases over time, which is a positive indication that the model
    is learning. However, on the validation split, the accuracy begins to decrease,
    and the loss begins to increase, which is a sign that the model may be overfitting
    to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you have learned how to build a classification model to discriminate
    between the binding properties of various molecules based on their other molecular
    attributes. The classification model was equivalent to a logistic regression model
    since it had only one layer and was preceded by a sigmoid activation function.
    With only one layer, there is a weight for each input feature and a single value
    for the bias. The sigmoid activation function transforms the output of the layer
    into a value between `0` and `1`, which is then rounded to represent your two
    classes. `0.5` and above represents one class, the molecule with binding properties,
    and below `0.5` represents the other class, molecules with non-binding properties.
  prefs: []
  type: TYPE_NORMAL
- en: The next activity will summarize your learning in this chapter by combining
    your knowledge of creating multi-layer ANNs as you accomplished in *Exercise 4.03*,
    *Creating a Multi-Layer ANN with TensorFlow*, and *Activity 4.01*, *Creating a
    Multi-Layer ANN with TensorFlow*, with your knowledge of creating classification
    models from *Exercise 4.04*, *Creating a Logistic Regression Model as an ANN with
    TensorFlow*. You will use the same dataset as in the preceding activity but change
    the target variable to make it more suitable for a classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.02: Creating a Multi-Layer Classification ANN with TensorFlow'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The feature dataset, `superconductivity.csv`, contains the properties of superconductors
    including the atomic mass of the material and its density. Importantly, the dataset
    also contains the critical temperature of the material, which is the temperature
    at which the material exhibits superconductive properties. You are required to
    determine which superconductors will express superconductive properties above
    the boiling point of nitrogen (77.36 K), thereby allowing superconductivity using
    liquid nitrogen, which is readily available. Your target variable will have a
    `true` value when the critical temperature is above 77.36 K and `false` below,
    indicating whether the material expresses superconductive properties above the
    boiling point of nitrogen.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The `superconductivity.csv` file can be found here: [http://packt.link/sOCPh](http://packt.link/sOCPh).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Jupyter notebook to complete the activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the TensorFlow and pandas libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load in the `superconductivity.csv` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop any rows that have null values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the target values to `true` when values of the `critical_temp` column are
    above `77.36` and `false` when below. The feature dataset is the remaining columns
    in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rescale the feature dataset using a standard scaler.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a model of the Keras `Sequential` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an input layer, three hidden layers of sizes `32`, `16`, and `8`, and an
    output layer with a `sigmoid` activation function of size `1` to the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the model with an RMSprop optimizer with a learning rate equal to `0.0001`
    and binary cross-entropy for the loss and compute the accuracy metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a callback to write logs to TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to the training data for `50` epochs and a validation split equal
    to 0%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model on the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: View the model architecture and model-fitting process in TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor263).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section, you have begun your foray into building, training, and evaluating
    classification models using TensorFlow. You have seen that they are built in much
    the same way as ANNs for regression tasks with the primary difference being the
    activation function on the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you began your journey into creating ANNs in TensorFlow. You
    saw how simple it is to create regression and classification models by utilizing
    Keras layers. Keras layers are distinct classes that exist in a separate library
    that uses TensorFlow in the backend. Due to their popularity and ease of use,
    they are now included in TensorFlow and can be called in the same way as any other
    TensorFlow class.
  prefs: []
  type: TYPE_NORMAL
- en: You created ANNs with fully connected layers, varying layers, beginning with
    an ANN that resembles a linear regression algorithm, which is equivalent to a
    single-layer ANN. Then, you added layers to your ANN and added activation functions
    to the output of the layers. Activation functions can be used to determine whether
    a unit is fired or can be used to bind the value of the output from a given unit.
    Regression models aim to predict a continuous variable from the data provided.
    In the exercises and activities throughout this chapter, you attempted to predict
    the temperature in Seoul given data from weather stations, and the critical temperature
    of superconducting materials given various material properties.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you explored classification models, which aim to classify data into
    distinct classes. These models are similar to regression models in the way they
    are set up; however, an activation is used on the final output to bind the output
    values between two numbers that represent whether or not the data point is classified
    into the class. You began with binary classification models, which aim to classify
    the data into two classes, and demonstrated the concept of binary classification
    with an exercise in which you classified molecules into classes that represent
    their binding properties based on other attributes of the molecules' properties.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will explore classification models in more depth. You
    will learn some of the intricacies and capabilities of classification models,
    including how to classify data that has more than two distinct classes (known
    as multi-class classification), and whether data points can have more than one
    positive label (known as multi-label classification). You will address how to
    structure the architecture to create these models, the appropriate loss functions
    to use when training, and the relevant metrics to calculate to understand whether
    models are performing well.
  prefs: []
  type: TYPE_NORMAL
