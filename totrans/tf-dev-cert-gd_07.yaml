- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image Classification with Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) are the go-to algorithms when
    it comes to image classification. In the 1960s, neuroscientists Hubel and Wiesel
    conducted a study on the visual cortex in cats and monkeys. Their work unraveled
    how we visually process information in a hierarchical structure, showing how visual
    systems are organized into a series of layers where each layer is responsible
    for a different aspect of visual processing. This earned them a Nobel Prize, but
    more importantly, it served as the basis upon which CNNs are built. CNNs, by virtue
    of their nature, are well designed to work with data with spatial structures such
    as images.'
  prefs: []
  type: TYPE_NORMAL
- en: However, in the early days, CNNs did not have the limelight due to a number
    of factors, such as insufficient training data, underdeveloped network architecture,
    insufficient computational resources, and the absence of modern techniques such
    as data augmentation and dropout. In the 2012 ImageNet Large Scale Visual Recognition
    Challenge, the **machine learning** (**ML**) community was taken by storm when
    a CNN architecture called AlexNet outperformed all other methods by a large margin.
    Today, ML practitioners apply CNNs to achieve state-of-the-art performance on
    computer vision tasks such as image classification, image segmentation, and object
    detection, among others.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will examine CNNs to see how they do things differently
    from the fully connected neural networks we have used so far. We will start with
    the challenges faced by fully connected networks when working with image data,
    after which we will explore the anatomy of CNNs. We will look at the core building
    blocks of CNN architecture and their overall impact on the performance of the
    network. Next, we will build an image classifier using a CNN architecture with
    the Fashion MNIST dataset, then move on to building a real-world image classifier.
    We will be working with color images of different sizes, and our target objects
    are in different positions within the image.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a sound understanding of what CNNs
    are and why they are superior to fully connected networks when it comes to image
    classification tasks. Also, you will be able to effectively build, train, tune,
    and test CNN models on real-world image classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The anatomy of CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fashion MNIST with CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weather data classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying hyperparameters to improve the model’s performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating image classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges of image recognition with fully connected networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification with
    Neural Networks*, we applied a **deep neural network** (**DNN**) to the Fashion
    MNIST dataset. We saw how every neuron in the input layer is connected to every
    neuron in the hidden layer and those in the hidden layer are connected to neurons
    in the output layer, hence the name *fully connected*. While this architecture
    can solve many ML problems, they are not well suited for modeling image classification
    tasks, due to the spatial nature of image data. Let’s say you are looking at a
    picture of a face; the positioning and orientation of the features on the face
    enable you to know it is a human face even when you just focus on a specific feature,
    such as the eyes. Instinctively, you know it’s a face by virtue of the spatial
    relationship between the features of the face; however, DNNs do not see this bigger
    picture when looking at images. They process each pixel in the image as independent
    features, without taking the spatial relationships between these features into
    consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue with using fully connected architectures is the curse of dimensionality.
    Let’s say we are working with a real-world image of size 150 x 150 with 3 color
    channels, **red, green, and blue** (**RGB**); we will have an input size of 67,500\.
    As all the neurons are connected to neurons in the next layer, if we feed these
    values into a hidden layer with 500 neurons, we will have 67,500 x 500 = 33,750,000
    parameters, and this number of parameters will grow exponentially as we add more
    layers, making it resource intensive to apply this type of network to image classification
    tasks. Another accompanying problem we could stumble upon is overfitting; this
    happens due to the large number of parameters in our network. If we have images
    of larger sizes or we add more neurons to our networks, the number of trainable
    parameters will grow exponentially, and it could become impractical to train such
    a network due to cost and resource requirements. In light of these challenges,
    there is a need for a more sophisticated architecture, and this is where CNNs
    come in with their ability to uncover spatial relationships and hierarchies, ensuring
    features are recognized irrespective of where they are located within an image.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**Spatial relationship** refers to how features within an image are arranged
    in relation to each other in terms of position, distance, and orientation.'
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last section, we saw some of the challenges DNNs grappled with when
    dealing with visual recognition tasks. These issues include the lack of spatial
    awareness, high dimensionality, computational inefficiency, and the risk of overfitting.
    How do we overcome these challenges? This is where CNNs come into the picture.
    CNNs by design are uniquely positioned to handle image data. Let''s go through
    *Figure 7**.1* and uncover why and how CNNs stand out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – The anatomy of a CNN](img/B18118_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – The anatomy of a CNN
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break down the different layers in the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layer – the eyes of the network**: Our journey begins with
    us feeding in images into the convolutional layer; this layer can be viewed as
    the “eyes of our network.” Their job is primarily to extract vital features. Unlike
    DNNs, where each neuron is connected to every neuron in the next layer, CNNs apply
    filters (also known as kernels) to capture local patterns within an image in a
    hierarchical fashion. The output of the interactions between a segment of the
    input image that the filter slides over is called a feature map. As shown in *Figure
    7**.2*, we can see that each feature map highlights specific patterns in the shirt
    that we passed into the network. Images go through CNNs in a hierarchical fashion
    with filters in the earlier layers adept at capturing simple features, while those
    in subsequent layers capture more complex patterns, mimicking the hierarchical
    structure of a human’s visual cortex. Another important property of CNNs is parameter
    sharing – this happens because patterns are only learned once and applied everywhere
    else across an image. This ensures that the visual ability of the model is not
    location-specific. In ML, we refer to this concept as **translation invariance**
    – the network’s ability to detect a shirt regardless of whether it is aligned
    to the right or left or centered within an image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Visualization of features captured by a convolutional layer](img/B18118_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Visualization of features captured by a convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: '**Pooling layer – the summarizer**: After the convolutional layer comes the
    pooling layer. This layer can be viewed as a summarizer in CNNs as it focuses
    on condensing the overall dimensionality of the feature maps while retaining important
    features, as illustrated in *Figure 7**.3*. By methodically downsampling the feature
    maps, CNNs significantly not only reduce the number of parameters required for
    image processing but also improve the overall computational efficiency of CNNs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.3 – An example of the pooling operation, preserving essential details](img/B18118_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – An example of the pooling operation, preserving essential details
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully connected layer – the decision maker**: Our image traverses a series
    of convolution and pooling layers that extract features and reduce the dimensionality
    of feature maps, eventually reaching the fully connected layer. This layer can
    be viewed as the decision maker. This layer offers high-level reasoning as it
    brings together all the important details collected through the layers and uses
    them to make the final classification verdict. One of the hallmarks of CNNs is
    its end-to-end learning process, which seamlessly integrates feature extraction
    and image classification. This methodological and hierarchical learning approach
    makes CNN a well-suited tool for image recognition and analysis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have only scratched the surface of how CNNs work. Let's now drill down into
    the key operations that take place within the different layers, starting with
    convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now know that convolutional layers apply filters, which slide over patches
    of the input image. A typical CNN applies multiple filters, with each filter learning
    a specific kind of feature by interacting with the input image. By combining the
    detected features, a CNN arrives at a comprehensive understanding of the image
    features and uses this detailed information to classify the input image. Mathematically,
    this convolution process involves the dot product between a patch of the input
    image and the filter (a small matrix), as illustrated in *Figure 7**.4*. This
    process yields an output known as the **activation map** or **feature map**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Convolution operation – applying a filter to an input image
    to generate a feature map](img/B18118_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Convolution operation – applying a filter to an input image to
    generate a feature map
  prefs: []
  type: TYPE_NORMAL
- en: As the filter slides over the patches of the image, it produces a feature map
    for each dot operation. Feature maps are a representation of the input image in
    which certain visual patterns are enhanced by the filter, as shown in *Figure
    7**.2*. When we stack feature maps from all the filters in the network, we arrive
    at a rich, multi-faceted view of the input image, which gives later layers adequate
    information to learn more complex patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5  – a (top) and b (bottom): Dot product computation](img/B18118_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5 – a (top) and b (bottom): Dot product computation'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.5 a*, we see a dot product operation in progress, as a filter
    slides over a section of the input image resulting in a destination pixel value
    of 13\. If we move the filter 1 pixel to the right, as shown in *Figure 7**.5
    b*, we will arrive at the next destination pixel value of 14\. If we continue
    sliding the filter one pixel at a time over the input image, we will achieve the
    complete output shown in *Figure* *7**.5 b*.
  prefs: []
  type: TYPE_NORMAL
- en: We have now seen how convolution operations work; however, there are various
    types of convolutional layers that we can apply in CNNs. For image classification,
    we typically use 2D convolutional layers, while we apply 1D convolution layers
    for audio processing and 3D convolutional layers for video processing. When designing
    our convolutional layer, there are a number of adjustable hyperparameters that
    can impact the performance of our network, such as the number of filters, the
    size of the filters, stride, and padding. It is pertinent to explore how these
    hyperparameters impact our network.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin this exploration by looking at the impact of the number of filters
    in a convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of the number of filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By increasing the number of filters within a CNN, we empower it to learn a richer
    and more diverse representation of the input image. The more filters we have,
    the more representation will be learned. However, more filters mean more parameters
    to train, and this could not only increase the computational cost but also slow
    down the training process and increase the risk of overfitting. When deciding
    on the number of filters to apply to your network, it is important to consider
    the type of data in use. If the data has a lot of variability, you may need more
    filters to capture the diversity in your data, whereas with smaller datasets,
    you should be more conservative to reduce the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of the size of the filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now know that filters are small matrices that slide over our input image
    to produce feature maps. The size of the filter we apply to the input image will
    determine the level and type of features that will be extracted from the input
    image. The filter size is the dimension of the filter – that is, the height and
    width of the filter matrix. Typically, you will come across 3x3, 5x5, and 7x7
    filters. Smaller filters will cover a smaller patch of the input image, while
    a larger filter will cover a more extensive section of the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Granularity of features** – Smaller filters such as 3x3 filters can be applied
    to capture finer and more local details of an image such as edges, textures, and
    corners, while larger filters such as 7x7 filters can learn broader patterns such
    as face shapes or object parts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computational efficiency** – Smaller filters cover a smaller receptive field
    of the input image, as illustrated in *Figure 7**.6*, which means they will require
    more operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Convolution operation with a 3x3 filter](img/B18118_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Convolution operation with a 3x3 filter
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a larger filter covers a large segment of the input image,
    as shown in *Figure 7**.7*. However, many modern CNN architectures (for example,
    VGG) use 3x3 filters. Stacking these smaller filters together would increase the
    depth of the network and enhance the capabilities of these filters to capture
    more complex patterns with a smaller number of parameters in comparison to using
    a large filter, which makes smaller filters easier to train.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Convolution operation with a 5x5 filter](img/B18118_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Convolution operation with a 5x5 filter
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameter count** – A larger filter typically has more weight in comparison
    to a smaller filter; for example, a 5x5 filter will have 25 parameters and a 3x3
    filter will have 9 parameters. Here, we are ignoring the depth for the sake of
    simplicity. Hence, larger filters will contribute to making the model more complex
    in comparison to smaller filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impact of stride
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stride is an important hyperparameter in CNNs. It determines the number of pixels
    a filter moves over an input image. We can liken stride to the step we take when
    walking; if we take small steps, it will take us a longer time to reach our destination,
    while larger steps will ensure we reach it much quicker. In *Figure 7**.8*, we
    apply a stride of 1, which means the filter moves over the input image 1 pixel
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Convolution operation with a stride of 1](img/B18118_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Convolution operation with a stride of 1
  prefs: []
  type: TYPE_NORMAL
- en: If we apply a stride of 2, it means the filter will move 2 pixels at a time,
    as illustrated in *Figure 7**.9*. We see that a large stride will lead to a reduced
    spatial dimension of the output feature map. We can see this when we compare the
    output of both figures.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Convolution operation with a stride of 2](img/B18118_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Convolution operation with a stride of 2
  prefs: []
  type: TYPE_NORMAL
- en: When we apply a larger stride, it can increase the computational efficiency,
    but it also reduces the spatial resolution of the input image. Hence, we need
    to consider this trade-off when selecting the right stride for our network. Next,
    let's examine the border effect.
  prefs: []
  type: TYPE_NORMAL
- en: The boundary problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a filter slides over the input image performing convolution operations,
    it soon reaches the borders or the edges, where it becomes difficult to perform
    dot product operations due to the absence of pixels outside the image boundaries.
    This results in the output feature map being smaller than the input image as a
    result of loss of information around the edges or borders. This issue is referred
    to as the **edge effect** or the **boundary problem** in ML. In *Figure 7**.10*,
    we can observe that we are unable to perform a dot product operation on the bottom-left
    corner as we cannot center the filter over the highlighted pixel value of 3 without
    some part of the filter falling out of the defined image boundary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Showing the boundary problem](img/B18118_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Showing the boundary problem
  prefs: []
  type: TYPE_NORMAL
- en: To fix the boundary issue and preserve the spatial dimension of the output feature
    map, we may want to apply padding. Let's discuss this concept next.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Padding** is a technique we can apply to our convolution process to prevent
    the boundary effect by adding extra pixels to the edges, as shown in *Figure 7**.11*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – A padded image undergoing convolution operation](img/B18118_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – A padded image undergoing convolution operation
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now perform dot product operations on pixels at the edges, hence preserving
    information at the edges. Padding can also be applied to maintain the spatial
    dimension pre- and post-convolution. This could prove useful in deep CNN architecture
    with several convolution layers. Let’s look at the two main types of padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Valid Padding (No Padding)**: Here, no padding is applied. This can be useful
    when we want to achieve a reduced spatial dimensionality, especially in deeper
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Same Padding**: Here, we set padding to ensure the output feature map and
    the input image dimensions are the same. We use this when maintaining spatial
    dimensionality is of paramount importance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we move on to examining the pooling layer, let's put together the different
    hyperparameters we have discussed in the convolutional layer and see them in action.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Figure 7**.12*, we have a 7x7 input image and a 3x3 filter. Here, we use
    a stride of 1 and set padding to Valid (no padding).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Setting the hyperparameters](img/B18118_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Setting the hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the output feature map of a convolutional operation, we can apply
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: ( W − F + 2P _ S ) + 1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this formula, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*W* represents the size of the input image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*F* stands for the filter size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S* represents stride'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P* stands for padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we input the respective values into the equation, we get a resulting value
    of 5, which means we will have a 5x5 output feature map. If we alter any of the
    values, it will impact the size of the output feature map one way or the other.
    For example, if we increase the stride size, we will have a smaller output feature
    map, while if we set padding to same, this will increase the size of the output.
    We can now move on from the convolution operations and explore pooling next.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pooling** is an important operation that takes play in the pooling layer
    of a CNN. It is a technique used to downsample the spatial dimension of individual
    feature maps generated by the convolutional layers. Let''s examine some important
    types of pooling layers. We’ll begin by exploring max pooling, as shown in *Figure
    7**.13*. Here, we see how max pooling operations work. The pooling layer simply
    takes the highest value from each region of the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – A max pooling operation](img/B18118_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – A max pooling operation
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling enjoys several benefits as it is intuitive and easy to implement.
    It is also efficient since it simply extracts the highest value in a region, and
    it has been applied with good effect across diverse tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Average pooling, as the name suggests, reduces the data dimensionality by taking
    the average value for a designated region, as illustrated in *Figure 7**.14*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – An average pooling operation](img/B18118_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – An average pooling operation
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, min pooling extracts the minimum value in a specified region
    of the input data. Pooling reduces the spatial size of the output feature maps
    and this, in turn, reduces the memory requirement for storing intermediate representations.
    Pooling can be beneficial to a network; however, excessive pooling can be counterproductive
    as this could lead to information loss. After the pooling layer, we arrive at
    the fully connected layer, the decision maker of our network.
  prefs: []
  type: TYPE_NORMAL
- en: The fully connected layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final component of our CNN architecture is the fully connected layer. Unlike
    the convolutional layer, here, every neuron is connected to every neuron in the
    next layer. This layer is responsible for decision-making, such as classifying
    whether our input image is a shirt or a hat. The fully connected layer takes the
    learned features from the earlier layers and maps them to their corresponding
    labels. We have now covered CNNs in theory; let's now proceed by applying them
    to our fashion dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Fashion MNIST 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, you are already familiar with this dataset, as we used it in [*Chapter
    5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification with Neural Networks*,
    and [*Chapter 6*](B18118_06.xhtml#_idTextAnchor129), *Improving the Model*. Now,
    let''s see how CNNs compare to the simple neural networks we have worked with
    so far. We will continue in the same spirit as before. We start by importing the
    required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will import the requisite libraries for preprocessing, modeling, and visualizing
    our ML model using TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will load the Fashion MNIST dataset from TensorFlow Datasets using
    the `load_data()` function. This function returns our training and testing data
    consisting of NumPy arrays. The training data consists of `x_train` and `y_train`,
    and the test data is made up of `x_test` and `y_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can confirm the data size by using the `len` function on our training and
    testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we run the code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we have a training data size of 60,000 images and test data
    of 10,000 images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In CNNs, unlike the DNNs we used previously, we need to account for the color
    channels of the input images. Currently, our training and testing data has a shape
    of `(batch_size, height, width)` for grayscale images, with a single channel.
    However, CNN models require a 4D input tensor, made up of `batch_size`, `height`,
    `width`, and `channels`. We can fix this data mismatch by simply reshaping our
    data and converting the elements to `float32` values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This preprocessing step is standard before training an ML model, as most models
    require floating-point input. Since our images are grayscale, there is only one
    color channel, which is why we reshape the data to include a single channel dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pixel value of our data (training and testing data) ranges from `0` to
    `255`, where `0` represents black and `255` represents white. We normalize our
    data by dividing the pixel values by 255 to bring the pixel values in our data
    to a scale of between `0` and `1`. We do this to enable our model to converge
    faster and perform better:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the `to_categorical` function from the `utils` module of `tf.keras`
    to convert our labels (`y_train` and `y_test`) that have an integer value of `0`
    to `9` into one-hot encoded arrays. The `to_categorical` function takes two arguments:
    the labels to be converted, and the number of classes; it returns a one-hot encoded
    array, as shown in *Figure 7**.15*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.15 – A one-hot encoded array](img/B18118_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – A one-hot encoded array
  prefs: []
  type: TYPE_NORMAL
- en: 'The one-hot encoded vectors will have a length of `10`, with a number `1` in
    the index that corresponds to the label for a given data point, and `0` in all
    other indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the Sequential Model API from `tf.keras.model`, we will create a CNN
    architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first layer is a convolution layer composed of 64 filters of size 3x3 to
    process the input images, which have a shape of 28x28 pixels and 1 channel (grayscale).
    ReLU is used as the activation function. The subsequent max pooling layer is a
    2D pooling layer that applies max pooling to downsample the output of the convolution
    layer, reducing the dimensionality of the feature maps. The `flatten` layer takes
    the output of the pooling layer and flattens it into a 1D array, which is then
    processed by the fully connected layer. The output layer contains `softmax` activation
    for multiclass classification and 10 neurons, one for each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compile and fit the model on our training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `compile()` function takes three arguments: loss function (`categorical_crossentropy`,
    since this is a multi-class classification task), optimizer (`adam`), and metrics
    (`accuracy`). After compiling the model, we used the `fit()` function to train
    the model on the training data. We specified the number of epochs as `10` and
    used 20% of the training data for validation purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 10 epochs, we arrive at a training accuracy of `0.9785` and a validation
    accuracy of `0.9133`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `summary` function is a very useful way to get a high-level overview of
    the model’s architecture and understand the number of parameters and the shape
    of the output tensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output returns the five layers that make up our current model architecture.
    It also displays the output shape and the number of parameters of each layer.
    The total number of parameters is 1,386,506\. From the output, we see that the
    output shape from the convolution layer is 26x26 as a result of the border effect
    since we did not apply padding. Next, the max pooling layer halves the pixel size
    after which we flatten the data and generate predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will use the `evaluate` function to evaluate our model on test
    data. The `evaluate` function returns the loss and the accuracy of the model on
    test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our model achieved an accuracy of `0.9079` on the test data, surpassing the
    performance of the architectures used in [*Chapter 6*](B18118_06.xhtml#_idTextAnchor129)*,
    Improving the Model*. We can try to further improve the model’s performance by
    adjusting the hyperparameters and applying data augmentation. Let's turn our attention
    to real-world images, where CNNs clearly outshine our previous models.
  prefs: []
  type: TYPE_NORMAL
- en: Working with real-world images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-world images pose a different type of challenge as these images are usually
    colored images with three color channels (red, green, and blue), unlike the grayscale
    images we used from our fashion MNIST dataset. In *Figure 7**.16*, where we see
    an example of real-world images from the weather dataset that we will be modeling
    shortly, you will notice the images are of varying sizes. This introduces another
    layer of complexity that requires additional preprocessing steps such as resizing
    or cropping to ensure all our images are of uniform dimensions before we feed
    them into our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Images from the weather dataset](img/B18118_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Images from the weather dataset
  prefs: []
  type: TYPE_NORMAL
- en: Another issue we may encounter when working with real-world images is the presence
    of various noise sources. For example, we may have images in our dataset taken
    in conditions with uneven lighting or unintended blurring. Again, we could have
    images with multiple objects or other unintended distractions in the background
    among the images in our real-world dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To address these issues, we could apply noise reduction techniques such as denoising
    to improve the quality of our data. We could also use object detection techniques
    such as bounding boxes or segmentation to help us identify the target object within
    an image with multiple objects. The good part is TensorFlow is well equipped with
    a comprehensive set of tools tailored to handling these challenges. One important
    tool from TensorFlow is the `tf.image` module, which offers an array of image
    preprocessing functionalities such as resizing various adjustments (for example,
    brightness, contrast, hue, and saturation), application of bounding boxes, cropping,
    flipping, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: However, this module is beyond the scope of this book and the exam itself. But,
    if you wish to learn more about this module, you can visit the TensorFlow documentation
    at [https://www.tensorflow.org/api_docs/python/tf/image](https://www.tensorflow.org/api_docs/python/tf/image).
    Another tool in TensorFlow’s arsenal is `ImageDataGenerator`, which enables us
    to perform data augmentation on the fly, offering us the ability to preprocess
    and perform augmentative actions (such as rotation and flipping images) in real
    time as we feed these images into our training pipeline. Let's proceed to work
    with our real-world image dataset and see `ImageDataGenerator` in action.
  prefs: []
  type: TYPE_NORMAL
- en: Weather dataset classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this case study, we will be working as a computer vision consultant for
    an emerging start-up called WeatherBIG. You have been assigned the responsibility
    of developing an image classification system that will be used to identify different
    weather conditions; the dataset for this task can be found on Kaggle using this
    link: [https://www.kaggle.com/datasets/rahul29g/weatherdataset](https://www.kaggle.com/datasets/rahul29g/weatherdataset).
    The dataset has been packaged into three folders made up of a training folder,
    a validation folder, and a testing folder. Each of these folders has subfolders
    with each weather class. Let’s get started with the task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing several libraries to build our image classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have used several of these libraries in our previous experiments; however,
    let's address the functionalities of a few libraries that we will be using for
    the first time. The `os` module acts as a bridge to our operating system. It gives
    us the ability to read from and write to our filesystem, while `pathlib` offers
    us an intuitive, object-oriented way to streamline our file navigation tasks.
    For image manipulation, we use `PIL`, and we also have the `ImageDataGenerator`
    class from the `tensorflow.keras.preprocessing.image` module for our data preprocessing
    steps, batch generation, and data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can access/download the dataset for this case study from [https://www.kaggle.com/datasets/rahul29g/weatherdataset](https://www.kaggle.com/datasets/rahul29g/weatherdataset)
    and upload it to Google Drive. Once you do this, you can easily follow along with
    the code in this section. In my case, the data is stored in this root directory:
    `/content/drive/MyDrive/weather dataset`. In your case, your root directory will
    be different, so make sure you change the directory path to match the directory
    where the dataset is stored in your Google Drive: `root_dir = "/``content/drive/MyDrive/weather
    dataset"`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we apply the `os.walk` function to access the root directory and generate
    information about the content of all the directories and subdirectories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the code returns a tuple made up of the path of each directory and
    the number of images within each of them, as illustrated in *Figure 7**.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – A snapshot directory and subdirectories](img/B18118_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – A snapshot directory and subdirectories
  prefs: []
  type: TYPE_NORMAL
- en: We use this step to get a sense of the contents of each directory and subdirectory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `retrieve_labels` function to fetch and display labels and their
    corresponding counts from the training, test, and validation directories. To craft
    this function, we use the `listdir` method from the `os` module and we pass in
    the respective directory paths (`train_dir`, `test_dir`, and `val_dir`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We specify the path to the training, test, and validation directories in the
    `train_dir`, `test_dir`, and `val_dir` arguments, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we run the code, it returns the training data, test data, validation data
    labels, and the number of labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'For our exploration, let''s craft a function called `view_random_images` to
    randomly access and display images from the subdirectories within our dataset.
    The function takes in the main directory that holds the subdirectories housing
    our images and the number of images we want to display. We apply `listdir` to
    access the subdirectories and to introduce randomness in the selection process.
    We use the `shuffle` function from the `random` library for shuffling and selecting
    images randomly. Matplotlib is used to display the specified number of random
    images in our function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s try out the function by setting `num_images` to `4` and examine some
    data in our `train` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This returns four randomly selected images, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![7.18 – Randomly selected images from the weather dataset](img/B18118_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 7.18 – Randomly selected images from the weather dataset
  prefs: []
  type: TYPE_NORMAL
- en: From the data displayed, we can see the images come in various sizes (height
    and weight) and we will need to fix this preprocessing issue. We will be using
    the `ImageDataGenerator` class from TensorFlow. Let's discuss this next.
  prefs: []
  type: TYPE_NORMAL
- en: Image data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw, in *Figure 7**.18*, that our training images are of different sizes.
    Here, we will be resizing and normalizing our data before training. Also, we want
    to develop an efficient method of loading our training data in batches, ensuring
    optimized memory usage with seamless integration with our model’s training process.
    To achieve all of this, we will be utilizing the `ImageDataGenerator` class from
    the `TensorFlow.keras.preprocessing.image` module. In [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186),
    *Handling Overfitting,* we will take our application of `ImageDataGenerator` further
    by using it to enlarge our training dataset by developing variants of our image
    data by rotating, flipping, and zooming. This could help our model become more
    robust and reduce the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful tool to aid our data preprocessing task is the `flow_from_directory`
    method. We can use this method to build data pipelines. It is especially useful
    when we are working on large-scale, real-world data because of its ability to
    automate reading, resizing, and batching images for model training or inference.
    The `flow_from_directory` method takes three main arguments. The first is the
    directory path that contains our image data. Next, we specify the desired size
    of the images before we feed them into our neural network. Then, we also have
    to specify the batch size to determine the number of images we want to process
    simultaneously. We can tailor the process more by specifying other parameters,
    such as color mode, class mode, and shuffle. Let's now take a look at a typical
    directory structure for a multiclass classification problem, as illustrated in
    *Figure 7**.19*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – The directory structure for a multiclass classification problem](img/B18118_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – The directory structure for a multiclass classification problem
  prefs: []
  type: TYPE_NORMAL
- en: 'When applying the `flow_from_directory` method, it is important that we organize
    our images in a well-structured directory, with subdirectories for each unique
    class label as displayed in *Figure 7**.19*. Here, we have four subdirectories,
    one for each class label in our weather dataset. Once all the images are in the
    appropriate subdirectories, we can apply `flow_from_directory` to set up an iterator.
    This iterator is adjustable so that we can define parameters such as the image
    size and batch size and decide whether we want to shuffle our data or not. Let’s
    apply these new ideas to our current case study:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define three instances of the `ImageDataGenerator` class: one for
    training, one for validation, and one for testing. We apply a rescaling factor
    of 1/255 to the pixel values of images in each instance to normalize our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `flow_from_directory` to import images from the respective training,
    validation, and testing directories, and the resulting data is stored in our `train_data`,
    `valid_data`, and `test_data` variables. In addition to specifying the directories
    in our `flow_from_directory` method, you will notice we also specified not just
    the target size (224 x244) and batch size (64) but also the type of problem we
    are tackling as `categorical` because we are dealing with a multi-classification
    use case. We have now successfully completed our data preprocessing steps. Let''s
    move on to modeling our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use a CNN architecture made up of three sets of convolutional and pooling
    layers. In the first convolutional layer, we apply 16 filters with a filter size
    of 3\. Notice the input shape also matches the shape defined in our preprocessing
    step. After the first convolutional layer, we apply max pooling of 2x2\. Next,
    we reach the second convolutional layer, which utilizes 32 filters, each 3x3 in
    size, followed by another 2x2 max pooling layer. The final convolutional layer
    has 64 filters, each 3x3 in size, followed by another max pooling layer, which
    further downsamples the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we reach the fully connected layers. Here, we first flatten the 3D output
    of the earlier layers into a 1D array. Then, we feed the data into dense layers
    for final classification. We proceed by compiling and fitting our model to our
    data. It’s important to note that in our `compile` step, we use `CategoricalCrossentropy`
    for our `loss` function as we are dealing with a task with multiple classes, and
    we set `metrics` to `accuracy`. The resulting output is a probability distribution
    over the four classes in our dataset, with the class with the highest probability
    being the predicted label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'We train our model for 10 epochs, attaining a training accuracy of around 94%
    on training data and 91% on validation data. We use the `summary` method to obtain
    information about the different layers in the model. This information includes
    the layer-wise overview, output shape, and number of parameters used (trainable
    and non-trainable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'From our model’s summary, we see our architecture has three convolutional (`Conv2D`)
    layers, each accompanied by a pooling (`MaxPooling2D`) layer. Information flows
    from these layers into the fully connected layer, where the final classification
    is carried out. Let''s drill down into each of the layers and unpack the information
    they provide us with. The first convolutional layer is with an output shape of
    `(None, 222, 222, 16)`. Here, `None` means we didn’t hardcode the batch size,
    which gives us the flexibility to use different batch sizes with ease. Next, we
    have `222, 222`, which represents the dimension of the output feature map; we
    lose 2 pixels in height and weight because of the boundary effect if we do not
    apply padding. Finally, `16` represents the number of filters or kernels used,
    which means we will have an output of 16 different feature maps from each of the
    filters. You will also notice this layer has `448` parameters. To calculate the
    number of parameters in the convolutional layers, we use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(Filter width × Filter height × Input channels + 1(for bias)) × Number of
    filters = Total number of parameters in the* *convolutional layer*'
  prefs: []
  type: TYPE_NORMAL
- en: When we key in the values into the formula, we arrive at (3 × 3 × 3 + 1) × 16
    = 448 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The next layer is the first pooling layer, which is a `MaxPooling2D` layer that
    downsamples the output feature maps from the convolutional layer. Here, we have
    an output shape of `(None, 111, 111, 16)`. From the output, you can see that the
    spatial dimension has been reduced to half, and it is also important to note that
    pooling layers have no parameters, as you will observe with all the pooling layers
    in our model’s summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we reach the second convolutional layer and notice the depth of our output
    has increased to `32`. This happens because we employed 32 filters in this layer;
    hence, we will have 32 different feature maps returned. Also, we have the spatial
    dimension of the feature maps again reduced by two pixels because of the boundary
    effect. We can easily calculate the number of parameters in this layer as follows:
    (3 × 3 × 16 + 1) × 32 = 4,640 parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we reach the second pooling layer, which downsamples the feature maps
    further to `(None, 54, 54, 32)`. The final convolutional layer uses 64 filters,
    so it has an output shape of `(None, 52, 52, 64)` and 18,496 parameters. The final
    pooling layer again reduces the dimension of our data to `(None, 26, 26, 64)`.
    The output of the final pooling layer is fed into the `Flatten` layer, which reshapes
    the data from a 3D tensor into a 1D tensor with a size of 26 x 26 x 64 = 43,264\.
    This is fed into the first `Dense` layer, which has an output shape of `(None,
    1050)`. To calculate the number of parameters in the `Dense` layer, we use this
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(Number of input nodes + 1) × Number of* *output nodes*'
  prefs: []
  type: TYPE_NORMAL
- en: When we input the values, we get (43,264 + 1) × 1,050 = 45,428,250 parameters.
    The final `Dense` layer is the output layer and it has a shape of `(None, 4)`,
    where `4` represents the number of unique classes in our data that we want to
    predict. This layer has (1,050 + 1) × 4 = 4,204 parameters due to its connections,
    biases, and the number of output neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we evaluate our model using the `evaluate` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: We reach an accuracy of 91% on our test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare our CNN architecture with two DNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'We build a DNN called `model_2` made up of 4 `Dense` layers, with `1200`, `600`,
    `300`, and `4` neurons, respectively. Apart from the output layer, which uses
    the `softmax` function for classification, all the other layers use ReLU as their
    activation function. We compile and fit `model_2` in the same way as `model_1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: After 10 epochs, we reach a validation accuracy of 74.86% and when we examine
    the model’s summary, we see that we have used a total of 181,536,904 parameters,
    which is 4 times the size of our CNN architecture parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at another DNN architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'We use another set of 4 `Dense` layers, with `1000`, `500`, `500`, and `4`
    neurons, respectively. We fit and compile `model_3` as well for 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'We reach a validation accuracy of 77.65% after 10 epochs and this model has
    around 151,282,004 parameters; the results are not close to those of our CNN architecture.
    Let''s proceed to compare all three models on test data, which is what we want
    to be judging our models on. To do this, we will write a function to generate
    a DataFrame showing the names, the loss, and the accuracy of the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'The `evaluate_models()` function takes a list of models, model names, and test
    data as input and returns a DataFrame with the evaluation results for each model
    as percentages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: When we run the code, it generates the table shown in *Figure 7**.20*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – A DataFrame showing the experimental results of all three models](img/B18118_07_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – A DataFrame showing the experimental results of all three models
  prefs: []
  type: TYPE_NORMAL
- en: 'From the results, we can clearly see that Model 1 is ahead. You may wish to
    experiment with larger DNNs but you will soon run out of memory. For larger datasets,
    the results may be much worse for DNNs. Next, let''s look at how we fared on our
    training and validation data with Model 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'We created a function to plot the training and validation loss and accuracy
    using matplotlib. We pass `history_1` into our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Loss and accuracy plot for Model 1](img/B18118_07_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Loss and accuracy plot for Model 1
  prefs: []
  type: TYPE_NORMAL
- en: From the plot, we can see that our training accuracy rises steadily but falls
    below its highest point just before the 10th epoch. Also, our validation data
    experiences a sharp fall in accuracy. Our loss veers off from the fourth epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw the power of CNNs. We began by examining the challenges
    faced by DNNs for visual recognition tasks. Next, we journeyed through the anatomy
    of CNNs, zooming in on the various moving parts, such as the convolutional, pooling,
    and fully connected layers. Here, we saw the impact and effect of different hyperparameters,
    and we also discussed the boundary effect. Next, we moved on to using all we learned
    to build a real-world weather classifier using two DNNs and a CNN. Our CNN model
    outperformed the DNNs, showcasing the strength of CNNs in handling image-based
    problems. Also, we discussed and applied some TensorFlow functions that streamline
    data preprocessing and modeling when we are working with image data.
  prefs: []
  type: TYPE_NORMAL
- en: By now you should have a good understanding of the structure and operations
    of CNNs and how to use them to solve real-world image classification problems,
    as well as utilizing various tools in TensorFlow to effectively and efficiently
    preprocess image data for improved model performance. In the next chapter, we
    will address the issue of overfitting in neural networks and explore various techniques
    to overcome this challenge, ensuring that our models generalize well to unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will use some old tricks such as callbacks and hyperparameter
    tuning to see whether we can improve our model’s performance. We will also experiment
    with data augmentation and other new techniques to improve our model’s performance.
    We draw the curtains on our task for now until [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186)*,*
    *Handling Overfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s test what we have learned in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the components of a typical CNN architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a convolutional layer work in a CNN architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is pooling and why is it used in a CNN architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of a fully connected layer in a CNN architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the impact of the padding on a convolution operation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of using TensorFlow image data generators?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, you can check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Dumoulin, V., & Visin, F. (2016). *A guide to convolution arithmetic for deep*
    *learning*. [http://arxiv.org/abs/1603.07285](http://arxiv.org/abs/1603.07285)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gulli, A., Kapoor, A. and Pal, S., 2019\. *Deep Learning with TensorFlow 2
    and Keras*. Birmingham: Packt Publishing Ltd'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kapoor, A., Gulli, A. and Pal, S. (2020) *Deep Learning with TensorFlow and
    Keras Third Edition: Build and deploy supervised, unsupervised, deep, and reinforcement
    learning models*. Packt Publishing Ltd'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). *ImageNet classification
    with deep convolutional neural networks*. In Advances in neural information processing
    systems (pp. 1097-1105)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang, Y., & Yang, H. (2018). *Food classification with convolutional neural
    networks and multi-class linear discernment analysis*. In 2018 IEEE International
    Conference on Information Reuse and Integration (IRI) (pp. 1-5). IEEE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang, Z., Ma, H., Fu, H., & Zha, C. (2020). *Scene-Free Multi-Class Weather
    Classification on Single Images*. IEEE Access, 8, 146038-146049\. doi:10.1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `tf.image` module: [https://www.tensorflow.org/api_docs/python/tf/image](https://www.tensorflow.org/api_docs/python/tf/image)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
