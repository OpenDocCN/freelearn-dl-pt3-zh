- en: Chapter 4. Using Reinforcement Learning for Predictive Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a human being, we learn from past experiences. We haven't become so charming
    by accident. Years of positive compliments as well as negative criticism have
    all helped shape us into who we are today. We learn how to ride a bike by trying
    out different muscle movements until it just clicks. When you perform actions,
    you're sometimes rewarded immediately. This is all about **reinforcement learning**
    (**RL**).
  prefs: []
  type: TYPE_NORMAL
- en: This lesson is all about designing a machine learning system driven by criticisms
    and rewards. We will see how to apply reinforcement learning algorithms for the
    predictive model on real-life datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered throughout this lesson:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning for predictive analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notation, policy, and utility in RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a multiarmed bandit's predictive model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a stock price predictive model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a technical perspective, whereas supervised and unsupervised learning appears
    at opposite ends of the spectrum, RL exists somewhere in the middle. It's not
    supervised learning because the training data comes from the algorithm deciding
    between exploration and exploitation. And it's not unsupervised because the algorithm
    receives feedback from the environment. As long as you are in a situation where
    performing an action in a state produces a reward, you can use reinforcement learning
    to discover a good sequence of actions to take the maximum expected rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of an RL agent will be to maximize the total reward that it receives
    in the long run. The third main sub element is the `value` function.
  prefs: []
  type: TYPE_NORMAL
- en: While the rewards determine an immediate desirability of the states, the values
    indicate the long-term desirability of states, taking into account the states
    that may follow and the available rewards in these states. The `value` function
    is specified with respect to the chosen policy. During the learning phase, an
    agent tries actions that determine the states with the highest value, because
    these actions will get the best amount of reward in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning in Predictive Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Figure 1 shows a person making decisions to arrive at their destination. Moreover,
    suppose on your drive from home to work you always choose the same route. But
    one day your curiosity takes over and you decide to try a different path in hopes
    for a shorter commute. This dilemma of trying out new routes or sticking to the
    best-known route is an example of exploration versus exploitation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement Learning in Predictive Analytics](img/04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An agent always try to reach the destination passing through route'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning techniques are being used in many areas. A general idea
    that is being pursued right now is creating an algorithm that doesn't need anything
    apart from a description of its task. When this kind of performance is achieved,
    it will be applied virtually everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Notation, Policy, and Utility in RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may notice that reinforcement learning jargon involves anthropomorphizing
    the algorithm into taking actions in situations to receive rewards. In fact, the
    algorithm is often referred to as an agent that acts with the environment. You
    can just think of it like an intelligent hardware agent sensing with sensors and
    interacting with the environment using its actuators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it shouldn''t be a surprise that much of RL theory is applied in
    robotics. Figure 2 demonstrates the interplay between states, actions, and rewards.
    If you start at state **s1**, you can perform action **a1** to obtain a reward
    r (**s1**, **a1**). Actions are represented by arrows, and states are represented
    by circles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Notation, Policy, and Utility in RL](img/04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An agent is performing an action on a state produces a reward'
  prefs: []
  type: TYPE_NORMAL
- en: A robot performs actions to change between different states. But how does it
    decide which action to take? Well, it's all about using different or a concrete
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In reinforcement learning lingo, we call the strategy a policy. The goal of
    reinforcement learning is to discover a good strategy. One of the most common
    ways to solve it is by observing the long-term consequences of actions in each
    state. The short-term consequence is easy to calculate: that''s just the reward.
    Although performing an action yields an immediate reward, it''s not always a good
    idea to greedily choose the action with the best reward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s a lesson in life too because the most immediate best thing to do might
    not always be the most satisfying in the long run. The best possible policy is
    called the optimal policy, and it''s often the holy grail of RL as shown in figure
    3, which shows the optimal action given any state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy](img/04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A policy defines an action to be taken in a given state'
  prefs: []
  type: TYPE_NORMAL
- en: We've so far seen one type of policy where the agent always chooses the action
    with the greatest immediate reward, called a greedy policy. Another simple example
    of a policy is arbitrarily choosing an action, called random policy. If you come
    up with a policy to solve a reinforcement learning problem, it's often a good
    idea to double-check that your learned policy performs better than both the random
    and greedy policies.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will also see how to develop another robust policy called policy
    gradients, where a neural network learns a policy for picking actions by adjusting
    its weights through gradient descent using feedback from the environment. We will
    see that although both the approaches are used, policy gradient is more direct
    and optimistic.
  prefs: []
  type: TYPE_NORMAL
- en: Utility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The long-term reward is called a utility. It turns out that, if we know the
    utility of performing an action at a state, then it''s easy to solve reinforcement
    learning. For example, to decide which action to take, we simply select the action
    that produces the highest utility. However, uncovering these utility values is
    the harder part to be sorted out. The utility of performing an action *a* at a
    state **s** is written as a function *Q(s, a)*, called the utility function that
    predicts the expected immediate reward plus rewards following an optimal policy
    gave the state-action input which is shown in figure 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utility](img/04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Using a utility function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most reinforcement learning algorithms boil down to just three main steps:
    infer, do, and learn. During the first step, the algorithm selects the best action
    (*a*) given a state (*s*) using the knowledge it has so far. Next, it does the
    action to find out the reward (*r*) as well as the next state (*s''*). Then it
    improves its understanding of the world using the newly acquired knowledge *(s,
    r, a, s'')*. However, this is just a naive way to calculate the utility; you would
    agree on this too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the question is what could be a more robust way to compute it? Here are
    two cents from my side. We can calculate the utility of a particular state-action
    pair *(s, a)* by recursively considering the utilities of future actions. The
    utility of your current action is influenced not just by the immediate reward,
    but also the next best action, as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utility](img/04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous formula, *s'* denotes the next state, and *a'* denotes the next
    action. The reward of taking action *a* in state *s* is denoted by *r(s, a)*.
    Here, *γ* is a hyperparameter that you get to choose, called the discount factor.
    If *γ* is *0*, then the agent chooses the action that maximizes the immediate
    reward. Higher values of *γ* will make the agent put more importance in considering
    long-term consequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we have more hyperparameters to be considered. For example, if
    a vacuum cleaner robot is expected to learn to solve tasks quickly but not necessarily
    optimally, we might want to set a faster learning rate. Alternatively, if a robot
    is allowed more time to explore and exploit, we might tune down the learning rate.
    Let''s call the learning rate **α**, and change our utility function as follows
    (note that when *α = 1*, both the equations are identical):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Utility](img/04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In summary, an RL problem can be solved if we know this *Q(s, a)* function.
    Here comes the machine learning strategy called neural networks, which are a way
    to approximate functions given enough training data. Also, TensorFlow is the perfect
    tool to deal with neural networks because it comes with many essential algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the next two sections, we will see two examples of such implementation with
    TensorFlow. The first example is a naïve way of developing a multiarmed bandit
    agent for the predictive model. Then, the second example is a bit more advanced
    using neural network implementation for stock price prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a Multiarmed Bandit's Predictive Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the simplest RL problems is called n-armed bandits. The thing is there
    are n-many slot machines but each has different fixed payout probability. The
    goal is to maximize the profit by always choosing the machine with the best payout.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, we will also see how to use policy gradient that produces
    explicit outputs. For our multiarmed bandits, we don't need to formalize these
    outputs on any particular state. To be simpler, we can design our network such
    that it will consist of just a set of weights that are corresponding to each of
    the possible arms to be pulled in the bandit. Then, we will represent how good
    an agent thinks to pull each arm to make maximum profit. A naive way is to initialize
    these weights to 1 so that the agent will be optimistic about each arm's potential
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: To update the network, we can try choosing an arm with a greedy policy that
    we discussed earlier. Our policy is such that the agent receives a reward of either
    `1` or `-1` once it has issued an action. I know this is not a realistic imagination
    but most of the time the agent will choose an action randomly that corresponds
    to the largest expected value.
  prefs: []
  type: TYPE_NORMAL
- en: We will start developing a simple but effective bandit agent incrementally for
    solving multiarmed bandit problems. At first, there will be no state, that is,
    we will have a stateless agent. Then, we will see that using a stateless bandit
    agent to solve a complex problem is so biased that we cannot use it in real life.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we will increase the agent complexity by adding or converting the sample
    bandits into contextual bandits. The contextual bandits then will be a state full
    agent so can solve our predicting problem more efficiently. Finally, we will further
    increase the agent complexity by converting the textual bandits to full RL agent
    before deploying it:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the required library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the required library and packages/modules needed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Defining bandits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For this example, I am using a four-armed bandit. The `getBandit` function
    that generates a random number from a normal distribution has the mean of 0\.
    The lower the Bandit number, the more likely a positive reward will be awarded.
    As stated earlier, this is just a naïve but greedy way to train the agent so that
    it learns to choose a bandit that will generate not only the positive but also
    the maximum reward. Here I have listed the bandits so that Bandit 4 most often
    provides a positive reward:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Developing an agent for the bandits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code creates a very simple neural agent consisting of a set of
    values for each of the `bandits`. Each value is estimated to be 1 based on the
    return value from the `bandits`. We use a policy gradient method to update the
    agent by moving the value for the selected action toward the received reward.
    At first, we need to reset the graph as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, the next two lines do the actual choosing by establishing the feed-forward
    part of the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, before starting the training process, we need to initiate the training
    process itself. Since we already know the reward, now it''s time to feed them
    and choose an action in the network to compute the loss and use it to update the
    network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to define the objective function that is loss:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And then let''s make the training process slow to make it exhaustive utilizing
    the learning rate:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then use the gradient descent `optimizer` and instantiate the `training`
    operation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it''s time to define the training parameters such as a total number of
    iterations to train the agent, `reward` function, and a `random` action. The reward
    here sets the scoreboard for `bandits` to `0`, and by choosing a random action,
    we set the probability of taking a random action:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we initialize the global variables:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to train the agent by taking actions to the environment and receiving
    rewards. We start by creating a TensorFlow session and launch the TensorFlow graph.
    Then, iterate the training process up to a total number of iterations. Then, we
    choose either a random act or one from the network. We then compute the reward
    from picking one of the `bandits`. Then, we make the training process consistent
    and update the network. Finally, we update the scoreboard:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let''s evaluate the above model as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first iteration generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second iteration generates a different result as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that if you see the limitation of this agent being a stateless agent so
    randomly predicts which bandits to choose. In that situation, there are no environmental
    states, and the agent must simply learn to choose which action is best to take.
    To get rid of this problem, we can think of developing contextual bandits.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the contextual bandits, we can introduce and make the proper utilization
    of the state. The state consists of an explanation of the environment that the
    agent can use to make more intelligent and informed actions. The thing is that
    instead of using a single bandit we can chain multiple bandits together. So what
    would be the function of the state? Well, the state of the environment tells the
    agent to choose a bandit from the available list. On the other hand, the goal
    of the agent is to learn the best action for any number of bandits.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This way, the agent faces an issue since each bandit may have different reward
    probabilities for each arm and agent needs to learn how to perform an action on
    the state of the environment. Otherwise, the agent cannot achieve the maximum
    reward possible:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Developing a Multiarmed Bandit''s Predictive Model](img/04_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5: Stateless versus contextual bandits'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As mentioned earlier, to get rid of this issue, we can build a single-layer
    neural network so that it can take a state and yield an action. Now, similar to
    the random bandits, we can use a policy-gradient update method too so that the
    network update is easier to take actions for maximizing the reward. This simplified
    way of posting an RL problem is referred to as the contextual bandit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Developing contextual bandits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This example was adopted and extended based on "Simple Reinforcement Learning
    with TensorFlow Part 1.5: Contextual Bandits" By Arthur Juliani published at [https://medium.com/](https://medium.com/).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At first, let's define our contextual bandits. For this example, we will see
    how to use three four-armed bandits, that is, each Bandit has four arms that can
    be pulled to make an action. Since each bandit is contextual and has a state,
    so their arms have different success probabilities. This requires different actions
    to be performed to yield the best predictive result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here, we define a class named `contextual_bandit()` consisting of a constructor
    and two user defined functions: `getBandit()` and `pullArm()`. The `getBandit()`
    function generates a random number from a normal distribution with a mean of `0`.
    The lower the Bandit number, the more likely a positive reward will be returned
    to be utilized. We want our agent to learn to choose the banditarm that will most
    often give a positive reward. Of course, it depends on the bandit presented. This
    constructor lists out all of our bandits. We assume the current state being armed
    `4`, `2`, `3`, and `1` that is the most optimal respectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Also, if you see carefully, most of the reinforcement learning algorithms follow
    similar implementation patterns. Thus, it''s a good idea to create a class with
    the relevant methods to reference later, such as an abstract class or interface:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Developing a policy-based agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following class `ContextualAgent` helps develop our simple, but very effective
    neural and contextual agent. We supply the current state as input and it then
    returns an action that is conditioned on the state of the environment. This is
    the most important step toward making a stateless agent a stateful one to be able
    to solve a full RL problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here, I tried to develop this agent such that it uses a single set of weights
    for choosing a particular arm given a bandit. The policy gradient method is used
    to update the agent by moving the value for a particular action toward achieving
    maximum reward:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training the contextual bandit agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At first, we clear the default TensorFlow graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we define some parameters that will be used to train the agent:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, before starting the training, we need to load the bandits and then our
    agent:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, to maximize the objective function toward total rewards, `weights` is
    used to evaluate to look into the network. We also set the scoreboard for `bandits`
    to `0` initially:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we initialize all the variables using `global_variables_initializer()function`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will start the training. The training is similar to the random
    one we have done in the preceding example. However, here the main objective of
    the training is to compute the mean reward for each of the bandits so that we
    can evaluate the agent''s prediction accuracy later on by utilizing them:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Evaluating the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, that we have the mean reward for all the four bandits, it''s time to utilize
    them to predict something interesting, that is, which bandit''s arm will maximize
    the reward. Well, at first we can initialize some variables to estimate the prediction
    accuracy as well:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then let''s start evaluating the agent''s prediction performance:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see, all the predictions made are right predictions. Now we can
    compute the accuracy as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Fantastic, well done! We have managed to design and develop a more robust bandit
    agent by means of a contextual agent that can accurately predict which arm, that
    is, the action of a bandit that would help to achieve the maximum reward, that
    is, profit.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see another interesting but very useful application
    for stock price prediction, where we will see how to develop a policy-based Q
    Learning agent out of the box of the RL.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a Stock Price Predictive Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An emerging area for applying is the stock market trading, where a trader acts
    like a reinforcement agent since buying and selling (that is, action) particular
    stock changes the state of the trader by generating profit or loss, that is, reward.
    The following figure shows some of the most active stocks on July 15, 2017 (for
    an example):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a Stock Price Predictive Model](img/04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: [https://finance.yahoo.com/](https://finance.yahoo.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we want to develop an intelligent agent that will predict stock prices
    such that a trader will buy at a low price and sell at a high price. However,
    this type of prediction is not so easy and is dependent on several parameters
    such as the current number of stocks, recent historical prices, and most importantly,
    on the available budget to be invested for buying and selling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The states in this situation are a vector containing information about the
    current budget, current number of stocks, and a recent history of stock prices
    (the last 200 stock prices). So each state is a 202-dimensional vector. For simplicity,
    there are only three actions to be performed by a stock market agent: buy, sell,
    and hold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have the state and action, what else do you need? Policy, right? Yes,
    we should have a good policy, so based on that an action will be performed in
    a state. A simple policy can consist of the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: Buying (that is, action) a stock at the current stock price (that is, state)
    decreases the budget while incrementing the current stock count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selling a stock trades it in for money at the current share price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holding does neither, and performing this action simply waits for a particular
    time period and yields no reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To find the stock prices, we can use the `yahoo_finance` library in Python.
    A general warning you might experience is "**HTTPError: HTTP Error 400: Bad Request**".
    But keep trying.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to get familiar with this module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: So as of July 14, 2017, the stock price of Microsoft Inc. went higher, from
    72.24 to 72.78, which means about a 7.5% increase. However, this small and just
    one-day data doesn't give us any significant information. But, at least we got
    to know the present state for this particular stock or instrument.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `yahoo_finance`, issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it would be worth looking at the historical data. The following function
    helps us get the historical data for Microsoft Inc:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `get_prices()` method takes several parameters such as the share symbol
    of an instrument in the stock market, the opening date, and the end date. You
    will also like to specify and cache the historical data to avoid repeated downloading.
    Once you have downloaded the data, it's time to plot the data to get some insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function helps us to plot the price:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can call these two functions by specifying a real argument as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here I have chosen a wide range for the historical data of 17 years to get
    a better insight. Now, let''s take a look at the output of this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a Stock Price Predictive Model](img/04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Historical stock price data of Microsoft Inc. from 2000 to 2017'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to learn a policy that gains the maximum net worth from trading
    in the stock market. So what will a trading agent be achieving in the end? Figure
    8 gives you some clue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a Stock Price Predictive Model](img/04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Some insight and a clue that shows, based on the current price, up
    to $160 profit can be made'
  prefs: []
  type: TYPE_NORMAL
- en: Well, figure 8 shows that if the agent buys a certain instrument with price
    $20 and sells at a peak price say at $180, it will be able to make $160 reward,
    that is, profit.
  prefs: []
  type: TYPE_NORMAL
- en: So, implementing such an intelligent agent using RL algorithms is a cool idea?
  prefs: []
  type: TYPE_NORMAL
- en: 'From the previous example, we have seen that for a successful RL agent, we
    need two operations well defined, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: How to select an action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to improve the utility Q-function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be more specific, given a state, the decision policy will calculate the next
    action to take. On the other hand, improve Q-function from a new experience of
    taking an action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, most reinforcement learning algorithms boil down to just three main steps:
    infer, perform, and learn. During the first step, the algorithm selects the best
    action (*a*) given a state (*s*) using the knowledge it has so far. Next, it performs
    the action to find out the reward (*r*) as well as the next state (*s''*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, it improves its understanding of the world using the newly acquired knowledge
    *(s, r, a, s'')* as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a Stock Price Predictive Model](img/04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Steps to be performed for implementing an intelligent stock price
    prediction agent'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's start implementing the decision policy based on which action will
    be taken for buying, selling, or holding a stock item. Again, we will do it an
    incremental way. At first, we will create a random decision policy and evaluate
    the agent's performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before that, let''s create an abstract class so that we can implement it
    accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The next task that can be performed is to inherit from this superclass to implement
    a random decision policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The previous class did nothing except defining a function named `select_action
    ()`, which will randomly pick an action without even looking at the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you would like to use this policy, you can run it on the real-world
    stock price data. This function takes care of exploration and exploitation at
    each interval of time, as shown in the following figure that form states S1, S2,
    and S3\. The policy suggests an action to be taken, which we may either choose
    to exploit or otherwise randomly explore another action. As we get rewards for
    performing an action, we can update the policy function over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a Stock Price Predictive Model](img/04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A rolling window of some size iterates through the stock prices
    over time'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fantastic, so we have the policy and now it''s time to utilize this policy
    to make decisions and return the performance. Now, imagine a real scenario—suppose
    you''re trading on Forex or ForTrade platform, then you can recall that you also
    need to compute the portfolio and the current profit or loss, that is, reward.
    Typically, these can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'At first, we can initialize values that depend on computing the net worth of
    a portfolio, where the state is a `hist+2`dimensional vector. In our case, it
    would be 202 dimensional. Then we define the range of tuning the range up to:'
  prefs: []
  type: TYPE_NORMAL
- en: Length of the prices selected by the user query – (history + 1), since we start
    from 0, we subtract 1 instead. Then, we should calculate the updated value of
    the portfolio and from the portfolio, we can calculate the value of the reward,
    that is, profit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we have already defined our random policy, so we can then select an action
    from the current policy. Then, we repeatedly update the portfolio values based
    on the action in each iteration and the new portfolio value after taking the action
    can be calculated. Then, we need to compute the reward from taking an action at
    a state. Nevertheless, we also need to update the policy after experiencing a
    new action. Finally, we compute the final portfolio worth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous simulation predicts a somewhat good result; however, it produces
    random results too often. Thus, to obtain a more robust measurement of success,
    let''s run the simulation a couple of times and average the results. Doing so
    may take a while to complete, say 100 times, but the results will be more reliable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous function computes the average portfolio and the standard deviation
    by iterating the previous simulation function 100 times. Now, it''s time to evaluate
    the previous agent. As already stated, there will be three possible actions to
    be taken by the stock trading agent such as buy, sell, and hold. We have a state
    vector of 202 dimension and budget only $`1000`. Then, the evaluation goes as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The first one is the mean and the second one is the standard deviation of the
    final portfolio. So, our stock prediction agent predicts that as a trader you/we
    could make a profit about $513\. Not bad. However, the problem is that since we
    have utilized a random decision policy, the result is not so reliable. To be more
    specific, the second execution will definitely produce a different result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we should develop a more robust decision policy. Here comes the
    use of neural network-based QLearning for decision policy. Next, we will see a
    new hyperparameter epsilon to keep the solution from getting stuck when applying
    the same action over and over. The lesser its value, the more often it will randomly
    explore new actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a Stock Price Predictive Model](img/04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The input is the state space vector with three outputs, one for
    each output''s Q-value'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I am going to write a class containing their functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Constructor`: This helps to set the hyperparameters from the Q-function. It
    also helps to set the number of hidden nodes in the neural networks. Once we have
    these two, it helps to define the input and output tensors. It then defines the
    structure of the neural network. Further, it defines the operations to compute
    the utility. Then, it uses an optimizer to update model parameters to minimize
    the loss and sets up the session and initializes variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`select_action`: This function exploits the best option with probability 1-epsilon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update_q`: This updates the Q-function by updating its model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this lesson, we have discussed a wonderful field of machine learning called
    reinforcement learning with TensorFlow. We have discussed it from the theoretical
    as well as practical point of view. Reinforcement learning is the natural tool
    when a problem can be framed by states that change due to actions that can be
    taken by an agent to discover rewards. There are three primary steps in implementing
    the algorithm: infer the best action from the current state, perform the action,
    and learn from the results.'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to implement RL agents for making predictions by knowing the
    `action`, `state`, `policy`, and `utility` functions. We have seen how to develop
    RL-based agents using random policy as well as neural network-based QLearning
    policy. QLearning is an approach to solve reinforcement learning, where you develop
    an algorithm to approximate the utility function (`Q`-function). Once a good enough
    approximation is found, you can start inferring best actions to take from each
    state. In particular, we have seen two step-by-step examples that show how we
    could develop a multiarmed bandit agent and a stock price prediction agent with
    very good accuracy. But, be advised that the actual stock market is a much more
    complicated beast, and the techniques used in this lesson generalize too many
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: This is more or less the end of our little journey with TensorFlow. I hope you'd
    a smooth journey and gained a lot of knowledge on TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: I wish you all the best for your future projects. Keep learning and exploring!
  prefs: []
  type: TYPE_NORMAL
- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In reinforcement learning lingo, we call the ______ a policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether the following statement is True or False: The goal of reinforcement
    learning is to discover a good strategy. One of the most common ways to solve
    it is by observing the long-term consequences of actions in each state.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to train the agent by taking actions to the environment and receiving
    ______.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether the following statement is True or False: Using the contextual
    bandits, we cannot introduce and make the proper utilization of the state.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To find the stock prices, we can use the _______ library in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: get_prices
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: plot_prices
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: yahoo_finance
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: finance_yahoo
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
