- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Deep Learning with TensorFlow and Keras, Third Edition*, is a concise yet
    thorough introduction to modern neural networks, artificial intelligence, and
    deep learning technologies designed especially for software engineers and data
    scientists. The book is the natural follow-up of the books *Deep Learning with
    Keras* [1] and *TensorFlow 1.x Deep Learning Cookbook* [2] previously written
    by the same authors.'
  prefs: []
  type: TYPE_NORMAL
- en: This book provides a very detailed panorama of the evolution of learning technologies
    over the past six years. The book presents dozens of working deep neural networks
    coded in Python using TensorFlow 2.x, a modular network library based on Keras-like
    APIs [1].
  prefs: []
  type: TYPE_NORMAL
- en: '**Artificial Intelligence** (**AI**) lays the ground for everything this book
    discusses. **Machine Learning** (**ML**) is a branch of AI, and **Deep Learning**
    (**DL**) is in turn a subset of ML. This section will briefly discuss these three
    concepts, which you will regularly encounter throughout the rest of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: AI denotes any activity where machines mimic intelligent behaviors typically
    shown by humans. More formally, it is a research field in which machines aim to
    replicate cognitive capabilities such as learning behaviors, proactive interaction
    with the environment, inference and deduction, computer vision, speech recognition,
    problem-solving, knowledge representation, and perception. AI builds on elements
    of computer science, mathematics, and statistics, as well as psychology and other
    sciences studying human behaviors. There are multiple strategies for building
    AI. During the 1970s and 1980s, “expert” systems became extremely popular. The
    goal of these systems was to solve complex problems by representing the knowledge
    with a large number of manually defined if-then rules. This approach worked for
    small problems on very specific domains, but it was not able to scale up for larger
    problems and multiple domains. Later, AI focused more and more on methods based
    on statistical methods that are part of ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML is a subdiscipline of AI that focuses on teaching computers how to learn
    without the need to be programmed for specific tasks. The key idea behind ML is
    that it is possible to create algorithms that learn from, and make predictions
    on, data. There are three different broad categories of ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**, in which the machine is presented with input data
    and the desired output, and the goal is to learn from those training examples
    in such a way that meaningful predictions can be made for data that the machine
    has never observed before.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**, in which the machine is presented with input data
    only, and the machine has to subsequently find some meaningful structure by itself,
    with no external supervision or input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**, in which the machine acts as an agent, interacting
    with the environment. The machine is provided with “rewards” for behaving in a
    desired manner, and “penalties” for behaving in an undesired manner. The machine
    attempts to maximize rewards by learning to develop its behavior accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL took the world by storm in 2012\. During that year, the ImageNet 2012 challenge
    was launched with the goal of predicting the content of photographs using a subset
    of a large hand-labeled dataset. A deep learning model named AlexNet achieved
    a top-5 error rate of 15.3%, a significant improvement with respect to previous
    state-of-the-art results. According to the Economist, *Suddenly people started
    to pay attention, not just within the AI community but across the technology industry
    as a whole*.
  prefs: []
  type: TYPE_NORMAL
- en: That was only the beginning. Today, DL techniques are successfully applied in
    heterogeneous domains including, but not limited to, healthcare, environment,
    green energy, computer vision, text analysis, multimedia, finance, retail, gaming,
    simulation, industry, robotics, and self-driving cars. In each of these domains,
    DL techniques can solve problems with a level of accuracy that was not possible
    using previous methods.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at the past eight years, it is fascinating and exciting to see
    the extent of the contributions that DL has made to science and industry. There
    is no reason to believe that the next eight years will see any less contribution;
    indeed, as the field of DL continues to advance, we anticipate that we’ll see
    even more exciting and fascinating contributions provided by DL.
  prefs: []
  type: TYPE_NORMAL
- en: This book introduces you to the magic of deep learning. We will start with simple
    models and progressively will introduce increasingly sophisticated models. The
    approach will always be hands-on, with a healthy dose of code to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are a data scientist with experience in ML or an AI programmer with some
    exposure to neural networks, you will find this book a useful entry point to DL
    with TensorFlow. If you are a software engineer with a growing interest in the
    DL tsunami, you will find this book a foundational platform to broaden your knowledge
    on the topic. Basic knowledge of Python is required for this book.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Chapter 1*, *Neural Network Foundations with TF*, is where we learn the basics
    of TensorFlow, an open-source library developed by Google for machine learning
    and deep learning. In addition, we introduce the basics of neural networks and
    deep learning, two areas of machine learning that had incredible growth during
    the last few years. The idea behind this chapter is to provide all the tools needed
    to do basic but fully hands-on deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 2*, *Regression and Classification*, focuses on the fundamental tasks
    in ML techniques: regression and classification. We will learn how to use TensorFlow
    to build simple, multiple, and multivariate regression models. We will use logistic
    regression to solve a multi-class classification problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 3*, *Convolutional Neural Networks*, covers how to use deep learning
    ConvNets for recognizing MNIST handwritten characters with high accuracy. We use
    the CIFAR 10 dataset to build a deep learning classifier with 10 categories, and
    the ImageNet dataset to build an accurate classifier with 1,000 categories. In
    addition, we investigate how to use large deep learning networks such as VGG16
    and very deep networks such as InceptionV3\. We will conclude with a discussion
    on transfer learning'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 4*, *Word Embeddings*, is where we describe the origins of and theory
    behind distributed representations and word embeddings and chart the progress
    of word embeddings from static word-based embeddings more dynamic and expressive
    embeddings based on sentences and paragraphs. We also explore how the idea of
    word embeddings can be extended to include non-word sequences as well, such as
    nodes in a graph or user sessions in a web application. The chapter also contains
    multiple examples of using word embeddings of various kinds.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 5*, *Recurrent Neural Networks*, describes an important architectural
    subclass of neural networks that are optimized for handling sequence data such
    as natural language or time series. We describe the important architectures in
    this genre, such as **LSTM** (**Long Short-Term Memory**) and **GRU** (**Gated
    Recurrent Unit**) and show how they can be extended to handle bidirectional states
    and states across batches. We also provide examples of using RNNs with various
    topologies for specific tasks, such as generating text, sentiment analysis, and
    part-of-speech tagging. We also describe the popular seq2seq architecture, which
    uses a pair of RNNs in an encoder-decoder pipeline to solve a variety of NLP tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 6*, *Transformers*, covers transformers, a deep learning architecture
    that has revolutionized the traditional natural language processing field. We
    start by reviewing the key intuitions behind the architecture and various categories
    of transformers, together with a deep dive into the most popular models. Then,
    we focus on implementations both based on the vanilla architecture and on popular
    libraries, such as Hugging Face and TensorFlow Hub. After that, we briefly discuss
    evaluation, optimization, and some of the best practices commonly adopted when
    using transformers. The last section is devoted to reviewing how transformers
    can be used to perform computer vision tasks, a totally different domain from
    NLP. That requires a careful definition of the attention mechanism. In the end,
    attention is all you need! And at the core of attention, there is nothing more
    than the cosine similarity between vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 7*, *Unsupervised Learning*, delves into unsupervised learning models.
    It will cover techniques required for clustering and dimensionality reduction
    like PCA, k-means, and self-organized maps. It will go into the details of Boltzmann
    machines and their implementation using TensorFlow. The concepts covered will
    be extended to build **Restricted Boltzmann Machines** (**RBMs**).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 8*, *Autoencoders*, describes autoencoders, a class of neural networks
    that attempt to recreate the input as its target. It will cover different varieties
    of autoencoders like sparse autoencoders, convolutional autoencoders, and denoising
    autoencoders. The chapter will train a denoising autoencoder to remove noise from
    input images. It will demonstrate how autoencoders can be used to create MNIST
    digits. It will also cover the steps involved in building an LSTM autoencoder
    to generate sentence vectors. Finally, we will learn how to build a variational
    autoencoder to generate images.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 9*, *Generative Models*, focuses on **Generative Adversarial Networks**
    (**GANs**). We start with the first proposed GAN model and use it to forge MNIST
    characters. The chapter shows you how to use deep convolutional GANs to create
    celebrity images. The chapter discusses the various GAN architectures like SRGAN,
    InfoGAN, and CycleGAN. The chapter covers a range of cool GAN applications. Finally,
    the chapter concludes with a TensorFlow implementation of CycleGAN to convert
    winter-summer images.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 10*, *Sel**f-Supervised Learning*, provides an overview of various
    strategies used for self-supervised learning in computer vision, audio, and natural
    language processing. It covers self-prediction through strategies such as autoregressive
    generation, masked generation, relationship prediction, and hybrids of these approaches.
    It also covers contrastive learning, a popular technique for self-supervised learning,
    and its application to various pretext tasks in various application domains.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 11*, *Reinforcement Learning*, focuses on reinforcement learning,
    covering the Q-learning algorithm and the Bellman equation. The chapter covers
    discounted rewards, exploration and exploitation, and discount factors. It explains
    policy-based and model-based reinforcement learning. We will build a **Deep Q-learning
    Network** (**DQN**) to play an Atari game. And finally, we learn how to train
    agents using the policy gradient algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 12*, *Probabilistic TensorFlow*, introduces TensorFlow Probability,
    the library built over TensorFlow to perform probabilistic reasoning and statistical
    analysis. The chapter demonstrates how to use TensorFlow Probability to generate
    synthetic data. We will build Bayesian networks and perform inference. The chapter
    also introduces the concept of uncertainty, aleatory and epistemic, and how to
    calculate the uncertainty of your trained models.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 13*, *An Introduction to AutoML*, introduces AutoML, whose goal is
    to enable domain experts who are unfamiliar with machine learning technologies
    to use ML techniques easily. We will go through a practical exercise using Google
    Cloud Platform and do quite a bit of hands-on work after briefly discussing the
    fundamentals. The chapter covers automatic data preparation, automatic feature
    engineering, and automatic model generation. Then, we introduce AutoKeras and
    Google Cloud AutoML with its multiple solutions for table, vision, text, translation,
    and video processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 14*, *The Math Behind Deep Learning*, covers the math behind deep
    learning. This topic is quite advanced and not necessarily required for practitioners.
    However, it is recommended reading to understand what is going on “under the hood”
    when we play with neural networks. We start with a historical introduction, and
    then we will review the high school concept of derivatives and gradients and introduce
    the gradient descent and backpropagation algorithms commonly used to optimize
    deep learning networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 15*, *Tensor Processing Unit*, discusses TPUs. TPUs are very special
    ASIC chips developed at Google for executing neural network mathematical operations
    in an ultra-fast manner. The core of the computation is a systolic multiplier
    that computes multiple dot products (row * column) in parallel, thus accelerating
    the computation of basic deep learning operations. Think of a TPU as a special-purpose
    co-processor for deep learning that is focused on matrix or tensor operations.
    We will review the four generations of TPUs so far, plus an additional Edge TPU
    for IoT.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 16*, *Other Useful Deep Learning Libraries*, introduces other deep
    learning frameworks. We will explore Hugging Face, OpenAI’s GPT3, and DALL-E 2\.
    The chapter introduces another very popular deep learning framework, PyTorch.
    We also cover H2O.ai and its AutoML module. The chapter also briefly discusses
    the ONNX open-source format for deep learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 17*, *Graph Neura**l Networks*, introduces graphs and graph machine
    learning, with particular emphasis on graph neural networks and the popular **Deep
    Graph Library** (**DGL**). We describe the theory behind various commonly used
    graph layers used in GNNs (and available in DGL) and provide examples of GNNs
    used for node classification, link prediction, and graph classification. We also
    show how to work with your own graph dataset and customize graph layers to create
    novel GNN architectures. We then cover more cutting-edge advances in the field
    of Graph ML, such as heterogeneous graphs and temporal graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 18*, *Machine Learning Best Practices*, focuses on strategies and
    practices to follow to get the best model in training and production. The chapter
    discusses the best practices from two different perspectives: the best practices
    for the data and the best practices with respect to models.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 19*, *TensorFlow 2 Ecosystem*, lays out the different components of
    the TensorFlow ecosystem. We introduce TensorFlow Hub, a repository for pretrained
    deep learning models. The chapter talks about TensorFlow Datasets – a collection
    of ready-to-use datasets. We will also talk about TensorFlow Lite and TensorFlow
    JS – the framework for mobile and embedded systems and the web. Finally, the chapter
    talks about federated learning, a decentralized machine learning framework.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 20*, *Advanced Convolutional Neural Networks*, shows more advanced
    uses for **convolutional neural networks** (**CNNs**). We will explore how CNNs
    can be applied within the areas of computer vision, video, textual documents,
    audio, and music. We’ll conclude with a section summarizing convolution operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code bundle for the book is hosted on GitHub at [https://packt.link/dltf](https://packt.link/dltf).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781803232911_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781803232911_ColorImages.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`CodeInText`: Indicates code words in the text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. For example: “Each neuron can be initialized with specific weights via
    the `''kernel_initializer''` parameter.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see on
    the screen. For instance, words in menus or dialog boxes appear in the text like
    this. For example: “A **Deep Convolutional Neural Network** (**DCNN**) consists
    of many neural network layers.”'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: Email `feedback@packtpub.com` and mention the book’s
    title in the subject of your message. If you have questions about any aspect of
    this book, please email us at `questions@packtpub.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you reported this to us. Please visit [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    click **Submit Errata**, and fill in the form.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Deep Learning with Keras: Implementing deep learning models and neural networks
    with the power of Python*, Paperback – 26 Apr 2017, Antonio Gulli, Sujit Pal'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*TensorFlow 1.x Deep Learning Cookbook*: *Over 90 unique recipes to solve artificial-intelligence
    driven problems with Python*, Antonio Gulli, Amita Kapoor'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Share your thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve read *Deep Learning with TensorFlow and Keras, Third Edition*, we’d
    love to hear your thoughts! Please [click here to go straight to the Amazon review
    page](https://packt.link/r/1803232919) for this book and share your feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
