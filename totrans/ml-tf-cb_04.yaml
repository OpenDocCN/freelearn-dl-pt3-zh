- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression may be one of the most important algorithms in statistics,
    machine learning, and science in general. It's one of the most widely used algorithms,
    and it is very important to understand how to implement it and its various flavors.
    One of the advantages that linear regression has over many other algorithms is
    that it is very interpretable. We end up with a number (a coefficient) for each
    feature and such a number directly represents how that feature influences the
    target (the so-called dependent variable).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you had to predict the selling value of a house and you obtained
    a dataset of historical sales comprising house characteristics (such as the lot
    size, indicators of the quality and condition of the house, and the distance from
    the city center), you could easily apply a linear regression. You could obtain
    a reliable estimator in a few steps and the resulting model would be easy to understand
    and explain to others, too. A linear regression, in fact, first estimates a baseline
    value, called the intercept, and then estimates a multiplicative coefficient for
    each feature. Each coefficient can transform each feature into a positive and
    negative part of the prediction. By summing the baseline and all the coefficient-transformed
    features, you get your final prediction. Therefore, in our house sale price prediction
    problem, you could get a positive coefficient for the lot size, implying that
    larger lots will sell for more, and a negative coefficient for the distance from
    the city center, an indicator that estates located in the outskirts have less
    market value.
  prefs: []
  type: TYPE_NORMAL
- en: Computing such kinds of models with TensorFlow is fast, suitable for big data,
    and much easier to put into production because it will be accessible to general
    interpretation by inspection of a weights vector.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to recipes explaining how linear regression
    is implemented in TensorFlow, via Estimators or Keras, and then move on to providing
    solutions that are even more practical. In fact, we will explain how to tweak
    it using different loss functions, how to regularize coefficients in order to
    achieve feature selection in your models, and how to use regression for classification,
    for non-linear problems, and when you have categorical variables with high-cardinality
    (high-cardinality means variables with many unique values).
  prefs: []
  type: TYPE_NORMAL
- en: Remember that all the code is available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover recipes involving linear regression. We start
    with the mathematical formulation for solving linear regression with matrices,
    before moving on to implementing standard linear regression and variants with
    the TensorFlow paradigm. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning the TensorFlow way of regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turning a Keras model into an Estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding loss functions in linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Lasso and Ridge regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resorting to non-linear solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Wide & Deep models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, you will find that creating linear models (and some
    non-linear ones, too) using TensorFlow is easy using the recipes provided.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the TensorFlow way of linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The statistical approach in linear regression, using matrices and decomposition
    methods on data, is very powerful. In any event TensorFlow has another means to
    solve for the coefficients of a slope and an intercept in a regression problem.
    TensorFlow can achieve a result in such problems iteratively, that is, gradually
    learning the best linear regression parameters that will minimize the loss, as
    we have seen in the recipes in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting fact is that you actually don''t have to write all the code
    from scratch when dealing with a regression problem in TensorFlow: Estimators
    and Keras can assist you in doing that. Estimators are to be found in `tf.estimator`,
    a high-level API in TensorFlow.'
  prefs: []
  type: TYPE_NORMAL
- en: Estimators were introduced in TensorFlow 1.3 (see [https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0-rc2](https://github.com/tensorflow/tensorflow/releases/tag/v1.3.0-rc2))
    as ''**canned Estimators''**, pre-made specific procedures (such as regression
    models or basic neural networks) created to simplify training, evaluation, predicting,
    and the exporting of models for serving. Using pre-made procedures aids development
    in an easier and more intuitive way, leaving the low-level API for customized
    or research solutions (for instance, when you want to test the solutions you found
    in a paper or when your problem requires a completely customized approach). Moreover,
    Estimators are easily deployed on CPUs, GPUs, or TPUs, as well as on a local host
    or on a distributed multi-server environment, without any further code changes
    on your model, making them suitable for ready-to-production use cases. That is
    the reason why Estimators are absolutely not going away anytime soon from TensorFlow,
    even if Keras, as presented in the previous chapter, is the main high-level API
    for TensorFlow 2.x. On the contrary, more and more support and development will
    be made to integrate between Keras and Estimators and you will soon realize in
    our recipes how easily you can turn Keras models into your own custom Estimators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Four steps are involved in developing an Estimator model:'
  prefs: []
  type: TYPE_NORMAL
- en: Acquire your data using `tf.data` functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate the feature column(s)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate and train the Estimator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model's performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our recipes, we will explore all four steps providing you with reusable solutions
    for each.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will loop through batches of data points and let TensorFlow
    update the slope and y intercept. Instead of generated data, we will use the Boston
    Housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Originating in the paper by Harrison, D. and Rubinfeld, D.L. *Hedonic Housing
    Prices and the Demand for Clean Air* (J. Environ. Economics & Management, vol.5,
    81-102, 1978), the Boston Housing dataset can be found in many analysis packages
    (such as in scikit-learn) and is present at the UCI Machine Learning Repository,
    as well as at the original StatLib archive (http://lib.stat.cmu.edu/datasets/boston).
    It is a classical dataset for regression problems, but not a trivial one. For
    instance, the samples are ordered and if you do not shuffle the examples randomly,
    you may produce ineffective and biased models when you make a train/test split.
  prefs: []
  type: TYPE_NORMAL
- en: Going into the details, the dataset is made up of 506 census tracts of Boston
    from the 1970 census and it features 21 variables regarding various aspects that
    could affect real estate value. The target variable is the median monetary value
    of the houses, expressed in thousands of USD. Among the available features, there
    are a number of obvious ones, such as the number of rooms, the age of the buildings,
    and the crime levels in the neighborhood, and some others that are a bit less
    obvious, such as the pollution concentration, the availability of nearby schools,
    the access to highways, and the distance from employment centers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to our solution, specifically, we will find an optimal of the features
    that will assist us in estimating the house prices in Boston. Before talking more
    about the effects of different loss functions on this problem in the next section,
    we are also going to show you how to create a regression Estimator in TensorFlow
    starting from Keras functions, which opens up important customizations for solving
    different problems.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the necessary libraries, and then load the data in-memory
    using pandas functions. We will also separate predictors from targets (the MEDV,
    median house values) and divide the data into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then declare two key functions for our recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`make_input_fn`, which is a function that creates a `tf.data` dataset from
    a pandas DataFrame turned into a Python dictionary of pandas Series (the features
    are the keys, the values are the feature vectors). It also provides batch size
    definition and random shuffling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`define_feature_columns`, which is a function that maps each column name to
    a specific `tf.feature_column` transformation. `tf.feature_column` is a TensorFlow
    module ([https://www.tensorflow.org/api_docs/python/tf/feature_column](https://www.tensorflow.org/api_docs/python/tf/feature_column))
    offering functions that can process any kind of data in a suitable way for being
    inputted into a neural network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `make_input_fn` function is used to instantiate two data functions, one
    for training (the data is shuffled, with a batch size of 256 and set to consume
    1,400 epochs), and one for test (set to a single epoch, no shuffling, so the ordering
    is the original one).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `define_feature_columns` function is used to map the numeric variables
    using the `numeric_column` function ([https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column))
    and the categorical ones using `categorical_column_with_vocabulary_list` ([https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list)).
    Both will signal to our Estimator how to handle such data in the optimal manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As a next step, we pass to instantiate the Estimator for a linear regression
    model. We will just recall the formula for the linear model, *y = aX +b*, which
    implies that there is a coefficient for the intercept value and then a coefficient
    for each feature or feature transformation (for instance, categorical data is
    one-hot encoded, so you have a single coefficient for each value of the variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we just have to train the model and evaluate its performance. The metric
    used is the root mean squared error (the less the better):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the reported results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here is a good place to note how to see whether the model is overfitting or
    underfitting the data. If our data is broken into test and training sets, and
    the performance is greater on the training set and lower on the test set, then
    we are overfitting the data. If the accuracy is still increasing on both the test
    and training sets, then the model is underfitting and we should continue training.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the training ended with an average loss of 25.0\. Our test average
    is instead 32.7, implying we have probably overfitted and we should reduce the
    training iterations.
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the performances of the Estimator as it trains the data and
    as it is compared to the test set results. This requires the use of TensorBoard
    ([https://www.tensorflow.org/tensorboard/](https://www.tensorflow.org/tensorboard/)),
    TensorFlow's visualization kit, which will be explained in more detail later in
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: In any event, you can just replicate the visualizations by using the `4\. Linear
    Regression with TensorBoard.ipynb` notebook instead of the `4\. Linear Regression.ipynb`
    version. Both can be found in the book's GitHub repository at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\01 Linear regression.png](img/B16254_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: TensorBoard visualization of the training loss of the regression
    Estimator'
  prefs: []
  type: TYPE_NORMAL
- en: The visualization shows that the Estimator fitted the problem quickly, reaching
    an optimal value after 1,000 observed batches. Afterward, it oscillated near the
    minimum loss value reached. The test performance, represented by a blue dot, is
    near the best reached value, thereby proving that the model is performing and
    stable even with unseen examples.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Estimator that calls the proper TensorFlow functionalities, sifts the data
    from the data functions, and converts the data into the proper form based on the
    matched feature name and `tf.feature_column` function does the entire job. All
    that remains is to check the fitting. Actually, the optimal line found by the
    Estimator is not guaranteed to be the line of best fit. Convergence to the line
    of best fit depends on the number of iterations, batch size, learning rate, and
    loss function. It is always good practice to observe the loss function over time
    as this can help you troubleshoot problems or hyperparameter changes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to increase the performance of your linear model, interactions
    could be the key. This means that you create a combination between two variables
    and that combination can explain the target better than the features taken singularly.
    In our Boston Housing dataset, combining the average room number in a house and
    the proportion of the lower income population in an area can reveal more about
    the type of neighborhood and help infer the housing value of the area. We combine
    the two just by pointing them out to the `tf.feature_column.crossed_column` function.The
    Estimator, also receiving this output among the features, will automatically create
    the interaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here is the plot of the training loss and the resulting test set result.
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\02 Linear regression.png](img/B16254_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: TensorBoard plot of the regression model with interactions'
  prefs: []
  type: TYPE_NORMAL
- en: Observe how the fitting is now faster and much more stable than before, indicating
    that we provided more informative features to the model (the interactions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful recipe function is suitable for handling predictions: the Estimator
    returns them as a dictionary. A simple function will convert everything into a
    more useful array of predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Having your predictions as an array will help you to reuse and export the results
    in a more convenient way than a dictionary could.
  prefs: []
  type: TYPE_NORMAL
- en: Turning a Keras model into an Estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to now, we have worked out our linear regression models using specific Estimators
    from the `tf.estimator` module. This has clear advantages because our model is
    mostly run automatically and we can easily deploy it in a scalable way on the
    cloud (such as Google Cloud Platform, offered by Google) and on different kinds
    of servers (CPU-, GPU-, and TPU-based). Anyway, by using Estimators, we may lack
    the flexibility in our model architecture as required by our data problem, which
    is instead offered by the Keras modular approach that we discussed in the previous
    chapter. In this recipe, we will remediate this by showing how we can transform
    Keras models into Estimators and thus take advantage of both the Estimators API
    and Keras versatility at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the same Boston Housing dataset as in the previous recipe, while
    also making use of the `make_input_fn` function. As before, we need our core packages
    to be imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will also need to import the Keras module from TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Importing `tf.keras` as `keras` will also allow you to easily reuse any previous
    script that you wrote using the standalone Keras package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first step will be to redefine the function creating the feature columns.
    In fact, now we have to specify an input to our Keras model, something that was
    not necessary with native Estimators since they just need a `tf.feature` function
    mapping the feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The same goes for interactions. Here, too, we need to define the input that
    will be used by our Keras model (in this case, one-hot encoding):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After preparing the necessary inputs, we can proceed to the model itself. The
    inputs will be collected in a feature layer that will pass the data to a `batchNormalization`
    layer, which will automatically standardize it. After that the data will be directed
    to the output node, which will produce the numeric output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, having set all the necessary inputs, new functions are created
    and we can run them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now obtained a working Keras model. We can convert it into an Estimator
    using the `model_to_estimator` function. This requires the establishment of a
    temporary directory for the Estimator''s outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Having `canned` the Keras model into an Estimator, we can proceed as before
    to train the model and evaluate the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When we plot the fitting process using TensorBoard, we will observehow the
    training trajectory is quite similar to the one obtained by previous Estimators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\03 canned linear regression.png](img/B16254_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Canned Keras linear Estimator training'
  prefs: []
  type: TYPE_NORMAL
- en: Canned Keras Estimators are indeed a quick and robust way to bind together the
    flexibility of user-defined solutions by Keras and the high-performance training
    and deployment from Estimators.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `model_to_estimator` function is not a wrapper of your Keras model. Instead,
    it parses your model and transforms it into a static TensorFlow graph, allowing
    distributed training and scaling for your model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One great advantage of using linear models is to be able to explore their weights
    and get an idea of what feature is producing the result we obtained. Each coefficient
    will tell us, given the fact that the inputs are standardized by the batch layer,
    how that feature is impacted with respect to the others (the coefficients are
    comparable in terms of absolute value) and whether it is adding or subtracting
    from the result (given a positive or negative sign):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Anyway, if we extract the weights from our model we will find out that we cannot
    easily interpret them because they have no labels and the dimensionality is different
    since the `tf.feature` functions have applied different transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a function that can extract the correct labels from our feature columns
    as we mapped them prior to feeding them to our canned Estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This function only works with TensorFlow version 2.2 or later because in earlier
    TensorFlow 2.x versions the `get_config` method was not present in `tf.feature`
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can extract all the labels and meaningfully match each weight in the
    output to its respective feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the weights, you can easily get the contribution of each feature
    to the result by observing the sign and the magnitude of each coefficient. The
    scale of the feature can, however, influence the magnitude unless you previously
    statistically standardized the features by subtracting the mean and dividing by
    the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding loss functions in linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to know the effect of loss functions in algorithm convergence.
    Here, we will illustrate how the L1 and L2 loss functions affect convergence and
    predictions in linear regression. This is the first customization that we are
    applying to our canned Keras Estimator. More recipes in this chapter will enhance
    that initial Estimator by adding more functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the same Boston Housing dataset as in the previous recipe, as well
    as utilize the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: However, we will change our loss functions and learning rates to see how convergence
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The start of the program is the same as the last recipe. We therefore load
    the necessary packages and also we download the Boston Housing dataset, if it
    is not already available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we need to redefine our `create_linreg` by adding a new parameter
    controlling the type of loss. The default is still the mean squared error (L2
    loss), but now it can be easily changed when instantiating the canned Estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After doing so, we can train our model explicitly using the `Ftrl` optimizer
    with a different learning rate, more suitable for an L1 loss (we set the loss
    to mean absolute error):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the results that we obtained by switching to an L1 loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now visualize the training performances along iterations using TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\04 understanding loss.png](img/B16254_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Mean absolute error optimization'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting plot shows a nice descent of the mean absolute error, which simply
    slows down after 400 iterations and tends to stabilize in a plateau after 1,400
    iterations.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When choosing a loss function, we must also choose a corresponding learning
    rate that will work with our problem. Here, we test two situations, the first
    in which L2 is adopted and the second in which L1 is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: If our learning rate is small, our convergence will take more time. However,
    if our learning rate is too large, we will have issues with our algorithm never
    converging.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand what is happening, we should look at how a large learning rate
    and small learning rate act on **L1 norms** and **L2 norms**. If the rate is too
    large, L1 can get stuck at a suboptimal result, whereas L2 can achieve an even
    worse performance. To visualize this, we will look at a one-dimensional representation
    of learning steps on both norms, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: What can happen with the L1 and L2 norm with larger and smaller
    learning rates'
  prefs: []
  type: TYPE_NORMAL
- en: Small learning rates, as depicted in the preceding diagram, are indeed a guarantee
    of a better optimization in any case. Larger rates do not really work with L2,
    but may prove just suboptimal with L1 by stopping further optimizations after
    a while, without causing any further damage.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Lasso and Ridge regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are ways to limit the influence of coefficients on the regression output.
    These methods are called regularization methods, and two of the most common regularization
    methods are Lasso and Ridge regression. We cover how to implement both of these
    in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lasso and Ridge regression are very similar to regular linear regression, except
    that we add regularization terms to limit the slopes (or partial slopes) in the
    formula. There may be multiple reasons for this, but a common one is that we wish
    to restrict the number of features that have an impact on the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the Boston Housing dataset again and set up our functions in the
    same way as in the previous recipes. In particular we need `define_feature_columns_layers`,
    `make_input_fn`, and `create_interactions`. We again first load the libraries,
    and then we define a new `create_ridge_linreg` where we set a new Keras model
    using `keras.regularizers.l2` as the `regularizer` of our dense layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, we can again run our previous linear model with L1 loss
    and see the results improve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the Ridge regression results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, here is the plot of the training using TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\05 ridge.png](img/B16254_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Ridge regression training loss'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also replicate that for L1 regularization by creating a new function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the results obtained from the L1 Lasso regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, here is the plot of the training loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\06 Lasso.png](img/B16254_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Lasso regression training loss'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the Ridge and Lasso approach, we notice that they are not too dissimilar
    in terms of training loss, but the test result favors Lasso. This could be explained
    by a noisy variable that had to be excluded in order for the model to improve,
    since Lasso routinely excludes non-useful variables from the prediction estimation
    (by assigning a zero coefficient to them), whereas Ridge just down-weights them.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We implement Lasso regression by adding a continuous Heaviside step function
    to the loss function of linear regression. Owing to the steepness of the step
    function, we have to be careful with step size. Too big a step size and it will
    not converge. For Ridge regression, see the change required in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Elastic net regression is a type of regression that combines Lasso regression
    with Ridge regression by adding L1 and L2 regularization terms to the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing elastic net regression is straightforward following the previous
    two recipes, because you just need to change the regularizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We just create a `create_elasticnet_linreg` function, which picks up as parameters
    the values of L1 and L2 strengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we re-run the complete steps for training from data and obtain an
    evaluation of the model''s performances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the training loss plot for the ElasticNet model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\07 ElasticNet.png](img/B16254_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: ElasticNet training loss'
  prefs: []
  type: TYPE_NORMAL
- en: The test results obtained do not differ too much from Ridge and Lasso, landing
    somewhere between them. As stated previously, the problem involves removing variables
    from the dataset in order to improve the performances, and as we've now seen the
    Lasso model is the best for doing so.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, we will implement logistic regression to predict the probability
    of breast cancer using the Breast Cancer Wisconsin dataset ([https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))).
    We will be predicting the diagnosis from features that are computed from a digitized
    image of a **fine needle aspiration** (**FNA**) of a breast mass. An FNA is a
    common breast cancer test, consisting of a small tissue biopsy that can be examined
    under a microscope.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can immediately be used for a classification model, without further
    transformations, since the target variable consists of 357 benign cases and 212
    malignant ones. The two classes do not have the exact same consistency (an important
    requirement when doing binary classification with regression models), but they
    are not extremely different, allowing us to build a straightforward example and
    evaluate it using plain accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please remember to check whether your classes are balanced (in other words,
    having approximately the same number of cases), otherwise you will have to apply
    specific recipes to balance the cases, such as applying weights, or your model
    may provide inaccurate predictions (you can refer to the following Stack Overflow
    question if you just need further details: [https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras](https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras)).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is a way to turn linear regression into a binary classification.
    This is accomplished by transforming the linear output into a sigmoid function
    that scales the output between zero and one. The target is a zero or one, which
    indicates whether a data point is in one class or another. Since we are predicting
    a number between zero and one, the prediction is classified into class value 1
    if the prediction is above a specified cut-off value, and class 0 otherwise. For
    the purpose of this example, we will specify that cutoff to be 0.5, which will
    make the classification as simple as rounding the output.
  prefs: []
  type: TYPE_NORMAL
- en: When classifying, anyway, sometimes you need to control the kinds of mistakes
    you make, and this is especially true for medical applications (such as the example
    we are proposing), but it may be a sensible problem for other ones, too (for example,
    in the case of fraud detection in the insurance or banking sectors). In fact,
    when you classify, you get correct guesses, but also **false positives** and **false
    negatives**. False positives are the errors the model makes when it predicts a
    positive (class 1), but the true label is negative. False negatives, on the other
    hand, are cases labeled by the model as negative when they are actually positive.
  prefs: []
  type: TYPE_NORMAL
- en: When using a 0.5 threshold for deciding the class (positive or negative class),
    you are actually equating the expectations for false positives and false negatives.
    In reality, according to your problem, false positive and false negative errors
    may have different consequences. In the case of detecting cancer, clearly you
    absolutely do not want false negatives because that would mean predicting a patient
    as healthy when they are instead facing a life-threatening situation.
  prefs: []
  type: TYPE_NORMAL
- en: By setting the classification threshold higher or lower, you can trade-off false
    positives for false negatives. Higher thresholds will have more false negatives
    than false positives. Lower ones will have fewer false negatives but more false
    positives. For our recipe, we will just use the 0.5 threshold, but please be aware
    that the threshold is also something you have to consider for your model's real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the libraries and recovering the data from the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we specify the logistic regression function. The main modification with
    respect to our linear regression model is that we change the activation in the
    single output neuron from `linear` to `sigmoid`, which is enough to obtain a logistic
    regression because our output will be a probability expressed in the range 0.0
    to 1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we run our procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the reported accuracy of our logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, here you can find the loss plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: TensorBoard plot of training loss for a logistic regression model'
  prefs: []
  type: TYPE_NORMAL
- en: Using a few commands, we achieved a good result in terms of accuracy and loss
    for this problem, in spite of a slightly unbalanced target class (more benign
    cases than malignant ones).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression predictions are based on the sigmoid curve and, to modify
    our previous linear model accordingly, we just need to switch to a sigmoid activation.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you are predicting a multi-class or multi-label you don't need to extend
    the binary model using different kinds of **One Versus All** (**OVA**) strategies,
    but you just need to extend the number of output nodes to match the number of
    classes you need to predict. Using multiple neurons with sigmoid activation, you
    will obtain a multi-label approach, while using a softmax activation, you'll get
    a multi-class prediction. You will find more recipes in the later chapters of
    this book that indicate how to do this using simple Keras functions.
  prefs: []
  type: TYPE_NORMAL
- en: Resorting to non-linear solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear models are approachable and interpretable, given the one-to-one relation
    between feature columns and regression coefficients. Sometimes, anyway, you may
    want to try non-linear solutions in order to check whether models that are more
    complex can model your data better and solve your prediction problem in a more
    expert manner. **Support Vector Machines** (**SVMs**) are an algorithm that rivaled
    neural networks for a long time and they are still a viable option thanks to recent
    developments in terms of random features for large-scale kernel machines (Rahimi,
    Ali; Recht, Benjamin. Random features for large-scale kernel machines. In: *Advances
    in neural information processing systems*. 2008\. pp. 1177-1184). In this recipe,
    we demonstrate how to leverage Keras and obtain a non-linear solution to a classification
    problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will still be using functions from the previous recipes, including `define_feature_columns_layers`
    and `make_input_fn`. As in the logistic regression recipe, we will continue using
    the breast cancer dataset. As before, we need to load the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: At this point we are ready to proceed with the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the previous packages, we also specifically import the `RandomFourierFeatures`
    function, which can apply a non-linear transformation to the input. Depending
    on the loss function, a `RandomFourierFeatures` layer can approximate kernel-based
    classifiers and regressors. After this, we just need to apply our usual single-output
    node and get our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the TensorFlow 2.x version you are using you may need to import
    it from different modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we develop the `create_svc` function. It contains an L2 regularizer for
    the final dense node, a batch normalization layer for the input, and a `RandomFourierFeatures`
    layer inserted among them. In this intermediate layer non-linearities are generated
    and you can set the `output_dim` parameter in order to determine the number of
    non-linear interactions that will be produced by the layers. Naturally, you can
    contrast the overfitting caused after setting higher `output_dim` values by raising
    the L2 regularization value, thereby achieving more regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous recipes, we define the different columns, we set the model
    and the optimizer, we prepare the input function, and finally we train and evaluate
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the reported accuracy. For an even better result, you have to try different
    combinations of the output dimension of the `RandomFourierFeatures` layer and
    the regularization term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the loss plot from TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Loss plot for the RandomFourierFeatures-based model'
  prefs: []
  type: TYPE_NORMAL
- en: The plot is indeed quite nice, thanks to the fact that we used a larger batch
    than usual. Given the complexity of the task, due to the large number of neurons
    to be trained, a larger batch generally works better than a smaller one.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Random Fourier features are a way to approximate the work done by SVM kernels,
    thereby achieving a lower computational complexity and making such an approach
    also feasible for a neural network implementation. If you require a more in-depth
    explanation, you can read the original paper, quoted at the beginning of the recipe,
    or you can take advantage of this very clear answer on Stack Exchange: [https://stats.stackexchange.com/questions/327646/how-does-a-random-kitchen-sink-work#327961](https://stats.stackexchange.com/questions/327646/how-does-a-random-kitchen-sink-work#327961).'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on the loss function, you can obtain different non-linear models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hinge loss** sets your model in an SVM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic loss** turns your model into kernel logistic regression (classification
    performance is almost the same as SVM, but kernel logistic regression can provide
    class probabilities)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean squared error** transforms your model into a kernel regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is up to you to decide what loss to try first, and decide how to set the
    dimension of the output from the random Fourier transformation. By way of a general
    suggestion you could start with a large number of output nodes and iteratively
    test whether shrinking their number improves the result.
  prefs: []
  type: TYPE_NORMAL
- en: Using Wide & Deep models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear models can boast a great advantage over complex models: they are efficient
    and easily interpretable, even when you work with many features and with features
    that interact with each other. Google researchers mentioned this aspect as the
    power of **memorization** because your linear model records the association between
    the features and the target into single coefficients. On the other hand, neural
    networks are blessed with the power of **generalization**, because in their complexity
    (they use multiple layers of weights and they interrelate each input), they can
    manage to approximate the general rules that govern the outcome of a process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wide & Deep models, as conceived by Google researchers ([https://arxiv.org/abs/1606.07792](https://arxiv.org/abs/1606.07792)),
    can blend memorization and generalization because they combine a linear model,
    applied to numeric features, together with generalization, applied to sparse features,
    such as categories encoded into a sparse matrix. Therefore, **wide** in their
    name implies the regression part, and **deep** the neural network aspect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://1.bp.blogspot.com/-Dw1mB9am1l8/V3MgtOzp3uI/AAAAAAAABGs/mP-3nZQCjWwdk6qCa5WraSpK8A7rSPj3ACLcB/s640/image04.png](img/B16254_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: How wide models (linear models) blend with neural networks in
    Wide & Deep models (from the paper by Cheng, Heng-Tze, et al. "Wide & deep learning
    for recommender systems." Proceedings of the 1st workshop on deep learning for
    recommender systems. 2016)'
  prefs: []
  type: TYPE_NORMAL
- en: Such a blend can achieve the best results when working on recommender system
    problems (such as the one featured in Google Play). Wide & Deep models work the
    best in recommendation problems because each part handles the right kind of data.
    The **wide** part handles the features relative to the user's characteristics
    (dense numeric features, binary indicators, or their combination in interaction
    features) that are more stable over time, whereas the **deep** part processes
    feature strings representing previous software downloads (sparse inputs on very
    large matrices), which instead are more variable over time and so require a more
    sophisticated kind of representation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Actually, Wide & Deep models also work fine with many other data problems, recommender
    systems being their speciality, and such models are readily available among Estimators
    (see [https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedEstimator](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedEstimator)).
    In this recipe we will use a mixed data dataset, the **Adult dataset** ([https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult)).
    Also widely known as the **Census dataset**, the purpose of this dataset is to
    predict whether your income exceeds $50K/annum based on census data. The available
    features are quite varied, from continuous values related to age to variables
    with a large number of classes, including occupation. We will then use each different
    type of feature to feed the correct part of the Wide & Deep model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by downloading the Adult dataset from the UCI archive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we select a subset of features for our purposes and we extract the target
    variable and transform it from the string type to the int type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset requires additional manipulation since some fields present missing
    values. We treat them by replacing missing values with a mean value. As a general
    rule, we have to impute all missing data before feeding it into a TensorFlow model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can proceed to define columns by means of the proper `tf.feature_column`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numeric columns**: dealing with numeric values (such as the age)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical columns**: dealing with categorical values when the unique categories
    are just a few in number (such as the gender)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embeddings**: dealing with categorical values when unique categories are
    many in number by mapping categorical values into a dense, low-dimensional, numeric
    space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also define the function that faciliates the interaction of categorical
    and numeric columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that all the functions have been defined, we map the different columns
    and add some meaningful interaction (such as crossing education with occupation).
    We map high-dimensional categorical features into a fixed lower-dimensional numeric
    space of 32 dimensions by setting the dimension parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Having mapped the features, we then input them into our estimator (see [https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier)),
    specifying the feature columns to be handled by the wide part and those by the
    deep part. For each part we also specify an optimizer (usually Ftrl for the linear
    part and Adam for the deep part) and, for the deep part, we specify the architecture
    of hidden layers as a list of numbers of neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed to define the input function (no different to what we have
    done in the other recipes presented in this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we train the Estimator for 1,500 steps and evaluate the results on
    the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain an accuracy of about 0.83 on our test set, as reported using the
    evaluate method on the Estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot of the training loss and the test estimate (the blue dot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\Luca\Dropbox\Packt\Packt - TensorFlow Machine Learning Cookbook\new
    chapters\images\10 wide and deep.png](img/B16254_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Training loss and test estimate for the Wide & Deep model'
  prefs: []
  type: TYPE_NORMAL
- en: 'For full prediction probabilities, we just extract them from the dictionary
    data type used by the Estimator. The `predict_proba` function will return a NumPy
    array with the probabilities for the positive (income in excess of USD 50K) and
    negative classes in distinct columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Wide & Deep models represent a way to handle linear models together with a more
    complex approach involving neural networks. As for other Estimators, this Estimator
    is also quite straightforward and easy to use. The keys for the success of the
    recipe in terms of other applications definitely rest upon defining an input data
    function and mapping the features with the more suitable functions from `tf.features_columns`.
  prefs: []
  type: TYPE_NORMAL
