["```\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gym\nimport multiprocessing\nimport threading\nimport numpy as np\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \n```", "```\nenv = gym.make('MountainCarContinuous-v0') \n```", "```\nstate_shape = env.observation_space.shape[0] \n```", "```\naction_shape = env.action_space.shape[0] \n```", "```\naction_bound = [env.action_space.low, env.action_space.high] \n```", "```\nnum_workers = multiprocessing.cpu_count() \n```", "```\nnum_episodes = 2000 \n```", "```\nnum_timesteps = 200 \n```", "```\nglobal_net_scope = 'Global_Net' \n```", "```\nupdate_global = 10 \n```", "```\ngamma = 0.90 \n```", "```\nbeta = 0.01 \n```", "```\nlog_dir = 'logs' \n```", "```\nclass ActorCritic(object): \n```", "```\n def __init__(self, scope, sess, globalAC=None): \n```", "```\n self.sess=sess \n```", "```\n self.actor_optimizer = tf.train.RMSPropOptimizer(0.0001, name='RMSPropA') \n```", "```\n self.critic_optimizer = tf.train.RMSPropOptimizer(0.001, name='RMSPropC') \n```", "```\n if scope == global_net_scope:\n            with tf.variable_scope(scope): \n```", "```\n self.state = tf.placeholder(tf.float32, [None, state_shape], 'state') \n```", "```\n self.actor_params, self.critic_params = self.build_network(scope)[-2:] \n```", "```\n else:\n            with tf.variable_scope(scope): \n```", "```\n self.state = tf.placeholder(tf.float32, [None, state_shape], 'state') \n```", "```\n self.action_dist = tf.placeholder(tf.float32, [None, action_shape], 'action') \n```", "```\n self.target_value = tf.placeholder(tf.float32, [None, 1], 'Vtarget') \n```", "```\n mean, variance, self.value, self.actor_params, self.critic_params = self.build_network(scope) \n```", "```\n td_error = tf.subtract(self.target_value, self.value, name='TD_error') \n```", "```\n with tf.name_scope('critic_loss'):\n                    self.critic_loss = tf.reduce_mean(tf.square(td_error)) \n```", "```\n normal_dist = tf.distributions.Normal(mean, variance) \n```", "```\n with tf.name_scope('actor_loss'): \n```", "```\n log_prob = normal_dist.log_prob(self.action_dist) \n```", "```\n entropy_pi = normal_dist.entropy() \n```", "```\n self.loss = log_prob * td_error + (beta * entropy_pi)\n                    self.actor_loss = tf.reduce_mean(-self.loss) \n```", "```\n with tf.name_scope('select_action'):\n                    self.action = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0), action_bound[0], action_bound[1]) \n```", "```\n with tf.name_scope('local_grad'):\n                    self.actor_grads = tf.gradients(self.actor_loss, self.actor_params)\n                    self.critic_grads = tf.gradients(self.critic_loss, self.critic_params) \n```", "```\n with tf.name_scope('sync'): \n```", "```\n with tf.name_scope('push'):\n                    self.update_actor_params = self.actor_optimizer.apply_gradients(zip(self.actor_grads, globalAC.actor_params))\n                    self.update_critic_params = self.critic_optimizer.apply_gradients(zip(self.critic_grads, globalAC.critic_params)) \n```", "```\n with tf.name_scope('pull'):\n                    self.pull_actor_params = [l_p.assign(g_p) for l_p, g_p in zip(self.actor_params, globalAC.actor_params)]\n                    self.pull_critic_params = [l_p.assign(g_p) for l_p, g_p in zip(self.critic_params, globalAC.critic_params)] \n```", "```\n def build_network(self, scope): \n```", "```\n w_init = tf.random_normal_initializer(0., .1) \n```", "```\n with tf.variable_scope('actor'):\n            l_a = tf.layers.dense(self.state, 200, tf.nn.relu, kernel_initializer=w_init, name='la')\n            mean = tf.layers.dense(l_a, action_shape, tf.nn.tanh,kernel_initializer=w_init, name='mean')\n            variance = tf.layers.dense(l_a, action_shape, tf.nn.softplus, kernel_initializer=w_init, name='variance') \n```", "```\n with tf.variable_scope('critic'):\n            l_c = tf.layers.dense(self.state, 100, tf.nn.relu, kernel_initializer=w_init, name='lc')\n            value = tf.layers.dense(l_c, 1, kernel_initializer=w_init, name='value') \n```", "```\n actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n        critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic') \n```", "```\n return mean, variance, value, actor_params, critic_params \n```", "```\n def update_global(self, feed_dict):\n        self.sess.run([self.update_actor_params, self.update_critic_params], feed_dict) \n```", "```\n def pull_from_global(self):\n        self.sess.run([self.pull_actor_params, self.pull_critic_params]) \n```", "```\n def select_action(self, state):\n\n        state = state[np.newaxis, :]\n\n        return self.sess.run(self.action, {self.state: state})[0] \n```", "```\nclass Worker(object): \n```", "```\n def __init__(self, name, globalAC, sess): \n```", "```\n self.env = gym.make('MountainCarContinuous-v0').unwrapped \n```", "```\n self.name = name \n```", "```\n self.AC = ActorCritic(name, sess, globalAC) \n```", "```\n self.sess=sess \n```", "```\n def work(self):\n        global global_rewards, global_episodes \n```", "```\n total_step = 1 \n```", "```\n batch_states, batch_actions, batch_rewards = [], [], [] \n```", "```\n while not coord.should_stop() and global_episodes < num_episodes: \n```", "```\n state = self.env.reset() \n```", "```\n Return = 0 \n```", "```\n for t in range(num_timesteps): \n```", "```\n if self.name == 'W_0':\n                    self.env.render() \n```", "```\n action = self.AC.select_action(state) \n```", "```\n next_state, reward, done, _ = self.env.step(action) \n```", "```\n done = True if t == num_timesteps - 1 else False \n```", "```\n Return += reward \n```", "```\n batch_states.append(state)\n                batch_actions.append(action)\n                batch_rewards.append((reward+8)/8) \n```", "```\n if total_step % update_global == 0 or done:\n                    if done:\n                        v_s_ = 0\n                    else:\n                        v_s_ = self.sess.run(self.AC.value, {self.AC.state: next_state[np.newaxis, :]})[0, 0] \n```", "```\n batch_target_value = []\n                    for reward in batch_rewards[::-1]:\n                        v_s_ = reward + gamma * v_s_\n                        batch_target_value.append(v_s_) \n```", "```\n batch_target_value.reverse() \n```", "```\n batch_states, batch_actions, batch_target_value = np.vstack(batch_states), np.vstack(batch_actions), np.vstack(batch_target_value) \n```", "```\n feed_dict = {\n                                 self.AC.state: batch_states,\n                                 self.AC. action_dist: batch_actions,\n                                 self.AC.target_value: batch_target_value,\n                                 } \n```", "```\n self.AC.update_global(feed_dict) \n```", "```\n batch_states, batch_actions, batch_rewards = [], [], [] \n```", "```\n self.AC.pull_from_global() \n```", "```\n state = next_state\n                total_step += 1 \n```", "```\n if done:\n                    if len(global_rewards) < 5:\n                        global_rewards.append(Return)\n                    else:\n                        global_rewards.append(Return)\n                        global_rewards[-1] =(np.mean(global_rewards[-5:]))\n\n                    global_episodes += 1\n                    break \n```", "```\nglobal_rewards = []\nglobal_episodes = 0 \n```", "```\nsess = tf.Session() \n```", "```\nwith tf.device(\"/cpu:0\"):\n\n    global_agent = ActorCritic(global_net_scope,sess) \n```", "```\n worker_agents = []\n    for i in range(num_workers):\n        i_name = 'W_%i' % i\n        worker_agents.append(Worker(i_name, global_agent,sess)) \n```", "```\ncoord = tf.train.Coordinator() \n```", "```\nsess.run(tf.global_variables_initializer()) \n```", "```\nif os.path.exists(log_dir):\n    shutil.rmtree(log_dir)\ntf.summary.FileWriter(log_dir, sess.graph) \n```", "```\nworker_threads = []\nfor worker in worker_agents:\n    job = lambda: worker.work()\n    t = threading.Thread(target=job)\n    t.start()\n    worker_threads.append(t)\ncoord.join(worker_threads) \n```"]