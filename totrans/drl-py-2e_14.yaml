- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Distributional Reinforcement Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式强化学习
- en: In this chapter, we will learn about distributional reinforcement learning.
    We will begin the chapter by understanding what exactly distributional reinforcement
    learning is and why it is useful. Next, we will learn about one of the most popular
    distributional reinforcement learning algorithms called **categorical DQN**. We
    will understand what a categorical DQN is and how it differs from the DQN we learned
    in *Chapter 9*, *Deep Q Networks and Its Variants*, and then we will explore the
    categorical DQN algorithm in detail.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习分布式强化学习。我们将通过理解分布式强化学习究竟是什么以及它为何有用来开始本章。接下来，我们将学习一种非常流行的分布式强化学习算法，叫做
    **分类 DQN**。我们将了解分类 DQN 是什么，它与我们在 *第 9 章*，*深度 Q 网络及其变体* 中学习的 DQN 有什么不同，然后我们将详细探讨分类
    DQN 算法。
- en: Following this, we will learn another interesting algorithm called **Quantile
    Regression DQN** (**QR-DQN**). We will understand what a QR-DQN is and how it
    differs from a categorical DQN, and then we will explore the QR-DQN algorithm
    in detail.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习另一个有趣的算法，叫做 **分位数回归 DQN**（**QR-DQN**）。我们将了解 QR-DQN 是什么，它与分类 DQN 有什么不同，然后我们将详细探讨
    QR-DQN 算法。
- en: At the end of the chapter, we will learn about the policy gradient algorithm
    called the **Distributed Distributional Deep Deterministic Policy Gradient** (**D4PG**).
    We will learn what the D4PG is and how it differs from the DDPG we covered in *Chapter
    12*, *Learning DDPG, TD3, and SAC*, in detail
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将学习一种名为 **分布式分布式深度确定性策略梯度**（**D4PG**）的策略梯度算法。我们将详细了解 D4PG 是什么，以及它与我们在
    *第 12 章*，*学习 DDPG、TD3 和 SAC* 中学习的 DDPG 有什么不同。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Why distributional reinforcement learning?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么使用分布式强化学习？
- en: Categorical DQN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类 DQN
- en: Quantile regression DQN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分位数回归 DQN
- en: Distributed distributional deep deterministic policy gradient
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式分布式深度确定性策略梯度
- en: Let's begin the chapter by understanding what distributional reinforcement learning
    is and why we need it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过理解什么是分布式强化学习以及我们为什么需要它来开始本章。
- en: Why distributional reinforcement learning?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用分布式强化学习？
- en: Say we are in state *s* and we have two possible actions to perform in this
    state. Let the actions be *up* and *down*. How do we decide which action to perform
    in the state? We compute Q values for all actions in the state and select the
    action that has the maximum Q value. So, we compute *Q*(*s*, up) and *Q*(*s*,
    down) and select the action that has the maximum Q value.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于状态 *s*，并且在这个状态下有两种可能的动作可以执行。设这两种动作分别是 *上* 和 *下*。我们如何决定在这个状态下执行哪个动作呢？我们为该状态下的所有动作计算
    Q 值，并选择 Q 值最大的动作。因此，我们计算 *Q*(*s*, 上) 和 *Q*(*s*, 下)，并选择 Q 值最大的动作。
- en: 'We learned that the Q value is the expected return an agent would obtain when
    starting from state *s* and performing an action *a* following the policy ![](img/B15558_14_001.png):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，Q 值是一个智能体从状态 *s* 开始并执行一个动作 *a* 后，根据策略获得的预期回报！[](img/B15558_14_001.png)：
- en: '![](img/B15558_14_002.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_002.png)'
- en: But there is a small problem in computing the Q value in this manner because
    the Q value is just an expectation of the return, and the expectation does not
    include the intrinsic randomness. Let's understand exactly what this means with
    an example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但使用这种方法计算 Q 值时有一个小问题，因为 Q 值仅仅是回报的期望值，而期望值并未包含内在的随机性。让我们通过一个例子来准确理解这意味着什么。
- en: Let's suppose we want to drive from work to home and we have two routes **A**
    and **B**. Now, we have to decide which route is better, that is, which route
    helps us to reach home in the minimum amount of time. To find out which route
    is better, we can calculate the Q values and select the route that has the maximum
    Q value, that is, the route that gives us the maximum expected return.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要从工作地点开车回家，我们有两条路线 **A** 和 **B**。现在，我们需要决定哪条路线更好，也就是说，哪条路线帮助我们在最短的时间内到达家里。为了找出哪条路线更好，我们可以计算
    Q 值，并选择 Q 值最大的路线，也就是给我们最大期望回报的路线。
- en: Say the Q value of choosing route *A* is *Q*(*s*, *A*) = 31, and the Q value
    of choosing route *B* is *Q*(*s*, *B*) = 28\. Since the Q value (the expected
    return of route **A**) is higher, we can choose route **A** to travel home. But
    are we missing something here? Instead of viewing the Q value as an expectation
    over a return, can we directly look into the distribution of return and make a
    better decision?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设选择路线*Ａ*的Q值为*Q*(*s*, *A*) = 31，选择路线*B*的Q值为*Q*(*s*, *B*) = 28。由于Q值（路线**A**的期望回报）较高，我们可以选择路线**A**回家。但这里是不是遗漏了什么呢？我们能不能直接看回报的分布，而不是把Q值看作回报的期望，从而做出更好的决策？
- en: Yes!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！
- en: 'But first, let''s take a look at the distribution of route **A** and route
    **B** and understand which route is best. The following plot shows the distribution
    of route **A**. It tells us with 70% probability we reach home in 10 minutes,
    and with 30% probability we reach home in 80 minutes. That is, if we choose route
    **A** we usually reach home in 10 minutes but when there is heavy traffic we reach
    home in 80 minutes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们看看路线**A**和路线**B**的分布，并理解哪条路线更优。以下图表显示了路线**A**的分布。它告诉我们，有70%的概率在10分钟内到家，而有30%的概率需要80分钟才能到家。也就是说，如果选择路线**A**，我们通常能在10分钟内到家，但当遇到交通拥堵时，我们需要80分钟才能到家：
- en: '![](img/B15558_14_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_01.png)'
- en: 'Figure 14.1: Distribution of route A'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：路线A的分布
- en: '*Figure 14.2* shows the distribution of route **B**. It tells us that with
    80% probability we reach home in 20 minutes and with 20% probability we reach
    home in 60 minutes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.2*显示了路线**B**的分布。它告诉我们，有80%的概率在20分钟内到家，20%的概率需要60分钟才能到家。'
- en: 'That is, if we choose route **B** we usually reach home in 20 minutes but when
    there is heavy traffic we reach home in 60 minutes:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果我们选择路线**B**，通常在20分钟内到家，但当遇到交通拥堵时，我们需要60分钟才能到家：
- en: '![](img/B15558_14_02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_02.png)'
- en: 'Figure 14.2: Distribution of route B'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：路线B的分布
- en: After looking at these two distributions, it makes more sense to choose route
    **B** instead of choosing route **A**. With route **B**, even in the worst case,
    that is, even when there is heavy traffic, we can reach home in 60 minutes. But
    with route **A**, when there is heavy traffic, we reach home in 80 minutes. So,
    it is a wise decision to choose route **B** rather than **A**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 看过这两种分布后，我们会发现选择路线**B**比选择路线**A**更有意义。选择路线**B**时，即使在最糟糕的情况下，也就是遇到交通拥堵时，我们能在60分钟内到家。但选择路线**A**时，在交通繁忙时，我们需要80分钟才能到家。因此，选择路线**B**而不是**A**是明智的决定。
- en: Similarly, if we can observe the distribution of return of route **A** and route
    **B**, we can understand more information and we will miss out on these details
    when we take actions just based on the maximum expected return, that is, the maximum
    Q value. So, instead of using the expected return to select an action, we use
    the distribution of return and then select optimal action based on the distribution.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果我们观察路线**A**和路线**B**的回报分布，我们就能获得更多信息。如果我们仅仅根据最大期望回报（即最大Q值）来采取行动，那么这些细节将会被忽略。因此，我们不是用期望回报来选择行动，而是使用回报的分布，然后根据分布选择最优的行动。
- en: This is the basic idea and motivation behind distributional reinforcement learning.
    In the next section, we will learn one of the most popular distributional reinforcement
    learning algorithms, called categorical DQN, which is also known as the C51 algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分布式强化学习的基本思想和动机。在下一节中，我们将学习一种最流行的分布式强化学习算法——分类DQN，也叫做C51算法。
- en: Categorical DQN
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类DQN
- en: In the last section, we learned why it is more beneficial to choose an action
    based on the distribution of return than to choose an action based on the Q value,
    which is just the expected return. In this section, we will understand how to
    compute the distribution of return using an algorithm called categorical DQN.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了为什么基于回报分布来选择行动比仅仅基于Q值（即期望回报）来选择行动更有利。在这一节中，我们将学习如何使用一种名为分类DQN的算法来计算回报的分布。
- en: The distribution of return is often called the value distribution or return
    distribution. Let *Z* be the random variable and *Z*(*s*, *a*) denote the value
    distribution of a state *s* and an action *a*. We know that the Q function is
    represented by *Q*(*s*, *a*) and it gives the value of a state-action pair. Similarly,
    now we have *Z*(*s*, *a*) and it gives the value distribution (return distribution)
    of the state-action pair.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回报的分布通常被称为值分布或回报分布。设*Z*为随机变量，*Z*(*s*, *a*)表示状态*s*和动作*a*的值分布。我们知道Q函数表示为*Q*(*s*,
    *a*)，它给出一个状态-动作对的值。同样，现在我们有了*Z*(*s*, *a*)，它给出状态-动作对的值分布（回报分布）。
- en: Okay, how can we compute *Z*(*s*, *a*)? First, let's recollect how we compute
    *Q*(*s*, *a*).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何计算*Z*(*s*, *a*)？首先，让我们回顾一下如何计算*Q*(*s*, *a*)。
- en: In DQN, we learned that we use a neural network to approximate the Q function,
    *Q*(*s*, *a),* Since we use a neural network to approximate the Q function, we
    can represent the Q function by ![](img/B15558_12_331.png), where ![](img/B15558_14_004.png)
    is the parameter of the network. Given a state as an input to the network, it
    outputs the Q values of all the actions that can be performed in that state, and
    then we select the action that has the maximum Q value.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在DQN中，我们学习到使用神经网络来逼近Q函数，*Q*(*s*, *a*)。由于我们使用神经网络来逼近Q函数，我们可以通过![](img/B15558_12_331.png)来表示Q函数，其中![](img/B15558_14_004.png)是网络的参数。给定一个状态作为输入到网络，它输出所有可以在该状态下执行的动作的Q值，然后我们选择具有最大Q值的动作。
- en: Similarly, in categorical DQN, we use a neural network to approximate the value
    of *Z*(*s*, *a*). We can represent this by ![](img/B15558_14_005.png), where ![](img/B15558_09_087.png)
    is the parameter of the network. Given a state as an input to the network, it
    outputs the value distribution (return distribution) of all the actions that can
    be performed in that state as an output and then we select an action based on
    this value distribution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在分类DQN中，我们使用神经网络来逼近*Z*(*s*, *a*)的值。我们可以通过![](img/B15558_14_005.png)来表示这一点，其中![](img/B15558_09_087.png)是网络的参数。给定一个状态作为输入到网络，它输出所有可以在该状态下执行的动作的值分布（回报分布），然后我们根据这个值分布选择一个动作。
- en: 'Let''s understand the difference between the DQN and categorical DQN with an
    example. Suppose we are in the state *s* and say our action space has two actions
    *a* and *b*. Now, as shown in *Figure 14.3*, given the state *s* as an input to
    the DQN, it returns the Q value of all the actions, then we select the action
    that has the maximum Q value, whereas in the categorical DQN, given the state
    *s* as an input, it returns the value distribution of all the actions, then we
    select the action based on this value distribution:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解DQN和分类DQN之间的区别。假设我们处于状态*s*，并且假设我们的动作空间有两个动作*a*和*b*。现在，如*图14.3*所示，给定状态*s*作为输入，DQN返回所有动作的Q值，然后我们选择具有最大Q值的动作，而在分类DQN中，给定状态*s*作为输入，它返回所有动作的值分布，然后我们根据这个值分布选择一个动作：
- en: '![](img/B15558_14_03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_03.png)'
- en: 'Figure 14.3: DQN vs categorical DQN'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：DQN与分类DQN
- en: Okay, how can we train the network? In DQN, we learned that we train the network
    by minimizing the loss between the target Q value and the Q value predicted by
    the network. We learned that the target Q value is obtained by the Bellman optimality
    equation. Thus, we minimize the loss between the target value (the optimal Bellman
    Q value) and the predicted value (the Q value predicted by the network) and train
    the network.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何训练网络？在DQN中，我们学习到通过最小化目标Q值与网络预测的Q值之间的损失来训练网络。我们知道目标Q值是通过贝尔曼最优方程获得的。因此，我们最小化目标值（最优贝尔曼Q值）与预测值（网络预测的Q值）之间的损失并训练网络。
- en: Similarly, in categorical DQN, we train the network by minimizing the loss between
    the target value distribution and the value distribution predicted by the network.
    Okay, how can we obtain the target value distribution? In DQN, we obtained the
    target Q value using the Bellman equation; similarly in categorical DQN, we can
    obtain the target value distribution using the distributional Bellman equation.
    What's the distributional Bellman equation? First, let's recall the Bellman equation
    before learning about the distributional Bellman equation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在分类DQN中，我们通过最小化目标值分布与网络预测的值分布之间的损失来训练网络。好的，我们如何获得目标值分布？在DQN中，我们使用贝尔曼方程来获得目标Q值；类似地，在分类DQN中，我们可以使用分布式贝尔曼方程来获得目标值分布。那么，什么是分布式贝尔曼方程？首先，在学习分布式贝尔曼方程之前，让我们回顾一下贝尔曼方程。
- en: 'We learned that the Bellman equation for the Q function *Q*(*s*, *a*) is given
    as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，Q函数*Q*(*s*, *a*)的贝尔曼方程表示为：
- en: '![](img/B15558_14_007.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_007.png)'
- en: 'Similarly, the Bellman equation for the value distribution *Z*(*s*, *a*) is
    given as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，值分布*Z*(*s*, *a*)的贝尔曼方程表示为：
- en: '![](img/B15558_14_008.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_008.png)'
- en: This equation is called the distributional Bellman equation. Thus, in categorical
    DQN, we train the network by minimizing the loss between the target value distribution,
    which is given by the distributional Bellman equation, and the value distribution
    predicted by the network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程被称为分布式贝尔曼方程。因此，在分类DQN中，我们通过最小化目标值分布（由分布式贝尔曼方程给出）和网络预测的值分布之间的损失来训练网络。
- en: Okay, what loss function should we use? In DQN, we use the **mean squared error**
    (**MSE**) as our loss function. Unlike a DQN, we cannot use the MSE as the loss
    function in the categorical DQN because in categorical DQN, we predict the probability
    distribution and not the Q value. Since we are dealing with the distribution we
    use the cross entropy loss as our loss function. Thus, in categorical DQN, we
    train the network by minimizing the cross entropy loss between the target value
    distribution and the value distribution predicted by the network.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们应该使用什么样的损失函数呢？在DQN中，我们使用**均方误差**（**MSE**）作为我们的损失函数。与DQN不同的是，在分类DQN中，我们不能使用MSE作为损失函数，因为在分类DQN中，我们预测的是概率分布，而不是Q值。由于我们处理的是分布，因此我们使用交叉熵损失作为我们的损失函数。因此，在分类DQN中，我们通过最小化目标值分布与网络预测的值分布之间的交叉熵损失来训练网络。
- en: In a nutshell, a categorical DQN is similar to DQN, except that in a categorical
    DQN, we predict the value distribution whereas in a DQN we predict the Q value.
    Thus, given a state as an input, a categorical DQN returns the value distribution
    of each action in that state. We train the network by minimizing the cross entropy
    loss between the target value distribution, which is given by the distributional
    Bellman equation, and the value distribution predicted by the network.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，分类DQN与DQN相似，唯一不同的是在分类DQN中，我们预测的是值分布，而在DQN中，我们预测的是Q值。因此，给定一个状态作为输入，分类DQN返回该状态下每个动作的值分布。我们通过最小化目标值分布（由分布式贝尔曼方程给出）和网络预测的值分布之间的交叉熵损失来训练网络。
- en: Now that we have understood what a categorical DQN is and how it differs from
    a DQN, in the next section we will learn how exactly the categorical DQN predicts
    the value distribution.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了分类DQN是什么以及它与DQN的区别，在接下来的部分中，我们将学习分类DQN是如何准确预测值分布的。
- en: Predicting the value distribution
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测值分布
- en: '*Figure 14.4* shows a simple value distribution:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.4*展示了一个简单的值分布：'
- en: '![](img/B15558_14_04.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_04.png)'
- en: 'Figure 14.4: Value distribution'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：值分布
- en: The horizontal axis values are called support or atoms and the vertical axis
    values are the probability. We denote the support by *Z* and the probability by
    *P*. In order to predict the value distribution, along with the state, our network
    takes the support of the distribution as input and it returns the probability
    of each value in the support.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 水平轴的值称为支持或原子，垂直轴的值则是概率。我们用*Z*表示支持，用*P*表示概率。为了预测值的分布以及状态，我们的网络将分布的支持作为输入，并返回支持中每个值的概率。
- en: So, now, we will see how to compute the support of the distribution. To compute
    support, first, we need to decide the number of values of the support *N*, the
    minimum value of the support ![](img/B15558_14_009.png), and the maximum value
    of the support ![](img/B15558_14_010.png). Given a number of support *N*, we divide
    them into *N* equal parts from ![](img/B15558_14_009.png) to ![](img/B15558_14_012.png).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现在我们将看到如何计算分布的支持。为了计算支持，首先我们需要决定支持的值的数量*N*、支持的最小值![](img/B15558_14_009.png)和支持的最大值![](img/B15558_14_010.png)。给定支持的数量*N*，我们将其从![](img/B15558_14_009.png)到![](img/B15558_14_012.png)分成*N*个相等的部分。
- en: 'Let''s understand this with an example. Say the number of support *N* = 5,
    the minimum value of support ![](img/B15558_14_013.png), and the maximum value
    of the support ![](img/B15558_14_014.png). Now, how can we find the values of
    the support? In order to find the values of the support, first, we will compute
    the step size called ![](img/B15558_14_015.png). The value of ![](img/B15558_14_016.png)
    can be computed as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这一点。假设支撑值数量 *N* = 5，支撑值的最小值为 ![](img/B15558_14_013.png)，最大值为 ![](img/B15558_14_014.png)。现在，如何找到支撑值呢？为了找到支撑值，首先，我们需要计算一个步长，记作
    ![](img/B15558_14_015.png)。值 ![](img/B15558_14_016.png) 可以通过以下公式计算：
- en: '![](img/B15558_14_017.png)![](img/B15558_14_018.png)![](img/B15558_14_019.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_017.png)![](img/B15558_14_018.png)![](img/B15558_14_019.png)'
- en: 'Now, to compute the values of support, we start with the minimum value of support
    ![](img/B15558_14_009.png) and add ![](img/B15558_14_021.png) to every value until
    we reach the number of support *N*. In our example, we start with ![](img/B15558_14_022.png),
    which is 2, and we add ![](img/B15558_14_023.png) to every value until we reach
    the number of support *N*. Thus, the support values become:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算支撑值，我们从支撑值的最小值开始 ![](img/B15558_14_009.png)，并将 ![](img/B15558_14_021.png)
    加到每个值上，直到我们达到支撑值数量 *N*。在我们的示例中，我们从 ![](img/B15558_14_022.png) 开始，它是2，然后我们将 ![](img/B15558_14_023.png)
    加到每个值上，直到我们达到支撑值数量 *N*。因此，支撑值变为：
- en: '![](img/B15558_14_024.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_024.png)'
- en: 'Thus, we can write the value of support as ![](img/B15558_14_025.png). The
    following Python snippet gives us more clarity on how to obtain the support values:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将支撑值表示为 ![](img/B15558_14_025.png)。以下Python代码片段可以帮助我们更清楚地了解如何获得支撑值：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Okay, we have learned how to compute the support of the distribution, now how
    does the neural network take this support as input and return the probabilities?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经了解了如何计算分布的支撑值，现在神经网络是如何将这个支撑值作为输入并返回概率的呢？
- en: In order to predict the value distribution, along with the state, we also need
    to give the support of the distribution as input and then the network returns
    the probabilities of our value distribution as output. Let's understand this with
    an example. Say we are in a state *s* and we have two actions to perform in this
    state, and let the actions be *up* and *down*. Say our calculated support values
    are *z*[1], *z*[2], and *z*[3].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测价值分布，除了状态外，我们还需要将分布的支撑值作为输入，然后网络返回我们价值分布的概率作为输出。让我们通过一个例子来理解这一点。假设我们处于状态
    *s*，并且在这个状态下有两个动作可供选择，分别是 *上* 和 *下*。假设我们计算得到的支撑值是 *z*[1]、*z*[2] 和 *z*[3]。
- en: 'As *Figure 14.5* shows, along with giving the state *s* as input to the network,
    we also give the support of our distribution *z*[1], *z*[2], and *z*[3]. Then
    our network returns the probabilities *p*[i](*s*, *a*) of the given support for
    the distribution of action *up* and distribution of action *down*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 *图14.5* 所示，在将状态 *s* 作为输入传入网络的同时，我们还将分布的支撑值 *z*[1]、*z*[2] 和 *z*[3] 输入。然后，网络返回给定支撑值对应的动作
    *上* 和动作 *下* 的分布概率 *p*[i](*s*, *a*)：
- en: '![](img/B15558_14_05.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_05.png)'
- en: 'Figure 14.5: A categorical DQN'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：分类DQN
- en: The authors of the categorical DQN paper (see the *Further reading* section
    for more details) suggest that it will be efficient to set the number of support
    *N* as 51, and so the categorical DQN is also known as the C51 algorithm. Thus,
    we have learned how categorical DQN predicts the value distribution. In the next
    section, we will learn how to select the action based on this predicted value
    distribution.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分类DQN论文的作者（详见 *进一步阅读* 部分）建议将支撑值数量 *N* 设置为51，因此分类DQN也被称为C51算法。因此，我们已经了解了分类DQN如何预测价值分布。在下一部分，我们将学习如何基于这个预测的价值分布选择动作。
- en: Selecting an action based on the value distribution
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于价值分布选择动作
- en: We have learned that a categorical DQN returns the value distribution of each
    action in the given state. But how can we select the best action based on the
    value distribution predicted by the network?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学过分类DQN返回给定状态下每个动作的价值分布。但我们如何根据网络预测的价值分布来选择最佳动作呢？
- en: We generally select an action based on the Q value, that is, we usually select
    the action that has the maximum Q value. But now we don't have a Q value; instead,
    we have a value distribution. How can we select an action based on the value distribution?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常基于Q值选择动作，也就是说，我们通常选择具有最大Q值的动作。但现在我们没有Q值，而是有一个价值分布。那我们如何基于价值分布选择动作呢？
- en: 'First, we will extract the Q value from the value distribution and then we
    select the action as the one that has the maximum Q value. Okay, how can we extract
    the Q value? We can compute the Q value by just taking the expectation of the
    value distribution. The expectation of the distribution is given as the sum of
    support *z*[i] multiplied by their corresponding probability *p*[i]. So the expectation
    of the value distribution *Z* is given as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从价值分布中提取Q值，然后选择具有最大Q值的动作。好的，如何提取Q值呢？我们可以通过计算价值分布的期望值来获得Q值。分布的期望值可以表示为支持度*z*[i]与它们相应的概率*p*[i]的乘积的总和。因此，价值分布*Z*的期望值为：
- en: '![](img/B15558_14_026.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_026.png)'
- en: Where *z*[i] is the support and *p*[i] is the probability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*z*[i]是支持度，*p*[i]是概率。
- en: 'Thus, the Q value of the value distribution can be computed as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，价值分布的Q值可以计算为：
- en: '![](img/B15558_14_027.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_027.png)'
- en: 'After computing the Q value, we select the best action as the one that has
    the maximum Q value:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算Q值后，我们选择Q值最大的动作作为最佳动作：
- en: '![](img/B15558_14_028.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_028.png)'
- en: Let's understand how this works exactly. Suppose we are in the state *s* and
    say we have two actions in the state. Let the actions be *up* and *down*. First,
    we need to compute support. Let the number of support *N* = 3, the minimum value
    of the support ![](img/B15558_14_013.png), and the maximum value of the support
    ![](img/B15558_14_030.png). Then, our computed support values will be [2,3,4].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们准确理解这一过程。假设我们处于状态*s*，并且在该状态下有两个动作。假设这两个动作分别为*up*和*down*。首先，我们需要计算支持度。假设支持度的数量*N*
    = 3，支持度的最小值为 ![](img/B15558_14_013.png)，最大值为 ![](img/B15558_14_030.png)。然后，我们计算出的支持度值为[2,3,4]。
- en: 'Now, along with the state *s*, we feed the support, then the categorical DQN
    returns the probabilities *p*[i](*s*, *a*) of the given support for the value
    distribution of action *up* and distribution of action *down* as shown here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将支持度与状态*s*一起输入，然后类别DQN返回给定支持度下的动作*up*和动作*down*的价值分布概率*p*[i](*s*, *a*)，如图所示：
- en: '![](img/B15558_14_06.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_06.png)'
- en: 'Figure 14.6: Categorical DQN'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：类别DQN
- en: Now, how can we select the best action, based on these two value distributions?
    First, we will extract the Q value from the value distributions and then we select
    the action that has the maximum Q value.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如何根据这两个价值分布选择最佳动作呢？首先，我们将从价值分布中提取Q值，然后选择具有最大Q值的动作。
- en: 'We learned that the Q value can be extracted from the value distribution as
    the sum of support multiplied by their probabilities:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，Q值可以通过将支持度与相应的概率相乘的和从价值分布中提取出来：
- en: '![](img/B15558_14_027.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_027.png)'
- en: 'So, we can compute the Q value of action *up* in state *s* as:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以计算在状态*s*中动作*up*的Q值，如下所示：
- en: '![](img/B15558_14_032.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_032.png)'
- en: 'Now, we can compute the Q value of action *down* in state *s* as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算在状态*s*中动作*down*的Q值，如下所示：
- en: '![](img/B15558_14_033.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_033.png)'
- en: Now, we select the action that has the maximum Q value. Since the action *up*
    has the high Q value, we select the action *up* as the best action.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们选择具有最大Q值的动作。由于动作*up*具有较高的Q值，因此我们选择动作*up*作为最佳动作。
- en: Wait! What makes categorical DQN special then? Because just like DQN, we are
    selecting the action based on the Q value at the end. One important point we have
    to note is that, in DQN, we compute the Q value based on the expectation of the
    return directly, but in categorical DQN, first, we learn the return distribution
    and then we compute the Q value based on the expectation of the return distribution,
    which captures the intrinsic randomness.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！那么，类别DQN究竟有什么特别之处？因为像DQN一样，我们最终也是根据Q值选择动作。一个我们必须注意的重要点是，在DQN中，我们是直接根据回报的期望来计算Q值的，但在类别DQN中，我们首先学习回报分布，然后基于回报分布的期望来计算Q值，这样能够捕捉到内在的随机性。
- en: We have learned that the categorical DQN outputs the value distribution of all
    the actions in the given state and then we extract the Q value from the value
    distribution and select the action that has the maximum Q value as the best action.
    But the question is how exactly does our categorical DQN learn? How do we train
    the categorical DQN to predict the accurate value distribution? Let's discuss
    this in the next section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，类别DQN输出给定状态下所有动作的价值分布，然后从中提取Q值并选择具有最大Q值的动作作为最佳动作。但问题是，我们的类别DQN到底是如何学习的？我们如何训练类别DQN以预测准确的价值分布？我们将在下一节讨论这个问题。
- en: Training the categorical DQN
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练分类DQN
- en: 'We train the categorical DQN by minimizing the cross entropy loss between the
    target value distribution and the predicted value distribution. How can we compute
    the target distribution? We can compute the target distribution using the distributional
    Bellman equation given as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过最小化目标值分布和预测值分布之间的交叉熵损失来训练分类DQN。我们如何计算目标分布呢？我们可以通过以下的分布贝尔曼方程来计算目标分布：
- en: '![](img/B15558_14_034.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_034.png)'
- en: 'Where ![](img/B15558_14_035.png) represents the immediate reward *r*, which
    we obtain while performing an action *a* in the state *s* and moving to the next
    state ![](img/B15558_14_036.png), so we can just denote ![](img/B15558_14_035.png)
    by *r*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_14_035.png) 表示即时奖励*r*，这是在执行动作*a*并从状态*s*移动到下一个状态 ![](img/B15558_14_036.png)
    时获得的奖励，因此我们可以将 ![](img/B15558_14_035.png) 简单表示为*r*：
- en: '![](img/B15558_14_038.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_038.png)'
- en: Remember in DQN we computed the target value using the target network parameterized
    by ![](img/B15558_14_039.png)? Similarly, here, we use the target categorical
    DQN parameterized by ![](img/B15558_14_040.png) to compute the target distribution.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在DQN中，我们使用由 ![](img/B15558_14_039.png) 参数化的目标网络计算目标值吗？同样地，这里我们使用由 ![](img/B15558_14_040.png)
    参数化的目标分类DQN来计算目标分布。
- en: After computing the target distribution, we train the network by minimizing
    the cross entropy loss between the target value distribution and the predicted
    value distribution. One important point we need to note here is that we can apply
    the cross entropy loss between any two distributions only when their supports
    are equal; when their supports are not equal we cannot apply the cross entropy
    loss.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 计算目标分布后，我们通过最小化目标值分布与预测值分布之间的交叉熵损失来训练网络。这里有一个重要的点需要注意：只有在目标分布和预测分布的支持相等时，才能应用交叉熵损失；如果它们的支持不相等，我们就无法应用交叉熵损失。
- en: 'For instance, *Figure 14.7* shows the support of both the target and predicted
    distribution is the same, (1,2,3,4). Thus, in this case, we can apply the cross
    entropy loss:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*图 14.7* 显示了目标分布和预测分布的支持相同，(1,2,3,4)。因此，在这种情况下，我们可以应用交叉熵损失：
- en: '![](img/B15558_14_07.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_07.png)'
- en: 'Figure 14.7: Target and predicted distribution'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：目标分布与预测分布
- en: In *Figure 14.8*, we can see that the target distribution support (1,3,4,5)
    and the predicted distribution support (1,2,3,4) are different, so in this case,
    we cannot apply the cross entropy loss.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 14.8*中，我们可以看到目标分布的支持（1,3,4,5）和预测分布的支持（1,2,3,4）是不同的，因此在这种情况下，我们无法应用交叉熵损失。
- en: '![](img/B15558_14_08.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_08.png)'
- en: 'Figure 14.8: Target and predicted distribution'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：目标分布与预测分布
- en: So, when the support of the target and prediction distribution is different,
    we perform a special step called the projection step using which we can make the
    support of the target and prediction distribution equal. Once we make the support
    of the target and prediction distribution equal then we can apply the cross entropy loss.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当目标分布和预测分布的支持不同时，我们会执行一个特殊的步骤，称为投影步骤，借助这个步骤，我们可以使目标分布和预测分布的支持相等。一旦我们使目标和预测分布的支持相等，就可以应用交叉熵损失。
- en: In the next section, we will learn how exactly the projection works and how
    it makes the support of the target and prediction distribution equal.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将学习投影步骤是如何工作的，以及它如何使目标分布和预测分布的支持相等。
- en: Projection step
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 投影步骤
- en: Let's understand how exactly the projection step works with an example. Suppose
    the input support is *z* = [1, 2].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解投影步骤是如何工作的。假设输入支持是*z* = [1, 2]。
- en: 'Let the probability of predicted distribution be *p* = [0.5, 0.5]. *Figure
    14.9* shows the predicted distribution:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 设预测分布的概率为*p* = [0.5, 0.5]。*图 14.9* 显示了预测分布：
- en: '![](img/B15558_14_09.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_09.png)'
- en: 'Figure 14.9: Predicted distribution'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：预测分布
- en: 'Let the probability of target distribution be *p* = [0.3, 0.7]. Let the reward
    *r* = 0.1 and the discount factor ![](img/B15558_14_041.png). The target distribution
    support value is computed as![](img/B15558_14_042.png), so, we can write:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 设目标分布的概率为*p* = [0.3, 0.7]。设奖励*r* = 0.1，折扣因子为 ![](img/B15558_14_041.png)。目标分布的支持值计算为
    ![](img/B15558_14_042.png)，因此，我们可以写为：
- en: '![](img/B15558_14_043.png)![](img/B15558_14_044.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_043.png)![](img/B15558_14_044.png)'
- en: 'Thus, the target distribution becomes:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标分布变为：
- en: '![](img/B15558_14_10.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_10.png)'
- en: 'Figure 14.10: Target distribution'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：目标分布
- en: As we can observe from the preceding plots, the supports of the predicted distribution
    and target distribution are different. The predicted distribution has the support
    [1, 2] while the target distribution has the support [1, 1.9], so in this case,
    we cannot apply the cross entropy loss.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中我们可以看到，预测分布和目标分布的支持范围不同。预测分布的支持范围是[1, 2]，而目标分布的支持范围是[1, 1.9]，因此在这种情况下，我们无法直接应用交叉熵损失函数。
- en: Now, using the projection step we can convert the support of our target distribution
    to be the same support as the predicted distribution. Once the supports of the
    predicted and target distribution are the same then we can apply the cross entropy
    loss.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用投影步骤，我们可以将目标分布的支持范围转换为与预测分布相同的支持范围。一旦预测分布和目标分布的支持范围一致，我们就可以应用交叉熵损失函数。
- en: Okay, what's that projection step exactly? How can we apply it and convert the
    support of the target distribution to match the support of the predicted distribution?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么这个投影步骤到底是什么呢？我们该如何应用它，将目标分布的支持范围转换为与预测分布的支持范围一致呢？
- en: Let's understand this with the same example. As the following shows, we have
    the target distribution support [1, 1.9] and we need to make it equal to the predicted
    distribution support [1, 2], how can we do that?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过相同的例子来理解这个问题。如下面所示，我们的目标分布支持范围是[1, 1.9]，而我们需要将其调整为预测分布支持范围[1, 2]，我们该如何操作呢？
- en: '![](img/B15558_14_11.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_11.png)'
- en: 'Figure 14.11: Target distribution'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：目标分布
- en: 'So, what we can do is that we can distribute the probability 0.7 from the support
    1.9 to the support 1 and 2:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以做的是将0.7的概率从支持1.9分配到支持1和2：
- en: '![](img/B15558_14_12.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_12.png)'
- en: 'Figure 14.12: Target distribution'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12：目标分布
- en: Okay, but how can we distribute the probabilities from the support 1.9 to the
    support 1 and 2? Should it be an equal distribution? Of course not. Since 2 is
    closer to 1.9, we distribute more probability to 2 and less to 1\.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但我们如何将概率从支持1.9分配到支持1和2呢？是不是应该均匀分配呢？当然不是。由于2比1.9更接近，我们将更多的概率分配给2，较少的分配给1。
- en: As shown in *Figure 14.13*, from 0.7, we will distribute 0.63 to support 2 and
    0.07 to support 1.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图14.13*所示，从0.7开始，我们将0.63的概率分配给支持2，0.07的概率分配给支持1。
- en: '![](img/B15558_14_13.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_13.png)'
- en: 'Figure 14.13: Target distribution'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.13：目标分布
- en: 'Thus, now our target distribution will look like:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们的目标分布将变为：
- en: '![](img/B15558_14_14.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_14_14.png)'
- en: 'Figure 14.14: Target distribution'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.14：目标分布
- en: From *Figure 14.14*, we can see that support of the target distribution is changed
    from [1, 1.9] to [1, 2] and now it matches the support of the predicted distribution.
    This step is called the projection step.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图14.14*中我们可以看到，目标分布的支持范围已从[1, 1.9]变化为[1, 2]，现在它与预测分布的支持范围一致。这一步被称为投影步骤。
- en: What we learned is just a simple example, consider a case where our target and
    predicted distribution support varies very much. In this case, we cannot manually
    determine the amount of probability we have to distribute across the supports
    to make them equal. So, we introduce a set of steps to perform the projection,
    as the following shows. After performing these steps, our target distribution
    support will match our predicted distribution by distributing the probabilities
    across the support.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所学到的只是一个简单的例子，假设目标分布和预测分布的支持范围差异很大。在这种情况下，我们无法手动确定该将多少概率分配到各个支持范围，使其相等。因此，我们引入了一套步骤来进行投影，具体步骤如下。执行这些步骤后，我们的目标分布支持范围将通过将概率分配到支持范围上，从而与预测分布支持范围匹配。
- en: First, we initialize an array *m* with its shape as the number of support with
    zero values. The *m* denotes the distributed probability of the target distribution
    after the projection step.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化一个数组*m*，其形状为支持范围的数量，值为零。*m*表示在投影步骤后目标分布的分配概率。
- en: 'For *j* in range of the number of support:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于* j *，遍历支持范围的数量：
- en: 'Compute the target support value: ![](img/B15558_14_045.png)'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标支持值：![](img/B15558_14_045.png)
- en: 'Compute the value of *b*: ![](img/B15558_14_046.png)'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算*b*的值：![](img/B15558_14_046.png)
- en: 'Compute the lower bound and the upper bound: ![](img/B15558_14_047.png)'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算下限和上限：![](img/B15558_14_047.png)
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_048.png)'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下限上分配概率：![](img/B15558_14_048.png)
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_049.png)'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上限上分配概率：![](img/B15558_14_049.png)
- en: Understanding how exactly these projection steps work is a little tricky! So,
    let's understand this by considering the same example we used earlier. Let z =
    [1, 2], *N* = 2, ![](img/B15558_14_050.png), and ![](img/B15558_14_051.png).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the probability of predicted distribution be *p* = [0.5, 0.5]. *Figure
    14.15* shows the predicted distribution:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_15.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: Predicted distribution'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the probability of target distribution be *p* = [0.3, 0.7]. Let the reward
    *r* = 0.1 and the discount factor ![](img/B15558_14_041.png), and we know ![](img/B15558_14_053.png),
    thus, the target distribution becomes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_16.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.16: Target distribution'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 14.16*, we can infer that support in target distribution is different
    from the predicted distribution. Now, we will learn how to perform the projection
    using the preceding steps.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize an array *m* with its shape as the number of support with
    zero values. Thus, *m* = [0, 0].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration, j=0**:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Compute the target support value:![](img/B15558_14_054.png)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of *b*:![](img/B15558_14_055.png)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the lower and upper bound:![](img/B15558_14_056.png)
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the probability on the lower bound:![](img/B15558_14_057.png)![](img/B15558_14_058.png)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the probability on the upper bound:![](img/B15558_14_059.png)![](img/B15558_14_060.png)
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the 1st iteration, the value of *m* becomes [0, 0].
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration, j=1**:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Compute the target support value:![](img/B15558_14_061.png)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of *b*:![](img/B15558_14_062.png)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the lower and upper bound of *b*:![](img/B15558_14_063.png)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the probability on the lower bound:![](img/B15558_14_064.png)![](img/B15558_14_065.png)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the probability on the upper bound:![](img/B15558_14_066.png)![](img/B15558_14_067.png)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the second iteration, the value of *m* becomes [0.07, 0,63]. The number
    of iterations = the length of our support. Since the length of our support is
    2, we will stop here and thus the value of *m* becomes our new distributed probability
    for the modified support, as *Figure 14.17* shows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_17.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.17: Target distribution'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet will give us more clarity on how exactly the projection
    step works:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that we have understood how to compute the target value distribution and
    how we can make the support of target value distribution equal to the support
    of predicted value distribution using the projection step, we will learn how to
    compute the cross entropy loss. Cross entropy loss is given as:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_068.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Where *y* is the actual value and ![](img/B15558_14_069.png) is the predicted
    value. Thus, we can write:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_070.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Where *m* is the target probabilities from the target value distribution and
    *p*(*s*, *a*) is the predicted probabilities from the predicted value distribution.
    We train our network by minimizing the cross entropy loss.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using a categorical DQN, we select the action based on the distribution
    of the return (value distribution). In the next section, we will put all these
    concepts together and see how a categorical DQN works.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we initialize the main network parameter ![](img/B15558_09_106.png) with
    random values, and we initialize the target network parameter ![](img/B15558_14_039.png)
    by just copying the main network parameter ![](img/B15558_09_054.png). We also
    initialize the replay buffer ![](img/B15558_09_124.png).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each step in the episode, we feed the state of the environment and
    support values to the main categorical DQN parameterized by ![](img/B15558_09_098.png).
    The main network takes the support and state of the environment as input and returns
    the probability value for each support. Then the Q value of the value distribution
    can be computed as the sum of support multiplied by their probabilities:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_075.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q value of all the actions in the state, we select the
    best action in the state *s* as the one that has the maximum Q value:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_028.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: However, instead of selecting the action that has the maximum Q value all the
    time, we select the action using the epsilon-greedy policy. With the epsilon-greedy
    policy, we select a random action with probability epsilon and with the probability
    1-epsilon, we select the best action that has the maximum Q value. We perform
    the selected action, move to the next state, obtain the reward, and store this
    transition information ![](img/B15558_14_077.png) in the replay buffer ![](img/B15558_09_124.png).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Now, we sample a transition ![](img/B15558_14_077.png) from the replay buffer
    ![](img/B15558_09_124.png) and feed the next state ![](img/B15558_12_376.png)
    and support values to the target categorical DQN parameterized by ![](img/B15558_12_025.png).
    The target network takes the support and next state ![](img/B15558_12_376.png)
    as input and returns the probability value for each support.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the Q value can be computed as the sum of support multiplied by their
    probabilities:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_082.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q value of all next state-action pairs, we select the best
    action in the state ![](img/B15558_14_083.png) as the one that has the maximum
    Q value:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_084.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Now, we perform the projection step. The *m* denotes the distributed probability
    of the target distribution after the projection step.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'For *j* in range of the number of support:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the target support value: ![](img/B15558_14_085.png)'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the value of *b*: ![](img/B15558_14_046.png)'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the lower bound and the upper bound: ![](img/B15558_14_087.png)'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_088.png)'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_089.png)'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After performing the projection step, compute the cross entropy loss:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_090.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Where *m* is the target probabilities from the target value distribution and
    *p*(*s*, *a*) is the predicted probabilities from the predicted value distribution.
    We train our network by minimizing the cross entropy loss.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: We don't update the target network parameter ![](img/B15558_14_040.png) in every
    time step. We freeze the target network parameter ![](img/B15558_14_040.png) for
    several time steps, and then we copy the main network parameter ![](img/B15558_10_037.png)
    to the target network parameter ![](img/B15558_14_039.png). We keep repeating
    the preceding steps for several episodes to approximate the optimal value distribution.
    To give us a more detailed understanding, the categorical DQN algorithm is given
    in the next section.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – categorical DQN
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The categorical DQN algorithm is given in the following steps:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_10_095.png) with random
    values
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_14_039.png) by copying
    the main network parameter ![](img/B15558_10_037.png)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_14_098.png), the number of support
    (atoms), and also ![](img/B15558_14_022.png) and ![](img/B15558_14_100.png)
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes perform *step 5*
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for ![](img/B15558_14_101.png):'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the state *s* and support values to the main categorical DQN parameterized
    by ![](img/B15558_09_054.png) and get the probability value for each support.
    Then compute the Q value as ![](img/B15558_14_103.png)
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the Q value, select an action using the epsilon-greedy policy,
    that is, with the probability epsilon, select random action *a* and with probability
    1-epsilon, select the action as ![](img/B15558_14_028.png)
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action and move to the next state ![](img/B15558_14_105.png)
    and obtain the reward *r*
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the transition information in the replay buffer ![](img/B15558_14_098.png)
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a transition from the replay buffer ![](img/B15558_12_259.png)
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the next state ![](img/B15558_12_376.png) and support values to the target
    categorical DQN parameterized by ![](img/B15558_14_040.png) and get the probability
    value for each support. Then compute the value as ![](img/B15558_14_110.png)
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the Q value, we select the best action in the state ![](img/B15558_14_105.png)
    as the one that has the maximum Q value ![](img/B15558_14_112.png)
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the array *m* with zero values with its shape as the number of support
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *j* in range of the number of support:'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the target support value: ![](img/B15558_14_113.png)'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the value of b: ![](img/B15558_14_114.png)'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the lower bound and upper bound: ![](img/B15558_14_047.png)'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_116.png)'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_117.png)'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the cross entropy loss: ![](img/B15558_14_118.png)'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the loss using gradient descent and update the parameter of the main
    network
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Freeze the target network parameter ![](img/B15558_14_040.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_098.png)
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned the categorical DQN algorithm, to understand how a categorical
    DQN works, we will implement it in the next section.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Playing Atari games using a categorical DQN
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's implement the categorical DQN algorithm to play Atari games. The code
    used in this section is adapted from an open-source categorical DQN implementation,
    [https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo](https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo),
    provided by Prince Wen.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Defining the variables
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's define some of the important variables.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the ![](img/B15558_14_022.png) and ![](img/B15558_14_012.png):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Initialize the number of atoms (supports):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Set the discount factor, ![](img/B15558_03_190.png):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Set the batch size:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Set the time step at which we want to update the target network:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Set the epsilon value that is used in the epsilon-greedy policy:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Defining the replay buffer
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let''s define the buffer length:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the replay buffer as a deque structure:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We define a function called `sample_transitions` that returns the randomly
    sampled minibatch of transitions from the replay buffer:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Defining the categorical DQN class
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's define a class called `Categorical_DQN` where we will implement the categorical
    DQN algorithm. Instead of looking into the whole code at once, we will look into
    only the important parts. The complete code used in this section is available
    in the GitHub repo of the book.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'For a clear understanding, let''s take a look into the code line by line:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Defining the init method
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, let''s define the init method:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Start the TensorFlow session:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Initialize the ![](img/B15558_14_022.png) and ![](img/B15558_14_012.png):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Initialize the number of atoms:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Initialize the epsilon value:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Get the state shape of the environment:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Get the action shape of the environment:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize the time step:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Initialize the target state shape:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the placeholder for the state:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the placeholder for the action:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the placeholder for the *m* value (the distributed probability of the
    target distribution):'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Compute the value of ![](img/B15558_14_015.png) as ![](img/B15558_14_127.png):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Compute the support values as ![](img/B15558_14_025.png):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Build the categorical DQN:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Initialize all the TensorFlow variables:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Building the categorical DQN
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `build_network` for building a deep network.
    Since we are dealing with Atari games, we use the convolutional neural network:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the first convolutional layer:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define the second convolutional layer:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Flatten the feature maps obtained as a result of the second convolutional layer:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define the first dense layer:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define the second dense layer:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Concatenate the second dense layer with the action:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define the third layer and apply the softmax function to the result of the
    third layer and obtain the probabilities for each of the atoms:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, let''s define a function called `build_categorical_DQN` for building the
    main and target categorical DQNs:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Define the main categorical DQN and obtain the probabilities:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the target categorical DQN and obtain the probabilities:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Compute the main Q value with the probabilities obtained from the main categorical
    DQN as ![](img/B15558_14_129.png):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Similarly, compute the target Q value with probabilities obtained from the
    target categorical DQN as ![](img/B15558_14_130.png):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define the cross entropy loss as ![](img/B15558_14_131.png):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define the optimizer and minimize the cross entropy loss using the Adam optimizer:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Get the main network parameters:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Get the target network parameters:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define the `update_target_net` operation for updating the target network parameters
    by copying the parameters of the main network:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Defining the train function
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `train` to train the network:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Increment the time step:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Get the target Q values:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Select the next state action ![](img/B15558_14_132.png) as the one that has
    the maximum Q value:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Initialize an array *m* with its shape as the number of support with zero values.
    The *m* denotes the distributed probability of the target distribution after the
    projection step:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Get the probability for each atom using the target categorical DQN:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Perform the projection step:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Train the network by minimizing the loss:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Update the target network parameters by copying the main network parameters:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Selecting the action
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `select_action` for selecting the action:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We generate a random number, and if the number is less than epsilon we select
    the random action, else we select the action that has the maximum Q value:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Training the network
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s start training the network. First, create the Atari game environment
    using `gym`. Let''s create a Tennis game environment:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Create an object to our `Categorical_DQN` class:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Set the number of episodes:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'For each episode:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Set `done` to `False`:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Initialize the return:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'While the episode is not over:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Render the environment:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Select an action:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Perform the selected action:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Update the return:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Store the transition information in the replay buffer:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'If the length of the replay buffer is greater than or equal to the buffer size
    then start training the network by sampling transitions from the replay buffer:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Update the state to the next state:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Print the return obtained in the episode:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now that we have learned how a categorical DQN works and how to implement it,
    in the next section, we will learn about another interesting algorithm.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Quantile Regression DQN
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look into another interesting distributional RL algorithm
    called QR-DQN. It is a distributional DQN algorithm similar to the categorical
    DQN; however, it has several features that make it more advantageous than a categorical
    DQN.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Math essentials
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before going ahead, let''s recap two important concepts that we use in QR-DQN:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantile**'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverse cumulative distribution function** (**Inverse CDF**)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we divide our distribution into equal areas of probability, they are called
    quantiles. For instance, as *Figure 14.18* shows, we have divided our distribution
    into two equal areas of probabilities and we have two quantiles with 50% probability
    each:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_18.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.18: 2-quantile plot'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Inverse CDF (quantile function)
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand an **inverse cumulative distribution function** (**inverse CDF**),
    first, let's learn what a **cumulative distribution function** (**CDF**) is.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a random variable *X*, and *P*(*X*) denotes the probability distribution
    of *X*. Then the cumulative distribution function is expressed as:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_133.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
- en: It basically implies that *F*(*x*) can be obtained by adding up all the probabilities
    that are less than or equal to *x*.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following CDF:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_19.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.19: CDF'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding plot, ![](img/B15558_14_134.png) represents the cumulative
    probability, that is, ![](img/B15558_14_135.png). Say *i* =1, then ![](img/B15558_14_136.png).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'The CDF takes *x* as an input and returns the cumulative probability ![](img/B15558_14_137.png).
    Hence, we can write:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_138.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
- en: Say *x* = 2, then we get ![](img/B15558_14_139.png).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will look at the inverse CDF. Inverse CDF, as the name suggests, is
    the inverse of the CDF. That is, in CDF, given the support *x*, we obtain the
    cumulative probability ![](img/B15558_14_140.png), whereas in inverse CDF, given
    the cumulative probability ![](img/B15558_10_026.png), we obtain the support *x*.
    Inverse CDF can be expressed as:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_142.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
- en: 'The following plot shows the inverse CDF:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_20.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.20: Inverse CDF'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 14.20*, given the cumulative probability ![](img/B15558_14_143.png),
    we obtain the support *x*.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Say ![](img/B15558_14_144.png), then we get *x* = 2.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: We have learned that the quantiles are equally divided probabilities. As *Figure
    14.20* shows, we have three quantiles *q*[1] to *q*[3] with equally divided probabilities
    and the quantile values are [0.3,0.6,1.0], which are just our cumulative probabilities.
    Hence, we can say that the inverse CDF (quantile function) helps us to obtain
    the value of support given the equally divided probabilities. Note that in inverse
    CDF, the support should always be increasing as it is based on the cumulative
    probability.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned what the quantile function is, we will gain an understanding
    of how we can make use of the quantile function in the distributional RL setting
    using an algorithm called QR-DQN.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Understanding QR-DQN
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In categorical DQN (C51), we learned that in order to predict the value distribution,
    the network takes the support of the distribution as input and returns the probabilities.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: To compute the support, we also need to decide the number of support *N*, the
    minimum value of support ![](img/B15558_14_022.png), and the maximum value of
    support ![](img/B15558_14_146.png).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recollect in C51, our support values are equally spaced at fixed locations
    ![](img/B15558_14_147.png) and we feed this equally spaced support as input and
    obtained the non-uniform probabilities ![](img/B15558_14_148.png). As *Figure
    14.21* shows, in C51, we feed the equally spaced support ![](img/B15558_14_149.png)
    as input to the network along with the state(s) and obtain the non-uniform probabilities
    ![](img/B15558_14_150.png) as output:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_21.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.21: Categorical DQN'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'QR-DQN can be viewed just as the opposite of C51\. In QR-DQN, to estimate the
    value distribution, we feed the uniform probabilities ![](img/B15558_14_151.png)
    and the network outputs the supports at variable locations ![](img/B15558_14_152.png).
    As shown in the following figure, we feed the uniform probabilities ![](img/B15558_14_153.png)
    as input to the network along with the state(s) and obtain the support ![](img/B15558_14_154.png)
    placed at variable locations as output:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_22.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.22: QR-DQN'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Thus, from the two preceding figures we can observe that, in a categorical DQN,
    along with the state, we feed the fixed support at equally spaced intervals as
    input to the network and it returns the non-uniform probabilities, whereas in
    a QR-DQN, along with the state, we feed the fixed uniform probabilities as input
    to the network and it returns the support at variable locations (unequally spaced
    support).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Okay, but what's the use of this? How does a QR-DQN work exactly? Let's explore
    this in detail.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: We understood that a QR-DQN takes the uniform probabilities as input and returns
    the support values for estimating the value distribution. Can we make use of the
    quantile function to estimate the value distribution? Yes! We learned that the
    quantile function helps us to obtain the values of support given the equally divided
    probabilities. Thus, in QR-DQN, we estimate the value distribution by estimating
    the quantile function.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'The quantile function is given as:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_155.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
- en: Where *z* is the support and ![](img/B15558_14_143.png) is the equally divided
    cumulative probability. Thus, we can obtain the support *z* given ![](img/B15558_14_157.png).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *N* be the number of quantiles, then the probability can be obtained as:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_158.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
- en: For example, if *N* = 4, then *p* = [0.25, 0.25\. 0.25, 0.25]. If *N* = 5, then
    p = [0.20, 0.20, 0.20, 0.20, 0.20].
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we decide the number of quantiles *N*, the cumulative probabilities ![](img/B15558_14_143.png)
    (quantile values) can be obtained as:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_160.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
- en: For example, if *N* = 4, then ![](img/B15558_14_161.png). If *N* = 5, then![](img/B15558_14_162.png).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: We just feed this equally divided cumulative probability ![](img/B15558_14_163.png)
    (quantile values) as input to the QR-DQN and it returns the support value. That
    is, we have learned that the QR-DQN estimates the value distribution as the quantile
    function, so we just feed the ![](img/B15558_14_164.png) and obtain the support
    values *z* of the value distribution.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this with a simple example. Say we are in a state *s* and
    we have two possible actions *up* and *down* to perform in the state. As shown
    in the following figure, along with giving the state *s* as input to the network,
    we also feed the quantile value ![](img/B15558_14_165.png), which is just the
    equally divided cumulative probability. Then our network returns the support for
    the distribution of action *up* and the distribution of action *down*:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_23.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.23: QR-DQN'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: If you recollect, in C51, we computed the probability *p*(*s*, *a*) for the
    given state and action, whereas here in QR-DQN, we compute the support *z*(*s*,
    *a*) for the given state and action.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Note that we use capital *Z*(*s*, *a*) to represent the value distribution and
    small *z*(*s*, *a*) to represent the support of the distribution.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can also compute the target value distribution using the quantile
    function. Then we train our network by minimizing the distance between the predicted
    quantile and the target quantile distribution.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, the fundamental question is why are we doing this? How it is more beneficial
    than C51? There are several advantages of quantile regression DQN over categorical
    DQN. In quantile regression DQN:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: We don't have to choose the number of supports and the bounds of support, which
    is ![](img/B15558_14_009.png) and ![](img/B15558_14_167.png).
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no limitations on the bounds of support, thus the range of returns
    can vary across states.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also get rid of the projection step that we performed in the C51 to match
    the supports of the target and predicted distribution.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One more important advantage of a QR-DQN is that it minimizes the p-Wasserstein
    distance between the predicted and target distribution. But why is this important?
    Minimizing the Wasserstein distance between the target and predicted distribution
    helps us in attaining convergence better than minimizing the cross entropy.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, what exactly is the p-Wasserstein distance? The p-Wasserstein distance,
    *W*[p], is characterized as the *L*^p metric on inverse CDF. Say we have two distributions
    *U* and *V*, then the p-Wasserstein metric between these two distributions is
    given as:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_168.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_14_169.png) and ![](img/B15558_14_170.png) denote the inverse
    CDF of the distributions *U* and *V* respectively. Thus, minimizing the distance
    between two inverse CDFs implies that we minimize the Wasserstein distance.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: We learned that in QR-DQN, we train the network by minimizing the distance between
    the predicted and target distribution, and both of them are quantile functions
    (inverse CDF). Thus, minimizing the distance between the predicted and target
    distribution (inverse CDFs) implies that we minimize the Wasserstein distance.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the QR-DQN paper (see the *Further reading* section for more
    details) also highlighted that instead of computing the support for the quantile
    values ![](img/B15558_14_157.png), they suggest using the quantile midpoint values
    ![](img/B15558_14_172.png). The quantile midpoint can be computed as:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_173.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
- en: That is, the value of the support *z* can be obtained using quantile midpoint
    values as ![](img/B15558_14_174.png) instead of obtaining support using the quantile
    values as ![](img/B15558_14_175.png).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'But why the quantile midpoint? The quantile midpoint acts as a unique minimizer,
    that is, the Wasserstein distance between two inverse CDFs will be less when we
    use quantile midpoint values ![](img/B15558_14_176.png) instead of quantile values
    ![](img/B15558_14_157.png). Since we are trying to minimize the Wasserstein distance
    between the target and predicted distribution, we can use quantile midpoints ![](img/B15558_14_178.png)
    so that the distance between them will be less. For instance, as *Figure 14.24*
    shows, the Wasserstein distance is less when we use the quantile midpoint values
    ![](img/B15558_14_179.png) instead of quantile values ![](img/B15558_14_164.png):'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_24.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.24: Using quantile midpoint values instead of quantile values'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Source ([https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf))
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, in QR-DQNs, we compute the value distribution as a quantile function.
    So, we just feed the cumulative probabilities that are equally divided probabilities
    into the network and obtain the support values of the distribution and we train
    the network by minimizing the Wasserstein distance between the target and predicted
    distribution.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: Action selection
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Action selection in QR-DQN is just the same as in C51\. First, we extract Q
    value from the predicted value distribution and then we select the action as the
    one that has the maximum Q value. We can extract the Q value by just taking the
    expectation of the value distribution. The expectation of distribution is given
    as a sum of support multiplied by their corresponding probability.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'In C51, we computed the Q value as:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_027.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
- en: Where *p*[i](*s*, *a*) is the probability given by the network for state *s*
    and action *a* and *z*[i] is the support.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'Whereas in a QR-DQN, our network outputs the support instead of the probability.
    So, the Q value in the QR-DQN can be computed as:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_182.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
- en: Where *z*[i](*s*, *a*) is the support given by the network for state *s* and
    action *a* and *p*[i] is the probability.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the Q value, we select the action that has the maximum Q value.
    For instance, let''s say, we have a state *s* and two actions in the state, let
    them be *up* and *down*.The Q value for action *up* in the state *s* is computed
    as:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_183.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
- en: 'The Q value for action *down* in the state *s* is computed as:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_184.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q value, we select the optimal action as the one that has
    the maximum Q value:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_185.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
- en: Now that we have learned how to select actions in QR-DQN, in the next section,
    we will look into the loss function of QR-DQN.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In C51, we used cross entropy loss as our loss function because our network
    predicts the probability of the value distribution. So we used the cross entropy
    loss to minimize the probabilities between the target and predicted distribution.
    But in QR-DQN, we predict the support of the distribution instead of the probabilities.
    That is, in QR-DQN, we feed the probabilities as input and predict the support
    as output. So, how can we define the loss function for a QR-DQN?
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: We can use the quantile regression loss to minimize the distance between the
    target support and the predicted support. But first, let's understand how to calculate
    the target support value.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going ahead, let''s recall how we compute the target value in a DQN.
    In DQN, we use the Bellman equation and compute the target value as:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_186.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, we select action ![](img/B15558_14_187.png) by taking
    the maximum Q value over all possible next state-action pairs.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in QR-DQN, to compute the target value, we can use the distributional
    Bellman equation. The distributional Bellman equation can be given as:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_188.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
- en: 'So, the target support *z*[j] can be computed as:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_189.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
- en: 'To compute support *z*[j] for the state ![](img/B15558_14_036.png), we also
    need to select some action ![](img/B15558_14_132.png). How can we select an action?
    We just compute the return distribution of all next state-action pairs using the
    target network and select the action ![](img/B15558_14_192.png) that has the maximum
    Q value:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_193.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
- en: Now that we have learned how to compute the target support value, let's see
    how to compute the quantile regression loss. The advantage of using the quantile
    regression loss is that it adds a penalty to the overestimation and underestimation
    error. Let's understand this with an example.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Let's say the target support value is [1, 5, 10, 15, 20] and the predicted support
    value is [100, 5, 10, 15, 20]. As we can see, our predicted support has a very
    high value in the initial quantile and then it is decreasing. In the inverse CDF
    section, we learned that support should always be increasing as it is based on
    the cumulative probability. But if you look at the predicted values the support
    starts from 100 and then decreases.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider another case. Suppose the target support value is [1, 5, 10,
    15, 20] and the predicted support value is [1, 5, 10, 15, 4]. As we can see, our
    predicted support value is increasing from the initial quantile and then it is
    decreasing to 4 in the final quantile. But this should not happen. Since we are
    using inverse CDF, our support values should always be increasing.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we need to make sure that our support should be increasing and not decreasing.
    So, if the initial quantile values are overestimated with high values and if the
    later quantile values are underestimated with low values, we can penalize them.
    That is, we multiply the overestimated value by ![](img/B15558_14_163.png) and
    the underestimated value by ![](img/B15558_14_195.png). Okay, how can we determine
    if the value is overestimated or underestimated?
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: First, we compute the difference between the target and the predicted value.
    Let *u* be the difference between the target support value and the predicted support
    value. Then, if the value of *u* is less than 0, we multiply *u* by ![](img/B15558_14_196.png),
    else we multiply *u* by ![](img/B15558_10_071.png). This is known as **quantile
    regression loss**.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: But the problem with quantile regression loss is that it will not be smooth
    at 0 and it makes the gradient stay constant. So, instead of using quantile regression
    loss, we use a new modified version of loss called quantile Huber loss.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how exactly quantile Huber loss works, first, let''s look into
    the Huber loss. Let''s denote the difference between our actual and predicted
    values as *u*. Then the Huber loss ![](img/B15558_14_198.png) can be given as:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_199.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
- en: Let ![](img/B15558_14_200.png), then, when the absolute value, ![](img/B15558_14_201.png),
    is less than or equal to ![](img/B15558_14_202.png), the Huber loss is given as
    the quadratic loss, ![](img/B15558_14_203.png), else it is a linear loss, ![](img/B15558_14_204.png).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python snippet helps us to understand Huber loss better:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Now that we have understood what the Huber loss ![](img/B15558_14_198.png) is,
    let's look into the quantile Huber loss. In the quantile Huber loss, when the
    value of *u* (the difference between target and predicted support) is less than
    0, then we multiply the Huber loss ![](img/B15558_14_198.png) by ![](img/B15558_14_207.png),
    and when the value of *u* is greater than or equal to 0, we multiply the Huber
    loss ![](img/B15558_14_198.png) by ![](img/B15558_14_157.png).
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how a QR-DQN works, in the next section, we will
    look into another interesting algorithm called D4PG.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Distributional DDPG
  id: totrans-494
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**D4PG**, which stands for **D**istributed **D**istributional **D**eep **D**eterministic
    **P**olicy **G**radient, is one of the most interesting policy gradient algorithms.
    We can make a guess about how D4PG works just by its name. As the name suggests,
    D4PG is basically a combination of **deep deterministic policy gradient** (**DDPG**)
    and distributional reinforcement learning, and it works in a distributed fashion.
    Confused? Let''s go deeper and understand how D4PG works in detail.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: To understand how D4PG works, it is highly recommended to revise the DDPG algorithm
    we covered in *Chapter 12*, *Learning DDPG, TD3, and SAC*. We learned that DDPG
    is an actor critic method where the actor tries to learn the policy while the
    critic tries to evaluate the policy produced by the actor using the Q function.
    The critic uses the deep Q network for estimating the Q function and the actor
    uses the policy network for computing the policy. Thus, the actor performs an
    action while the critic gives feedback to the action performed by the actor and,
    based on the critic feedback, the actor network will be updated.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: D4PG works just like DDPG but in the critic network, instead of using a DQN
    for estimating the Q function, we can use our distributional DQN to estimate the
    value distribution. That is, in the previous sections, we have learned several
    distributional DQN algorithms, such as C51 and QR-DQN. So, in the critic network,
    instead of using a regular DQN, we can use any distributional DQN algorithm, say
    C51.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from this, D4PG also proposes several changes to the DDPG architecture.
    So, we will get into the details and learn how exactly D4PG differs from DDPG.
    Before going ahead, let''s be clear with the notation:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: The policy network parameter is represented by ![](img/B15558_13_234.png) and
    the target policy network parameter is represented by ![](img/B15558_12_210.png).
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The critic network parameter is represented by ![](img/B15558_09_098.png) and
    the target critic network parameter is represented by ![](img/B15558_14_039.png).
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we are talking about a deterministic policy, let's represent it by ![](img/B15558_14_214.png),
    and our policy is parameterized by the policy network, so we can denote the policy
    by ![](img/B15558_14_215.png).
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will understand how exactly the critic and actor network in D4PG works.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: Critic network
  id: totrans-503
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In DDPG, we learned that we use the critic network to estimate the Q function.
    Thus, given a state and action, the critic network estimates the Q function as
    ![](img/B15558_14_216.png). To train the critic network we minimize the MSE between
    the target Q value given by the Bellman optimality equation and the Q value predicted
    by the network.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: 'The target value in DDPG is computed as:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_217.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
- en: 'Once we compute the target value, we compute the loss as the MSE between the
    target value and the predicted value as:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_047.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
- en: 'Where *K* denotes the number of transitions randomly sampled from the replay
    buffer. After computing the loss, we compute the gradients ![](img/B15558_14_219.png)
    and update the critic network parameter using gradient descent:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_052.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
- en: Now, let's talk about the critic in D4PG. As we learned in D4PG, we use the
    distributional DQN to estimate the Q value. Thus, given a state and action, the
    critic network estimates the value distribution as ![](img/B15558_14_221.png).
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: To train the critic network, we minimize the distance between the target value
    distribution given by the distributional Bellman equation and the value distribution
    predicted by the network.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: 'The target value distribution in D4PG is computed as:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_222.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
- en: 'As you can observe, equation (2) is similar to (1) except that we just replaced
    *Q* with *Z*, indicating that we are computing the target value distribution.
    D4PG proposes one more change to the target value computation (2). Instead of
    using the one-step return *r*, we use the **N-step return**, and it can be expressed
    as:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_223.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
- en: Where *N* is the length of the transition, which we sample from the replay buffer.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the target value distribution, we can compute the distance
    between the target value distribution and the predicted value distribution as:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_224.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
- en: 'Where *d* denotes any distance measure for measuring the distance between two
    distributions. Say we are using C51, then *d* denotes the cross entropy and *K*
    denotes the number of transitions sampled from the replay buffer. After computing
    the loss, we calculate the gradients and update the critic network parameter.
    The gradients can be computed as:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_225.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
- en: D4PG proposes a small change to our gradient updates. In D4PG, we use a **prioritized
    experience replay.** Let's say we have an experience replay buffer of size *R*.
    Each transition in the replay buffer will have a non-uniform probability *p*[i].
    The non-uniform probability helps us to give more importance to one transition
    than the other. Say we have a sample *i*, then its probability can be given as
    ![](img/B15558_14_226.png) or ![](img/B15558_14_227.png). While updating the critic
    network, we weight the updates using ![](img/B15558_14_228.png), which gives importance
    to the updates.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus our gradient computation becomes:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_229.png)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
- en: After computing the gradient, we can update the critic network parameter using
    gradient descent as ![](img/B15558_12_052.png). Now that we have understood how
    the critic network works in D4PG, let's look into the actor network in the next
    section.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: Actor network
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s quickly recap how the actor network in DDPG works. In DDPG, we
    learned that the actor network takes the state as input and returns the action:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_231.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
- en: Note that we are using the deterministic policy in the continuous action space,
    and to explore new actions we just add some noise ![](img/B15558_14_232.png) to
    the action produced by the actor network since the action is a continuous value.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our modified action can be represented as:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_233.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the objective function of the actor is to generate an action that maximizes
    the Q value produced by the citric network:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_234.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_14_235.png).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: We learned that to maximize the objective, we compute the gradients of our objective
    function ![](img/B15558_11_014.png) and update the actor network parameter by
    performing gradient ascent.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s come to D4PG. In D4PG we perform the same steps with a little difference.
    Note that here we are not using the Q function in the critic. Instead, we are
    computing the value distribution and thus our objective function becomes:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_237.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
- en: 'Where the action, ![](img/B15558_14_235.png) and just like we saw in DDPG,
    to maximize the objective, first, we compute the gradients of our objective function
    ![](img/B15558_14_238.png). After computing the gradients we update the actor
    network parameter by performing gradient ascent:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_068.png)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
- en: We learned that D4PG is a **distributed** algorithm, meaning that instead of
    using one actor, we use *L* **number of actors**, each of which acts parallel
    and is independent of the environment, collects experience, and stores the experience
    in the replay buffer. Then we update the network parameter to the actors periodically.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to summarize, D4PG is similar to DDPG except for the following:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: We use the distributional DQN in the critic network instead of using the regular
    DQN to estimate the Q values.
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate *N*-step returns in the target instead of calculating the one-step
    return.
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a prioritized experience replay and add importance to the gradient update
    in the critic network.
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of using one actor, we use **L** independent actors, each of which acts
    in parallel, collects experience, and stores the experience in the replay buffer.
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have understood how D4PG works, putting together all the concepts
    we have learned, let's look into the algorithm of D4PG in the next section.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – D4PG
  id: totrans-547
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let ![](img/B15558_14_240.png) denote the time steps at which we want to update
    the target critic and actor network parameters. We set ![](img/B15558_14_241.png),
    which states that we update the target critic network and target actor network
    parameter for every 2 steps of the episode. Similarly, let ![](img/B15558_14_242.png)denote
    thetime steps at which we want to replicate the network weights to the **L** actors.
    We set ![](img/B15558_14_243.png), which states that we replicate the network
    weights to the actors on every 2 steps of the episode.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm of D4PG is given as follows:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the critic network parameter ![](img/B15558_09_056.png) and actor
    network parameter ![](img/B15558_14_245.png)
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize target critic network parameter ![](img/B15558_14_246.png) and target
    actor network parameter ![](img/B15558_14_247.png) by copying from ![](img/B15558_09_054.png)
    and ![](img/B15558_14_249.png) respectively
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_14_098.png)
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the *L* number of actors
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *step 6*
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for ![](img/B15558_14_101.png):'
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_266.png)
  id: totrans-556
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value distribution of the critic, that is,![](img/B15558_14_253.png)
  id: totrans-557
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network and calculate the gradient as ![](img/B15558_14_229.png)
  id: totrans-558
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After computing the gradients, update the critic network parameter using gradient
    descent: ![](img/B15558_14_255.png)'
  id: totrans-559
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the actor network ![](img/B15558_11_014.png)
  id: totrans-560
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the actor network parameter by gradient ascent:![](img/B15558_14_257.png)
  id: totrans-561
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod ![](img/B15558_14_258.png), then:'
  id: totrans-562
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic and target actor network parameter using soft replacement
    as ![](img/B15558_14_259.png) and ![](img/B15558_14_260.png) respectively
  id: totrans-563
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If *t* mod ![](img/B15558_14_261.png), then:'
  id: totrans-564
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Replicate the network weights to the actors
  id: totrans-565
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'And we perform the following steps in the actor network:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Select action *a* based on the policy ![](img/B15558_14_262.png) and exploration
    noise, that is,![](img/B15558_14_233.png)
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_14_264.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_092.png)
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 1* to *2* until the learner finishes
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, we have learned how D4PG works.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-571
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by understanding how distributional reinforcement learning
    works. We learned that in distributional reinforcement learning, instead of selecting
    an action based on the expected return, we select the action based on the distribution
    of return, which is often called the value distribution or return distribution.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned about the categorical DQN algorithm, also known as C51, where
    we feed the state and support of the distribution as the input and the network
    returns the probabilities of the value distribution. We also learned how the projection
    step matches the support of the target and predicted the value distribution so
    that we can apply the cross entropy loss.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: Going ahead, we learned about quantile regression DQNs, where we feed the state
    and also the equally divided cumulative probabilities ![](img/B15558_14_157.png)
    as input to the network and it returns the support value of the distribution.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about how D4PG works, and we also learned
    how it varies from DDPG.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-576
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s test our knowledge of distributional reinforcement learning by answering
    the following questions:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: What is distributional reinforcement learning?
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a categorical DQN?
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the categorical DQN called the C51 algorithm?
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the quantile function?
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a QR-DQN differ from a categorical DQN?
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does D4PG differ from DDPG?
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-584
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, refer to the following papers:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: '**A Distributional Perspective on Reinforcement Learning** by *Marc G. Bellemare*,
    *Will Dabney*, *Remi Munos*, [https://arxiv.org/pdf/1707.06887.pdf](https://arxiv.org/pdf/1707.06887.pdf)'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributional Reinforcement Learning with Quantile Regression** by *Will
    Dabney*, *Mark Rowland*, *Marc G. Bellemare*, *Rémi Munos*, [https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf)'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed Distributional Deep Deterministic Policy Gradient** by *Gabriel
    Barth-Maron*, *et al*., [https://arxiv.org/pdf/1804.08617.pdf](https://arxiv.org/pdf/1804.08617.pdf)'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
