- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributional Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about distributional reinforcement learning.
    We will begin the chapter by understanding what exactly distributional reinforcement
    learning is and why it is useful. Next, we will learn about one of the most popular
    distributional reinforcement learning algorithms called **categorical DQN**. We
    will understand what a categorical DQN is and how it differs from the DQN we learned
    in *Chapter 9*, *Deep Q Networks and Its Variants*, and then we will explore the
    categorical DQN algorithm in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we will learn another interesting algorithm called **Quantile
    Regression DQN** (**QR-DQN**). We will understand what a QR-DQN is and how it
    differs from a categorical DQN, and then we will explore the QR-DQN algorithm
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we will learn about the policy gradient algorithm
    called the **Distributed Distributional Deep Deterministic Policy Gradient** (**D4PG**).
    We will learn what the D4PG is and how it differs from the DDPG we covered in *Chapter
    12*, *Learning DDPG, TD3, and SAC*, in detail
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why distributional reinforcement learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile regression DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed distributional deep deterministic policy gradient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin the chapter by understanding what distributional reinforcement learning
    is and why we need it.
  prefs: []
  type: TYPE_NORMAL
- en: Why distributional reinforcement learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Say we are in state *s* and we have two possible actions to perform in this
    state. Let the actions be *up* and *down*. How do we decide which action to perform
    in the state? We compute Q values for all actions in the state and select the
    action that has the maximum Q value. So, we compute *Q*(*s*, up) and *Q*(*s*,
    down) and select the action that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that the Q value is the expected return an agent would obtain when
    starting from state *s* and performing an action *a* following the policy ![](img/B15558_14_001.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_002.png)'
  prefs: []
  type: TYPE_IMG
- en: But there is a small problem in computing the Q value in this manner because
    the Q value is just an expectation of the return, and the expectation does not
    include the intrinsic randomness. Let's understand exactly what this means with
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose we want to drive from work to home and we have two routes **A**
    and **B**. Now, we have to decide which route is better, that is, which route
    helps us to reach home in the minimum amount of time. To find out which route
    is better, we can calculate the Q values and select the route that has the maximum
    Q value, that is, the route that gives us the maximum expected return.
  prefs: []
  type: TYPE_NORMAL
- en: Say the Q value of choosing route *A* is *Q*(*s*, *A*) = 31, and the Q value
    of choosing route *B* is *Q*(*s*, *B*) = 28\. Since the Q value (the expected
    return of route **A**) is higher, we can choose route **A** to travel home. But
    are we missing something here? Instead of viewing the Q value as an expectation
    over a return, can we directly look into the distribution of return and make a
    better decision?
  prefs: []
  type: TYPE_NORMAL
- en: Yes!
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, let''s take a look at the distribution of route **A** and route
    **B** and understand which route is best. The following plot shows the distribution
    of route **A**. It tells us with 70% probability we reach home in 10 minutes,
    and with 30% probability we reach home in 80 minutes. That is, if we choose route
    **A** we usually reach home in 10 minutes but when there is heavy traffic we reach
    home in 80 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: Distribution of route A'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14.2* shows the distribution of route **B**. It tells us that with
    80% probability we reach home in 20 minutes and with 20% probability we reach
    home in 60 minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, if we choose route **B** we usually reach home in 20 minutes but when
    there is heavy traffic we reach home in 60 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: Distribution of route B'
  prefs: []
  type: TYPE_NORMAL
- en: After looking at these two distributions, it makes more sense to choose route
    **B** instead of choosing route **A**. With route **B**, even in the worst case,
    that is, even when there is heavy traffic, we can reach home in 60 minutes. But
    with route **A**, when there is heavy traffic, we reach home in 80 minutes. So,
    it is a wise decision to choose route **B** rather than **A**.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if we can observe the distribution of return of route **A** and route
    **B**, we can understand more information and we will miss out on these details
    when we take actions just based on the maximum expected return, that is, the maximum
    Q value. So, instead of using the expected return to select an action, we use
    the distribution of return and then select optimal action based on the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: This is the basic idea and motivation behind distributional reinforcement learning.
    In the next section, we will learn one of the most popular distributional reinforcement
    learning algorithms, called categorical DQN, which is also known as the C51 algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we learned why it is more beneficial to choose an action
    based on the distribution of return than to choose an action based on the Q value,
    which is just the expected return. In this section, we will understand how to
    compute the distribution of return using an algorithm called categorical DQN.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of return is often called the value distribution or return
    distribution. Let *Z* be the random variable and *Z*(*s*, *a*) denote the value
    distribution of a state *s* and an action *a*. We know that the Q function is
    represented by *Q*(*s*, *a*) and it gives the value of a state-action pair. Similarly,
    now we have *Z*(*s*, *a*) and it gives the value distribution (return distribution)
    of the state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we compute *Z*(*s*, *a*)? First, let's recollect how we compute
    *Q*(*s*, *a*).
  prefs: []
  type: TYPE_NORMAL
- en: In DQN, we learned that we use a neural network to approximate the Q function,
    *Q*(*s*, *a),* Since we use a neural network to approximate the Q function, we
    can represent the Q function by ![](img/B15558_12_331.png), where ![](img/B15558_14_004.png)
    is the parameter of the network. Given a state as an input to the network, it
    outputs the Q values of all the actions that can be performed in that state, and
    then we select the action that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in categorical DQN, we use a neural network to approximate the value
    of *Z*(*s*, *a*). We can represent this by ![](img/B15558_14_005.png), where ![](img/B15558_09_087.png)
    is the parameter of the network. Given a state as an input to the network, it
    outputs the value distribution (return distribution) of all the actions that can
    be performed in that state as an output and then we select an action based on
    this value distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the difference between the DQN and categorical DQN with an
    example. Suppose we are in the state *s* and say our action space has two actions
    *a* and *b*. Now, as shown in *Figure 14.3*, given the state *s* as an input to
    the DQN, it returns the Q value of all the actions, then we select the action
    that has the maximum Q value, whereas in the categorical DQN, given the state
    *s* as an input, it returns the value distribution of all the actions, then we
    select the action based on this value distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: DQN vs categorical DQN'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we train the network? In DQN, we learned that we train the network
    by minimizing the loss between the target Q value and the Q value predicted by
    the network. We learned that the target Q value is obtained by the Bellman optimality
    equation. Thus, we minimize the loss between the target value (the optimal Bellman
    Q value) and the predicted value (the Q value predicted by the network) and train
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in categorical DQN, we train the network by minimizing the loss between
    the target value distribution and the value distribution predicted by the network.
    Okay, how can we obtain the target value distribution? In DQN, we obtained the
    target Q value using the Bellman equation; similarly in categorical DQN, we can
    obtain the target value distribution using the distributional Bellman equation.
    What's the distributional Bellman equation? First, let's recall the Bellman equation
    before learning about the distributional Bellman equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that the Bellman equation for the Q function *Q*(*s*, *a*) is given
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the Bellman equation for the value distribution *Z*(*s*, *a*) is
    given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_008.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation is called the distributional Bellman equation. Thus, in categorical
    DQN, we train the network by minimizing the loss between the target value distribution,
    which is given by the distributional Bellman equation, and the value distribution
    predicted by the network.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what loss function should we use? In DQN, we use the **mean squared error**
    (**MSE**) as our loss function. Unlike a DQN, we cannot use the MSE as the loss
    function in the categorical DQN because in categorical DQN, we predict the probability
    distribution and not the Q value. Since we are dealing with the distribution we
    use the cross entropy loss as our loss function. Thus, in categorical DQN, we
    train the network by minimizing the cross entropy loss between the target value
    distribution and the value distribution predicted by the network.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, a categorical DQN is similar to DQN, except that in a categorical
    DQN, we predict the value distribution whereas in a DQN we predict the Q value.
    Thus, given a state as an input, a categorical DQN returns the value distribution
    of each action in that state. We train the network by minimizing the cross entropy
    loss between the target value distribution, which is given by the distributional
    Bellman equation, and the value distribution predicted by the network.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood what a categorical DQN is and how it differs from
    a DQN, in the next section we will learn how exactly the categorical DQN predicts
    the value distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the value distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 14.4* shows a simple value distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: Value distribution'
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal axis values are called support or atoms and the vertical axis
    values are the probability. We denote the support by *Z* and the probability by
    *P*. In order to predict the value distribution, along with the state, our network
    takes the support of the distribution as input and it returns the probability
    of each value in the support.
  prefs: []
  type: TYPE_NORMAL
- en: So, now, we will see how to compute the support of the distribution. To compute
    support, first, we need to decide the number of values of the support *N*, the
    minimum value of the support ![](img/B15558_14_009.png), and the maximum value
    of the support ![](img/B15558_14_010.png). Given a number of support *N*, we divide
    them into *N* equal parts from ![](img/B15558_14_009.png) to ![](img/B15558_14_012.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this with an example. Say the number of support *N* = 5,
    the minimum value of support ![](img/B15558_14_013.png), and the maximum value
    of the support ![](img/B15558_14_014.png). Now, how can we find the values of
    the support? In order to find the values of the support, first, we will compute
    the step size called ![](img/B15558_14_015.png). The value of ![](img/B15558_14_016.png)
    can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_017.png)![](img/B15558_14_018.png)![](img/B15558_14_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, to compute the values of support, we start with the minimum value of support
    ![](img/B15558_14_009.png) and add ![](img/B15558_14_021.png) to every value until
    we reach the number of support *N*. In our example, we start with ![](img/B15558_14_022.png),
    which is 2, and we add ![](img/B15558_14_023.png) to every value until we reach
    the number of support *N*. Thus, the support values become:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can write the value of support as ![](img/B15558_14_025.png). The
    following Python snippet gives us more clarity on how to obtain the support values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Okay, we have learned how to compute the support of the distribution, now how
    does the neural network take this support as input and return the probabilities?
  prefs: []
  type: TYPE_NORMAL
- en: In order to predict the value distribution, along with the state, we also need
    to give the support of the distribution as input and then the network returns
    the probabilities of our value distribution as output. Let's understand this with
    an example. Say we are in a state *s* and we have two actions to perform in this
    state, and let the actions be *up* and *down*. Say our calculated support values
    are *z*[1], *z*[2], and *z*[3].
  prefs: []
  type: TYPE_NORMAL
- en: 'As *Figure 14.5* shows, along with giving the state *s* as input to the network,
    we also give the support of our distribution *z*[1], *z*[2], and *z*[3]. Then
    our network returns the probabilities *p*[i](*s*, *a*) of the given support for
    the distribution of action *up* and distribution of action *down*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: A categorical DQN'
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the categorical DQN paper (see the *Further reading* section
    for more details) suggest that it will be efficient to set the number of support
    *N* as 51, and so the categorical DQN is also known as the C51 algorithm. Thus,
    we have learned how categorical DQN predicts the value distribution. In the next
    section, we will learn how to select the action based on this predicted value
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an action based on the value distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have learned that a categorical DQN returns the value distribution of each
    action in the given state. But how can we select the best action based on the
    value distribution predicted by the network?
  prefs: []
  type: TYPE_NORMAL
- en: We generally select an action based on the Q value, that is, we usually select
    the action that has the maximum Q value. But now we don't have a Q value; instead,
    we have a value distribution. How can we select an action based on the value distribution?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will extract the Q value from the value distribution and then we
    select the action as the one that has the maximum Q value. Okay, how can we extract
    the Q value? We can compute the Q value by just taking the expectation of the
    value distribution. The expectation of the distribution is given as the sum of
    support *z*[i] multiplied by their corresponding probability *p*[i]. So the expectation
    of the value distribution *Z* is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_026.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *z*[i] is the support and *p*[i] is the probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the Q value of the value distribution can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q value, we select the best action as the one that has
    the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_028.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's understand how this works exactly. Suppose we are in the state *s* and
    say we have two actions in the state. Let the actions be *up* and *down*. First,
    we need to compute support. Let the number of support *N* = 3, the minimum value
    of the support ![](img/B15558_14_013.png), and the maximum value of the support
    ![](img/B15558_14_030.png). Then, our computed support values will be [2,3,4].
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, along with the state *s*, we feed the support, then the categorical DQN
    returns the probabilities *p*[i](*s*, *a*) of the given support for the value
    distribution of action *up* and distribution of action *down* as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Categorical DQN'
  prefs: []
  type: TYPE_NORMAL
- en: Now, how can we select the best action, based on these two value distributions?
    First, we will extract the Q value from the value distributions and then we select
    the action that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that the Q value can be extracted from the value distribution as
    the sum of support multiplied by their probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can compute the Q value of action *up* in state *s* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can compute the Q value of action *down* in state *s* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_033.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we select the action that has the maximum Q value. Since the action *up*
    has the high Q value, we select the action *up* as the best action.
  prefs: []
  type: TYPE_NORMAL
- en: Wait! What makes categorical DQN special then? Because just like DQN, we are
    selecting the action based on the Q value at the end. One important point we have
    to note is that, in DQN, we compute the Q value based on the expectation of the
    return directly, but in categorical DQN, first, we learn the return distribution
    and then we compute the Q value based on the expectation of the return distribution,
    which captures the intrinsic randomness.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned that the categorical DQN outputs the value distribution of all
    the actions in the given state and then we extract the Q value from the value
    distribution and select the action that has the maximum Q value as the best action.
    But the question is how exactly does our categorical DQN learn? How do we train
    the categorical DQN to predict the accurate value distribution? Let's discuss
    this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training the categorical DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We train the categorical DQN by minimizing the cross entropy loss between the
    target value distribution and the predicted value distribution. How can we compute
    the target distribution? We can compute the target distribution using the distributional
    Bellman equation given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/B15558_14_035.png) represents the immediate reward *r*, which
    we obtain while performing an action *a* in the state *s* and moving to the next
    state ![](img/B15558_14_036.png), so we can just denote ![](img/B15558_14_035.png)
    by *r*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_038.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember in DQN we computed the target value using the target network parameterized
    by ![](img/B15558_14_039.png)? Similarly, here, we use the target categorical
    DQN parameterized by ![](img/B15558_14_040.png) to compute the target distribution.
  prefs: []
  type: TYPE_NORMAL
- en: After computing the target distribution, we train the network by minimizing
    the cross entropy loss between the target value distribution and the predicted
    value distribution. One important point we need to note here is that we can apply
    the cross entropy loss between any two distributions only when their supports
    are equal; when their supports are not equal we cannot apply the cross entropy
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, *Figure 14.7* shows the support of both the target and predicted
    distribution is the same, (1,2,3,4). Thus, in this case, we can apply the cross
    entropy loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Target and predicted distribution'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14.8*, we can see that the target distribution support (1,3,4,5)
    and the predicted distribution support (1,2,3,4) are different, so in this case,
    we cannot apply the cross entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: Target and predicted distribution'
  prefs: []
  type: TYPE_NORMAL
- en: So, when the support of the target and prediction distribution is different,
    we perform a special step called the projection step using which we can make the
    support of the target and prediction distribution equal. Once we make the support
    of the target and prediction distribution equal then we can apply the cross entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how exactly the projection works and how
    it makes the support of the target and prediction distribution equal.
  prefs: []
  type: TYPE_NORMAL
- en: Projection step
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's understand how exactly the projection step works with an example. Suppose
    the input support is *z* = [1, 2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the probability of predicted distribution be *p* = [0.5, 0.5]. *Figure
    14.9* shows the predicted distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Predicted distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the probability of target distribution be *p* = [0.3, 0.7]. Let the reward
    *r* = 0.1 and the discount factor ![](img/B15558_14_041.png). The target distribution
    support value is computed as![](img/B15558_14_042.png), so, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_043.png)![](img/B15558_14_044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the target distribution becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Target distribution'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from the preceding plots, the supports of the predicted distribution
    and target distribution are different. The predicted distribution has the support
    [1, 2] while the target distribution has the support [1, 1.9], so in this case,
    we cannot apply the cross entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Now, using the projection step we can convert the support of our target distribution
    to be the same support as the predicted distribution. Once the supports of the
    predicted and target distribution are the same then we can apply the cross entropy
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what's that projection step exactly? How can we apply it and convert the
    support of the target distribution to match the support of the predicted distribution?
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this with the same example. As the following shows, we have
    the target distribution support [1, 1.9] and we need to make it equal to the predicted
    distribution support [1, 2], how can we do that?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Target distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what we can do is that we can distribute the probability 0.7 from the support
    1.9 to the support 1 and 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Target distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, but how can we distribute the probabilities from the support 1.9 to the
    support 1 and 2? Should it be an equal distribution? Of course not. Since 2 is
    closer to 1.9, we distribute more probability to 2 and less to 1\.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 14.13*, from 0.7, we will distribute 0.63 to support 2 and
    0.07 to support 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: Target distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, now our target distribution will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: Target distribution'
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 14.14*, we can see that support of the target distribution is changed
    from [1, 1.9] to [1, 2] and now it matches the support of the predicted distribution.
    This step is called the projection step.
  prefs: []
  type: TYPE_NORMAL
- en: What we learned is just a simple example, consider a case where our target and
    predicted distribution support varies very much. In this case, we cannot manually
    determine the amount of probability we have to distribute across the supports
    to make them equal. So, we introduce a set of steps to perform the projection,
    as the following shows. After performing these steps, our target distribution
    support will match our predicted distribution by distributing the probabilities
    across the support.
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize an array *m* with its shape as the number of support with
    zero values. The *m* denotes the distributed probability of the target distribution
    after the projection step.
  prefs: []
  type: TYPE_NORMAL
- en: 'For *j* in range of the number of support:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the target support value: ![](img/B15558_14_045.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the value of *b*: ![](img/B15558_14_046.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the lower bound and the upper bound: ![](img/B15558_14_047.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_048.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_049.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding how exactly these projection steps work is a little tricky! So,
    let's understand this by considering the same example we used earlier. Let z =
    [1, 2], *N* = 2, ![](img/B15558_14_050.png), and ![](img/B15558_14_051.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the probability of predicted distribution be *p* = [0.5, 0.5]. *Figure
    14.15* shows the predicted distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: Predicted distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the probability of target distribution be *p* = [0.3, 0.7]. Let the reward
    *r* = 0.1 and the discount factor ![](img/B15558_14_041.png), and we know ![](img/B15558_14_053.png),
    thus, the target distribution becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.16: Target distribution'
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 14.16*, we can infer that support in target distribution is different
    from the predicted distribution. Now, we will learn how to perform the projection
    using the preceding steps.
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize an array *m* with its shape as the number of support with
    zero values. Thus, *m* = [0, 0].
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration, j=0**:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the target support value:![](img/B15558_14_054.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of *b*:![](img/B15558_14_055.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the lower and upper bound:![](img/B15558_14_056.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the probability on the lower bound:![](img/B15558_14_057.png)![](img/B15558_14_058.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the probability on the upper bound:![](img/B15558_14_059.png)![](img/B15558_14_060.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the 1st iteration, the value of *m* becomes [0, 0].
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration, j=1**:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the target support value:![](img/B15558_14_061.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of *b*:![](img/B15558_14_062.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the lower and upper bound of *b*:![](img/B15558_14_063.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the probability on the lower bound:![](img/B15558_14_064.png)![](img/B15558_14_065.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the probability on the upper bound:![](img/B15558_14_066.png)![](img/B15558_14_067.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the second iteration, the value of *m* becomes [0.07, 0,63]. The number
    of iterations = the length of our support. Since the length of our support is
    2, we will stop here and thus the value of *m* becomes our new distributed probability
    for the modified support, as *Figure 14.17* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.17: Target distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet will give us more clarity on how exactly the projection
    step works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have understood how to compute the target value distribution and
    how we can make the support of target value distribution equal to the support
    of predicted value distribution using the projection step, we will learn how to
    compute the cross entropy loss. Cross entropy loss is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_068.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *y* is the actual value and ![](img/B15558_14_069.png) is the predicted
    value. Thus, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_070.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *m* is the target probabilities from the target value distribution and
    *p*(*s*, *a*) is the predicted probabilities from the predicted value distribution.
    We train our network by minimizing the cross entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using a categorical DQN, we select the action based on the distribution
    of the return (value distribution). In the next section, we will put all these
    concepts together and see how a categorical DQN works.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we initialize the main network parameter ![](img/B15558_09_106.png) with
    random values, and we initialize the target network parameter ![](img/B15558_14_039.png)
    by just copying the main network parameter ![](img/B15558_09_054.png). We also
    initialize the replay buffer ![](img/B15558_09_124.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each step in the episode, we feed the state of the environment and
    support values to the main categorical DQN parameterized by ![](img/B15558_09_098.png).
    The main network takes the support and state of the environment as input and returns
    the probability value for each support. Then the Q value of the value distribution
    can be computed as the sum of support multiplied by their probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_075.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q value of all the actions in the state, we select the
    best action in the state *s* as the one that has the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_028.png)'
  prefs: []
  type: TYPE_IMG
- en: However, instead of selecting the action that has the maximum Q value all the
    time, we select the action using the epsilon-greedy policy. With the epsilon-greedy
    policy, we select a random action with probability epsilon and with the probability
    1-epsilon, we select the best action that has the maximum Q value. We perform
    the selected action, move to the next state, obtain the reward, and store this
    transition information ![](img/B15558_14_077.png) in the replay buffer ![](img/B15558_09_124.png).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we sample a transition ![](img/B15558_14_077.png) from the replay buffer
    ![](img/B15558_09_124.png) and feed the next state ![](img/B15558_12_376.png)
    and support values to the target categorical DQN parameterized by ![](img/B15558_12_025.png).
    The target network takes the support and next state ![](img/B15558_12_376.png)
    as input and returns the probability value for each support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the Q value can be computed as the sum of support multiplied by their
    probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_082.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q value of all next state-action pairs, we select the best
    action in the state ![](img/B15558_14_083.png) as the one that has the maximum
    Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_084.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we perform the projection step. The *m* denotes the distributed probability
    of the target distribution after the projection step.
  prefs: []
  type: TYPE_NORMAL
- en: 'For *j* in range of the number of support:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the target support value: ![](img/B15558_14_085.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the value of *b*: ![](img/B15558_14_046.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the lower bound and the upper bound: ![](img/B15558_14_087.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_088.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_089.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After performing the projection step, compute the cross entropy loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_090.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *m* is the target probabilities from the target value distribution and
    *p*(*s*, *a*) is the predicted probabilities from the predicted value distribution.
    We train our network by minimizing the cross entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: We don't update the target network parameter ![](img/B15558_14_040.png) in every
    time step. We freeze the target network parameter ![](img/B15558_14_040.png) for
    several time steps, and then we copy the main network parameter ![](img/B15558_10_037.png)
    to the target network parameter ![](img/B15558_14_039.png). We keep repeating
    the preceding steps for several episodes to approximate the optimal value distribution.
    To give us a more detailed understanding, the categorical DQN algorithm is given
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – categorical DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The categorical DQN algorithm is given in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_10_095.png) with random
    values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_14_039.png) by copying
    the main network parameter ![](img/B15558_10_037.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_14_098.png), the number of support
    (atoms), and also ![](img/B15558_14_022.png) and ![](img/B15558_14_100.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes perform *step 5*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for ![](img/B15558_14_101.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the state *s* and support values to the main categorical DQN parameterized
    by ![](img/B15558_09_054.png) and get the probability value for each support.
    Then compute the Q value as ![](img/B15558_14_103.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the Q value, select an action using the epsilon-greedy policy,
    that is, with the probability epsilon, select random action *a* and with probability
    1-epsilon, select the action as ![](img/B15558_14_028.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action and move to the next state ![](img/B15558_14_105.png)
    and obtain the reward *r*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the transition information in the replay buffer ![](img/B15558_14_098.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a transition from the replay buffer ![](img/B15558_12_259.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the next state ![](img/B15558_12_376.png) and support values to the target
    categorical DQN parameterized by ![](img/B15558_14_040.png) and get the probability
    value for each support. Then compute the value as ![](img/B15558_14_110.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: After computing the Q value, we select the best action in the state ![](img/B15558_14_105.png)
    as the one that has the maximum Q value ![](img/B15558_14_112.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the array *m* with zero values with its shape as the number of support
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *j* in range of the number of support:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the target support value: ![](img/B15558_14_113.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the value of b: ![](img/B15558_14_114.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the lower bound and upper bound: ![](img/B15558_14_047.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the lower bound: ![](img/B15558_14_116.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distribute the probability on the upper bound: ![](img/B15558_14_117.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the cross entropy loss: ![](img/B15558_14_118.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the loss using gradient descent and update the parameter of the main
    network
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Freeze the target network parameter ![](img/B15558_14_040.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned the categorical DQN algorithm, to understand how a categorical
    DQN works, we will implement it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Playing Atari games using a categorical DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's implement the categorical DQN algorithm to play Atari games. The code
    used in this section is adapted from an open-source categorical DQN implementation,
    [https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo](https://github.com/princewen/tensorflow_practice/tree/master/RL/Basic-DisRL-Demo),
    provided by Prince Wen.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Defining the variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's define some of the important variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the ![](img/B15558_14_022.png) and ![](img/B15558_14_012.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the number of atoms (supports):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the discount factor, ![](img/B15558_03_190.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the time step at which we want to update the target network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the epsilon value that is used in the epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Defining the replay buffer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let''s define the buffer length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the replay buffer as a deque structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function called `sample_transitions` that returns the randomly
    sampled minibatch of transitions from the replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Defining the categorical DQN class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's define a class called `Categorical_DQN` where we will implement the categorical
    DQN algorithm. Instead of looking into the whole code at once, we will look into
    only the important parts. The complete code used in this section is available
    in the GitHub repo of the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a clear understanding, let''s take a look into the code line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Defining the init method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, let''s define the init method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the ![](img/B15558_14_022.png) and ![](img/B15558_14_012.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the number of atoms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the epsilon value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the state shape of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the action shape of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the target state shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the *m* value (the distributed probability of the
    target distribution):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the value of ![](img/B15558_14_015.png) as ![](img/B15558_14_127.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the support values as ![](img/B15558_14_025.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the categorical DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all the TensorFlow variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Building the categorical DQN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `build_network` for building a deep network.
    Since we are dealing with Atari games, we use the convolutional neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the first convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Flatten the feature maps obtained as a result of the second convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the first dense layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second dense layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate the second dense layer with the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the third layer and apply the softmax function to the result of the
    third layer and obtain the probabilities for each of the atoms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define a function called `build_categorical_DQN` for building the
    main and target categorical DQNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the main categorical DQN and obtain the probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the target categorical DQN and obtain the probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the main Q value with the probabilities obtained from the main categorical
    DQN as ![](img/B15558_14_129.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, compute the target Q value with probabilities obtained from the
    target categorical DQN as ![](img/B15558_14_130.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the cross entropy loss as ![](img/B15558_14_131.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the optimizer and minimize the cross entropy loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the main network parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the target network parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `update_target_net` operation for updating the target network parameters
    by copying the parameters of the main network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Defining the train function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `train` to train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Increment the time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the target Q values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the next state action ![](img/B15558_14_132.png) as the one that has
    the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize an array *m* with its shape as the number of support with zero values.
    The *m* denotes the distributed probability of the target distribution after the
    projection step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the probability for each atom using the target categorical DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the projection step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the network by minimizing the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the target network parameters by copying the main network parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Selecting the action
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s define a function called `select_action` for selecting the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate a random number, and if the number is less than epsilon we select
    the random action, else we select the action that has the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Training the network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s start training the network. First, create the Atari game environment
    using `gym`. Let''s create a Tennis game environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an object to our `Categorical_DQN` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'For each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Set `done` to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'While the episode is not over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Select an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the transition information in the replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'If the length of the replay buffer is greater than or equal to the buffer size
    then start training the network by sampling transitions from the replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the state to the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the return obtained in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have learned how a categorical DQN works and how to implement it,
    in the next section, we will learn about another interesting algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Quantile Regression DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look into another interesting distributional RL algorithm
    called QR-DQN. It is a distributional DQN algorithm similar to the categorical
    DQN; however, it has several features that make it more advantageous than a categorical
    DQN.
  prefs: []
  type: TYPE_NORMAL
- en: Math essentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before going ahead, let''s recap two important concepts that we use in QR-DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantile**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverse cumulative distribution function** (**Inverse CDF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we divide our distribution into equal areas of probability, they are called
    quantiles. For instance, as *Figure 14.18* shows, we have divided our distribution
    into two equal areas of probabilities and we have two quantiles with 50% probability
    each:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.18: 2-quantile plot'
  prefs: []
  type: TYPE_NORMAL
- en: Inverse CDF (quantile function)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand an **inverse cumulative distribution function** (**inverse CDF**),
    first, let's learn what a **cumulative distribution function** (**CDF**) is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a random variable *X*, and *P*(*X*) denotes the probability distribution
    of *X*. Then the cumulative distribution function is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_133.png)'
  prefs: []
  type: TYPE_IMG
- en: It basically implies that *F*(*x*) can be obtained by adding up all the probabilities
    that are less than or equal to *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following CDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.19: CDF'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding plot, ![](img/B15558_14_134.png) represents the cumulative
    probability, that is, ![](img/B15558_14_135.png). Say *i* =1, then ![](img/B15558_14_136.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The CDF takes *x* as an input and returns the cumulative probability ![](img/B15558_14_137.png).
    Hence, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_138.png)'
  prefs: []
  type: TYPE_IMG
- en: Say *x* = 2, then we get ![](img/B15558_14_139.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will look at the inverse CDF. Inverse CDF, as the name suggests, is
    the inverse of the CDF. That is, in CDF, given the support *x*, we obtain the
    cumulative probability ![](img/B15558_14_140.png), whereas in inverse CDF, given
    the cumulative probability ![](img/B15558_10_026.png), we obtain the support *x*.
    Inverse CDF can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following plot shows the inverse CDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.20: Inverse CDF'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 14.20*, given the cumulative probability ![](img/B15558_14_143.png),
    we obtain the support *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Say ![](img/B15558_14_144.png), then we get *x* = 2.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned that the quantiles are equally divided probabilities. As *Figure
    14.20* shows, we have three quantiles *q*[1] to *q*[3] with equally divided probabilities
    and the quantile values are [0.3,0.6,1.0], which are just our cumulative probabilities.
    Hence, we can say that the inverse CDF (quantile function) helps us to obtain
    the value of support given the equally divided probabilities. Note that in inverse
    CDF, the support should always be increasing as it is based on the cumulative
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned what the quantile function is, we will gain an understanding
    of how we can make use of the quantile function in the distributional RL setting
    using an algorithm called QR-DQN.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding QR-DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In categorical DQN (C51), we learned that in order to predict the value distribution,
    the network takes the support of the distribution as input and returns the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the support, we also need to decide the number of support *N*, the
    minimum value of support ![](img/B15558_14_022.png), and the maximum value of
    support ![](img/B15558_14_146.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recollect in C51, our support values are equally spaced at fixed locations
    ![](img/B15558_14_147.png) and we feed this equally spaced support as input and
    obtained the non-uniform probabilities ![](img/B15558_14_148.png). As *Figure
    14.21* shows, in C51, we feed the equally spaced support ![](img/B15558_14_149.png)
    as input to the network along with the state(s) and obtain the non-uniform probabilities
    ![](img/B15558_14_150.png) as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.21: Categorical DQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'QR-DQN can be viewed just as the opposite of C51\. In QR-DQN, to estimate the
    value distribution, we feed the uniform probabilities ![](img/B15558_14_151.png)
    and the network outputs the supports at variable locations ![](img/B15558_14_152.png).
    As shown in the following figure, we feed the uniform probabilities ![](img/B15558_14_153.png)
    as input to the network along with the state(s) and obtain the support ![](img/B15558_14_154.png)
    placed at variable locations as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.22: QR-DQN'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, from the two preceding figures we can observe that, in a categorical DQN,
    along with the state, we feed the fixed support at equally spaced intervals as
    input to the network and it returns the non-uniform probabilities, whereas in
    a QR-DQN, along with the state, we feed the fixed uniform probabilities as input
    to the network and it returns the support at variable locations (unequally spaced
    support).
  prefs: []
  type: TYPE_NORMAL
- en: Okay, but what's the use of this? How does a QR-DQN work exactly? Let's explore
    this in detail.
  prefs: []
  type: TYPE_NORMAL
- en: We understood that a QR-DQN takes the uniform probabilities as input and returns
    the support values for estimating the value distribution. Can we make use of the
    quantile function to estimate the value distribution? Yes! We learned that the
    quantile function helps us to obtain the values of support given the equally divided
    probabilities. Thus, in QR-DQN, we estimate the value distribution by estimating
    the quantile function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quantile function is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_155.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *z* is the support and ![](img/B15558_14_143.png) is the equally divided
    cumulative probability. Thus, we can obtain the support *z* given ![](img/B15558_14_157.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *N* be the number of quantiles, then the probability can be obtained as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_158.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, if *N* = 4, then *p* = [0.25, 0.25\. 0.25, 0.25]. If *N* = 5, then
    p = [0.20, 0.20, 0.20, 0.20, 0.20].
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we decide the number of quantiles *N*, the cumulative probabilities ![](img/B15558_14_143.png)
    (quantile values) can be obtained as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_160.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, if *N* = 4, then ![](img/B15558_14_161.png). If *N* = 5, then![](img/B15558_14_162.png).
  prefs: []
  type: TYPE_NORMAL
- en: We just feed this equally divided cumulative probability ![](img/B15558_14_163.png)
    (quantile values) as input to the QR-DQN and it returns the support value. That
    is, we have learned that the QR-DQN estimates the value distribution as the quantile
    function, so we just feed the ![](img/B15558_14_164.png) and obtain the support
    values *z* of the value distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this with a simple example. Say we are in a state *s* and
    we have two possible actions *up* and *down* to perform in the state. As shown
    in the following figure, along with giving the state *s* as input to the network,
    we also feed the quantile value ![](img/B15558_14_165.png), which is just the
    equally divided cumulative probability. Then our network returns the support for
    the distribution of action *up* and the distribution of action *down*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.23: QR-DQN'
  prefs: []
  type: TYPE_NORMAL
- en: If you recollect, in C51, we computed the probability *p*(*s*, *a*) for the
    given state and action, whereas here in QR-DQN, we compute the support *z*(*s*,
    *a*) for the given state and action.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we use capital *Z*(*s*, *a*) to represent the value distribution and
    small *z*(*s*, *a*) to represent the support of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can also compute the target value distribution using the quantile
    function. Then we train our network by minimizing the distance between the predicted
    quantile and the target quantile distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, the fundamental question is why are we doing this? How it is more beneficial
    than C51? There are several advantages of quantile regression DQN over categorical
    DQN. In quantile regression DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: We don't have to choose the number of supports and the bounds of support, which
    is ![](img/B15558_14_009.png) and ![](img/B15558_14_167.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no limitations on the bounds of support, thus the range of returns
    can vary across states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also get rid of the projection step that we performed in the C51 to match
    the supports of the target and predicted distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One more important advantage of a QR-DQN is that it minimizes the p-Wasserstein
    distance between the predicted and target distribution. But why is this important?
    Minimizing the Wasserstein distance between the target and predicted distribution
    helps us in attaining convergence better than minimizing the cross entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, what exactly is the p-Wasserstein distance? The p-Wasserstein distance,
    *W*[p], is characterized as the *L*^p metric on inverse CDF. Say we have two distributions
    *U* and *V*, then the p-Wasserstein metric between these two distributions is
    given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_168.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_14_169.png) and ![](img/B15558_14_170.png) denote the inverse
    CDF of the distributions *U* and *V* respectively. Thus, minimizing the distance
    between two inverse CDFs implies that we minimize the Wasserstein distance.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that in QR-DQN, we train the network by minimizing the distance between
    the predicted and target distribution, and both of them are quantile functions
    (inverse CDF). Thus, minimizing the distance between the predicted and target
    distribution (inverse CDFs) implies that we minimize the Wasserstein distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the QR-DQN paper (see the *Further reading* section for more
    details) also highlighted that instead of computing the support for the quantile
    values ![](img/B15558_14_157.png), they suggest using the quantile midpoint values
    ![](img/B15558_14_172.png). The quantile midpoint can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_173.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, the value of the support *z* can be obtained using quantile midpoint
    values as ![](img/B15558_14_174.png) instead of obtaining support using the quantile
    values as ![](img/B15558_14_175.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'But why the quantile midpoint? The quantile midpoint acts as a unique minimizer,
    that is, the Wasserstein distance between two inverse CDFs will be less when we
    use quantile midpoint values ![](img/B15558_14_176.png) instead of quantile values
    ![](img/B15558_14_157.png). Since we are trying to minimize the Wasserstein distance
    between the target and predicted distribution, we can use quantile midpoints ![](img/B15558_14_178.png)
    so that the distance between them will be less. For instance, as *Figure 14.24*
    shows, the Wasserstein distance is less when we use the quantile midpoint values
    ![](img/B15558_14_179.png) instead of quantile values ![](img/B15558_14_164.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.24: Using quantile midpoint values instead of quantile values'
  prefs: []
  type: TYPE_NORMAL
- en: Source ([https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, in QR-DQNs, we compute the value distribution as a quantile function.
    So, we just feed the cumulative probabilities that are equally divided probabilities
    into the network and obtain the support values of the distribution and we train
    the network by minimizing the Wasserstein distance between the target and predicted
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Action selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Action selection in QR-DQN is just the same as in C51\. First, we extract Q
    value from the predicted value distribution and then we select the action as the
    one that has the maximum Q value. We can extract the Q value by just taking the
    expectation of the value distribution. The expectation of distribution is given
    as a sum of support multiplied by their corresponding probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In C51, we computed the Q value as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_027.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *p*[i](*s*, *a*) is the probability given by the network for state *s*
    and action *a* and *z*[i] is the support.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whereas in a QR-DQN, our network outputs the support instead of the probability.
    So, the Q value in the QR-DQN can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_182.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *z*[i](*s*, *a*) is the support given by the network for state *s* and
    action *a* and *p*[i] is the probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the Q value, we select the action that has the maximum Q value.
    For instance, let''s say, we have a state *s* and two actions in the state, let
    them be *up* and *down*.The Q value for action *up* in the state *s* is computed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_183.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Q value for action *down* in the state *s* is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q value, we select the optimal action as the one that has
    the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_185.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have learned how to select actions in QR-DQN, in the next section,
    we will look into the loss function of QR-DQN.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In C51, we used cross entropy loss as our loss function because our network
    predicts the probability of the value distribution. So we used the cross entropy
    loss to minimize the probabilities between the target and predicted distribution.
    But in QR-DQN, we predict the support of the distribution instead of the probabilities.
    That is, in QR-DQN, we feed the probabilities as input and predict the support
    as output. So, how can we define the loss function for a QR-DQN?
  prefs: []
  type: TYPE_NORMAL
- en: We can use the quantile regression loss to minimize the distance between the
    target support and the predicted support. But first, let's understand how to calculate
    the target support value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going ahead, let''s recall how we compute the target value in a DQN.
    In DQN, we use the Bellman equation and compute the target value as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_186.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, we select action ![](img/B15558_14_187.png) by taking
    the maximum Q value over all possible next state-action pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in QR-DQN, to compute the target value, we can use the distributional
    Bellman equation. The distributional Bellman equation can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_188.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the target support *z*[j] can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_189.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To compute support *z*[j] for the state ![](img/B15558_14_036.png), we also
    need to select some action ![](img/B15558_14_132.png). How can we select an action?
    We just compute the return distribution of all next state-action pairs using the
    target network and select the action ![](img/B15558_14_192.png) that has the maximum
    Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_193.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have learned how to compute the target support value, let's see
    how to compute the quantile regression loss. The advantage of using the quantile
    regression loss is that it adds a penalty to the overestimation and underestimation
    error. Let's understand this with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say the target support value is [1, 5, 10, 15, 20] and the predicted support
    value is [100, 5, 10, 15, 20]. As we can see, our predicted support has a very
    high value in the initial quantile and then it is decreasing. In the inverse CDF
    section, we learned that support should always be increasing as it is based on
    the cumulative probability. But if you look at the predicted values the support
    starts from 100 and then decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider another case. Suppose the target support value is [1, 5, 10,
    15, 20] and the predicted support value is [1, 5, 10, 15, 4]. As we can see, our
    predicted support value is increasing from the initial quantile and then it is
    decreasing to 4 in the final quantile. But this should not happen. Since we are
    using inverse CDF, our support values should always be increasing.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we need to make sure that our support should be increasing and not decreasing.
    So, if the initial quantile values are overestimated with high values and if the
    later quantile values are underestimated with low values, we can penalize them.
    That is, we multiply the overestimated value by ![](img/B15558_14_163.png) and
    the underestimated value by ![](img/B15558_14_195.png). Okay, how can we determine
    if the value is overestimated or underestimated?
  prefs: []
  type: TYPE_NORMAL
- en: First, we compute the difference between the target and the predicted value.
    Let *u* be the difference between the target support value and the predicted support
    value. Then, if the value of *u* is less than 0, we multiply *u* by ![](img/B15558_14_196.png),
    else we multiply *u* by ![](img/B15558_10_071.png). This is known as **quantile
    regression loss**.
  prefs: []
  type: TYPE_NORMAL
- en: But the problem with quantile regression loss is that it will not be smooth
    at 0 and it makes the gradient stay constant. So, instead of using quantile regression
    loss, we use a new modified version of loss called quantile Huber loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how exactly quantile Huber loss works, first, let''s look into
    the Huber loss. Let''s denote the difference between our actual and predicted
    values as *u*. Then the Huber loss ![](img/B15558_14_198.png) can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_199.png)'
  prefs: []
  type: TYPE_IMG
- en: Let ![](img/B15558_14_200.png), then, when the absolute value, ![](img/B15558_14_201.png),
    is less than or equal to ![](img/B15558_14_202.png), the Huber loss is given as
    the quadratic loss, ![](img/B15558_14_203.png), else it is a linear loss, ![](img/B15558_14_204.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python snippet helps us to understand Huber loss better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have understood what the Huber loss ![](img/B15558_14_198.png) is,
    let's look into the quantile Huber loss. In the quantile Huber loss, when the
    value of *u* (the difference between target and predicted support) is less than
    0, then we multiply the Huber loss ![](img/B15558_14_198.png) by ![](img/B15558_14_207.png),
    and when the value of *u* is greater than or equal to 0, we multiply the Huber
    loss ![](img/B15558_14_198.png) by ![](img/B15558_14_157.png).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how a QR-DQN works, in the next section, we will
    look into another interesting algorithm called D4PG.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Distributional DDPG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**D4PG**, which stands for **D**istributed **D**istributional **D**eep **D**eterministic
    **P**olicy **G**radient, is one of the most interesting policy gradient algorithms.
    We can make a guess about how D4PG works just by its name. As the name suggests,
    D4PG is basically a combination of **deep deterministic policy gradient** (**DDPG**)
    and distributional reinforcement learning, and it works in a distributed fashion.
    Confused? Let''s go deeper and understand how D4PG works in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand how D4PG works, it is highly recommended to revise the DDPG algorithm
    we covered in *Chapter 12*, *Learning DDPG, TD3, and SAC*. We learned that DDPG
    is an actor critic method where the actor tries to learn the policy while the
    critic tries to evaluate the policy produced by the actor using the Q function.
    The critic uses the deep Q network for estimating the Q function and the actor
    uses the policy network for computing the policy. Thus, the actor performs an
    action while the critic gives feedback to the action performed by the actor and,
    based on the critic feedback, the actor network will be updated.
  prefs: []
  type: TYPE_NORMAL
- en: D4PG works just like DDPG but in the critic network, instead of using a DQN
    for estimating the Q function, we can use our distributional DQN to estimate the
    value distribution. That is, in the previous sections, we have learned several
    distributional DQN algorithms, such as C51 and QR-DQN. So, in the critic network,
    instead of using a regular DQN, we can use any distributional DQN algorithm, say
    C51.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from this, D4PG also proposes several changes to the DDPG architecture.
    So, we will get into the details and learn how exactly D4PG differs from DDPG.
    Before going ahead, let''s be clear with the notation:'
  prefs: []
  type: TYPE_NORMAL
- en: The policy network parameter is represented by ![](img/B15558_13_234.png) and
    the target policy network parameter is represented by ![](img/B15558_12_210.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The critic network parameter is represented by ![](img/B15558_09_098.png) and
    the target critic network parameter is represented by ![](img/B15558_14_039.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we are talking about a deterministic policy, let's represent it by ![](img/B15558_14_214.png),
    and our policy is parameterized by the policy network, so we can denote the policy
    by ![](img/B15558_14_215.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will understand how exactly the critic and actor network in D4PG works.
  prefs: []
  type: TYPE_NORMAL
- en: Critic network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In DDPG, we learned that we use the critic network to estimate the Q function.
    Thus, given a state and action, the critic network estimates the Q function as
    ![](img/B15558_14_216.png). To train the critic network we minimize the MSE between
    the target Q value given by the Bellman optimality equation and the Q value predicted
    by the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The target value in DDPG is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_217.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we compute the target value, we compute the loss as the MSE between the
    target value and the predicted value as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_047.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *K* denotes the number of transitions randomly sampled from the replay
    buffer. After computing the loss, we compute the gradients ![](img/B15558_14_219.png)
    and update the critic network parameter using gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_052.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's talk about the critic in D4PG. As we learned in D4PG, we use the
    distributional DQN to estimate the Q value. Thus, given a state and action, the
    critic network estimates the value distribution as ![](img/B15558_14_221.png).
  prefs: []
  type: TYPE_NORMAL
- en: To train the critic network, we minimize the distance between the target value
    distribution given by the distributional Bellman equation and the value distribution
    predicted by the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The target value distribution in D4PG is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_222.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can observe, equation (2) is similar to (1) except that we just replaced
    *Q* with *Z*, indicating that we are computing the target value distribution.
    D4PG proposes one more change to the target value computation (2). Instead of
    using the one-step return *r*, we use the **N-step return**, and it can be expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_223.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *N* is the length of the transition, which we sample from the replay buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the target value distribution, we can compute the distance
    between the target value distribution and the predicted value distribution as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_224.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *d* denotes any distance measure for measuring the distance between two
    distributions. Say we are using C51, then *d* denotes the cross entropy and *K*
    denotes the number of transitions sampled from the replay buffer. After computing
    the loss, we calculate the gradients and update the critic network parameter.
    The gradients can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_225.png)'
  prefs: []
  type: TYPE_IMG
- en: D4PG proposes a small change to our gradient updates. In D4PG, we use a **prioritized
    experience replay.** Let's say we have an experience replay buffer of size *R*.
    Each transition in the replay buffer will have a non-uniform probability *p*[i].
    The non-uniform probability helps us to give more importance to one transition
    than the other. Say we have a sample *i*, then its probability can be given as
    ![](img/B15558_14_226.png) or ![](img/B15558_14_227.png). While updating the critic
    network, we weight the updates using ![](img/B15558_14_228.png), which gives importance
    to the updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus our gradient computation becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_229.png)'
  prefs: []
  type: TYPE_IMG
- en: After computing the gradient, we can update the critic network parameter using
    gradient descent as ![](img/B15558_12_052.png). Now that we have understood how
    the critic network works in D4PG, let's look into the actor network in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Actor network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s quickly recap how the actor network in DDPG works. In DDPG, we
    learned that the actor network takes the state as input and returns the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_231.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we are using the deterministic policy in the continuous action space,
    and to explore new actions we just add some noise ![](img/B15558_14_232.png) to
    the action produced by the actor network since the action is a continuous value.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our modified action can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_233.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the objective function of the actor is to generate an action that maximizes
    the Q value produced by the citric network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_234.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_14_235.png).
  prefs: []
  type: TYPE_NORMAL
- en: We learned that to maximize the objective, we compute the gradients of our objective
    function ![](img/B15558_11_014.png) and update the actor network parameter by
    performing gradient ascent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s come to D4PG. In D4PG we perform the same steps with a little difference.
    Note that here we are not using the Q function in the critic. Instead, we are
    computing the value distribution and thus our objective function becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_237.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the action, ![](img/B15558_14_235.png) and just like we saw in DDPG,
    to maximize the objective, first, we compute the gradients of our objective function
    ![](img/B15558_14_238.png). After computing the gradients we update the actor
    network parameter by performing gradient ascent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_12_068.png)'
  prefs: []
  type: TYPE_IMG
- en: We learned that D4PG is a **distributed** algorithm, meaning that instead of
    using one actor, we use *L* **number of actors**, each of which acts parallel
    and is independent of the environment, collects experience, and stores the experience
    in the replay buffer. Then we update the network parameter to the actors periodically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to summarize, D4PG is similar to DDPG except for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the distributional DQN in the critic network instead of using the regular
    DQN to estimate the Q values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate *N*-step returns in the target instead of calculating the one-step
    return.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a prioritized experience replay and add importance to the gradient update
    in the critic network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of using one actor, we use **L** independent actors, each of which acts
    in parallel, collects experience, and stores the experience in the replay buffer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have understood how D4PG works, putting together all the concepts
    we have learned, let's look into the algorithm of D4PG in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – D4PG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let ![](img/B15558_14_240.png) denote the time steps at which we want to update
    the target critic and actor network parameters. We set ![](img/B15558_14_241.png),
    which states that we update the target critic network and target actor network
    parameter for every 2 steps of the episode. Similarly, let ![](img/B15558_14_242.png)denote
    thetime steps at which we want to replicate the network weights to the **L** actors.
    We set ![](img/B15558_14_243.png), which states that we replicate the network
    weights to the actors on every 2 steps of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm of D4PG is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the critic network parameter ![](img/B15558_09_056.png) and actor
    network parameter ![](img/B15558_14_245.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize target critic network parameter ![](img/B15558_14_246.png) and target
    actor network parameter ![](img/B15558_14_247.png) by copying from ![](img/B15558_09_054.png)
    and ![](img/B15558_14_249.png) respectively
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_14_098.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the *L* number of actors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, repeat *step 6*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for ![](img/B15558_14_101.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_266.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value distribution of the critic, that is,![](img/B15558_14_253.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss of the critic network and calculate the gradient as ![](img/B15558_14_229.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After computing the gradients, update the critic network parameter using gradient
    descent: ![](img/B15558_14_255.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of the actor network ![](img/B15558_11_014.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the actor network parameter by gradient ascent:![](img/B15558_14_257.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t* mod ![](img/B15558_14_258.png), then:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target critic and target actor network parameter using soft replacement
    as ![](img/B15558_14_259.png) and ![](img/B15558_14_260.png) respectively
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If *t* mod ![](img/B15558_14_261.png), then:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Replicate the network weights to the actors
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'And we perform the following steps in the actor network:'
  prefs: []
  type: TYPE_NORMAL
- en: Select action *a* based on the policy ![](img/B15558_14_262.png) and exploration
    noise, that is,![](img/B15558_14_233.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action *a*, move to the next state ![](img/B15558_14_264.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_092.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 1* to *2* until the learner finishes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, we have learned how D4PG works.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by understanding how distributional reinforcement learning
    works. We learned that in distributional reinforcement learning, instead of selecting
    an action based on the expected return, we select the action based on the distribution
    of return, which is often called the value distribution or return distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned about the categorical DQN algorithm, also known as C51, where
    we feed the state and support of the distribution as the input and the network
    returns the probabilities of the value distribution. We also learned how the projection
    step matches the support of the target and predicted the value distribution so
    that we can apply the cross entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Going ahead, we learned about quantile regression DQNs, where we feed the state
    and also the equally divided cumulative probabilities ![](img/B15558_14_157.png)
    as input to the network and it returns the support value of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about how D4PG works, and we also learned
    how it varies from DDPG.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s test our knowledge of distributional reinforcement learning by answering
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is distributional reinforcement learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a categorical DQN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the categorical DQN called the C51 algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the quantile function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a QR-DQN differ from a categorical DQN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does D4PG differ from DDPG?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, refer to the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A Distributional Perspective on Reinforcement Learning** by *Marc G. Bellemare*,
    *Will Dabney*, *Remi Munos*, [https://arxiv.org/pdf/1707.06887.pdf](https://arxiv.org/pdf/1707.06887.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributional Reinforcement Learning with Quantile Regression** by *Will
    Dabney*, *Mark Rowland*, *Marc G. Bellemare*, *Rémi Munos*, [https://arxiv.org/pdf/1710.10044.pdf](https://arxiv.org/pdf/1710.10044.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed Distributional Deep Deterministic Policy Gradient** by *Gabriel
    Barth-Maron*, *et al*., [https://arxiv.org/pdf/1804.08617.pdf](https://arxiv.org/pdf/1804.08617.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
