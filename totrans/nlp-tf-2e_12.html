<html><head></head><body>
  <div id="_idContainer691" class="Basic-Text-Frame">
    <h1 id="_idParaDest-305" class="chapterTitle">Appendix A: Mathematical Foundations and Advanced TensorFlow</h1>
    <p class="normal">Here we will discuss some concepts that will be useful for helping you to understand certain details provided in the chapters. First, we will discuss several mathematical data structures found throughout the book, followed by a description of the various operations performed on those data structures. After that, we will discuss the concept of probabilities. Probabilities play a vital role in machine learning, as they usually give insights into how uncertain a model is about its prediction. Finally, we will conclude this appendix with a guide on how to use TensorBoard as a visualization tool for word embeddings.</p>
    <h1 id="_idParaDest-306" class="heading-1">Basic data structures</h1>
    <h2 id="_idParaDest-307" class="heading-2">Scalar</h2>
    <p class="normal">A scalar<a id="_idIndexMarker1151"/> is a single number, unlike a matrix or a vector. For example, 1.3 is a scalar. A scalar can be mathematically denoted as follows: <img src="../Images/B14070_12_001.png" alt="" style="height: 1.05em !important; vertical-align: -0.18em !important;"/>.</p>
    <p class="normal">Here, <em class="italic">R</em> is the real number space.</p>
    <h2 id="_idParaDest-308" class="heading-2">Vectors</h2>
    <p class="normal">A vector<a id="_idIndexMarker1152"/> is an array of numbers. Unlike a set, where there is no order to the elements, a vector has a certain order to the elements. An example vector is <code class="inlineCode">[1.0, 2.0, 1.4, 2.3]</code>. Mathematically, it can be denoted as follows: </p>
    <p class="center"><img src="../Images/B14070_12_002.png" alt="" style="height: 1.35em !important;"/></p>
    <p class="center"><img src="../Images/B14070_12_003.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Here, <em class="italic">R</em> is the real number<a id="_idIndexMarker1153"/> space and <em class="italic">n</em> is the number of elements in the vector.</p>
    <h2 id="_idParaDest-309" class="heading-2">Matrices</h2>
    <p class="normal">A matrix<a id="_idIndexMarker1154"/> can be thought of as a two-dimensional arrangement of a collection of scalars. In other words, a matrix can be thought of as a vector of vectors. An example matrix is shown as follows: </p>
    <p class="center"><img src="../Images/B14070_12_004.png" alt="" style="height: 3.13em !important;"/></p>
    <p class="normal">A more general matrix of size <img src="../Images/B14070_12_005.png" alt="" style="height: 1.05em !important; vertical-align: -0.14em !important;"/> can be mathematically defined like this: </p>
    <p class="center"><img src="../Images/B14070_12_006.png" alt="" style="height: 4.47em !important;"/></p>
    <p class="normal">And: </p>
    <p class="center"><img src="../Images/B14070_12_007.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Here, <em class="italic">m</em> is the number of rows of the matrix, <em class="italic">n</em> is the number of columns in the matrix, and <em class="italic">R</em> is the real number space.</p>
    <h2 id="_idParaDest-310" class="heading-2">Indexing of a matrix</h2>
    <p class="normal">We will be using zero-indexed<a id="_idIndexMarker1155"/> notation (that is, indexes that start with 0).</p>
    <p class="normal">To index a single element from a matrix at the <em class="italic">(i, j)</em><sup class="superscript">th</sup> position, we use the following notation: </p>
    <p class="center"><img src="../Images/B14070_12_008.png" alt="" style="height: 1.35em !important;"/></p>
    <p class="normal">Referring to the previously defined matrix, we get the following: </p>
    <p class="center"><img src="../Images/B14070_12_004.png" alt="" style="height: 3.13em !important;"/></p>
    <p class="normal">We index an element from <em class="italic">A</em> like this: </p>
    <p class="center"><img src="../Images/B14070_12_010.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">We denote a single row of any matrix <em class="italic">A</em> as shown here:</p>
    <p class="center"><img src="../Images/B14070_12_011.png" alt="" style="height: 1.35em !important;"/></p>
    <p class="normal">For our example matrix, we can denote the second row (indexed as 1) of the matrix as shown here: </p>
    <p class="center"><img src="../Images/B14070_12_012.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">We denote the slice starting from the <em class="italic">(i, k)</em><sup class="superscript">th</sup> index to the <em class="italic">(j, l)</em><sup class="superscript">th</sup> index of any matrix <em class="italic">A</em> as shown here:</p>
    <p class="center"><img src="../Images/B14070_12_013.png" alt="" style="height: 3.45em !important;"/></p>
    <p class="normal">In our example matrix, we can denote<a id="_idIndexMarker1156"/> the slice from first row third column to second row fourth column as shown here:</p>
    <p class="center"><img src="../Images/B14070_12_014.png" alt="" style="height: 1.98em !important;"/></p>
    <h1 id="_idParaDest-311" class="heading-1">Special types of matrices</h1>
    <h2 id="_idParaDest-312" class="heading-2">Identity matrix</h2>
    <p class="normal">An identity matrix<a id="_idIndexMarker1157"/> is a square matrix where values are equal to 1 on the diagonal of the matrix and 0 everywhere else. Mathematically, it can be shown as follows: </p>
    <p class="center"><img src="../Images/B14070_12_015.png" alt="" style="height: 2.08em !important;"/></p>
    <p class="normal">This would look like the following:</p>
    <p class="center"><img src="../Images/B14070_12_016.png" alt="" style="height: 4.38em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_12_017.png" alt="" style="height: 1.05em !important; vertical-align: -0.18em !important;"/>.</p>
    <p class="normal">The identity matrix gives the following<a id="_idIndexMarker1158"/> nice property when multiplied with another matrix <em class="italic">A</em>:</p>
    <p class="center"><img src="../Images/B14070_12_018.png" alt="" style="height: 1.05em !important;"/></p>
    <h2 id="_idParaDest-313" class="heading-2">Square diagonal matrix</h2>
    <p class="normal">A square diagonal matrix<a id="_idIndexMarker1159"/> is a more general case of the identity matrix, where the values along the diagonal can take any value and the off-diagonal values are zeros: </p>
    <p class="center"><img src="../Images/B14070_12_019.png" alt="" style="height: 4.80em !important;"/></p>
    <h2 id="_idParaDest-314" class="heading-2">Tensors</h2>
    <p class="normal">An <em class="italic">n</em>-dimensional<a id="_idIndexMarker1160"/> matrix is called a <strong class="keyWord">tensor</strong>. In other words, a matrix with an arbitrary number of dimensions is called a tensor. For example, a four-dimensional tensor can be denoted as shown here: </p>
    <p class="center"><img src="../Images/B14070_12_020.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Here, <em class="italic">R</em> is the real number space.</p>
    <h1 id="_idParaDest-315" class="heading-1">Tensor/matrix operations</h1>
    <h2 id="_idParaDest-316" class="heading-2">Transpose</h2>
    <p class="normal">Transpose is an important operation<a id="_idIndexMarker1161"/> defined for matrices or tensors. For a matrix, the transpose is defined as follows:</p>
    <p class="center"><img src="../Images/B14070_12_021.png" alt="" style="height: 1.35em !important;"/></p>
    <p class="normal">Here, <em class="italic">A</em><sup class="superscript">T</sup> denotes the transpose of <em class="italic">A</em>.</p>
    <p class="normal">An example of the transpose operation can be illustrated as follows: </p>
    <p class="center"><img src="../Images/B14070_12_004.png" alt="" style="height: 3.13em !important;"/></p>
    <p class="normal">After the transpose operation: </p>
    <p class="center"><img src="../Images/B14070_12_023.png" alt="" style="height: 4.38em !important;"/></p>
    <p class="normal">For a tensor, transpose can be seen as permuting the dimensions order. For example, let’s define a tensor <em class="italic">S</em>, as shown here: </p>
    <p class="center"><img src="../Images/B14070_12_024.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Now one transpose<a id="_idIndexMarker1162"/> operation (out of many) can be defined as follows: </p>
    <p class="center"><img src="../Images/B14070_12_025.png" alt="" style="height: 1.05em !important;"/></p>
    <h2 id="_idParaDest-317" class="heading-2">Matrix multiplication</h2>
    <p class="normal">Matrix multiplication<a id="_idIndexMarker1163"/> is another important operation that appears quite frequently in linear algebra.</p>
    <p class="normal">Given the matrices <img src="../Images/B14070_12_026.png" alt="" style="height: 1.05em !important; vertical-align: -0.23em !important;"/> and <img src="../Images/B14070_12_027.png" alt="" style="height: 1.05em !important; vertical-align: -0.20em !important;"/>, the multiplication of <em class="italic">A</em> and <em class="italic">B</em> is defined as follows: </p>
    <p class="center"><img src="../Images/B14070_12_028.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_12_029.png" alt="" style="height: 1.05em !important; vertical-align: -0.26em !important;"/>.</p>
    <p class="normal">Consider this example:</p>
    <p class="center"><img src="../Images/B14070_12_030.png" alt="" style="height: 3.23em !important;"/></p>
    <p class="center"><img src="../Images/B14070_12_031.png" alt="" style="height: 2.08em !important;"/></p>
    <p class="normal">This gives <img src="../Images/B14070_12_032.png" alt="" style="height: 1.05em !important; vertical-align: -0.21em !important;"/>, and the value<a id="_idIndexMarker1164"/> of <em class="italic">C</em> is as follows: </p>
    <p class="center"><img src="../Images/B14070_12_033.png" alt="" style="height: 3.23em !important;"/></p>
    <h2 id="_idParaDest-318" class="heading-2">Element-wise multiplication</h2>
    <p class="normal">Element-wise matrix multiplication (or the <strong class="keyWord">Hadamard product</strong>) is computed<a id="_idIndexMarker1165"/> for two matrices that have the same shape. Given the matrices <img src="../Images/B14070_12_034.png" alt="" style="height: 1.05em !important; vertical-align: -0.17em !important;"/> and <img src="../Images/B14070_12_035.png" alt="" style="height: 1.05em !important; vertical-align: -0.12em !important;"/>, the element-wise multiplication of <em class="italic">A</em> and <em class="italic">B</em> is defined as follows: </p>
    <p class="center"><img src="../Images/B14070_12_036.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Here, <img src="../Images/B14070_12_037.png" alt="" style="height: 1.05em !important; vertical-align: -0.14em !important;"/>.</p>
    <p class="normal">Consider this example: </p>
    <p class="center"><img src="../Images/B14070_12_038.png" alt="" style="height: 3.23em !important;"/></p>
    <p class="normal">This gives <img src="../Images/B14070_12_039.png" alt="" style="height: 1.05em !important; vertical-align: -0.22em !important;"/>, and the value of <em class="italic">C</em> is as follows: </p>
    <p class="center"><img src="../Images/B14070_12_040.png" alt="" style="height: 3.23em !important;"/></p>
    <h2 id="_idParaDest-319" class="heading-2">Inverse</h2>
    <p class="normal">The inverse<a id="_idIndexMarker1166"/> of the matrix <em class="italic">A</em> is denoted by <em class="italic">A</em><sup class="superscript">-1</sup>, where it satisfies the following condition: </p>
    <p class="center"><img src="../Images/B14070_12_041.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Inverse is very useful if we are trying to solve a system of linear equations. Consider this example: </p>
    <p class="center"><img src="../Images/B14070_12_042.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">We can solve<a id="_idIndexMarker1167"/> for <img src="../Images/B14070_12_043.png" alt="" style="height: 1.05em !important; vertical-align: -0.11em !important;"/> like this: </p>
    <p class="center"><img src="../Images/B14070_12_044.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">This can be written as <img src="../Images/B14070_12_045.png" alt="" style="height: 1.25em !important; vertical-align: -0.34em !important;"/>, using the associative law – that is, <img src="../Images/B14070_12_046.png" alt="" style="height: 1.25em !important; vertical-align: -0.36em !important;"/>.</p>
    <p class="normal">Next, we will get, where <img src="../Images/B14070_12_049.png" alt="" style="height: 1.05em !important; vertical-align: -0.13em !important;"/> is the identity matrix.</p>
    <p class="normal">Lastly, <img src="../Images/B14070_12_050.png" alt="" style="height: 1.05em !important; vertical-align: -0.19em !important;"/> because <img src="../Images/B14070_12_051.png" alt="" style="height: 1.05em !important; vertical-align: -0.14em !important;"/>.</p>
    <p class="normal">For example, polynomial regression, one of the regression techniques, uses a linear system of equations to solve the regression problem. Regression is similar to classification, but instead of outputting a class, regression models output a continuous value. Let’s look at an example problem: given the number of bedrooms in a house, we’ll calculate the real-estate value of the house. Formally, a polynomial regression problem can be written as follows: </p>
    <p class="center"><img src="../Images/B14070_12_052.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Here,<img src="../Images/B14070_12_053.png" alt="" style="height: 1.25em !important; vertical-align: -0.30em !important;"/> is the <em class="italic">i</em><sup class="superscript">th</sup> data input, where <img src="../Images/B14070_12_054.png" alt="" style="height: 1.15em !important; vertical-align: -0.14em !important;"/> is the input, <img src="../Images/B14070_12_055.png" alt="" style="height: 1.25em !important; vertical-align: -0.18em !important;"/>is the label, and <img src="../Images/B14070_12_056.png" alt="" style="height: 1.05em !important; vertical-align: -0.09em !important;"/> is the noise in data. In our example, <img src="../Images/B14070_12_057.png" alt="" style="height: 1.05em !important; vertical-align: -0.12em !important;"/> is the number of bedrooms and <img src="../Images/B14070_12_058.png" alt="" style="height: 1.25em !important; vertical-align: -0.19em !important;"/> is the price of the house. This can be written as a system of linear equations as follows: </p>
    <p class="center"><img src="../Images/B14070_12_059.png" alt="" style="height: 6.15em !important;"/></p>
    <p class="normal">However, <em class="italic">A</em><sup class="superscript">-1</sup> does not exist for all <em class="italic">A</em>. There are certain conditions that need to be satisfied in order for the inverse to exist for a matrix. For example, to define the inverse, <em class="italic">A</em> needs to be a square matrix (that is, <img src="../Images/B14070_12_060.png" alt="" style="height: 1.05em !important; vertical-align: -0.20em !important;"/>). Even when the inverse exists, we cannot always find<a id="_idIndexMarker1168"/> it in the closed form; sometimes it can only be approximated with finite-precision computers. If the inverse exists, there are several algorithms for finding it, which we will discuss next.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">When it is said that <em class="italic">A</em> needs to be a square matrix for the inverse<a id="_idIndexMarker1169"/> to exist, we refer to the<a id="_idIndexMarker1170"/> standard inversion. There exist variants<a id="_idIndexMarker1171"/> of the inverse operation (for example, the <strong class="keyWord">Moore-Penrose inverse</strong>, also known as pseudoinverse) that can perform matrix inversion on general <img src="../Images/B14070_12_061.png" alt="" style="height: 0.95em !important; vertical-align: -0.05em !important;"/> matrices.</p>
    </div>
    <h2 id="_idParaDest-320" class="heading-2">Finding the matrix inverse – Singular Value Decomposition (SVD)</h2>
    <p class="normal">Let’s now see how we<a id="_idIndexMarker1172"/> can use SVD to find the inverse<a id="_idIndexMarker1173"/> of a matrix <em class="italic">A</em>. SVD factorizes <em class="italic">A</em> into three different matrices, as shown here: </p>
    <p class="center"><img src="../Images/B14070_12_062.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Here the columns of <em class="italic">U</em> are known as left singular vectors, columns of <em class="italic">V</em> are known as right singular vectors, and diagonal<a id="_idIndexMarker1174"/> values of <em class="italic">D</em> (a diagonal matrix) are known as singular values. Left singular vectors are the eigenvectors of <img src="../Images/B14070_12_063.png" alt="" style="height: 1.05em !important; vertical-align: -0.09em !important;"/> and the right singular vectors are the eigenvectors of <img src="../Images/B14070_12_064.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>. Finally, the singular values are the square roots of the eigenvalues of <img src="../Images/B14070_12_065.png" alt="" style="height: 1.05em !important; vertical-align: -0.13em !important;"/> and <img src="../Images/B14070_12_066.png" alt="" style="height: 1.05em !important; vertical-align: -0.09em !important;"/>. The eigenvector <img src="../Images/B14070_12_067.png" alt="" style="height: 1.05em !important; vertical-align: -0.09em !important;"/> and its corresponding eigenvalue <img src="../Images/B14070_12_068.png" alt="" style="height: 1.05em !important; vertical-align: -0.16em !important;"/> of the square matrix <em class="italic">A</em> satisfy the following condition: </p>
    <p class="center"><img src="../Images/B14070_12_069.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Then, if the SVD exists, the inverse of <em class="italic">A</em> is given by this: </p>
    <p class="center"><img src="../Images/B14070_12_070.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Since <em class="italic">D</em> is diagonal, <em class="italic">D</em><sup class="superscript">-1</sup> is simply the element-wise reciprocal of the nonzero elements of <em class="italic">D</em>. SVD is an important matrix factorization technique that appears on many occasions in machine learning. For example, SVD is used for calculating <strong class="keyWord">Principal Component Analysis</strong> (<strong class="keyWord">PCA</strong>), which is a popular dimensionality reduction technique for data (a purpose similar to that of t-SNE, which we saw in <em class="chapterRef">Chapter 4</em><em class="italic">, Advanced Word Vector Algorithms</em>). Another, more NLP-oriented application of SVD is document ranking. That is, when you want to get the most relevant documents (and rank them by relevance to some term, for example, <em class="italic">football</em>), SVD can be used<a id="_idIndexMarker1175"/> to achieve this. To learn more about SVD, you can consult this blog post, which provides a geometric intuition<a id="_idIndexMarker1176"/> on SVD, as well as showing <a id="_idIndexMarker1177"/>how it’s applied in PCA: <a href="https://gregorygundersen.com/blog/2018/12/10/svd/"><span class="url">https://gregorygundersen.com/blog/2018/12/10/svd/</span></a>.</p>
    <h2 id="_idParaDest-321" class="heading-2">Norms</h2>
    <p class="normal">A norm<a id="_idIndexMarker1178"/> is used as a measure of the <em class="italic">size</em> of the vector (that is, of the values in the vector). The <em class="italic">p</em><sup class="superscript">th</sup> norm is calculated and denoted as shown here: </p>
    <p class="center"><img src="../Images/B14070_12_071.png" alt="" style="height: 3.65em !important;"/></p>
    <p class="normal">For example, the <em class="italic">L2</em> norm would be this: </p>
    <p class="center"><img src="../Images/B14070_12_072.png" alt="" style="height: 3.55em !important;"/></p>
    <h2 id="_idParaDest-322" class="heading-2">Determinant</h2>
    <p class="normal">The determinant<a id="_idIndexMarker1179"/> of a square matrix is denoted by <img src="../Images/B14070_12_073.png" alt="" style="height: 1.25em !important; vertical-align: -0.36em !important;"/>. The determinant is very useful in many ways. For example, <em class="italic">A</em> is invertible if, and only if, the determinant is nonzero. The determinant is also interpreted as the product of all the eigenvalues of the matrix. The determinant of a <em class="italic">2</em>x<em class="italic">2</em> matrix <em class="italic">A</em>,</p>
    <p class="center"><img src="../Images/B14070_12_075.png" alt="" style="height: 2.08em !important;"/></p>
    <p class="normal">is denoted as</p>
    <p class="center"><img src="../Images/B14070_12_076.png" alt="" style="height: 2.08em !important;"/></p>
    <p class="normal">and computed as</p>
    <p class="center"><img src="../Images/B14070_12_077.png" alt="" style="height: 2.08em !important;"/></p>
    <p class="normal">The following equation<a id="_idIndexMarker1180"/> shows the calculations for the determinant of a <em class="italic">3x3</em> matrix: </p>
    <p class="center"><img src="../Images/B14070_12_078.png" alt="" style="height: 3.55em !important;"/></p>
    <p class="center"><img src="../Images/B14070_12_079.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_12_080.png" alt="" style="height: 1.25em !important;"/></p>
    <h1 id="_idParaDest-323" class="heading-1">Probability</h1>
    <p class="normal">Next, we will discuss the terminology<a id="_idIndexMarker1181"/> related to probability theory. Probability theory is a vital part of machine learning, as modeling data with probabilistic models allows us to draw conclusions about how uncertain a model is about some predictions. Consider a use case of sentiment analysis. We want to output a prediction (positive/negative) for a given movie review. Though the model outputs some value between 0 and 1 (0 for negative and 1 for positive) for any sample we input, the model doesn’t know how <em class="italic">uncertain</em> it is about its answer.</p>
    <p class="normal">Let’s understand how uncertainty helps us to make better predictions. For example, a deterministic model (i.e. a model that outputs an exact value instead of a distribution for the value) might incorrectly say the positivity of the review <em class="italic">I never lost interest</em> is 0.25 (that is, it’s more likely to be a negative comment). However, a probabilistic model will give a mean value and a standard deviation for the prediction. For example, it will say, this prediction has a mean of 0.25 and a standard deviation of 0.5. With the second model, we know that the prediction is likely to be wrong due to the high standard deviation. However, in the deterministic model, we don’t have this luxury. This property is especially valuable for critical machine systems (for example, a terrorism risk assessment model).</p>
    <p class="normal">To develop such probabilistic machine learning models (for example, Bayesian logistic regression, Bayesian neural<a id="_idIndexMarker1182"/> networks, or Gaussian processes), you should be familiar with basic probability theory. Therefore, we will provide some basic probability information here.</p>
    <h2 id="_idParaDest-324" class="heading-2">Random variables</h2>
    <p class="normal">A random variable<a id="_idIndexMarker1183"/> is a variable that can take some<a id="_idIndexMarker1184"/> value at random. Also, random variables are represented as <em class="italic">x</em><sub class="subscript">1</sub>, <em class="italic">x</em><sub class="subscript">2</sub>, and so on. Random variables can be of two types: discrete and continuous.</p>
    <h3 id="_idParaDest-325" class="heading-3">Discrete random variables</h3>
    <p class="normal">A discrete random variable<a id="_idIndexMarker1185"/> is a variable that can take discrete<a id="_idIndexMarker1186"/> random values. For example, trials of flipping a coin can be modeled as a random variable; that is, the side a coin lands on when you flip it is a discrete variable as the value can only be <em class="italic">heads</em> or <em class="italic">tails</em>. Alternatively, the value you get when you roll a die is discrete, as well, as the values can only come from the set <code class="inlineCode">{1,2,3,4,5,6}</code>.</p>
    <h3 id="_idParaDest-326" class="heading-3">Continuous random variables</h3>
    <p class="normal">A continuous random variable<a id="_idIndexMarker1187"/> is a variable that can take<a id="_idIndexMarker1188"/> any real value, that is, if <em class="italic">x</em> is a continuous random variable: </p>
    <p class="center"><img src="../Images/B14070_12_081.png" alt="" style="height: 1.05em !important;"/></p>
    <p class="normal">Here, <em class="italic">R</em> is the real number space.</p>
    <p class="normal">For example, the height of a person is a continuous random variable as it can take any real value.</p>
    <h2 id="_idParaDest-327" class="heading-2">The probability mass/density function</h2>
    <p class="normal">The <strong class="keyWord">probability mass function</strong> (<strong class="keyWord">PMF</strong>) or the <strong class="keyWord">probability density function</strong> (<strong class="keyWord">PDF</strong>) is a way of showing the probability <a id="_idIndexMarker1189"/>distribution over different values<a id="_idIndexMarker1190"/> a random variable can take. For discrete variables, a PMF is defined, and for continuous variables, a PDF is defined. <em class="italic">Figure A.1</em> shows an example PMF:</p>
    <figure class="mediaobject"><img src="../Images/B14070_12_01.png" alt="The probability mass/density function"/></figure>
    <p class="packt_figref">A.1: Probability mass function (PMF) discrete</p>
    <p class="normal">The preceding PMF might be achieved by a <em class="italic">biased</em> die. In this graph, we<a id="_idIndexMarker1191"/> can see that there is a high probability<a id="_idIndexMarker1192"/> of getting a 3 with this die. Such a graph can be obtained by running a number of trials (say, 100) and then counting the number of times each face fell on top. Finally, you would divide each count by the number of trials to obtain the normalized probabilities. Note that all the probabilities should add up to 1, as shown here: </p>
    <p class="center"><img src="../Images/B14070_12_082.png" alt="" style="height: 1.15em !important;"/></p>
    <p class="normal">The same concept is extended to a continuous random variable to obtain a PDF. Say that we are trying to model the probability of a certain height given a population. Unlike the discrete case, we do not have individual values to calculate the probability for, but rather a continuous spectrum of values (in the example, it extends from <em class="italic">0</em> to <em class="italic">2.4 m</em>). If we are to draw a graph for this example like the one in <em class="italic">Figure A.1</em>, we need to think of it in terms of infinitesimally small bins. For example, we find out the probability density of a person’s height being between <em class="italic">0.0 m-0.01 m, 0.01-0.02 m, ..., 1.8 m-1.81 m, …</em>, and so on. The probability density can be calculated using the following formula: </p>
    <p class="center"><img src="../Images/B14070_12_083.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">Then, we will plot those bars close<a id="_idIndexMarker1193"/> to each other to obtain a continuous<a id="_idIndexMarker1194"/> curve, as shown in <em class="italic">Figure A.2</em>. Note that the probability density for a given bin can be greater than <em class="italic">1</em> (since it’s density), but the area under the curve must be 1:</p>
    <figure class="mediaobject"><img src="../Images/B14070_12_02.png" alt="The probability mass/density function"/></figure>
    <p class="packt_figref">Figure A.2: Probability density function (PDF) continuous</p>
    <p class="normal">The shape shown in <em class="italic">Figure A.2</em> is known<a id="_idIndexMarker1195"/> as the normal (or Gaussian) distribution. It is also called the <em class="italic">bell curve</em>. We previously gave just an intuitive<a id="_idIndexMarker1196"/> explanation of how to think about a continuous probability density function. </p>
    <p class="normal">More formally, a continuous PDF of the normal distribution has an equation and is defined as follows. Let’s assume that a continuous random variable <em class="italic">X</em> has a normal distribution with mean <img src="../Images/B14070_12_084.png" alt="" style="height: 1.15em !important; vertical-align: -0.18em !important;"/> and standard deviation <img src="../Images/B14070_12_085.png" alt="" style="height: 1.05em !important; vertical-align: -0.08em !important;"/>. The probability of <em class="italic">X = x</em> for any value of <em class="italic">x</em> is given by this formula: </p>
    <p class="center"><img src="../Images/B14070_12_086.png" alt="" style="height: 2.70em !important;"/></p>
    <p class="normal">You should get the area (which needs to be 1 for a valid PDF) if you integrate this quantity over all possible infinitesimally small <em class="italic">dx</em> values, as denoted by this formula: </p>
    <p class="center"><img src="../Images/B14070_12_087.png" alt="" style="height: 2.70em !important;"/></p>
    <p class="normal">The integral of the normal for the arbitrary <em class="italic">a</em>, <em class="italic">b</em> values is given by the following formula: </p>
    <p class="center"><img src="../Images/B14070_12_088.png" alt="" style="height: 2.60em !important;"/></p>
    <p class="normal">Using this, we can get the integral of the normal distribution, where <img src="../Images/B14070_12_089.png" alt="" style="height: 1.15em !important; vertical-align: -0.26em !important;"/> and <img src="../Images/B14070_12_090.png" alt="" style="height: 1.15em !important; vertical-align: -0.25em !important;"/>: </p>
    <p class="center"><img src="../Images/B14070_12_091.png" alt="" style="height: 3.55em !important;"/></p>
    <p class="normal">This gives the accumulation<a id="_idIndexMarker1197"/> of all the probability values for all the values of <em class="italic">x</em> and gives<a id="_idIndexMarker1198"/> you a value of 1. </p>
    <div class="note">
      <p class="normal">You can find more<a id="_idIndexMarker1199"/> information at <a href="http://mathworld.wolfram.com/GaussianIntegral.html"><span class="url">http://mathworld.wolfram.com/GaussianIntegral.html</span></a>, or for a less complex discussion, refer to <a href="https://en.wikipedia.org/wiki/Gaussian_integral"><span class="url">https://en.wikipedia.org/wiki/Gaussian_integral</span></a>.</p>
    </div>
    <h2 id="_idParaDest-328" class="heading-2">Conditional probability</h2>
    <p class="normal">Conditional probability<a id="_idIndexMarker1200"/> represents the probability<a id="_idIndexMarker1201"/> of an event happening given the occurrence of another event. For example, given two random variables, <em class="italic">X</em> and <em class="italic">Y</em>, the conditional probability of <em class="italic">X = x</em>, given that <em class="italic">Y = y</em>, is denoted by this formula: </p>
    <p class="center"><img src="../Images/B14070_12_092.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">A real-world<a id="_idIndexMarker1202"/> example of such a probability<a id="_idIndexMarker1203"/> would be as follows: </p>
    <p class="center"><img src="../Images/B14070_12_093.png" alt="" style="height: 1.25em !important;"/></p>
    <h2 id="_idParaDest-329" class="heading-2">Joint probability</h2>
    <p class="normal">Given two random<a id="_idIndexMarker1204"/> variables, <em class="italic">X</em> and <em class="italic">Y</em>, we will refer<a id="_idIndexMarker1205"/> to the probability of <em class="italic">X = x</em> together with <em class="italic">Y = y</em> as the joint probability of <em class="italic">X = x</em> and <em class="italic">Y = y</em>. This is denoted by the following formula: </p>
    <p class="center"><img src="../Images/B14070_12_094.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">If <em class="italic">X</em> and <em class="italic">Y</em> are mutually exclusive events, this expression reduces to this: </p>
    <p class="center"><img src="../Images/B14070_12_095.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">A real-world example of this is as follows: </p>
    <p class="center"><img src="../Images/B14070_12_096.png" alt="" style="height: 1.25em !important;"/></p>
    <h2 id="_idParaDest-330" class="heading-2">Marginal probability</h2>
    <p class="normal">A marginal probability distribution<a id="_idIndexMarker1206"/> is the probability<a id="_idIndexMarker1207"/> distribution of a subset of random variables, given the joint probability distribution of all variables. For example, consider that two random variables, <em class="italic">X</em> and <em class="italic">Y</em>, exist, and we already know <img src="../Images/B14070_12_097.png" alt="" style="height: 1.25em !important; vertical-align: -0.35em !important;"/> and we want to calculate <em class="italic">P(x)</em>: </p>
    <p class="center"><img src="../Images/B14070_12_098.png" alt="" style="height: 2.83em !important;"/></p>
    <p class="normal">Intuitively, we are taking the sum over all possible values of <em class="italic">Y</em>, effectively making the probability of <em class="italic">Y = 1</em>. </p>
    <h2 id="_idParaDest-331" class="heading-2">Bayes’ rule</h2>
    <p class="normal">Bayes’ rule gives<a id="_idIndexMarker1208"/> us a way to calculate <img src="../Images/B14070_12_099.png" alt="" style="height: 1.25em !important; vertical-align: -0.31em !important;"/> if we already know <img src="../Images/B14070_12_100.png" alt="" style="height: 1.25em !important; vertical-align: -0.39em !important;"/>, and <img src="../Images/B14070_12_101.png" alt="" style="height: 1.25em !important; vertical-align: -0.21em !important;"/>. We can easily arrive at Bayes’ rule as follows: </p>
    <p class="center"><img src="../Images/B14070_12_102.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">Now let’s take the middle and right parts: </p>
    <p class="center"><img src="../Images/B14070_12_103.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="center"><img src="../Images/B14070_12_104.png" alt="" style="height: 2.50em !important;"/></p>
    <p class="normal">This is Bayes’ rule. Let’s put<a id="_idIndexMarker1209"/> it simply, as shown here: </p>
    <p class="center"><img src="../Images/B14070_12_105.png" alt="" style="height: 2.50em !important;"/></p>
    <h1 id="_idParaDest-332" class="heading-1">Visualizing word embeddings with TensorBoard</h1>
    <p class="normal">When we wanted to visualize<a id="_idIndexMarker1210"/> word embeddings in <em class="chapterRef">Chapter 3</em>, <em class="italic">Word2vec – Learning Word Embeddings,</em> we manually implemented the visualization<a id="_idIndexMarker1211"/> with the t-SNE algorithm. However, you also could use TensorBoard to visualize word embeddings. TensorBoard is a visualization tool provided with TensorFlow. You can use TensorBoard to visualize the TensorFlow variables in your program. This allows you to see how different variables behave over time (for example, model loss/accuracy), so you can identify potential issues in your model.</p>
    <p class="normal">TensorBoard enables you to visualize scalar values (e.g. loss values over training iterations) and vectors as histograms (e.g. model’s layer node activations). Apart from this, TensorBoard also allows you to visualize word embeddings. Therefore, it takes all the required code implementation away from you, if you need to analyze what the embeddings look like. Next, we will see how we can use TensorBoard to visualize word embeddings. The code for this exercise is provided in <code class="inlineCode">tensorboard_word_embeddings.ipynb</code> in the <code class="inlineCode">Appendix</code> folder.</p>
    <h2 id="_idParaDest-333" class="heading-2">Starting TensorBoard</h2>
    <p class="normal">First, we will list the steps for starting TensorBoard. TensorBoard acts as a service and runs on a specific port (by default, on <code class="inlineCode">6006</code>). To start<a id="_idIndexMarker1212"/> TensorBoard, you will need to follow these steps:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Open up Command Prompt (Windows) or Terminal (Ubuntu/macOS).</li>
      <li class="numberedList">Go into the project home directory.</li>
      <li class="numberedList">If you are using the python <code class="inlineCode">virtualenv</code>, activate the virtual environment where you have installed TensorFlow.</li>
      <li class="numberedList">Make sure that you can see the TensorFlow library through Python. To do this, follow these steps:<ol class="alphabeticList" style="list-style-type: lower-alpha;">
          <li class="alphabeticList" value="1">Type in <code class="inlineCode">python3</code>; you will get a <code class="inlineCode">&gt;&gt;&gt;</code> looking prompt</li>
          <li class="alphabeticList">Try <code class="inlineCode">import tensorflow as tf</code></li>
          <li class="alphabeticList">If you can run this successfully, you are fine</li>
          <li class="alphabeticList">Exit the <code class="inlineCode">python</code> prompt (that is, <code class="inlineCode">&gt;&gt;&gt;</code>) by typing <code class="inlineCode">exit()</code></li>
        </ol>
      </li>
      <li class="numberedList">Type in <code class="inlineCode">tensorboard --logdir=models</code>:<ol class="alphabeticList" style="list-style-type: lower-alpha;">
          <li class="alphabeticList" value="1">The <code class="inlineCode">--logdir</code> option points to the directory where you will create data to visualize</li>
          <li class="alphabeticList">Optionally, you can use <code class="inlineCode">--port=&lt;port_you_like&gt;</code> to change the port TensorBoard runs on</li>
        </ol>
      </li>
      <li class="numberedList">You should now get the following message:
        <pre class="programlisting con"><code class="hljs-con">TensorBoard 1.6.0 at &lt;url&gt;;:6006 (Press CTRL+C to quit)
</code></pre>
      </li>
      <li class="numberedList">Enter the <code class="inlineCode">&lt;url&gt;:6006</code> into the web browser. You should<a id="_idIndexMarker1213"/> be able to see an orange dashboard at this point. You won’t have anything to display because we haven’t generated any data.</li>
    </ol>
    <h2 id="_idParaDest-334" class="heading-2">Saving word embeddings and visualizing via TensorBoard</h2>
    <p class="normal">First, we will download<a id="_idIndexMarker1214"/> and load<a id="_idIndexMarker1215"/> the 50-dimensional<a id="_idIndexMarker1216"/> GloVe embeddings<a id="_idIndexMarker1217"/> file (<code class="inlineCode">glove.6B.zip</code>) from <a href="https://nlp.stanford.edu/projects/glove/"><span class="url">https://nlp.stanford.edu/projects/glove/</span></a> and place it in the <code class="inlineCode">Appendix</code> folder. We will load<a id="_idIndexMarker1218"/> the first 50,000 word vectors in the file and later use these to initialize a TensorFlow variable. We will also record the word strings of each word, as we will later provide these as labels for each point to display on TensorBoard:</p>
    <pre class="programlisting code"><code class="hljs-code">vocabulary_size = <span class="hljs-number">50000</span>
embedding_df = [] 
index = []
<span class="hljs-comment"># Open the zip file</span>
<span class="hljs-keyword">with</span> zipfile.ZipFile(<span class="hljs-string">'glove.6B.zip'</span>) <span class="hljs-keyword">as</span> glovezip:
    <span class="hljs-comment"># Read the file with 50 dimensional embeddings</span>
    <span class="hljs-keyword">with</span> glovezip.<span class="hljs-built_in">open</span>(<span class="hljs-string">'glove.6B.50d.txt'</span>) <span class="hljs-keyword">as</span> glovefile:
        <span class="hljs-comment"># Read line by line</span>
        <span class="hljs-keyword">for</span> li, line <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(glovefile):
            <span class="hljs-comment"># Print progress</span>
            <span class="hljs-keyword">if</span> (li+<span class="hljs-number">1</span>)%<span class="hljs-number">10000</span>==<span class="hljs-number">0</span>: <span class="hljs-built_in">print</span>(<span class="hljs-string">'.'</span>,end=<span class="hljs-string">''</span>)
                
            <span class="hljs-comment"># Get the word and the corresponding vector</span>
            line_tokens = line.decode(<span class="hljs-string">'utf-8'</span>).split(<span class="hljs-string">' '</span>)
            word = line_tokens[<span class="hljs-number">0</span>]
            vector = [<span class="hljs-built_in">float</span>(v) <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> line_tokens[<span class="hljs-number">1</span>:]]
            
            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(vector)==<span class="hljs-number">50</span>
            index.append(word)
            <span class="hljs-comment"># Update the embedding matrix</span>
            embedding_df.append(np.array(vector))
            
            <span class="hljs-comment"># If the first 50000 words being read, finish</span>
            <span class="hljs-keyword">if</span> li &gt;= vocabulary_size-<span class="hljs-number">1</span>:
                <span class="hljs-keyword">break</span>
embedding_df = pd.DataFrame(embedding_df, index=index)
</code></pre>
    <p class="normal">We have defined<a id="_idIndexMarker1219"/> our embeddings<a id="_idIndexMarker1220"/> as a pandas DataFrame. It has the vector<a id="_idIndexMarker1221"/> values as columns<a id="_idIndexMarker1222"/> and words as the index. </p>
    <figure class="mediaobject"><img src="../Images/B14070_12_03.png" alt=""/></figure>
    <p class="packt_figref">Figure A.3: GloVe vectors presented as a pandas DataFrame</p>
    <p class="normal">We will need to define TensorFlow-related variables and operations. Before doing this, we will create a directory called <code class="inlineCode">embeddings</code>, which will be used to store the variables:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create a directory to save our model</span>
log_dir = <span class="hljs-string">'embeddings'</span>
os.makedirs(log_dir, exist_ok=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Then, we will define a variable that will be initialized with the word embeddings we copied from the text file earlier:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Save the weights we want to analyse as a variable. </span>
embeddings = tf.Variable(embedding_df.values)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"weights.shape: </span><span class="hljs-subst">{embeddings.shape}</span><span class="hljs-string">"</span>)
<span class="hljs-comment"># Create a checkpoint from embedding</span>
checkpoint = tf.train.Checkpoint(embedding=embeddings)
checkpoint.save(os.path.join(log_dir, <span class="hljs-string">"embedding.ckpt"</span>))
</code></pre>
    <p class="normal">We also need to save a metadata<a id="_idIndexMarker1223"/> file. A metadata file contains labels/images<a id="_idIndexMarker1224"/> or other types of information<a id="_idIndexMarker1225"/> associated with the word<a id="_idIndexMarker1226"/> embeddings, so that when you hover over the embedding visualization, the corresponding points will show the word/label they represent. The metadata file should be of the <code class="inlineCode">.tsv</code> (tab-separated values) format and should contain <code class="inlineCode">vocabulary_size </code>rows in it, where each row contains a word in the order they appear in the embeddings matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(log_dir, <span class="hljs-string">'metadata.tsv'</span>), <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> embedding_df.index:
        f.write(w+<span class="hljs-string">'\n'</span>)
</code></pre>
    <p class="normal">Then, we will need to tell TensorFlow where it can find the metadata for the embedding data we saved to the disk. For this, we need to create a <code class="inlineCode">ProjectorConfig</code> object, which maintains various configuration details about the embedding we want to display. The details stored in the <code class="inlineCode">ProjectorConfig</code> folder will be saved to a file called <code class="inlineCode">projector_config.pbtxt</code> in the <code class="inlineCode">models</code> directory:</p>
    <pre class="programlisting code"><code class="hljs-code">config = projector.ProjectorConfig()
</code></pre>
    <p class="normal">Here, we will populate the required fields of the <code class="inlineCode">ProjectorConfig</code> object we created. First, we will tell it the name of the variable we’re interested in visualizing. Then, we will tell it where it can find the metadata corresponding to that variable:</p>
    <pre class="programlisting code"><code class="hljs-code">config = projector.ProjectorConfig()
<span class="hljs-comment"># You can add multiple embeddings. Here we add only one.</span>
embedding_config = config.embeddings.add()
embedding_config.tensor_name = <span class="hljs-string">"embedding/.ATTRIBUTES/VARIABLE_VALUE"</span>
<span class="hljs-comment"># Link this tensor to its metadata file (e.g. labels).</span>
embedding_config.metadata_path = <span class="hljs-string">'metadata.tsv'</span>
<span class="hljs-comment"># TensorBoard will read this file during startup.</span>
projector.visualize_embeddings(log_dir, config)
</code></pre>
    <p class="normal">Note that we are adding<a id="_idIndexMarker1227"/> the suffix <code class="inlineCode">/.ATTRIBUTES/VARIABLE_VALUE</code> to the name <code class="inlineCode">embedding</code>. This is required<a id="_idIndexMarker1228"/> for TensorBoard<a id="_idIndexMarker1229"/> to find this tensor. TensorBoard<a id="_idIndexMarker1230"/> will read the necessary files at startup:</p>
    <pre class="programlisting code"><code class="hljs-code">projector.visualize_embeddings(log_dir, config)
</code></pre>
    <p class="normal">Now if you load TensorBoard, you should see something similar to <em class="italic">Figure A.4</em>:</p>
    <figure class="mediaobject"><img src="../Images/B14070_12_04.png" alt=""/></figure>
    <p class="packt_figref">Figure A.4: TensorBoard view of the embeddings</p>
    <p class="normal">When you hover over<a id="_idIndexMarker1231"/> the displayed point cloud, it will show<a id="_idIndexMarker1232"/> the label of the word you’re currently<a id="_idIndexMarker1233"/> hovering over, as we provided<a id="_idIndexMarker1234"/> this information in the <code class="inlineCode">metadata.tsv</code> file. Furthermore, you have several options. The first option (shown with a dotted line and marked as <strong class="keyWord">1</strong>) will allow you to select a subset of the full embedding space. You can draw a bounding box over the area of the embedding space you’re interested in, and it will look as shown in <em class="italic">Figure A.5</em>. I have selected the embeddings from the right side of the visualization. You can see the full list of selected words on the right:</p>
    <figure class="mediaobject"><img src="../Images/B14070_12_05.png" alt=""/></figure>
    <p class="packt_figref">Figure A.5: Selecting a subset of the embedding space</p>
    <p class="normal">Another option you have<a id="_idIndexMarker1235"/> is the ability to view words themselves, instead of dots. You can<a id="_idIndexMarker1236"/> do this by selecting<a id="_idIndexMarker1237"/> the second option in <em class="italic">Figure A.4</em> (shown inside a solid box and marked as <strong class="keyWord">2</strong>). This would look as shown in <em class="italic">Figure A.6</em>. Additionally, you<a id="_idIndexMarker1238"/> can pan/zoom/rotate the view to your liking. If you click on the help button (shown within a solid box and marked as <strong class="keyWord">1</strong> in <em class="italic">Figure A.6</em>), it will show you a guide for controlling the view:</p>
    <figure class="mediaobject"><img src="../Images/B14070_12_06.png" alt=""/></figure>
    <p class="packt_figref">Figure A.6: Embedding vectors displayed as words instead of dots</p>
    <p class="normal">Finally, you can change<a id="_idIndexMarker1239"/> the visualization algorithm<a id="_idIndexMarker1240"/> from the panel<a id="_idIndexMarker1241"/> on the left-hand side (shown with a dashed line<a id="_idIndexMarker1242"/> and marked with <strong class="keyWord">3</strong> in <em class="italic">Figure A.4</em>).</p>
    <h1 id="_idParaDest-335" class="heading-1">Summary</h1>
    <p class="normal">Here we discussed some of the mathematical background as well as some implementations we did not cover in the other chapters. First, we discussed the mathematical notation for scalars, vectors, matrices, and tensors. Then, we discussed various operations performed on these data structures such as matrix multiplication and inversion. After that, we discussed various terminology that is useful for understanding probabilistic machine learning, such as probability density functions, joint probability, marginal probability, and Bayes’ rule. Finally, we ended the appendix with a guide to visualizing word embeddings using TensorBoard, a visualization platform that comes with TensorFlow.</p>
  </div>


  <div id="_idContainer695">
    <p class="BM-packtLogo"><img src="../Images/New_Packt_Logo1.png" alt=""/></p>
    <p class="normal"><span class="url">packt.com</span></p>
    <p class="normal">Subscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>
    <h1 id="_idParaDest-336" class="heading-1">Why subscribe?</h1>
    <ul>
      <li class="bulletList">Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</li>
      <li class="bulletList">Improve your learning with Skill Plans built especially for you</li>
      <li class="bulletList">Get a free eBook or video every month</li>
      <li class="bulletList">Fully searchable for easy access to vital information</li>
      <li class="bulletList">Copy and paste, print, and bookmark content</li>
    </ul>
    <p class="normal">At <span class="url">www.packt.com</span>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</p>
    <p class="eop"/>
    <h1 id="_idParaDest-337" class="chapterTitle">Other Books You May Enjoy</h1>
    <p class="normal">If you enjoyed this book, you may be interested in these other books by Packt:</p>
    <p class="BM-bookCover"><a href="https://www.packtpub.com/product/transformers-for-natural-language-processing/9781803247335?_ga=2.5602675.1586621222.1658751433-1060321437.1657688636"><img src="../Images/9781803247335.png" alt=""/></a></p>
    <p class="normal"><strong class="keyWord">Transformers for Natural Language Processing, Second Edition</strong></p>
    <p class="normal">Denis Rothman</p>
    <p class="normal">ISBN: 9781803247335</p>
    <ul>
      <li class="bulletList">Find out how ViT and CLIP label images (including blurry ones!) and create images from a sentence using DALL-E</li>
      <li class="bulletList">Discover new techniques to investigate complex language problems</li>
      <li class="bulletList">Compare and contrast the results of GPT-3 against T5, GPT-2, and BERT-based transformers</li>
      <li class="bulletList">Carry out sentiment analysis, text summarization, casual speech analysis, machine translations, and more using TensorFlow, PyTorch, and GPT-3</li>
      <li class="bulletList">Measure the productivity of key transformers to define their scope, potential, and limits in production</li>
    </ul>
    <p class="eop"/>
    <p class="BM-bookCover"><a href="https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312?_ga=2.204407376.1586621222.1658751433-1060321437.1657688636"><img src="../Images/9781801819312.png" alt=""/></a></p>
    <p class="normal"><strong class="keyWord">Machine Learning with PyTorch and Scikit-Learn</strong></p>
    <p class="normal">Sebastian Raschka</p>
    <p class="normal">Yuxi (Hayden) Liu</p>
    <p class="normal">Vahid Mirjalili</p>
    <p class="normal">ISBN: 9781801819312</p>
    <ul>
      <li class="bulletList">Explore frameworks, models, and techniques for machines to ‘learn’ from data</li>
      <li class="bulletList">Use scikit-learn for machine learning and PyTorch for deep learning</li>
      <li class="bulletList">Train machine learning classifiers on images, text, and more</li>
      <li class="bulletList">Build and train neural networks, transformers, and boosting algorithms</li>
      <li class="bulletList">Discover best practices for evaluating and tuning models</li>
      <li class="bulletList">Predict continuous target outcomes using regression analysis</li>
      <li class="bulletList">Dig deeper into textual and social media data using sentiment analysis</li>
    </ul>
    <p class="eop"/>
    <h1 id="_idParaDest-338" class="heading-1">Packt is searching for authors like you</h1>
    <p class="normal">If you’re interested in becoming an author for Packt, please visit <span class="url">authors.packtpub.com</span> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>
  </div>
  <div id="_idContainer696">
    <h1 id="_idParaDest-339" class="heading-1">Share your thoughts</h1>
    <p class="normal">Now you’ve finished <em class="italic">Natural Language Processing with TensorFlow, Second Edition</em>, we’d love to hear your thoughts! If you purchased the book from Amazon, please <a href="https://packt.link/r/1838641351"><span class="url">click here to go straight to the Amazon review page</span></a> for this book and share your feedback or leave a review on the site that you purchased it from.</p>
    <p class="normal">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p>
  </div>
</body></html>