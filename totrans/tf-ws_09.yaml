- en: 9\. Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to handle real sequential data. You will
    extend your knowledge of **artificial neural network** (**ANN**) models and **recurrent
    neural network** (**RNN**) architecture for training sequential data. You will
    also learn how to build an RNN model with an LSTM layer for natural language processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained hands-on experience of applying
    multiple LSTM layers to build RNNs for stock price predictions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequential data refers to datasets in which each data point is dependent on
    the previous ones. Think of it like a sentence, which is composed of a sequence
    of words that are related to each other. A verb will be linked to a subject and
    an adverb will be related to a verb. Another example is a stock price, where the
    price on a particular day is related to the price of the previous days. Traditional
    neural networks are not fit for processing this kind of data. There is a specific
    type of architecture that can ingest sequences of data. This chapter will introduce
    you to such models—known as **recurrent neural networks** (**RNNs**).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: An RNN model is a specific type of deep learning architecture in which the output
    of the model feeds back into the input. Models of this kind have their own challenges
    (known as vanishing and exploding gradients) that will be addressed later in the chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In many ways, an RNN is a representation of how a brain might work. RNNs use
    memory to help them learn. But how can they do this if information only flows
    in one direction? To understand this, you'll need to first review sequential data.
    This is a type of data that requires a working memory to process data effectively.
    Until now, you have only explored non-sequential models, such as a perceptron
    or CNN. In this chapter, you will look at sequential models such as RNN, LSTM,
    or GRU.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: Sequential versus non-sequential models'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.1: Sequential versus non-sequential models'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Sequential Data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequential data is information that happens in a sequence and is related to
    past and future data. An example of sequential data is time series data; as you
    perceive it, time only travels in one direction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a ball (as in *Figure 9.2*), and you want to predict where
    this ball will travel next. If you have no prior information about the direction
    from which the ball was thrown, you will simply have to guess. However, if in
    addition to the ball's current location, you also had information about its previous
    location, the problem would be much simpler. To be able to predict the ball's
    next location, you need the previous location information in a sequential (or
    ordered) form to make a prediction about future events.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2: Direction of the ball'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.2: Direction of the ball'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: RNNs function in a way that allows the sequence of the information to retain
    value with the help of internal memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）能够通过内部记忆帮助信息的顺序保持有效，从而发挥作用。
- en: You'll take a look at some examples of sequential data in the following section.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将展示一些顺序数据的示例。
- en: Examples of Sequential Data
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序数据的示例
- en: Sequential data is a specific type of data where the order of each piece of
    information is important, and they all depend on each other.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序数据是一种特殊类型的数据，其中每一条信息的顺序很重要，它们彼此依赖。
- en: One example of sequential data is financial data, such as stock prices. If you
    want to predict future data values for a given stock, you need to use previous
    values in time. In fact, you will work on stock prediction in *Exercise 9.01*,
    *Training an ANN for Sequential Data – Nvidia Stock Prediction*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序数据的一个例子是金融数据，比如股票价格。如果你想预测给定股票的未来数据值，就需要使用先前的时间值。事实上，你将在*练习 9.01*中进行股票预测，*为顺序数据训练一个人工神经网络——Nvidia
    股票预测*。
- en: 'Audio and text can also be considered sequential data. Audio can be split up
    into a sequence of sound waves, and text can be split up into sequences of either
    characters or words. The sound waves or sequences of characters or words should
    be processed in order to convey the desired result. Beyond these two examples
    that you encounter every day, there are many more examples in which sequential
    processing may be useful, from analyzing medical signals such as EEGs, projecting
    stock prices, and inferring and understanding genomic sequences. There are three
    categories of sequential data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和文本也可以视为顺序数据。音频可以分割成一系列声波，而文本可以分割成一系列字符或单词。声波或字符、单词序列应该按照顺序处理，以传达预期的结果。除了这两个日常遇到的例子外，顺序处理在很多其他领域也非常有用，比如分析医学信号（如脑电图）、预测股票价格以及推断和理解基因序列等。顺序数据有三种类型：
- en: '**Many-to-One** produces one output from many inputs.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一** 从多个输入产生一个输出。'
- en: '**One-to-Many** produces many outputs from one input.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多** 从一个输入产生多个输出。'
- en: '**Many-to-Many** produces many outputs from many inputs.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多** 从多个输入产生多个输出。'
- en: '![Figure 9.3: Categories of sequential data'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3：顺序数据的分类'
- en: '](img/B16341_09_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_03.jpg)'
- en: 'Figure 9.3: Categories of sequential data'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：顺序数据的分类
- en: 'Consider another example. Suppose you have a language model with a sentence
    or a phrase and you are trying to predict the word that comes next, as in the
    following figure:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，假设你有一个语言模型，输入是一个句子或短语，你正在尝试预测接下来的单词，如下图所示：
- en: '![Figure 9.4: Sentence example'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4：句子示例'
- en: '](img/B16341_09_04.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_04.jpg)'
- en: 'Figure 9.4: Sentence example'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：句子示例
- en: Say you're given the words `yesterday I took my car out for a…`, and you want
    to try to predict the next word, `drive`. One way you could do this is by building
    a deep neural network such as a feed-forward neural network. However, you would
    immediately run into a problem. A feed-forward network can only take a fixed-length
    input vector as its input; you have to specify the size of that input right from
    the start.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，你被给定了这句话 `yesterday I took my car out for a…`，并且你想预测下一个单词是 `drive`。你可以通过构建一个深度神经网络（如前馈神经网络）来实现这一点。然而，你会立即遇到一个问题：前馈网络只能接受固定长度的输入向量；你必须从一开始就指定输入的大小。
- en: Because of this, your model needs a way to be able to handle variable-length
    inputs. One way you can do this is by using a fixed window. That means that you
    force your input vector to be just a certain length. For example, you can split
    the sentence into groups of two consecutive words (also called a **bi-gram**)
    and predict the next one. This means that no matter where you're trying to make
    that next prediction, your model will only be taking in the previous two words
    as its input. You need to consider how you can numerically represent this data.
    One way you can do this is by taking a fixed-length vector and allocating some
    space in that vector for the first word and some space in that vector for the
    second word. In those spaces, encode the identity of each word. However, this
    is problematic.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你的模型需要能够处理可变长度输入的方式。你可以通过使用固定窗口来实现这一点。这意味着你强制将输入向量限制为特定的长度。例如，你可以将句子拆分为两个连续单词的组（也称为**双元语法**），然后预测下一个单词。这意味着，无论你尝试在哪个位置进行下一步预测，模型仅会将前两个单词作为输入。你需要考虑如何以数字的形式表示这些数据。一种方法是使用固定长度的向量，并为第一个单词分配一些空间，为第二个单词分配一些空间。在这些空间中，编码每个单词的身份。然而，这种方法存在问题。
- en: Why? Because you're using only a portion of the information available (that
    is, two consecutive words only). You have access to a limited window of data that
    doesn't give enough context to accurately predict what will be the next word.
    That means you cannot effectively model long-term dependencies. This is important
    in sentences like the one in *Figure 9.5* where you clearly need information from
    much earlier in the sentence to be able to accurately predict the next word.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？因为你只使用了部分可用信息（即仅仅是两个连续的单词）。你只能访问有限的数据窗口，这不足以提供足够的上下文来准确预测下一个单词。这意味着你无法有效地建模长期依赖性。这在像*图
    9.5*中的句子中非常重要，在那里你显然需要来自句子早期的信息，以便准确预测下一个单词。
- en: '![Figure 9.5: Sentence example'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5：句子示例'
- en: '](img/B16341_09_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_05.jpg)'
- en: 'Figure 9.5: Sentence example'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：句子示例
- en: If you were only looking at the past two or three words, you wouldn't be able
    to make this next prediction, which you know is `Italian`. So, this means that
    you really need a way to integrate the information in the sentence from start
    to finish.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只看过去的两个或三个单词，你就无法做出下一个预测，而你知道这个预测是`意大利语`。因此，这意味着你确实需要一种方法来整合从句子开始到结束的所有信息。
- en: To do this, you could use a set of counts as a fixed-length vector and use the
    entire sentence. This method is known as **bag of words**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，你可以使用一组计数作为固定长度向量，并使用整个句子。这种方法被称为**词袋模型**。
- en: You have a fixed-length vector regardless of the identity of the sentence, but
    what differs is adding the counts over this vocabulary. You can feed this into
    your model as an input to generate a prediction.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个固定长度的向量，不管句子的身份是什么，但不同之处在于，添加了对这个词汇表的计数。你可以将其作为输入传递给模型，以生成预测结果。
- en: However, there's another big problem with this. Using just the counts means
    that you lose all sequential information and all information about the prior history.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里还有一个大问题。仅仅使用计数意味着你失去了所有的顺序信息以及所有关于前历史的信息。
- en: Consider *Figure 9.6*. So, these two sentences, which have completely opposite
    semantic meanings would have the exact same representations in this bag of words
    format. This is because they have the exact same list of words, just in a different
    order. So, obviously, this isn't going to work. Another idea could be simply to
    extend the fixed window.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图 9.6*。所以，这两句话，尽管语义完全相反，但在这种词袋格式中会有完全相同的表示。因为它们有完全相同的单词列表，只是顺序不同。所以，显然，这种方法行不通。另一个想法是简单地扩展固定窗口。
- en: '![Figure 9.6: Bag of words example'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6：词袋示例'
- en: '](img/B16341_09_06.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_09_06.jpg)'
- en: 'Figure 9.6: Bag of words example'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：词袋示例
- en: Now, consider *Figure 9.7*. You can represent your sentence in this way, feed
    the sentence into your model, and generate your prediction. The problem is that
    if you were to feed this vector into a feed-forward neural network, each of these
    inputs, `yesterday I took my car`, would have a separate weight connecting it
    to the network. So, if you were to repeatedly see the word `yesterday` at the
    beginning of the sentence, the network may be able to learn that `yesterday` represents
    a time or a setting. However, if `yesterday` were to suddenly appear later in
    that fixed-length vector, at the end of a sentence, the network may have difficulty
    understanding the meaning of `yesterday`. This is because the parameters that
    are at the end of a vector may never have seen the term `yesterday` before, and
    the parameters from the beginning of the sentence weren't shared across the entire
    sequence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7: Sentence example'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_07.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.7: Sentence example'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'So, you need to be able to handle variable-length input and long-term dependencies,
    track sequential order, and have parameters that can be shared across the entirety
    of your sequence. Specifically, you need to develop models that can do the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Handle variable-length input sequences.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track long-term dependencies in the data.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain information about the sequence's order.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share parameters across the entirety of the sequence.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you do this with a model where information only flows in one direction?
    You need a different kind of neural network. You need a recursive model. You will
    practice processing sequential data in the following exercise.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9.01: Training an ANN for Sequential Data – Nvidia Stock Prediction'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will build a simple ANN model to predict the Nvidia stock
    price. But unlike examples from previous chapters, this time the input data is
    sequential. So, you need to manually do some processing to create a dataset that
    will contain the price of the stock for a given day as the target variable and
    the price for the previous 60 days as features. You are required to split the
    data into training and testing sets before and after the date `2019-01-01`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the `NVDA.csv` dataset here: [https://packt.link/Mxi80](https://packt.link/Mxi80).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter or Colab notebook.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the libraries needed. Use `numpy` for computation, `matplotlib` for
    plotting visualization, `pandas` to help work with your dataset, and `MinMaxScaler`
    to scale the dataset between zero and one:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Use the `read_csv()` function to read in the CSV file and store your dataset
    in a pandas DataFrame, `data`, for manipulation:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Call the `head()` function on your data to take a look at the first five rows
    of your DataFrame:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should get the following output:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.8: First five rows of output'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_08.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.8: First five rows of output'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding table shows the raw data. You can see that each row represents
    a day where you have information about the stock price when the market opened
    and closed, the highest price, the lowest price, and the adjusted close price
    of the stock (taking into account dividend or stock split, for instance).
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上表展示了原始数据。你可以看到，每一行代表一天，包含了当天开盘价和收盘价、最高价、最低价以及调整后的收盘价（例如考虑到股息或股票拆分等因素）。
- en: 'Now, split the training data. Use all data that is older than `2019-01-01`
    using the `Date` column for your training data. Save it as `data_training`. Save
    this in a separate file by using the `copy()` method:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，拆分训练数据。使用`Date`列中所有2019年1月1日之前的数据作为训练数据。将其保存为`data_training`。通过使用`copy()`方法，将其保存在单独的文件中：
- en: '[PRE3]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, split the test data. Use all data that is more recent than or equal to
    `2019-01-01` using the `Date` column. Save it as `data_test`. Save this in a separate
    file by using the `copy()` method:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，拆分测试数据。使用`Date`列中所有2019年1月1日及之后的数据，将其保存为`data_test`。通过使用`copy()`方法，将其保存在单独的文件中：
- en: '[PRE4]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Use `drop()` to remove your `Date` and `Adj Close` columns in your DataFrame.
    Remember that you used the `Date` column to split your training and test sets,
    so the date information is not needed. Use `axis = 1` to specify that you also
    want to drop labels from your columns. To make sure it worked, call the `head()`
    function to take a look at the first five rows of the DataFrame:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`drop()`方法删除数据框中的`Date`和`Adj Close`列。记住，你使用了`Date`列来划分训练集和测试集，因此不需要日期信息。使用`axis
    = 1`来指定你还希望删除列标签。为了确保操作成功，调用`head()`函数查看数据框的前五行：
- en: '[PRE5]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should get the following output:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '![Figure 9.9: New training data'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.9：新的训练数据'
- en: '](img/B16341_09_09.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_09.jpg)'
- en: 'Figure 9.9: New training data'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.9：新的训练数据
- en: This is the output you should get after removing those two columns.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是你在删除那两列之后应该得到的输出：
- en: 'Create a scaler from `MinMaxScaler` to scale `training_data` to numbers between
    zero and one. Use the `fit_transform` function to fit the model to the data and
    then transform the data according to the fitted model:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个来自`MinMaxScaler`的缩放器，将`training_data`缩放到0和1之间。使用`fit_transform`函数将模型拟合到数据并根据拟合的模型转换数据：
- en: '[PRE6]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should get the following output:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '![Figure 9.10: Scaled training data'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.10：已缩放的训练数据'
- en: '](img/B16341_09_10.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_09_10.jpg)'
- en: 'Figure 9.10: Scaled training data'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.10：已缩放的训练数据
- en: 'Split your data into `X_train` and `y_train` datasets:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为`X_train`和`y_train`数据集：
- en: '[PRE7]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Check the shape of `training_data`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`training_data`的形状：
- en: '[PRE8]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should get the following output:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE9]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see there are 868 observations in the training set.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到训练集包含868个观测值。
- en: 'Create a training dataset that has the previous 60 days'' stock prices so that
    you can predict the closing stock price for day 61\. Here, `X_train` will have
    two columns. The first column will store the values from 0 to 59, and the second
    will store values from 1 to 60\. In the first column of `y_train`, store the 61st
    value at index 60, and in the second column, store the 62nd value at index 61\.
    Use a `for` loop to create data in 60 time steps:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个训练数据集，包含过去60天的股价数据，这样你就可以预测第61天的收盘价。在这里，`X_train`将有两列。第一列将存储从0到59的值，第二列将存储从1到60的值。在`y_train`的第一列存储第61个值（索引为60），在第二列存储第62个值（索引为61）。使用`for`循环创建60个时间步的数据：
- en: '[PRE10]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Convert `X_train` and `y_train` into NumPy arrays:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`X_train`和`y_train`转换为NumPy数组：
- en: '[PRE11]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Call the `shape()` function on `X_train` and `y_train`:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`X_train`和`y_train`调用`shape()`函数：
- en: '[PRE12]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should get the following output:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE13]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding snippet shows that the prepared training set contains `808` observations
    with `60` days of data for the five features you kept (`Open`, `Low`, `High`,
    `Close`, and `Volume`).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码片段显示，准备好的训练集包含`808`个观测值，数据集包含60天的五个特征（`Open`、`Low`、`High`、`Close`和`Volume`）。
- en: 'Transform the data into a 2D matrix with the shape of the sample (the number
    of samples and the number of features in each sample). Stack the features for
    all 60 days on top of each other to get an output size of `(808, 300)`. Use the
    following code for this purpose:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为形状为样本矩阵（每个样本的样本数和特征数）的二维矩阵。将60天的所有特征堆叠在一起，得到输出形状为`(808, 300)`。使用以下代码完成此操作：
- en: '[PRE14]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You should get the following output:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到如下输出：
- en: '[PRE15]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, build an ANN. You will need some additional libraries for this. Use `Sequential`
    to initialize the neural net, `Input` to add an input layer, `Dense` to add a
    dense layer, and `Dropout` to help prevent overfitting:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Initialize the neural network by calling `regressor_ann = Sequential()`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Add an input layer with `shape` as `300`:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, add the first dense layer. Set it to `512` units, which will be your
    dimensionality for the output space. Use a ReLU activation function. Finally,
    add a dropout layer that will remove 20% of the units during training to prevent overfitting:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Add another dense layer with `128` units, ReLU as the activation function,
    and a dropout of `0.3`:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Add another dense layer with `64` units, ReLU as the activation function, and
    a dropout of `0.4`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Again, add another dense layer with `128` units, ReLU as the activation function,
    and a dropout of `0.3`:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Add a final dense layer with one unit:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Check the summary of the model:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You will get valuable information about your model layers and parameters.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.11: Model summary'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_11.jpg)'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.11: Model summary'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `compile()` method to configure your model for training. Choose Adam
    as your optimizer and mean squared error to measure your loss function:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, fit your model and set it to run on `10` epochs. Set your batch size
    to `32`:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You should get the following output:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.12: Training the model'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_12.jpg)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.12: Training the model'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test and predict the stock price and prepare the dataset. Check your data by
    calling the `head()` method:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should get the following output:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.13: First five rows of a DataFrame'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_13.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.13: First five rows of a DataFrame'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `tail(60)` method to create a `past_60_days` variable, which consists
    of the last 60 days of data in the training set. Add the `past_60_days` variable
    to the test data with the `append()` function. Assign `True` to `ignore_index`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, prepare your test data for predictions by repeating what you did for the
    training data in *steps 8* to *15*:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should get the following output:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Test some predictions for your stock prices by calling the `predict()` method
    on `X_test`:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Before looking at the results, reverse the scaling you did earlier so that
    the number you get as output will be at the correct scale using the `StandardScaler`
    utility class that you imported with `scaler.scale_`:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You should get the following output:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.14: Using StandardScaler'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_14.jpg)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.14: Using StandardScaler'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the first value in the preceding array to set your scale in preparation
    for the multiplication of `y_pred` and `y_test`. Recall that you are converting
    your data back from your earlier scale, in which you converted all values to between
    zero and one:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You should get the following output:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Multiply `y_pred` and `y_test` by `scale` to convert your data back to the
    proper values:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Review the real Nvidia stock price and your predictions:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should get the following output:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.15: Real Nvidia stock price versus your predictions'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_15.jpg)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.15: Real Nvidia stock price versus your predictions'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding graph, you can see that your trained model is able to capture
    some of the trends of the Nvidia stock price. Observe that the predictions are
    quite different from the real values. It is evident from this result that ANNs
    are not suited for sequential data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you saw the inability of simple ANNs to deal with sequential
    data. In the next section, you will learn about recurrent neural networks, which
    are designed to learn from the temporal dimensionality of sequential data. Then,
    in *Exercise 9.02*, *Building an RNN with LSTM Layer Nvidia Stock Prediction*,
    you will perform predictions on the same Nvidia stock price dataset using RNNs
    and compare your results.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first formulation of a recurrent-like neural network was created by John
    Hopfield in 1982\. He had two motivations for doing so:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Sequential processing of data
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling of neuronal connectivity
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, an RNN processes input data at each time step and stores information
    in its memory that will be used for the next step. Information is first transformed
    into vectors that can be processed by machines. The RNN then processes the vector
    sequence one at a time. As it processes each vector, it passes the previous hidden
    state. The hidden state retains information from the previous step, acting as
    a type of memory. It does this by combining the input and the previous hidden
    state with a tanh function that compresses the values between `-1` and `1`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this is how the RNN functions. RNNs don't need a lot of computation
    and work well with short sequences.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16: RNN data flow'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_16.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.16: RNN data flow'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Now turn your attention to applying neural networks to problems that involve
    sequential processing of data. You've already learned a bit about why these sorts
    of tasks require a fundamentally different type of network architecture from what
    you've seen so far.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: RNN Architecture
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will go through the key principles behind RNNs, how they are fundamentally
    different from what you've learned so far, and how RNN computation actually works.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: But before you do that, take one step back and consider the standard feed-forward
    neural network that was discussed previously.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: In feed-forward neural networks, data propagates in one direction only, that
    is, from input to output.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, you need a different kind of network architecture to handle sequential
    data. RNNs are particularly well-suited to handling cases in which you have a
    sequence of inputs rather than a single input. These are great for problems in
    which a sequence of data is being propagated to give a single output.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine that you are training a model that takes a sequence of
    words as input and outputs an emotion associated with that sequence. Similarly,
    consider cases in which, instead of returning a single output, you could have
    a sequence of inputs and propagate them through your network, where each time
    step in the sequence generates an output.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, RNNs are networks that offer a mechanism to persist previously processed
    data over time and use it to make future predictions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17: RNN computation'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_17.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.17: RNN computation'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, at some time step denoted by t, the RNN takes in `X`t
    as the input, and at that time step, it computes a prediction value, `Y`t, which
    is the output of the network.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: In addition to that output, it saved an internal state, called update, `H`t.
    This internal state from time step `t` can then be used to complement the input
    of the next time step `t+1`. So, basically, it provides information about the
    previous step to the next one. This mechanism is called **recurrent** because
    information is being passed from one time step to the next within the network.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: What's really happening here? This is done by using a simple recurrence relation
    to process the sequential data. RNNs maintain internal state, `H`t, and combine
    it with the next input data, `X`t+1, to make a prediction, `Y`t+1, and store the
    new internal state, `H`t+1\. The key idea is that the state update is a combination
    of the previous state time step as well as the current input that the network
    is receiving.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s important to note that, in this computation, it''s the same function
    `f` of `W` and the same set of parameters that are used at every time step, and
    it''s those sets of parameters that you learn during the course of training. To
    get a better sense of how these networks work, step through the RNN algorithm:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: You begin by initializing your RNN and the hidden state of that network. You
    can denote a sentence for which you are interested in predicting the next word.
    The RNN computation simply consists of them looping through the words in this sentence.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each time step, you feed both the current word that you're considering, as
    well as the previous hidden state of your RNN into the network. This can then
    generate a prediction for the next word in the sequence and use this information
    to update its hidden state.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, after you've looped through all the words in the sentence, your prediction
    for that missing word is simply the RNN's output at that final time step.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see in the following diagram, this RNN computation includes both
    the internal state update and the formal output vector.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18: RNN data flow'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_18.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.18: RNN data flow'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Given the input vector, `X`t, the RNN applies a function to update its hidden
    state. This function is simply a standard neural net operation. It consists of
    multiplication by a weight matrix and the application of a non-linearity activation
    function. The key difference is that, in this case, you're feeding in both the
    input vector, `X`t, and the previous state as inputs to this function, `H`t-1.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Next, you apply a non-linearity activation function such as tanh to the previous
    step. You have these two weight matrices, and finally, your output, `y`t, at a
    given time step is then a modified, transformed version of this internal state.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: After you've looped through all the words in the sentence, your prediction for
    that missing word is simply the RNN's output at that final time step, after all
    the words have been fed through the model. So, as mentioned, RNN computation includes
    both internal state updates and formal output vectors.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Another way you can represent RNNs is by unrolling their modules over time.
    You can think of RNNs as having multiple copies of the same network, where each
    passes a message on to its descendant.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19: Computational graph with time'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_19.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.19: Computational graph with time'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: In this representation, you can make your weight matrices explicit, beginning
    with the weights that transform the input to the `H` weights that are used to
    transform the previous hidden state to the current hidden state, and finally the
    hidden state to the output.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that you use the same weight matrices at every time step.
    From these outputs, you can compute a loss at each time step. The computation
    of the loss will then complete your forward propagation through the network. Finally,
    to define the total loss, you simply sum the losses from all of the individual
    time steps. Since your loss is dependent on each time step, this means that, in
    training the network, you will have to also involve time as a component.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've got a bit of a sense of how these RNNs are constructed and how
    they function, you can walk through a simple example of how to implement an RNN
    from scratch in TensorFlow.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet uses a simple RNN from `keras.models.Sequential`. You
    specify the number of units as `1` and set the first input dimension to `None`
    as an RNN can process any number of time steps. A simple RNN uses tanh activation
    by default:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The preceding code creates a single layer with a single neuron.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'That was easy enough. Now you need to stack some additional recurrent layers.
    The code is similar, but there is a key difference here. You will notice `return_sequences=True`
    on all but the last layer. This is to ensure that the output is a 3D array. As
    you can see, the first two layers each have `20` units:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The RNN is defined as a layer, and you can build it by inheriting it from the
    layer class. You can also initialize your weight matrices and the hidden state
    of your RNN cell to zero.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The key step here is defining the call function, which describes how you make
    a forward pass through the network given an input `X`. And, to break down this
    call function, you would first update the hidden state according to the equation
    discussed previously.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Take the previous hidden state and the input `X`, multiply them by the relevant
    weight matrices, add them together, and then pass them through a non-linearity,
    like a hyperbolic tangent (tanh).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Then, the output is simply a transformed version of the hidden state, and at
    each time step, you return both the current output and the updated hidden state.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow has made it easy by having a built-in dense layer. The same applies
    to RNNs. TensorFlow has implemented these types of RNN cells with the simple RNN
    layer. But this type of layer has some limitations, such as vanishing gradients.
    You will look at this problem in the next section before exploring different types
    of recurrent layers.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing Gradient Problem
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you take a closer look at how gradients flow in this chain of repeating modules,
    you can see that between each time step you need to perform matrix multiplication.
    That means that the computation of the gradient—that is, the derivative of the
    loss with respect to the parameters, tracing all the way back to your initial
    state—requires many repeated multiplications of this weight matrix, as well as
    repeated use of the derivative of your activation function.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'You can have one of two scenarios that could be particularly problematic: the
    exploding gradient problem or the vanishing gradient problem.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The exploding gradients problem is when gradients become continuously larger
    and larger due to the matrix multiplication operation, and you can't optimize
    them anymore. One way you may be able to mitigate this is by performing what's
    called gradient clipping. This amounts to scaling back large gradients so that
    their values are smaller and closer to `1`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: You can also have the opposite problem where your gradients are too small. This
    is what is known as the vanishing gradient problem. This is when gradients become
    increasingly smaller (close to `0`) as you make these repeated multiplications,
    and you can no longer train the network. This is a very real problem when it comes
    to training RNNs.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a scenario in which you keep multiplying a number by some
    number that's in between zero and one. As you keep doing this repeatedly, that
    number is constantly shrinking until, eventually, it vanishes and becomes 0\.
    When this happens to gradients, it's hard to propagate errors further back into
    the past because the gradients are becoming smaller and smaller.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Consider the earlier example from the language model where you were trying to
    predict the next word. If you're trying to predict the last word in the following
    phrase, it's relatively clear what the next word is going to be. There's not that
    much of a gap between the key relevant information, such as the word "fish," and
    the place where the prediction is needed.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20: Word prediction'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_20.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.20: Word prediction'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: However, there are other cases where more context is necessary, like in the
    following example. Information from early in the sentence, `She lived in Spain`,
    suggests that the next word of the sentence after `she speaks fluent` is most
    likely the name of a language, `Spanish`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21: Sentence example'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_21.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.21: Sentence example'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: But you need the context of `Spain`, which is located at a much earlier position
    in this sentence, to be able to fill in the relevant gaps and identify which language
    is correct. As this gap between words that are semantically important grows, RNNs
    become increasingly unable to connect the dots and link these relevant pieces
    of information together. That is due to the vanishing gradient problem.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: How can you alleviate this? The first trick is simple. You can choose either
    tanh or sigmoid as your activation function. Both of these functions have derivatives
    that are less than `1`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Another simple trick you can use is to initialize the weights for the parameters
    of your network. It turns out that initializing the weights to the identity matrix
    helps prevent them shrinking to zero too rapidly during back-propagation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: But the final and most robust solution is to use a slightly more complex recurrent
    unit that can track long-term dependencies in the data more effectively. It can
    do this by controlling what information is passed through and what information
    is used to update its internal state. Specifically, this is the concept of a gated
    cell, like in the LSTM layer, which is the focus of the next section.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory Network
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LSTMs are well-suited to learning long-term dependencies and overcoming the
    vanishing gradient problem. They are very performant models for sequential data,
    and they're widely used by the deep learning community.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs have a chain-like structure. In an LSTM, the repeating unit contains different
    interacting layers. The key point is that these layers interact to selectively
    control the flow of information within the cell.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The key building block of the LSTM is a structure called a gate, which functions
    to enable the LSTM to selectively add or remove information from its cell state.
    Gates consist of a neural net layer like a sigmoid.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22: LSTM architecture'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_22.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.22: LSTM architecture'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Take a moment to think about what a gate like this would do in an LSTM. In this
    case, the sigmoid function would force its input to be between `0` and `1`. You
    can think of this mechanism as capturing how much of the information that's passed
    through the gate should be retained. It's between zero and one. This effectively
    gates the flow of information.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs process information through four simple steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the LSTM is to decide what information is going to be thrown
    away from the cell state, to forget irrelevant history. This is a function of
    both the prior internal state, `H`t-1, and the input, `X`t, because some of that
    information may not be important.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the LSTM decides what part of the new information is relevant and uses
    this to store this information in its cell state.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it takes both the relevant parts of the prior information, as well as
    the current input, and uses this to selectively update its cell state.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, it returns an output, and this is known as the output gate, which
    controls what information encoded in the cell state is sent to the network.![Figure
    9.23: LSTM processing steps'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16341_09_23.jpg)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.23: LSTM processing steps'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'The key takeaway here for LSTMs is the sequence of how they regulate information
    flow and storage. Once again, LSTMs operate as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Forgetting irrelevant history
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing what's new and what's important
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using its internal memory to update the internal state
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating an output
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important property of LSTMs is that all these different gating and update
    mechanisms work to create an internal cell state, `C`, which allows the uninterrupted
    flow of gradients through time. You can think of it as sort of a highway of cell
    states where gradients can flow uninterrupted. This enables you to alleviate and
    mitigate the vanishing gradient problem that's seen with standard RNNs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are able to maintain this separate cell state independently of what is
    output, and they use gates to control the flow of information by forgetting irrelevant
    history, storing relevant new information, selectively updating their cell state,
    and then returning a filtered version as the output.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The key point in terms of training and LSTMs is that maintaining the separate
    independent cell state allows the efficient training of an LSTM to backpropagate
    through time, which is discussed later.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've gone through the fundamental workings of RNNs, the backpropagation
    through time algorithm, and a bit about the LSTM architecture, you can put some
    of these concepts to work in the following example.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following LSTM model:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'First, you have initialized a neural network by calling `regressor = Sequential()`.
    Again, it''s important to note that in the last line you omit `return_sequences
    = True` because it is the final output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Then, the LSTM layer is added. In the first instance, set the LSTM layer to
    `50` units. Use a relu activation function and specify the shape of the training
    set. Finally, the dropout layer is added with `regressor.add(Dropout(0.2)`. The
    `0.2` means that 20% of the layers will be removed. Set `return_sequences = True`,
    which allows the return of the last output.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, add three more LSTM layers and one dense layer to the LSTM model.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are familiar with the basic concepts surrounding working with sequential
    data, it's time to complete the following exercise using some real data.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9.02: Building an RNN with an LSTM Layer – Nvidia Stock Prediction'
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will be working on the same dataset as for *Exercise 9.01*,
    *Training an ANN for Sequential Data – Nvidia Stock Prediction*. You will still
    try to predict the Nvidia stock price based on the data of the previous 60 days.
    But this time, you will be training an LSTM model. You will need to split the
    data into training and testing sets before and after the date `2019-01-01`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the `NVDA.csv` dataset here: [https://packt.link/Mxi80](https://packt.link/Mxi80).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to prepare the dataset like in *Exercise 9.01*, *Training an
    ANN for Sequential Data – Nvidia Stock Prediction* (*steps 1* to *15*) before
    applying the following code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Start building the LSTM. You will need some additional libraries for this.
    Use `Sequential` to initialize the neural net, `Dense` to add a dense layer, `LSTM`
    to add an LSTM layer, and `Dropout` to help prevent overfitting:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Initialize the neural network by calling `regressor = Sequential()`. Add four
    LSTM layers with `50`, `60`, `80`, and `120` units each. Use a ReLU activation
    function and assign `True` to `return_sequences` for all but the last LSTM layer.
    Provide the shape of your training set to the first LSTM layer. Finally, add dropout
    layers with 20%, 30%, 40%, and 50% dropouts:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Check the summary of the model using the `summary()` method:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You should get the following output:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.24: Model summary'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_24.jpg)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.24: Model summary'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see from the preceding figure, the summary provides valuable information
    about all model layers and parameters. This is a good way to make sure that your
    layers are in the order you wish and that they have the proper output shapes and
    parameters.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `compile()` method to configure your model for training. Choose Adam
    as your optimizer and mean squared error to measure your loss function:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Fit your model and set it to run on `10` epochs. Set your batch size equal
    to `32`:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You should get the following output:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.25: Training the model'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_25.jpg)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.25: Training the model'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test and predict the stock price and prepare the dataset. Check your data by
    calling the `head()` function:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You should get the following output:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.26: First five rows of the DataFrame'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_26.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.26: First five rows of the DataFrame'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `tail(60)` method to look at the last 60 days of data. You will use
    this information in the next step:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You should get the following output:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.27: Last 10 rows of the DataFrame'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_27.jpg)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.27: Last 10 rows of the DataFrame'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `tail(60)` method to create the `past_60_days` variable:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Add the `past_60_days` variable to your test data with the `append()` function.
    Set `True` to `ignore_index`. Drop the `Date` and `Adj Close` columns as you will
    not need that information:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Check the DataFrame to make sure that you successfully dropped `Date` and `Adj
    Close` by using the `head()` function:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You should get the following output:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.28: Checking the first five rows of the DataFrame'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_28.jpg)'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.28: Checking the first five rows of the DataFrame'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use `scaler.transform` from `StandardScaler` to perform standardization on
    inputs:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You should get the following output:'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.29: DataFrame standardization'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_29.jpg)'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.29: DataFrame standardization'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the preceding results, you can see that after standardization, all values
    are close to `0` now.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split your data into `X_test` and `y_test` datasets. Create a test dataset
    that has the previous 60 days'' stock prices, so that you can test the closing
    stock price for the 61st day. Here, `X_test` will have two columns. The first
    column will store the values from 0 to 59\. The second column will store values
    from 1 to 60\. In the first column of `y_test`, store the 61st value at index
    60, and in the second column, store the 62nd value at index 61\. Use a `for` loop
    to create data in 60 time steps:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Convert `X_test` and `y_test` into NumPy arrays:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You should get the following output:'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding result shows that there are `391` observations and for each of
    them you have the last `60` days'' data for the following five features: `Open`,
    `High`, `Low`, `Close`, and `Volume`. The target variable, on the other hand,
    contains `391` values.'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test some predictions for stock prices by calling `regressor.predict(X_test)`:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Before looking at the results, reverse the scaling you did earlier so that
    the number you get as output will be at the correct scale using the `StandardScaler`
    utility class that you imported with `scaler.scale_`:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You should get the following output:'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.30: Using StandardScaler'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_30.jpg)'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.30: Using StandardScaler'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the first value in the preceding array to set your scale in preparation
    for the multiplication of `y_pred` and `y_test`. Recall that you are converting
    your data back from the scale you did earlier when converting all values to between
    zero and one:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'You should get the following output:'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Multiply `y_pred` and `y_test` by `scale` to convert your data back to the
    proper values:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Use `y_pred` to view predictions for NVIDIA stock:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'You should get the following output:'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.31: Checking prediction'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_31.jpg)'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.31: Checking prediction'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding results show the predicted Nvidia stock price for the future dates.
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the real Nvidia stock price and your predictions:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'You should get the following output:'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.32: NVIDIA stock price visualization'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_32.jpg)'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.32: NVIDIA stock price visualization'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the gray line in *Figure 9.32*, your prediction model is
    pretty accurate, when compared to the actual stock price, which is shown by the
    black line.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you built an RNN with an LSTM layer for Nvidia stock prediction
    and completed the training, testing, and prediction steps.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Now, test the knowledge you've gained so far in this chapter in the following
    activity.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9.01: Building an RNN with Multiple LSTM Layers to Predict Power Consumption'
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `household_power_consumption.csv` dataset contains information related to
    electric power consumption measurements for a household over 4 years with a 1-minute
    sampling rate. You are required to predict the power consumption of a given minute
    based on previous measurements.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: You are tasked with adapting an RNN model with additional LSTM layers to predict
    household power consumption at the minute level. You will be building an RNN model
    with three LSTM layers.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the dataset here: [https://packt.link/qrloK](https://packt.link/qrloK).'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this activity:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Load the data.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the data by combining the `Date` and `Time` columns to form one single
    `Datetime` column that can be used then to sort the data and fill in missing values.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Standardize the data and remove the `Date`, `Time`, `Global_reactive_power`,
    and `Datetime` columns as they won't be needed for the predictions.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the data for a given minute to include the previous 60 minutes' values.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training and testing sets with, respectively, data before
    and after the index `217440`, which corresponds to the last month of data.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define and train an RNN model composed of three different layers of LSTM with
    `20`, `40`, and `80` units, followed by `50%` dropout and ReLU as the activation function.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions on the testing set with the trained model.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the predictions against the actual values on the entire dataset.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.33: Expected output of Activity 9.01'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_33.jpg)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.33: Expected output of Activity 9.01'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor280).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to apply RNNs to text.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**) is a quickly growing field that is
    both challenging and rewarding. NLP takes valuable data that has traditionally
    been very difficult for machines to make sense of and turns it into information
    that can be used. This data can take the form of sentences, words, characters,
    text, and audio, to name a few. Why is this such a difficult task for machines?
    To answer that question, consider the following examples.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the two sentences: *it is what it is* and *is it what it is*. These
    two sentences, though they have completely opposite semantic meanings, would have
    the exact same representations in this bag of words format. This is because they
    have the exact same words, just in a different order. So, you know that you need
    to use a sequential model to process this, but what else? There are several tools
    and techniques that have been developed to solve these problems. But before you
    get to that, you need to learn how to preprocess sequential data.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Data Preprocessing
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a quick review, preprocessing generally entails all the steps needed to
    train your model. Some common steps include data cleaning, data transformation,
    and data reduction. For natural language processing, more specifically, the steps
    could be all, some, or none of the following:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padding
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowercase conversion
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing punctuation
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following sections provide a more in-depth description of the steps that
    you will be using. For now, here''s an overview of each step:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset cleaning** encompasses the conversion of case to lowercase, the removal
    of punctuation marks, and so on.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenization** is breaking up a character sequence into specified units called tokens.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Padding** is a way to make input sentences of different sizes the same by
    padding them. Padding the sequences means ensuring that the sequences have a uniform
    length.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming** is truncating words down to their stem. For example, the words
    "rainy" and "raining" both have the stem "rain".'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset Cleaning
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, you create the `clean_text` function, which returns a list containing
    words once it has been cleaned. You will save all text as lowercase with `lower()`
    and encode it with `utf8` for character standardization:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Generating a Sequence and Tokenization
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow provides a dedicated class for generating a sequence of N-gram tokens
    – `Tokenizer` from `keras.preprocessing.text`:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Once you have instantiated a `Tokenizer()`, you can use the `fit_on_texts()`
    method to extract tokens from a corpus. This step will attribute an integer index
    to each unique word from the corpus:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'After the tokenizer has been trained on a corpus, you can access the indexes
    allocated to each word from your corpus with the `word_index` attribute:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'You can convert a sentence into a tokenized version using the `texts_to_sequences()`
    method:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'You can create a function that will generate an N-gram sequence of tokenized
    sentences from an input corpus with the following snippet:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `get_seq_of_tokens()` function trains a `Tokenizer()` on the given corpus.
    Then you need to iterate through each line of the corpus and convert them into
    their tokenized equivalents. Finally, for each tokenized sentence, you create
    the different sequences of N-gram from it.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will see how you can deal with variable sentence length with padding.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Padding Sequences
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed previously, deep learning models expect fixed-length input. But
    with text, the length of a sentence can vary. One way to overcome this is to transform
    all sentences to have the same length. You will need to set the maximum length
    of sentences. Then, for sentences that are shorter than this threshold, you can
    add padding, which will add a specific token value to fill the gap. On the other
    hand, longer sentences will be truncated to fit this constraint. You can use `pad_sequences()`
    to achieve this:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'You can create the `generate_padded_sequences` function, which will take `input_sequences`
    and generate the padded version of it:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Now that you know how to process raw text, have a look at the modeling step
    in the next section.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Back Propagation Through Time (BPTT)
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many types of sequential models. You've already used simple RNNs,
    deep RNNs, and LSTMs. Let's take a look at a couple of additional models used
    for NLP.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Remember that you trained feed-forward models by first making a forward pass
    through the network that goes from input to output. This is the standard feed-forward
    model where the layers are densely connected. To train this kind of model, you
    can backpropagate the gradients through the network, taking the derivative of
    the loss of each weight parameter in the network. Then, you can adjust the parameters
    to minimize the loss.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: But in RNNs, as discussed earlier, your forward pass through the network also
    consists of going forward in time, updating the cell state based on the input
    and the previous state, and generating an output, `Y`. At that time step, computing
    a loss and then finally summing these losses from the individual time steps gets
    your total loss.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: This means that instead of backpropagating errors through a single feed-forward
    network at a single time step, errors are backpropagated at each individual time
    step, and then, finally, across all time steps—all the way from where you are
    currently, to the beginning of the sequence.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: This is why it's called backpropagation through time. As you can see, all errors
    are flowing back in time to the beginning of your data sequence.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: A great example of machine translation and one of the most powerful and widely
    used applications of RNNs in industry is Google Translate. In machine translation,
    you input a sequence in one language and the task is to train the RNN to output
    that sequence in a new language. This is done by employing a dual structure with
    an encoder that encodes the sentence in its original language into a state vector
    and a decoder. This then takes that encoded representation as input and decodes
    it into a new language.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s a key problem though in this approach: all content that is fed into
    the encoder structure must be encoded into a single vector. This can become a
    huge information bottleneck in practice because you may have a large body of text
    that you want to translate. To get around this problem the researchers at Google
    developed an extension of RNN called **attention**.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Now, instead of the decoder only having access to the final encoded state, it
    can access the states of all the time steps in the original sentence. The weights
    of these vectors that connect the encoder states to the decoder are learned by
    the network during training. This is called attention because when the network
    learns, it places its attention on different parts of the input sentence.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: In this way, it effectively captures a sort of memory access to the important
    information in that original sentence. So, with building blocks such as attention
    and gated cells, like LSTMs, RNNs have really taken off in recent years and are
    being used in the real world quite successfully.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: You should have by now gotten a sense of how RNNs work and why they are so powerful
    for processing sequential data. You've seen why and how you can use RNNs to perform
    sequence modeling tasks by defining this recurrence relation. You also learned
    how you can train RNNs and looked at how gated cells such as LSTMs can help us
    model long-term dependencies.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, you will see how to use an LSTM model for predicting
    the next word of a text.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9.03: Building an RNN with an LSTM Layer for Natural Language Processing'
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will use an RNN with an LSTM layer to predict the final
    word of a news headline.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: The `Articles.csv` dataset contains raw text that consists of news titles. You
    will be training an LTSM model that will predict the next word of a given sentence.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the dataset here: [https://packt.link/RQVoB](https://packt.link/RQVoB).'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries needed:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'You should get the following output:'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Load the dataset locally by setting `curr_dir` to `content`. Create the `all_headlines`
    variable. Use a `for` loop to iterate over the files contained in the folder,
    and extract the headlines. Remove all headlines with the `Unknown` value. Print
    the length of `all_headlines`:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output will be as follows:'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Create the `clean_text` method to return a list containing words once it has
    been cleaned. Save all text as lowercase with the `lower()` method and encode
    it with `utf8` for character standardization. Finally, output 10 headlines from
    your corpus:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'You should get the following output:'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.34: Corpus'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_34.jpg)'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.34: Corpus'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use `tokenizer.fit` to extract tokens from the corpus. Each integer output corresponds
    with a specific word. With `input_sequences`, train features that will be a `list
    []`. With `token_list = tokenizer.texts_to_sequences`, convert each sentence into
    its tokenized equivalent. With `n_gram_sequence = token_list`, generate the N-gram
    sequences. Using `input_sequences.append(n_gram_sequence)`, append each N-gram
    sequence to the list of your features:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'You should get the following output:'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.35: N-gram tokens'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_35.jpg)'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.35: N-gram tokens'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Pad the sequences and obtain the `predictors` and `target` variables. Use `pad_sequence`
    to pad the sequences and make their lengths equal:'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Prepare your model for training. Add an input embedding layer with `model.add(Embedding)`.
    Add a hidden LSTM layer with `100` units and add a dropout of 10%. Then, add a
    dense layer with a softmax activation function. With the `compile` method, configure
    your model for training, setting your loss function to `categorical_crossentropy`,
    and use the Adam optimizer:'
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'You should get the following output:'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.36: Model summary'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_36.jpg)'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.36: Model summary'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit your model with `model.fit` and set it to run on `100` epochs. Set `verbose`
    equal to `5`:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'You should get the following output:'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.37: Training the model'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_37.jpg)'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.37: Training the model'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Write a function that will receive an input text, a model, and the number of
    next words to be predicted. This function will prepare the input text to be fed
    into the model that will predict the next word:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Output some of your generated text with the `print` function. Add your own words
    for the model to use and generate from. For example, in `the hottest new`, the
    integer `5` is the number of words output by the model:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'You should get the following output:'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.38: Generated text'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_38.jpg)'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.38: Generated text'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: In this result, you can see the text generated by your model for each sentence.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you have successfully predicted some news headlines. Not surprisingly,
    some of them may not be very impressive, but some are not too bad.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have all the essential knowledge about RNNs, try to test yourself
    by performing the next activity.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9.02: Building an RNN for Predicting Tweets'' Sentiment'
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `tweets.csv` dataset contains a list of tweets related to an airline company.
    Each of the tweets has been classified as having positive, negative, or neutral sentiment.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 'You have been tasked to analyze a sample of tweets for the company. Your goal
    is to build an RNN model that will be able to predict the sentiment of each tweet:
    either positive or negative.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find `tweets.csv` here: [https://packt.link/dVUd2](https://packt.link/dVUd2).'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Perform the following steps to complete this activity.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary packages.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the data (combine the `Date` and `Time` columns, name it `datetime`,
    sort the data, and fill in missing values).
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the text data (tokenize words and add padding).
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into training and testing sets with, respectively, the first
    10,000 tweets and the remaining tweets.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define and train an RNN model composed of two different layers of LSTM with,
    respectively, `50` and `100` units followed by 20% dropout and ReLU as the activation
    function.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions on the testing set with the trained model.
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.39: Expected output of Activity 9.02'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_39.jpg)'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.39: Expected output of Activity 9.02'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor281).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-504
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you explored different recurrent models for sequential data.
    You learned that each sequential data point is dependent on the prior sequence
    of data points, such as natural language text. You also learned why you must use
    models that allow for the sequence of data to be used by the model, and sequentially
    generate the next output.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduced RNN models that can make predictions for sequential
    data. You observed the way RNNs can loop back on themselves, which allows the
    output of the model to feed back into the input. You reviewed the types of challenges
    that you face with these models, such as vanishing and exploding gradients, and
    how to address them.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to utilize custom TensorFlow components
    to use within your models, including loss functions and layers.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
