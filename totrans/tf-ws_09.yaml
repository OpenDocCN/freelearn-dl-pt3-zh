- en: 9\. Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to handle real sequential data. You will
    extend your knowledge of **artificial neural network** (**ANN**) models and **recurrent
    neural network** (**RNN**) architecture for training sequential data. You will
    also learn how to build an RNN model with an LSTM layer for natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained hands-on experience of applying
    multiple LSTM layers to build RNNs for stock price predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequential data refers to datasets in which each data point is dependent on
    the previous ones. Think of it like a sentence, which is composed of a sequence
    of words that are related to each other. A verb will be linked to a subject and
    an adverb will be related to a verb. Another example is a stock price, where the
    price on a particular day is related to the price of the previous days. Traditional
    neural networks are not fit for processing this kind of data. There is a specific
    type of architecture that can ingest sequences of data. This chapter will introduce
    you to such models—known as **recurrent neural networks** (**RNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: An RNN model is a specific type of deep learning architecture in which the output
    of the model feeds back into the input. Models of this kind have their own challenges
    (known as vanishing and exploding gradients) that will be addressed later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In many ways, an RNN is a representation of how a brain might work. RNNs use
    memory to help them learn. But how can they do this if information only flows
    in one direction? To understand this, you'll need to first review sequential data.
    This is a type of data that requires a working memory to process data effectively.
    Until now, you have only explored non-sequential models, such as a perceptron
    or CNN. In this chapter, you will look at sequential models such as RNN, LSTM,
    or GRU.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: Sequential versus non-sequential models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.1: Sequential versus non-sequential models'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequential data is information that happens in a sequence and is related to
    past and future data. An example of sequential data is time series data; as you
    perceive it, time only travels in one direction.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a ball (as in *Figure 9.2*), and you want to predict where
    this ball will travel next. If you have no prior information about the direction
    from which the ball was thrown, you will simply have to guess. However, if in
    addition to the ball's current location, you also had information about its previous
    location, the problem would be much simpler. To be able to predict the ball's
    next location, you need the previous location information in a sequential (or
    ordered) form to make a prediction about future events.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2: Direction of the ball'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.2: Direction of the ball'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs function in a way that allows the sequence of the information to retain
    value with the help of internal memory.
  prefs: []
  type: TYPE_NORMAL
- en: You'll take a look at some examples of sequential data in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Sequential Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequential data is a specific type of data where the order of each piece of
    information is important, and they all depend on each other.
  prefs: []
  type: TYPE_NORMAL
- en: One example of sequential data is financial data, such as stock prices. If you
    want to predict future data values for a given stock, you need to use previous
    values in time. In fact, you will work on stock prediction in *Exercise 9.01*,
    *Training an ANN for Sequential Data – Nvidia Stock Prediction*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Audio and text can also be considered sequential data. Audio can be split up
    into a sequence of sound waves, and text can be split up into sequences of either
    characters or words. The sound waves or sequences of characters or words should
    be processed in order to convey the desired result. Beyond these two examples
    that you encounter every day, there are many more examples in which sequential
    processing may be useful, from analyzing medical signals such as EEGs, projecting
    stock prices, and inferring and understanding genomic sequences. There are three
    categories of sequential data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Many-to-One** produces one output from many inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-to-Many** produces many outputs from one input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to-Many** produces many outputs from many inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.3: Categories of sequential data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.3: Categories of sequential data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider another example. Suppose you have a language model with a sentence
    or a phrase and you are trying to predict the word that comes next, as in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4: Sentence example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.4: Sentence example'
  prefs: []
  type: TYPE_NORMAL
- en: Say you're given the words `yesterday I took my car out for a…`, and you want
    to try to predict the next word, `drive`. One way you could do this is by building
    a deep neural network such as a feed-forward neural network. However, you would
    immediately run into a problem. A feed-forward network can only take a fixed-length
    input vector as its input; you have to specify the size of that input right from
    the start.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, your model needs a way to be able to handle variable-length
    inputs. One way you can do this is by using a fixed window. That means that you
    force your input vector to be just a certain length. For example, you can split
    the sentence into groups of two consecutive words (also called a **bi-gram**)
    and predict the next one. This means that no matter where you're trying to make
    that next prediction, your model will only be taking in the previous two words
    as its input. You need to consider how you can numerically represent this data.
    One way you can do this is by taking a fixed-length vector and allocating some
    space in that vector for the first word and some space in that vector for the
    second word. In those spaces, encode the identity of each word. However, this
    is problematic.
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because you're using only a portion of the information available (that
    is, two consecutive words only). You have access to a limited window of data that
    doesn't give enough context to accurately predict what will be the next word.
    That means you cannot effectively model long-term dependencies. This is important
    in sentences like the one in *Figure 9.5* where you clearly need information from
    much earlier in the sentence to be able to accurately predict the next word.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5: Sentence example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.5: Sentence example'
  prefs: []
  type: TYPE_NORMAL
- en: If you were only looking at the past two or three words, you wouldn't be able
    to make this next prediction, which you know is `Italian`. So, this means that
    you really need a way to integrate the information in the sentence from start
    to finish.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, you could use a set of counts as a fixed-length vector and use the
    entire sentence. This method is known as **bag of words**.
  prefs: []
  type: TYPE_NORMAL
- en: You have a fixed-length vector regardless of the identity of the sentence, but
    what differs is adding the counts over this vocabulary. You can feed this into
    your model as an input to generate a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: However, there's another big problem with this. Using just the counts means
    that you lose all sequential information and all information about the prior history.
  prefs: []
  type: TYPE_NORMAL
- en: Consider *Figure 9.6*. So, these two sentences, which have completely opposite
    semantic meanings would have the exact same representations in this bag of words
    format. This is because they have the exact same list of words, just in a different
    order. So, obviously, this isn't going to work. Another idea could be simply to
    extend the fixed window.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6: Bag of words example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.6: Bag of words example'
  prefs: []
  type: TYPE_NORMAL
- en: Now, consider *Figure 9.7*. You can represent your sentence in this way, feed
    the sentence into your model, and generate your prediction. The problem is that
    if you were to feed this vector into a feed-forward neural network, each of these
    inputs, `yesterday I took my car`, would have a separate weight connecting it
    to the network. So, if you were to repeatedly see the word `yesterday` at the
    beginning of the sentence, the network may be able to learn that `yesterday` represents
    a time or a setting. However, if `yesterday` were to suddenly appear later in
    that fixed-length vector, at the end of a sentence, the network may have difficulty
    understanding the meaning of `yesterday`. This is because the parameters that
    are at the end of a vector may never have seen the term `yesterday` before, and
    the parameters from the beginning of the sentence weren't shared across the entire
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7: Sentence example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.7: Sentence example'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, you need to be able to handle variable-length input and long-term dependencies,
    track sequential order, and have parameters that can be shared across the entirety
    of your sequence. Specifically, you need to develop models that can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Handle variable-length input sequences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track long-term dependencies in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain information about the sequence's order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share parameters across the entirety of the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you do this with a model where information only flows in one direction?
    You need a different kind of neural network. You need a recursive model. You will
    practice processing sequential data in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9.01: Training an ANN for Sequential Data – Nvidia Stock Prediction'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will build a simple ANN model to predict the Nvidia stock
    price. But unlike examples from previous chapters, this time the input data is
    sequential. So, you need to manually do some processing to create a dataset that
    will contain the price of the stock for a given day as the target variable and
    the price for the previous 60 days as features. You are required to split the
    data into training and testing sets before and after the date `2019-01-01`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the `NVDA.csv` dataset here: [https://packt.link/Mxi80](https://packt.link/Mxi80).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter or Colab notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the libraries needed. Use `numpy` for computation, `matplotlib` for
    plotting visualization, `pandas` to help work with your dataset, and `MinMaxScaler`
    to scale the dataset between zero and one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `read_csv()` function to read in the CSV file and store your dataset
    in a pandas DataFrame, `data`, for manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `head()` function on your data to take a look at the first five rows
    of your DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.8: First five rows of output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.8: First five rows of output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding table shows the raw data. You can see that each row represents
    a day where you have information about the stock price when the market opened
    and closed, the highest price, the lowest price, and the adjusted close price
    of the stock (taking into account dividend or stock split, for instance).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, split the training data. Use all data that is older than `2019-01-01`
    using the `Date` column for your training data. Save it as `data_training`. Save
    this in a separate file by using the `copy()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, split the test data. Use all data that is more recent than or equal to
    `2019-01-01` using the `Date` column. Save it as `data_test`. Save this in a separate
    file by using the `copy()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `drop()` to remove your `Date` and `Adj Close` columns in your DataFrame.
    Remember that you used the `Date` column to split your training and test sets,
    so the date information is not needed. Use `axis = 1` to specify that you also
    want to drop labels from your columns. To make sure it worked, call the `head()`
    function to take a look at the first five rows of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.9: New training data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.9: New training data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the output you should get after removing those two columns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a scaler from `MinMaxScaler` to scale `training_data` to numbers between
    zero and one. Use the `fit_transform` function to fit the model to the data and
    then transform the data according to the fitted model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.10: Scaled training data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.10: Scaled training data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split your data into `X_train` and `y_train` datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the shape of `training_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see there are 868 observations in the training set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a training dataset that has the previous 60 days'' stock prices so that
    you can predict the closing stock price for day 61\. Here, `X_train` will have
    two columns. The first column will store the values from 0 to 59, and the second
    will store values from 1 to 60\. In the first column of `y_train`, store the 61st
    value at index 60, and in the second column, store the 62nd value at index 61\.
    Use a `for` loop to create data in 60 time steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `X_train` and `y_train` into NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `shape()` function on `X_train` and `y_train`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding snippet shows that the prepared training set contains `808` observations
    with `60` days of data for the five features you kept (`Open`, `Low`, `High`,
    `Close`, and `Volume`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Transform the data into a 2D matrix with the shape of the sample (the number
    of samples and the number of features in each sample). Stack the features for
    all 60 days on top of each other to get an output size of `(808, 300)`. Use the
    following code for this purpose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, build an ANN. You will need some additional libraries for this. Use `Sequential`
    to initialize the neural net, `Input` to add an input layer, `Dense` to add a
    dense layer, and `Dropout` to help prevent overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Initialize the neural network by calling `regressor_ann = Sequential()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add an input layer with `shape` as `300`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, add the first dense layer. Set it to `512` units, which will be your
    dimensionality for the output space. Use a ReLU activation function. Finally,
    add a dropout layer that will remove 20% of the units during training to prevent overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add another dense layer with `128` units, ReLU as the activation function,
    and a dropout of `0.3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add another dense layer with `64` units, ReLU as the activation function, and
    a dropout of `0.4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Again, add another dense layer with `128` units, ReLU as the activation function,
    and a dropout of `0.3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a final dense layer with one unit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will get valuable information about your model layers and parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.11: Model summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.11: Model summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `compile()` method to configure your model for training. Choose Adam
    as your optimizer and mean squared error to measure your loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, fit your model and set it to run on `10` epochs. Set your batch size
    to `32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.12: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.12: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test and predict the stock price and prepare the dataset. Check your data by
    calling the `head()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.13: First five rows of a DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.13: First five rows of a DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `tail(60)` method to create a `past_60_days` variable, which consists
    of the last 60 days of data in the training set. Add the `past_60_days` variable
    to the test data with the `append()` function. Assign `True` to `ignore_index`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, prepare your test data for predictions by repeating what you did for the
    training data in *steps 8* to *15*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test some predictions for your stock prices by calling the `predict()` method
    on `X_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before looking at the results, reverse the scaling you did earlier so that
    the number you get as output will be at the correct scale using the `StandardScaler`
    utility class that you imported with `scaler.scale_`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.14: Using StandardScaler'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.14: Using StandardScaler'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the first value in the preceding array to set your scale in preparation
    for the multiplication of `y_pred` and `y_test`. Recall that you are converting
    your data back from your earlier scale, in which you converted all values to between
    zero and one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Multiply `y_pred` and `y_test` by `scale` to convert your data back to the
    proper values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Review the real Nvidia stock price and your predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.15: Real Nvidia stock price versus your predictions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.15: Real Nvidia stock price versus your predictions'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding graph, you can see that your trained model is able to capture
    some of the trends of the Nvidia stock price. Observe that the predictions are
    quite different from the real values. It is evident from this result that ANNs
    are not suited for sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you saw the inability of simple ANNs to deal with sequential
    data. In the next section, you will learn about recurrent neural networks, which
    are designed to learn from the temporal dimensionality of sequential data. Then,
    in *Exercise 9.02*, *Building an RNN with LSTM Layer Nvidia Stock Prediction*,
    you will perform predictions on the same Nvidia stock price dataset using RNNs
    and compare your results.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first formulation of a recurrent-like neural network was created by John
    Hopfield in 1982\. He had two motivations for doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential processing of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling of neuronal connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, an RNN processes input data at each time step and stores information
    in its memory that will be used for the next step. Information is first transformed
    into vectors that can be processed by machines. The RNN then processes the vector
    sequence one at a time. As it processes each vector, it passes the previous hidden
    state. The hidden state retains information from the previous step, acting as
    a type of memory. It does this by combining the input and the previous hidden
    state with a tanh function that compresses the values between `-1` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this is how the RNN functions. RNNs don't need a lot of computation
    and work well with short sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16: RNN data flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.16: RNN data flow'
  prefs: []
  type: TYPE_NORMAL
- en: Now turn your attention to applying neural networks to problems that involve
    sequential processing of data. You've already learned a bit about why these sorts
    of tasks require a fundamentally different type of network architecture from what
    you've seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: RNN Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will go through the key principles behind RNNs, how they are fundamentally
    different from what you've learned so far, and how RNN computation actually works.
  prefs: []
  type: TYPE_NORMAL
- en: But before you do that, take one step back and consider the standard feed-forward
    neural network that was discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: In feed-forward neural networks, data propagates in one direction only, that
    is, from input to output.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, you need a different kind of network architecture to handle sequential
    data. RNNs are particularly well-suited to handling cases in which you have a
    sequence of inputs rather than a single input. These are great for problems in
    which a sequence of data is being propagated to give a single output.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine that you are training a model that takes a sequence of
    words as input and outputs an emotion associated with that sequence. Similarly,
    consider cases in which, instead of returning a single output, you could have
    a sequence of inputs and propagate them through your network, where each time
    step in the sequence generates an output.
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, RNNs are networks that offer a mechanism to persist previously processed
    data over time and use it to make future predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17: RNN computation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.17: RNN computation'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, at some time step denoted by t, the RNN takes in `X`t
    as the input, and at that time step, it computes a prediction value, `Y`t, which
    is the output of the network.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to that output, it saved an internal state, called update, `H`t.
    This internal state from time step `t` can then be used to complement the input
    of the next time step `t+1`. So, basically, it provides information about the
    previous step to the next one. This mechanism is called **recurrent** because
    information is being passed from one time step to the next within the network.
  prefs: []
  type: TYPE_NORMAL
- en: What's really happening here? This is done by using a simple recurrence relation
    to process the sequential data. RNNs maintain internal state, `H`t, and combine
    it with the next input data, `X`t+1, to make a prediction, `Y`t+1, and store the
    new internal state, `H`t+1\. The key idea is that the state update is a combination
    of the previous state time step as well as the current input that the network
    is receiving.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s important to note that, in this computation, it''s the same function
    `f` of `W` and the same set of parameters that are used at every time step, and
    it''s those sets of parameters that you learn during the course of training. To
    get a better sense of how these networks work, step through the RNN algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: You begin by initializing your RNN and the hidden state of that network. You
    can denote a sentence for which you are interested in predicting the next word.
    The RNN computation simply consists of them looping through the words in this sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each time step, you feed both the current word that you're considering, as
    well as the previous hidden state of your RNN into the network. This can then
    generate a prediction for the next word in the sequence and use this information
    to update its hidden state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, after you've looped through all the words in the sentence, your prediction
    for that missing word is simply the RNN's output at that final time step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see in the following diagram, this RNN computation includes both
    the internal state update and the formal output vector.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18: RNN data flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.18: RNN data flow'
  prefs: []
  type: TYPE_NORMAL
- en: Given the input vector, `X`t, the RNN applies a function to update its hidden
    state. This function is simply a standard neural net operation. It consists of
    multiplication by a weight matrix and the application of a non-linearity activation
    function. The key difference is that, in this case, you're feeding in both the
    input vector, `X`t, and the previous state as inputs to this function, `H`t-1.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you apply a non-linearity activation function such as tanh to the previous
    step. You have these two weight matrices, and finally, your output, `y`t, at a
    given time step is then a modified, transformed version of this internal state.
  prefs: []
  type: TYPE_NORMAL
- en: After you've looped through all the words in the sentence, your prediction for
    that missing word is simply the RNN's output at that final time step, after all
    the words have been fed through the model. So, as mentioned, RNN computation includes
    both internal state updates and formal output vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Another way you can represent RNNs is by unrolling their modules over time.
    You can think of RNNs as having multiple copies of the same network, where each
    passes a message on to its descendant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19: Computational graph with time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.19: Computational graph with time'
  prefs: []
  type: TYPE_NORMAL
- en: In this representation, you can make your weight matrices explicit, beginning
    with the weights that transform the input to the `H` weights that are used to
    transform the previous hidden state to the current hidden state, and finally the
    hidden state to the output.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that you use the same weight matrices at every time step.
    From these outputs, you can compute a loss at each time step. The computation
    of the loss will then complete your forward propagation through the network. Finally,
    to define the total loss, you simply sum the losses from all of the individual
    time steps. Since your loss is dependent on each time step, this means that, in
    training the network, you will have to also involve time as a component.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've got a bit of a sense of how these RNNs are constructed and how
    they function, you can walk through a simple example of how to implement an RNN
    from scratch in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet uses a simple RNN from `keras.models.Sequential`. You
    specify the number of units as `1` and set the first input dimension to `None`
    as an RNN can process any number of time steps. A simple RNN uses tanh activation
    by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates a single layer with a single neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'That was easy enough. Now you need to stack some additional recurrent layers.
    The code is similar, but there is a key difference here. You will notice `return_sequences=True`
    on all but the last layer. This is to ensure that the output is a 3D array. As
    you can see, the first two layers each have `20` units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The RNN is defined as a layer, and you can build it by inheriting it from the
    layer class. You can also initialize your weight matrices and the hidden state
    of your RNN cell to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The key step here is defining the call function, which describes how you make
    a forward pass through the network given an input `X`. And, to break down this
    call function, you would first update the hidden state according to the equation
    discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Take the previous hidden state and the input `X`, multiply them by the relevant
    weight matrices, add them together, and then pass them through a non-linearity,
    like a hyperbolic tangent (tanh).
  prefs: []
  type: TYPE_NORMAL
- en: Then, the output is simply a transformed version of the hidden state, and at
    each time step, you return both the current output and the updated hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow has made it easy by having a built-in dense layer. The same applies
    to RNNs. TensorFlow has implemented these types of RNN cells with the simple RNN
    layer. But this type of layer has some limitations, such as vanishing gradients.
    You will look at this problem in the next section before exploring different types
    of recurrent layers.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing Gradient Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you take a closer look at how gradients flow in this chain of repeating modules,
    you can see that between each time step you need to perform matrix multiplication.
    That means that the computation of the gradient—that is, the derivative of the
    loss with respect to the parameters, tracing all the way back to your initial
    state—requires many repeated multiplications of this weight matrix, as well as
    repeated use of the derivative of your activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can have one of two scenarios that could be particularly problematic: the
    exploding gradient problem or the vanishing gradient problem.'
  prefs: []
  type: TYPE_NORMAL
- en: The exploding gradients problem is when gradients become continuously larger
    and larger due to the matrix multiplication operation, and you can't optimize
    them anymore. One way you may be able to mitigate this is by performing what's
    called gradient clipping. This amounts to scaling back large gradients so that
    their values are smaller and closer to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: You can also have the opposite problem where your gradients are too small. This
    is what is known as the vanishing gradient problem. This is when gradients become
    increasingly smaller (close to `0`) as you make these repeated multiplications,
    and you can no longer train the network. This is a very real problem when it comes
    to training RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a scenario in which you keep multiplying a number by some
    number that's in between zero and one. As you keep doing this repeatedly, that
    number is constantly shrinking until, eventually, it vanishes and becomes 0\.
    When this happens to gradients, it's hard to propagate errors further back into
    the past because the gradients are becoming smaller and smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the earlier example from the language model where you were trying to
    predict the next word. If you're trying to predict the last word in the following
    phrase, it's relatively clear what the next word is going to be. There's not that
    much of a gap between the key relevant information, such as the word "fish," and
    the place where the prediction is needed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20: Word prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.20: Word prediction'
  prefs: []
  type: TYPE_NORMAL
- en: However, there are other cases where more context is necessary, like in the
    following example. Information from early in the sentence, `She lived in Spain`,
    suggests that the next word of the sentence after `she speaks fluent` is most
    likely the name of a language, `Spanish`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21: Sentence example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.21: Sentence example'
  prefs: []
  type: TYPE_NORMAL
- en: But you need the context of `Spain`, which is located at a much earlier position
    in this sentence, to be able to fill in the relevant gaps and identify which language
    is correct. As this gap between words that are semantically important grows, RNNs
    become increasingly unable to connect the dots and link these relevant pieces
    of information together. That is due to the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: How can you alleviate this? The first trick is simple. You can choose either
    tanh or sigmoid as your activation function. Both of these functions have derivatives
    that are less than `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Another simple trick you can use is to initialize the weights for the parameters
    of your network. It turns out that initializing the weights to the identity matrix
    helps prevent them shrinking to zero too rapidly during back-propagation.
  prefs: []
  type: TYPE_NORMAL
- en: But the final and most robust solution is to use a slightly more complex recurrent
    unit that can track long-term dependencies in the data more effectively. It can
    do this by controlling what information is passed through and what information
    is used to update its internal state. Specifically, this is the concept of a gated
    cell, like in the LSTM layer, which is the focus of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LSTMs are well-suited to learning long-term dependencies and overcoming the
    vanishing gradient problem. They are very performant models for sequential data,
    and they're widely used by the deep learning community.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs have a chain-like structure. In an LSTM, the repeating unit contains different
    interacting layers. The key point is that these layers interact to selectively
    control the flow of information within the cell.
  prefs: []
  type: TYPE_NORMAL
- en: The key building block of the LSTM is a structure called a gate, which functions
    to enable the LSTM to selectively add or remove information from its cell state.
    Gates consist of a neural net layer like a sigmoid.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22: LSTM architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_09_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.22: LSTM architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Take a moment to think about what a gate like this would do in an LSTM. In this
    case, the sigmoid function would force its input to be between `0` and `1`. You
    can think of this mechanism as capturing how much of the information that's passed
    through the gate should be retained. It's between zero and one. This effectively
    gates the flow of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs process information through four simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the LSTM is to decide what information is going to be thrown
    away from the cell state, to forget irrelevant history. This is a function of
    both the prior internal state, `H`t-1, and the input, `X`t, because some of that
    information may not be important.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the LSTM decides what part of the new information is relevant and uses
    this to store this information in its cell state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it takes both the relevant parts of the prior information, as well as
    the current input, and uses this to selectively update its cell state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, it returns an output, and this is known as the output gate, which
    controls what information encoded in the cell state is sent to the network.![Figure
    9.23: LSTM processing steps'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16341_09_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.23: LSTM processing steps'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key takeaway here for LSTMs is the sequence of how they regulate information
    flow and storage. Once again, LSTMs operate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Forgetting irrelevant history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing what's new and what's important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using its internal memory to update the internal state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating an output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important property of LSTMs is that all these different gating and update
    mechanisms work to create an internal cell state, `C`, which allows the uninterrupted
    flow of gradients through time. You can think of it as sort of a highway of cell
    states where gradients can flow uninterrupted. This enables you to alleviate and
    mitigate the vanishing gradient problem that's seen with standard RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are able to maintain this separate cell state independently of what is
    output, and they use gates to control the flow of information by forgetting irrelevant
    history, storing relevant new information, selectively updating their cell state,
    and then returning a filtered version as the output.
  prefs: []
  type: TYPE_NORMAL
- en: The key point in terms of training and LSTMs is that maintaining the separate
    independent cell state allows the efficient training of an LSTM to backpropagate
    through time, which is discussed later.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've gone through the fundamental workings of RNNs, the backpropagation
    through time algorithm, and a bit about the LSTM architecture, you can put some
    of these concepts to work in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following LSTM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'First, you have initialized a neural network by calling `regressor = Sequential()`.
    Again, it''s important to note that in the last line you omit `return_sequences
    = True` because it is the final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Then, the LSTM layer is added. In the first instance, set the LSTM layer to
    `50` units. Use a relu activation function and specify the shape of the training
    set. Finally, the dropout layer is added with `regressor.add(Dropout(0.2)`. The
    `0.2` means that 20% of the layers will be removed. Set `return_sequences = True`,
    which allows the return of the last output.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, add three more LSTM layers and one dense layer to the LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are familiar with the basic concepts surrounding working with sequential
    data, it's time to complete the following exercise using some real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9.02: Building an RNN with an LSTM Layer – Nvidia Stock Prediction'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will be working on the same dataset as for *Exercise 9.01*,
    *Training an ANN for Sequential Data – Nvidia Stock Prediction*. You will still
    try to predict the Nvidia stock price based on the data of the previous 60 days.
    But this time, you will be training an LSTM model. You will need to split the
    data into training and testing sets before and after the date `2019-01-01`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the `NVDA.csv` dataset here: [https://packt.link/Mxi80](https://packt.link/Mxi80).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to prepare the dataset like in *Exercise 9.01*, *Training an
    ANN for Sequential Data – Nvidia Stock Prediction* (*steps 1* to *15*) before
    applying the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start building the LSTM. You will need some additional libraries for this.
    Use `Sequential` to initialize the neural net, `Dense` to add a dense layer, `LSTM`
    to add an LSTM layer, and `Dropout` to help prevent overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the neural network by calling `regressor = Sequential()`. Add four
    LSTM layers with `50`, `60`, `80`, and `120` units each. Use a ReLU activation
    function and assign `True` to `return_sequences` for all but the last LSTM layer.
    Provide the shape of your training set to the first LSTM layer. Finally, add dropout
    layers with 20%, 30%, 40%, and 50% dropouts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the summary of the model using the `summary()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.24: Model summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.24: Model summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see from the preceding figure, the summary provides valuable information
    about all model layers and parameters. This is a good way to make sure that your
    layers are in the order you wish and that they have the proper output shapes and
    parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `compile()` method to configure your model for training. Choose Adam
    as your optimizer and mean squared error to measure your loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit your model and set it to run on `10` epochs. Set your batch size equal
    to `32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.25: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.25: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test and predict the stock price and prepare the dataset. Check your data by
    calling the `head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.26: First five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.26: First five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `tail(60)` method to look at the last 60 days of data. You will use
    this information in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.27: Last 10 rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.27: Last 10 rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `tail(60)` method to create the `past_60_days` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `past_60_days` variable to your test data with the `append()` function.
    Set `True` to `ignore_index`. Drop the `Date` and `Adj Close` columns as you will
    not need that information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the DataFrame to make sure that you successfully dropped `Date` and `Adj
    Close` by using the `head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.28: Checking the first five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.28: Checking the first five rows of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use `scaler.transform` from `StandardScaler` to perform standardization on
    inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.29: DataFrame standardization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.29: DataFrame standardization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the preceding results, you can see that after standardization, all values
    are close to `0` now.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split your data into `X_test` and `y_test` datasets. Create a test dataset
    that has the previous 60 days'' stock prices, so that you can test the closing
    stock price for the 61st day. Here, `X_test` will have two columns. The first
    column will store the values from 0 to 59\. The second column will store values
    from 1 to 60\. In the first column of `y_test`, store the 61st value at index
    60, and in the second column, store the 62nd value at index 61\. Use a `for` loop
    to create data in 60 time steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `X_test` and `y_test` into NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding result shows that there are `391` observations and for each of
    them you have the last `60` days'' data for the following five features: `Open`,
    `High`, `Low`, `Close`, and `Volume`. The target variable, on the other hand,
    contains `391` values.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test some predictions for stock prices by calling `regressor.predict(X_test)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before looking at the results, reverse the scaling you did earlier so that
    the number you get as output will be at the correct scale using the `StandardScaler`
    utility class that you imported with `scaler.scale_`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.30: Using StandardScaler'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.30: Using StandardScaler'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the first value in the preceding array to set your scale in preparation
    for the multiplication of `y_pred` and `y_test`. Recall that you are converting
    your data back from the scale you did earlier when converting all values to between
    zero and one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Multiply `y_pred` and `y_test` by `scale` to convert your data back to the
    proper values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `y_pred` to view predictions for NVIDIA stock:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.31: Checking prediction'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.31: Checking prediction'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding results show the predicted Nvidia stock price for the future dates.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the real Nvidia stock price and your predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.32: NVIDIA stock price visualization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.32: NVIDIA stock price visualization'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the gray line in *Figure 9.32*, your prediction model is
    pretty accurate, when compared to the actual stock price, which is shown by the
    black line.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you built an RNN with an LSTM layer for Nvidia stock prediction
    and completed the training, testing, and prediction steps.
  prefs: []
  type: TYPE_NORMAL
- en: Now, test the knowledge you've gained so far in this chapter in the following
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9.01: Building an RNN with Multiple LSTM Layers to Predict Power Consumption'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `household_power_consumption.csv` dataset contains information related to
    electric power consumption measurements for a household over 4 years with a 1-minute
    sampling rate. You are required to predict the power consumption of a given minute
    based on previous measurements.
  prefs: []
  type: TYPE_NORMAL
- en: You are tasked with adapting an RNN model with additional LSTM layers to predict
    household power consumption at the minute level. You will be building an RNN model
    with three LSTM layers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the dataset here: [https://packt.link/qrloK](https://packt.link/qrloK).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the data by combining the `Date` and `Time` columns to form one single
    `Datetime` column that can be used then to sort the data and fill in missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Standardize the data and remove the `Date`, `Time`, `Global_reactive_power`,
    and `Datetime` columns as they won't be needed for the predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the data for a given minute to include the previous 60 minutes' values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training and testing sets with, respectively, data before
    and after the index `217440`, which corresponds to the last month of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define and train an RNN model composed of three different layers of LSTM with
    `20`, `40`, and `80` units, followed by `50%` dropout and ReLU as the activation function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions on the testing set with the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the predictions against the actual values on the entire dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.33: Expected output of Activity 9.01'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.33: Expected output of Activity 9.01'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor280).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to apply RNNs to text.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**) is a quickly growing field that is
    both challenging and rewarding. NLP takes valuable data that has traditionally
    been very difficult for machines to make sense of and turns it into information
    that can be used. This data can take the form of sentences, words, characters,
    text, and audio, to name a few. Why is this such a difficult task for machines?
    To answer that question, consider the following examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the two sentences: *it is what it is* and *is it what it is*. These
    two sentences, though they have completely opposite semantic meanings, would have
    the exact same representations in this bag of words format. This is because they
    have the exact same words, just in a different order. So, you know that you need
    to use a sequential model to process this, but what else? There are several tools
    and techniques that have been developed to solve these problems. But before you
    get to that, you need to learn how to preprocess sequential data.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a quick review, preprocessing generally entails all the steps needed to
    train your model. Some common steps include data cleaning, data transformation,
    and data reduction. For natural language processing, more specifically, the steps
    could be all, some, or none of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowercase conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following sections provide a more in-depth description of the steps that
    you will be using. For now, here''s an overview of each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset cleaning** encompasses the conversion of case to lowercase, the removal
    of punctuation marks, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenization** is breaking up a character sequence into specified units called tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Padding** is a way to make input sentences of different sizes the same by
    padding them. Padding the sequences means ensuring that the sequences have a uniform
    length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming** is truncating words down to their stem. For example, the words
    "rainy" and "raining" both have the stem "rain".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset Cleaning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, you create the `clean_text` function, which returns a list containing
    words once it has been cleaned. You will save all text as lowercase with `lower()`
    and encode it with `utf8` for character standardization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Generating a Sequence and Tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow provides a dedicated class for generating a sequence of N-gram tokens
    – `Tokenizer` from `keras.preprocessing.text`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have instantiated a `Tokenizer()`, you can use the `fit_on_texts()`
    method to extract tokens from a corpus. This step will attribute an integer index
    to each unique word from the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'After the tokenizer has been trained on a corpus, you can access the indexes
    allocated to each word from your corpus with the `word_index` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'You can convert a sentence into a tokenized version using the `texts_to_sequences()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'You can create a function that will generate an N-gram sequence of tokenized
    sentences from an input corpus with the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The `get_seq_of_tokens()` function trains a `Tokenizer()` on the given corpus.
    Then you need to iterate through each line of the corpus and convert them into
    their tokenized equivalents. Finally, for each tokenized sentence, you create
    the different sequences of N-gram from it.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will see how you can deal with variable sentence length with padding.
  prefs: []
  type: TYPE_NORMAL
- en: Padding Sequences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed previously, deep learning models expect fixed-length input. But
    with text, the length of a sentence can vary. One way to overcome this is to transform
    all sentences to have the same length. You will need to set the maximum length
    of sentences. Then, for sentences that are shorter than this threshold, you can
    add padding, which will add a specific token value to fill the gap. On the other
    hand, longer sentences will be truncated to fit this constraint. You can use `pad_sequences()`
    to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'You can create the `generate_padded_sequences` function, which will take `input_sequences`
    and generate the padded version of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Now that you know how to process raw text, have a look at the modeling step
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Back Propagation Through Time (BPTT)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many types of sequential models. You've already used simple RNNs,
    deep RNNs, and LSTMs. Let's take a look at a couple of additional models used
    for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that you trained feed-forward models by first making a forward pass
    through the network that goes from input to output. This is the standard feed-forward
    model where the layers are densely connected. To train this kind of model, you
    can backpropagate the gradients through the network, taking the derivative of
    the loss of each weight parameter in the network. Then, you can adjust the parameters
    to minimize the loss.
  prefs: []
  type: TYPE_NORMAL
- en: But in RNNs, as discussed earlier, your forward pass through the network also
    consists of going forward in time, updating the cell state based on the input
    and the previous state, and generating an output, `Y`. At that time step, computing
    a loss and then finally summing these losses from the individual time steps gets
    your total loss.
  prefs: []
  type: TYPE_NORMAL
- en: This means that instead of backpropagating errors through a single feed-forward
    network at a single time step, errors are backpropagated at each individual time
    step, and then, finally, across all time steps—all the way from where you are
    currently, to the beginning of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: This is why it's called backpropagation through time. As you can see, all errors
    are flowing back in time to the beginning of your data sequence.
  prefs: []
  type: TYPE_NORMAL
- en: A great example of machine translation and one of the most powerful and widely
    used applications of RNNs in industry is Google Translate. In machine translation,
    you input a sequence in one language and the task is to train the RNN to output
    that sequence in a new language. This is done by employing a dual structure with
    an encoder that encodes the sentence in its original language into a state vector
    and a decoder. This then takes that encoded representation as input and decodes
    it into a new language.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s a key problem though in this approach: all content that is fed into
    the encoder structure must be encoded into a single vector. This can become a
    huge information bottleneck in practice because you may have a large body of text
    that you want to translate. To get around this problem the researchers at Google
    developed an extension of RNN called **attention**.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, instead of the decoder only having access to the final encoded state, it
    can access the states of all the time steps in the original sentence. The weights
    of these vectors that connect the encoder states to the decoder are learned by
    the network during training. This is called attention because when the network
    learns, it places its attention on different parts of the input sentence.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, it effectively captures a sort of memory access to the important
    information in that original sentence. So, with building blocks such as attention
    and gated cells, like LSTMs, RNNs have really taken off in recent years and are
    being used in the real world quite successfully.
  prefs: []
  type: TYPE_NORMAL
- en: You should have by now gotten a sense of how RNNs work and why they are so powerful
    for processing sequential data. You've seen why and how you can use RNNs to perform
    sequence modeling tasks by defining this recurrence relation. You also learned
    how you can train RNNs and looked at how gated cells such as LSTMs can help us
    model long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, you will see how to use an LSTM model for predicting
    the next word of a text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9.03: Building an RNN with an LSTM Layer for Natural Language Processing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will use an RNN with an LSTM layer to predict the final
    word of a news headline.
  prefs: []
  type: TYPE_NORMAL
- en: The `Articles.csv` dataset contains raw text that consists of news titles. You
    will be training an LTSM model that will predict the next word of a given sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the dataset here: [https://packt.link/RQVoB](https://packt.link/RQVoB).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset locally by setting `curr_dir` to `content`. Create the `all_headlines`
    variable. Use a `for` loop to iterate over the files contained in the folder,
    and extract the headlines. Remove all headlines with the `Unknown` value. Print
    the length of `all_headlines`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `clean_text` method to return a list containing words once it has
    been cleaned. Save all text as lowercase with the `lower()` method and encode
    it with `utf8` for character standardization. Finally, output 10 headlines from
    your corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.34: Corpus'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.34: Corpus'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use `tokenizer.fit` to extract tokens from the corpus. Each integer output corresponds
    with a specific word. With `input_sequences`, train features that will be a `list
    []`. With `token_list = tokenizer.texts_to_sequences`, convert each sentence into
    its tokenized equivalent. With `n_gram_sequence = token_list`, generate the N-gram
    sequences. Using `input_sequences.append(n_gram_sequence)`, append each N-gram
    sequence to the list of your features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.35: N-gram tokens'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.35: N-gram tokens'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Pad the sequences and obtain the `predictors` and `target` variables. Use `pad_sequence`
    to pad the sequences and make their lengths equal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare your model for training. Add an input embedding layer with `model.add(Embedding)`.
    Add a hidden LSTM layer with `100` units and add a dropout of 10%. Then, add a
    dense layer with a softmax activation function. With the `compile` method, configure
    your model for training, setting your loss function to `categorical_crossentropy`,
    and use the Adam optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.36: Model summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.36: Model summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit your model with `model.fit` and set it to run on `100` epochs. Set `verbose`
    equal to `5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.37: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.37: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Write a function that will receive an input text, a model, and the number of
    next words to be predicted. This function will prepare the input text to be fed
    into the model that will predict the next word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output some of your generated text with the `print` function. Add your own words
    for the model to use and generate from. For example, in `the hottest new`, the
    integer `5` is the number of words output by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.38: Generated text'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.38: Generated text'
  prefs: []
  type: TYPE_NORMAL
- en: In this result, you can see the text generated by your model for each sentence.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you have successfully predicted some news headlines. Not surprisingly,
    some of them may not be very impressive, but some are not too bad.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have all the essential knowledge about RNNs, try to test yourself
    by performing the next activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9.02: Building an RNN for Predicting Tweets'' Sentiment'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `tweets.csv` dataset contains a list of tweets related to an airline company.
    Each of the tweets has been classified as having positive, negative, or neutral sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have been tasked to analyze a sample of tweets for the company. Your goal
    is to build an RNN model that will be able to predict the sentiment of each tweet:
    either positive or negative.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find `tweets.csv` here: [https://packt.link/dVUd2](https://packt.link/dVUd2).'
  prefs: []
  type: TYPE_NORMAL
- en: Perform the following steps to complete this activity.
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the data (combine the `Date` and `Time` columns, name it `datetime`,
    sort the data, and fill in missing values).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the text data (tokenize words and add padding).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into training and testing sets with, respectively, the first
    10,000 tweets and the remaining tweets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define and train an RNN model composed of two different layers of LSTM with,
    respectively, `50` and `100` units followed by 20% dropout and ReLU as the activation
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions on the testing set with the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.39: Expected output of Activity 9.02'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_09_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.39: Expected output of Activity 9.02'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor281).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you explored different recurrent models for sequential data.
    You learned that each sequential data point is dependent on the prior sequence
    of data points, such as natural language text. You also learned why you must use
    models that allow for the sequence of data to be used by the model, and sequentially
    generate the next output.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduced RNN models that can make predictions for sequential
    data. You observed the way RNNs can loop back on themselves, which allows the
    output of the model to feed back into the input. You reviewed the types of challenges
    that you face with these models, such as vanishing and exploding gradients, and
    how to address them.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to utilize custom TensorFlow components
    to use within your models, including loss functions and layers.
  prefs: []
  type: TYPE_NORMAL
