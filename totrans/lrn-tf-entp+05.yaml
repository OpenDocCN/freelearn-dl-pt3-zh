- en: 'Chapter 3:'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation and Manipulation Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to convert the two common data types into
    structures suitable for ingestion pipelines—structured CSVs or pandas DataFrames
    into a dataset, and unstructured data such as images into **TFRecords**.
  prefs: []
  type: TYPE_NORMAL
- en: Along the way, there will be some tips and utility functions that are reusable
    in many situations. You will also understand the rationale of the conversion process.
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated in the previous chapter, TensorFlow Enterprise takes advantage
    of the flexibility offered by the Google Cloud AI platform to access training
    data. Once access to the training data is resolved, our next task is to develop
    a workflow to let the model consume the data efficiently. In this chapter, we
    will learn how to examine and manipulate commonly used data structures.
  prefs: []
  type: TYPE_NORMAL
- en: While TensorFlow can consume Pythonic data structures such as pandas or numpy
    directly, for resource throughput and ingestion efficiency, TensorFlow built the
    dataset API to convert data from its native Pythonic structure into TensorFlow's
    specific structure. The dataset API can handle and parse many commonly used types
    of data. For instance, structured or tabular data with defined schemas are typically
    presented as a pandas DataFrame. The dataset API converts this data structure
    into a Tensorflow dataset. Image data is typically presented as a numpy array.
    In TensorFlow, it is preferred to convert it into `TFRecord`.
  prefs: []
  type: TYPE_NORMAL
- en: In working with these data structures, it is important to be certain that the
    conversion process is performed correctly and that the data can be verified. This
    chapter will demonstrate some techniques that help to ensure that data structure
    conversions are done correctly; for example, decoding a byte stream into an image.
    It is always helpful to decode these data structures into a readable format just
    for the purpose of a quick check of the data quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with the TensorFlow dataset as applied to structured data. In
    particular, we''ll cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting tabular data to a TensorFlow dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting distributed CSV files to a TensorFlow dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling image data for input pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoding `TFRecord` and reconstructing the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling image data at scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting tabular data to a TensorFlow dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Tabular** or **comma separated values (CSV)** data with fixed schemas and
    data types are commonly encountered. We typically work it into a pandas DataFrame.
    We have seen in the previous chapter how this can be easily done when the data
    is hosted in a **BigQuery table** (the BigQuery magic command that returns a query
    result to a pandas DataFrame by default).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at how to handle data that can fit into the memory. In this
    example, we are going to read a public dataset using the BigQuery magic command,
    so we can easily obtain the data in a pandas DataFrame. Then we are going to convert
    it to a TensorFlow dataset. A TensorFlow dataset is the data structure for streaming
    training data in batches without using up the compute node's runtime memory.
  prefs: []
  type: TYPE_NORMAL
- en: Converting a BigQuery table to a TensorFlow dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each of the following steps is executed in a cell. Again, use any of the AI
    platforms you prefer (AI Notebook, Deep Learning VM, Deep Learning Container).
    An AI notebook is the simplest and cheapest choice:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The table in this example is selected for demonstration purposes only. We are
    going to treat `daily_deaths` as if it is the target for machine learning model
    training. While we are going to treat it as if it is our training data (in other
    words, containing features and target columns), in actual training data engineering
    practices, there are other steps involved, such as feature engineering, aggregation,
    and normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the data from BigQuery, so we can be sure of its data structure
    and the data type of each column, and then take a preview of the table:![Figure
    3.1 – Using BigQuery to examine the data structure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image001.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.1 – Using BigQuery to examine the data structure
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the preceding query is run, you will see output as shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Table preview'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.2 – Table preview
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the library, define the variables, and define the project ID only if you
    are running in a different project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the BigQuery magic command to read a table into a pandas DataFrame (`train_raw_df`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take a look at a few sample rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Output of a few rows from the table'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.3 – Output of a few rows from the table
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Some columns are categorical. We need to encode them as integers. First, we
    designate the column as a pandas categorical feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we replace the column content with category code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we repeat this procedure for the other categorical columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make lists to hold column names according to data type. The reason is to ensure
    that the dataset can cast the columns in our DataFrame to the correct TensorFlow
    data type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Creating a dataset from a pandas DataFrame requires us to specify the correct
    column names and the data type. Column names are held in the respective list based
    on their data type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Look at the structure of the dataset to make sure its metadata is as specified
    during the creation process in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The tensor shapes and data types are in the exact order as indicated in the
    previous step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now you have created a dataset from a pandas DataFrame. The dataset is now a
    part of the input pipeline. If this dataset's features and targets are properly
    normalized and selected (for example, having performed a normalization operation
    such as min-max scaling, or a standardization operation such as Z-score conversion
    if the distribution of the column data can be assumed to be Gaussian), then it
    is ready to be fed into a model for training as-is.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, from this exercise, you''ve learned the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Use BigQuery as much as possible to examine data schemas and data types first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data that can fit into the memory, leverage the BigQuery magic command to
    output a pandas DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin the column names by their data types for clarity and organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encode categorical features to integers so they can be cast into a TensorFlow
    data type that is compatible with a TensorFlow dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting distributed CSV files to a TensorFlow dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are not sure about the data size, or are unsure as to whether it can
    all fit in the Python runtime's memory, then reading the data into a pandas DataFrame
    is not a viable option. In this case, we may use a **TF dataset** to directly
    access the data without opening it.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, when data is stored in a storage bucket as parts, the naming convention
    follows a general pattern. This pattern is similar to that of a `*`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When storing distributed files in a Google Cloud Storage bucket, a common pattern
    for filenames is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, there is the following pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There is always a pattern in the filenames. The TensorFlow module `tf.io.gfile.glob`
    is a convenient API that encodes such filename patterns in a distributed filesystem.
    This is critical for inferring distributed files that are stored in a storage
    bucket. In this section, we will use this API to infer our structured data (multiple
    CSV files), which is distributed in a storage bucket. Once inferred, we will then
    convert it to a dataset (using `tf.data.experimental.make_csv_dataset`).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing an example CSV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we need multiple CSV files of the same schema for this demonstration, we
    may use open source CSV data such as the **Pima Indians Diabetes** dataset (CSV)
    as our data source. This CSV is hosted in [https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv).
  prefs: []
  type: TYPE_NORMAL
- en: 'You may simply run the following command on your local system (where you downloaded
    the aforementioned file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Again, for demonstration purposes only, we need to split this data into multiple
    smaller CSVs, and then upload these CSVs to a Google Cloud Storage bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'The column names for this file are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Column names are not included in the CSV thus we may split the file into multiple
    parts without extracting the header row. Let’s start with steps below:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the file into multiple parts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you have downloaded the CSV, you may split it into multiple parts with
    the following `awk` command. This will split the file into multiple CSV parts
    at every 200 rows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following CSV files are generated:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload the files to storage. After you have created multiple CSV files from
    the downloaded file, you may upload these files to a Google Cloud Storage bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Uploading CSV files to a Cloud Storage bucket'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Uploading CSV files to a Cloud Storage bucket
  prefs: []
  type: TYPE_NORMAL
- en: 'All the files are here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Multi-part CSV file in the Cloud Storage bucket'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Multi-part CSV file in the Cloud Storage bucket
  prefs: []
  type: TYPE_NORMAL
- en: Building filename patterns with TensorFlow I/O
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the files are uploaded, let''s now go to our AI Platform notebook environment
    and execute the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`Tf.io.gfile.glob` takes a file pattern string as the input and creates a `filenames`
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a list of filenames that match the pattern, we are ready to
    convert these files to a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dataset from CSV files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, multiple CSV files are stored with either no header or all with
    a header. In this case, there is no header. We need to prepare the column names
    for the CSV before we convert it to dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the source for column names: [https://data.world/data-society/pima-indians-diabetes-database](https://data.world/data-society/pima-indians-diabetes-database).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we need to specify that the first lines in these files are not headers
    as we convert the CSVs to a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In `make_csv_dataset`, we use a list of filenames as the input and specify there
    is no header, and we then assign `COLUMN_NAMES`, make small batches for showing
    the result, select a column as the target column (`'Outcome'`), and set the number
    of epochs to `1` since we are not going to train a model with it at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we may verify the content of the dataset. Recall that since we specified
    a column as the label, that means the rest of the columns are features. The output
    will be a tuple that contains features and targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the first batch of the dataset, which contains five observations,
    and print the data in features and target columns. In a dataset, the data is stored
    as arrays, and each column is now a key-value pair. Within `features` is another
    level of key-value pairs for each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: During training, the data will be passed to the training process in batches,
    and not as a single file to be opened and possibly consume a large amount of runtime
    memory. In the preceding example, we see that as a good practice, distributed
    files stored in Cloud Storage follow a certain naming pattern. The `tf.io.gfile.glob`
    API can easily infer multiple files that are distributed in a Cloud Storage bucket.
    We may easily use `tf.data.experimental.make_csv_dataset` to create a dataset
    instance from the `gfile` instance. Overall, the `tf.io` and `tf.data` APIs together
    make it possible to build a data input pipeline without explicitly reading data
    into memory.
  prefs: []
  type: TYPE_NORMAL
- en: Handling image data for input pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there are many types of unstructured data, images are probably the most
    frequently encountered type. TensorFlow provided `TFRecord` as a type of dataset
    for image data. In this section, we are going to learn how to convert image data
    in Cloud Storage into a `TFRecord` object for input pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: When working with image data in a TensorFlow pipeline, the raw image is typically
    converted to a `TFRecord` object for the same reason as for CSV or DataFrames.
    Compared to a raw numpy array, a `TFRecord` object is a more efficient and scalable
    representation of the image collections. Converting raw images to a `TFRecord`
    object is not a straightforward process. In `TFRecord`, the data is stored as
    a binary string. In this section, we are going to show how to do this step by
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the conversion process of converting a raw image to a `TFRecord`
    object. Feel free to upload your own images to the JupyterLab instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Upload images of your choice to the JupyterLab runtime. Create a folder for
    your images that we are going to upload. Give the folder a name, and this is the
    folder where the images will be uploaded:![Figure 3.6 – Creating a folder in the
    notebook runtime
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.6 – Creating a folder in the notebook runtime
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that the folder has a name, you may proceed to the next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Double-click on the folder you just named. Now you are inside this folder. In
    this example, I named this folder `image-ai-platform-examle`. Then, within this
    folder, I created another folder named `maldives`. Once inside, you may click
    the upload button to upload a few of your own images to this folder:![Figure 3.7
    – Uploading an item to JupyterLab runtime
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.7 – Uploading an item to JupyterLab runtime
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, I uploaded an image named `maldives-1.jpg`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You may acquire the path to this image by right-clicking on the image file:![Figure
    3.8 – Finding the path to images uploaded to the notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.8 – Finding the path to images uploaded to the notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You may paste the file path to a notepad or editor for quick reference for the
    next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select `images-ai-platform-example/maldives/maldives-1.jpg`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Display the image for verification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Displaying the image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.9 – Displaying the image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a dictionary to map the filename with a label. We can use the `my_image`
    alias as the key and we may verify this dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Constructing a protobuf message
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we have `image_labels` that map image files to their labels. The next thing
    we need to do is to convert this image to a `tf.Example` `image_label`, the `tf.Example`
    message consists of key-value pairs. The key-value pairs are the metadata of the
    image, including the three dimensions and their respective values, the label and
    its value, and finally the image itself in byte array format. The values are represented
    as `tf.Tensor`. Let's now construct this protobuf message.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this time, the `tf.Example` protobuf message can only accept three types
    of `tf.Tensor`. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.train.ByteList` can handle `string` `and` `byte`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.train.FloatList` can handle `float (float32)` `and` `double (float64)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.train.Int64List` can handle `bool`, `enum`, `int32`, `uint32`, `int64`
    `and` `uint64`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most other generic data types can be coerced into one of these three types
    as per TensorFlow''s documentation, which is available at: [https://www.tensorflow.org/tutorials/load_data/tfrecord#tftrainexample](https://www.tensorflow.org/tutorials/load_data/tfrecord#tftrainexample):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we may use these functions that are provided as per TensorFlow''s documentation.
    These functions can convert values to types that are compatible with `tf.Example`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Generally speaking, from the pattern in the preceding function, we can see that
    the raw value from the data is first coerced into one of the three acceptable
    types and then it is converted to a `feature`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we can open the image as a byte string and extract its dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now construct a dictionary that puts these key-value pairs together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that the feature dictionary consists of key-value pairs of the metadata,
    where the values are one of the three coerced data types for `tf.Example`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will then convert this dictionary to `tf.Train.Features`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert a `tf.Features` protobuf message to a `tf.Example` protobuf message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, create a directory for storing `tfrecords`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify a target name, and then execute the write operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The image is now written into a protobuf message, which is a collection of key-value
    pairs that store its dimensions, labels, and raw images (the image value is stored
    as a byte string).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Decoding TFRecord and reconstructing the image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we learned how to write a `.jpg` image into a `TFRecord`
    dataset. Now we are going to see how to read it back and display it. An important
    requirement is that you must know the feature structure of the `TFRecord` protobuf
    as indicated by its keys. The feature structure is the same as the feature description
    used to build the `TFRecord` in the previous section. In other words, in the same
    way as a raw image was structured into a `tf.Example` protobuf with a defined
    feature description, we can use that feature description to parse or reconstruct
    the image using the same knowledge stored in the feature description:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read `TFRecord` back from the path where it is stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dictionary to specify the keys and values in `TFRecord`, and use it
    to parse all elements in the `TFRecord` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, `_parse_image_function` uses `image_feature_description`
    to parse the `tfrecord` protobuf. We use the `map` function to apply `_parse_iamge_function`
    to each image in `read_back_tfrecord`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will show the image using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Displaying a dataset as an image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – Displaying a dataset as an image
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to convert raw data (an image) to `TFRecord`
    format, and verify that the conversion was done correctly by reading the `TFRecord`
    back and displaying it as an image. From this example, we can also see that in
    order to decode and inspect `TFRecord` data, we need the feature dictionary as
    was used during the encoding process. It is important to bear this in mind when
    working with `TFRecord`.
  prefs: []
  type: TYPE_NORMAL
- en: Handling image data at scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Handling data and their respective labels is simple if the everything can be
    loaded into Python engine's runtime memory. However, in the case of constructing
    a data pipeline for ingestion into a model training workflow, we want to ingest
    or stream data in batches so that we don't rely on the runtime memory to hold
    all the training data. In this case, maintaining the one-to-one relationship between
    the data (image) and label has to be preserved. We are going to see how to do
    this with `TFRecord`. We have already seen how to convert one image to a `TFRecord`.
    With multiple images, the conversion process is exactly the same for each image.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at how we can reuse and refactor the code from the previous
    section to apply to a batch of images. Since you have seen how it was done for
    a single image, you will have little to no problem understanding the code and
    rationale here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, when working with images for classification, we would organize images
    in the following directory structure, starting with a base directory (in other
    words, project name). The next level of directories are `train`, `validation`,
    and `test`. Within each of the three directories, there are image class directories.
    In other words, labels are the lowest directory name. For example, the directories
    may be organized into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, below this level, we would have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way of demonstrating the organization of images by classes is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Images are placed in a directory based on their class. In this section, the
    example is simplified to the following structure in Cloud Storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'You may find example jpg images in: [https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_03/from_gs](https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_03/from_gs)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each folder name corresponds to the label of images. The general procedure
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Copy images stored in the Cloud Storage bucket to the JupyterLab runtime.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map image filenames to their respective labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write each image's dimension, label, and byte array to the `tf.Example` protobuf.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store multiple protobufs together in a single `TFRecord`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing the steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the detailed steps that need to be run in each cell for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy images from a storage bucket to the notebook runtime:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, a folder, `from_gs`, is created, and the `image-collection` bucket
    is copied into it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Consider the base directory to be `/from_gs/image-collection`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Base directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image016.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.11 – Base directory
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since this example is for demonstrating how to create `TFRecordDataset`, and
    not about partitioning data into training, validation, and testing, we can go
    right to the image class directory level, as shown in the following screenshot:![Figure
    3.12 – Image class directory level
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image017.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.12 – Image class directory level
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Upon inspecting one of the image class directories, we see the image files:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Image files'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image018.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.13 – Image files
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import libraries and designate the label names as `CLASS_NAMES`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And CLASS_NAMES is captured properly as shown:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to construct a dictionary that maps filenames to their respective
    label from `CLASS_NAMES`. We can use `glob` to encode directory and filename patterns.
    A couple of empty lists are created so that we may iterate recursively through
    directories, and append the path-to-filename into the filename list, and the label
    (denoted by the directory name) into the class list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once both lists are populated in exact order, we may zip these lists together
    and encode the result as key-value pairs (dictionary):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As indicated, this is a dictionary, with the keys being the file path, and the
    values encoding respective labels (image classes).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We want to convert our data into a `tf.Example` protobuf message, which is
    the predecessor format for `TFRecord`. `tf.Example` requires us to specify features
    in the image (metadata such as the image width pixel count, height pixel count,
    or data such as decimal values expressed as a `numpy` array). The three data types
    designated by `tf.Example` are `tf.train.BytesList`, `tf.train.FloatList`, and
    `tf.train.Int64List`. Therefore, commonly observed Pythonic data types need to
    be coerced into one of these three types. These are what each `tf.Example` data
    type can accept and coerce:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`tf.train.BytesList`: `string`, `byte`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.train.FloatList`: `float` (float32, float64)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.train.Int64List`: `bool`, `enum`, `int32`, `uint32`, `int64`, `uint64`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to coerce common data types into the respective compatible `tf.Example`
    data type, the TensorFlow team provides the following helper functions:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we want to convert a string of text (byte string) into a feature of the
    type `tf.train.ByteList`, the following function first converts the text (which
    is an eager tensor) into a `numpy` array, because `tf.train.BytesList` currently
    can only unpack `numpy` format into a byte list. After a protobuf message''s value
    is casted to the `ByteList` type, then it is converted into a `f`eature object
    with the `ByteList` data type:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'If we need to convert numbers with floating points into a feature of the `tf.train.FloatList`
    type, then the following function does the job:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'And finally, for generating a feature of the `tf.train.Int64List` type, this
    can be done accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A note of caution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`tf.train.Feature` accepts one feature at a time. Each of these functions deals
    with converting and coercing one data feature at a time. This function is different
    from `tf.train.Features`, which accepts a dictionary of multiple features. In
    the next step, we are going to use `tf.train.Features`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Consolidate the workflow of creating the `tf.Example` protobuf message into
    a wrapper function. This function takes two inputs: a byte string that represents
    the image, and the corresponding label of that image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside this function, first, the image shape is specified through the output
    of `decode_jpeg`, which converts a byte array into a `jpeg`. Dimension values
    are held in `image_shape` as a `numpy` array, and we may pass these values into
    the feature dictionary. Inside the `feature` dictionary, keys are specified, and
    corresponding values are derived and type casted from the helper functions in
    the previous step. The feature dictionary is then used to specify schemas of the
    feature into a `features` protobuf. The `feature` protobuf is then converted to
    an example protobuf, which is the final format to be serialized into a `TFRecord`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write multiple image files into `TFRecords` by looping through `image_label_dict`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding steps, we wrote all seven images in the three classes into
    a single `TFRecord`.
  prefs: []
  type: TYPE_NORMAL
- en: Reading TFRecord and displaying it as images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be assured about the image data as presented by the TFRecord format, it
    would be helpful if we can read it back and display it, just to be sure everything
    was formatted correctly. Now, let''s read `TFRecord` back and display it as images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the same API as in the previous section to read `tfrecords`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the specs for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Parse the protobuf. This is also exactly the same as shown in the previous
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the images with the help of the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And you should see all the images contained in this protobuf message. For brevity,
    we will show only two images, and notice that Figures 3.14 and 3.15 have different
    dimensions, which are preserved and retrieved correctly by the protobuf.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the first image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Image of Maldives class (1)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0191.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 – Image of Maldives class (1)
  prefs: []
  type: TYPE_NORMAL
- en: 'And here''s the second image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Image of Maldives class (2)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 – Image of Maldives class (2)
  prefs: []
  type: TYPE_NORMAL
- en: A few words about having multiple images in a single TFRecord
  prefs: []
  type: TYPE_NORMAL
- en: You have seen that whether it's one image or multiple images, everything can
    be written in a single `TFRecord`. There is no right or wrong way as to which
    one is preferred, as factors such as memory and I/O bandwidth all come into play.
    A rule of thumb is to distribute your training images to at least 32 - 128 shards
    (each shard is a `TFRecord`) to maintain a file-level parallelism in the I/O process
    whenever you have sufficient images to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided explanations and examples for dealing with commonly seen
    structured and unstructured data. We first looked at how to read and format a
    pandas DataFrame or CSV type of data structure and converted it to a dataset for
    efficient data ingestion pipelines. Then, as regards unstructured data, we used
    image files as examples. While dealing with image data, we have to organize these
    image files in a hierarchical pattern, such that labels can be easily mapped to
    each image file. `TFRecord` is the preferred format for handling image data, as
    it wraps the image dimension, label, and image raw bytes together in a format
    known as `tf.Example`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to take a look at reusable models and patterns
    that can consume these data structures we have learned here.
  prefs: []
  type: TYPE_NORMAL
