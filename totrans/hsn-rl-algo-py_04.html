<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Solving Problems with Dynamic Programming</h1>
                </header>
            
            <article>
                
<p><span>The purposes of this chapter are manifold. We will introduce many topics that are essential to the understanding of reinforcement problems and the first algorithms that are used to solve them. Whereas, in the previous chapters, we talked about <strong>reinforcement learning</strong> (<strong>RL</strong>) from a broad and non-technical point of view, here, we will formalize this understanding to develop the first algorithms to solve a simple game. </span></p>
<p>The RL problem can be formulated as a <strong>Markov decision process</strong> (<strong>MDP</strong>), a framework that provides a formalization of the key elements of RL, such as value functions and the expected reward. RL algorithms can then be created using these mathematical components. They differ from each other by how these components are combined and on the assumptions made while designing them.</p>
<p>For this reason, as we'll see in this chapter, RL algorithms can be categorized into three main categories that can overlap each other. This is because some algorithms can unify characteristics from more than one category. Once these pivotal concepts have been explained, we'll present the first type of algorithm, called dynamic programming, which can solve problems when given complete information about the environment. </p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>MDP</li>
<li>Categorizing RL algorithms</li>
<li>Dynamic programming</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MDP</h1>
                </header>
            
            <article>
                
<p>An MDP expresses the problem of sequential decision-making, where actions influence the next states and the results. MDPs are general and flexible enough to provide a formalization of the problem of learning a goal through interactions, the same problem that is addressed with RL. Thus we can express and reason with RL problems in terms of MDPs.</p>
<p>An MDP is four-tuple (S,A,P,R):</p>
<ul>
<li><em>S</em> is the state space, with a finite set of states.</li>
<li><em>A</em> is the action space, with a finite set of actions.</li>
<li><em>P</em> is a transition function, which defines the probability of reaching a state, <em>s′</em>, from <em>s</em> through an action, <em>a</em>. In <em>P(s′, s, a) = p(s′| s, a)</em>, the transition function is equal to the conditional probability of <em>s′</em> given <em>s</em> and <em>a</em>.</li>
<li><em>R</em> is the reward function, which determines the value received for transitioning to state <em>s′</em> after taking action <em>a</em> from state <em>s</em>.</li>
</ul>
<p>An illustration of an MDP is given in the following diagram. The arrows represent the transitions between two states, with the transition probabilities attached to the tail of the arrows and the rewards on the body of the arrows. For their properties, the transition probabilities of a state must add up to 1. In this example, the final state is represented with a square (state <em>S<sub>5</sub></em>) and for simplicity, we have represented an MDP with a single action:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1810 image-border" src="assets/aacec67b-f1bb-4c5b-b003-716c51e2b48e.png" style="width:31.08em;height:21.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.1 Example of an MDP with five states and one action</div>
<p class="mce-root">The MDP is controlled by a sequence of discrete time steps that create a trajectory of states and actions (<em>S<sub>0</sub>, A<sub>0</sub>, S<sub>1</sub>, A<sub>1</sub>, ...</em>), where the states follow the dynamics of the MDP, namely the state transition function, <em>p(s′|s, a)</em>. In this way, the transition function fully characterizes the environment's dynamics. </p>
<p>By definition, the transition function and the reward function are determined only by the current state, and not from the sequence of the previous states visited. This property is called the <strong>Markov property</strong>, which means that the process is memory-less and the future state depends only on the current one, and not on its history. Thus, a state holds all the information. A system with such a property is called <strong>fully observable</strong>.</p>
<p>In many practical RL cases, the Markov property does not hold up, and for practicality, we can get around the problem by assuming it is an MDP and using a finite number of previous states (a finite history): <em>S<sub>t</sub></em>, <em>S<sub>t-1</sub></em>, <em>S<sub>t-2</sub></em>, ..., <em>S</em><sub><em>t-k</em></sub>. In this case, the system is <strong>partially observable</strong> and the states are called <strong>observations</strong>. We'll use this strategy in the Atari games, where we'll use row pixels as the input of the agent. This is because the single frame is static and does not carry information about the speed or direction of the objects. Instead, these values can be retrieved using the previous three or four frames (it is still an approximation).</p>
<p>The final objective of an MDP is to find a policy, π, that maximizes the cumulative reward, <sub><img class="fm-editor-equation" src="assets/b27acbaf-de95-4dc7-b6e9-bd508392ce23.png" style="width:6.92em;height:3.00em;"/></sub>, where <em>R<sub>π</sub></em> is the reward obtained at each step by following the policy, <span>π</span>. A<span> solution of an MDP is found when a policy takes the best possible action in each state of the MDP. This policy is known as</span> the <strong>optimal</strong> <span><strong>policy</strong>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy</h1>
                </header>
            
            <article>
                
<p>The policy chooses the actions to be taken in a given situation and can be categorized as deterministic or stochastic.</p>
<p>A deterministic policy is denoted as <em>a<sub>t</sub> = µ(st)</em>, while a stochastic policy can be denoted as <em>a<sub>t</sub> ~ <span>π(.|s<sub>t</sub>)</span></em>, where the tilde symbol (~) means <strong>has distribution</strong>. Stochastic policies are used when it is better to consider an action distribution; for example, when it is preferable to inject a noisy action into the system.</p>
<p>Generally, stochastic policies can be categorical or Gaussian. The former case is similar to a classification problem and is computed as a softmax function across the categories. In the latter case, the actions are sampled from a Gaussian distribution, described by a mean and a standard deviation (or variance). These parameters can also be functions of states. </p>
<p class="mce-root"/>
<p>When using parameterized policies, we'll define them with the letter <em>θ</em>. For example, in the case of a deterministic policy, it would be written as <em>µ<sub>θ</sub> (s<sub>t</sub>)</em>.</p>
<div class="packt_infobox">Policy, decision-maker, and agent are three terms that express the same concept, so, in this book, we'll use these terms interchangeably. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Return</h1>
                </header>
            
            <article>
                
<p>When running a policy in an MDP, the sequence of state and action (<em>S<sub>0</sub></em><span>, </span><em>A<sub>0</sub></em><span>, </span><em>S<sub>1</sub></em><span>, </span><em>A<sub>1</sub></em><span>, ...</span>) is called <strong>trajectory</strong> or <strong>rollout</strong><em>,</em> and is denoted by <img class="fm-editor-equation" src="assets/7e13ad77-456c-493a-8343-9ffc8a8bf955.png" style="width:0.83em;height:0.92em;"/>. In each trajectory, a sequence of rewards will be collected as a result of the actions. A function of these rewards is called <strong>return</strong> and in its most simplified version, it is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/263dd699-3fe4-4a42-a6ec-3c5ee28e18e4.png" style="width:34.25em;height:4.58em;"/></p>
<p>At this point, the return can be analyzed <span>separately </span>for trajectories with infinite and finite horizons. This distinction is needed because in the case of interactions within an environment that do not terminate, the sum previously presented will always have an infinite value. This situation is dangerous because it doesn't provide any information. Such tasks are called continuing tasks and need another formulation of the reward. The best solution is to give more weight to the short-term rewards while giving less importance to those in the distant future. This is accomplished by using a value between 0 and 1 called the <strong>discount factor</strong> denoted with the symbol λ<em>. </em>Thus, the return <strong>G</strong> can be reformulated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e72838f1-d23d-4c2d-a455-08853a791dc4.png" style="width:38.17em;height:4.58em;"/></p>
<p>This formula can be viewed as a way to prefer actions that are closer in time with respect to those that will be encountered in the distant future. Take this example—you win the lottery and you can decide when you would like to collect the prize. You would probably prefer to collect it within a few days rather than in a few years. <img style="font-size: 1em;width:0.75em;height:1.17em;" class="fm-editor-equation" src="assets/f64405b5-36bf-47d8-8d63-38e0da4636d5.png"/><span> is the value that defines how long you are willing to wait to collect the prize. If </span><img style="font-size: 1em;width:3.08em;height:1.17em;" class="fm-editor-equation" src="assets/15638a94-11a5-4f72-8968-90fede169fa4.png"/>,<span> that means that you are not bothered about when you collect the prize. If </span><span><sub><img class="fm-editor-equation" src="assets/aa898182-0ce4-47e4-82fc-51be617d1fe8.png" style="width:2.83em;height:1.08em;"/></sub></span>, that<span> means that you want it immediately. </span></p>
<p class="mce-root"/>
<p>In cases of trajectories with a finite horizon, meaning trajectories with a natural ending, tasks are called <strong>episodic</strong> (it derives from the term episode, which is another word for trajectory). In episodic tasks, the original formula (1) works, but nevertheless, it is preferred to have a variation of it with the discount factor:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a2ced63d-7d0b-4d81-8688-db95ece7a973.png" style="width:38.17em;height:4.75em;"/></p>
<p>With a finite but long horizon, the use of a discount factor increases the stability of algorithms, considering that long future rewards are only partially considered. <span>In practice, discount factor values between 0.9 and 0.999 are used.</span></p>
<p>A trivial but very useful decomposition of formula (3) is the definition of return in terms of the return at timestep <em>t + 1</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/32c5321a-1dee-4421-ad5c-b9c7c7efc629.png" style="width:21.58em;height:1.42em;"/></p>
<p>When simplifying the notation, it becomes the following: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c1de6332-c6af-457a-b6cd-f721ea802575.png" style="width:20.75em;height:1.58em;"/></p>
<p>Then, using the return notation, we can define the goal of RL to find an optimal policy, <img class="fm-editor-equation" src="assets/14497f73-ee08-44a6-b1e7-74e633c68357.png" style="width:0.83em;height:0.83em;"/>, that maximizes the expected return as <sub><img class="fm-editor-equation" src="assets/36d54163-a38b-424c-84d1-5bb6dccabbec.png" style="width:9.92em;height:1.50em;"/></sub>, where <sub><img class="fm-editor-equation" src="assets/abb94ffb-0ccf-4754-b31e-eed001662724.png" style="width:2.67em;height:1.50em;"/></sub> is the expected value of a random variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Value functions</h1>
                </header>
            
            <article>
                
<p>The return <sub><img class="fm-editor-equation" src="assets/e9046fe7-5a5d-46e6-90d1-c519d768a125.png" style="width:2.17em;height:1.17em;"/></sub> provides a good insight into the trajectory's value, but still, it doesn't give any indication of the quality of the single states visited. This quality indicator is important because it can be used by the policy to choose the next best action. The policy has to just choose the action that will result in the next state with the highest quality. The <strong>value function</strong> does exactly this: it estimates the <strong>quality</strong> in terms of the expected return from a state following a policy. Formally, the value function is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/803e2f8b-f844-4f77-976a-56019065ffbf.png" style="width:27.58em;height:4.58em;"/></p>
<p class="mce-root"/>
<p>The <strong>action-value function</strong>, similar to the value function, is the expected return from a state but is also conditioned on the first action. It is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6a4ec12a-7d62-4c8c-b3b5-8c19daafaddc.png" style="width:39.00em;height:4.58em;"/></p>
<p>The value function and action-value function are also called the <strong>V-function</strong> and <strong>Q-function </strong><span>respectively</span><span>, and are strictly correlated with each other since the value function can also be defined in terms of the action-value function:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/488cf9ca-f963-4c79-a1e5-5523c6c50d75.png" style="width:12.00em;height:1.58em;"/></p>
<p>Knowing the optimal <sub><img class="fm-editor-equation" src="assets/8a9c0fe1-da0e-43e5-9e66-e7b81af625a9.png" style="width:1.25em;height:1.17em;"/></sub>, the optimal value function is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/538c8450-86a3-4138-a125-112d7dd8b047.png" style="width:13.08em;height:1.58em;"/></p>
<p>That's because the optimal action is <sub><img class="fm-editor-equation" src="assets/9e78caff-8401-4f9a-a736-3076e1ec787a.png" style="width:13.08em;height:1.42em;"/></sub>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bellman equation</h1>
                </header>
            
            <article>
                
<p><span><strong>V</strong> </span>and<span> <strong>Q</strong></span><span> </span>can<span> be estimated by running trajectories that follow the </span>policy, <img class="fm-editor-equation" src="assets/28b05043-d213-4654-8fb0-f170fdcdb514.png" style="width:0.92em;height:0.92em;"/>, and<span> then averaging the values obtained. This technique is effective and is used in many contexts, but is very expensive considering that the return requires the rewards from the full trajectory.</span></p>
<p><span>Luckily, the Bellman equation defines the action-value function and the value function recursively, enabling their estimations from subsequent states. The Bellman equation does that by using the reward obtained in the present state and the value of its successor state. We already saw the recursive formulation of the return (in formula (5)) and we can apply it to the state value:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/11c116e3-4fc4-42f3-830a-e709ef88eb8e.png" style="width:34.33em;height:3.33em;"/></p>
<p>Similarly, we can adapt the Bellman equation for the action-value function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fd00dc43-1da2-4a23-9d9d-224e4e9d5294.png" style="width:27.50em;height:4.42em;"/></p>
<p>Now, with (6) and (7), <sub><img class="fm-editor-equation" src="assets/14df8976-b491-4874-bf26-1879dfdc30fb.png" style="width:1.17em;height:1.00em;"/></sub> and <sub><img class="fm-editor-equation" src="assets/825e2f3a-c490-4ee0-9c32-aaed59534c36.png" style="width:1.25em;height:1.00em;"/></sub> are updated only with the values of the successive states, without the need to unroll the trajectory to the end, as required in the old definition.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categorizing RL algorithms</h1>
                </header>
            
            <article>
                
<p>Before deep diving into the first RL algorithm that solves the optimal Bellman equation, we want to give a broad but detailed overview of RL algorithms. We need to do this because their distinctions can be quite confusing. There are many parts involved in the design of algorithms, and many characteristics have to be considered before deciding which algorithm best fits the actual needs of the user. The scope of this overview presents the big picture of RL so that in the next chapters, where we'll give a comprehensive theoretical and practical view of these algorithms, you will already see the general objective and have a clear idea of their location in the map of RL algorithms.</p>
<p>The first distinction is between <span>model-based and </span>model-free algorithms. As the name suggests, the first requires a model of the environment, while the second is free from this condition. The model of the environment is highly valuable because it carries precious information that can be used to find the desired policies; however, in most cases, the model is almost impossible to obtain. For example, it can be quite easy to model the game tic-tac-toe, while it can be difficult to model the waves of the sea. To this end, model-free algorithms can learn information without any assumptions about the environment. A representation of the categories of RL algorithms is visible in figure 3.2.</p>
<p>Here<span> the distinction</span> is shown between model-based and model-free, and two widely known <span>model-free </span>approaches, namely policy gradient and value-based. Also, as we'll see in later chapters, a combination of those is possible:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1812 image-border" src="assets/a612c49f-f471-4f07-9d64-dc5dbe0377ca.png" style="width:32.08em;height:21.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.2. Categorization of RL algorithms</div>
<p class="mce-root">The first distinction is between model-free and model-based. Model-free RL algorithms can be further decomposed in policy gradient and value-based algorithms. Hybrids are methods that combine important characteristics of both methods. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-free algorithms</h1>
                </header>
            
            <article>
                
<p>In the absence of a model, <strong>model-free</strong> (<strong>MF</strong>) algorithms run trajectories within a given policy to gain experience and to improve the agent. MF algorithms are made up of three main steps that are repeated until a good policy is created:</p>
<ol>
<li>The generation of new samples by running the policy in the environment. The trajectories are run until a final state is reached or for a fixed number of steps. </li>
<li>The estimation of the return function.</li>
<li>The improvement of the policy using the samples collected, and the estimation done in step 2. </li>
</ol>
<p class="mce-root"/>
<p>These three components are at the heart of this type of algorithm, but based on how each step is performed, they generate different algorithms. Value-based algorithms and policy gradient algorithms are two such examples. They seem to be very different, but they are based on similar principles and both use the three-step approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Value-based algorithms</h1>
                </header>
            
            <article>
                
<p><span>Value-based algorithms</span>, also known as <strong>v<span>alue function algorithms</span></strong>, use a paradigm that's very similar to the one we saw in the previous section. That is, they use the Bellman equation to learn the Q-function, which in turn is used to learn a policy. In the most common setting, they use deep neural networks as a function approximator and other tricks to deal with high variance and general instabilities. To a certain degree, value-based algorithms are closer to supervised regression algorithms.</p>
<p>Typically, these algorithms are off-policy, meaning they are not required to optimize the same policy that was used to generate the data. This means that these methods can learn from previous experience, as they can store the sampled data in a replay buffer. The ability to use previous samples makes the value function more sample-efficient than other model-free algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy gradient algorithms</h1>
                </header>
            
            <article>
                
<p>The other family of MF algorithms is that of the <span>policy gradient </span>methods (or <span>policy optimization methods</span>). They have a more direct and obvious interpretation of the RL problem, as they learn directly from a parametric policy by updating the parameters in the direction of the improvements. It's based on the RL principle that good actions should be encouraged (by boosting the gradient of the policy upward) while discouraging bad actions.</p>
<p>Contrary to value function algorithms, policy optimization mainly requires on-policy data, making these algorithms more sample inefficient. <span>Policy </span>optimization<span> methods can be quite </span>unstable<span> due to the fact that taking the steepest ascent in the presence of surfaces with high curvature can easily result in moving too far in any given direction, falling down into a bad region. To address this problem, many algorithms have been proposed, such as optimizing the policy only within a trust region, or optimizing a surrogate clipped objective function to limit changes to the policy.</span></p>
<p>A major advantage of policy gradient methods is that they <span>easily </span>handle environments with continuous action spaces. This is a very difficult thing to approach with value function algorithms as they learn Q-values for discrete pairs of states and actions.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Actor-Critic algorithms</h1>
                </header>
            
            <article>
                
<p><strong>Actor-Critic</strong> (<strong>AC</strong>) algorithms are on-policy policy gradient algorithms that also <span>learn </span>a value function (generally a Q-function) called a critic to provide feedback to the policy, the actor. Imagine that you, the actor, want to go to the supermarket via a new route. Unfortunately, before arriving at the destination, your boss calls you requiring you to go back to work. Because you didn't reach the supermarket, you don't know if the new road is actually faster than the old one. But if you reached a familiar location, you can estimate the time you'll need to go from there to the supermarket and calculate whether the new path is preferable. This estimate is what the critic does. In this way, you can improve the actor even though you didn't reach the final goal.</p>
<p>Combining a critic with an actor has been shown to be very effective and is commonly used in policy gradient algorithms. This technique can also be combined with other ideas used in policy optimization, such as trust-region algorithms. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hybrid algorithms</h1>
                </header>
            
            <article>
                
<p>Advantages of both value functions and policy gradient algorithms can be merged, creating hybrid algorithms that can be more sample efficient and robust.</p>
<p>Hybrid approaches combine Q-functions and policy gradients to symbiotically and mutually improve each other. These methods estimate the expected Q-function of deterministic actions to directly improve the policy. </p>
<div class="packt_infobox"><span>Be aware that because AC algorithms learn and use a value function, they are categorized as policy gradients and not as hybrid algorithms. This is because the main underlying objective is that of policy gradient methods. The value function is only an upgrade to provide additional information.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-based RL</h1>
                </header>
            
            <article>
                
<p>Having a model of the environment means that the state transitions and the rewards can be predicted for each state-action tuple (without any interaction with the real environment). As we already mentioned, the model is known only in limited cases, but when it is known, it can be used in many different ways. The most obvious application of the model is to use it to plan future actions. Planning <span>is a concept used to express the organization of future moves when the consequences of the next actions are already known. </span>For example, if you know exactly what moves your enemy will make, you can think ahead and plan all your actions before executing the first one. As a downside, planning can be very expensive and isn't a trivial process.</p>
<p>A model can also be learned through interactions with the environment, assimilating the consequences (both in terms of the states and rewards) of an action. This solution is not always the best one because teaching a model could be terribly expensive in the real world. Moreover, if only a rough approximation of the environment is understood by the model, it could lead to disastrous results. </p>
<p>A model, whether known or learned, can be used both to plan and to improve the policy, and can be integrated into different phases of an RL algorithm. Well-known cases of model-based RL involve pure planning, embedded planning to improve the policy, and generated samples from an approximate model.</p>
<p>A set of algorithms that use a model to estimate a value function is called <strong>dynamic programming</strong> (<strong>DP</strong>) and will be studied later in this chapter. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Algorithm diversity</h1>
                </header>
            
            <article>
                
<p>Why are there so many types of RL algorithms? This is because there isn't one that is better than all the others in every context. Each one is designed for different needs and to take care of different aspects. The most notable differences are stability, sample efficiency, and wall clock time (training time). These will be more clear as we progress through the book but as a rule of thumb, policy gradient algorithms are more stable and reliable than value function algorithms. On the other hand, value function methods are more sample efficient as they are off-policy and can use prior experience. In turn, model-based algorithms are more sample efficient than Q-learning algorithms but their computational cost is much higher and they are slower.</p>
<p>Besides the ones just presented, there are other trade-offs that have to be taken into consideration while designing and deploying an algorithm (such as ease of use and robustness), which is not a trivial process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic programming</h1>
                </header>
            
            <article>
                
<p>DP is a general algorithmic paradigm that breaks up a problem into smaller chunks of overlapping subproblems, and then finds the solution to the original problem by combining the solutions of the subproblems.</p>
<p>DP can be used in reinforcement learning and is among one of the simplest approaches. It is suited to computing optimal policies by being provided with a perfect model of the environment.</p>
<p class="mce-root"/>
<p>DP is an important stepping stone in the history of RL algorithms and provides the foundation for the next generation of algorithms, but it is computationally very expensive. DP works with MDPs with a limited number of states and actions as it has to update the value of each state (or action-value), taking into consideration all the other possible states. Moreover, DP algorithms store value functions in an array or in a table. This way of storing information is effective and fast as there isn't any loss of information, but it does require the storage of large tables. Since DP algorithms use tables to store value functions, it is called tabular learning. This is opposed to approximated learning, which uses approximated value functions to store the values in a fixed size function, such as an artificial neural network. </p>
<p>DP uses <strong>bootstrapping</strong>, meaning that it improves the estimation value of a state by using the expected value of the following states. A<span>s we have already seen, b</span>ootstrapping is used in the Bellman equation. Indeed, DP applies the Bellman equations, (6) and (7), to estimate <img class="fm-editor-equation" src="assets/d1e5a85d-b3db-40e3-8673-6e550b0970cc.png" style="width:1.25em;height:1.00em;"/> and/or <img class="fm-editor-equation" src="assets/c3f1b6e5-7143-4c81-a8e4-e18a3b34132f.png" style="width:1.17em;height:1.00em;"/>. This can be done using the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b748b0d7-08fe-47f8-8b6f-3f2d8d554826.png" style="width:26.83em;height:1.58em;"/></p>
<p>Or by using the Q-function: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ae17b27a-87c2-4b74-9d79-82d604815cae.png" style="width:34.08em;height:1.75em;"/></p>
<p><span>Then, once the optimal value and action-value function are found, the optimal policy can be found by just taking the actions that maximize the expectation. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy evaluation and policy improvement</h1>
                </header>
            
            <article>
                
<p>To find the optimal policy, you first need to find the optimal value function. An iterative procedure that does this is called <strong>policy evaluation</strong>—it creates a <img class="fm-editor-equation" src="assets/e066f1bb-6f23-4534-a641-d2c5b5744803.png" style="width:5.08em;height:1.50em;"/> <span>seq</span>uence that iteratively improves the value function for a policy, <img class="fm-editor-equation" src="assets/efe619a7-adea-4fca-8f5c-0526379caee6.png" style="width:0.92em;height:0.92em;"/>, using the state value transition of the model, the expectation of the next state, and the immediate reward. Therefore, it creates a sequence of improving value functions using the Bellman equation:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/35ce6c80-4531-418a-89ee-d95018a5eb01.png" style="width:38.75em;height:8.08em;"/></p>
<p>This sequence will converge to the optimal value as <img class="fm-editor-equation" src="assets/2de8443b-3c7c-4562-81a4-8c8909dc7721.png" style="width:3.42em;height:1.00em;"/>. Figure 3.3 shows the update of <img class="fm-editor-equation" src="assets/fcc876d2-ff8c-43bd-b4ab-94ca788e93c7.png" style="width:3.75em;height:1.25em;"/> using the successive state values:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1813 image-border" src="assets/750b984e-76e9-4fb4-8565-bbf6e0dc1e0f.png" style="width:37.08em;height:19.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.3. The update of <img class="fm-editor-equation" src="assets/772727b6-3e7b-41bf-a9c3-2b329fc37397.png" style="width:4.75em;height:1.58em;"/> using formula (8)</div>
<p>The value function (8) can be updated only if the state transition function, <kbd>p</kbd>, and the reward function, <kbd>r</kbd>, for every state and action are known, so only if the model of the environment is completely known. </p>
<p>Note that the first summation of the actions in (8) is needed for stochastic policies because the policy outputs a probability for each action. For simplicity from now on, we'll consider only deterministic policies. </p>
<p>Once the value functions are improved, it can be used to find a better policy. This procedure is called <em>policy improvement</em> and is about finding a policy, <img class="fm-editor-equation" src="assets/9c7a1dd0-2b3e-4a41-98fd-25aafdd44903.png" style="width:1.08em;height:1.25em;"/>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/76d1e369-d645-4f9f-86df-577aa539974b.png" style="width:43.42em;height:3.33em;"/></p>
<p>It creates a policy, <img class="fm-editor-equation" src="assets/bc4324c4-27a9-4444-9262-f5739c2b6eae.png" style="width:1.33em;height:1.50em;"/>, from the value function, <img class="fm-editor-equation" src="assets/54caf5aa-e5a1-4022-a5ed-3e801909c297.png" style="width:1.33em;height:1.17em;"/>, of the original policy, <img class="fm-editor-equation" src="assets/b07eeebf-18a3-489d-9769-5c9736bbb58c.png" style="width:0.92em;height:0.92em;"/>. As can be formally demonstrated, the new policy, <img class="fm-editor-equation" src="assets/e5fc8c16-f6c2-4c49-a973-0fbc3d372902.png" style="width:1.33em;height:1.50em;"/>, is always better than <img class="fm-editor-equation" src="assets/a999e245-6d4c-4c0f-8ee9-5b1020f8eb35.png" style="width:0.92em;height:0.92em;"/>, and the policy is optimal if and only if <img class="fm-editor-equation" src="assets/46dec1a9-9029-46fb-87ff-c75694ccf873.png" style="width:0.92em;height:1.08em;"/> is optimal. The combination of policy evaluation and policy improvement gives rise to two algorithms to compute the optimal policy. One is called <strong>policy iteration</strong> and the other is called <strong>value iteration</strong>. Both use policy evaluation to <span>monotonically</span><span> </span><span>improve the value function and policy improvement to estimate the new policy. The only difference is that policy iteration executes the two phases cyclically, while value iteration combines them in a single update.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy iteration</h1>
                </header>
            
            <article>
                
<p>Policy iteration cycles between policy evaluation, which updates <img class="fm-editor-equation" src="assets/385c7c7e-e70d-4c66-b996-ce801f533505.png" style="width:1.25em;height:1.08em;"/> under the current policy, <img class="fm-editor-equation" src="assets/147da112-7f02-4562-8164-d6d642b64073.png" style="width:0.92em;height:0.92em;"/>, using formula (8), and policy improvement (9), which computes <img class="fm-editor-equation" src="assets/1fefbb96-584a-47d3-8820-6b078fc78d85.png" style="width:1.33em;height:1.50em;"/> using the improved value function, <img class="fm-editor-equation" src="assets/2c92dcf9-5506-484f-8ee1-6e799304cca7.png" style="width:1.17em;height:1.08em;"/>. Eventually, after <img class="fm-editor-equation" src="assets/61d2c68a-4610-4ead-923f-9ea65c7f0c41.png" style="width:0.92em;height:1.00em;"/> cycles, the algorithm will result in an optimal policy, <img class="fm-editor-equation" src="assets/a65f1839-d684-4a3e-85bb-741fb82da1cc.png" style="width:1.58em;height:1.42em;"/>.</p>
<p>The pseudocode is as follows:</p>
<pre>Initialize <sub><img class="fm-editor-equation" src="assets/296b08e4-1bbe-4a7d-b82f-348344424f0b.png" style="width:2.58em;height:1.33em;"/></sub> and <sub><img class="fm-editor-equation" src="assets/569f8ffb-0948-4e8c-93a3-7fe12df6abc8.png" style="width:2.33em;height:1.50em;"/></sub> for every state <img class="fm-editor-equation" src="assets/79cad62d-2871-4fd8-a56c-358ea3f6cb8d.png" style="width:0.75em;height:1.00em;"/><br/><br/><strong>while</strong> <sub><img class="fm-editor-equation" src="assets/9a940658-b350-4def-a52e-d8c73338da96.png" style="width:0.92em;height:0.92em;"/></sub> is not stable:<br/><br/>    &gt; policy evaluation<br/>   <strong>while</strong> <sub><img class="fm-editor-equation" src="assets/4d814211-461a-465d-8ca0-ddf80fb78834.png" style="width:1.33em;height:1.17em;"/></sub> is not stable:<br/>        <strong>for</strong> each state s:<br/>            <img class="fm-editor-equation" src="assets/378aaf55-013d-49b6-a983-79c6886f7090.png" style="width:24.50em;height:3.92em;"/> <br/><br/>    &gt; policy improvement<br/>    <strong>for</strong> each state s:<br/>        <img class="fm-editor-equation" src="assets/a6a0a783-369d-4db8-8392-d5a0e3b981b1.png" style="width:26.00em;height:3.92em;"/> </pre>
<p>After an initialization phase, the outer loop iterates through policy evaluation and policy iteration until a stable policy is found. On each of these iterations, policy evaluation evaluates the policy found during the preceding policy improvement steps, which in turn use the estimated value function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy iteration applied to FrozenLake</h1>
                </header>
            
            <article>
                
<p>To consolidate the ideas behind policy iteration, we'll apply it to a game called FrozenLake. Here, the environment consists of a <span>4 x 4 </span>grid. Using four actions that correspond to the directions (0 is left, 1 is down, 2 is right, and 3 is up), the agent has to move to the opposite side of the grid without falling in the holes. Moreover, movement is uncertain, and the agent has the possibility of movement in other directions. So, in such a situation, it could be beneficial not to move in the intended direction. A reward of +1 is assigned when the end goal is reached. The map of the game is shown in figure 3.4. S is the start position, the star is the end position, and the spirals are the holes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1814 image-border" src="assets/c9d98d17-74ab-42f6-80eb-d542848be9d0.png" style="width:7.08em;height:7.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.4 Map of the FrozenLake game</div>
<p>With all the tools needed, let's see how to solve it.</p>
<div class="packt_infobox">All the code explained in this chapter is available on the GitHub repository of this book, using the following link: <a href="https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python">https://https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python</a></div>
<p>First, we have to create the environment, initializing the value function and the policy:</p>
<pre><span>env </span><span>=</span><span> gym.make(</span><span>'FrozenLake-v0')<br/></span><span>env </span><span>=</span><span> env.unwrapped<br/></span><span>nA </span><span>=</span><span> env.action_space.n<br/></span><span>nS </span><span>=</span><span> env.observation_space.n<br/></span>V <span>=</span><span> np.zeros(nS)<br/></span>policy <span>=</span><span> np.zeros(nS)</span><span><br/></span></pre>
<p>Then, we have to create the main cycle that does one step of policy evaluation and one step of policy improvement. This cycle finishes whenever the policy is stable. To do this, use the following code:</p>
<pre><span>policy_stable </span><span>=</span><span> </span><span>False<br/></span><span>it </span><span>=</span><span> </span><span>0<br/></span><span>while</span><span> </span><span>not</span><span> policy_stable:<br/>    </span><span>policy_evaluation(V, policy)<br/>    p</span><span>olicy_stable </span><span>=</span><span> policy_improvement(V, policy)<br/>    </span><span>it </span><span>+=</span><span> </span><span>1</span></pre>
<p>In the end, we can print the number of iterations completed, the value function, the policy, and the score reached running some test games:</p>
<div>
<pre><span>print</span><span>(</span><span>'Converged after </span><span>%i</span><span> policy iterations'</span><span>%</span><span>(it))<br/></span>run_episodes(env, V, policy)<br/>print<span>(V.reshape((</span><span>4</span><span>,</span><span>4</span><span>)))<br/></span>print<span>(policy.reshape((</span><span>4</span><span>,</span><span>4</span><span>)))</span></pre></div>
<p>Now, before defining <kbd>policy_evaluation</kbd>, we can create a function to evaluate the expected action-value that will also be used in <kbd>policy_improvement</kbd>:</p>
<div>
<pre><span>def</span><span> </span><span>eval_state_action</span><span>(V, s, a, gamma</span><span>=</span><span>0.99</span><span>):<br/></span><span>    return</span><span> np.sum([p </span><span>*</span><span> (rew </span><span>+</span><span> gamma</span><span>*</span><span>V[next_s]) </span><span>for</span><span> p, next_s, rew, _ </span><span>in</span><span> env.P[s][a]])</span></pre></div>
<p>Here, <kbd>env.P</kbd> is a dictionary that contains all the information about the dynamics of the environment.</p>
<p><kbd>gamma</kbd> is the discount factor, with 0.99 being a standard value to use for simple and medium difficulty problems. The higher it is, the more difficult it is for the agent to predict the value of a state because it should look further into the future.</p>
<p>Next, we can define the <kbd>policy_evaluation</kbd> function. <kbd>policy_evaluation</kbd> has to calculate <span>formula (</span>8) under the current policy for every state until it reaches steady values. Because the policy is deterministic, we only evaluate one action:</p>
<div>
<pre><span>def</span><span> </span><span>policy_evaluation</span><span>(V, policy, eps</span><span>=</span><span>0.0001</span><span>):<br/></span><span>    while</span><span> </span><span>True</span><span>:<br/></span><span>        delta </span><span>=</span><span> </span><span>0<br/></span><span>        for</span><span> s </span><span>in</span><span> </span><span>range</span><span>(nS):<br/></span><span>            old_v </span><span>=</span><span> V[s]<br/></span><span>            V[s] </span><span>=</span><span> eval_state_action(V, s, policy[s])<br/></span><span>            delta </span><span>=</span><span> </span><span>max</span><span>(delta, np.abs(old_v </span><span>-</span><span> V[s]))<br/></span><span>        if</span><span> delta </span><span>&lt;</span><span> eps:<br/></span><span>            break<br/></span></pre></div>
<p>We consider the value function stable whenever <kbd>delta</kbd> is lower than the threshold, <kbd>eps</kbd>. When these conditions are met, the <kbd>while</kbd> loop statement is stopped.</p>
<p><kbd>policy_improvement</kbd> takes the value function and the policy and iterates them across all of the states to update the policy based on the new value function: </p>
<div>
<pre><span>def</span><span> </span><span>policy_improvement</span><span>(V, policy):<br/></span><span>    policy_stable </span><span>=</span><span> </span><span>True<br/></span><span>    for</span><span> s </span><span>in</span><span> </span><span>range</span><span>(nS):<br/></span><span>        old_a </span><span>=</span><span> policy[s]<br/></span><span>        policy[s] </span><span>=</span><span> np.argmax([eval_state_action(V, s, a) </span><span>for</span><span> a </span><span>in</span><span> </span><span>range</span><span>(nA)])<br/></span><span>        if</span><span> old_a </span><span>!=</span><span> policy[s]: <br/></span><span>            policy_stable </span><span>=</span><span> </span><span>False<br/></span><span>    return</span><span> policy_stable</span></pre></div>
<p><kbd>policy_improvement(V, policy)</kbd> returns <kbd>False</kbd> until the policy changes. That's because it means that the policy isn't stable yet.</p>
<p>The final snippet of code runs some games to test the new policy and prints the number of games won:</p>
<div>
<pre><span>def</span><span> </span><span>run_episodes</span><span>(env, V, policy, num_games</span><span>=</span><span>100</span><span>):<br/></span><span>    tot_rew </span><span>=</span><span> </span><span>0<br/></span><span>    state </span><span>=</span><span> env.reset()<br/></span><span>    for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(num_games):<br/></span><span>        done </span><span>=</span><span> </span><span>False<br/></span><span>        while</span><span> </span><span>not</span><span> done:<br/></span><span>            next_state, reward, done, _ </span><span>=</span><span> env.step(policy[state])<br/></span><span>            state </span><span>=</span><span> next_state<br/></span><span>            tot_rew </span><span>+=</span><span> reward <br/></span><span>            if</span><span> done:<br/></span><span>                state </span><span>=</span><span> env.reset()<br/></span><span>    print</span><span>(</span><span>'Won </span><span>%i</span><span> of </span><span>%i</span><span> games!'</span><span>%</span><span>(tot_rew, num_games))</span></pre></div>
<p>That's it. </p>
<p>It converges in about 7 iterations and wins approximately 85% of games: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1815 image-border" src="assets/7d88776b-8ea4-4c76-abdf-e61bc3a217f1.png" style="width:24.17em;height:11.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.5 Results of the FrozenLake game. The optimal policy is on the left and the optimal state values are on the right</div>
<p>The policy resulting from the code is shown on the left of figure 3.5. You can see that it takes strange directions, but it's only because it follows the dynamics of the environment. On the right of figure 3.5, the final state's values are presented. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Value iteration</h1>
                </header>
            
            <article>
                
<p>Value iteration is the other dynamic programming algorithm to find optimal values in an MDP, but unlike policy iterations that execute policy evaluations and policy iterations in a loop, value iteration combines the two methods in a single update. In particular, it updates the value of a state by selecting the best action immediately:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4ac987cf-3e3e-4ae7-9158-2de7a6806d5a.png" style="width:30.92em;height:3.08em;"/></p>
<p>The code for value iteration is even simpler than the policy iteration code, summarized in the following pseudocode: </p>
<pre>Initialize <img class="fm-editor-equation" src="assets/207a9598-26a0-4a7f-adcc-eaadf508668a.png" style="width:1.83em;height:1.08em;"/> for every state <img class="fm-editor-equation" src="assets/6249e54a-93cc-4d56-beac-48daad47bf48.png" style="width:0.67em;height:0.83em;"/><br/><br/><strong>while</strong> <img class="fm-editor-equation" src="assets/9d992fac-43a3-41d3-8588-bc7e22f006de.png" style="width:0.75em;height:0.83em;"/> is not stable:<br/>    &gt; value iteration<br/>    <strong>for</strong> each state s:<br/>        <img class="fm-editor-equation" src="assets/c7b053c8-78a2-40da-bcfc-9988cc658e67.png" style="width:21.67em;height:3.25em;"/><br/><br/>&gt; compute the optimal policy:<br/><img class="fm-editor-equation" src="assets/58841971-bfa7-4d4e-98a2-5bb22da905c2.png" style="width:19.83em;height:3.17em;"/></pre>
<p>The only difference is in the new value estimation update and in the absence of a proper policy iteration module. The resulting optimal policy is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/63edbf60-b522-4259-a519-9a0c94167b59.png" style="width:30.67em;height:3.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Value iteration applied to FrozenLake</h1>
                </header>
            
            <article>
                
<p>We can now apply value iteration to the FrozenLake game in order to compare the two DP algorithms and to see whether they converge to the same policy and value function.</p>
<p>Let's define <kbd>eval_state_action</kbd> as before to estimate the action state value for a state-action pair:</p>
<div>
<pre><span>def</span><span> </span><span>eval_state_action</span><span>(V, s, a, gamma</span><span>=</span><span>0.99</span><span>):<br/></span><span>    return</span><span> np.sum([p </span><span>*</span><span> (rew </span><span>+</span><span> gamma</span><span>*</span><span>V[next_s]) </span><span>for</span><span> p, next_s, rew, _ </span><span>in</span><span> env.P[s][a]])</span></pre></div>
<p>Then, we create the main body of the value iteration algorithm:</p>
<div>
<pre><span>def</span><span> </span><span>value_iteration</span><span>(eps</span><span>=</span><span>0.0001</span><span>):<br/></span><span>    V </span><span>=</span><span> np.zeros(nS)<br/></span><span>    it </span><span>=</span><span> </span><span>0<br/></span><span>    while</span><span> </span><span>True</span><span>:<br/></span><span>        delta </span><span>=</span><span> </span><span>0<br/>        # update the value for each state<br/></span><span>        for</span><span> s </span><span>in</span><span> </span><span>range</span><span>(nS):<br/></span><span>            old_v </span><span>=</span><span> V[s]<br/></span><span>            V[s] </span><span>=</span><span> np.max([eval_state_action(V, s, a) </span><span>for</span><span> a </span><span>in</span><span> </span><span>range</span><span>(nA)]) # equation 3.10<br/></span><span>            delta </span><span>=</span><span> </span><span>max</span><span>(delta, np.abs(old_v </span><span>-</span><span> V[s]))<br/>        # if stable, break the cycle<br/></span><span>        if</span><span> delta </span><span>&lt;</span><span> eps:<br/></span><span>            break<br/></span><span>        else</span><span>:<br/></span><span>            print</span><span>(</span><span>'Iter:'</span><span>, it, </span><span>' delta:'</span><span>, np.round(delta,5))<br/></span><span>        it </span><span>+=</span><span> </span><span>1<br/></span><span>    return</span><span> V</span></pre></div>
<p><span>It loops until it reaches a steady value function (determined by the threshold, </span><kbd>eps</kbd><span>) and for each iteration, it updates the value of each state using formula (10).</span></p>
<p>As for the policy iteration, <kbd>run_episodes</kbd> executes some games to test the policy. The only difference is that in this case, the policy is determined at the same time that <kbd>run_episodes</kbd> is executed (for policy iteration, we defined the action for every state beforehand):</p>
<div>
<pre><span>def</span><span> </span><span>run_episodes</span><span>(env, V, num_games</span><span>=</span><span>100</span><span>):<br/></span><span>    tot_rew </span><span>=</span><span> </span><span>0<br/></span><span>    state </span><span>=</span><span> env.reset()<br/><br/></span><span>    for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(num_games):<br/></span><span>        done </span><span>=</span><span> </span><span>False<br/><br/></span><span>        while</span><span> </span><span>not</span><span> done:<br/>            # choose the best action using the value function<br/></span><span>            action </span><span>=</span><span> np.argmax([eval_state_action(V, state, a) </span><span>for</span><span> a </span><span>in</span><span> </span><span>range</span><span>(nA)]) #(11)<br/></span><span>            next_state, reward, done, _ </span><span>=</span><span> env.step(action)<br/></span><span>            state </span><span>=</span><span> next_state<br/></span><span>            tot_rew </span><span>+=</span><span> reward <br/></span><span>            if</span><span> done:<br/></span><span>                state </span><span>=</span><span> env.reset()<br/><br/></span><span>    print</span><span>(</span><span>'Won </span><span>%i</span><span> of </span><span>%i</span><span> games!'</span><span>%</span><span>(tot_rew, num_games))</span></pre></div>
<p>Finally, we can create the environment, unwrap it, run the value iteration, and execute some test games:</p>
<div>
<pre><span>env </span><span>=</span><span> gym.make(</span><span>'FrozenLake-v0'</span><span>)<br/></span><span>env </span><span>=</span><span> env.unwrapped<br/><br/></span><span>nA </span><span>=</span><span> env.action_space.n<br/></span><span>nS </span><span>=</span><span> env.observation_space.n<br/><br/></span><span>V </span><span>=</span><span> value_iteration(eps</span><span>=</span><span>0.0001</span><span>)<br/></span><span>run_episodes(env, V, </span><span>100</span><span>)<br/></span><span>print</span><span>(V.reshape((</span><span>4</span><span>,</span><span>4</span><span>)))</span></pre></div>
<p>The output will be similar to the following:</p>
<pre>Iter: 0 delta: 0.33333<br/>Iter: 1 delta: 0.1463<br/>Iter: 2 delta: 0.10854<br/>...<br/>Iter: 128 delta: 0.00011<br/>Iter: 129 delta: 0.00011<br/>Iter: 130 delta: 0.0001<br/>Won 86 of 100 games!<br/>[[0.54083394 0.49722378 0.46884941 0.45487071]<br/> [0.55739213 0.         0.35755091 0.        ]<br/> [0.5909355  0.64245898 0.61466487 0.        ]<br/> [0.         0.74129273 0.86262154 0.        ]]</pre>
<p>The value iteration algorithm converges after 130 iterations. The resulting value function and policy are the same as the policy iteration algorithm. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>An RL problem can be formalized as an MDP, providing an abstract framework for learning goal-based problems. An MDP is defined by a set of states, actions, rewards, and transition probabilities, and solving an MDP means finding a policy that maximizes the expected reward in each state. The Markov property is intrinsic to the MDP and ensures that the future states depend only on the current one, not on its history. </p>
<p>Using the definition of MDP, we formulated the concepts of policy, return function, expected return, action-value function, and value function. The latter two can be defined in terms of the values of the subsequent states, and the equations are called Bellman equations. These equations are useful because they provide a method to compute value functions in an iterative way. The optimal value functions can then be used to find the optimal policy.</p>
<p class="mce-root">RL algorithms can be categorized as model-based or <span>model-free</span>. While the former requires a model of the environment to plan the next actions, the latter is independent of the model and can learn by direct interaction with the environment. Model-free algorithms can be further divided into policy gradient and value function algorithms. Policy gradient algorithms learn directly from the policy through gradient ascent and are typically on-policy. Value function algorithms are usually off-policy, and learn an action-value function or value function in order to create the policy. These two methods can be brought together to give rise to methods that combine the advantages of both worlds. </p>
<p>DP is the first set of model-based algorithms that we looked at in depth. It is used whenever the full model of the environment is known and when it is constituted by a limited number of states and actions. DP algorithms use bootstrapping to estimate the value of a state and they learn the optimal policy through two processes: policy evaluation and policy improvement. Policy evaluation computes the state value function for an arbitrary policy, while policy improvement improves the policy using the value function obtained from the policy evaluation process. </p>
<p>By combining policy improvement and policy evaluation, the policy iteration algorithm and the value iteration algorithm can be created. The main difference between the two is that while policy iteration runs iteratively of policy evaluation and policy improvement, value iteration combines the two processes in a single update. </p>
<p>Though DP suffers from the curse of dimensionality (the complexity grows exponentially with the number of states), the ideas behind policy evaluation and policy iteration are key in almost all RL algorithms because they use a generalized version of them.</p>
<p>Another disadvantage of DP is that it requires the exact model of the environment, limiting its applicability to many other problems.</p>
<p>In the next chapter, you'll see how V-functions and Q-functions can be used to learn a policy, using problems where the model is unknown by sampling directly from the environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What's an MDP?</li>
<li>What's a stochastic policy?</li>
<li>How can a return function be defined in terms of the return at the next time step?</li>
<li>Why is the Bellman equation so important?</li>
<li>What are the limiting factors of DP algorithms?</li>
<li>What is policy evaluation?</li>
<li>How do policy iteration and value iteration differ?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>Sutton and Barto, <em>Reinforcement Learning</em>, Chapters 3 and 4</li>
</ul>


            </article>

            
        </section>
    </body></html>