<html><head></head><body>
		<div id="_idContainer079">
			<h1 id="_idParaDest-210"><em class="italic"><a id="_idTextAnchor248"/>Chapter 7</em>: Captioning Images with CNNs and RNNs</h1>
			<p>Equipping neural networks with the ability to describe visual scenes in a human-readable fashion has to be one of the most interesting yet challenging applications of deep learning. The <a id="_idIndexMarker601"/>main difficulty arises from the fact that this problem combines two major subfields<a id="_idIndexMarker602"/> of artificial intelligence: <strong class="bold">Computer Vision</strong> (<strong class="bold">CV</strong>) and <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>).</p>
			<p>The architectures of <a id="_idIndexMarker603"/>most image captioning networks use a <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>) to encode images in a numeric format so <a id="_idIndexMarker604"/>that they're suitable for the consumption of the decoder, which is typically a <strong class="bold">Recurrent Neural Network</strong> (<strong class="bold">RNN</strong>). This is a kind of network specialized in learning from sequential data, such as time series, video, and text.</p>
			<p>As we'll see in this chapter, the challenges of building a system with these capabilities start with preparing the data, which we'll cover in the first recipe. Then, we'll implement an image captioning solution from scratch. In the third recipe, we'll use this model to generate captions for our own pictures. Finally, in the fourth recipe, we'll learn how to include an attention mechanism in our architecture so that we can understand what parts of the image the network is looking at when generating each word in the output caption.</p>
			<p>Pretty interesting, don't you agree?</p>
			<p>Specifically, we'll cover the following recipes in this chapter:</p>
			<ul>
				<li>Implementing a reusable image caption feature extrac<a id="_idTextAnchor249"/><a id="_idTextAnchor250"/>tor</li>
				<li>Implementing an image captioning network</li>
				<li>Generating captions for your own photos</li>
				<li>Implementing an image captioning network on COCO with attention</li>
				<li>Let's get started!</li>
			</ul>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor251"/>Technical requirements</h1>
			<p>Image captioning is a problem that requires vast amounts of resources in terms of memory, storage, and computing power. My recommendation is that you use a cloud-based solution such as AWS or FloydHub to run the recipes in this chapter unless you have sufficiently capable hardware. As expected, a GPU is of paramount importance to complete the recipes in this chapter. In the <em class="italic">Getting ready</em> section of each recipe, you'll find what you'll need to prepare. The code of this chapter is available here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3qmpVme">https://bit.ly/3qmpVme</a>.</p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor252"/>Implementing a reusable image caption feature extractor</h1>
			<p>The first step<a id="_idIndexMarker605"/> of creating an image captioning, deep learning-based solution is to transform the data into a format that can be used by certain networks. This means we must encode images as vectors, or tensors, and the text as embeddings, which are vectorial representations of sentences. </p>
			<p>In this recipe, we will implement a customizable and reusable component that will allow us to preprocess the data we'll need to implement an image captioner beforehand, thus saving us tons of time later on in the process.</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor253"/>Getting ready</h2>
			<p>The dependencies we need are <strong class="source-inline">tqdm</strong> (to display a nice progress bar) and <strong class="source-inline">Pillow</strong> (to load and manipulate images using TensorFlow's built-in functions):</p>
			<p class="source-code">$&gt; pip install Pillow tqdm</p>
			<p>We will use the <strong class="source-inline">Flickr8k</strong> dataset, which is available on <strong class="bold">Kaggle</strong>: <a href="https://www.kaggle.com/adityajn105/flickr8k">https://www.kaggle.com/adityajn105/flickr8k</a>. Log in or sign up, download it, and decompress it in a directory of your choosing. For the purposes of this tutorial, we assume the data is in the <strong class="source-inline">~/.keras/datasets/flickr8k</strong> folder.</p>
			<p>Here are some sample images:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B14768_07_001.jpg" alt="Figure 7.1 – Sample images from Flickr8k&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Sample images from Flickr8k</p>
			<p>With that, we <a id="_idIndexMarker606"/>are good to go! </p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor254"/>How to do it…</h2>
			<p>Follow these steps to create a reusable feature extractor for image captioning problems:</p>
			<ol>
				<li>Import all the necessary dependencies:<p class="source-code">import glob</p><p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">import pickle</p><p class="source-code">from string import punctuation</p><p class="source-code">import numpy as np</p><p class="source-code">import tqdm</p><p class="source-code">from tensorflow.keras.applications.vgg16 import *</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p><p class="source-code">from tensorflow.keras.preprocessing.sequence import \</p><p class="source-code">    pad_sequences</p><p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p><p class="source-code">from tensorflow.keras.utils import to_categorical</p><p class="source-code">from tqdm import tqdm</p></li>
				<li>Define<a id="_idIndexMarker607"/> the <strong class="source-inline">ImageCaptionFeatureExtractor</strong> class and its constructor:<p class="source-code">class ImageCaptionFeatureExtractor(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 output_path,</p><p class="source-code">                 start_token='beginsequence',</p><p class="source-code">                 end_token='endsequence',</p><p class="source-code">                 feature_extractor=None,</p><p class="source-code">                 input_shape=(224, 224, 3)):</p></li>
				<li>Next, we must receive the path where the outputs will be stored, along with the tokens that we'll use to delimit the start and end of a text sequence. We must also take the input shape of the feature extractor as an argument. Next, let's store these values as members:<p class="source-code">        self.input_shape = input_shape</p><p class="source-code">        if feature_extractor is None:</p><p class="source-code">            input = Input(shape=input_shape)</p><p class="source-code">            self.feature_extractor = VGG16(input_ </p><p class="source-code">                                     tensor=input,</p><p class="source-code">                                   weights='imagenet',</p><p class="source-code">                                   include_top=False)</p><p class="source-code">        else:</p><p class="source-code">            self.feature_extractor = feature_extractor</p><p class="source-code">        self.output_path = output_path</p><p class="source-code">        self.start_token = start_token</p><p class="source-code">        self.end_token = end_token</p><p class="source-code">        self.tokenizer = Tokenizer()</p><p class="source-code">        self.max_seq_length = None</p></li>
				<li>If we don't<a id="_idIndexMarker608"/> receive any <strong class="source-inline">feature_extractor</strong>, we'll use <strong class="source-inline">VGG16</strong> by default. Next, define a public method that will extract the features from an image, given its path:<p class="source-code">    def extract_image_features(self, image_path):</p><p class="source-code">        image = load_img(image_path,</p><p class="source-code">                       target_size=self.input_shape[:2])</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        image = np.expand_dims(image, axis=0)</p><p class="source-code">        image = preprocess_input(image)</p><p class="source-code">        return self.feature_extractor.predict(image)[0]</p></li>
				<li>In order to clean the captions, we must get rid of all the punctuation characters and single-letter words (such as <em class="italic">a</em>). The <strong class="source-inline">_clean_captions()</strong> method performs this <a id="_idIndexMarker609"/>task, and also adds special tokens; that is, <strong class="source-inline">self.start_token</strong> and <strong class="source-inline">self.end_token</strong>:<p class="source-code">    def _clean_captions(self, captions):</p><p class="source-code">        def remove_punctuation(word):</p><p class="source-code">            translation = str.maketrans('', '',</p><p class="source-code">                                        punctuation)</p><p class="source-code">            return word.translate(translation)</p><p class="source-code">        def is_valid_word(word):</p><p class="source-code">            return len(word) &gt; 1 and word.isalpha()</p><p class="source-code">        cleaned_captions = []</p><p class="source-code">        for caption in captions:</p><p class="source-code">            caption = caption.lower().split(' ')</p><p class="source-code">            caption = map(remove_punctuation, caption)</p><p class="source-code">            caption = filter(is_valid_word, caption)</p><p class="source-code">            cleaned_caption = f'{self.start_token} ' \</p><p class="source-code">                              f'{“ “.join(caption)} ' \</p><p class="source-code">                              f'{self.end_token}'</p><p class="source-code">            cleaned_captions.append(cleaned_caption)</p><p class="source-code">        return cleaned_captions</p></li>
				<li>We also need to compute the length of the longest caption, which we can do with the <strong class="source-inline">_get_max_seq_length()</strong> method. This is defined as follows:<p class="source-code">    def _get_max_seq_length(self, captions):</p><p class="source-code">        max_sequence_length = -1</p><p class="source-code">        for caption in captions:</p><p class="source-code">            caption_length = len(caption.split(' '))</p><p class="source-code">            max_sequence_length = </p><p class="source-code">                            max(max_sequence_length,</p><p class="source-code">                                      caption_length)</p><p class="source-code">        return max_sequence_length</p></li>
				<li>Define a <a id="_idIndexMarker610"/>public method, <strong class="source-inline">extract_features()</strong>, which receives a list of image paths and captions and uses them to extract features from both the images and text sequences:<p class="source-code">    def extract_features(self, images_path, captions):</p><p class="source-code">        assert len(images_path) == len(captions)</p></li>
				<li>Note that both lists must be of the same size. The next step is to clean the captions, compute the maximum sequence length, and fit a tokenizer to all the captions:<p class="source-code">        captions = self._clean_captions(captions)</p><p class="source-code">        self.max_seq_length=self._get_max_seq_ </p><p class="source-code">                                   length(captions) </p><p class="source-code">        self.tokenizer.fit_on_texts(captions)</p></li>
				<li>We'll iterate over each image path and caption pair, extracting the features from the image. Then, we'll save an entry in our <strong class="source-inline">data_mapping</strong> <strong class="source-inline">dict</strong>, associating the image ID (present in <strong class="source-inline">image_path</strong>) with the corresponding visual <a id="_idIndexMarker611"/>features and clean caption:<p class="source-code">        data_mapping = {}</p><p class="source-code">        print('\nExtracting features...')</p><p class="source-code">        for i in tqdm(range(len(images_path))):</p><p class="source-code">            image_path = images_path[i]</p><p class="source-code">            caption = captions[i]</p><p class="source-code">         feats = self.extract_image_features(image_ path)</p><p class="source-code">            image_id = image_path.split(os.path.sep)[-1]</p><p class="source-code">            image_id = image_id.split('.')[0]</p><p class="source-code">            data_mapping[image_id] = {</p><p class="source-code">                'features': feats,</p><p class="source-code">                'caption': caption</p><p class="source-code">            }</p></li>
				<li>We'll save this <strong class="source-inline">data_mapping</strong> to disk, in pickle format:<p class="source-code">        out_path = f'{self.output_path}/data_mapping.pickle'</p><p class="source-code">        with open(out_path, 'wb') as f:</p><p class="source-code">            pickle.dump(data_mapping, f, protocol=4)</p></li>
				<li>We'll complete this method by creating and storing the sequences that'll be inputted to an image captioning network in the future:<p class="source-code">        self._create_sequences(data_mapping)</p></li>
				<li>The following method creates the input and output sequences that will be used to train an image captioning model (see the <em class="italic">How it works…</em> section for a deeper explanation). We will start by determining the number of output classes, which is the vocabulary size plus one (to account for out-of-vocabulary tokens). We must also<a id="_idIndexMarker612"/> define the lists where we'll store the sequences:<p class="source-code">    def _create_sequences(self, mapping):</p><p class="source-code">        num_classes = len(self.tokenizer.word_index) + 1</p><p class="source-code">        in_feats = []</p><p class="source-code">        in_seqs = []</p><p class="source-code">        out_seqs = []</p></li>
				<li>Next, we'll iterate over each features-caption pair. We will transform the caption from a string into a sequence of numbers that represents the words in the sentence:<p class="source-code">        print('\nCreating sequences...')</p><p class="source-code">        for _, data in tqdm(mapping.items()):</p><p class="source-code">            feature = data['features']</p><p class="source-code">            caption = data['caption']</p><p class="source-code">            seq = self.tokenizer.texts_to_</p><p class="source-code">                       sequences([caption])</p><p class="source-code">            seq = seq[0]</p></li>
				<li>Next, we'll generate as many input sequences as there are words in a caption. Each input sequence will be used to generate the next word in the sequence. Therefore, for a given index, <strong class="source-inline">i</strong>, the input sequence will be all the elements up to <strong class="source-inline">i-1</strong>, while the corresponding output sequence, or label, will be the one-hot encoded element at <strong class="source-inline">i</strong> (the next word). To ensure all the input sequences are the same length, we must pad them:<p class="source-code">            for i in range(1, len(seq)):</p><p class="source-code">                input_seq = seq[:i]</p><p class="source-code">                input_seq, = </p><p class="source-code">                   pad_sequences([input_seq],</p><p class="source-code">                                          </p><p class="source-code">                     self.max_seq_length)</p><p class="source-code">                out_seq = seq[i]</p><p class="source-code">                out_seq = to_categorical([out_seq],</p><p class="source-code">                                         </p><p class="source-code">                                       num_classes)[0]</p></li>
				<li>We then add <a id="_idIndexMarker613"/>the visual feature vector, the input sequence, and the output sequence to the corresponding lists:<p class="source-code">                in_feats.append(feature)</p><p class="source-code">                in_seqs.append(input_seq)</p><p class="source-code">                out_seqs.append(out_seq)</p></li>
				<li>Finally, we must write the sequences to disk, in pickle format:<p class="source-code">        file_paths = [</p><p class="source-code">            f'{self.output_path}/input_features.pickle',</p><p class="source-code">            f'{self.output_path}/input_sequences.pickle',</p><p class="source-code">            f'{self.output_path}/output_sequences.</p><p class="source-code">                                                 pickle']</p><p class="source-code">        sequences = [in_feats,</p><p class="source-code">                     in_seqs,</p><p class="source-code">                     out_seqs]</p><p class="source-code">        for path, seq in zip(file_paths, sequences):</p><p class="source-code">            with open(path, 'wb') as f:</p><p class="source-code">                pickle.dump(np.array(seq), f, </p><p class="source-code">                            protocol=4)</p></li>
				<li>Let's define <a id="_idIndexMarker614"/>the paths to the <strong class="source-inline">Flickr8k</strong> images and captions:<p class="source-code">BASE_PATH = (pathlib.Path.home() / '.keras' / 'datasets'       </p><p class="source-code">                                      /'flickr8k')</p><p class="source-code">IMAGES_PATH = str(BASE_PATH / 'Images')</p><p class="source-code">CAPTIONS_PATH = str(BASE_PATH / 'captions.txt')</p></li>
				<li>Create an instance of the feature extractor class we just implemented:<p class="source-code">extractor = ImageCaptionFeatureExtractor(output_path='.')</p></li>
				<li>List all the image files in the <strong class="source-inline">Flickr8k</strong> dataset:<p class="source-code">image_paths = list(glob.glob(f'{IMAGES_PATH}/*.jpg'))</p></li>
				<li>Read the contents of the captions file:<p class="source-code">with open(CAPTIONS_PATH, 'r') as f:</p><p class="source-code">    text = f.read()</p><p class="source-code">    lines = text.split('\n')</p></li>
				<li>Now, we must create a map that will associate each image with multiple captions. The key is the image ID, while the value is a list of all captions associated <a id="_idIndexMarker615"/>with such an image:<p class="source-code">mapping = {}</p><p class="source-code">for line in lines:</p><p class="source-code">    if '.jpg' not in line:</p><p class="source-code">        continue</p><p class="source-code">    tokens = line.split(',', maxsplit=1)</p><p class="source-code">    if len(line) &lt; 2:</p><p class="source-code">        continue</p><p class="source-code">    image_id, image_caption = tokens</p><p class="source-code">    image_id = image_id.split('.')[0]</p><p class="source-code">    captions_per_image = mapping.get(image_id, [])</p><p class="source-code">    captions_per_image.append(image_caption)</p><p class="source-code">    mapping[image_id] = captions_per_image</p></li>
				<li>We will only keep one caption per image:<p class="source-code">captions = []</p><p class="source-code">for image_path in image_paths:</p><p class="source-code">    image_id = image_path.split('/')[-1].split('.')[0]</p><p class="source-code">    captions.append(mapping[image_id][0])</p></li>
				<li>Finally, we must use our extractor to produce the data mapping and corresponding input sequences:<p class="source-code">extractor.extract_features(image_paths, captions)</p><p>This process may take a while. After several minutes, we should see the following files in the output path:</p><p class="source-code">data_mapping.pickle     input_features.pickle   input_sequences.pickle  output_sequences.pickle</p></li>
			</ol>
			<p>We'll see how <a id="_idIndexMarker616"/>this all works in the next section.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor255"/>How it works…</h2>
			<p>In this recipe, we learned that one of the keys to creating a good image captioning system is to put the data in a suitable format. This allows the network to learn how to describe, with text, what's happening in a visual scenario.</p>
			<p>There are many ways to frame an image captioning problem, but the most popular and effective way is to use each word to generate the next word in the caption. This way, we'll construct the sentence, word by word, passing each intermediate output as the input to the next cycle. (This is how <strong class="bold">RNNs</strong> work. To read more about them, refer to the <em class="italic">See also</em> section.)</p>
			<p>You might be wondering how we pass the visual information to the network. This is where the feature extraction step is crucial, because we convert each image in our dataset into a numeric vector that summarizes the spatial information in each picture. Then, we pass the same feature vector along each input sequence when training the network. This way, the network will learn to associate all the words in a caption with the same image.</p>
			<p>If we're not careful, we could get trapped in an endless loop of word generation. How can we prevent this? By using a special token to signal the end of a sequence (this means the network should stop producing words when it encounters such a token). In our case, this token is, by default, <strong class="source-inline">endsequence</strong>. </p>
			<p>A similar problem is how to start a sequence. Which word should we use? In this case, we must also resort to a special token (our default is <strong class="source-inline">beginsequence</strong>). This acts as a seed that the network will use to start producing captions.</p>
			<p>All of this might sound confusing now, and that's because we've only focused on the data preprocessing stage. In the remaining recipes of this chapter, we'll leverage the work we've <a id="_idIndexMarker617"/>done here to train many different image captioners, and all the pieces will fall into place! </p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor256"/>See also</h2>
			<p>Here's a great <a id="_idIndexMarker618"/>explanation of how <strong class="bold">RNNs</strong> work: <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg">https://www.youtube.com/watch?v=UNmqTiOnRfg</a>.</p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor257"/>Implementing an image captioning network</h1>
			<p>An image<a id="_idIndexMarker619"/> captioning architecture is comprised of an encoder and a decoder. The encoder is a <strong class="bold">CNN</strong> (typically a pre-trained one), which converts input images into numeric vectors. These vectors are then passed, along with text sequences, to the decoder, which is an <strong class="bold">RNN</strong>, that will learn, based on these values, how to iteratively generate each word in the corresponding caption.</p>
			<p>In this recipe, we'll implement an image captioner that's been trained on the <strong class="source-inline">Flickr8k</strong> dataset. We'll leverage the feature extractor we implemented in the <em class="italic">Implementing a reusable image caption feature extractor</em> recipe.</p>
			<p>Let's begin, shall we?</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor258"/>Getting ready</h2>
			<p>The external dependencies we'll be using in this recipe are <strong class="source-inline">Pillow</strong>, <strong class="source-inline">nltk</strong>, and <strong class="source-inline">tqdm</strong>. You can install them all at once with the following command:</p>
			<p class="source-code">$&gt; pip install Pillow nltk tqdm</p>
			<p>We will use <a id="_idIndexMarker620"/>the <strong class="source-inline">Flickr8k</strong> dataset, which you can get from <strong class="bold">Kaggle</strong>: <a href="https://www.kaggle.com/adityajn105/flickr8k">https://www.kaggle.com/adityajn105/flickr8k</a>. In order to fetch it, log in or sign up, download it, and decompress its contents in a location of your preference. For the purposes of this tutorial, we assume the data is in the <strong class="source-inline">~/.keras/datasets/flickr8k</strong> directory.</p>
			<p>The following are some sample images from the <strong class="source-inline">Flickr8k</strong> dataset:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B14768_07_002.jpg" alt="Figure 7.2 – Sample images from Flickr8k&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Sample images from Flickr8k</p>
			<p>Let's head over<a id="_idIndexMarker621"/> to the next section to start this recipe's implementation.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor259"/>How to do it…</h2>
			<p>Follow these steps to implement a deep learning-based image captioning system:</p>
			<ol>
				<li value="1">First, we must import all of the required packages:<p class="source-code">import glob</p><p class="source-code">import pathlib</p><p class="source-code">import pickle</p><p class="source-code">import numpy as np</p><p class="source-code">from nltk.translate.bleu_score import corpus_bleu</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from tensorflow.keras.applications.vgg16 import *</p><p class="source-code">from tensorflow.keras.callbacks import ModelCheckpoint</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.preprocessing.sequence import \</p><p class="source-code">    pad_sequences</p><p class="source-code">from ch7.recipe1.extractor import ImageCaptionFeatureExtractor</p></li>
				<li>Define the paths to the images and captions, as well as the output path, which is where we'll<a id="_idIndexMarker622"/> store the artifacts that will be created in this recipe:<p class="source-code">BASE_PATH = (pathlib.Path.home() / '.keras' / 'datasets'     </p><p class="source-code">             /'flickr8k')</p><p class="source-code">IMAGES_PATH = str(BASE_PATH / 'Images')</p><p class="source-code">CAPTIONS_PATH = str(BASE_PATH / 'captions.txt')</p><p class="source-code">OUTPUT_PATH = '.'</p></li>
				<li>Define a function that will load a list of image paths and their corresponding captions. This implementation is similar to <em class="italic">Steps</em> <em class="italic">20</em> through <em class="italic">22</em> of the <em class="italic">Implementing a reusable image caption feature extractor</em> recipe:<p class="source-code">def load_paths_and_captions():</p><p class="source-code">    image_paths = list(glob.glob(f'{IMAGES_PATH}/*.jpg'))</p><p class="source-code">    with open(f'{CAPTIONS_PATH}', 'r') as f:</p><p class="source-code">        text = f.read()</p><p class="source-code">        lines = text.split('\n')</p><p class="source-code">    mapping = {}</p><p class="source-code">    for line in lines:</p><p class="source-code">        if '.jpg' not in line:</p><p class="source-code">            continue</p><p class="source-code">        tokens = line.split(',', maxsplit=1)</p><p class="source-code">        if len(line) &lt; 2:</p><p class="source-code">            continue</p><p class="source-code">        image_id, image_caption = tokens</p><p class="source-code">        image_id = image_id.split('.')[0]</p><p class="source-code">        captions_per_image = mapping.get(image_id, [])</p><p class="source-code">        captions_per_image.append(image_caption)</p><p class="source-code">        mapping[image_id] = captions_per_image</p></li>
				<li>Compile <a id="_idIndexMarker623"/>all the captions:<p class="source-code">    all_captions = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image_id = image_path.split('/')[-</p><p class="source-code">                   1].split('.')[0]</p><p class="source-code">        all_captions.append(mapping[image_id][0])</p><p class="source-code">    return image_paths, all_captions</p></li>
				<li>Define a function that will build the architecture of the network, which receives the vocabulary size, the maximum sequence length, and the encoder's input shape:<p class="source-code">def build_network(vocabulary_size,</p><p class="source-code">                  max_sequence_length,</p><p class="source-code">                  input_shape=(4096,)):</p></li>
				<li>The first part of the network receives the feature vectors and passes them through <a id="_idIndexMarker624"/>a fully connected <strong class="source-inline">ReLU</strong> activated layer:<p class="source-code">    x = Dropout(rate=0.5)(feature_inputs)</p><p class="source-code">    x = Dense(units=256)(x)</p><p class="source-code">    feature_output = ReLU()(x)</p></li>
				<li>The second part of the layer receives the text sequences, transformed into numeric vectors, and trains an embedding of 256 elements. Then, it passes that embedding to an <strong class="source-inline">LSTM</strong> layer:<p class="source-code">    sequence_inputs = </p><p class="source-code">            Input(shape=(max_sequence_length,))</p><p class="source-code">    y = Embedding(input_dim=vocabulary_size,</p><p class="source-code">                  output_dim=256,</p><p class="source-code">                  mask_zero=True)(sequence_inputs)</p><p class="source-code">    y = Dropout(rate=0.5)(y)</p><p class="source-code">    sequence_output = LSTM(units=256)(y)</p></li>
				<li>We concatenate the outputs of these two parts and pass the concatenation through a fully connected network, with an output layer with as many units as there are words in our vocabulary. By <strong class="source-inline">Softmax</strong> activating this output, we get a one-hot encoded vector that corresponds to a word in the vocabulary:<p class="source-code">    z = Add()([feature_output, sequence_output])</p><p class="source-code">    z = Dense(units=256)(z)</p><p class="source-code">    z = ReLU()(z)</p><p class="source-code">    z = Dense(units=vocabulary_size)(z)</p><p class="source-code">    outputs = Softmax()(z)</p></li>
				<li>Finally, we build the model, passing the image features and text sequences as inputs, and outputting the one-hot encoded vectors:<p class="source-code">    return Model(inputs=[feature_inputs, </p><p class="source-code">                  sequence_inputs],</p><p class="source-code">                 outputs=outputs)</p></li>
				<li>Define a <a id="_idIndexMarker625"/>function that will convert an integer index into a word by using the tokenizer's internal mapping:<p class="source-code">def get_word_from_index(tokenizer, index):</p><p class="source-code">    return tokenizer.index_word.get(index, None)</p></li>
				<li>Define a function that will produce a caption. It will start by feeding the <strong class="source-inline">beginsequence</strong> token to the network, which will iteratively construct the sentence until the maximum sequence length is reached, or the <strong class="source-inline">endsequence</strong> token is encountered:<p class="source-code">def produce_caption(model,</p><p class="source-code">                    tokenizer,</p><p class="source-code">                    image,</p><p class="source-code">                    max_sequence_length):</p><p class="source-code">    text = 'beginsequence'</p><p class="source-code">    for _ in range(max_sequence_length):</p><p class="source-code">       sequence = tokenizer.texts_to_sequences([text])[0]</p><p class="source-code">        sequence = pad_sequences([sequence],</p><p class="source-code">               maxlen=max_sequence_length)</p><p class="source-code">        prediction = model.predict([[image], sequence])</p><p class="source-code">        index = np.argmax(prediction)</p><p class="source-code">        word = get_word_from_index(tokenizer, index)</p><p class="source-code">        if word is None:</p><p class="source-code">            break</p><p class="source-code">        text += f' {word}'</p><p class="source-code">        if word == 'endsequence':</p><p class="source-code">            break</p><p class="source-code">    return text</p></li>
				<li>Define a<a id="_idIndexMarker626"/> function that will evaluate the model's performance. First, we'll produce a caption for each feature corresponding to an image in the test dataset:<p class="source-code">def evaluate_model(model, features, captions, </p><p class="source-code">                     tokenizer,</p><p class="source-code">                   max_seq_length):</p><p class="source-code">    actual = []</p><p class="source-code">    predicted = []</p><p class="source-code">    for feature, caption in zip(features, captions):</p><p class="source-code">        generated_caption = produce_caption(model,</p><p class="source-code">                                            tokenizer,</p><p class="source-code">                                            feature,</p><p class="source-code">                                     max_seq_length)</p><p class="source-code">        actual.append([caption.split(' ')])</p><p class="source-code">        predicted.append(generated_caption.split(' '))</p></li>
				<li>Next, we'll <a id="_idIndexMarker627"/>compute the <strong class="bold">BLEU</strong> score using different weights. Although the <strong class="bold">BLEU</strong> score is outside the scope of this recipe, you can find an excellent article that explains it in depth in the <em class="italic">See also</em> section. All you need to know is that it's used to measure how well a generated caption compares to a set of reference captions:<p class="source-code">    for index, weights in enumerate([(1, 0, 0, 0),</p><p class="source-code">                                     (.5, .5, 0, 0),</p><p class="source-code">                                     (.3, .3, .3, 0),</p><p class="source-code">                                     (.25, .25, .25, </p><p class="source-code">                                        .25)],</p><p class="source-code">                                    start=1):</p><p class="source-code">        b_score = corpus_bleu(actual, predicted, weights)</p><p class="source-code">        print(f'BLEU-{index}: {b_score}')</p></li>
				<li>Load the image paths and captions:<p class="source-code">image_paths, all_captions = load_paths_and_captions()</p></li>
				<li>Create the image extractor model:<p class="source-code">extractor_model = VGG16(weights='imagenet')</p><p class="source-code">inputs = extractor_model.inputs</p><p class="source-code">outputs = extractor_model.layers[-2].output</p><p class="source-code">extractor_model = Model(inputs=inputs, outputs=outputs)</p></li>
				<li>Create the image caption feature extractor (passing the regular image extractor we <a id="_idIndexMarker628"/>created in <em class="italic">Step 15</em>) and use it to extract the sequences from the data:<p class="source-code">extractor = ImageCaptionFeatureExtractor(</p><p class="source-code">    feature_extractor=extractor_model,</p><p class="source-code">    output_path=OUTPUT_PATH)</p><p class="source-code">extractor.extract_features(image_paths, all_captions)</p></li>
				<li>Load the pickled input and output sequences we created in <em class="italic">Step 16</em>:<p class="source-code">pickled_data = []</p><p class="source-code">for p in [f'{OUTPUT_PATH}/input_features.pickle',</p><p class="source-code">          f'{OUTPUT_PATH}/input_sequences.pickle',</p><p class="source-code">          f'{OUTPUT_PATH}/output_sequences.pickle']:</p><p class="source-code">    with open(p, 'rb') as f:</p><p class="source-code">        pickled_data.append(pickle.load(f))</p><p class="source-code">input_feats, input_seqs, output_seqs = pickled_data</p></li>
				<li>Use 80% of the data for training and 20% for testing:<p class="source-code">(train_input_feats, test_input_feats,</p><p class="source-code"> train_input_seqs, test_input_seqs,</p><p class="source-code"> train_output_seqs,</p><p class="source-code"> test_output_seqs) = train_test_split(input_feats,</p><p class="source-code">                                      input_seqs,</p><p class="source-code">                                      output_seqs,</p><p class="source-code">                                      train_size=0.8,</p><p class="source-code">                                      random_state=9)</p></li>
				<li>Instantiate and compile the model. Because, in the end, this is a multi-class classification <a id="_idIndexMarker629"/>problem, we'll use <strong class="source-inline">categorical_crossentropy</strong> as our loss function:<p class="source-code">vocabulary_size = len(extractor.tokenizer.word_index) + 1</p><p class="source-code">model = build_network(vocabulary_size,</p><p class="source-code">                      extractor.max_seq_length)</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer='adam')</p></li>
				<li>Because the training process is so resource-intensive and the network tends to give the best results early on, let's create a <strong class="source-inline">ModelCheckpoint</strong> callback that will store the model with the lowest validation loss:<p class="source-code">checkpoint_path = ('model-ep{epoch:03d}-</p><p class="source-code">                     loss{loss:.3f}-'</p><p class="source-code">                   'val_loss{val_loss:.3f}.h5')</p><p class="source-code">checkpoint = ModelCheckpoint(checkpoint_path,</p><p class="source-code">                             monitor='val_loss',</p><p class="source-code">                             verbose=1,</p><p class="source-code">                             save_best_only=True,</p><p class="source-code">                             mode='min')</p></li>
				<li>Fit the model over 30 epochs. Notice that we must pass two set of inputs or features, but only a set of labels:<p class="source-code">EPOCHS = 30</p><p class="source-code">model.fit(x=[train_input_feats, train_input_seqs],</p><p class="source-code">          y=train_output_seqs,</p><p class="source-code">          epochs=EPOCHS,</p><p class="source-code">          callbacks=[checkpoint],</p><p class="source-code">          validation_data=([test_input_feats,test_input_</p><p class="source-code">                                                 seqs],</p><p class="source-code">                                       test_output_seqs))</p></li>
				<li>Load the best<a id="_idIndexMarker630"/> model. This may vary from run to run, but in this recipe, it's stored in the <strong class="source-inline">model-ep003-loss3.847-val_loss4.328.h5</strong> file:<p class="source-code">model = load_model('model-ep003-loss3.847-</p><p class="source-code">                   val_loss4.328.h5')</p></li>
				<li>Load the data mapping, which contains all the features paired with the ground truth captions. Extract the features and mappings into separate collections:<p class="source-code">with open(f'{OUTPUT_PATH}/data_mapping.pickle', 'rb') as f:</p><p class="source-code">    data_mapping = pickle.load(f)</p><p class="source-code">feats = [v['features'] for v in data_mapping.values()]</p><p class="source-code">captions = [v['caption'] for v in data_mapping.values()]</p></li>
				<li>Evaluate the model:<p class="source-code">evaluate_model(model,</p><p class="source-code">               features=feats,</p><p class="source-code">               captions=captions,</p><p class="source-code">               tokenizer=extractor.tokenizer,</p><p class="source-code">               max_seq_length=extractor.max_seq_length)</p><p>This step might take a while. In the end, you'll see an output similar to this:</p><p class="source-code">BLEU-1: 0.35674398077995173</p><p class="source-code">BLEU-2: 0.17030332240763874</p><p class="source-code">BLEU-3: 0.12170338107914261</p><p class="source-code">BLEU-4: 0.05493477725774873</p></li>
			</ol>
			<p>Training an image captioner is not an easy task. However, by executing the proper steps, in the correct <a id="_idIndexMarker631"/>order, we were able to create a fairly capable one that performed well on the test set, based on the <strong class="bold">BLEU</strong> score shown in the preceding code block. Head over to the next section to see how it all works!</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor260"/>How it works…</h2>
			<p>In this recipe, we implemented an image captioning network from scratch. Although this might seem complicated at first, we must remember it is a variation of an encoder-decoder architecture, similar to the ones we studied in <a href="B14768_05_Final_JM_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 5</em></a>, <em class="italic">Reducing Noise with Autoencoders</em>, and <a href="B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214"><em class="italic">Chapter 6</em></a>, <em class="italic">Generative Models and Adversarial Attacks</em>. </p>
			<p>In this case, the encoder is just a fully connected and shallow network that maps the features we extracted from the pre-trained model on ImageNet, to a vector of 256 elements. </p>
			<p>On the other hand, the decoder, instead of using transposed convolutions, uses an <strong class="bold">RNN</strong> that receives both text sequences (mapped to numeric vectors) and image features, concatenated into a long sequence of 512 elements. </p>
			<p>The network is trained so that it learns to predict the next word in a sentence, given all the words it generated in previous time steps. Note that in each cycle, we pass the same feature vector that corresponds to the image, so the network learns to map certain words, in a particular order, to describe the visual data encoded in such a vector.</p>
			<p>The output of the network is one-hot encoded, which means that only the position its corresponding to the words the network believes should come next in the sentence contains a 1, while the remaining positions contain a 0.</p>
			<p>To generate captions, we follow a similar process. Of course, we somehow need to tell the model to start producing words. With this in mind, we pass the <strong class="source-inline">beginsequence</strong> token to the network and iterate until we reach the maximum sequence length, or the model outputs an <strong class="source-inline">endsequence</strong> token. Remember, we take the output of each iteration and <a id="_idIndexMarker632"/> use it as input for the next cycle.</p>
			<p>This might seem confusing and cumbersome at first, but you now have the building blocks you need to tackle any image captioning problem!</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor261"/>See also</h2>
			<p>Here's an excellent<a id="_idIndexMarker633"/> read if you wish to fully understand the <strong class="bold">BLEU</strong> score: <a href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/">https://machinelearningmastery.com/calculate-bleu-score-for-text-python/</a>.</p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor262"/>Generating captions for your own photos</h1>
			<p>Training a<a id="_idIndexMarker634"/> good image captioning system is only one part of the equation. To actually use it, we must perform a series of steps, akin to the ones we executed during the training phase. </p>
			<p>In this recipe, we'll use a trained image captioning network to produce textual descriptions of new images. </p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor263"/>Getting ready</h2>
			<p>Although we don't need external dependencies for this particular recipe, we need access to a trained image captioning network, along with the cleaned captions that will be used to fit it. I highly recommend that you complete the <em class="italic">Implementing a reusable image caption feature extractor</em> and <em class="italic">Implementing an image captioning network</em> recipes before tackling this one.</p>
			<p>Are you ready? Let's start captioning!</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor264"/>How to do it…</h2>
			<p>Follow this series of steps to produce captions for your own images:</p>
			<ol>
				<li value="1">As usual, let's begin by importing the necessary dependencies:<p class="source-code">import glob</p><p class="source-code">import pickle</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow.keras.applications.vgg16 import *</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.preprocessing.sequence import \</p><p class="source-code">    pad_sequences</p><p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p><p class="source-code">from ch7.recipe1.extractor import ImageCaptionFeatureExtractor</p></li>
				<li>Define a<a id="_idIndexMarker635"/> function that will translate an integer index into the corresponding word using the tokenizer's mapping:<p class="source-code">def get_word_from_index(tokenizer, index):</p><p class="source-code">    return tokenizer.index_word.get(index, None)</p></li>
				<li>Define the <strong class="source-inline">produce_caption()</strong> function, which takes the captioning model, the tokenizer, an image to describe, and the maximum sequence length to generate a textual description of the input visual scene:<p class="source-code">def produce_caption(model,</p><p class="source-code">                    tokenizer,</p><p class="source-code">                    image,</p><p class="source-code">                    max_sequence_length):</p><p class="source-code">    text = 'beginsequence'</p><p class="source-code">    for _ in range(max_sequence_length):</p><p class="source-code">       sequence = tokenizer.texts_to_sequences([text])[0]</p><p class="source-code">       sequence = pad_sequences([sequence],</p><p class="source-code">                             maxlen=max_sequence_length)</p><p class="source-code">        prediction = model.predict([[image], sequence])</p><p class="source-code">        index = np.argmax(prediction)</p><p class="source-code">        word = get_word_from_index(tokenizer, index)</p><p class="source-code">        if word is None:</p><p class="source-code">            break</p><p class="source-code">        text += f' {word}'</p><p class="source-code">        if word == 'endsequence':</p><p class="source-code">            break</p><p class="source-code">    return text</p><p>Note that we must keep generating words until we either encounter the <strong class="source-inline">endsequence</strong> token or we reach the maximum sequence length.</p></li>
				<li>Define a pre-trained <strong class="bold">VGG16</strong> network, which we'll use as our image feature extractor:<p class="source-code">extractor_model = VGG16(weights='imagenet')</p><p class="source-code">inputs = extractor_model.inputs</p><p class="source-code">outputs = extractor_model.layers[-2].output</p><p class="source-code">extractor_model = Model(inputs=inputs, outputs=outputs)</p></li>
				<li>Pass the image<a id="_idIndexMarker636"/> extractor to an instance of <strong class="source-inline">ImageCaptionFeatureExtractor()</strong>:<p class="source-code">extractor = ImageCaptionFeatureExtractor(</p><p class="source-code">    feature_extractor=extractor_model)</p></li>
				<li>Load the cleaned captions we used to train the model. We need them to fit the tokenizer in <em class="italic">Step 7</em>:<p class="source-code">with open('data_mapping.pickle', 'rb') as f:</p><p class="source-code">    data_mapping = pickle.load(f)</p><p class="source-code">captions = [v['caption'] for v in </p><p class="source-code">            data_mapping.values()]</p></li>
				<li>Instantiate a <strong class="source-inline">Tokenizer()</strong> and fit it to all the captions. Also, compute the maximum sequence length:<p class="source-code">tokenizer = Tokenizer()</p><p class="source-code">tokenizer.fit_on_texts(captions)</p><p class="source-code">max_seq_length = extractor._get_max_seq_length(captions)</p></li>
				<li>Load the trained network (in this case, the name of the network is <strong class="source-inline">model-ep003-loss3.847-val_loss4.328.h5</strong>):<p class="source-code">model = load_model('model-ep003-loss3.847-</p><p class="source-code">                     val_loss4.328.h5')</p></li>
				<li>Iterate over all the test images in the current location, extracting the corresponding numeric features:<p class="source-code">for idx, image_path in enumerate(glob.glob('*.jpg'), </p><p class="source-code">                                   start=1):</p><p class="source-code">    img_feats = (extractor</p><p class="source-code">                 .extract_image_features(image_path))</p></li>
				<li>Produce the<a id="_idIndexMarker637"/> caption and remove the <strong class="source-inline">beginsequence</strong> and <strong class="source-inline">endsequence</strong> special tokens:<p class="source-code">    description = produce_caption(model,</p><p class="source-code">                                  tokenizer,</p><p class="source-code">                                  img_feats,</p><p class="source-code">                                  max_seq_length)</p><p class="source-code">    description = (description</p><p class="source-code">                   .replace('beginsequence', '')</p><p class="source-code">                   .replace('endsequence', ''))</p></li>
				<li>Open the image, add the generated caption as its title, and save it:<p class="source-code">    image = plt.imread(image_path)</p><p class="source-code">    plt.imshow(image)</p><p class="source-code">    plt.title(description)</p><p class="source-code">    plt.savefig(f'{idx}.jpg')</p><p>Here's an image where the network does a very good job of generating a proper caption:</p></li>
			</ol>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B14768_07_003.jpg" alt="Figure 7.3 – We can see that the caption is very close to what's actually happening&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – We can see that the caption is very close to what's actually happening</p>
			<p>Here's another <a id="_idIndexMarker638"/>example where the network is technically correct, although it could be more precise:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B14768_07_004.jpg" alt="Figure 7.4 – A football player in a red uniform is, indeed, in the air, but there is more going on&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – A football player in a red uniform is, indeed, in the air, but there is more going on</p>
			<p>Finally, here's an instance where the network is clueless:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B14768_07_005.jpg" alt="Figure 7.5 – The network couldn't describe this scene&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – The network couldn't describe this scene</p>
			<p>With that, we've <a id="_idIndexMarker639"/>seen that our model does well on some images, but still has room for improvement. We'll dive a bit deeper in the next section.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor265"/>How it works…</h2>
			<p>In this recipe, we learned that image captioning is a difficult problem that heavily depends on many factors. Some of these factors are as follows: </p>
			<ul>
				<li>A well-trained <strong class="bold">CNN</strong> to extract high-quality visual features</li>
				<li>A rich set of descriptive captions for each image</li>
				<li>Embeddings with enough capacity to encode the expressiveness of the vocabulary with minimal loss</li>
				<li>A powerful <strong class="bold">RNN</strong> to learn how to put all of this together</li>
			</ul>
			<p>Despite these clear challenges, in this recipe, we used a trained network on the <strong class="source-inline">Flickr8k</strong> dataset to generate captions for new images. The process we followed is similar to the one we implemented to train the system in that, first, we must go from an image to a feature vector. Then, we must fit a tokenizer to our vocabulary to get a proper mechanism so that we can go from sequences to human-readable words. Finally, we assemble the captions one word at a time, passing the image features along with the sequence we've built so far. How do we know when to stop, though? We have two stopping criteria:</p>
			<ul>
				<li>The caption reached the maximum sequence length.</li>
				<li>The network encountered the <strong class="source-inline">endsequence</strong> token.</li>
			</ul>
			<p>Lastly, we tested<a id="_idIndexMarker640"/> our solution on several images, with varied results. In some instances, the network is capable of producing very precise descriptions, while on other occasions, it generates somewhat vague captions. It also missed the mark completely in the last example, which is a clear indication of how much room for improvement there is.</p>
			<p>If you want to take a look at other captioned images, consult the official repository: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7/recipe3">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch7/recipe3</a>.</p>
			<h1 id="_idParaDest-226">Implementing an image caption<a id="_idTextAnchor266"/><a id="_idTextAnchor267"/>ing network on COCO with attention</h1>
			<p>A great<a id="_idIndexMarker641"/> way to understand <a id="_idIndexMarker642"/>how <a id="_idIndexMarker643"/>an image captioning network generates its descriptions is by adding an attention component to the architecture. This lets us appreciate what parts of the photo a network was looking at when it generated each word. </p>
			<p>In this recipe, we'll train an end-to-end image captioning system on the more challenging <strong class="bold">Common Objects in Context</strong> (<strong class="bold">COCO</strong>) dataset. We'll also equip our network with<a id="_idIndexMarker644"/> an attention mechanism to improve its performance and to help us understand its inner reasoning.</p>
			<p>This is a long and advanced recipe, but don't panic! We'll go step by step. If you want to dive deeper into the theory that supports this implementation, take a look at the <em class="italic">See also</em> section.</p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor268"/>Getting ready</h2>
			<p>Although we'll be using the <strong class="source-inline">COCO</strong> dataset, you don't need to do anything beforehand, because we'll download it as <a id="_idIndexMarker645"/>part of the recipe (however, you can read more about this seminal<a id="_idIndexMarker646"/> dataset<a id="_idIndexMarker647"/> here: https://cocodataset.org/#home).</p>
			<p>The following is a sample from the <strong class="source-inline">COCO</strong> dataset:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B14768_07_006.jpg" alt="Figure 7.6 – Sample images from COCO&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Sample images from COCO</p>
			<p>Let's get to work!</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor269"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import all the necessary dependencies:<p class="source-code">import json</p><p class="source-code">import os</p><p class="source-code">import time</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.utils import shuffle</p><p class="source-code">from tensorflow.keras.applications.inception_v3 import *</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import \</p><p class="source-code">    SparseCategoricalCrossentropy</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">from tensorflow.keras.preprocessing.sequence import \</p><p class="source-code">    pad_sequences</p><p class="source-code">from tensorflow.keras.preprocessing.text import Tokenizer</p><p class="source-code">from tensorflow.keras.utils import get_file</p></li>
				<li>Define<a id="_idIndexMarker648"/> an <a id="_idIndexMarker649"/>alias<a id="_idIndexMarker650"/> for <strong class="source-inline">tf.data.experimental.AUTOTUNE</strong>:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p></li>
				<li>Define a function that will load an image. It must return both the image and its path:<p class="source-code">def load_image(image_path):</p><p class="source-code">    image = tf.io.read_file(image_path)</p><p class="source-code">    image = tf.image.decode_jpeg(image, channels=3)</p><p class="source-code">    image = tf.image.resize(image, (299, 299))</p><p class="source-code">    image = preprocess_input(image)</p><p class="source-code">    return image, image_path</p></li>
				<li>Define a function that will get the maximum sequence length. This will be useful later on:<p class="source-code">def get_max_length(tensor):</p><p class="source-code">    return max(len(t) for t in tensor)</p></li>
				<li>Define for the image captioning network a <a id="_idIndexMarker651"/>function <a id="_idIndexMarker652"/>that will load an image from disk (stored in <strong class="source-inline">NumPy</strong> format):<p class="source-code">def load_image_and_caption(image_name, caption):</p><p class="source-code">    image_name = image_name.decode('utf-8').split('/')</p><p class="source-code">                                      [-1]</p><p class="source-code">    image_tensor = np.load(f'./{image_name}.npy')</p><p class="source-code">    return image_tensor, caption</p></li>
				<li>Implement <strong class="bold">Bahdanau's Attention</strong> using <a id="_idIndexMarker653"/>model subclassing: <p class="source-code">class BahdanauAttention(Model):</p><p class="source-code">    def __init__(self, units):</p><p class="source-code">        super(BahdanauAttention, self).__init__()</p><p class="source-code">        self.W1 = Dense(units)</p><p class="source-code">        self.W2 = Dense(units)</p><p class="source-code">        self.V = Dense(1)</p></li>
				<li>The previous block defined the network layers. Now, let's define the forward pass inside the <strong class="source-inline">call()</strong> method:<p class="source-code">    def call(self, features, hidden):</p><p class="source-code">        hidden_with_time_axis = tf.expand_dims(hidden, </p><p class="source-code">                                                 1)</p><p class="source-code">        score = tf.nn.tanh(self.W1(features) +</p><p class="source-code">                        self.W2(hidden_with_time_axis))</p><p class="source-code">        attention_w = tf.nn.softmax(self.V(score), </p><p class="source-code">                                     axis=1)</p><p class="source-code">        ctx_vector = attention_w * features</p><p class="source-code">        ctx_vector = tf.reduce_sum(ctx_vector, axis=1)</p><p class="source-code">        return ctx_vector, attention_w</p></li>
				<li>Define<a id="_idIndexMarker654"/> the<a id="_idIndexMarker655"/> image <a id="_idIndexMarker656"/>encoder. This is just a <strong class="bold">CNN</strong> that receives a feature vector and passes it through a dense layer, which is then activated with <strong class="source-inline">ReLU</strong>:<p class="source-code">class CNNEncoder(Model):</p><p class="source-code">    def __init__(self, embedding_dim):</p><p class="source-code">        super(CNNEncoder, self).__init__()</p><p class="source-code">        self.fc = Dense(embedding_dim)</p><p class="source-code">    def call(self, x):</p><p class="source-code">        x = self.fc(x)</p><p class="source-code">        x = tf.nn.relu(x)</p><p class="source-code">        return x</p></li>
				<li>Define the <a id="_idIndexMarker657"/>decoder. This is an <strong class="bold">RNN</strong> that uses <strong class="source-inline">GRU</strong> and attention to learn how to produce<a id="_idIndexMarker658"/> captions<a id="_idIndexMarker659"/> from the visual feature vectors and the text input sequences:<p class="source-code">class RNNDecoder(Model):</p><p class="source-code">    def __init__(self, embedding_size, units, </p><p class="source-code">                     vocab_size):</p><p class="source-code">        super(RNNDecoder, self).__init__()</p><p class="source-code">        self.units = units</p><p class="source-code">        self.embedding = Embedding(vocab_size, </p><p class="source-code">                                    embedding_size)</p><p class="source-code">        self.gru = GRU(self.units,</p><p class="source-code">                       return_sequences=True,</p><p class="source-code">                       return_state=True,</p><p class="source-code">                       recurrent_initializer='glorot_</p><p class="source-code">                       uniform')</p><p class="source-code">        self.fc1 = Dense(self.units)</p><p class="source-code">        self.fc2 = Dense(vocab_size)</p><p class="source-code">        self.attention = BahdanauAttention(self.units)</p></li>
				<li>Now that we've defined the layers in the <strong class="bold">RNN</strong> architecture, let's implement the forward pass. First, we must pass the inputs through the attention sub-network: <p class="source-code">    def call(self, x, features, hidden):</p><p class="source-code">        context_vector, attention_weights = \</p><p class="source-code">            self.attention(features, hidden)</p></li>
				<li>Then, we must pass the input sequence (<strong class="source-inline">x</strong>) through the embedding layer and concatenate it with the context vector we received from the attention mechanism:<p class="source-code">        x = self.embedding(x)</p><p class="source-code">        expanded_context = tf.expand_dims(context_vector, </p><p class="source-code">                                           1)</p><p class="source-code">        x = Concatenate(axis=-1)([expanded_context, x])</p></li>
				<li>Next, we <a id="_idIndexMarker660"/>must<a id="_idIndexMarker661"/> pass the merged tensor to the <strong class="source-inline">GRU</strong> layer, and then <a id="_idIndexMarker662"/>through the dense layers. This returns the output sequence, the state, and the attention weights:<p class="source-code">        output, state = self.gru(x)</p><p class="source-code">        x = self.fc1(output)</p><p class="source-code">        x = tf.reshape(x, (-1, x.shape[2]))</p><p class="source-code">        x = self.fc2(x)</p></li>
				<li>Finally, we must define a method that will reset the hidden state:<p class="source-code">    def reset_state(self, batch_size):</p><p class="source-code">        return tf.zeros((batch_size, self.units))</p></li>
				<li>Define <strong class="source-inline">ImageCaptionerClass</strong>. The constructor instantiates the basic components, which are the encoder, the decoder, the tokenizer, and the optimizer and loss functions needed to train the whole system:<p class="source-code">class ImageCaptioner(object):</p><p class="source-code">    def __init__(self, embedding_size, units, </p><p class="source-code">                 vocab_size,</p><p class="source-code">                 tokenizer):</p><p class="source-code">        self.tokenizer = tokenizer</p><p class="source-code">        self.encoder = CNNEncoder(embedding_size)</p><p class="source-code">        self.decoder = RNNDecoder(embedding_size, </p><p class="source-code">                                    units,</p><p class="source-code">                                  vocab_size)</p><p class="source-code">        self.optimizer = Adam()</p><p class="source-code">        self.loss = SparseCategoricalCrossentropy(</p><p class="source-code">            from_logits=True,</p><p class="source-code">            reduction='none')</p></li>
				<li>Create <a id="_idIndexMarker663"/>a <a id="_idIndexMarker664"/>method<a id="_idIndexMarker665"/> that will compute the loss function:<p class="source-code">    def loss_function(self, real, predicted):</p><p class="source-code">        mask = tf.math.logical_not(tf.math.equal(real, </p><p class="source-code">                                               0))</p><p class="source-code">        _loss = self.loss(real, predicted)</p><p class="source-code">        mask = tf.cast(mask, dtype=_loss.dtype)</p><p class="source-code">        _loss *= mask</p><p class="source-code">        return tf.reduce_mean(_loss)</p></li>
				<li>Next, define a function that will perform a single training step. We will start by creating the hidden state and the input, which is just a batch of singleton sequences containing the index of the <strong class="source-inline">&lt;start&gt;</strong> token, a special element used to <a id="_idIndexMarker666"/>signal<a id="_idIndexMarker667"/> the <a id="_idIndexMarker668"/>beginning of a sentence:<p class="source-code">    @tf.function</p><p class="source-code">    def train_step(self, image_tensor, target):</p><p class="source-code">        loss = 0</p><p class="source-code">        hidden = </p><p class="source-code">       self.decoder.reset_state(target.shape[0])</p><p class="source-code">        start_token_idx = </p><p class="source-code">       self.tokenizer.word_index['&lt;start&gt;']</p><p class="source-code">        init_batch = [start_token_idx] * </p><p class="source-code">        target.shape[0]</p><p class="source-code">        decoder_input = tf.expand_dims(init_batch, 1)</p></li>
				<li>Now, we must encode the image tensor. Then, we'll iteratively pass the resulting features to the decoder, along with the outputted sequence so far, and the hidden state. For a deeper explanation on how <strong class="bold">RNNs</strong> work, head to the <em class="italic">See also</em> section:<p class="source-code">      with tf.GradientTape() as tape:</p><p class="source-code">            features = self.encoder(image_tensor)</p><p class="source-code">            for i in range(1, target.shape[1]):</p><p class="source-code">                preds, hidden, _ = </p><p class="source-code">                self.decoder(decoder_input,</p><p class="source-code">                            features,</p><p class="source-code">                             hidden)</p><p class="source-code">                loss += self.loss_function(target[:, i],</p><p class="source-code">                                           preds)</p><p class="source-code">                decoder_input = </p><p class="source-code">                       tf.expand_dims(target[:, i],1)</p></li>
				<li>Notice in the previous block that we computed the loss at each time step. To get the<a id="_idIndexMarker669"/> total <a id="_idIndexMarker670"/>loss, we <a id="_idIndexMarker671"/>must calculate the average. For the network to actually learn, we must backpropagate the total loss by computing the gradients and applying them via the optimizer:<p class="source-code">        total_loss = loss / int(target.shape[1])</p><p class="source-code">        trainable_vars = (self.encoder.trainable_</p><p class="source-code">                            variables +</p><p class="source-code">                          self.decoder.trainable_</p><p class="source-code">                             variables)</p><p class="source-code">        gradients = tape.gradient(loss, trainable_vars)</p><p class="source-code">        self.optimizer.apply_gradients(zip(gradients,</p><p class="source-code">                                      trainable_vars))</p><p class="source-code">        return loss, total_loss</p></li>
				<li>The last method in this class is in charge of training the system:<p class="source-code">    def train(self, dataset, epochs, num_steps):</p><p class="source-code">        for epoch in range(epochs):</p><p class="source-code">            start = time.time()</p><p class="source-code">            total_loss = 0</p><p class="source-code">            for batch, (image_tensor, target) \</p><p class="source-code">                    in enumerate(dataset):</p><p class="source-code">                batch_loss, step_loss = \</p><p class="source-code">                    self.train_step(image_tensor, target)</p><p class="source-code">                total_loss += step_loss</p></li>
				<li>Every <a id="_idIndexMarker672"/>100 <a id="_idIndexMarker673"/>epochs, we'll <a id="_idIndexMarker674"/>print the loss. At the end of each epoch, we will also print the epoch loss and elapsed time:<p class="source-code">                if batch % 100 == 0:</p><p class="source-code">                    loss = batch_loss.numpy()</p><p class="source-code">                    loss = loss / int(target.shape[1])</p><p class="source-code">                    print(f'Epoch {epoch + 1}, batch </p><p class="source-code">                                             {batch},'</p><p class="source-code">                          f' loss {loss:.4f}')</p><p class="source-code">            print(f'Epoch {epoch + 1},'</p><p class="source-code">                  f' loss {total_loss / </p><p class="source-code">                           num_steps:.6f}')</p><p class="source-code">            epoch_time = time.time() - start</p><p class="source-code">            print(f'Time taken: {epoch_time} seconds. </p><p class="source-code">                   \n')</p></li>
				<li>Download and<a id="_idIndexMarker675"/> unzip the <strong class="source-inline">COCO</strong> dataset's annotation files. If they're already in the<a id="_idIndexMarker676"/> system, just<a id="_idIndexMarker677"/> store the file path:<p class="source-code">INPUT_DIR = os.path.abspath('.')</p><p class="source-code">annots_folder = '/annotations/'</p><p class="source-code">if not os.path.exists(INPUT_DIR + annots_folder):</p><p class="source-code">    origin_url = ('http://images.cocodataset.org/</p><p class="source-code">            annotations''/annotations_trainval2014.zip')</p><p class="source-code">    cache_subdir = os.path.abspath('.')</p><p class="source-code">    annots_zip = get_file('all_captions.zip',</p><p class="source-code">                          cache_subdir=cache_subdir,</p><p class="source-code">                          origin=origin_url,</p><p class="source-code">                          extract=True)</p><p class="source-code">    annots_file = (os.path.dirname(annots_zip) +</p><p class="source-code">                  '/annotations/captions_train2014.json')</p><p class="source-code">    os.remove(annots_zip)</p><p class="source-code">else:</p><p class="source-code">    annots_file = (INPUT_DIR +</p><p class="source-code">                  '/annotations/captions_train2014.json')</p></li>
				<li>Download <a id="_idIndexMarker678"/>and <a id="_idIndexMarker679"/>unzip <a id="_idIndexMarker680"/>the <strong class="source-inline">COCO</strong> dataset's image files. If they're already in the system, just store the file path:<p class="source-code">image_folder = '/train2014/'</p><p class="source-code">if not os.path.exists(INPUT_DIR + image_folder):</p><p class="source-code">    origin_url = ('http://images.cocodataset.org/zips/'</p><p class="source-code">                  'train2014.zip')</p><p class="source-code">    cache_subdir = os.path.abspath('.')</p><p class="source-code">    image_zip = get_file('train2014.zip',</p><p class="source-code">                         cache_subdir=cache_subdir,</p><p class="source-code">                         origin=origin_url,</p><p class="source-code">                         extract=True)</p><p class="source-code">    PATH = os.path.dirname(image_zip) + image_folder</p><p class="source-code">    os.remove(image_zip)</p><p class="source-code">else:</p><p class="source-code">    PATH = INPUT_DIR + image_folder</p></li>
				<li>Load<a id="_idIndexMarker681"/> the <a id="_idIndexMarker682"/>image<a id="_idIndexMarker683"/> paths and the captions. We must add the special <strong class="source-inline">&lt;start&gt;</strong> and <strong class="source-inline">&lt;end&gt;</strong> tokens to each caption so that they're in our vocabulary. These special tokens let us specify where a sequence begins and ends, respectively:<p class="source-code">with open(annots_file, 'r') as f:</p><p class="source-code">    annotations = json.load(f)</p><p class="source-code">captions = []</p><p class="source-code">image_paths = []</p><p class="source-code">for annotation in annotations['annotations']:</p><p class="source-code">    caption = '&lt;start&gt;' + annotation['caption'] + ' &lt;end&gt;'</p><p class="source-code">    image_id = annotation['image_id']</p><p class="source-code">    image_path = f'{PATH}COCO_train2014_{image_id:012d}.jpg'</p><p class="source-code">    image_paths.append(image_path)</p><p class="source-code">    captions.append(caption)</p></li>
				<li>Because <strong class="source-inline">COCO</strong> is massive, and it would take ages to train a model on it, we'll select <a id="_idIndexMarker684"/> a<a id="_idIndexMarker685"/> random<a id="_idIndexMarker686"/> sample of 30,000 images, along with their captions:<p class="source-code">train_captions, train_image_paths = shuffle(captions,</p><p class="source-code">                                        image_paths, </p><p class="source-code">                                      random_state=42)</p><p class="source-code">SAMPLE_SIZE = 30000</p><p class="source-code">train_captions = train_captions[:SAMPLE_SIZE]</p><p class="source-code">train_image_paths = train_image_paths[:SAMPLE_SIZE]</p><p class="source-code">train_images = sorted(set(train_image_paths))</p></li>
				<li>Let's use a pre-trained instance of <strong class="source-inline">InceptionV3</strong> as our image feature extractor:<p class="source-code">feature_extractor = InceptionV3(include_top=False,</p><p class="source-code">                                weights='imagenet')</p><p class="source-code">feature_extractor = Model(feature_extractor.input,</p><p class="source-code">                          feature_extractor.layers[-</p><p class="source-code">                                        1].output)</p></li>
				<li>Create a <strong class="source-inline">tf.data.Dataset</strong> that maps image paths to tensors. Use it to go over <a id="_idIndexMarker687"/>all<a id="_idIndexMarker688"/> the images <a id="_idIndexMarker689"/>in our sample, convert them into feature vectors, and save them as <strong class="source-inline">NumPy</strong> arrays. This will allow us to save memory in the future:<p class="source-code">BATCH_SIZE = 8</p><p class="source-code">image_dataset = (tf.data.Dataset</p><p class="source-code">                 .from_tensor_slices(train_images)</p><p class="source-code">                 .map(load_image, </p><p class="source-code">                     num_parallel_calls=AUTOTUNE)</p><p class="source-code">                 .batch(BATCH_SIZE))</p><p class="source-code">for image, path in image_dataset:</p><p class="source-code">    batch_features = feature_extractor.predict(image)</p><p class="source-code">    batch_features = tf.reshape(batch_features,</p><p class="source-code">                             (batch_features.shape[0],</p><p class="source-code">                                 -1,</p><p class="source-code">                            batch_features.shape[3]))</p><p class="source-code">    for batch_feature, p in zip(batch_features, path):</p><p class="source-code">        feature_path = p.numpy().decode('UTF-8')</p><p class="source-code">        image_name = feature_path.split('/')[-1]</p><p class="source-code">        np.save(f'./{image_name}', batch_feature.numpy())</p></li>
				<li>Train a tokenizer on the top 5,000 words in our captions. Then, convert each text into <a id="_idIndexMarker690"/>a numeric sequence and pad them so that they are all the same size. Also, compute <a id="_idIndexMarker691"/>the <a id="_idIndexMarker692"/>maximum sequence length:<p class="source-code">top_k = 5000</p><p class="source-code">filters = '!”#$%&amp;()*+.,-/:;=?@[\]^_`{|}~ '</p><p class="source-code">tokenizer = Tokenizer(num_words=top_k,</p><p class="source-code">                      oov_token='&lt;unk&gt;',</p><p class="source-code">                      filters=filters)</p><p class="source-code">tokenizer.fit_on_texts(train_captions)</p><p class="source-code">tokenizer.word_index['&lt;pad&gt;'] = 0</p><p class="source-code">tokenizer.index_word[0] = '&lt;pad&gt;'</p><p class="source-code">train_seqs = tokenizer.texts_to_sequences(train_captions)</p><p class="source-code">captions_seqs = pad_sequences(train_seqs, </p><p class="source-code">                              padding='post')</p><p class="source-code">max_length = get_max_length(train_seqs)</p></li>
				<li>We'll <a id="_idIndexMarker693"/>use 20% of<a id="_idIndexMarker694"/> the<a id="_idIndexMarker695"/> data to test our model and the remaining 80% to train it:<p class="source-code">(images_train, images_val, caption_train, caption_val) = \</p><p class="source-code">    train_test_split(train_img_paths,</p><p class="source-code">                     captions_seqs,</p><p class="source-code">                     test_size=0.2,</p><p class="source-code">                     random_state=42)</p></li>
				<li>We'll load batches of 64 images (along with their captions) at a time. Notice that we're using the <strong class="source-inline">load_image_and_caption()</strong> function, defined in <em class="italic">Step 5</em>, which reads the feature vector corresponding to the images, stored in <strong class="source-inline">NumPy</strong> format. Moreover, because<a id="_idIndexMarker696"/> this function works at the <strong class="source-inline">NumPy</strong> level, we <a id="_idIndexMarker697"/>must wrap it <a id="_idIndexMarker698"/>with <strong class="source-inline">tf.numpy_function</strong> so that it can be used as a valid TensorFlow function within the <strong class="source-inline">map()</strong> method:<p class="source-code">BATCH_SIZE = 64</p><p class="source-code">BUFFER_SIZE = 1000</p><p class="source-code">dataset = (tf.data.Dataset</p><p class="source-code">           .from_tensor_slices((images_train, </p><p class="source-code">                                caption_train))</p><p class="source-code">           .map(lambda i1, i2:</p><p class="source-code">                tf.numpy_function(</p><p class="source-code">                    load_image_and_caption,</p><p class="source-code">                    [i1, i2],</p><p class="source-code">                    [tf.float32, tf.int32]),</p><p class="source-code">                num_parallel_calls=AUTOTUNE)</p><p class="source-code">           .shuffle(BUFFER_SIZE)</p><p class="source-code">           .batch(BATCH_SIZE)</p><p class="source-code">           .prefetch(buffer_size=AUTOTUNE))</p></li>
				<li>Let's instantiate an <strong class="source-inline">ImageCaptioner</strong>. The embeddings will have 256 elements, and the number of units for the decoder and the attention model will be 512. The vocabulary size is 5,001. Finally, we must pass the fitted tokenizer from <em class="italic">Step 27</em>:<p class="source-code">image_captioner = ImageCaptioner(embedding_size=256,</p><p class="source-code">                                 units=512,</p><p class="source-code">                                 vocab_size=top_k + 1,</p><p class="source-code">                                 tokenizer=tokenizer)</p><p class="source-code">EPOCHS = 30</p><p class="source-code">num_steps = len(images_train) // BATCH_SIZE</p><p class="source-code">image_captioner.train(dataset, EPOCHS, num_steps)</p></li>
				<li>Define a <a id="_idIndexMarker699"/>function<a id="_idIndexMarker700"/> that will evaluate the image captioner on an image. It<a id="_idIndexMarker701"/> must receive the encoder, the decoder, the tokenizer, the image to caption, the maximum sequence length, and the shape of the attention vector. We will start by creating a placeholder array, which is where we'll store the subplots that comprise the attention plot:<p class="source-code">def evaluate(encoder, decoder, tokenizer, image, </p><p class="source-code">              max_length,</p><p class="source-code">             attention_shape):</p><p class="source-code">    attention_plot = np.zeros((max_length,</p><p class="source-code">                               attention_shape))</p></li>
				<li>Next, we must initialize the hidden state, extract the features from the input image, and pass them to the encoder. We must also initialize the decoder input by creating a singleton sequence with the <strong class="source-inline">&lt;start&gt;</strong> token index:<p class="source-code">    hidden = decoder.reset_state(batch_size=1)</p><p class="source-code">    temp_input = tf.expand_dims(load_image(image)[0], </p><p class="source-code">                                    0)</p><p class="source-code">    image_tensor_val = feature_extractor(temp_input)</p><p class="source-code">    image_tensor_val = tf.reshape(image_tensor_val,</p><p class="source-code">                           (image_tensor_val.shape[0],</p><p class="source-code">                                   -1,</p><p class="source-code">                          image_tensor_val.shape[3]))</p><p class="source-code">    feats = encoder(image_tensor_val)</p><p class="source-code">    start_token_idx = tokenizer.word_index['&lt;start&gt;']</p><p class="source-code">    dec_input = tf.expand_dims([start_token_idx], 0)</p><p class="source-code">    result = []</p></li>
				<li>Now, let's<a id="_idIndexMarker702"/> build<a id="_idIndexMarker703"/> the <a id="_idIndexMarker704"/>caption until we reach the maximum sequence length or encounter the <strong class="source-inline">&lt;end&gt;</strong> token:<p class="source-code">    for i in range(max_length):</p><p class="source-code">        (preds, hidden, attention_w) = \</p><p class="source-code">            decoder(dec_input, feats, hidden)</p><p class="source-code">        attention_plot[i] = tf.reshape(attention_w,</p><p class="source-code">                                       (-1,)).numpy()</p><p class="source-code">        pred_id = tf.random.categorical(preds,</p><p class="source-code">                                     1)[0][0].numpy()</p><p class="source-code">        result.append(tokenizer.index_word[pred_id])</p><p class="source-code">        if tokenizer.index_word[pred_id] == '&lt;end&gt;':</p><p class="source-code">            return result, attention_plot</p><p class="source-code">        dec_input = tf.expand_dims([pred_id], 0)</p><p class="source-code">    attention_plot = attention_plot[:len(result), :]</p><p class="source-code">    return result, attention_plot</p></li>
				<li>Notice that <a id="_idIndexMarker705"/>for <a id="_idIndexMarker706"/>each word, we update <strong class="source-inline">attention_plot</strong> with<a id="_idIndexMarker707"/> the weights returned by the decoder.</li>
				<li>Let's define a function that will plot the attention the network pays to each word in the caption. It receives the image, a list of the individual words that comprise the caption (<strong class="source-inline">result</strong>), <strong class="source-inline">attention_plot</strong> returned by <strong class="source-inline">evaluate()</strong>, and the output path where we'll store the graph:<p class="source-code">def plot_attention(image, result,</p><p class="source-code">                   attention_plot, output_path):</p><p class="source-code">    tmp_image = np.array(load_image(image)[0])</p><p class="source-code">    fig = plt.figure(figsize=(10, 10))</p></li>
				<li>We'll iterate over each word to create a subplot of the corresponding attention graph, titled with the specific word it's linked to:<p class="source-code">    for l in range(len(result)):</p><p class="source-code">        temp_att = np.resize(attention_plot[l], (8, 8))</p><p class="source-code">        ax = fig.add_subplot(len(result) // 2,</p><p class="source-code">                             len(result) // 2,</p><p class="source-code">                             l + 1)</p><p class="source-code">        ax.set_title(result[l])</p><p class="source-code">        image = ax.imshow(tmp_image)</p><p class="source-code">        ax.imshow(temp_att,</p><p class="source-code">                  cmap='gray',</p><p class="source-code">                  alpha=0.6,</p><p class="source-code">                  extent=image.get_extent())</p></li>
				<li>Finally, we<a id="_idIndexMarker708"/> can<a id="_idIndexMarker709"/> save<a id="_idIndexMarker710"/> the full plot:<p class="source-code">    plt.tight_layout()</p><p class="source-code">    plt.show()</p><p class="source-code">    plt.savefig(output_path)</p></li>
				<li>Evaluate the network on a random image from the validation set:<p class="source-code">attention_features_shape = 64</p><p class="source-code">random_id = np.random.randint(0, len(images_val))</p><p class="source-code">image = images_val[random_id]</p></li>
				<li>Build and clean the actual (ground truth) caption:<p class="source-code">actual_caption = ' '.join([tokenizer.index_word[i]</p><p class="source-code">                         for i in caption_val[random_id]</p><p class="source-code">                           if i != 0])</p><p class="source-code">actual_caption = (actual_caption</p><p class="source-code">                  .replace('&lt;start&gt;', '')</p><p class="source-code">                  .replace('&lt;end&gt;', ''))</p></li>
				<li>Generate<a id="_idIndexMarker711"/> the<a id="_idIndexMarker712"/> caption <a id="_idIndexMarker713"/>for the validation image:<p class="source-code">result, attention_plot = evaluate(image_captioner               </p><p class="source-code">                                encoder,</p><p class="source-code">                   image_captioner.decoder,</p><p class="source-code">                                  tokenizer,</p><p class="source-code">                                  image,</p><p class="source-code">                                  max_length,</p><p class="source-code">                          attention_feats_shape)</p></li>
				<li>Build and clean the predicted caption:<p class="source-code">predicted_caption = (' '.join(result)</p><p class="source-code">                     .replace('&lt;start&gt;', '')</p><p class="source-code">                     .replace('&lt;end&gt;', '')) </p></li>
				<li>Print the ground truth and generated captions, and then save the attention plot to disk:<p class="source-code">print(f'Actual caption: {actual_caption}')</p><p class="source-code">print(f'Predicted caption: {predicted_caption}')</p><p class="source-code">output_path = './attention_plot.png'</p><p class="source-code">plot_attention(image, result, attention_plot, output_path)</p></li>
				<li>In the following code block, we can appreciate the similarity between the real caption <a id="_idIndexMarker714"/>and the<a id="_idIndexMarker715"/> one <a id="_idIndexMarker716"/>outputted by our model:<p class="source-code">Actual caption: a lone giraffe stands in the midst of a grassy area</p><p class="source-code">Predicted caption: giraffe standing in a dry grass near trees</p><p>Now, let's take a look at the attention plot:</p></li>
			</ol>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B14768_07_007.jpg" alt="Figure 7.7 – Attention plot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Attention plot</p>
			<p>Take note of the areas the network looked at when generating each word in the caption. Lighter squares mean that more attention was paid to those pixels. For instance, to produce the word <em class="italic">giraffe</em>, the network looked at the surroundings of the giraffe in the photo. Also, we can see that when the network generated the word <em class="italic">grass</em>, it looked at the giraffe legs, which have a grass portion behind them. Isn't that amazing?</p>
			<p>We'll look at this in more detail in the <em class="italic">How it works…</em> section.</p>
			<p>How it works…</p>
			<p>In this recipe, we implemented a more complete image captioning system, this time on the considerably<a id="_idIndexMarker717"/> more <a id="_idIndexMarker718"/>challenging <strong class="source-inline">COCO</strong> dataset, which is not only several orders of<a id="_idIndexMarker719"/> magnitude bigger than <strong class="source-inline">Flickr8k</strong>, but much more varied and, therefore, harder for the network to understand.</p>
			<p>Nevertheless, we gave our network an advantage by providing it with an attention mechanism, inspired by the impressive breakthrough proposed by Dzmitry Bahdanau (take a look at the <em class="italic">See also</em> section for more details). This capability gives the model the power to perform a soft search for parts of the source caption that are relevant to predicting a target word or simply put, producing the best next word in the output sentence. Such an attention mechanism works as an advantage over the traditional approach, which consists of using a fixed-length vector (as we did in the <em class="italic">Implementing an image captioning network</em> recipe) from which the decoder generates the output sentence. The problem with such a representation is that it tends to act as a bottleneck when it comes to improving performance.</p>
			<p>Also, the attention mechanism allows us to understand how the network thinks to produce captions in a more intuitive way. </p>
			<p>Because neural networks are complex pieces of software (often akin to a black box), using visual techniques to inspect their inner workings is a great tool at our disposal that can aid us in the training, fine-tuning, and optimization process.</p>
			<p>See also</p>
			<p>In this recipe, we implemented our architecture using the Model Subclassing pattern, which you can read more about here: <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">https://www.tensorflow.org/guide/keras/custom_layers_and_models</a>. </p>
			<p>Take a look at the following link for a great refresher on <strong class="bold">RNNs</strong>: <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg">https://www.youtube.com/watch?v=UNmqTiOnRfg</a>. </p>
			<p>Finally, I highly encourage you to read Dzmitry Bahdanau's paper about the attention mechanism we just implemented and used: <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.</p>
		</div>
	</body></html>