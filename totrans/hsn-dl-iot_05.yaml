- en: Image Recognition in IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many IoT applications, including smart homes, smart cities, and smart healthcare,
    will extensively use image recognition-based decision-making (such as facial recognition
    for a smart door or lock) in the future. **Machine learning** (**ML**) and **deep
    learning** (**DL**) algorithms are useful for image recognition and decision-making.
    Consequently, they are very promising for IoT applications. This chapter will
    cover hands-on DL-based image data processing for IoT applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of this chapter will briefly describe different IoT applications
    and their image detection-based decision-making. Furthermore, it will briefly
    discuss an IoT application and its image detection-based implementation in a real-world
    scenario. In the second part of the chapter, we shall present a hands-on image
    detection implementation of an application using a DL algorithm. In this chapter,
    we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: IoT applications and image recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case one: Image-based road fault detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case two: Image-based smart solid waste separation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning for image recognition in IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs for image recognition in IoT applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data pre-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IoT applications and image recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The image recognition landscape in IoT applications is rapidly changing. Significant
    advances in mobile processing power, edge computing, and machine learning are
    paving the way for the widespread use of image recognition in many IoT applications.
    For example, omnipresent mobile devices (which are a key components in many IoT
    applications) equipped with high-resolution cameras facilitate the generation
    of images and videos by everyone, everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, intelligent video cameras, such as IP cameras and Raspberry Pis with
    cameras, are used in many places, such as smart homes, campuses, and factories,
    for different applications. Many IoT applications—including smart cities, smart
    homes, smart health, smart education, smart factories, and smart agriculture—make
    decisions using image recognition/classification. As shown in the following diagram, these
    applications use one or more of the following image recognition services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5027b9b-10a4-4fb5-9d03-75004e180697.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s us discuss the previous image in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**People Identification**: Generally, secure and friendly access to home, office,
    and any other premises is a challenging task. The use of smart devices, including
    IoT solutions, can offer secure and friendly access to many premises. Let''s consider
    the example of office or home access. We use one or more keys access to our homes
    or offices. If we lose these keys, this could not only inconvenience us but put
    our security at risk if somebody else finds them. In this context, image recognition-based
    people identification can be used as a keyless access method for a smart home
    or office.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object Identification**: IoT-based automated object identification is highly
    desirable in many domains, including driverless cars, smart cities, and smart
    factories. For example, smart city applications, such as smart vehicle license
    plate recognition and vehicle detection, as well as city-wide public asset monitoring,
    can use image recognition-based object detection services. Similarly, a smart
    factory can use the object detection service for inventory management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial Recognition**: The image processing-based facial detection and recognition
    landscape is changing so rapidly that it will be a commodity soon. Smartphones
    with biometrics will then be the norm. Smartphones and IoT-based facial recognition
    can be used in many applications, such as safety and security, and smart education.
    For example, in a smart class (education), a face recognition system can be used
    to identify the response to a lecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event Detection**: Symptoms of many human diseases (such as  hand foot mouth),
    animal diseases (such as, foot and mouth, and poultry diseases), and plant diseases
    are explicit and visible. These diseases can be digitally detected using IoT solutions
    integrated with DL-based image classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case one – image-based automated fault detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Public assets (such as roads, public buildings, and tourist places) in a city
    are heterogeneous and distributed within the city. Most cities in the world face
    challenges in monitoring, fault detection, and reporting these assets. For example,
    in many UK cities, citizens often report faults, but the accuracy and efficiency
    of the reporting is an issue in many cases. In a smart city, these assets can
    be monitored, and their faults can be detected and reported through an IoT application.
    For example, a vehicle (such as a city council vehicle) attached with one or more
    sensors (such as a camera or a mic) can be used for the road fault monitoring
    and detection.
  prefs: []
  type: TYPE_NORMAL
- en: Roads are important assets in a city, and they have many faults. Potholes, bumps,
    and road roughness are some of the most frustrating hazards and anomalies experienced
    by commuters and vehicles. Importantly, vehicles may frequently face suspension
    problems, steering misalignment, and punctures, which could also lead to accidents.
    The cost of road-fault-related damages is significant. For example, pothole-related
    damage alone cost UK drivers £1.7 billion a year. An IoT application with the
    support of an appropriate DL algorithm can be used to automatically detect these
    faults and report them appropriately. This reduces the number of road-fault-related
    damage in a cost-effective way.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing use case one
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, the implementation of the use case consists
    of three main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7204812-461d-4169-8ae5-af23821b9b21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us learn about the components in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensors and data gathering**:The selection of sensors for data gathering
    depends on the assets and fault types. If we use a smartphone as the edge-computing
    device, its camera can be used for sensing and data gathering about road faults.
    On the contrary, if we use Raspberry Pi as the edge-computing device, we need
    to use an external camera, as there is no built-in camera within the Raspberry
    Pi. The preceding diagram shows the Raspberry Pi and camera used for the use case
    implementation. We used a Raspberry Pi 3 model B+ with 1 GB RAM and a 5-megapixel
    sensor with an Omnivision OV5647 sensor in a fixed-focus lens. The sampling or
    photographic rate of the camera will depend on the vehicle''s speed and the availability
    of a road''s faults. For example, if the smartphone camera or the camera installed
    on the vehicle can capture one picture a second, the phone or  Raspberry Pi will
    be able to detect the faults within two seconds if the speed of the vehicle is
    40 km/h or less. Once the image is sensed and captured, it will be sent to the
    detection method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault detection and reporting**:In this phase, the edge-computing device
    will be installed with one app. The installed app in a smartphone or Raspberry
    Pi will be loaded with pre-trained fault detection and a classification model.
    Once the vehicle’s smartphone or Raspberry Pi camera takes a picture (following
    a sampling rate), these models will detect and classify a potential fault and
    report to the application server (local council).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Council''s server and Fault Detection Model**:The council''s server is responsible
    for the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the model for fault detection and classification using reference datasets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disseminating and updating the models for the edge-computing device
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Receiving and storing the fault data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image-based model learning and validation of road's fault detection is at the
    heart of the implementation. The second part (covered in the sections starting
    from *Transfer learning for image recognition in IoT*) of the chapter will describe
    the implementation of the DL-based anomaly detection of the previous use case. All
    the necessary codes are available in the chapter's code folder.
  prefs: []
  type: TYPE_NORMAL
- en: Use case two – image-based smart solid waste separation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Solid waste is a global challenge. The management of solid waste is expensive,
    and improper waste management is also seriously impacting the global economy,
    public health, and the environment. Generally, solid waste, such as plastic, glass
    bottles, and paper, are recyclable, and they need an effective recycling method
    to become economically and environmentally beneficial. However, in most countries,
    the existing recycling processes are done manually. In addition, citizens or consumers
    often become confused about the recycling method.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, IoT with the support of machine learning and deep learning,
    especially image-based object recognition, can identify the type of waste and
    help sort it accordingly without any human intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing use case two
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The implementation of image-based smart solid waste separation includes two
    key components:'
  prefs: []
  type: TYPE_NORMAL
- en: A bin with an individual chamber with a controllable lid for each type of solid
    waste
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An IoT infrastructure with a DL model for image recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first component of the implementation is not within the scope of this book,
    and we are considering the component as available for this implementation. As
    shown in the following diagram, the IoT implementation of the use case consists
    of two main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58f00fef-94d2-4c5c-beff-e05f95948f48.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Sensors and data gathering**: Selection of sensors for data gathering depends
    on the types of solid waste and their features. For example, many glass and plastic
    bottles are very similar in color and appearance. However, their weights are generally
    distinctly different. For the use case, we are considering two sensors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more cameras to capture an image of trash when it enters into a bin through
    the entry point
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A weight sensor to get the weight of the trash
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We use Raspberry Pi as the computing platform. The use case was tested using
    a Raspberry Pi 3 model B+ with 1 GB RAM and a 5 megapixel sensor with an Omnivision
    OV5647 sensor in a fixed-focus lens. Once the image and weight are sensed and
    captured, they are sent to the sorting method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Trash detection and sorting**: This is the key element of the implementation.
    The Raspberry Pi will be loaded with a pretrained trash detection and sorting
    model using DL. Once the detection algorithm detects trash and sorts it, it will
    actuate the control system to open the appropriate lid and move it into the bin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use case scenario is focusing on waste management in urban public areas,
    including parks, tourist attractions, landscaping, and other recreational areas.
    Generally, citizens and/or visitors in these areas individually dispose of their
    waste. Importantly, they dispose of items in small numbers, from single items
    to just a few items.
  prefs: []
  type: TYPE_NORMAL
- en: All of the following sections will describe the implementation of the DL-based
    image recognition needed for the aforementioned use cases. All the necessary codes
    are available in the chapter's code folder.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning for image recognition in IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, transfer learning means transferring pre-trained machine learning
    model representations to another problem. In recent years, this is becoming a
    popular means of applying DL models to a problem, especially in image processing
    and recognition, as it enables training a DL model with comparatively little data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows two models:'
  prefs: []
  type: TYPE_NORMAL
- en: An architecture for a standard DL model (a)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An architecture for a transfer-learning DL model (b):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/adc41d97-4ee5-461a-8bca-ef6bbabcea35.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the figure of an architecture for a standard DL model, a fully trained
    neural net takes input values in an initial layer and then sequentially feeds
    this information forward with necessary transformation until the second-to-last
    layer (which is also known as the **bottleneck layer**) has constructed a high-level
    representation of the input that can more easily be transformed into a final output.
    The complete training of the model involves the optimization of weight and bias
    terms used in each connection (labeled in blue). In large and heterogeneous datasets,
    the number of these weight and bias terms could be in the millions.
  prefs: []
  type: TYPE_NORMAL
- en: In transfer learning, we can use the early and middle layers and only re-train
    the latter layers. One popular approach to transfer learning is to reuse the pre-trained
    weights for the whole network other than the last layer and relearn the weights
    of the last layer or classification part by retraining the network using the new
    dataset. As shown in the diagram of an architecture for a transfer-learning DL
    model, we reused the orange connections and retrained the network using the new
    dataset to learn the last layer’s green connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many pre-trained DL models, including the Inception-v3 and MobileNets models,
    are available to be used for transfer learning. The Inception-v3 model, which
    was trained for the ImageNet *Large Visual Recognition Challenge*, classifies
    images into 1,000 classes, such as *Zebra*, *Dalmatian*, and *Dishwasher*. Inception-v3 consists
    of two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: A feature extraction part with a convolutional neural network, which extracts
    features from the input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A classification part with fully connected and softmax layers, which classifies
    the input data based on the features identified in part one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to use Inception-v3, we can reuse the feature extraction part and
    re-train the classification part with our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer learning offers two benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Training on new data is faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to solve a problem with less training data rather than learning
    from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These features of transfer learning are especially useful for the implementation
    of DL models in IoT's resource-constrained edge devices, as we do not need to
    train the resource-hungry feature extraction part. Thus, the model can be trained
    using less computational resources and time.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs for image recognition in IoT applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Convolutional Neural Network** (**CNN**) has different implementations. **AlexNet**
    is one such implementation, and it won the ImageNet Challenge: ILSVRC 2012\. Since
    then, CNNs have become omnipresent in computer vision and image detection and
    classification. Until April 2017, the general trend was to make deeper and more
    complicated networks to achieve higher accuracy. However, these deeper and complex
    networks offered improved accuracy but did not always make the networks more efficient,
    particularly in terms of size and speed. In many real-world applications, especially
    in IoT applications, such as a self-driving car and patient monitoring, recognition
    tasks need to be accomplished in a timely fashion on a resource-constrained (processing,
    memory) platform.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, MobileNet V1 was introduced in April 2017\. This version of
    Mobilenet was an improvement on its second version (MobileNetV2) in April 2018. **Mobilenets** and
    their variants are the efficient CNN DL model's IoT applications, especially for
    image recognition-based IoT applications. In the following paragraphs, we present
    a brief overview of MobileNets.
  prefs: []
  type: TYPE_NORMAL
- en: MobileNets are the implementations of most popular and widely used DL models,
    namely CNNs. They are especially designed for resource-constrained mobile devices
    to support classification, detection, and prediction. Personal mobile devices,
    including smartphones, wearable devices, and smartwatches, installed with DL models
    improve user experience, offering any time, anywhere access, with the additional
    benefits of security, privacy, and energy consumption. Importantly, new emerging
    applications in mobile devices will need ever-more efficient neural networks to
    interact with the real world in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the standard convolutional filters (figure
    a) are replaced by two layers in Mobilenet V1\. It uses a depthwise convolution
    (figure b) and a pointwise convolution (figure c) to build a depthwise separable
    filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2294acd6-1260-41c1-80fc-d142b3d6cb0a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The main motivation of MobileNet V1 is that convolutional layers are expensive
    to compute, and they can be replaced by so-called **depthwise separable convolutions**.
    In MobileNet V1, the depthwise convolution process uses a single filter to every
    input channel, and the pointwise convolution then uses a 1 x 1 convolution process
    to the outputs of the earlier depthwise convolution. As shown in the diagram of
    a standard convolution filter, a standard convolution both filters and combines
    inputs into a new set of outputs in one step. Unlike standard CNNs, the depthwise
    separable convolution (factorized) in MobileNets splits this into two layers (as
    shown in the diagram of Mobilenet V1): a layer for filtering and a separate layer
    for combining.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram presents the factorized architecture of Mobilenet V1\.
    This factorization drastically reduces computation and model size as the model
    needs to calculate a significantly smaller number of parameters. For example,
    MobileNet V1 needs to calculate 4.2 million parameters, whereas a full convolution
    network needs to calculate 29.3 million parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6aac716a-4376-421f-b2ee-b1bf9f081ed6.png)'
  prefs: []
  type: TYPE_IMG
- en: MobileNet V2 is an updated and significantly improved version of MobileNet V1\.
    It has greatly improved and pushes existing mobile visual recognition, including
    classification, detection, and semantic segmentation. Like MobileNet V1, the MobileNet
    V2 was released as part of the TensorFlow-Slim Image Classification Library. If
    needed, you can explore this in Google's Colaboratory. In addition, MobileNet
    V2 is available as modules on TF-Hub, and pre-trained checkpoints or saved models
    can be found at [https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet) [and
    can be used as transfer learning.](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram presents a simple architecture of MobileNet V2.  MobileNet
    V2  has been developed as an extension of MobileNet V1\. It uses depth-wise separable
    convolution as efficient building blocks. In addition, MobileNet V2 includes two
    new features in the architecture. One is the linear bottlenecks between the layers,
    and the other one is shortcut connections between the bottlenecks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b80688e0-0d3f-465f-b2ff-976064b84f48.png)'
  prefs: []
  type: TYPE_IMG
- en: Collecting data for use case one
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can collect data using a smartphone camera or a Raspberry Pi camera and
    prepare the dataset by ourselves, or download existing images from the internet
    (that is, via Google, Bing, and so on) and prepare the dataset. Alternatively,
    we can use an existing open source dataset. For use case one, we have used a combination
    of both. We have downloaded an existing dataset on pothole images (one of the
    most common road faults) from and updated the dataset with more images from Google
    images. The open source dataset (`PotDataset`) for pothole recognition was published
    by Cranfield University, UK. The dataset includes images of pothole objects and
    non-pothole objects, including manholes, pavements, road markings, and shadows.
    The images were manually annotated and organized into the following folders:'
  prefs: []
  type: TYPE_NORMAL
- en: Manhole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pavement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pothole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Road markings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shadow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the dataset from use case one
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is essential to explore the dataset before applying DL algorithms to the
    data. For the exploration, we can run `image_explorer.py` on the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram presents a snapshot of the data exploration process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07f715ce-8c41-4106-95fe-475b550a1361.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the diagram of data exploration, the differences between pothole
    and non-pothole objects are not always obvious if we are using only the smartphone
    camera. A combination of an IR and smartphone camera can improve the situation.
    In addition, we found that the pothole images we have used here might not be enough
    to cover a wide range of potholes such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Many images in the used dataset show that the potholes are already maintained/fixed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a few images of a large-sized pothole in the used dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this context, we decided to update the pothole images dataset by collecting
    more images from the internet. Next, we briefly discuss the data collection process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Search**:Use any browser (we used Chrome), go to Google, and search for *pothole
    images* in Google Images. Your search window will look like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can select copyright-free images by clicking on *Tools* and changing the
    usage rights to *Labeled for reuse with modification.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e95a92bb-cdea-48e0-a81d-acbe3b9b806b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Gathering Images URLs**: This step is to use a few lines of JavaScript code
    to gather the image URLs. The gathered URLs can be used in Python to download
    the images. As shown in the following screenshot, select the JavaScript console
    (assuming you use the Chrome web browser, but you can use Firefox as well) by
    clicking **View** | **Developer **| **JavaScript Console** (in macOS) and customize
    and control **Google Chrome** | **More tools** | **Developer tools** (in Windows
    OS):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8dc8d211-0d45-458c-a81c-fa9da19a824d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you have selected the JavaScript console, you will see a browser window
    such as the following screenshot, and this will enable you to execute JavaScript
    in a REPL-like manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dd135fa-09e2-46a8-9a84-986e8a490d14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now do the following in order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll the page and go down until you have found all useful images (note: please
    use images that are not subject to copyright) for your dataset. After that, you
    need to collect the URLs for the selected images.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now move to the JavaScript console and then copy and paste the following JavaScript
    codes into the console:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code will pull the jQuery JavaScript library.  Now you
    can use a CSS selector to collect a list of URLs using the following lines of
    code:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, write the URLs to a file (one per line) using the following lines
    of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once you execute the preceding lines of code, you will have a file named `imageurls.txt`
    in your default download directory.  If you want to download them into a specific
    folder, then write `hiddenComponents.download = 'your fooler/imageurls.txt` instead
    of `hiddenComponents.download = 'imageurls.txt'` in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: '**Downloading the images**: Now you are ready to download the running the images
    `download_images.py` (available in code folder of the chapter) in the previously
    downloaded `imageurls.txt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Exploration**: Once we have downloaded the images, we need to explore them
    in order delete the irrelevant images. We can do this through a bit of manual
    inspection. After this, we need to resize and convert them into grayscale images
    to match the previously downloaded dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/46a249b1-8e27-4169-a8c4-50d62acc6e8d.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the folder structure of the pothole and non-pothole
    images datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data for use case two
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As is the case with use case one, we can collect data through digital cameras
    or use an existing open source or a combination of both. We are using an existing
    and open source dataset for the implementation of the sorting algorithm. The dataset
    was collected from urban environments of the USA . As solid waste types may vary
    by country, it is better to update the dataset based on the country the use case
    will be used for. The dataset consists of six types of solid wastes: glass, paper,
    cardboard, plastic, metal, and trash. The dataset consists of 2,527 images, and
    they were annotated and organized into the following folders, as shown in the 
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/688c0162-ec3e-4ecb-97e1-60c59f450432.png)'
  prefs: []
  type: TYPE_IMG
- en: Data exploration of use case two
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following presents a snapshot of the data exploration for use case two.
    As we can see, glass and plastic images could be confusing to the sorting algorithm.
    In this context, weight sensor data can be useful for fixing this issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1efe9aad-74f3-4cfe-879a-da94f0bf12da.png)'
  prefs: []
  type: TYPE_IMG
- en: Data pre-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is an essential step for a DL pipeline. The existing datasets on pothole
    images and the solid waste images used in the use cases are pre-processed and
    are ready to be used for training, validation, and testing. As shown in the following
    diagram, both the original and modified (additional images downloaded for the
    pothole class) are organized as sub-folders, each named after one of the five
    categories and containing only images from that category. There are a few issues
    to be noted during the training image set preparation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data size**:We need to collect at least a hundred images for each class to
    train a model that works well. The more we can gather, the better the accuracy
    of the trained model is likely to be. Each of the five categories in the used
    dataset has more than 1,000 sample images. We also made sure that the images are
    a good representation of what our application will actually face in real implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data heterogeneity**:Data collected for training should be heterogeneous.
    For example, images about potholes need to be taken in as wide a variety of situations
    as we can, at different times, and with different devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, we are using transfer learning that does not require
    training from scratch; retraining of the models with a new dataset will sufficiently
    work in many cases. We retrained two popular architectures or models of CNN, namely
    Incentive V3 and Mobilenet V1, on a desktop computer, which is replicating the
    city council’s server. In both models, it took less than an hour to retrain the
    models, which is an advantage of the transfer learning approach. We need to understand
    the list of key arguments before running the `retrain.py`file, which is in the
    code folder. If we type in our Terminal (in Linux or macOS) or Command Prompt
    (Windows) `python retrain.py -h`, we shall see a window like the following screenshot
    with additional information (that is, an overview of each argument). The compulsory
    argument is the image directory, and it is one of the dataset directories shown
    in the preceding figures on the folder view of the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/716599bf-9f65-4cd9-82e6-b46defa25ad4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following, we are presenting two examples of command: one to retrain
    the Incentive model V3 and the other to retain Mobilenet V1 on the modified dataset
    (dataset-modified). To retrain Incentive V3, we did not pass the architecture
    argument value as it is the default architecture included in `retrain.py`. For
    the rest of the arguments, including data split ratio among training, validation,
    and test, we used the default values. In this use case, we are using the split
    rule of the data that put 80% of the images into the main training set, keeping
    10% separate for validation during training, and the final 10% of the data as
    a testing set. The testing set is to test the real-world classification performance
    of the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the training and validation of the Mobilenet V1 model, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we run the preceding commands, it will generate the retrain models (`retrained_graph.pb`),
    labels text (`retrained_labels.txt`) in the given directory and summary directory
    consists of train and validation summary information of the models. The summary
    information `(--summaries_dir`  argument with default value `retrain_logs`) can
    be used by TensorBoard to visualize different aspects of the models, including
    the networks, and their performance graphs. If we type the following command in
    the terminal or Command Prompt, it will run TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once TensorBoard is running, navigate your web browser to `localhost:6006` to
    view the TensorBoard and view the network of the corresponding model. The following
    diagrams **(a)** and **(b)** show the network for Incentive V3 and Mobilenet V1
    respectively. The diagram demonstrates the complexity of Incentive V3 compared
    to Mobilenet V1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22d6ea4b-2faa-4ca9-8936-08c84bc6711f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the second use case, we have retrained only the Mobilenet V1 on the solid
    waste dataset. You can retrain the model as mentioned by only providing an image
    or dataset directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Firstly, we have identified the size of the retrain models. As shown in the
    following screenshot, Mobilenet V1 requires only 17.1 MB (for both use cases),
    which is than one-fifth of Incentive V3 (92.3 MB), and this model can be easily
    deployed in resource-constrained IoT devices, including Raspberry Pi or smartphones.
    Secondly, we have evaluated the performance of the models. Two levels of performance
    evaluation have been done for the use cases: (i) dataset-wide evaluation or testing
    has been done during the retraining phase on the desktop PC platform/server, and
    (ii) an individual image or sample (real-life image) was tested or evaluated in
    the Raspberry Pi 3 environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89912302-26f7-4677-b93e-6f01168445dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Model performance (use case one)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the evaluation performances of use case one are presented in the following
    screenshots. The following six screenshots present the training, validation, and
    testing performances of Incentive V3 and Mobilenet V1 models on the two sets of
    data. The first three screenshots present the results generated in the terminal
    after retraining the models, and the last three screenshots are generated from
    the TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot presents the evaluation results of Incentive V3 on
    the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f968d524-a60a-4c52-8817-65c4cf04c4f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of Incentive V3 on
    the modified dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5eaee1f3-7570-44ed-bd87-1d28aa93f7f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of Mobilenet V1 on
    the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb61326-9ce7-4ded-97a7-8b0b1c20e223.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of  Mobilenet V1 on
    the modified dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eaa8056d-ed33-4e1e-9b9e-e57e812d82f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of Incentive V3 on
    the original dataset generated by TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/943f8ef3-d31f-4d57-87e3-a52137a240c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of  Mobilenet V1 on
    the original dataset generated by TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2469605e-54d7-4ff8-8437-fa1e9a518650.png)'
  prefs: []
  type: TYPE_IMG
- en: From all the previous model performance screenshots, it is clear that both training
    and validation accuracies are well above 90%, which is enough for fault detection.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagrams show the classification or object detection performances
    on individual samples. For these, we have used two different sets of classification
    code (available in the chapter's code folder).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first screenshot is showing the snapshot of running the classifier for
    Mobilenet V1 on two samples. As we can see from all results, test or evaluation
    accuracy is well above 94%, and with such accuracy, the DL models (CNNs) have
    the potential to detect objects, including potholes, manholes, and other objects
    on the road. However, object detection time on the Pi 3 was in the range of three
    to five seconds, which needs to be improved if we want to use them in real-time
    detection and actuation. In addition, results show that models trained on the
    modified dataset have a good chance to provide high detection or testing accuracy
    in a real environment (shown in the preceding screenshots), especially in detecting
    potholes, as this class of data has been improved by adding diverse images from
    the googled images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8256fc92-b359-45b0-8b38-4f76fc35cd19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of pothole detection
    with the Incentive V3 model trained on the original dataset (Pi 3 B+):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e62eb2e-fec0-4466-bfb5-c1296717e078.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram presents the evaluation results of manhole detection
    with the Incentive V3 model trained on the original dataset (Pi 3 B+):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09716d1d-c32b-400c-a862-160475bb473c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram presents the evaluation results of  pothole detection
    with the Mobilenet V1 model trained on the original dataset (Pi 3 B+):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/072508d6-ea2a-4eef-9d33-a287aaec601d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram presents the evaluation results of  manhole detection
    with the Mobilenet V1 model trained on the original dataset (Pi 3 B+):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0460b70-2622-48f5-aa3e-8fee182a53f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Model performance (use case two)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the evaluation performances of use case two are presented in the following
    screenshots. For this use case, we are presenting only the results for Mobilenet
    V1 .The following diagrams present the training, validation, and testing performances
    of the Mobilenet V1 models on the two datasets. As we can see from the following
    screenshot, the test accuracy is not that high (77.5%) but good enough for solid
    waste detection and sorting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81309be7-f043-4486-ae9f-2243d26318db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of Mobilenet V1 on
    the dataset generated by TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a8271f6-78a8-4302-8e08-c7c4f63c25ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following three screenshots show the classification or object (solid waste)
    detection performances on individual samples. The first screenshot presents the
    evaluation results of glass detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/784a2d8a-4dcb-4d74-88d4-725e03dee6ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of plastic detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d037aa38-1eb6-40be-b709-4377b1bb1ee7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot presents the evaluation results of metal detection
    using Mobilenet V1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a1d55f8-9114-4ba5-a4c8-39bd2a1cff87.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first part of this chapter, we briefly described different IoT applications
    and their image detection-based decision-making. In addition, we briefly discussed
    two use cases: image detection-based road fault detection, and image detection-based
    solid waste sorting. The first application can detect potholes on the road using
    a smartphone camera or a Raspberry Pi camera. The second application detects different
    types of solid waste and sorts them according to smart recycling.'
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of the chapter, we briefly discussed transfer learning with
    a few example networks, and examined its usefulness in resource-constrained IoT
    applications. In addition, we discussed the rationale behind selecting a CNN,
    including two popular implementations, namely Inception V3 and Mobilenet V1\.
    The rest of the chapter described all the necessary components of the DL pipeline
    for the Inception V3 and Mobilenet V1 models.
  prefs: []
  type: TYPE_NORMAL
- en: In many IoT applications, image recognition alone may not be enough for object
    and/or subject detection. In this context, sometimes, audio/speech/voice recognition
    can be useful. [Chapter 3](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml), Audio/Speech/Voice
    Recognition in IoT, will present DL-based speech/voice data analysis and recognition
    in IoT applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Smart patrolling: An efficient road surface monitoring using smartphone sensors
    and crowdsourcing*, Gurdit Singh, Divya Bansal, Sanjeev Sofat, Naveen Aggarwal, *Pervasive
    and Mobile Computing*, volume 40, 2017, pages 71-88'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Road Damage Detection Using Deep Neural Networks with Images Captured Through
    a Smartphone*, Hiroya Maeda, Yoshihide Sekimoto, Toshikazu Seto, Takehiro Kashiyama,
    Hiroshi Omata, arXiv:1801.09454'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Potholes cost UK drivers £1.7 billion a year: Here''s how to claim if you
    car is damaged*, Luke John Smith: [https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK](https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What a Waste: A Global Review of Solid Waste Management*, D Hoornweg and P
    Bhada-Tata, World Bank, Washington, DC, USA, 2012'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Efficient Convolutional Neural Networks for Mobile Vision Applica**tions*,
    Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias
    Weyand, Marco Andreetto, Hartwig Adam, *MobileNets: *arXiv:1704.04861'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Imagenet classification with deep convolutional neural networks*, A Krizhevsky,
    I Sutskever, G E Hinton, in *Advances in Neural Information Processing Systems*,
    pages 1,097–1,105, 2012\. 1, 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MobileNetV2: Inverted Residuals and Linear Bottlenecks*, Mark Sandler, Andrew
    Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, arXiv:1801.04381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pothole dataset: [https://cord.cranfield.ac.uk/articles/PotDataset/5999699 ](https://cord.cranfield.ac.uk/articles/PotDataset/5999699)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trashnet: [https://github.com/garythung/trashnet](https://github.com/garythung/trashnet)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
