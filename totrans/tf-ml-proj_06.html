<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Stock Prices using Gaussian Process Regression</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn about a new model for forecasting known as <strong>Gaussian processes</strong>, popularly abbreviated as <strong>GPs</strong>, this is extremely popular in forecasting applications where we want to model non-linear functions with a few data points and also to quantify uncertainty in predictions.</p>
<p>We will use Gaussian processes to predict the stock prices of three major stocks, namely, Google, Netflix, and the <strong>General Electric</strong> <span>(GE) </span>company.</p>
<p>The rest of this chapter is divided into the following sections:</p>
<ul>
<li>Understanding Bayes' rule</li>
<li>Bayesian inference</li>
<li>Introducing Gaussian processes</li>
<li>Understanding the stock market dataset</li>
<li>Applying Gaussian processes to predict stock market prices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding Bayes' rule</h1>
                </header>
            
            <article>
                
<p>Let us begin by reviewing the Bayes' rule and it's associated terminology, before we start with our project. </p>
<p>Bayes' rule is used to describe the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, <span>let's say we want to predict the probability a person having diabetes. If we know the preliminary medical test results, we can hope to get a more accurate prediction than when we don't know results of the test. Let's put some numbers around this to understand mathematically:</span></p>
<ul>
<li>1% of population has diabetes ( and therefore 99% do not)</li>
<li>Preliminary tests detect diabetes 80% of the time when it is there ( therefore 20% of time we require advanced tests)</li>
<li>10% of time preliminary test detect diabetes even when it is not there (therefore 90% of time they give the correct result):</li>
</ul>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 22%"/>
<td style="width: 28%">
<p><strong>Diabetes (1%)</strong></p>
</td>
<td style="width: 25.7565%">
<p><strong>No diabetes (99%)</strong></p>
</td>
</tr>
<tr>
<td style="width: 22%">
<p><strong>Test Positive</strong></p>
</td>
<td style="width: 28%">
<p>80%</p>
</td>
<td style="width: 25.7565%">
<p>10%</p>
</td>
</tr>
<tr>
<td style="width: 22%">
<p><strong>Test Negative</strong></p>
</td>
<td style="width: 28%">
<p>20%</p>
</td>
<td style="width: 25.7565%">
<p>90%</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p> </p>
<p>So, if a person has diabetes, we will be looking at first column and he has 80% chance of being detected. And if a person doesn't have diabetes, we will be looking at second column and he has 10% chance of testing positive for diabetes from preliminary tests.</p>
<p>Now let's say a person was detected positively for diabetes from preliminary test. What are the chances that he actually has diabetes?</p>
<p>As it turns out, a renowned scientist Thomas Bayes' ( 171-1761) provided a mathematical framework to compute probability in the cases like above. His mathematical formula can be given as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/951a9bc4-f393-4cca-b9ec-ec069aa20acc.png" style="width:15.75em;height:2.42em;"/></p>
<p>Where:</p>
<ul>
<li><img class="fm-editor-equation" src="assets/9b5a1e8a-8850-47f6-af12-bdfefc76b742.png" style="width:2.00em;height:1.00em;"/> denotes the probability of diabetes in a randomly selected person which is 1% in this case.<img class="fm-editor-equation" src="assets/3ed237e2-b687-4dce-9b02-5cfc221d8105.png" style="width:2.33em;height:1.17em;"/> is also known as<span> </span><strong>prior</strong><span> </span>in Bayesian terminology which denotes our belief for an event without any additional information.</li>
<li><img class="fm-editor-equation" src="assets/a8e2c941-82d0-4bce-8131-7003c7d02c84.png" style="width:7.67em;height:1.42em;"/> denotes the probability of a person having diabetes given that he was detected positive by preliminary search results. In Bayesian terminology, this is also known as<span> </span><strong>posterior </strong>which denotes the updated probability of an event after having obtained additional information.</li>
<li><img class="fm-editor-equation" src="assets/2da3eb1b-c253-48d6-a05c-5bceeaef1d24.png" style="width:7.25em;height:1.33em;"/> <span><span>denotes the probability of getting a positive result in preliminary test given a person has diabetes. It is 80% in this case.</span></span></li>
<li><img class="fm-editor-equation" src="assets/57855b2f-e2c4-4b39-980f-f4b984330f3c.png" style="width:6.25em;height:1.42em;"/> denotes the probability that a random person will be tested positive in preliminary test. This can also be written down as: </li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/651f55bb-405e-4212-8f20-32b7567eff2e.png" style="width:34.75em;height:1.33em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/29bff235-bef5-4999-977b-54d6c0121efa.png" style="width:20.42em;height:1.33em;"/></p>
<p>Bayes' rule is used heavily to quantify uncertainty in predictions of machine learning systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Introducing Bayesian inference</h1>
                </header>
            
            <article>
                
<p>Now that we know about the basics of Bayes' rule, let's try to understand the concept of Bayesian inference or modeling.</p>
<p>As we know, real-world environments are always dynamic, noisy, observation costly, and time-sensitive. When business decisions are based on forecasting in these environments, we want to not only produce better forecasts, but also quantify the uncertainty in these forecasts. For this reason, the theory of Bayesian inferences is extremely handy as it provides a principled approach to such problems.</p>
<p>For a typical time series model, we effectively carry out curve fitting based on <em>y</em> when given the <em>x</em> variable. This helps to fit a curve based on past observations. Let's try to understand its limitations. Consider the following example of temperature in a city:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 126.667px">
<p><strong>Day</strong></p>
</td>
<td style="width: 146.667px">
<p><strong>Temperature</strong></p>
</td>
</tr>
<tr>
<td style="width: 126.667px">
<p>May 1 10 AM</p>
</td>
<td style="width: 146.667px">
<p>10.5 degrees Celsius</p>
</td>
</tr>
<tr>
<td style="width: 126.667px">
<p>May 15 10 AM</p>
</td>
<td style="width: 146.667px">
<p>17.5 degrees Celsius</p>
</td>
</tr>
<tr>
<td style="width: 126.667px">
<p>May 30 10 AM</p>
</td>
<td style="width: 146.667px">
<p>25 degrees Celsius</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Using curve fitting, we obtain the following model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7ad60f2e-4859-40f2-9249-754813b4c20e.png" style="width:11.58em;height:1.17em;"/></p>
<p>However, this will imply that the temperature function is linear, and, on tenth day, we expect the temperature to be 15 <span>degrees </span>Celsius. <span>It is common knowledge that the temperature of a city fluctuates a lot during a day and it </span>depends <span>on when we take the readings. Curve fitting defines one of the functions over a given set of readings.</span></p>
<p>This example leads us to the conclusion that there is a family of curves that can model the given observations. The idea of the distribution of curves that model the given observation is central to Bayesian inference or modeling. The question now is: what should be the process of choosing one function over this family of functions? Or whether we should, in fact, choose one?</p>
<p>One way to narrow down this family of functions is to short list a subset of them based on our prior knowledge about the problem. For example, we know that in May we don't expect temperatures to go below zero degrees Celsius. We can use this knowledge and discard all the functions which have points below zero degrees. Another popular way to think about this problem is to define a distribution over a function space based on our prior knowledge. Furthermore, the job of modeling, in this case, is to refine the distribution over possible functions based on the observed data points. Since these models don't have any defined parameters, they are popularly known as <strong>Bayesian non-parametric models</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Gaussian processes</h1>
                </header>
            
            <article>
                
<p>The Gaussian process (GP) can be thought of as an alternative Bayesian approach to regression problems. They are also referred to as infinite dimensional Gaussian distributions. GP defines a priori over functions that can be converted into a posteriori once we have observed a few data points. Although it doesn’t seem possible to define distributions over functions, it turns out that we only need to define distributions over a function's values at observed data points.</p>
<p>Formally, let's say that we observed a function, <img class="fm-editor-equation" src="assets/a278cd36-da50-4984-b8a2-a5a31a0f41d5.png" style="width:0.67em;height:1.33em;"/>, at n values <img class="fm-editor-equation" src="assets/8038a339-08c0-4bd9-bec2-d847bcdd5923.png" style="width:7.17em;height:1.00em;"/> as <img class="fm-editor-equation" src="assets/2de559fe-a00c-4832-acc5-2500e03fe6a6.png" style="width:10.67em;height:1.42em;"/>. The function is a GP if all of the values, <img class="fm-editor-equation" src="assets/272f6482-2994-476c-95d1-50943eb6a3ea.png" style="width:9.42em;height:1.25em;"/>, are jointly Gaussian, with a mean of <img class="fm-editor-equation" src="assets/694a4dd4-0161-4ce4-94bb-13b0eb5de4cd.png" style="width:2.17em;height:1.33em;"/> and a covariance of <img class="fm-editor-equation" src="assets/688c125d-6b48-472b-b56c-8840ca089f5b.png" style="width:1.50em;height:1.17em;"/>  given by <img class="fm-editor-equation" src="assets/0efeffbe-fa9f-424b-9683-1e937eb3cdec.png" style="width:7.50em;height:1.42em;"/>. Here, the <img class="fm-editor-equation" src="assets/1c3d2b1a-ad87-4bf6-89ff-553cd3009dcf.png" style="width:0.67em;height:1.17em;"/> function defines how two variables are related to each other. We will discuss different kinds of kernels later in this section. The joint Gaussian distribution of many Gaussian variables is also known as Multivariate Gaussian. </p>
<p>From the previous temperature example, we can imagine that various functions can be fit to the given observations on temperature. Some functions are smoother than others. One way to capture smoothness is by using the covariance matrix. The covariance matrix ensures that two values (<img class="fm-editor-equation" src="assets/2c19505a-d914-41fb-b347-50ac27de6216.png" style="width:3.17em;height:1.17em;"/>) close in the input space produce closer values in the output space (<img class="fm-editor-equation" src="assets/dff74ee8-c4bb-4b6e-b92b-df324c62a94c.png" style="width:6.75em;height:1.58em;"/>).</p>
<p>Essentially, the problem we are trying to solve through GP is as follows: given a set of inputs, <img class="fm-editor-equation" src="assets/22a0a040-b0f1-4ee4-a7df-4d05f59026ab.png" style="width:1.00em;height:1.00em;"/>, and its values, <img class="fm-editor-equation" src="assets/b78eee49-4717-466d-a15d-e549943f25c3.png" style="width:0.58em;height:1.17em;"/>, we are trying to estimate the distribution over outputs, <img class="fm-editor-equation" src="assets/1a8e2103-1df5-43e0-b5b9-a0110238c6ed.png" style="width:1.08em;height:1.17em;"/>, for a new set of inputs, <img class="fm-editor-equation" src="assets/1e052223-2b40-49aa-8eec-84799a90fb52.png" style="width:1.33em;height:1.00em;"/>. Mathematically, the quantity we are trying to estimate can be written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fb393d3a-f09d-488f-8ad2-609f363feadb.png" style="width:6.42em;height:1.25em;"/></p>
<p>To obtain this, we model <img class="fm-editor-equation" src="assets/f0780df4-977d-42b6-ad5b-cbcdbd14ea16.png" style="width:0.75em;height:1.50em;"/> as a GP so that we know that both <img class="fm-editor-equation" src="assets/8e69496e-df16-4a8c-8aff-03c2682fc80f.png" style="width:0.67em;height:1.33em;"/> and <img class="fm-editor-equation" src="assets/90734fc7-ecae-44f7-8d7a-a1414ba3a81f.png" style="width:1.17em;height:1.33em;"/> are coming from a multivariate Gaussian with the following mean and covariance function:</p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 150px"><img class="alignnone size-full wp-image-404 image-border" src="assets/4effe6bc-fa12-4378-89ce-144622aec16e.png" style="width:18.00em;height:4.17em;"/></p>
<p>Where <img class="fm-editor-equation" src="assets/1b140f99-0195-4a9c-a7f9-fadd2dc131fa.png" style="width:0.75em;height:1.00em;"/> and <img class="fm-editor-equation" src="assets/7c45a81a-67c0-4dae-971b-009496dafd32.png" style="width:1.25em;height:0.92em;"/> represent the prior mean of the distribution of <img class="fm-editor-equation" src="assets/f0780df4-977d-42b6-ad5b-cbcdbd14ea16.png" style="width:0.67em;height:1.33em;"/> and <img class="fm-editor-equation" src="assets/1a8e2103-1df5-43e0-b5b9-a0110238c6ed.png" style="width:1.00em;height:1.08em;"/> over observed and unobserved function values, respectively, <img class="fm-editor-equation" src="assets/26a39e62-f73f-45b1-8f00-383feb03aeca.png" style="width:1.00em;height:0.92em;"/> represents the matrix obtained after applying the kernel function to each of the observed values, <img class="fm-editor-equation" src="assets/22a0a040-b0f1-4ee4-a7df-4d05f59026ab.png" style="width:1.00em;height:1.00em;"/>.</p>
<p>Kernel function tries to map the similarity between two data points in the input space to the output space. Let's assume, there are two data points <img class="fm-editor-equation" src="assets/52170400-b12c-4c42-bb24-a55a896d7743.png" style="width:1.33em;height:1.08em;"/> and <img class="fm-editor-equation" src="assets/8f98f264-1213-4dc6-902c-e0a1c367f07f.png" style="width:1.25em;height:1.08em;"/> with corresponding function values as <img class="fm-editor-equation" src="assets/d44d4d04-8614-4401-8287-44f8ac6a7d78.png" style="width:2.83em;height:1.50em;"/> and <img class="fm-editor-equation" src="assets/5fa623e9-d9e4-4274-aaf2-f1cd53fe6392.png" style="width:2.83em;height:1.50em;"/>. The Kernel function measures how the closeness between two <span>points </span><img class="fm-editor-equation" src="assets/52170400-b12c-4c42-bb24-a55a896d7743.png" style="width:1.42em;height:1.17em;"/><span> and </span><img class="fm-editor-equation" src="assets/8f98f264-1213-4dc6-902c-e0a1c367f07f.png" style="width:1.50em;height:1.33em;"/><span> in the input space maps to the similarity or correlation between their function values <img class="fm-editor-equation" src="assets/b7e763af-d663-46c2-ac24-12eaf5ad80d7.png" style="width:2.58em;height:1.33em;"/> and <img class="fm-editor-equation" src="assets/12d419c0-ad2c-4b7b-a0b6-63af38e88879.png" style="width:2.75em;height:1.42em;"/>.</span></p>
<p><span>We apply this kernel function to all the pairs of observations in the dataset, thereby, creating a matrix of similarities known as Kernel/Covariance matrix (<em>K</em>).</span> Assuming, there are 10 input data points, the kernel function will be applied to each pair of data points leading to a 10x10 Kernel Matrix (<em>K</em>). If the function values at two data points <img class="fm-editor-equation" src="assets/52170400-b12c-4c42-bb24-a55a896d7743.png" style="width:1.42em;height:1.17em;"/> and <img class="fm-editor-equation" src="assets/8f98f264-1213-4dc6-902c-e0a1c367f07f.png" style="width:1.50em;height:1.33em;"/> is expected to be similar, kernel is expected to have high value at <em>(i,j)</em> in the matrix. We do a detailed discussion on different kernels in GPs in the next section.</p>
<p>In the equation, <img class="fm-editor-equation" src="assets/db2e0ad6-026c-43cc-86be-aeb8a27e9bef.png" style="width:1.58em;height:1.17em;"/> represents the matrix obtained by applying the same kernel function to values in the training and testing dataset, and <img class="fm-editor-equation" src="assets/8061a3b0-c2f5-4795-b4a8-d3d2a3c94b18.png" style="width:1.83em;height:1.08em;"/> is the matrix obtained by measuring the similarity between the input values in the test set.</p>
<p>At this point, we will assume that there is some linear algebra magic that can help us to achieve the conditional distribution of <img class="fm-editor-equation" src="assets/fb393d3a-f09d-488f-8ad2-609f363feadb.png" style="width:6.92em;height:1.33em;"/> from the joint distribution and obtain the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e2639840-4335-48e2-840c-4ffe2865b2d2.png" style="width:14.00em;height:1.42em;"/></p>
<div class="packt_infobox"><span>We are going to skip the derivations, but if you would like to know more, you can visit </span>Rasmussen and Williams<span> (</span><a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">http://www.gaussianprocess.org/gpml/chapters/RW.pdf</a><span>). </span></div>
<p>With this analytical result, we have access to the entire distribution of function values over the testing dataset. Modeling the predictions as distributions also helps in quantifying the uncertainty surrounding the predictions, which is quite important in many time series applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing kernels in GPs</h1>
                </header>
            
            <article>
                
<p>In many applications, we find that the prior mean is always set to zero as it is simple, convenient and works well for many applications. However, choosing the appropriate kernels for the task is not always straightforward. As mentioned in the previous section, kernels effectively try to map the similarity between two input data points in the input space to the output (function) space. The only requirement for the kernel function (<img class="fm-editor-equation" src="assets/66ba8791-225f-4f4e-93a7-079e2199a612.png" style="width:0.83em;height:1.42em;"/>) is that it should map any two input values<span> </span><img class="fm-editor-equation" src="assets/8f12aaf0-d55b-4194-b4d7-89dbc34c2dae.png" style="width:1.42em;height:1.17em;"/><span> and </span><img class="fm-editor-equation" src="assets/bfad4856-888b-4329-84c0-ba9970c3aadc.png" style="width:1.50em;height:1.33em;"/> to a scalar such that Kernel Matrix (<img class="fm-editor-equation" src="assets/83a5b785-4c39-456a-8f54-52ba57395735.png" style="font-size: 1em;width:1.17em;height:1.08em;"/>) <span>is a positi</span><span>ve/semi-definite for it to be a valid covariance function. </span></p>
<div class="packt_tip">For sake of brevity, we exclude explanation of fundamental concepts of Covariance matrices and how they are always positive semi-definite. We encourage the readers to refer to lecture notes from <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2008/lecture-notes/MIT6_436JF08_lec15.pdf">MIT</a> (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2008/lecture-notes/MIT6_436JF08_lec15.pdf)</div>
<p>While a complete discussion of all of the types of kernels is beyond the scope of this chapter, we will discuss the two kernels used to build this project:</p>
<ul>
<li><strong>White noise kernel: </strong>As the name suggests, the white noise kernel adds a white noise (of variance<span>) to the existing covariance matrix. Mathematically, it is given as follows:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dba08700-b379-4d43-bdca-a513b15a2d9b.png" style="width:11.92em;height:1.83em;"/></p>
<p style="padding-left: 60px">If there are many settings, the data points are not accurate and are corrupted by some random noise. Noise in the input data can be modeled by adding a white noise kernel to the covariance matrix.</p>
<ul>
<li><strong>Squared exponential (SE) kernel:</strong> Given two scalars, <img class="fm-editor-equation" src="assets/7d0effde-cd01-4263-a54c-7c95219de40a.png" style="width:1.25em;height:1.00em;"/> and <img class="fm-editor-equation" src="assets/2d724806-82a9-4682-85eb-865b58604c98.png" style="width:1.17em;height:1.00em;"/>, the squared exponential kernel is given by the following equation:</li>
</ul>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 120px"><img class="alignnone size-full wp-image-402 image-border" src="assets/360c9c14-b89e-4808-9583-3aa0d293ffe5.png" style="width:19.75em;height:2.42em;"/></p>
<p style="padding-left: 60px">Here, <img class="fm-editor-equation" src="assets/3c68e226-e471-4656-9a57-905bb111b4ff.png" style="width:0.92em;height:0.92em;"/> is a scaling factor and <img class="fm-editor-equation" src="assets/240d0539-3f80-4c0f-9079-a7efc4089974.png" style="width:0.50em;height:1.42em;"/> which is the smoothness parameter determines the smoothness of kernel function. It is quite a popular kernel because the functions drawn from the GP through this kernel are infinitely differentiable, which makes it suitable for many applications.</p>
<p>Here are some samples that have been drawn from a GP with an SE kernel that has <img class="fm-editor-equation" src="assets/3c68e226-e471-4656-9a57-905bb111b4ff.png" style="width:0.92em;height:0.92em;"/> fixed to 1:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-359 image-border" src="assets/eefbb334-ce35-454c-b87d-1b8ac9cdacc6.png" style="width:107.00em;height:46.17em;"/></p>
<p>We can observe that the functions become smoother as <img class="fm-editor-equation" src="assets/c616a4ff-2e8f-4c08-9ce5-816548dadc00.png" style="width:0.42em;height:1.17em;"/> increases. For more information on different kinds of kernels, refer to <em>The Kernel Cookbook</em> (<a href="https://www.cs.toronto.edu/~duvenaud/cookbook/" target="_blank">https://www.cs.toronto.edu/~duvenaud/cookbook/</a>https://www.cs.toronto.edu/~duvenaud/cookbook/)</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing the hyper parameters of a kernel</h1>
                </header>
            
            <article>
                
<p>So far, we have defined kernels with different parameters. For example, in a squared exponential kernel, we have the parameters <img class="fm-editor-equation" src="assets/e2c9b8c5-d9d5-4682-ad41-55184f6e0f33.png" style="width:0.92em;height:0.92em;"/> and <img class="fm-editor-equation" src="assets/a76bfe27-56de-458c-afc8-d2f256a4a6ee.png" style="width:0.33em;height:0.92em;"/>. Let's denote the parameter set of any kernel as <img class="fm-editor-equation" src="assets/0c20142e-ae5a-453a-83f3-4da976648679.png" style="width:0.58em;height:1.08em;"/>. The question now is, how do we estimate <img class="fm-editor-equation" src="assets/4f6c6973-0029-47aa-b77a-23a0aab277e6.png" style="width:0.58em;height:1.08em;"/>?</p>
<p>As mentioned previously, we model the distribution of the output of <img class="fm-editor-equation" src="assets/93fd2135-8859-4e8d-80d0-ecbbcb8d9237.png" style="width:0.58em;height:1.17em;"/> function to be a random sample from a multivariate Gaussian distribution. In this manner, the marginal likelihood of observed data points is a multivariate Gaussian that's been conditioned on the input points <img class="fm-editor-equation" src="assets/4b1002f7-07c2-4b86-bc5e-a630fc9b13d1.png" style="width:0.92em;height:0.92em;"/> and the parameter <img class="fm-editor-equation" src="assets/4f6c6973-0029-47aa-b77a-23a0aab277e6.png" style="font-size: 1em;width:0.58em;height:1.08em;"/><span>. Thus, we can choose <img class="fm-editor-equation" src="assets/4f6c6973-0029-47aa-b77a-23a0aab277e6.png" style="width:0.58em;height:1.08em;"/></span><span> by maximizing the likelihood of the observed data points over this assumption.</span></p>
<p>Now that we have understood how GPs make predictions, let's see how we can make predictions on the stock market using GPs and potentially make some money.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying GPs to stock market prediction</h1>
                </header>
            
            <article>
                
<p>In this project, we will try to predict the prices of three major stocks in the market. The dataset for this exercise can be downloaded from Yahoo Finance (<a href="https://finance.yahoo.com">https://finance.yahoo.com</a>). We downloaded the entire stock history for three companies:</p>
<ul>
<li>Google (<a href="https://finance.yahoo.com/quote/GOOG">https://finance.yahoo.com/quote/GOOG</a>)</li>
<li>Netflix (<a href="https://finance.yahoo.com/quote/NFLX">https://finance.yahoo.com/quote/NFLX</a>)</li>
<li>General Electric company (<a href="https://finance.yahoo.com/quote/GE">https://finance.yahoo.com/quote/GE</a>)</li>
</ul>
<p>We choose three datasets to compare GP performance across different stocks. Feel free to try this for more stocks.</p>
<div class="packt_infobox">All of these datasets are present in the GitHub repository. Thus, there is no need to download them again to run the code.</div>
<p>The CSV files in the dataset have multiple columns. They are as follows:</p>
<ul>
<li><strong>Date</strong>: Calendar date when the price of the stock was measured.</li>
<li><strong>Open</strong>: The opening price of the day.</li>
<li><strong>High:</strong> The highest price of the day.</li>
<li><strong>Low:</strong> The lowest price of the day.</li>
<li><strong>Close:</strong> The closing price of the day.</li>
<li><strong>Adj Close:</strong> The adjusted closing price is the closing price of the stock that has been amended to include any dividends or other corporate actions before the following day's opening. This is our target variable or Y in the dataset.</li>
<li><strong>Volume:</strong> The volume denotes the number of shares traded during a day.</li>
</ul>
<p class="mce-root"/>
<p>To begin our project, we will consider two forecasting problems each, for all three stock datasets: </p>
<ul>
<li>In the first problem, we will train on prices from the years 2008-2016 and predict for the entire year of 2017</li>
<li>In the second problem, we will train on prices from the years 2008-2018 (up to the third quarter) and predict the fourth quarter of 2018</li>
</ul>
<p>For predicting stock prices, we don't need to model the entire time series of a stock as a single time series, as in many classical methods (an example of this is regression). For GP, each time series is divided into several time series (one for each year) for every stock. Intuitively, this makes sense, as each stock follows a yearly cycle. </p>
<p><span>The time series of each year of a stock is an input to the model as a separate time series. Therefore, the forecasting problem becomes as follows: predict future prices of the stock given multiple yearly time series (one for each historical year) as the input. As GP models are distributions over functions, we want to predict the mean and uncertainty at each data point in the future.</span></p>
<p><span>Before modeling, we need to normalize the prices to be zero mean and unit standard deviation. </span>This is a requirement in Gaussian processes because of the following:</p>
<ul>
<li>We assume the prior on the output distribution to be zero mean, so normalization is required to match our assumption.</li>
<li>Many kernels for the covariance matrix have scale parameters in them. Normalizing the input helps us to get better estimates of kernel parameters.</li>
<li>For obtaining the posterior distribution in Gaussian processes, we have to invert the covariance matrix. Normalization helps to avoid any kind of numerical issues with this procedure. Note that we haven't discussed the linear algebra of obtaining the posterior in detail in this chapter.</li>
</ul>
<p><span>Once the data has been normalized, we can train our model and predict prices using Gaussian processes. For modeling, we use the plug, and play functions from the GPflow (<a href="https://github.com/GPflow/GPflow">https://github.com/GPflow/GPflow</a>) library, which is a wrapper on top of TensorFlow for Gaussian processes. </span></p>
<p>The independent variable (X) in the prediction problem is comprised of two factors:</p>
<ul>
<li>The year </li>
<li>The day of the year</li>
</ul>
<p>The dependent variable (Y) in the problem is the normalized adjusted closing price for each day in a year as mentioned before.</p>
<p>Before training the model, we need to define the prior and kernel function for Gaussian Processes. For this problem, we use the standard zero mean prior. We use a kernel function for the covariance matrix that is generated as the sum of two kernels which are defined as follows:</p>
<ul>
<li>Squared exponential (or RBF, as mentioned in the GPflow package) kernel with <kbd>lengthscale</kbd> = <kbd>1</kbd> and <kbd>variance</kbd> = 63. </li>
<li>White noise with initial <kbd>variance</kbd> to be very low, such as <em>1e-10.</em></li>
</ul>
<p>The idea behind choosing squared exponential is that it is infinitely differentiable and it's the easiest one to understand. White noise is used to account for any systemic noise we might observe in our target variables. While it may not be the best choice of kernels, it is good for understanding purposes. Feel free to experiment with other kernels and see whether they work well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a stock price prediction model</h1>
                </header>
            
            <article>
                
<p>We will begin our project by processing the data present in the dataset:</p>
<ol>
<li>Create a dataframe with yearly time series for each stock. Represent each year's stock price by an individual column in that dataframe. Restrict number of rows in the dataframe to 252 which is roughly the number of trading days in a year. Also add the fiscal quarter associated with each row of data as a separate column.</li>
</ol>
<pre style="padding-left: 60px">def get_prices_by_year(self):<br/>   df = self.modify_first_year_data()<br/>   for i in range(1, len(self.num_years)):<br/>       df = pd.concat([df, pd.DataFrame(self.get_year_data(year=self.num_years[i], normalized=True))], axis=1)<br/>   df = df[:self.num_days]<br/>   quarter_col = []<br/>   num_days_in_quarter = self.num_days // 4<br/>   for j in range(0, len(self.quarter_names)):<br/>       quarter_col.extend([self.quarter_names[j]]*num_days_in_quarter)<br/>   quarter_col = pd.DataFrame(quarter_col)<br/>   df = pd.concat([df, quarter_col], axis=1)<br/>   df.columns = self.num_years + ['Quarter']<br/>   df.index.name = 'Day'<br/>   df = self.fill_nans_with_mean(df)<br/>   return df</pre>
<div class="packt_infobox">Note that there are almost 252 trading days in a year, as stock markets are closed on weekends.</div>
<ol start="2">
<li>Even if there are more trading days in a particular year (like leap year), limit data to 252 days to ensure consistency across the years. In case the number of trading days is less than 252 in a particular year, extrapolate the data to 252 days by imputing the mean price of the year for missing days. Implement the following code to achieve this:</li>
</ol>
<pre style="padding-left: 60px">def fill_nans_with_mean(self, df):<br/>   years = self.num_years[:-1]<br/>   df_wo_last_year = df.loc[:,years]<br/>   df_wo_last_year = df_wo_last_year.fillna(df_wo_last_year.mean())<br/>   df_wo_last_year[self.num_years[-1]] = df[self.num_years[-1]]<br/>   df= df_wo_last_year<br/><br/>   return<span> </span>df</pre>
<ol start="3">
<li>For each year, normalize the prices to transform the yearly series to zero mean and unit standard deviation. Also, subtract the first day price from all of the data points in that year. This basically forces a yearly time series to start from zero, thereby avoiding any influence of previous year's prices on it.</li>
</ol>
<pre style="padding-left: 60px">def normalized_data_col(self, df):<br/>   price_normalized = pd.DataFrame()<br/>   date_list = list(df.Date)<br/>   self.num_years = sorted(list(set([date_list[i].year for i in range(0, len(date_list))])))<br/>   for i in range(0, len(self.num_years)):<br/>       prices_data = self.get_year_data(year=self.num_years[i], normalized=False)<br/>       prices_data = [(prices_data[i] - np.mean(prices_data)) / np.std(prices_data) for i in range(0, len(prices_data))]<br/>       prices_data = [(prices_data[i] - prices_data[0]) for i in range(0, len(prices_data))]<br/>       price_normalized = price_normalized.append(prices_data, ignore_index=True)<br/>   return price_normalized</pre>
<div class="packt_infobox">Please make sure to install this library as mentioned in the <strong>README</strong> file in the repository for this chapter, before executing the code.</div>
<ol start="4">
<li>As mentioned in the previous section, generate the covariance matrix as a sum of two kernels:</li>
</ol>
<pre style="padding-left: 60px">kernel<span> </span>=<span> </span>gpflow.kernels.RBF(2,<span> </span>lengthscales=1,<span> </span>variance=63)<span> </span>+<span> </span>gpflow.kernels.White(2,<span> </span>variance=1e-10)</pre>
<p style="padding-left: 30px">We use the SciPy optimizer in GPflow package to optimize hyper parameters using maximum likelihood estimation. <span>Scipy is a standard optimizer from Python library Scipy. If you are not familiar with Scipy optimizer, please refer to the </span>official page<span> (<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html</a>).</span></p>
<ol start="5">
<li>Implement the final wrapper function<span> </span><kbd>make_gp_predictions</kbd><span> t</span>o train a GP model and make future price predictions. Following are the steps that the function implements<span>:</span>
<ol>
<li>Takes the input of training data, start and end of training period and prediction year and quarter. </li>
<li>Constructs 2 separate series using data from the start year of the training period, one for the independent variables (X) and one for the target (Y). Each element in series (X) represents <span>each day of the year and consists </span>of two independent variables, year and day of the year. For example, for start year 2008, X looks like [[2008,1], [<span>2008,2</span>],[<span>2008,3</span>].......[<span>2008,252</span>]].</li>
<li>Appends the independent and target variables for each subsequent year to list X and Y respectively. </li>
<li>If input <kbd>pred_quarters</kbd> is not None, predicts for the <span>quarters</span> specified instead of the entire year. For example, if <kbd>pred_quarters</kbd> is [4] and <kbd>pred_year </kbd>is 2018, the function will predict for Quarter 4 of 2018 using all the data till Quarter 3 of 2018. </li>
<li>Defines the kernel function as mentioned before and trains the GP model using Scipy optimizer. </li>
<li>Predicts the stock prices for the prediction period.</li>
</ol>
</li>
</ol>
<pre>def make_gp_predictions(self, start_year, end_year, pred_year, pred_quarters = []):<br/>   start_year, end_year, pred_year= int(start_year),int(end_year), int(pred_year)<br/>   years_quarters = list(range(start_year, end_year + 1)) + ['Quarter']<br/>   years_in_train = years_quarters[:-2]<br/>   price_df = self.preprocessed_data.prices_by_year[self.preprocessed_data.prices_by_year.columns.intersection(years_quarters)]<br/>   num_days_in_train = list(price_df.index.values)<br/>   #Generating X and Y for Training<br/>   first_year_prices = price_df[start_year]<br/>   if start_year == self.preprocessed_data.num_years[0]:<br/>       first_year_prices = (first_year_prices[first_year_prices.iloc[:] != 0])<br/>       first_year_prices = (pd.Series([0.0], index=[first_year_prices.index[0]-1])).append(first_year_prices)<br/>   first_year_days = list(first_year_prices.index.values)<br/>   first_year_X = np.array([[start_year, day] for day in first_year_days])<br/>   X = first_year_X<br/>   Target = np.array(first_year_prices)<br/>   for year in years_in_train[1:]:<br/>       current_year_prices = list(price_df.loc[:, year])<br/>       current_year_X = np.array([[year, day] for day in num_days_in_train])<br/>       X = np.append(X, current_year_X, axis=0)<br/>       Target = np.append(Target, current_year_prices)<br/>   final_year_prices = price_df[end_year]<br/>   final_year_prices = final_year_prices[final_year_prices.iloc[:].notnull()]<br/>   final_year_days = list(final_year_prices.index.values)<br/>   if pred_quarters is not None:<br/>       length = 63 * (pred_quarters[0] - 1)<br/>       final_year_days = final_year_days[:length]<br/>       final_year_prices = final_year_prices[:length]<br/>   final_year_X = np.array([[end_year, day] for day in final_year_days])<br/>   X = np.append(X, final_year_X, axis=0)<br/>   Target = np.append(Target, final_year_prices)<br/>   if pred_quarters is not None:<br/>       days_for_prediction = [day for day in<br/>                              range(63 * (pred_quarters[0]-1), 63 * pred_quarters[int(len(pred_quarters) != 1)])]<br/>   else:<br/>       days_for_prediction = list(range(0, self.preprocessed_data.num_days))<br/>   x_mesh = np.linspace(days_for_prediction[0], days_for_prediction[-1]<br/>                        , 2000)<br/>   x_pred = ([[pred_year, x_mesh[i]] for i in range(len(x_mesh))])<br/>   X = X.astype(np.float64)<br/>   Target = np.expand_dims(Target, axis=1)<br/>   kernel = gpflow.kernels.RBF(2, lengthscales=1, variance=63) + gpflow.kernels.White(2, variance=1e-10)<br/>   self.gp_model = gpflow.models.GPR(X, Target, kern=kernel)<br/>   gpflow.train.ScipyOptimizer().minimize(self.gp_model)<br/>   y_mean, y_var = self.gp_model.predict_y(x_pred)<br/>   return x_mesh, y_mean, y_var</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the results obtained</h1>
                </header>
            
            <article>
                
<p>Let's try to understand how good our predictions are for each of the stocks:</p>
<ul>
<li><strong>Netflix (NFLX)</strong>: The following diagram i<span>llustrates the prices of the Netflix stock from <strong>2002</strong> through <strong>2018</strong>:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-370 image-border" src="assets/cb7fcc4b-bb57-4b83-ae7f-7270e5ed0165.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">The price for the year <strong>2018</strong> is defined using the two vertical lines. It shows the growth in the price of the stock throughout the entire year.</p>
<p style="padding-left: 60px">As per the first problem case, we consider the period from <strong>2008-2016</strong> for training:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-371 image-border" src="assets/fd1a998d-3f38-4faa-a166-0525b69a5fe4.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">Normalizing the prices by each year for modeling gives us the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-372 image-border" src="assets/f737d653-8cb7-4ae2-9ac8-75e5ac1c1db7.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">Predicting the prices of the stock for the whole year of <strong>2017</strong> with a <strong>95% confidence interval</strong> gives us the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-373 image-border" src="assets/7811b0b1-6932-4e9c-ada7-77b8e03e74bd.png" style="width:100.00em;height:48.42em;"/></p>
<p style="padding-left: 60px">Comparing of the generated values with the actual values, it is clear that the model falters by predicting the value to be less than the actual value. However, the reason for this could be the highs and lows of the prices of Netflix in the year <strong>2016</strong>. These are not captured through the basic-level kernel used in this project.</p>
<p style="padding-left: 60px">For the second problem case, we consider the train period from <strong>2008-2018</strong>, including the first three quarters. The plot for the Netflix stock price during this period is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-374 image-border" src="assets/6d5f521d-56ef-49e0-bf1b-bce74fb8e187.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">Using the normalized values, we achieve the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-375 image-border" src="assets/db9c27fd-f922-4581-8a9d-d366987faffe.png" style="width:100.00em;height:48.33em;"/></p>
<p style="padding-left: 60px">As we can see, the trend is very well captured in this prediction, along with uncertainty.</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><strong>General Electric company</strong> (<strong>GE</strong>): To understand the difference between the actual and the predicted prices, it is necessary to plot the graph that has the actual values. Here is the plot that illustrates the historical prices of GE stock:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-376 image-border" src="assets/98fb46c8-2454-4644-aa7a-8b94f92a6d24.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">As mentioned previously, the dotted vertical lines represent the prices in the year <strong>2018</strong>.</p>
<p style="padding-left: 60px">As per our first problem case, we consider the period from <strong>2008-2016</strong> for training. The chart for that period is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-378 image-border" src="assets/1c0b1610-d4b0-4b19-a6ae-fa7136effc43.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">There was a huge dip in <strong>2009</strong>, but since then, the stock has been growing steadily.</p>
<p style="padding-left: 60px">Since we used normalized prices for every year for modeling purposes, let's look at the input data for our model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-379 image-border" src="assets/e5192d44-9ff1-42c2-96d7-19cb2d86e7d2.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">The predictions with normalized prices for the whole of <strong>2017</strong> with a <strong>95% confidence interval</strong> are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-380 image-border" src="assets/0a7e5592-8b81-429c-bca4-702e1235d8e7.png" style="width:100.00em;height:48.17em;"/></p>
<p style="padding-left: 60px">As we can see, the model has captured an accurate trend of the stock.</p>
<p style="padding-left: 60px">For the second prediction, we consider the train period from <strong>2008-2018</strong>, including the first three quarters of <strong>2018</strong>. The plot for the GE stock price during that period is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-381 image-border" src="assets/85e5b5bc-822e-4d47-82fc-7b68fadb8ce4.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">The predicted prices for that period with a <strong>95% confidence interval</strong> are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-382 image-border" src="assets/cd225648-98fe-4d6c-8db8-06b1229778a6.png" style="width:100.00em;height:48.42em;"/></p>
<ul>
<li><strong>Google</strong> (<strong>GOOG</strong>): The following chart illustrates the historical prices of Google stock:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><strong><img class="alignnone size-full wp-image-383 image-border" src="assets/59c4504b-3445-44a0-af60-f9b932723bdb.png" style="width:75.42em;height:37.67em;"/></strong></p>
<p style="padding-left: 60px">As specified previously, the dotted vertical lines represent the prices for <strong>2018</strong>.</p>
<p style="padding-left: 60px">As per the first problem case, the period from <strong>2008-2016</strong> is essential for training, so let's look at the chart for this period:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-384 image-border" src="assets/39bbfb2c-a830-425f-97c6-7fbf49be392a.png" style="border: 1em solid black;text-align: center;width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 90px">There was a huge dip in <strong>2009</strong> and since then, the stock has been growing steadily, except during <strong>2015</strong>.</p>
<p style="padding-left: 90px">Since we used normalized prices by year for modeling, let's look at the input data for our model:</p>
<p class="CDPAlignCenter CDPAlign" style="padding-left: 30px"><img class="alignnone size-full wp-image-385 image-border" src="assets/efd040ee-dcf9-409a-9b3b-ab13c4378d12.png" style="width:100.00em;height:49.58em;"/></p>
<p style="padding-left: 60px">Predictions (normalized prices) for a complete year, <strong>2017</strong>, with a <strong>95% confidence interval</strong>, are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-386 image-border" src="assets/36c9abde-18a1-4d39-bed8-808fca53dcb5.png" style="width:100.00em;height:48.58em;"/></p>
<p style="padding-left: 60px">We were able to capture the overall upward trend of the price, but with very wide confidence bands.</p>
<p style="padding-left: 60px">For the next prediction, we considered the train period from <strong>2008-2018</strong>, including the first three quarters. A plot of the Google stock price during that period is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-387 image-border" src="assets/57e40b7a-b603-4f75-aafd-1a67782d40c5.png" style="width:100.00em;height:50.00em;"/></p>
<p style="padding-left: 60px">The predicted prices for that period with a <strong>95% confidence interval</strong> are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-388 image-border" src="assets/fd5abf5d-fa5a-4e41-9983-ad3ff9162666.png" style="width:99.92em;height:47.92em;"/></p>
<p>In 2018, the trend has been better captured overall.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about a very popular Bayesian forecasting model known as the Gaussian process and used it to predict stock prices.</p>
<p>In the first part of this chapter, we looked at the forecasting problem by sampling an appropriate function from a multivariate Gaussian rather than use predicting point forecasts. We looked at a special kind of non-parametric Bayesian model named Gaussian processes. </p>
<p>Thereafter, we used GP to predict the prices of three stocks, namely Google, Netflix, and GE, for 2017 and Q4 2018. We observed that our predictions were mostly within a 95% confidence interval, but far from perfect.</p>
<p>Gaussian processes are used widely in applications where we need to model non-linear functions with uncertainty with very few data points. However, they sometimes fail to scale to very high dimensional problems in which other deep learning algorithms, such as LSTM, would perform better.</p>
<p>In the next chapter, we will take a closer look at an unsupervised approach to detecting credit card frauds using auto-encoders.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are Gaussian processes?</li>
<li>Can you improve the predictions by trying out different kernels?</li>
<li>Can you apply GP model to other stocks in S&amp;P 500 and compare its performance with the ones mentioned here?</li>
</ol>


            </article>

            
        </section>
    </body></html>