<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer217">
<h1 class="chapterNumber">5</h1>
<h1 class="chapterTitle" id="_idParaDest-123">Recurrent Neural Networks</h1>
<p class="normal">In <em class="italic">Chapter 3</em>, we learned about <strong class="keyWord">Convolutional Neural Networks</strong> (<strong class="keyWord">CNNs</strong>) and saw how they exploit the spatial geometry of their inputs. For example, CNNs for images apply convolutions to initially small patches of the image, and progress to larger and larger areas of the image using pooling operations. Convolutions and pooling operations for images are in two dimensions: the width and the height. For audio and text streams, one-dimensional convolution and pooling operations are applied along the time dimension, and for video streams, these operations are applied in three dimensions: along the height, width, and time dimensions.</p>
<p class="normal">In this chapter, we will focus on <strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>), a class of neural networks that is popularly used on text inputs. RNNs are very flexible and have been used to solve problems such as speech recognition, language modeling, machine translation, sentiment analysis, and image captioning, to name a few. RNNs exploit the sequential nature of their input. Sequential inputs could be text, speech, time series, and anything else where the occurrence of an element in a sequence is dependent on the elements that came before it. In this chapter, we will see examples of various RNNs and learn how to implement them with TensorFlow.</p>
<p class="normal">We will first look at the internals of a basic RNN cell and how it deals with these sequential dependencies in the input. We will also learn about some limitations of the basic RNN cell (implemented as SimpleRNN in Keras) and see how two popular variants of the SimpleRNN cell – the <strong class="keyWord">Long Short-Term Memory</strong> (<strong class="keyWord">LSTM</strong>) and the <strong class="keyWord">Gated Recurrent Unit</strong> (<strong class="keyWord">GRU</strong>) – overcome this limitation.</p>
<p class="normal">We will then zoom out one level and consider the RNN layer itself, which is just the RNN cell applied to every time step. An RNN can be thought of as a graph of RNN cells, where each cell performs the same operation on successive elements of the sequence. We will describe some simple modifications to improve performance, such as making the RNN bidirectional and/or stateful.</p>
<p class="normal">Finally, we look at some standard RNN topologies and the kind of applications they can be used to solve. RNNs can be adapted to different types of applications by rearranging the cells in the graph. We will see some examples of these configurations and how they are used to solve specific problems. We will also consider the sequence-to-sequence (or seq2seq) architecture, which has been used with great success in machine translation and various other fields. We will then look at what an attention mechanism is, and how it can be used to improve the performance of sequence-to-sequence architectures.</p>
<p class="normal">In this chapter, we will cover the following topics:</p>
<ul>
<li class="bulletList">The basic RNN cell</li>
<li class="bulletList">RNN cell variants</li>
<li class="bulletList">RNN variants</li>
<li class="bulletList">RNN topologies</li>
<li class="bulletList">Encoder-decoder architectures – seq2seq</li>
<li class="bulletList">Attention mechanism</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp5"><span class="url">https://packt.link/dltfchp5</span></a>.</p>
</div>
<p class="normal">It is often said that a journey of a thousand miles starts with a single step, so in that spirit, let’s begin our study of RNNs by first considering the RNN cell.</p>
<h1 class="heading-1" id="_idParaDest-124">The basic RNN cell</h1>
<p class="normal">Traditional multilayer<a id="_idIndexMarker471"/> perceptron neural networks make the assumption that all inputs are independent of each other. This assumption is not true for many types of sequence data. For example, words in a sentence, musical notes in a composition, stock prices over time, or even molecules in a compound are examples of sequences where an element will display a dependence on previous elements.</p>
<p class="normal">RNN cells incorporate this dependence by having a hidden state, or memory, that holds the essence of what has been seen so far. The value of the hidden state at any point in time is a function of the value of the hidden state at the previous time step, and the value of the input at the current time step, that is:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_05_001.png" style="height: 1.25em !important;" width="275"/></p>
<p class="normal">Here, <em class="italic">h</em><sub class="italic">t</sub> and <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub> are the values of the hidden states at the time <em class="italic">t</em> and <em class="italic">t-1</em> respectively, and <em class="italic">x</em><sub class="italic">t</sub> is the value of the input at time <em class="italic">t</em>. Notice that the equation is recursive, that is, <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub> can be represented in terms of <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-2</sub> and <em class="italic">x</em><sub class="italic">t-1</sub>, and so on, until the beginning of the sequence. This is how RNNs encode and incorporate information from arbitrarily long sequences.</p>
<p class="normal">We can also represent <a id="_idIndexMarker472"/>the RNN cell graphically, as shown in <em class="italic">Figure 5.1(a)</em>. At time <em class="italic">t</em>, the cell has an input <em class="italic">x(t)</em> and output <em class="italic">y(t)</em>. Part of the output <em class="italic">y(t)</em> (represented by the hidden state <em class="italic">h</em><sub class="italic">t</sub>) is fed back into the cell for use at a later time step <em class="italic">t+1</em>.</p>
<p class="normal">Just as in a traditional neural network, where the learned parameters are stored as weight matrices, the RNN’s parameters are defined by the three weight matrices <em class="italic">U</em>, <em class="italic">V</em>, and <em class="italic">W</em>, corresponding to the weights of the input, output, and hidden states respectively:</p>
<figure class="mediaobject"><img alt="Diagram, schematic  Description automatically generated" height="323" src="../Images/B18331_05_01.png" width="793"/></figure>
<p class="packt_figref">Figure 5.1: (a) Schematic of an RNN cell; (b) the RNN cell unrolled</p>
<p class="normal"><em class="italic">Figure 5.1(b)</em> shows the same RNN in an “unrolled view.” Unrolling just means that we draw the network out for the complete sequence. The network shown here has three time steps, suitable for processing three element sequences. Note that the weight matrices <em class="italic">U</em>, <em class="italic">V</em>, and <em class="italic">W</em>, that we spoke about earlier, are shared between each of the time steps. This is because we are applying the same operation to different inputs at each time step. Being able to share these weights across all the time steps greatly reduces the number of parameters that the RNN needs to learn.</p>
<p class="normal">We can also describe <a id="_idIndexMarker473"/>the RNN as a computation graph in terms of equations. The internal state of the RNN at a time <em class="italic">t</em> is given by the value of the hidden vector <em class="italic">h(t)</em>, which is the sum of the weight matrix <em class="italic">W</em> and the hidden state <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub> at time <em class="italic">t-1</em>, and the product of the weight matrix <em class="italic">U</em> and the input <em class="italic">x</em><sub class="subscript">t</sub> at time <em class="italic">t</em>, passed through a <code class="inlineCode">tanh</code> activation function. The choice of <code class="inlineCode">tanh</code> over other activation functions such as sigmoid has to do with it being more efficient for learning in practice and helps combat the vanishing gradient problem, which we will learn about later in the chapter.</p>
<div class="note">
<p class="normal">For notational convenience, in all our equations describing different types of RNN architectures in this chapter, we have omitted explicit reference to the bias terms by incorporating them within the matrix. Consider the following equation of a line in an n-dimensional space. Here, <em class="italic">w</em><sub class="subscript">1</sub> through <em class="italic">w</em><sub class="italic">n</sub> refer to the coefficients of the line in each of the <em class="italic">n</em> dimensions, and the bias <em class="italic">b</em> refers to the y-intercept along each of these dimensions:</p>
<p class="normal"><img alt="" height="46" src="../Images/B18331_05_002.png" style="height: 1.15em !important; vertical-align: 0.03em !important;" width="546"/></p>
<p class="normal">We can rewrite the equation in matrix notation as follows:</p>
<p class="normal"><img alt="" height="46" src="../Images/B18331_05_003.png" style="height: 1.15em !important; vertical-align: 0.03em !important;" width="200"/></p>
<p class="normal">Here, <em class="italic">W</em> is a matrix of shape (<em class="italic">m, n</em>) and <em class="italic">b</em> is a vector of shape (<em class="italic">m, 1</em>), where <em class="italic">m</em> is the number of rows corresponding to the records in our dataset, and <em class="italic">n</em> is the number of columns corresponding to the features for each record. Equivalently, we can eliminate the vector <em class="italic">b</em> by folding it into our matrix <em class="italic">W</em> by treating the <em class="italic">b</em> vector as a feature column corresponding to the “unit” feature of <em class="italic">W</em>. Thus:</p>
<p class="normal"><img alt="" height="46" src="../Images/B18331_05_004.png" style="height: 1.15em !important; vertical-align: 0.03em !important;" width="742"/></p>
<p class="normal">Here, <em class="italic">W’</em> is a matrix of shape (<em class="italic">m, n+1</em>), where the last column contains the values of <em class="italic">b</em>.</p>
<p class="normal">The resulting notation ends up being more compact and (we believe) easier to comprehend and retain as well.</p>
</div>
<p class="normal">The output vector <em class="italic">y</em><sub class="italic">t</sub> at time <em class="italic">t</em> is the product of the weight matrix <em class="italic">V</em> and the hidden state <em class="italic">h</em><sub class="italic">t</sub>, passed through a<a id="_idIndexMarker474"/> softmax activation, such that the resulting vector is a set of output probabilities:</p>
<p class="center"><img alt="" height="46" src="../Images/B18331_05_005.png" style="height: 1.15em !important; vertical-align: 0.10em !important;" width="417"/></p>
<p class="center"><img alt="" height="50" src="../Images/B18331_05_006.png" style="height: 1.25em !important;" width="338"/></p>
<p class="normal">Keras provides the SimpleRNN recurrent layer that incorporates all the logic we have seen so far, as well as the more advanced variants such as LSTM and GRU, which we will learn about later in this chapter. Strictly speaking, it is not necessary to understand how they work to start building with them.</p>
<p class="normal">However, an understanding of the structure and equations is helpful when you need to build your own specialized RNN cell to overcome a specific problem.</p>
<p class="normal">Now that we understand the flow of data forward through the RNN cell, that is, how it combines its input and hidden states to produce the output and the next hidden state, let us now examine the flow of gradients in the reverse direction. This is a process called <strong class="keyWord">Backpropagation Through Time</strong> (<strong class="keyWord">BPTT</strong>).</p>
<h2 class="heading-2" id="_idParaDest-125">Backpropagation through time (BPTT)</h2>
<p class="normal">Just like traditional neural <a id="_idIndexMarker475"/>networks, training RNNs also involves the backpropagation of gradients. The difference, in this case, is that since the weights are shared by all time steps, the gradient at each output depends not only on the <a id="_idIndexMarker476"/>current time step but also on the previous ones. This process is called backpropagation through time [11]. Because the weights <em class="italic">U</em>, <em class="italic">V</em>, and <em class="italic">W</em>, are shared across the different time steps in the case of RNNs, we need to sum up the gradients across the various time steps in the case of BPTT. This is the key difference between traditional backpropagation and BPTT.</p>
<p class="normal">Consider the RNN with five time steps shown in <em class="italic">Figure 5.2</em>. During the forward pass, the network produces predictions <em class="italic">ŷ</em><sub class="subscript">t</sub> at time <em class="italic">t</em> that are compared with the label <em class="italic">y</em><sub class="subscript">t</sub> to compute a loss <em class="italic">L</em><sub class="subscript">t</sub>. During backpropagation (shown by the dotted lines), the gradients of the loss with respect to the weights <em class="italic">U</em>, <em class="italic">V</em>, and <em class="italic">W</em>, are computed at each time step and the parameters updated with the sum of the gradients:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="294" src="../Images/B18331_05_02.png" width="593"/></figure>
<p class="packt_figref">Figure 5.2: Backpropagation through time</p>
<p class="normal">The following equation shows the gradient of the loss with respect to <em class="italic">W</em>. We focus on this weight because it is the <a id="_idIndexMarker477"/>cause of the phenomenon known as the vanishing and exploding gradient problem.</p>
<p class="normal">This problem manifests as the <a id="_idIndexMarker478"/>gradients of the loss approaching either zero or infinity, making the network hard to train. To understand why this happens, consider the equation of the SimpleRNN we saw earlier; the hidden state <em class="italic">h</em><sub class="italic">t</sub> is dependent on <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub>, which in turn is dependent on <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-2</sub>, and so on:</p>
<p class="center"><img alt="" height="117" src="../Images/B18331_05_007.png" style="height: 2.92em !important;" width="233"/></p>
<p class="normal">Let’s now see what happens to this gradient at time step <em class="italic">t=3</em>. By the chain rule, the gradient of the loss with respect to <em class="italic">W</em> can be decomposed to a product of three sub-gradients. The gradient of the hidden state <em class="italic">h</em><sub class="subscript">2</sub> with respect to <em class="italic">W</em> can be further decomposed as the sum of the gradient of each hidden state with respect to the previous one. Finally, each gradient of the hidden state with respect to the previous one can be further decomposed as the product of gradients of the current hidden state against the previous hidden state:</p>
<p class="center"><img alt="" height="142" src="../Images/B18331_05_008.png" style="height: 3.55em !important;" width="1225"/></p>
<p class="normal">Similar calculations are done to compute the gradient of the other losses <em class="italic">L</em><sub class="subscript">0</sub> through <em class="italic">L</em><sub class="subscript">4</sub> with respect to <em class="italic">W</em>, and sum them up into the gradient update for <em class="italic">W</em>. We will not explore the math further in this book, but this WildML blog post [12] has a very good explanation of BPTT, including a more detailed derivation of the math behind the process.</p>
<h2 class="heading-2" id="_idParaDest-126">Vanishing and exploding gradients</h2>
<p class="normal">The reason BPTT is particularly sensitive to the problem of vanishing and exploding gradients comes from the product part of the expression representing the final formulation of the <a id="_idIndexMarker479"/>gradient of the loss with respect to <em class="italic">W</em>. Consider the case where the individual gradients of a hidden state with respect to the previous one are less than 1.</p>
<p class="normal">As we backpropagate <a id="_idIndexMarker480"/>across multiple time steps, the product of gradients becomes smaller and smaller, ultimately leading to the problem of vanishing gradients. Similarly, if the gradients are larger than 1, the products get larger and larger, and ultimately lead to the problem of exploding gradients.</p>
<p class="normal">Of the two, exploding gradients are more easily detectable. The gradients will become very large and turn<a id="_idIndexMarker481"/> into <strong class="keyWord">Not a Number</strong> (<strong class="keyWord">NaN</strong>), and the training process will crash. Exploding gradients can be controlled by clipping them at a predefined threshold [13]. TensorFlow 2.0 allows you to clip gradients using the <code class="inlineCode">clipvalue</code> or <code class="inlineCode">clipnorm</code> parameter during optimizer construction, or by explicitly clipping gradients using <code class="inlineCode">tf.clip_by_value</code>.</p>
<p class="normal">The effect of vanishing gradients is that gradients from time steps that are far away do not contribute anything to the learning process, so the RNN ends up not learning any long-range dependencies. While there are a few approaches toward minimizing the problem, such as proper initialization of the <em class="italic">W</em> matrix, more aggressive regularization, using ReLU instead of <code class="inlineCode">tanh</code> activation, and pretraining the layers using unsupervised methods, the most popular solution is to use LSTM or GRU architectures, both of which will be explained shortly. These architectures have been designed to deal with vanishing gradients and learn long-term dependencies more effectively.</p>
<h1 class="heading-1" id="_idParaDest-127">RNN cell variants</h1>
<p class="normal">In this section, we’ll look at some <a id="_idIndexMarker482"/>cell variants of RNNs. We’ll begin by looking at a variant of the SimpleRNN cell: the LSTM RNN.</p>
<h2 class="heading-2" id="_idParaDest-128">Long short-term memory (LSTM)</h2>
<p class="normal">The LSTM is a variant of the <a id="_idIndexMarker483"/>SimpleRNN cell that is capable of learning long-term dependencies. LSTMs were first proposed by Hochreiter <a id="_idIndexMarker484"/>and SchmidHuber [14] and refined by many other researchers. They work well on a large variety of problems and are the most widely used RNN variant.</p>
<p class="normal">We have seen how the SimpleRNN combines the hidden state from the previous time step and the current input through a <code class="inlineCode">tanh</code> layer to implement recurrence. LSTMs also implement recurrence in a similar way, but instead of a single <code class="inlineCode">tanh</code> layer, there are four layers interacting in a very specific way. <em class="italic">Figure 5.3</em> illustrates the transformations that are applied in the hidden state at time step <em class="italic">t</em>.</p>
<p class="normal">The diagram looks complicated, but let’s look at it component by component. The line across the top of the diagram is the cell state <em class="italic">c</em>, representing the internal memory of the unit.</p>
<p class="normal">The line across the bottom is the hidden state <em class="italic">h</em>, and the <em class="italic">i</em>, <em class="italic">f</em>, <em class="italic">o</em>, and <em class="italic">g</em> gates are the mechanisms by which the LSTM works around the vanishing gradient problem. During training, the LSTM learns the parameters of these gates:</p>
<figure class="mediaobject"><img alt="" height="408" src="../Images/B18331_05_03.png" width="626"/></figure>
<p class="packt_figref">Figure 5.3: An LSTM cell</p>
<p class="normal">An alternative way to think about how these gates work inside an LSTM cell is to consider the equations of the cell. These equations describe how the value of the hidden state <em class="italic">h</em><sub class="subscript">t</sub> at time <em class="italic">t</em> is calculated from the value of hidden state <em class="italic">h</em><sub class="subscript">t-1</sub> at the previous time step. In general, the equation-based description tends to be clearer and more concise and is usually the way a new cell design is presented in academic papers. Diagrams, when provided, may or may not be comparable to the ones you saw earlier. For these reasons, it <a id="_idIndexMarker485"/>usually makes sense to learn to read the equations and visualize the cell design. To that end, we will describe the other cell<a id="_idIndexMarker486"/> variants in this book using equations only.</p>
<p class="normal">The set of equations representing an LSTM is shown as follows:</p>
<p class="center"><img alt="" height="304" src="../Images/B18331_05_009.png" style="height: 7.60em !important;" width="529"/></p>
<p class="normal">Here, <em class="italic">i</em>, <em class="italic">f</em>, and <em class="italic">o</em> are the input, forget, and output gates. They are computed using the same equations but with different parameter matrices <em class="italic">W</em><sub class="italic">i</sub>, <em class="italic">U</em><sub class="italic">i</sub>, <em class="italic">W</em><sub class="italic">f</sub>, <em class="italic">U</em><sub class="italic">f</sub>, and <em class="italic">W</em><sub class="subscript">o</sub>, <em class="italic">U</em><sub class="subscript">o</sub>. The sigmoid function modulates the output of these gates between 0 and 1, so the output vectors produced can be multiplied element-wise with another vector to define how much of the second vector can pass through the first one.</p>
<p class="normal">The forget gate defines how much of the previous state <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub> you want to allow to pass through. The input gate defines how much of the newly computed state for the current input <em class="italic">x</em><sub class="italic">t</sub> you want to let through, and the output gate defines how much of the internal state you want to expose to the next layer. The internal hidden state <em class="italic">g</em> is computed based on the current input <em class="italic">x</em><sub class="italic">t</sub> and the previous hidden state <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub>. Notice that the equation for <em class="italic">g</em> is identical to that of the SimpleRNN, except that in this case, we will modulate the output by the output of input vector <em class="italic">i</em>.</p>
<p class="normal">Given <em class="italic">i</em>, <em class="italic">f</em>, <em class="italic">o</em>, and <em class="italic">g</em>, we can now calculate the cell state <em class="italic">c</em><sub class="subscript">t</sub> at time <em class="italic">t</em> as the cell state <em class="italic">c</em><sub class="italic">t</sub><sub class="subscript">-1</sub> at time (<em class="italic">t-1</em>) multiplied by the value of the forget gate <em class="italic">g</em>, plus the state <em class="italic">g</em> multiplied by the input gate <em class="italic">i</em>. This is basically a way to combine the previous memory and the new input – setting the forget gate to 0 ignores the old memory and setting the input gate to 0 ignores the newly computed state. Finally, the hidden state <em class="italic">h</em><sub class="italic">t</sub> at time <em class="italic">t</em> is computed as the memory <em class="italic">c</em><sub class="italic">t</sub> at time <em class="italic">t</em>, with the output gate <em class="italic">o</em>.</p>
<p class="normal">One thing to realize is that the LSTM is a drop-in replacement for a SimpleRNN cell; the only difference is that LSTMs are resistant to the vanishing gradient problem. You can replace an RNN cell in a network with an LSTM without worrying about any side effects. You should generally see better results along with longer training times.</p>
<p class="normal">TensorFlow 2.0 also <a id="_idIndexMarker487"/>provides a ConvLSTM2D implementation based on the paper by Shi, et al. [18], where the matrix multiplications are replaced by convolution operators.</p>
<p class="normal">If you would like to <a id="_idIndexMarker488"/>learn more about LSTMs, please take a look at the WildML RNN tutorial [15] and Christopher Olah’s blog post [16]. The first covers LSTMs in somewhat greater detail, and the second takes you step by step through the computations in a very visual way.</p>
<p class="normal">Now that we have covered LTSMs, we will cover the other popular RNN cell architecture – GRUs.</p>
<h2 class="heading-2" id="_idParaDest-129">Gated recurrent unit (GRU)</h2>
<p class="normal">The GRU is a variant of the LSTM <a id="_idIndexMarker489"/>and was introduced by Cho, et al [17]. It retains the LSTM’s resistance to the vanishing gradient problem, but its<a id="_idIndexMarker490"/> internal structure is simpler, and is, therefore, faster to train, since fewer computations are needed to make updates to its hidden state.</p>
<p class="normal">Instead of the input (<em class="italic">i</em>), forgot (<em class="italic">f</em>), and output (<em class="italic">o</em>) gates in the LSTM cell, the GRU cell has two gates, an update gate <em class="italic">z</em> and a reset gate <em class="italic">r</em>. The update gate defines how much previous memory to keep around, and the reset gate defines how to combine the new input with the previous memory. There is no persistent cell state distinct from the hidden state as it is in LSTM.</p>
<p class="normal">The GRU cell defines the computation of the hidden state <em class="italic">h</em><sub class="italic">t</sub> at time <em class="italic">t</em> from the hidden state <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub> at the previous time step using the following set of equations:</p>
<p class="center"><img alt="" height="196" src="../Images/B18331_05_010.png" style="height: 4.90em !important;" width="525"/></p>
<p class="normal">The outputs of the update gate <em class="italic">z</em> and the reset gate <em class="italic">r</em> are both computed using a combination of the previous hidden state <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub> and the current input <em class="italic">x</em><sub class="italic">t</sub>. The sigmoid function modulates the <a id="_idIndexMarker491"/>output of these functions between 0 and 1. The cell state <em class="italic">c</em> is computed as a function of the output of the reset gate <em class="italic">r</em> and input <em class="italic">x</em><sub class="italic">t</sub>. Finally, the hidden state <em class="italic">h</em><sub class="subscript">t</sub> at time <em class="italic">t</em> is computed as a function of the cell state <em class="italic">c</em> and the previous hidden state <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub>. The parameters <em class="italic">W</em><sub class="italic">z</sub>, <em class="italic">U</em><sub class="italic">z</sub>, <em class="italic">W</em><sub class="italic">r</sub>, <em class="italic">U</em><sub class="italic">r</sub>, and <em class="italic">W</em><sub class="italic">c</sub>, <em class="italic">U</em><sub class="italic">c</sub>, are learned during training.</p>
<p class="normal">Similar to LSTM, TensorFlow 2.0 (<code class="inlineCode">tf.keras</code>) provides <a id="_idIndexMarker492"/>an implementation for the basic GRU layer as well, which is a drop-in replacement for the RNN cell.</p>
<h2 class="heading-2" id="_idParaDest-130">Peephole LSTM</h2>
<p class="normal">The peephole LSTM is an <a id="_idIndexMarker493"/>LSTM variant that was first proposed by Gers and Schmidhuber [19]. It adds “peepholes” to the input, forget, and output gates, so they can see <a id="_idIndexMarker494"/>the previous cell state <em class="italic">c</em><sub class="italic">t</sub><sub class="subscript">-1</sub>. The equations for computing the hidden state <em class="italic">h</em><sub class="italic">t</sub>, at time <em class="italic">t</em>, from the hidden state <em class="italic">h</em><sub class="italic">t</sub><sub class="subscript">-1</sub> at the previous time step, in a peephole LSTM are shown next.</p>
<p class="normal">Notice that the only difference from the equations for the LSTM is the additional <em class="italic">c</em><sub class="italic">t</sub><sub class="subscript">-1</sub> term for computing outputs of the input (<em class="italic">i</em>), forget (<em class="italic">f</em>), and output (<em class="italic">o</em>) gates:</p>
<p class="center"><img alt="" height="304" src="../Images/B18331_05_009.png" style="height: 7.60em !important;" width="529"/></p>
<p class="normal">TensorFlow 2.0 provides an experimental implementation of the peephole LSTM cell. To use this in your own RNN layers, you will need to wrap the cell (or list of cells) in the RNN wrapper, as shown in the following code snippet:</p>
<pre class="programlisting code"><code class="hljs-code">hidden_dim = <span class="hljs-number">256</span>
peephole_cell = tf.keras.experimental.PeepholeLSTMCell(hidden_dim)
rnn_layer = tf.keras.layers.RNN(peephole_cell)
</code></pre>
<p class="normal">In the previous section, we saw some RNN cell variants that were developed to target specific inadequacies of the basic RNN cell. In the next section, we will look at variations in the architecture of the RNN network itself, which were built to address specific use cases.</p>
<h1 class="heading-1" id="_idParaDest-131">RNN variants</h1>
<p class="normal">In this section, we will look at a couple of variations of the basic RNN architecture that can provide performance improvements in some specific circumstances. Note that these strategies can be applied to <a id="_idIndexMarker495"/>different kinds of RNN cells, as well as for different RNN topologies, which we will learn about later.</p>
<h2 class="heading-2" id="_idParaDest-132">Bidirectional RNNs</h2>
<p class="normal">We have seen how, at any given<a id="_idIndexMarker496"/> time step <em class="italic">t</em>, the output of the RNN is dependent on the outputs at all previous time steps. However, it is entirely possible that the<a id="_idIndexMarker497"/> output is also dependent on the future outputs as well. This is especially true for applications such as natural language processing where the attributes of the word or phrase we are trying to predict may be dependent on the context given by the entire enclosing sentence, not just the words that came before it.</p>
<p class="normal">This problem can be solved using a bidirectional LSTM (see <em class="italic">Figure 5.4</em>), also called biLSTM, which is essentially two<a id="_idIndexMarker498"/> RNNs stacked on top of each other, one reading the input from left to right, and the other reading the input from the right to the left. </p>
<p class="normal">The output at each time step will be based on the hidden state of both RNNs. Bidirectional RNNs allow the network to place equal emphasis on the beginning and end of the sequence, and typically result in performance improvements:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="354" src="../Images/B18331_05_04.png" width="542"/></figure>
<p class="packt_figref">Figure 5.4: Bidirectional LSTM</p>
<p class="normal">TensorFlow 2.0 provides support for <a id="_idIndexMarker499"/>bidirectional RNNs through a bidirectional wrapper layer. To make an RNN layer bidirectional, all that is needed is to wrap the layer with this wrapper layer, which is shown as follows. Since the output of each pair of cells in the left and right<a id="_idIndexMarker500"/> LSTM in the biLSTM pair are concatenated (see <em class="italic">Figure 5.4</em>), it needs to return output from each cell. Hence, we set <code class="inlineCode">return_sequences</code> to <code class="inlineCode">True</code> (the default is <code class="inlineCode">False</code> meaning that the output is only returned from the last cell in the LSTM):</p>
<pre class="programlisting code"><code class="hljs-code">self.lstm = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(<span class="hljs-number">10</span>, return_sequences=<span class="hljs-literal">True</span>, 
        input_shape=(<span class="hljs-number">5</span>, <span class="hljs-number">10</span>))
)
</code></pre>
<p class="normal">The next major RNN variation we will look at is the Stateful RNN.</p>
<h2 class="heading-2" id="_idParaDest-133">Stateful RNNs</h2>
<p class="normal">RNNs can be stateful, which means that they can maintain state across batches during training. That is, the hidden<a id="_idIndexMarker501"/> state computed for a batch of training data will be used as the initial hidden state for the next batch of training data. However, this <a id="_idIndexMarker502"/>needs to be explicitly set, since TensorFlow 2.0 (<code class="inlineCode">tf.keras</code>) RNNs are stateless by default, and resets the state after each batch. Setting an RNN to be stateful means that it can build state across its training sequence and even maintain that state when doing predictions.</p>
<p class="normal">The benefits of using stateful RNNs are smaller network sizes and/or lower training times. The disadvantage is that we are now responsible for training the network with a batch size that reflects the periodicity of the data and resetting the state after each epoch. In addition, data should not be shuffled while training the network since the order in which the data is presented is relevant for stateful networks.</p>
<p class="normal">To set an RNN layer as stateful, set the named variable stateful to <code class="inlineCode">True</code>. In our example of a one-to-many topology for learning how to generate text, we provide an example of using a stateful RNN. Here, we train using data consisting of contiguous text slices, so setting the LSTM to stateful means that the hidden state generated from the previous text chunk is reused for the current text chunk.</p>
<p class="normal">In the next section on RNN topologies, we will look at different ways to set up the RNN network for different use cases.</p>
<h1 class="heading-1" id="_idParaDest-134">RNN topologies</h1>
<p class="normal">We have seen examples of <a id="_idIndexMarker503"/>how MLP and CNN architectures can be composed to form more complex networks. RNNs offer yet another degree of freedom, in that they allow sequence input and output. This means that RNN cells can be arranged in different ways to build networks that are adapted to solve different types of problems. <em class="italic">Figure 5.5</em> shows five different configurations of inputs, hidden layers, and outputs.</p>
<p class="normal">Of these, the first one (one-to-one) is not interesting from a sequence processing point of view, as it can be implemented as a simple dense network with one input and one output.</p>
<p class="normal">The one-to-many case has a single input and outputs a sequence. An example of such a network might be a network that can generate text tags from images [6], containing short text descriptions of different aspects of the image. Such a network would be trained with image input and labeled sequences of text representing the image tags:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="550" src="../Images/B18331_05_05.png" width="744"/></figure>
<p class="packt_figref">Figure 5.5: Common RNN topologies</p>
<p class="normal">The many-to-one case is<a id="_idIndexMarker504"/> the reverse; it takes a sequence of tensors as input but outputs a single tensor. Examples of such networks would be a sentiment analysis network [7], which takes as input a block of text such as a movie review and outputs a single sentiment value.</p>
<p class="normal">The many-to-many use case comes in two flavors. The first one is more popular and is better known as the seq2seq model. In this model, a sequence is read in and produces a context vector representing the input sequence, which is used to generate the output sequence.</p>
<p class="normal">The topology has been used with great success in the field of machine translation, as well as problems that can be reframed as machine translation problems. Real-life examples of the former can be found in [8, 9], and an example of the latter is described in [10].</p>
<p class="normal">The second many-to-many type has an output cell corresponding to each input cell. This kind of network is suited for use cases where there is a 1:1 correspondence between the input and output, such as time series. The major difference between this model and the seq2seq model is that the input does not have to be completely encoded before the decoding process begins.</p>
<p class="normal">In the next three sections, we<a id="_idIndexMarker505"/> provide examples of a one-to-many network that learns to generate text, a many-to-one network that does sentiment analysis, and a many-to-many network of<a id="_idIndexMarker506"/> the second type, which predicts <strong class="keyWord">Part-of-Speech</strong> (<strong class="keyWord">POS</strong>) for words in a sentence. Because of the popularity of the seq2seq network, we will cover it in more detail later in this chapter.</p>
<h2 class="heading-2" id="_idParaDest-135">Example ‒ One-to-many – Learning to generate text</h2>
<p class="normal">RNNs have been used extensively by the <strong class="keyWord">Natural Language Processing</strong> (<strong class="keyWord">NLP</strong>) community for various applications. One such<a id="_idIndexMarker507"/> application is to build language models. A<a id="_idIndexMarker508"/> language model is a model that allows us to predict the probability of a word in a text given previous words. Language models are important for various higher-level tasks such as machine translation, spelling correction, and so on.</p>
<p class="normal">The ability of a language model to predict the next word in a sequence makes it a generative model that allows us to generate text by sampling from the output probabilities of different words in the vocabulary. The training data is a sequence of words, and the label is the word appearing at the next time step in the sequence.</p>
<p class="normal">For our example, we will train a character-based RNN on the text of the children’s stories <em class="italic">Alice in Wonderland</em> and its sequel <em class="italic">Through the Looking Glass</em> by Lewis Carroll. We have chosen to build a character-based model because it has a smaller vocabulary and trains quicker. The idea is the same as training and using a word-based language model, except we will use characters instead of words. Once trained, the model can be used to generate some text in the same style.</p>
<p class="normal">The data for our example will come from the plain texts of two novels on the Project Gutenberg website [36]. Input to the network are sequences of 100 characters, and the corresponding output is another sequence of 100 characters, offset from the input by 1 position.</p>
<p class="normal">That is, if the input is the sequence [<em class="italic">c</em><sub class="subscript">1</sub>, <em class="italic">c</em><sub class="subscript">2</sub>, …,<em class="italic"> c</em><sub class="subscript">n</sub>], the output will be [<em class="italic">c</em><sub class="subscript">2</sub>, <em class="italic">c</em><sub class="subscript">3</sub>, …,<em class="italic"> c</em><sub class="subscript">n+1</sub>]. We will train the network for 50 epochs, and at the end of every 10 epochs, we will generate a fixed-size sequence of characters starting with a standard prefix. In the following example, we have used the prefix “Alice”, the name of the protagonist in our novels.</p>
<p class="normal">As always, we will first import<a id="_idIndexMarker509"/> the necessary libraries and set up some constants. Here, <code class="inlineCode">DATA_DIR</code> points to a data folder under the location where you downloaded the source code for this chapter. <code class="inlineCode">CHECKPOINT_DIR</code> is the location, a folder of checkpoints under the data folder, where we will save the weights of the model at the end of every 10 epochs:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> shutil
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
DATA_DIR = <span class="hljs-string">"./data"</span>
CHECKPOINT_DIR = os.path.join(DATA_DIR, <span class="hljs-string">"checkpoints"</span>)
</code></pre>
<p class="normal">Next, we download and prepare the data for our network to consume. The texts of both books are publicly available from the Project Gutenberg website. The <code class="inlineCode">tf.keras.utils.get_file()</code> function will check to see whether the file has already been downloaded to your local drive, and if not, it will download to a <code class="inlineCode">datasets</code> folder under the location of the code. We also preprocess the input a little here, removing newline and byte order mark characters from the text. This step will create the <code class="inlineCode">texts</code> variable, a flat list of characters for these two books:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">download_and_read</span><span class="hljs-function">(</span><span class="hljs-params">urls</span><span class="hljs-function">):</span>
    texts = []
    <span class="hljs-keyword">for</span> i, url <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(urls):
        p = tf.keras.utils.get_file(<span class="hljs-string">"</span><span class="hljs-string">ex1-{:d}.txt"</span>.<span class="hljs-built_in">format</span>(i), url,
            cache_dir=<span class="hljs-string">"."</span>)
        text = <span class="hljs-built_in">open</span>(p, <span class="hljs-string">"r"</span>).read()
        <span class="hljs-comment"># remove byte order mark</span>
        text = text.replace(<span class="hljs-string">"\ufeff"</span>, <span class="hljs-string">""</span>)
        <span class="hljs-comment"># remove newlines</span>
        text = text.replace(<span class="hljs-string">'\n'</span>, <span class="hljs-string">'</span><span class="hljs-string"> '</span>)
        text = re.sub(<span class="hljs-string">r'\s+'</span>, <span class="hljs-string">" "</span>, text)
        <span class="hljs-comment"># add it to the list</span>
        texts.extend(text)
    <span class="hljs-keyword">return</span> texts
texts = download_and_read([
    <span class="hljs-string">"http://www.gutenberg.org/cache/epub/28885/pg28885.txt"</span>,
    <span class="hljs-string">"https://www.gutenberg.org/files/12/12-0.txt"</span>
])
</code></pre>
<p class="normal">Next, we will create our vocabulary. In our case, our vocabulary contains 90 unique characters, composed of uppercase and lowercase alphabets, numbers, and special characters. We also create some mapping dictionaries to convert each vocabulary character into a unique integer and vice versa. As noted earlier, the input and output of the network is a<a id="_idIndexMarker510"/> sequence of characters. </p>
<p class="normal">However, the actual input and output of the network are sequences of integers, and we will use these mapping dictionaries to handle this conversion:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># create the vocabulary</span>
vocab = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(texts))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"vocab size: {:d}"</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(vocab)))
<span class="hljs-comment"># create mapping from vocab chars to ints</span>
char2idx = {c:i <span class="hljs-keyword">for</span> i, c <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocab)}
idx2char = {i:c <span class="hljs-keyword">for</span> c, i <span class="hljs-keyword">in</span> char2idx.items()}
</code></pre>
<p class="normal">The next step is to use these mapping dictionaries to convert our character sequence input into an integer sequence and then into a TensorFlow dataset. Each of our sequences is going to be 100 characters long, with the output being offset from the input by 1 character position. We first batch the dataset into slices of 101 characters, then apply the <code class="inlineCode">split_train_labels()</code> function to every element of the dataset to create our sequences dataset, which is a dataset of tuples of two elements, with each element of the tuple being a vector of size 100 and type <code class="inlineCode">tf.int64</code>. We then shuffle these sequences and create batches of 64 tuples for each input to our network. Each element of the dataset is now a tuple consisting of a pair of matrices, each of size (64, 100) and type <code class="inlineCode">tf.int64</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># numericize the texts</span>
texts_as_ints = np.array([char2idx[c] <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> texts])
data = tf.data.Dataset.from_tensor_slices(texts_as_ints)
<span class="hljs-comment"># number of characters to show before asking for prediction</span>
<span class="hljs-comment"># sequences: [None, 100]</span>
seq_length = <span class="hljs-number">100</span>
sequences = data.batch(seq_length + <span class="hljs-number">1</span>, drop_remainder=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">split_train_labels</span><span class="hljs-function">(</span><span class="hljs-params">sequence</span><span class="hljs-function">):</span>
    input_seq = sequence[<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>]
    output_seq = sequence[<span class="hljs-number">1</span>:]
    <span class="hljs-keyword">return</span> input_seq, output_seq
sequences = sequences.<span class="hljs-built_in">map</span>(split_train_labels)
<span class="hljs-comment"># set up for training</span>
<span class="hljs-comment"># batches: [None, 64, 100]</span>
batch_size = <span class="hljs-number">64</span>
steps_per_epoch = <span class="hljs-built_in">len</span>(texts) // seq_length // batch_size
dataset = sequences.shuffle(<span class="hljs-number">10000</span>).batch(
    batch_size, drop_remainder=<span class="hljs-literal">True</span>)
</code></pre>
<p class="normal">We are now ready to define our network. As before, we define our network as a subclass of <code class="inlineCode">tf.keras.Model</code>, as shown next. The network is fairly simple; it takes as input a sequence of integers of size 100 (<code class="inlineCode">num_timesteps</code>) and passes them through an embedding layer so that each integer in the sequence is converted into a vector of size 256 (<code class="inlineCode">embedding_dim</code>). So, assuming a batch size of 64, for our input sequence of size (64, 100), the output of the embedding layer is a matrix of shape (64, 100, 256).</p>
<p class="normal">The next layer is an RNN layer with 100 time steps. The implementation of RNN chosen is a GRU. This GRU layer will take, at each of its time steps, a vector of size (256,) and output a vector of shape (1024,) (<code class="inlineCode">rnn_output_dim</code>). Note also that the RNN is stateful, which means that the <a id="_idIndexMarker511"/>hidden state output from the previous training epoch will be used as input to the current epoch. The <code class="inlineCode">return_sequences=True</code> flag also indicates that the RNN will output at each of the time steps rather than an aggregate output at the last time steps.</p>
<p class="normal">Finally, each of the time steps will emit a vector of shape (1024,) into a dense layer that outputs a vector of shape (90,) (<code class="inlineCode">vocab_size</code>). The output from this layer will be a tensor of shape (64, 100, 90). Each position in the output vector corresponds to a character in our vocabulary, and the values correspond to the probability of that character occurring at that output position:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">CharGenModel</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, vocab_size, num_timesteps,</span>
<span class="hljs-params">            embedding_dim, **kwargs</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(CharGenModel, self).__init__(**kwargs)
        self.embedding_layer = tf.keras.layers.Embedding(
            vocab_size,
            embedding_dim
        )
        self.rnn_layer = tf.keras.layers.GRU(
            num_timesteps,
            recurrent_initializer=<span class="hljs-string">"glorot_uniform"</span>,
            recurrent_activation=<span class="hljs-string">"sigmoid"</span>,
            stateful=<span class="hljs-literal">True</span>,
            return_sequences=<span class="hljs-literal">True</span>)
        self.dense_layer = tf.keras.layers.Dense(vocab_size)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, x</span><span class="hljs-function">):</span>
        x = self.embedding_layer(x)
        x = self.rnn_layer(x)
        x = self.dense_layer(x)
        <span class="hljs-keyword">return</span> x
vocab_size = <span class="hljs-built_in">len</span>(vocab)
embedding_dim = <span class="hljs-number">256</span>
model = CharGenModel(vocab_size, seq_length, embedding_dim)
model.build(input_shape=(batch_size, seq_length))
</code></pre>
<p class="normal">Next, we define a loss function and compile our model. We will use the sparse categorical cross-entropy as our loss function because that is the standard loss function to use when our inputs and outputs are sequences of integers. For the optimizer, we will choose the Adam optimizer:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">loss</span><span class="hljs-function">(</span><span class="hljs-params">labels, predictions</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.losses.sparse_categorical_crossentropy(
        labels,
        predictions,
        from_logits=<span class="hljs-literal">True</span>
    )
model.<span class="hljs-built_in">compile</span>(optimizer=tf.optimizers.Adam(), loss=loss)
</code></pre>
<p class="normal">Normally, the character at <a id="_idIndexMarker512"/>each position of the output is found by computing the argmax of the vector at that position, that is, the character corresponding to the maximum <a id="_idIndexMarker513"/>probability value. This is known as greedy search. In the case of language models where the output of one time step becomes the input to the next time step, this can lead to a repetitive output. The two most common approaches to overcome this problem are either to sample the output randomly or to use beam search, which samples from <em class="italic">k</em> the most probable values at each time step. Here, we will use the <code class="inlineCode">tf.random.categorical()</code> function to sample the output randomly. The following function takes a string as a prefix and uses it to generate a string whose length is specified by <code class="inlineCode">num_chars_to_generate</code>. The temperature parameter is used to control the quality of the predictions. Lower values will create a more predictable output.</p>
<p class="normal">The logic follows a predictable pattern. We convert the sequence of characters in our <code class="inlineCode">prefix_string</code> into a sequence of integers, then <code class="inlineCode">expand_dims</code> to add a batch dimension so the input can be passed into our model. We then reset the state of the model. This is needed because our model is stateful, and we don’t want the hidden state of the first time step in our prediction run to be carried over from the one computed during training. We then run the input through our model and get back a prediction. This is the vector of shape (90,) representing the probabilities of each character in the vocabulary appearing at the next time step. We then reshape the prediction by removing the batch dimension and dividing by the temperature, and then randomly sampling from the vector. We then set our prediction as the input of the next time step. We repeat this for the number of characters <a id="_idIndexMarker514"/>we need to generate, converting each prediction back into character form and accumulating them in a list, and returning the list at the end of the loop:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">generate_text</span><span class="hljs-function">(</span><span class="hljs-params">model, prefix_string, char2idx, idx2char,</span>
<span class="hljs-params">        num_chars_to_generate=</span><span class="hljs-number">1000</span><span class="hljs-params">, temperature=</span><span class="hljs-number">1.0</span><span class="hljs-function">):</span>
    <span class="hljs-built_in">input</span> = [char2idx[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> prefix_string]
    <span class="hljs-built_in">input</span> = tf.expand_dims(<span class="hljs-built_in">input</span>, <span class="hljs-number">0</span>)
    text_generated = []
    model.reset_states()
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_chars_to_generate):
        preds = model(<span class="hljs-built_in">input</span>)
        preds = tf.squeeze(preds, <span class="hljs-number">0</span>) / temperature
        <span class="hljs-comment"># predict char returned by model</span>
        pred_id = tf.random.categorical(
            preds, num_samples=<span class="hljs-number">1</span>)[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>].numpy()
        text_generated.append(idx2char[pred_id])
        <span class="hljs-comment"># pass the prediction as the next input to the model</span>
        <span class="hljs-built_in">input</span> = tf.expand_dims([pred_id], <span class="hljs-number">0</span>)
    <span class="hljs-keyword">return</span> prefix_string + <span class="hljs-string">""</span>.join(text_generated)
</code></pre>
<p class="normal">Finally, we are ready to run our training and evaluation loop. As mentioned earlier, we will train our network for 50 epochs, and at every 10-epoch interval, we will try to generate some text with the model trained so far. Our prefix at each stage is the string <code class="inlineCode">"Alice "</code>. Notice that in order to accommodate a single string prefix, we save the weights after every 10 epochs and build a separate generative model with these weights but with an input shape with a batch size of 1. Here is the code to do this:</p>
<pre class="programlisting code"><code class="hljs-code">num_epochs = <span class="hljs-number">50</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs // <span class="hljs-number">10</span>):
    model.fit(
        dataset.repeat(),
        epochs=<span class="hljs-number">10</span>,
        steps_per_epoch=steps_per_epoch
        <span class="hljs-comment"># callbacks=[checkpoint_callback, tensorboard_callback]</span>
    )
    checkpoint_file = os.path.join(
        CHECKPOINT_DIR, <span class="hljs-string">"model_epoch_{:d}"</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>))
    model.save_weights(checkpoint_file)
    <span class="hljs-comment"># create generative model using the trained model so far</span>
    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)
    gen_model.load_weights(checkpoint_file)
    gen_model.build(input_shape=(<span class="hljs-number">1</span>, seq_length))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"after epoch: {:d}"</span>.<span class="hljs-built_in">format</span>(i+<span class="hljs-number">1</span>)*<span class="hljs-number">10</span>)
    <span class="hljs-built_in">print</span>(generate_text(gen_model, <span class="hljs-string">"Alice "</span>, char2idx, idx2char))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"---"</span>)
</code></pre>
<p class="normal">The output after the <a id="_idIndexMarker515"/>very first epoch of training contains words that are completely undecipherable:</p>
<pre class="programlisting con"><code class="hljs-con">Alice nIPJtce otaishein r. henipt il nn tu t hen mlPde hc efa hdtioDDeteeybeaewI teu"t e9B ce nd ageiw  eai rdoCr ohrSI ey Pmtte:vh ndte taudhor0-gu s5'ria,tr gn inoo luwomg Omke dee sdoohdn ggtdhiAoyaphotd t- kta e c t- taLurtn   hiisd tl'lpei od y' tpacoe dnlhr oG mGhod ut hlhoy .i, sseodli., ekngnhe idlue'aa'  ndti-rla nt d'eiAier adwe ai'otteniAidee hy-ouasq"plhgs tuutandhptiw  oohe.Rastnint:e,o odwsir"omGoeuall1*g taetphhitoge ds wr li,raa,  h$jeuorsu  h cidmdg't ku..n,HnbMAsn nsaathaa,' ase woe  ehf re ig"hTr ddloese eod,aed toe rh k. nalf bte seyr udG n,ug lei hn icuimty"onw Qee ivtsae zdrye g eut rthrer n sd,Zhqehd' sr caseruhel are fd yse e  kgeiiday odW-ldmkhNw endeM[harlhroa h Wydrygslsh EnilDnt e "lue "en wHeslhglidrth"ylds rln n iiato taue flitl nnyg ittlno re 'el yOkao itswnadoli'.dnd Akib-ehn hftwinh yd ee tosetf tonne.;egren t wf, ota nfsr, t&amp;he desnre e" oo fnrvnse aid na tesd is ioneetIf ·itrn tttpakihc s nih'bheY ilenf yoh etdrwdplloU ooaeedo,,dre snno'ofh o epst. lahehrw 
</code></pre>
<p class="normal">However, after about 30 epochs of training, we begin to see words that look familiar:</p>
<pre class="programlisting con"><code class="hljs-con">Alice Red Queen. He best I had defores it,' glily do flose time it makes the talking of find a hand mansed in she loweven to the rund not bright prough: the and she a chill be the sand using that whever sullusn--the dear of asker as 'IS now-- Chich the hood." "Oh!"' '_I'm num about--again was wele after a WAG LoANDE BITTER OF HSE!0 UUL EXMENN 1*.t, this wouldn't teese to Dumark THEVER Project Gutenberg-tmy of himid out flowal woulld: 'Nis song, Eftrin in pully be besoniokinote. "Com, contimemustion--of could you knowfum to hard, she can't the with talking to alfoeys distrint, for spacemark!' 'You gake to be would prescladleding readieve other togrore what it mughturied ford of it was sen!" You squs, _It I hap: But it was minute to the Kind she notion and teem what?" said Alice, make there some that in at the shills distringulf out to the Froge, and very mind to it were it?' the King was set telm, what's the old all reads talking a minuse. "Where ream put find growned his so," _you 'Fust to t
</code></pre>
<p class="normal">After 50 epochs of training, the model still has trouble expressing coherent thought but has learned to spell reasonably well. What is amazing here is that the model is character-based and<a id="_idIndexMarker516"/> has no knowledge of words, yet it learns to spell words that look like they might have come from the original text:</p>
<pre class="programlisting con"><code class="hljs-con">Alice Vex her," he prope of the very managed by this thill deceed. I will ear she a much daid. "I sha?' Nets: "Woll, I should shutpelf, and now and then, cried, How them yetains, a tround her about in a shy time, I pashng round the sandle, droug" shrees went on what he seting that," said Alice. "Was this will resant again. Alice stook of in a faid.' 'It's ale. So they wentle shall kneeltie-and which herfer--the about the heald in pum little each the UKECE P@TTRUST GITE Ever been my hever pertanced to becristrdphariok, and your pringing that why the King as I to the King remark, but very only all Project Grizly: thentiused about doment,' Alice with go ould, are wayings for handsn't replied as mave about to LISTE!' (If the UULE 'TARY-HAVE BUY DIMADEANGNE'G THING NOOT,' be this plam round an any bar here! No, you're alard to be a good aftered of the sam--I canon't?" said Alice. 'It's one eye of the olleations. Which saw do it just opened hardly deat, we hastowe. 'Of coum, is tried try slowing
</code></pre>
<p class="normal">Generating the next character or next word in the text isn’t the only thing you can do with this sort of model. Similar models have been built to make stock price predictions [3] or generate classical music [4]. Andrej Karpathy covers a few other fun examples, such as generating fake Wikipedia pages, algebraic geometry proofs, and Linux source code in his blog post [5].</p>
<p class="normal">The full code for this example is available in <code class="inlineCode">alice_text_generator.py</code> in the source code folder for this chapter. It can be run from the command line using the following command:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python alice_text_generator.py
</code></pre>
<p class="normal">Our next example will show an implementation of a many-to-one network for sentiment analysis.</p>
<h2 class="heading-2" id="_idParaDest-136">Example ‒ Many-to-one – Sentiment analysis</h2>
<p class="normal">In this example, we will use a <a id="_idIndexMarker517"/>many-to-one network that takes a sentence <a id="_idIndexMarker518"/>as input and predicts its sentiment as being either positive or negative. Our dataset is the Sentiment-labeled sentences dataset on the UCI Machine Learning Repository [20], a set of 3,000 sentences from reviews on Amazon, IMDb, and Yelp, each labeled with 0 if it expresses a negative sentiment, or 1 if it expresses a positive sentiment.</p>
<p class="normal">As usual, we will start with our imports:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> shutil
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score, confusion_matrix
</code></pre>
<p class="normal">The dataset is provided as a zip file, which expands into a folder containing three files of labeled sentences, one for each provider, with one sentence and label per line and with the sentence and label separated by the tab character. We first download the zip file, then parse the files into a list of <code class="inlineCode">(sentence, label)</code> pairs:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">download_and_read</span><span class="hljs-function">(</span><span class="hljs-params">url</span><span class="hljs-function">):</span>
    local_file = url.split(<span class="hljs-string">'/'</span>)[-<span class="hljs-number">1</span>]
    local_file = local_file.replace(<span class="hljs-string">"%20"</span>, <span class="hljs-string">" "</span>)
    p = tf.keras.utils.get_file(local_file, url,
        extract=<span class="hljs-literal">True</span>, cache_dir=<span class="hljs-string">"."</span>)
    local_folder = os.path.join(<span class="hljs-string">"</span><span class="hljs-string">datasets"</span>, local_file.split(<span class="hljs-string">'.'</span>)[<span class="hljs-number">0</span>])
    labeled_sentences = []
    <span class="hljs-keyword">for</span> labeled_filename <span class="hljs-keyword">in</span> os.listdir(local_folder):
        <span class="hljs-keyword">if</span> labeled_filename.endswith(<span class="hljs-string">"_labelled.txt"</span>):
            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(
                    local_folder, labeled_filename), <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> f:
                <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:
                    sentence, label = line.strip().split(<span class="hljs-string">'\t'</span>)
                    labeled_sentences.append((sentence, label))
    <span class="hljs-keyword">return</span> labeled_sentences
labeled_sentences = download_and_read(      
    <span class="hljs-string">"https://archive.ics.uci.edu/ml/machine-learning-databases/"</span> + 
    <span class="hljs-string">"00331/sentiment%20labelled%20sentences.zip"</span>)
sentences = [s <span class="hljs-keyword">for</span> (s, l) <span class="hljs-keyword">in</span> labeled_sentences]
labels = [<span class="hljs-built_in">int</span>(l) <span class="hljs-keyword">for</span> (s, l) <span class="hljs-keyword">in</span> labeled_sentences]
</code></pre>
<p class="normal">Our objective is to train the model so that, given a sentence as input, it learns to predict the corresponding sentiment provided in the label. Each sentence is a sequence of words. However, to input it into the model, we have to convert it into a sequence of integers. </p>
<p class="normal">Each integer in <a id="_idIndexMarker519"/>the sequence will point to a word. The mapping <a id="_idIndexMarker520"/>of integers to words for our corpus is called a vocabulary. Thus, we<a id="_idIndexMarker521"/> need to tokenize the sentences and produce a vocabulary. This is done using the following code:</p>
<pre class="programlisting code"><code class="hljs-code">tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(sentences)
vocab_size = <span class="hljs-built_in">len</span>(tokenizer.word_counts)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"vocabulary size: {:d}"</span>.<span class="hljs-built_in">format</span>(vocab_size))
word2idx = tokenizer.word_index
idx2word = {v:k <span class="hljs-keyword">for</span> (k, v) <span class="hljs-keyword">in</span> word2idx.items()}
</code></pre>
<p class="normal">Our vocabulary consists of 5,271 unique words. It is possible to make the size smaller by dropping words that occur fewer than some threshold number of times, which can be found by inspecting the <code class="inlineCode">tokenizer.word_counts</code> dictionary. In such cases, we need to add 1 to the vocabulary size for the UNK (unknown) entry, which will be used to replace every word that is not found in the vocabulary.</p>
<p class="normal">We also construct lookup dictionaries to convert from the word-to-word index and back. The first dictionary is useful during training to construct integer sequences to feed the network. The second dictionary is used to convert from the word index back into words in our prediction code later.</p>
<p class="normal">Each sentence can have a different number of words. Our model will require us to provide sequences of integers of identical length for each sentence. To support this requirement, it is common to choose a maximum sequence length that is large enough to accommodate most of the sentences in the training set. Any sentences that are shorter will be padded with zeros, and any sentences that are longer will be truncated. An easy way to choose a good value for the maximum sequence length is to look at the sentence length (as in the number of words) at different percentile positions:</p>
<pre class="programlisting code"><code class="hljs-code">seq_lengths = np.array([<span class="hljs-built_in">len</span>(s.split()) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sentences])
<span class="hljs-built_in">print</span>([(p, np.percentile(seq_lengths, p)) <span class="hljs-keyword">for</span> p
    <span class="hljs-keyword">in</span> [<span class="hljs-number">75</span>, <span class="hljs-number">80</span>, <span class="hljs-number">90</span>, <span class="hljs-number">95</span>, <span class="hljs-number">99</span>, <span class="hljs-number">100</span>]])
</code></pre>
<p class="normal">This gives us the following output:</p>
<pre class="programlisting con"><code class="hljs-con">[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]
</code></pre>
<p class="normal">As can be seen, the maximum sentence length is 71 words, but 99% of the sentences are under 36 words. If we choose a value of 64, for example, we should be able to get away with not having to truncate most of the sentences.</p>
<p class="normal">The preceding blocks <a id="_idIndexMarker522"/>of code can be run interactively multiple times <a id="_idIndexMarker523"/>to choose good values of vocabulary size and maximum sequence length respectively. In our example, we have chosen to keep all the words (so <code class="inlineCode">vocab_size = 5271</code>), and we have set our <code class="inlineCode">max_seqlen</code> to <code class="inlineCode">64</code>.</p>
<p class="normal">Our next step is to create a dataset that our model can consume. We first use our trained tokenizer to convert each sentence from a sequence of words (<code class="inlineCode">sentences</code>) into a sequence of integers (<code class="inlineCode">sentences_as_ints</code>), where each corresponding integer is the index of the word in the <code class="inlineCode">tokenizer.word_index</code>. It is then truncated and padded with zeros. </p>
<p class="normal">The labels are also converted into a NumPy array <code class="inlineCode">labels_as_ints</code>, and finally, we combine the tensors <code class="inlineCode">sentences_as_ints</code> and <code class="inlineCode">labels_as_ints</code> to form a TensorFlow dataset:</p>
<pre class="programlisting code"><code class="hljs-code">max_seqlen = <span class="hljs-number">64</span>
<span class="hljs-comment"># create dataset</span>
sentences_as_ints = tokenizer.texts_to_sequences(sentences)
sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(
    sentences_as_ints, maxlen=max_seqlen)
labels_as_ints = np.array(labels)
dataset = tf.data.Dataset.from_tensor_slices(
    (sentences_as_ints, labels_as_ints))
</code></pre>
<p class="normal">We want to set aside 1/3 of the dataset for evaluation. Of the remaining data, we will use 10% as an inline validation dataset, which the model will use to gauge its own progress during training, and the remaining as the training dataset. Finally, we create batches of 64 sentences for each dataset:</p>
<pre class="programlisting code"><code class="hljs-code">dataset = dataset.shuffle(<span class="hljs-number">10000</span>)
test_size = <span class="hljs-built_in">len</span>(sentences) // <span class="hljs-number">3</span>
val_size = (<span class="hljs-built_in">len</span>(sentences) - test_size) // <span class="hljs-number">10</span>
test_dataset = dataset.take(test_size)
val_dataset = dataset.skip(test_size).take(val_size)
train_dataset = dataset.skip(test_size + val_size)
batch_size = <span class="hljs-number">64</span>
train_dataset = train_dataset.batch(batch_size)
val_dataset = val_dataset.batch(batch_size)
test_dataset = test_dataset.batch(batch_size)
</code></pre>
<p class="normal">Next, we define our model. As you can see, the model is fairly straightforward, each input sentence is a sequence of integers of size <code class="inlineCode">max_seqlen</code> (64). This is input into an embedding layer that converts each word into a vector given by the size of the vocabulary + 1. The additional word is to account for the padding integer 0 that was introduced during the <code class="inlineCode">pad_sequences()</code> call above. The vector at each of the 64 time steps is then fed into a bidirectional LSTM layer, which converts each word into a vector of size (64,). The output <a id="_idIndexMarker524"/>of the LSTM at each time step is fed into a<a id="_idIndexMarker525"/> dense layer, which produces a vector of size (64,) with ReLU activation. The output of this dense layer is then fed into another dense layer, which outputs a vector of (1,) at each time step, modulated through a sigmoid activation.</p>
<p class="normal">The model is compiled with the binary cross-entropy loss function and the Adam optimizer, and then trained over 10 epochs:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">SentimentAnalysisModel</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, vocab_size, max_seqlen, **kwargs</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(SentimentAnalysisModel, self).__init__(**kwargs)
        self.embedding = tf.keras.layers.Embedding(
            vocab_size, max_seqlen)
        self.bilstm = tf.keras.layers.Bidirectional(
            tf.keras.layers.LSTM(max_seqlen)
        )
        self.dense = tf.keras.layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">"relu"</span>)
        self.out = tf.keras.layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">"sigmoid"</span>)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, x</span><span class="hljs-function">):</span>
        x = self.embedding(x)
        x = self.bilstm(x)
        x = self.dense(x)
        x = self.out(x)
        <span class="hljs-keyword">return</span> x
model = SentimentAnalysisModel(vocab_size+<span class="hljs-number">1</span>, max_seqlen)
model.build(input_shape=(batch_size, max_seqlen))
model.summary()
<span class="hljs-comment"># compile</span>
model.<span class="hljs-built_in">compile</span>(
    loss=<span class="hljs-string">"binary_crossentropy"</span>,
    optimizer=<span class="hljs-string">"adam"</span>,
    metrics=[<span class="hljs-string">"accuracy"</span>]
)
<span class="hljs-comment"># train</span>
data_dir = <span class="hljs-string">"./data"</span>
logs_dir = os.path.join(<span class="hljs-string">"</span><span class="hljs-string">./logs"</span>)
best_model_file = os.path.join(data_dir, <span class="hljs-string">"best_model.h5"</span>)
checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,
    save_weights_only=<span class="hljs-literal">True</span>,
    save_best_only=<span class="hljs-literal">True</span>)
tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)
num_epochs = <span class="hljs-number">10</span>
history = model.fit(train_dataset, epochs=num_epochs,
    validation_data=val_dataset,
    callbacks=[checkpoint, tensorboard])
</code></pre>
<p class="normal">As you can see from the output, the training set accuracy goes to 99.8% and the validation set accuracy goes to about 78.5%. Having a higher accuracy over the training set is expected since the<a id="_idIndexMarker526"/> model was trained on this dataset. You<a id="_idIndexMarker527"/> can also look at the following loss plot to see exactly where the model starts overfitting on the training set. Notice that the training loss keeps going down, but the validation loss comes down initially and then starts going up. It is at the point where it starts going up that we know that the model overfits on the training set:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 1/10
29/29 [==============================] - 7s 239ms/step - loss: 0.6918 - accuracy: 0.5148 - val_loss: 0.6940 - val_accuracy: 0.4750
Epoch 2/10
29/29 [==============================] - 3s 98ms/step - loss: 0.6382 - accuracy: 0.5928 - val_loss: 0.6311 - val_accuracy: 0.6000
Epoch 3/10
29/29 [==============================] - 3s 100ms/step - loss: 0.3661 - accuracy: 0.8250 - val_loss: 0.4894 - val_accuracy: 0.7600
Epoch 4/10
29/29 [==============================] - 3s 99ms/step - loss: 0.1567 - accuracy: 0.9564 - val_loss: 0.5469 - val_accuracy: 0.7750
Epoch 5/10
29/29 [==============================] - 3s 99ms/step - loss: 0.0768 - accuracy: 0.9875 - val_loss: 0.6197 - val_accuracy: 0.7450
Epoch 6/10
29/29 [==============================] - 3s 100ms/step - loss: 0.0387 - accuracy: 0.9937 - val_loss: 0.6529 - val_accuracy: 0.7500
Epoch 7/10
29/29 [==============================] - 3s 99ms/step - loss: 0.0215 - accuracy: 0.9989 - val_loss: 0.7597 - val_accuracy: 0.7550
Epoch 8/10
29/29 [==============================] - 3s 100ms/step - loss: 0.0196 - accuracy: 0.9987 - val_loss: 0.6745 - val_accuracy: 0.7450
Epoch 9/10
29/29 [==============================] - 3s 99ms/step - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.7770 - val_accuracy: 0.7500
Epoch 10/10
29/29 [==============================] - 3s 99ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.8344 - val_accuracy: 0.7450
</code></pre>
<p class="normal"><em class="italic">Figure 5.6</em> shows TensorBoard plots of accuracy and loss for the training and validation datasets:</p>
<figure class="mediaobject"><img alt="Chart, line chart, scatter chart  Description automatically generated" height="328" src="../Images/B18331_05_06.png" width="877"/></figure>
<p class="packt_figref">Figure 5.6: Accuracy and loss plots from TensorBoard for sentiment analysis network training</p>
<p class="normal">Our checkpoint callback has saved the best model based on the lowest value of validation loss, and<a id="_idIndexMarker528"/> we can now reload this for evaluation against our<a id="_idIndexMarker529"/> held out test set:</p>
<pre class="programlisting code"><code class="hljs-code">best_model = SentimentAnalysisModel(vocab_size+<span class="hljs-number">1</span>, max_seqlen)
best_model.build(input_shape=(batch_size, max_seqlen))
best_model.load_weights(best_model_file)
best_model.<span class="hljs-built_in">compile</span>(
    loss=<span class="hljs-string">"binary_crossentropy"</span>,
    optimizer=<span class="hljs-string">"adam"</span>,
    metrics=[<span class="hljs-string">"accuracy"</span>]
)
</code></pre>
<p class="normal">The easiest high-level way to evaluate a model against a dataset is to use the <code class="inlineCode">model.evaluate()</code> call:</p>
<pre class="programlisting code"><code class="hljs-code">test_loss, test_acc = best_model.evaluate(test_dataset)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"test loss: {:.3f}, test accuracy: {:.3f}"</span>.<span class="hljs-built_in">format</span>(
    test_loss, test_acc))
</code></pre>
<p class="normal">This gives us the <a id="_idIndexMarker530"/>following output:</p>
<pre class="programlisting con"><code class="hljs-con">test loss: 0.487, test accuracy: 0.782
</code></pre>
<p class="normal">We can also use <code class="inlineCode">model.predict()</code> to retrieve our predictions and compare them individually to the labels<a id="_idIndexMarker531"/> and use external tools (from scikit-learn, for example) to compute our results:</p>
<pre class="programlisting code"><code class="hljs-code">labels, predictions = [], []
idx2word[<span class="hljs-number">0</span>] = <span class="hljs-string">"PAD"</span>
is_first_batch = <span class="hljs-literal">True</span>
<span class="hljs-keyword">for</span> test_batch <span class="hljs-keyword">in</span> test_dataset:
   inputs_b, labels_b = test_batch
   pred_batch = best_model.predict(inputs_b)
   predictions.extend([(<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> p &gt; <span class="hljs-number">0.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> pred_batch])
   labels.extend([l <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> labels_b])
   <span class="hljs-keyword">if</span> is_first_batch:
       <span class="hljs-comment"># print first batch of label, prediction, and sentence</span>
       <span class="hljs-keyword">for</span> rid <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(inputs_b.shape[<span class="hljs-number">0</span>]):
           words = [idx2word[idx] <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> inputs_b[rid].numpy()]
           words = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> words <span class="hljs-keyword">if</span> w != <span class="hljs-string">"PAD"</span>]
           sentence = <span class="hljs-string">" "</span>.join(words)
           <span class="hljs-built_in">print</span>(<span class="hljs-string">"{:d}\t{:d}\t{:s}"</span>.<span class="hljs-built_in">format</span>(
               labels[rid], predictions[rid], sentence))
       is_first_batch = <span class="hljs-literal">False</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"accuracy score: {:.3f}"</span>.<span class="hljs-built_in">format</span>(accuracy_score(labels, predictions)))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"confusion matrix"</span>)
<span class="hljs-built_in">print</span>(confusion_matrix(labels, predictions))
</code></pre>
<p class="normal">For the first batch of 64 sentences in our test dataset, we reconstruct the sentence and display the label (first column) as well as the prediction from the model (second column). Here, we show the top 10 sentences. As you can see, the model gets it right for most sentences on this list:</p>
<pre class="programlisting con"><code class="hljs-con">LBL  PRED  SENT
1     1    one of my favorite purchases ever
1     1    works great
1     1    our waiter was very attentive friendly and informative
0     0    defective crap
0     1    and it was way to expensive
0     0    don't waste your money
0     0    friend's pasta also bad he barely touched it
1     1    it's a sad movie but very good
0     0    we recently witnessed her poor quality of management towards other guests as well
0     1    there is so much good food in vegas that i feel cheated for wasting an eating opportunity by going to rice and company
</code></pre>
<p class="normal">We also report the results across all sentences in the test dataset. As you can see, the test accuracy is the same<a id="_idIndexMarker532"/> as that reported by the <code class="inlineCode">evaluate</code> call. We have <a id="_idIndexMarker533"/>also generated the confusion matrix, which shows that out of 1,000 test examples, our sentiment analysis network predicted correctly 782 times and incorrectly 218 times:</p>
<pre class="programlisting con"><code class="hljs-con">accuracy score: 0.782
confusion matrix
[[391  97]
 [121 391]]
</code></pre>
<p class="normal">The full code for this example is available in <code class="inlineCode">lstm_sentiment_analysis.py</code> in the source code folder for this chapter. It can be run from the command line using the following command:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python lstm_sentiment_analysis.py
</code></pre>
<p class="normal">Our next example will describe a many-to-many network trained for POS tagging English text.</p>
<h2 class="heading-2" id="_idParaDest-137">Example ‒ Many-to-many – POS tagging</h2>
<p class="normal">In this example, we will use a GRU layer to<a id="_idIndexMarker534"/> build a network that does <strong class="keyWord">Part of Speech</strong> (<strong class="keyWord">POS</strong>) tagging. A POS is a grammatical <a id="_idIndexMarker535"/>category of words that are used in the <a id="_idIndexMarker536"/>same way across multiple sentences. Examples of POS are nouns, verbs, adjectives, and so on. For example, nouns are typically used to identify things, verbs are typically used to identify what they do, and adjectives are used to describe attributes of these things. POS tagging used to be done manually in the past, but this is now mostly a solved problem, initially through statistical models, and more recently by using deep learning models in an end-to-end manner, as described in Collobert, et al. [21].</p>
<p class="normal">For our training data, we will need sentences tagged with POS tags. The Penn Treebank [22] is one such dataset; it is a human-annotated corpus of about 4.5 million words of American English. However, it is a<a id="_idIndexMarker537"/> non-free resource. A 10% sample of the Penn Treebank is freely <a id="_idIndexMarker538"/>available as part of NLTK [23], which we will use to train our network.</p>
<p class="normal">Our model will take a sequence of words in a sentence as input, then will output the corresponding POS tag for <a id="_idIndexMarker539"/>each word. Thus, for an input sequence consisting of the words [The, cat, sat. on, the, mat, .], the output sequence should be the POS symbols <code class="inlineCode">[DT, NN, VB, IN, DT, NN, .]</code>.</p>
<p class="normal">In order to get the data, you need to install the NLTK library if it is not already installed (NLTK is included in the Anaconda distribution), as well as the 10% treebank dataset (not installed by default). To install NLTK, follow the steps on the NLTK install page [23]. To install the treebank dataset, perform the following in the Python REPL:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span> nltk
<span class="hljs-con-meta">&gt;&gt;&gt;</span> nltk.download("treebank")
</code></pre>
<p class="normal">Once this is done, we are ready to build our network. As usual, we will start by importing the necessary packages:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> shutil
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
<p class="normal">We will lazily import the NLTK treebank dataset into a pair of parallel flat files, one containing the sentences and the other containing a corresponding <strong class="keyWord">POS</strong> sequence:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">download_and_read</span><span class="hljs-function">(</span><span class="hljs-params">dataset_dir, num_pairs=</span><span class="hljs-literal">None</span><span class="hljs-function">):</span>
    sent_filename = os.path.join(dataset_dir, <span class="hljs-string">"treebank-sents.txt"</span>)
    poss_filename = os.path.join(dataset_dir, <span class="hljs-string">"treebank-poss.txt"</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span>(os.path.exists(sent_filename) <span class="hljs-keyword">and</span> os.path.exists(poss_filename)):
        <span class="hljs-keyword">import</span> nltk   
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(dataset_dir):
            os.makedirs(dataset_dir)
        fsents = <span class="hljs-built_in">open</span>(sent_filename, <span class="hljs-string">"w"</span>)
        fposs = <span class="hljs-built_in">open</span>(poss_filename, <span class="hljs-string">"w"</span>)
        sentences = nltk.corpus.treebank.tagged_sents()
        <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences:
            fsents.write(<span class="hljs-string">" "</span>.join([w <span class="hljs-keyword">for</span> w, p <span class="hljs-keyword">in</span> sent]) + <span class="hljs-string">"\n"</span>)
            fposs.write(<span class="hljs-string">" "</span>.join([p <span class="hljs-keyword">for</span> w, p <span class="hljs-keyword">in</span> sent]) + <span class="hljs-string">"\n"</span>)
        fsents.close()
        fposs.close()
    sents, poss = [], []
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(sent_filename, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> fsent:
        <span class="hljs-keyword">for</span> idx, line <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(fsent):
            sents.append(line.strip())
            <span class="hljs-keyword">if</span> num_pairs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> idx &gt;= num_pairs:
                <span class="hljs-keyword">break</span>
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(poss_filename, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> fposs:
        <span class="hljs-keyword">for</span> idx, line <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(fposs):
            poss.append(line.strip())
            <span class="hljs-keyword">if</span> num_pairs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> idx &gt;= num_pairs:
                <span class="hljs-keyword">break</span>
    <span class="hljs-keyword">return</span> sents, poss
sents, poss = download_and_read(<span class="hljs-string">"./datasets"</span>)
<span class="hljs-keyword">assert</span>(<span class="hljs-built_in">len</span>(sents) == <span class="hljs-built_in">len</span>(poss))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"# of records: {:d}"</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(sents)))
</code></pre>
<p class="normal">There are 3,194<a id="_idIndexMarker540"/> sentences in our dataset. The preceding code <a id="_idIndexMarker541"/>writes the sentences and corresponding tags into<a id="_idIndexMarker542"/> parallel files, that is, line 1 in <code class="inlineCode">treebank-sents.txt</code> contains the first sentence, and line 1 in <code class="inlineCode">treebank-poss.txt</code> contains the corresponding POS tags for each word in the sentence. <em class="italic">Table 5.1</em> shows two sentences from this dataset and their corresponding POS tags:</p>
<table class="table-container" id="table001-2">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Sentences</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">POS Tags</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.</p>
</td>
<td class="table-cell">
<p class="normal">NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group.</p>
</td>
<td class="table-cell">
<p class="normal">NNP NNP VBZ NN IN NNP NNP , DT NNP VBG NN.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 5.1: Sentences and their corresponding POS tags</p>
<p class="normal">We will then use the TensorFlow (<code class="inlineCode">tf.keras</code>) tokenizer to tokenize the sentences and create a list of sentence tokens. We reuse the same infrastructure to tokenize the POS, although we could have simply split on spaces. Each input record to the network is currently a sequence of text tokens, but they need to be a sequence of integers. During the tokenizing process, the Tokenizer also maintains the tokens in the vocabulary, from which we can build <a id="_idIndexMarker543"/>mappings from the token to the integer and back.</p>
<p class="normal">We have two vocabularies<a id="_idIndexMarker544"/> to consider, the vocabulary of word tokens in the sentence collection and the vocabulary of POS tags in the part-of-speech collection. The <a id="_idIndexMarker545"/>following code shows how to tokenize both collections and generate the necessary mapping dictionaries:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">tokenize_and_build_vocab</span><span class="hljs-function">(</span><span class="hljs-params">texts, vocab_size=</span><span class="hljs-literal">None</span><span class="hljs-params">, lower=</span><span class="hljs-literal">True</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">if</span> vocab_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)
    <span class="hljs-keyword">else</span>:
        tokenizer = tf.keras.preprocessing.text.Tokenizer(
            num_words=vocab_size+<span class="hljs-number">1</span>, oov_token=<span class="hljs-string">"UNK"</span>, lower=lower)
    tokenizer.fit_on_texts(texts)
    <span class="hljs-keyword">if</span> vocab_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        <span class="hljs-comment"># additional workaround, see issue 8092</span>
        <span class="hljs-comment"># https://github.com/keras-team/keras/issues/8092</span>
        tokenizer.word_index = {e:i <span class="hljs-keyword">for</span> e, i <span class="hljs-keyword">in</span>
            tokenizer.word_index.items() <span class="hljs-keyword">if</span> 
            i &lt;= vocab_size+<span class="hljs-number">1</span> }
    word2idx = tokenizer.word_index
    idx2word = {v:k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word2idx.items()}
    <span class="hljs-keyword">return</span> word2idx, idx2word, tokenizer
word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(
    sents, vocab_size=<span class="hljs-number">9000</span>)
word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(
    poss, vocab_size=<span class="hljs-number">38</span>, lower=<span class="hljs-literal">False</span>)
source_vocab_size = <span class="hljs-built_in">len</span>(word2idx_s)
target_vocab_size = <span class="hljs-built_in">len</span>(word2idx_t)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"vocab sizes (source): {:d}, (target): {:d}"</span>.<span class="hljs-built_in">format</span>(
    source_vocab_size, target_vocab_size))
</code></pre>
<p class="normal">Our sentences are going to be of different lengths, although the number of tokens in a sentence and their corresponding POS tag sequence are the same. The network expects the input to have the same length, so we have to decide how much to make our sentence length. The following (throwaway) code computes various percentiles and prints sentence lengths at these percentiles to the console:</p>
<pre class="programlisting code"><code class="hljs-code">sequence_lengths = np.array([<span class="hljs-built_in">len</span>(s.split()) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sents])
<span class="hljs-built_in">print</span>([(p, np.percentile(sequence_lengths, p))
    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> [<span class="hljs-number">75</span>, <span class="hljs-number">80</span>, <span class="hljs-number">90</span>, <span class="hljs-number">95</span>, <span class="hljs-number">99</span>, <span class="hljs-number">100</span>]])
[(<span class="hljs-number">75</span>, <span class="hljs-number">33.0</span>), (<span class="hljs-number">80</span>, <span class="hljs-number">35.0</span>), (<span class="hljs-number">90</span>, <span class="hljs-number">41.0</span>), (<span class="hljs-number">95</span>, <span class="hljs-number">47.0</span>), (<span class="hljs-number">99</span>, <span class="hljs-number">58.0</span>), (<span class="hljs-number">100</span>, <span class="hljs-number">271.0</span>)]
</code></pre>
<p class="normal">We see that we could probably get away with setting the sentence length to around 100 and have a few truncated sentences as a result. Sentences shorter than our selected length will be padded at the end. Because our dataset is small, we prefer to use as much of it as possible, so we end up choosing the maximum length.</p>
<p class="normal">The next step is to <a id="_idIndexMarker546"/>create the dataset from our inputs. First, we<a id="_idIndexMarker547"/> have to convert our sequence of tokens and POS tags in our input and output sequences into sequences of integers. Second, we have <a id="_idIndexMarker548"/>to pad shorter sequences to the maximum length of 271. Notice that we do an additional operation on the POS tag sequences after padding, rather than keep it as a sequence of integers; we convert it into a sequence of one-hot encodings using the <code class="inlineCode">to_categorical()</code> function. TensorFlow 2.0 does provide loss functions to handle outputs as a sequence of integers, but we want to keep our code as simple as possible, so we opt to do the conversion ourselves. Finally, we use the <code class="inlineCode">from_tensor_slices()</code> function to create our dataset, shuffle it, and split it up into training, validation, and test sets:</p>
<pre class="programlisting code"><code class="hljs-code">max_seqlen = <span class="hljs-number">271</span>
<span class="hljs-comment"># convert sentences to sequence of integers</span>
sents_as_ints = tokenizer_s.texts_to_sequences(sents)
sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(
    sents_as_ints, maxlen=max_seqlen, padding=<span class="hljs-string">"</span><span class="hljs-string">post"</span>)
<span class="hljs-comment"># convert POS tags to sequence of (categorical) integers</span>
poss_as_ints = tokenizer_t.texts_to_sequences(poss)
poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(
    poss_as_ints, maxlen=max_seqlen, padding=<span class="hljs-string">"post"</span>)
poss_as_catints = []
<span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> poss_as_ints:
    poss_as_catints.append(tf.keras.utils.to_categorical(p,
        num_classes=target_vocab_size+<span class="hljs-number">1</span>, dtype=<span class="hljs-string">"int32"</span>))
poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(
    poss_as_catints, maxlen=max_seqlen)
dataset = tf.data.Dataset.from_tensor_slices(
    (sents_as_ints, poss_as_catints))
idx2word_s[<span class="hljs-number">0</span>], idx2word_t[<span class="hljs-number">0</span>] = "PAD", "PAD"
<span class="hljs-comment"># split into training, validation, and test datasets</span>
dataset = dataset.shuffle(<span class="hljs-number">10000</span>)
test_size = <span class="hljs-built_in">len</span>(sents) // <span class="hljs-number">3</span>
val_size = (<span class="hljs-built_in">len</span>(sents) - test_size) // <span class="hljs-number">10</span>
test_dataset = dataset.take(test_size)
val_dataset = dataset.skip(test_size).take(val_size)
train_dataset = dataset.skip(test_size + val_size)
<span class="hljs-comment"># create batches</span>
batch_size = <span class="hljs-number">128</span>
train_dataset = train_dataset.batch(batch_size)
val_dataset = val_dataset.batch(batch_size)
test_dataset = test_dataset.batch(batch_size)
</code></pre>
<p class="normal">Next, we will define our model and instantiate it. Our model is a sequential model consisting of an embedding layer, a dropout layer, a bidirectional GRU layer, a dense layer, and a softmax <a id="_idIndexMarker549"/>activation layer. The input is a batch of integer sequences with shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">max_seqlen</code>). When passed through the <a id="_idIndexMarker550"/>embedding layer, each integer in the sequence is converted into a vector of size (<code class="inlineCode">embedding_dim</code>), so now the shape of our tensor is (<code class="inlineCode">batch_size</code>, <code class="inlineCode">max_seqlen</code>, <code class="inlineCode">embedding_dim</code>). Each of these vectors is passed to corresponding time steps of a bidirectional GRU with an output dimension of 256. </p>
<p class="normal">Because the GRU is<a id="_idIndexMarker551"/> bidirectional, this is equivalent to stacking one GRU on top of the other, so the tensor that comes out of the bidirectional GRU has the dimension (<code class="inlineCode">batch_size</code>, <code class="inlineCode">max_seqlen</code>, <code class="inlineCode">2*rnn_output_dimension</code>). Each time step tensor of shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">1</code>, <code class="inlineCode">2*rnn_output_dimension</code>) is fed into a dense layer, which converts each time step into a vector of the same size as the target vocabulary, that is, (<code class="inlineCode">batch_size</code>, <code class="inlineCode">number_of_timesteps</code>, <code class="inlineCode">output_vocab_size</code>). Each time step represents a probability distribution of output tokens, so the final softmax layer is applied to each time step to return a sequence of output POS tokens.</p>
<p class="normal">Finally, we declare the model with some parameters, then compile it with the Adam optimizer, the categorical cross-entropy loss function, and accuracy as the metric:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">POSTaggingModel</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, source_vocab_size, target_vocab_size,</span>
<span class="hljs-params">            embedding_dim, max_seqlen, rnn_output_dim, **kwargs</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(POSTaggingModel, self).__init__(**kwargs)
        self.embed = tf.keras.layers.Embedding(
            source_vocab_size, embedding_dim, input_length=max_seqlen)
        self.dropout = tf.keras.layers.SpatialDropout1D(<span class="hljs-number">0.2</span>)
        self.rnn = tf.keras.layers.Bidirectional(
            tf.keras.layers.GRU(rnn_output_dim, return_sequences=<span class="hljs-literal">True</span>))
        self.dense = tf.keras.layers.TimeDistributed(
            tf.keras.layers.Dense(target_vocab_size))
        self.activation = tf.keras.layers.Activation(<span class="hljs-string">"softmax"</span>)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, x</span><span class="hljs-function">):</span>
        x = self.embed(x)
        x = self.dropout(x)
        x = self.rnn(x)
        x = self.dense(x)
        x = self.activation(x)
        <span class="hljs-keyword">return</span> x
embedding_dim = <span class="hljs-number">128</span>
rnn_output_dim = <span class="hljs-number">256</span>
model = POSTaggingModel(source_vocab_size, target_vocab_size,
    embedding_dim, max_seqlen, rnn_output_dim)
model.build(input_shape=(batch_size, max_seqlen))
model.summary()
model.<span class="hljs-built_in">compile</span>(
    loss=<span class="hljs-string">"categorical_crossentropy"</span>,
    optimizer=<span class="hljs-string">"adam"</span>,
    metrics=[<span class="hljs-string">"accuracy"</span>, masked_accuracy()])
</code></pre>
<p class="normal">Observant readers might have noticed an additional <code class="inlineCode">masked_accuracy()</code> metric next to the <code class="inlineCode">accuracy</code> metric in the preceding code snippet. Because of the padding, there are a lot of zeros on both<a id="_idIndexMarker552"/> the label and the prediction, as a result <a id="_idIndexMarker553"/>of which the accuracy numbers are very optimistic. In fact, the validation<a id="_idIndexMarker554"/> accuracy reported at the end of the very first epoch is <code class="inlineCode">0.9116</code>. However, the quality of POS tags generated is very poor.</p>
<p class="normal">Perhaps the best approach is to replace the current loss function with one that ignores any matches where both numbers are zero; however, a simpler approach is to build a stricter metric and use that to judge when to stop the training. Accordingly, we build a new accuracy function <code class="inlineCode">masked_accuracy()</code> whose code is shown as follows:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">masked_accuracy</span><span class="hljs-function">():</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">masked_accuracy_fn</span><span class="hljs-function">(</span><span class="hljs-params">ytrue, ypred</span><span class="hljs-function">):</span>
        ytrue = tf.keras.backend.argmax(ytrue, axis=-<span class="hljs-number">1</span>)
        ypred = tf.keras.backend.argmax(ypred, axis=-<span class="hljs-number">1</span>)
        mask = tf.keras.backend.cast(
            tf.keras.backend.not_equal(ypred, <span class="hljs-number">0</span>), tf.int32)
        matches = tf.keras.backend.cast(
            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask
        numer = tf.keras.backend.<span class="hljs-built_in">sum</span>(matches)
        denom = tf.keras.backend.maximum(tf.keras.backend.<span class="hljs-built_in">sum</span>(mask), <span class="hljs-number">1</span>)
        accuracy =  numer / denom
        <span class="hljs-keyword">return</span> accuracy
    <span class="hljs-keyword">return</span> masked_accuracy_fn
</code></pre>
<p class="normal">We are now ready to <a id="_idIndexMarker555"/>train our model. As usual, we set up the <a id="_idIndexMarker556"/>model checkpoint and TensorBoard<a id="_idIndexMarker557"/> callbacks, and then call the <code class="inlineCode">fit()</code> convenience method on the model to train the model with a batch size of 128 for 50 epochs:</p>
<pre class="programlisting code"><code class="hljs-code">num_epochs = <span class="hljs-number">50</span>
best_model_file = os.path.join(data_dir, <span class="hljs-string">"best_model.h5"</span>)
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    best_model_file,
    save_weights_only=<span class="hljs-literal">True</span>,
    save_best_only=<span class="hljs-literal">True</span>)
tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)
history = model.fit(train_dataset,
    epochs=num_epochs,
    validation_data=val_dataset,
    callbacks=[checkpoint, tensorboard])
</code></pre>
<p class="normal">A truncated output of the training is shown as follows. As you can see, the <code class="inlineCode">masked_accuracy</code> and <code class="inlineCode">val_masked_accuracy</code> numbers seem more conservative than the <code class="inlineCode">accuracy</code> and <code class="inlineCode">val_accuracy</code> numbers. This is because the masked versions do not consider the sequence positions where the input is a PAD character:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 1/50
19/19 [==============================] - 8s 431ms/step - loss: 1.4363 - accuracy: 0.7511 - masked_accuracy_fn: 0.00
38 - val_loss: 0.3219 - val_accuracy: 0.9116 - val_masked_accuracy_fn: 0.5833
Epoch 2/50
19/19 [==============================] - 6s 291ms/step - loss: 0.3278 - accuracy: 0.9183 - masked_accuracy_fn: 0.17
12 - val_loss: 0.3289 - val_accuracy: 0.9209 - val_masked_accuracy_fn: 0.1357
Epoch 3/50
19/19 [==============================] - 6s 292ms/step - loss: 0.3187 - accuracy: 0.9242 - masked_accuracy_fn: 0.1615 - val_loss: 0.3131 - val_accuracy: 0.9186 - val_masked_accuracy_fn: 0.2236
Epoch 4/50
19/19 [==============================] - 6s 293ms/step - loss: 0.3037 - accuracy: 0.9186 - masked_accuracy_fn: 0.1831 - val_loss: 0.2933 - val_accuracy: 0.9129 - val_masked_accuracy_fn: 0.1062
Epoch 5/50
19/19 [==============================] - 6s 294ms/step - loss: 0.2739 - accuracy: 0.9182 - masked_accuracy_fn: 0.1054 - val_loss: 0.2608 - val_accuracy: 0.9230 - val_masked_accuracy_fn: 0.1407
...
Epoch 45/50
19/19 [==============================] - 6s 292ms/step - loss: 0.0653 - accuracy: 0.9810 - masked_accuracy_fn: 0.7872 - val_loss: 0.1545 - val_accuracy: 0.9611 - val_masked_accuracy_fn: 0.5407
Epoch 46/50
19/19 [==============================] - 6s 291ms/step - loss: 0.0640 - accuracy: 0.9815 - masked_accuracy_fn: 0.7925 - val_loss: 0.1550 - val_accuracy: 0.9616 - val_masked_accuracy_fn: 0.5441
Epoch 47/50
19/19 [==============================] - 6s 291ms/step - loss: 0.0619 - accuracy: 0.9818 - masked_accuracy_fn: 0.7971 - val_loss: 0.1497 - val_accuracy: 0.9614 - val_masked_accuracy_fn: 0.5535
Epoch 48/50
19/19 [==============================] - 6s 292ms/step - loss: 0.0599 - accuracy: 0.9825 - masked_accuracy_fn: 0.8033 - val_loss: 0.1524 - val_accuracy: 0.9616 - val_masked_accuracy_fn: 0.5579
Epoch 49/50
19/19 [==============================] - 6s 293ms/step - loss: 0.0585 - accuracy: 0.9830 - masked_accuracy_fn: 0.8092 - val_loss: 0.1544 - val_accuracy: 0.9617 - val_masked_accuracy_fn: 0.5621
Epoch 50/50
19/19 [==============================] - 6s 291ms/step - loss: 0.0575 - accuracy: 0.9833 - masked_accuracy_fn: 0.8140 - val_loss: 0.1569 - val_accuracy: 0.9615 - val_masked_accuracy_fn: 0.5511
11/11 [==============================] - 2s 170ms/step - loss: 0.1436 - accuracy: 0.9637 - masked_accuracy_fn: 0.5786
test loss: 0.144, test accuracy: 0.963, masked test accuracy: 0.578
</code></pre>
<p class="normal">Here are some <a id="_idIndexMarker558"/>examples of POS tags generated for some<a id="_idIndexMarker559"/> random sentences in the test set, shown together <a id="_idIndexMarker560"/>with the POS tags in the corresponding ground truth sentences. As you can see, while the metric values are not perfect, it seems to have learned to do POS tagging fairly well:</p>
<pre class="programlisting con"><code class="hljs-con">labeled  : among/IN segments/NNS that/WDT t/NONE 1/VBP continue/NONE 2/TO to/VB operate/RB though/DT the/NN company/POS 's/NN steel/NN division/VBD continued/NONE 3/TO to/VB suffer/IN from/JJ soft/NN demand/IN for/PRP its/JJ tubular/NNS goods/VBG serving/DT the/NN oil/NN industry/CC and/JJ other/NNS
predicted: among/IN segments/NNS that/WDT t/NONE 1/NONE continue/NONE 2/TO to/VB operate/IN though/DT the/NN company/NN 's/NN steel/NN division/NONE continued/NONE 3/TO to/IN suffer/IN from/IN soft/JJ demand/NN for/IN its/JJ tubular/NNS goods/DT serving/DT the/NNP oil/NN industry/CC and/JJ other/NNS
labeled  : as/IN a/DT result/NN ms/NNP ganes/NNP said/VBD 0/NONE t/NONE 2/PRP it/VBZ is/VBN believed/IN that/JJ little/CC or/DT no/NN sugar/IN from/DT the/CD 1989/NN 90/VBZ crop/VBN has/VBN been/NONE shipped/RB 1/RB yet/IN even/DT though/NN the/NN crop/VBZ year/CD is/NNS six/JJ
predicted: as/IN a/DT result/NN ms/IN ganes/NNP said/VBD 0/NONE t/NONE 2/PRP it/VBZ is/VBN believed/NONE that/DT little/NN or/DT no/NN sugar/IN from/DT the/DT 1989/CD 90/NN crop/VBZ has/VBN been/VBN shipped/VBN 1/RB yet/RB even/IN though/DT the/NN crop/NN year/NN is/JJ
labeled  : in/IN the/DT interview/NN at/IN headquarters/NN yesterday/NN afternoon/NN both/DT men/NNS exuded/VBD confidence/NN and/CC seemed/VBD 1/NONE to/TO work/VB well/RB together/RB
predicted: in/IN the/DT interview/NN at/IN headquarters/NN yesterday/NN afternoon/NN both/DT men/NNS exuded/NNP confidence/NN and/CC seemed/VBD 1/NONE to/TO work/VB well/RB together/RB
labeled  : all/DT came/VBD from/IN cray/NNP research/NNP
predicted: all/NNP came/VBD from/IN cray/NNP research/NNP
labeled  : primerica/NNP closed/VBD at/IN 28/CD 25/NONE u/RB down/CD 50/NNS
predicted: primerica/NNP closed/VBD at/CD 28/CD 25/CD u/CD down/CD
</code></pre>
<p class="normal">If you would like to run<a id="_idIndexMarker561"/> this code yourself, you can find the code in the code folder for this chapter. To run it from the command line, enter the following command. The output is written to the console:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python gru_pos_tagger.py
</code></pre>
<p class="normal">Now that we have seen some examples of three common RNN network topologies, let’s explore the most<a id="_idIndexMarker562"/> popular of them all – the seq2seq model, which is also known as the recurrent encoder-decoder architecture.</p>
<h1 class="heading-1" id="_idParaDest-138">Encoder-decoder architecture – seq2seq</h1>
<p class="normal">The example of a many-to-many network<a id="_idIndexMarker563"/> we just saw was mostly similar to the many-to-one network. The one important difference was that the RNN returns outputs at each time step instead of a single combined output at the end. One other noticeable feature was that the number of input time steps was equal to<a id="_idIndexMarker564"/> the number of output time steps. As you learn about the encoder-decoder architecture, which is the “other,” and arguably more popular, style of a many-to-many network, you will notice another difference – the output is in line with the input in a many-to-many network, that is, it is not necessary for the network to wait until all of the input is consumed before generating the output.</p>
<p class="normal">The encoder-decoder architecture is also called a seq2seq model. As the name implies, the network is composed of an encoder and a decoder part, both RNN-based and capable of consuming and returning sequences of outputs corresponding to multiple time steps. The biggest application of the seq2seq network has been in neural machine translation, although it is equally applicable for problems that can be roughly structured as translation problems. Some examples are sentence parsing [10] and image captioning [24]. The seq2seq model has also been used for time series analysis [25] and question answering.</p>
<p class="normal">In the seq2seq model, the encoder consumes the source sequence, which is a batch of integer sequences. The length of the sequence is the number of input time steps, which corresponds to the maximum input sequence length (padded or truncated as necessary). Thus, the dimensions of the input tensor are (<code class="inlineCode">batch_size</code>, <code class="inlineCode">number_of_encoder_timesteps</code>). This is passed into an embedding layer, which will convert the integer at each time step into an embedding vector. The output of the embedding is a tensor of shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">number_of_encoder_timesteps</code>, <code class="inlineCode">encoder_embedding_dim</code>).</p>
<p class="normal">This tensor is fed into an RNN, which converts the vector at each time step into the size corresponding to its encoding dimension. This vector is a combination of the current time step and all previous time steps. Typically, the encoder will return the output at the last time step, representing the context or “thought” vector for the entire sequence. This tensor has the shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">encoder_rnn_dim</code>).</p>
<p class="normal">The decoder network has a similar architecture as the encoder, except there is an additional dense layer at each time step to convert the output. The input to each time step on the decoder side is the hidden state of the previous time step and the input vector that is the token predicted by the decoder of the previous time step. For the very first time step, the hidden state is the context vector from the encoder, and the input vector corresponds to the<a id="_idIndexMarker565"/> token that will initiate sequence generation on the target side. For the translation <a id="_idIndexMarker566"/>use case, for <a id="_idIndexMarker567"/>example, it is a <strong class="keyWord">beginning-of-string</strong> (<strong class="keyWord">BOS</strong>) pseudo-token. The shape of the hidden signal is (<code class="inlineCode">batch_size</code>, <code class="inlineCode">encoder_rnn_dim</code>) and the shape of the input signal across all time steps is (<code class="inlineCode">batch_size</code>, <code class="inlineCode">number_of_decoder_timesteps</code>). </p>
<p class="normal">Once it passes through the embedding layer, the output tensor shape is (<code class="inlineCode">batch_size</code>, <code class="inlineCode">number_of_decoder_timesteps</code>, <code class="inlineCode">decoder_embedding_dim</code>). The next step is the decoder RNN layer, the output of which is a tensor of shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">number_of_decoder_timesteps</code>, <code class="inlineCode">decoder_rnn_dim</code>). The output at each time step is then sent through a dense layer, which converts the vector into the size of the target vocabulary, so the output of the dense layer is (<code class="inlineCode">batch_size</code>, <code class="inlineCode">number_of_decoder_timesteps</code>, <code class="inlineCode">output_vocab_size</code>). This is basically a probability distribution over tokens at each time step, so if we compute the argmax over the last dimension, we can convert it back into a predicted sequence of tokens in the target language. <em class="italic">Figure 5.7</em> shows a high-level view of the seq2seq architecture:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="265" src="../Images/B18331_05_07.png" width="790"/></figure>
<p class="packt_figref">Figure 5.7: Seq2seq network data flow. Image Source: Artur Suilin [25]</p>
<p class="normal">In the next section, we will look at an example of a seq2seq network for machine translation.</p>
<h2 class="heading-2" id="_idParaDest-139">Example ‒ seq2seq without attention for machine translation</h2>
<p class="normal">To understand the seq2seq model in greater detail, we will look at an example of one that learns how to translate <a id="_idIndexMarker568"/>from English to French using the French-English bilingual dataset from the Tatoeba Project (1997-2019) [26]. The dataset contains approximately 167,000 sentence pairs. To make our training go faster, we will only consider the first 30,000 sentence pairs for our training.</p>
<p class="normal">As always, we will start with the imports:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> shutil
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> unicodedata
<span class="hljs-keyword">from</span> nltk.translate.bleu_score <span class="hljs-keyword">import</span> sentence_bleu, SmoothingFunction
</code></pre>
<p class="normal">The data is provided as a remote zip file. The easiest way to access the file is to download it from <a href="http://www.manythings.org/anki/fra-eng.zip"><span class="url">http://www.manythings.org/anki/fra-eng.zip</span></a> and expand it locally using unzip. The zip file contains a tab-separated file called <code class="inlineCode">fra.txt</code>, with French and English sentence pairs separated by a tab, one pair per line. The code expects the <code class="inlineCode">fra.txt</code> file in a dataset folder in the same directory as itself. We want to extract three different datasets from it.</p>
<p class="normal">If you recall the structure of the seq2seq network, the input to the encoder is a sequence of English words. On the decoder side, the input is a set of French words, and the output is the sequence of French words offset by one time step. The following function will download the zip file, expand it, and create the datasets described before.</p>
<p class="normal">The input is preprocessed to <em class="italic">asciify</em> the characters, separate out specific punctuations from their neighboring word, and remove all characters other than alphabets and these specific punctuation symbols. Finally, the sentences are converted into lowercase. Each English sentence is just converted into a single sequence of words. Each French sentence is converted into two sequences, one preceded by the BOS pseudo-word and the other followed <a id="_idIndexMarker569"/>by the <strong class="keyWord">end-of-sentence</strong> (<strong class="keyWord">EOS</strong>) pseudo-word.</p>
<p class="normal">The first sequence starts at position 0 and stops one short of the final word in the sentence, and the second sequence starts at position 1 and goes all the way to the end of the sentence:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">preprocess_sentence</span><span class="hljs-function">(</span><span class="hljs-params">sent</span><span class="hljs-function">):</span>
    sent = <span class="hljs-string">""</span>.join([c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> unicodedata.normalize(<span class="hljs-string">"NFD"</span>, sent)
        <span class="hljs-keyword">if</span> unicodedata.category(c) != <span class="hljs-string">"Mn"</span>])
    sent = re.sub(<span class="hljs-string">r"([!.?])"</span>, <span class="hljs-string">r" \1"</span>, sent)
    sent = re.sub(<span class="hljs-string">r"[^a-zA-Z!.?]+"</span>, <span class="hljs-string">r" "</span>, sent)
    sent = re.sub(<span class="hljs-string">r"\s+"</span>, <span class="hljs-string">" "</span>, sent)
    sent = sent.lower()
    <span class="hljs-keyword">return</span> sent
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">download_and_read</span><span class="hljs-function">():</span>
    en_sents, fr_sents_in, fr_sents_out = [], [], []
    local_file = os.path.join(<span class="hljs-string">"datasets"</span>, <span class="hljs-string">"fra.txt"</span>)
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(local_file, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> fin:
        <span class="hljs-keyword">for</span> i, line <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(fin):
            en_sent, fr_sent = line.strip().split(<span class="hljs-string">'\t'</span>)
            en_sent = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> preprocess_sentence(en_sent).split()]
            fr_sent = preprocess_sentence(fr_sent)
            fr_sent_in = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> (<span class="hljs-string">"BOS "</span> + fr_sent).split()]
            fr_sent_out = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> (fr_sent + <span class="hljs-string">"</span><span class="hljs-string"> EOS"</span>).split()]
            en_sents.append(en_sent)
            fr_sents_in.append(fr_sent_in)
            fr_sents_out.append(fr_sent_out)
            <span class="hljs-keyword">if</span> i &gt;= num_sent_pairs - <span class="hljs-number">1</span>:
                <span class="hljs-keyword">break</span>
    <span class="hljs-keyword">return</span> en_sents, fr_sents_in, fr_sents_out
sents_en, sents_fr_in, sents_fr_out = download_and_read()
</code></pre>
<p class="normal">Our next step is to tokenize our inputs and create the vocabulary. Since we have sequences in two different<a id="_idIndexMarker570"/> languages, we will create two different tokenizers and vocabularies, one for each language. </p>
<p class="normal">The tf.keras framework provides a very powerful and versatile tokenizer class – here, we have set filters to an empty string and <code class="inlineCode">lower</code> to <code class="inlineCode">False</code> because we have already done what was needed for tokenization in our <code class="inlineCode">preprocess_sentence()</code> function. The Tokenizer creates various data structures from which we can compute the vocabulary sizes and lookup tables that allow us to go from word to word index and back.</p>
<p class="normal">Next, we handle different length sequences of words by padding with zeros at the end, using the <code class="inlineCode">pad_sequences()</code> function. Because our strings are fairly short, we do not do any truncation; we just pad to the maximum length of sentence that we have (8 words for English and 16 words for French):</p>
<pre class="programlisting code"><code class="hljs-code">tokenizer_en = tf.keras.preprocessing.text.Tokenizer(
    filters=<span class="hljs-string">""</span>, lower=<span class="hljs-literal">False</span>)
tokenizer_en.fit_on_texts(sents_en)
data_en = tokenizer_en.texts_to_sequences(sents_en)
data_en = tf.keras.preprocessing.sequence.pad_sequences(
    data_en, padding=<span class="hljs-string">"post"</span>)
tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(
    filters=<span class="hljs-string">""</span>, lower=<span class="hljs-literal">False</span>)
tokenizer_fr.fit_on_texts(sents_fr_in)
tokenizer_fr.fit_on_texts(sents_fr_out)
data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)
data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(
    data_fr_in, padding=<span class="hljs-string">"post"</span>)
data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)
data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(
    data_fr_out, padding=<span class="hljs-string">"post"</span>)
vocab_size_en = <span class="hljs-built_in">len</span>(tokenizer_en.word_index)
vocab_size_fr = <span class="hljs-built_in">len</span>(tokenizer_fr.word_index)
word2idx_en = tokenizer_en.word_index
idx2word_en = {v:k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word2idx_en.items()}
word2idx_fr = tokenizer_fr.word_index
idx2word_fr = {v:k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word2idx_fr.items()}
<span class="hljs-built_in">print</span>(<span class="hljs-string">"vocab size (en): {:d}, vocab size (fr): {:d}"</span>.<span class="hljs-built_in">format</span>(
    vocab_size_en, vocab_size_fr))
maxlen_en = data_en.shape[<span class="hljs-number">1</span>]
maxlen_fr = data_fr_out.shape[<span class="hljs-number">1</span>]
<span class="hljs-built_in">print</span>(<span class="hljs-string">"seqlen (en): {:d}, (fr): {:d}"</span>.<span class="hljs-built_in">format</span>(maxlen_en, maxlen_fr))
</code></pre>
<p class="normal">Finally, we convert the data<a id="_idIndexMarker571"/> into a TensorFlow dataset, and then split it into a training and test dataset:</p>
<pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">64</span>
dataset = tf.data.Dataset.from_tensor_slices(
    (data_en, data_fr_in, data_fr_out))
dataset = dataset.shuffle(<span class="hljs-number">10000</span>)
test_size = NUM_SENT_PAIRS // <span class="hljs-number">4</span>
test_dataset = dataset.take(test_size).batch(
    batch_size, drop_remainder=<span class="hljs-literal">True</span>)
train_dataset = dataset.skip(test_size).batch(
    batch_size, drop_remainder=<span class="hljs-literal">True</span>)
</code></pre>
<p class="normal">Our data is now ready to be used for training the seq2seq network, which we will define next. Our encoder is an embedding layer followed by a GRU layer. The input to the encoder is a sequence of integers, which is converted into a sequence of embedding vectors of size <code class="inlineCode">embedding_dim</code>. This sequence of vectors is sent to an RNN, which converts the input at each of the <code class="inlineCode">num_timesteps</code> time steps into a vector of size <code class="inlineCode">encoder_dim</code>. Only the output at the last time step is returned, as shown by <code class="inlineCode">return_sequences=False</code>.</p>
<p class="normal">The decoder has almost the same structure as the encoder, except that it has an additional dense layer<a id="_idIndexMarker572"/> that converts the vector of size <code class="inlineCode">decoder_dim</code>, which is output from the RNN, into a vector that represents the probability distribution across the target vocabulary. The decoder also returns outputs along with all its time steps.</p>
<p class="normal">In our example network, we have chosen our embedding dimension to be 128, followed by an encoder and decoder RNN dimension of 1024 each. Note that we have to add 1 to the vocabulary size for both the English and French vocabularies to account for the PAD character that was added during the <code class="inlineCode">pad_sequences()</code> step:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Encoder</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, vocab_size, num_timesteps,</span>
<span class="hljs-params">            embedding_dim, encoder_dim, **kwargs</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(Encoder, self).__init__(**kwargs)
        self.encoder_dim = encoder_dim
        self.embedding = tf.keras.layers.Embedding(
            vocab_size, embedding_dim, input_length=num_timesteps)
        self.rnn = tf.keras.layers.GRU(
            encoder_dim, return_sequences=<span class="hljs-literal">False</span>, return_state=<span class="hljs-literal">True</span>)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, x, state</span><span class="hljs-function">):</span>
        x = self.embedding(x)
        x, state = self.rnn(x, initial_state=state)
        <span class="hljs-keyword">return</span> x, state
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">init_state</span><span class="hljs-function">(</span><span class="hljs-params">self, batch_size</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">return</span> tf.zeros((batch_size, self.encoder_dim))
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Decoder</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, vocab_size, embedding_dim, num_timesteps,</span>
<span class="hljs-params">            decoder_dim, **kwargs</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(Decoder, self).__init__(**kwargs)
        self.decoder_dim = decoder_dim
        self.embedding = tf.keras.layers.Embedding(
            vocab_size, embedding_dim, input_length=num_timesteps)
        self.rnn = tf.keras.layers.GRU(
            decoder_dim, return_sequences=<span class="hljs-literal">True</span>, return_state=<span class="hljs-literal">True</span>)
        self.dense = tf.keras.layers.Dense(vocab_size)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, x, state</span><span class="hljs-function">):</span>
        x = self.embedding(x)
        x, state = self.rnn(x, state)
        x = self.dense(x)
        <span class="hljs-keyword">return</span> x, state
embedding_dim = <span class="hljs-number">256</span>
encoder_dim, decoder_dim = <span class="hljs-number">1024</span>, <span class="hljs-number">1024</span>
encoder = Encoder(vocab_size_en+<span class="hljs-number">1</span>, 
    embedding_dim, maxlen_en, encoder_dim)
decoder = Decoder(vocab_size_fr+<span class="hljs-number">1</span>, 
    embedding_dim, maxlen_fr, decoder_dim)
</code></pre>
<p class="normal">Now that we have defined our <code class="inlineCode">Encoder</code> and <code class="inlineCode">Decoder</code> classes, let’s revisit the dimensions of their inputs and outputs. The following piece of (throwaway) code can be used to print out the dimensions<a id="_idIndexMarker573"/> of the various inputs and outputs of the system. It has been left in for convenience as a commented-out block in the code supplied with this chapter:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> encoder_in, decoder_in, decoder_out <span class="hljs-keyword">in</span> train_dataset:
    encoder_state = encoder.init_state(batch_size)
    encoder_out, encoder_state = encoder(encoder_in, encoder_state)
    decoder_state = encoder_state
    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)
    <span class="hljs-keyword">break</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"encoder input          :"</span>, encoder_in.shape)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"encoder output         :"</span>, encoder_out.shape, <span class="hljs-string">"state:"</span>, encoder_state.shape)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"decoder output (logits):"</span>, decoder_pred.shape, <span class="hljs-string">"state:"</span>, decoder_state.shape)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"decoder output (labels):"</span>, decoder_out.shape)
</code></pre>
<p class="normal">This produces the following output, which is in line with our expectations. The encoder input is a batch of a sequence of integers, each sequence being of size 8, which is the maximum number of tokens in our English sentences, so its dimension is (<code class="inlineCode">batch_size</code>, <code class="inlineCode">maxlen_en</code>).</p>
<p class="normal">The output of the encoder is a single tensor (<code class="inlineCode">return_sequences=False</code>) of shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">encoder_dim</code>) and represents a batch of context vectors representing the input sentences. The encoder state tensor has the same dimensions. The decoder outputs are also a batch of sequences of integers, but the maximum size of a French sentence is 16; therefore, the dimensions are (<code class="inlineCode">batch_size</code>, <code class="inlineCode">maxlen_fr</code>). </p>
<p class="normal">The decoder predictions are a batch of probability distributions across all time steps; hence, the dimensions are (<code class="inlineCode">batch_size</code>, <code class="inlineCode">maxlen_fr</code>, <code class="inlineCode">vocab_size_fr+1</code>), and the decoder state is the same dimension as the encoder state (<code class="inlineCode">batch_size</code>, <code class="inlineCode">decoder_dim</code>):</p>
<pre class="programlisting con"><code class="hljs-con">encoder input          : (64, 8)
encoder output         : (64, 1024) state: (64, 1024)
decoder output (logits): (64, 16, 7658) state: (64, 1024)
decoder output (labels): (64, 16)
</code></pre>
<p class="normal">Next, we define the loss function. Because we padded our sentences, we don’t want to bias our results by <a id="_idIndexMarker574"/>considering the equality of pad words between the labels and predictions. Our loss function masks our predictions with the labels, so any padded positions on the label are also removed from the predictions, and we only compute our loss using the non-zero elements on both the label and predictions. This is done as follows:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">loss_fn</span><span class="hljs-function">(</span><span class="hljs-params">ytrue, ypred</span><span class="hljs-function">):</span>
    scce = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=<span class="hljs-literal">True</span>)
    mask = tf.math.logical_not(tf.math.equal(ytrue, <span class="hljs-number">0</span>))
    mask = tf.cast(mask, dtype=tf.int64)
    loss = scce(ytrue, ypred, sample_weight=mask)
    <span class="hljs-keyword">return</span> loss
</code></pre>
<p class="normal">Because the seq2seq model is not easy to package into a simple Keras model, we have to handle the training loop manually as well. Our <code class="inlineCode">train_step()</code> function handles the flow of data and computes the loss at each step, applies the gradient of the loss back to the trainable weights, and returns the loss.</p>
<p class="normal">Notice that the training code is not quite the same as what was described in our discussion of the seq2seq model earlier. Here, it appears that the entire <code class="inlineCode">decoder_input</code> is fed in one go into the decoder to produce the output offset by one time step, whereas in the discussion, we said that this happens sequentially, where the token generated in the previous time step is used as the input for the next time step.</p>
<p class="normal">This is a common technique used to<a id="_idIndexMarker575"/> train seq2seq networks, which is called <strong class="keyWord">Teacher Forcing</strong>, where the input to the decoder is the ground truth output instead of the prediction from the previous time step. This is preferred because it makes training faster but also results in some degradation in prediction quality. To offset this, techniques <a id="_idIndexMarker576"/>such as <strong class="keyWord">Scheduled Sampling</strong> can be used, where the input is sampled randomly either from the ground truth or the prediction at the previous time step, based on some threshold (this depends on the problem, but usually varies between 0.1 and 0.4):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">train_step</span><span class="hljs-function">(</span><span class="hljs-params">encoder_in, decoder_in, decoder_out, encoder_state</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        encoder_out, encoder_state = encoder(encoder_in, encoder_state)
        decoder_state = encoder_state
        decoder_pred, decoder_state = decoder(
            decoder_in, decoder_state)
        loss = loss_fn(decoder_out, decoder_pred)
  
    variables = (encoder.trainable_variables + 
        decoder.trainable_variables)
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, variables))
    <span class="hljs-keyword">return</span> loss
</code></pre>
<p class="normal">The <code class="inlineCode">predict()</code> method is <a id="_idIndexMarker577"/>used to randomly sample a single English sentence from the dataset and use the model trained so far to predict the French sentence. For reference, the label French sentence is also displayed. The <code class="inlineCode">evaluate()</code> method <a id="_idIndexMarker578"/>computes the <strong class="keyWord">BiLingual Evaluation Understudy</strong> (<strong class="keyWord">BLEU</strong>) score [35] between the labels and predictions across all records in the test set. BLEU scores are generally used where multiple ground truth labels exist (we have only one) and compare up to 4-grams (n-grams with <em class="italic">n=4</em>) in both reference and candidate sentences. Both the <code class="inlineCode">predict()</code> and <code class="inlineCode">evaluate()</code> methods are called at the end of every epoch:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">predict</span><span class="hljs-function">(</span><span class="hljs-params">encoder, decoder, batch_size,</span>
<span class="hljs-params">        sents_en, data_en, sents_fr_out,</span>
<span class="hljs-params">        word2idx_fr, idx2word_fr</span><span class="hljs-function">):</span>
    random_id = np.random.choice(<span class="hljs-built_in">len</span>(sents_en))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"input    : "</span>,  <span class="hljs-string">" "</span>.join(sents_en[random_id]))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"label    : "</span>, <span class="hljs-string">" "</span>.join(sents_fr_out[random_id]))
    encoder_in = tf.expand_dims(data_en[random_id], axis=<span class="hljs-number">0</span>)
    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=<span class="hljs-number">0</span>)
    encoder_state = encoder.init_state(<span class="hljs-number">1</span>)
    encoder_out, encoder_state = encoder(encoder_in, encoder_state)
    decoder_state = encoder_state
    decoder_in = tf.expand_dims(
        tf.constant([word2idx_fr[<span class="hljs-string">"BOS"</span>]]), axis=<span class="hljs-number">0</span>)
    pred_sent_fr = []
    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
        decoder_pred, decoder_state = decoder(
            decoder_in, decoder_state)
        decoder_pred = tf.argmax(decoder_pred, axis=-<span class="hljs-number">1</span>)
        pred_word = idx2word_fr[decoder_pred.numpy()[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]]
        pred_sent_fr.append(pred_word)
        <span class="hljs-keyword">if</span> pred_word == <span class="hljs-string">"EOS"</span>:
            <span class="hljs-keyword">break</span>
        decoder_in = decoder_pred
  
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"predicted: "</span>, <span class="hljs-string">" "</span>.join(pred_sent_fr))
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">evaluate_bleu_score</span><span class="hljs-function">(</span><span class="hljs-params">encoder, decoder, test_dataset,</span>
<span class="hljs-params">        word2idx_fr, idx2word_fr</span><span class="hljs-function">):</span>
    bleu_scores = []
    smooth_fn = SmoothingFunction()
    <span class="hljs-keyword">for</span> encoder_in, decoder_in, decoder_out <span class="hljs-keyword">in</span> test_dataset:
        encoder_state = encoder.init_state(batch_size)
        encoder_out, encoder_state = encoder(encoder_in, encoder_state)
        decoder_state = encoder_state
        decoder_pred, decoder_state = decoder(
            decoder_in, decoder_state)
        <span class="hljs-comment"># compute argmax</span>
        decoder_out = decoder_out.numpy()
        decoder_pred = tf.argmax(decoder_pred, axis=-<span class="hljs-number">1</span>).numpy()
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(decoder_out.shape[<span class="hljs-number">0</span>]):
            ref_sent = [idx2word_fr[j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> 
                decoder_out[i].tolist() <span class="hljs-keyword">if</span> j &gt; <span class="hljs-number">0</span>]
            hyp_sent = [idx2word_fr[j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> 
                decoder_pred[i].tolist() <span class="hljs-keyword">if</span> j &gt; <span class="hljs-number">0</span>]
            <span class="hljs-comment"># remove trailing EOS</span>
            ref_sent = ref_sent[<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>]
            hyp_sent = hyp_sent[<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>]
            bleu_score = sentence_bleu([ref_sent], hyp_sent,
                smoothing_function=smooth_fn.method1)
            bleu_scores.append(bleu_score)
    <span class="hljs-keyword">return</span> np.mean(np.array(bleu_scores))
</code></pre>
<p class="normal">The training loop is shown as follows. We will use the Adam optimizer for our model. We also set up a <a id="_idIndexMarker579"/>checkpoint so that we can save our model after every 10 epochs. We then train the model for 250 epochs, and print out the loss, an example sentence and its translation, and the BLEU score computed over the entire test set:</p>
<pre class="programlisting code"><code class="hljs-code">optimizer = tf.keras.optimizers.Adam()
checkpoint_prefix = os.path.join(checkpoint_dir, <span class="hljs-string">"ckpt"</span>)
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)
num_epochs = <span class="hljs-number">250</span>
eval_scores = []
<span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
    encoder_state = encoder.init_state(batch_size)
    <span class="hljs-keyword">for</span> batch, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataset):
        encoder_in, decoder_in, decoder_out = data
        <span class="hljs-comment"># print(encoder_in.shape, decoder_in.shape, decoder_out.shape)</span>
        loss = train_step(
            encoder_in, decoder_in, decoder_out, encoder_state)
  
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Epoch: {}, Loss: {:.4f}"</span>.<span class="hljs-built_in">format</span>(e + <span class="hljs-number">1</span>, loss.numpy()))
    <span class="hljs-keyword">if</span> e % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
        checkpoint.save(file_prefix=checkpoint_prefix)
  
    predict(encoder, decoder, batch_size, sents_en, data_en,
        sents_fr_out, word2idx_fr, idx2word_fr)
    eval_score = evaluate_bleu_score(encoder, decoder, 
        test_dataset, word2idx_fr, idx2word_fr)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Eval Score (BLEU): {:.3e}"</span>.<span class="hljs-built_in">format</span>(eval_score))
    <span class="hljs-comment"># eval_scores.append(eval_score)</span>
checkpoint.save(file_prefix=checkpoint_prefix)
</code></pre>
<p class="normal">The results from the first 5 and last 5 epochs of training are shown as follows. Notice that the loss has gone down<a id="_idIndexMarker580"/> from about 1.5 to around 0.07 in epoch 247. The BLEU scores have also gone up by around 2.5 times. Most impressive, however, is the difference in translation quality between the first 5 and last 5 epochs:</p>
<table class="table-container" id="table002-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Epoch-#</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Loss (Training)</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">BLEU Score (Test)</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">English</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">French (true)</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">French (predicted)</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">1</p>
</td>
<td class="table-cell">
<p class="normal">1.4119</p>
</td>
<td class="table-cell">
<p class="normal">1.957e-02</p>
</td>
<td class="table-cell">
<p class="normal">tom is special.</p>
</td>
<td class="table-cell">
<p class="normal">tom est special.</p>
</td>
<td class="table-cell">
<p class="normal">elle est tres bon.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">2</p>
</td>
<td class="table-cell">
<p class="normal">1.1067</p>
</td>
<td class="table-cell">
<p class="normal">2.244e-02</p>
</td>
<td class="table-cell">
<p class="normal">he hates shopping.</p>
</td>
<td class="table-cell">
<p class="normal">il deteste faire les courses.</p>
</td>
<td class="table-cell">
<p class="normal">il est tres mineure.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">3</p>
</td>
<td class="table-cell">
<p class="normal">0.9154</p>
</td>
<td class="table-cell">
<p class="normal">2.700e-02</p>
</td>
<td class="table-cell">
<p class="normal">did she say it?</p>
</td>
<td class="table-cell">
<p class="normal">l a t elle dit?</p>
</td>
<td class="table-cell">
<p class="normal">n est ce pas clair?</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">4</p>
</td>
<td class="table-cell">
<p class="normal">0.7817</p>
</td>
<td class="table-cell">
<p class="normal">2.803e-02</p>
</td>
<td class="table-cell">
<p class="normal">i d rather walk.</p>
</td>
<td class="table-cell">
<p class="normal">je prefererais marcher.</p>
</td>
<td class="table-cell">
<p class="normal">je suis alle a kyoto.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">5</p>
</td>
<td class="table-cell">
<p class="normal">0.6632</p>
</td>
<td class="table-cell">
<p class="normal">2.943e-02</p>
</td>
<td class="table-cell">
<p class="normal">i m in the car.</p>
</td>
<td class="table-cell">
<p class="normal">je suis dans la voiture.</p>
</td>
<td class="table-cell">
<p class="normal">je suis toujours inquiet.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">...</p>
</td>
<td class="table-cell"/>
<td class="table-cell"/>
<td class="table-cell"/>
<td class="table-cell"/>
<td class="table-cell"/>
</tr>
<tr>
<td class="table-cell">
<p class="normal">245</p>
</td>
<td class="table-cell">
<p class="normal">0.0896</p>
</td>
<td class="table-cell">
<p class="normal">4.991e-02</p>
</td>
<td class="table-cell">
<p class="normal">she sued him.</p>
</td>
<td class="table-cell">
<p class="normal">elle le poursuivit en justice.</p>
</td>
<td class="table-cell">
<p class="normal">elle l a poursuivi en justice.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">246</p>
</td>
<td class="table-cell">
<p class="normal">0.0853</p>
</td>
<td class="table-cell">
<p class="normal">5.011e-02</p>
</td>
<td class="table-cell">
<p class="normal">she isn t poor.</p>
</td>
<td class="table-cell">
<p class="normal">elle n est pas pauvre.</p>
</td>
<td class="table-cell">
<p class="normal">elle n est pas pauvre.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">247</p>
</td>
<td class="table-cell">
<p class="normal">0.0738</p>
</td>
<td class="table-cell">
<p class="normal">5.022e-02</p>
</td>
<td class="table-cell">
<p class="normal">which one is mine?</p>
</td>
<td class="table-cell">
<p class="normal">lequel est le mien?</p>
</td>
<td class="table-cell">
<p class="normal">lequel est le mien?</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">248</p>
</td>
<td class="table-cell">
<p class="normal">0.1208</p>
</td>
<td class="table-cell">
<p class="normal">4.931e-02</p>
</td>
<td class="table-cell">
<p class="normal">i m getting old.</p>
</td>
<td class="table-cell">
<p class="normal">je me fais vieux.</p>
</td>
<td class="table-cell">
<p class="normal">je me fais vieux.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">249</p>
</td>
<td class="table-cell">
<p class="normal">0.0837</p>
</td>
<td class="table-cell">
<p class="normal">4.856e-02</p>
</td>
<td class="table-cell">
<p class="normal">it was worth a try.</p>
</td>
<td class="table-cell">
<p class="normal">ca valait le coup d essayer.</p>
</td>
<td class="table-cell">
<p class="normal">ca valait le coup d essayer.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">250</p>
</td>
<td class="table-cell">
<p class="normal">0.0967</p>
</td>
<td class="table-cell">
<p class="normal">4.869e-02</p>
</td>
<td class="table-cell">
<p class="normal">don t back away.</p>
</td>
<td class="table-cell">
<p class="normal">ne reculez pas!</p>
</td>
<td class="table-cell">
<p class="normal">ne reculez pas!</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 5.2: Training results by epoch</p>
<p class="normal">The full code for this example can be found in the source code accompanying this chapter. You will need a <a id="_idIndexMarker581"/>GPU-based machine to run it, although you may be able to run it on the CPU using smaller network dimensions (<code class="inlineCode">embedding_dim</code>, <code class="inlineCode">encoder_dim</code>, <code class="inlineCode">decoder_dim</code>), smaller hyperparameters (<code class="inlineCode">batch_size</code>, <code class="inlineCode">num_epochs</code>), and a smaller number of sentence pairs. To run the code in its entirety, run the following command. The output will be written to the console:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python seq2seq_wo_attn.py
</code></pre>
<p class="normal">In the next section, we will look at a mechanism to improve the performance of the seq2seq network, by allowing it to<a id="_idIndexMarker582"/> focus on certain parts of the input more than on others in a data-driven way. This mechanism is known as the attention mechanism.</p>
<h1 class="heading-1" id="_idParaDest-140">Attention mechanism</h1>
<p class="normal">In the previous section, we saw how the context or thought vector from the last time step of the encoder is fed into the<a id="_idIndexMarker583"/> decoder as the initial hidden state. As the context flows through the time steps on the decoder, the signal gets combined with the decoder output and progressively gets weaker and weaker. The result is that the context does not have much effect on the later time steps in the decoder.</p>
<p class="normal">In addition, certain sections of the decoder output may depend more heavily on certain sections of the input. For example, consider an input “thank you very much,” and the corresponding output “merci beaucoup” for an English-to-French translation network such as the one we looked at in the previous section. Here, the English phrases “thank you,” and “very much,” correspond to the French “merci” and “beaucoup” respectively. This information is also not conveyed adequately through the single context vector.</p>
<p class="normal">The attention mechanism provides access to all encoder hidden states at every time step on the decoder. The decoder learns which part of the encoder states to pay more attention to. The use of attention has resulted in great improvements to the quality of machine translation, as well as a variety of standard natural language processing tasks.</p>
<p class="normal">The use of attention is not limited to seq2seq networks. For example, attention is a key component in the “Embed, Encode, Attend, Predict” formula for creating state-of-the-art deep learning models for NLP [34]. Here, attention has been used to preserve as much information as possible when downsizing from a larger to a more compact representation, for example, when reducing a sequence of word vectors into a single sentence vector.</p>
<p class="normal">Essentially, the attention mechanism provides a way to score tokens in the target against all tokens in the source and modify the input signal to the decoder accordingly. Consider an encoder-decoder architecture where the input and output time steps are denoted by indices <em class="italic">i</em> and <em class="italic">j</em> respectively, and the hidden states on the encoder and decoder at these respective time steps are denoted by <em class="italic">h</em><sub class="italic">i</sub> and <em class="italic">s</em><sub class="italic">j</sub>. Inputs to the encoder are denoted by <em class="italic">x</em><sub class="italic">i</sub>, and outputs from the decoder are denoted by <em class="italic">y</em><sub class="italic">j</sub>. In an encoder-decoder network without attention, the value of decoder state <em class="italic">s</em><sub class="italic">j</sub> is given by the hidden state <em class="italic">s</em><sub class="italic">j</sub><sub class="subscript">-1</sub> and output <em class="italic">y</em><sub class="italic">j</sub><sub class="subscript">-1</sub> at the previous time step. The attention mechanism adds a third signal <em class="italic">c</em><sub class="italic">j</sub>, known as the attention context. With attention, therefore, the decoder’s hidden state <em class="italic">s</em><sub class="italic">j</sub> is a function of <em class="italic">y</em><sub class="italic">j</sub><sub class="subscript">-1</sub>, <em class="italic">s</em><sub class="italic">j</sub><sub class="subscript">-1</sub>, and <em class="italic">c</em><sub class="italic">j</sub>, which is shown as follows:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_05_012.png" style="height: 1.35em !important; vertical-align: 0.01em !important;" width="329"/></p>
<p class="normal">The attention context signal <em class="italic">c</em><sub class="italic">j</sub> is computed as follows. For every decoder step <em class="italic">j</em>, we compute the alignment<a id="_idIndexMarker584"/> between the decoder state <em class="italic">s</em><sub class="italic">j</sub><sub class="subscript">-1</sub> and every encoder state <em class="italic">h</em><sub class="italic">i</sub>. This gives us a set of <em class="italic">N</em> similarity values <em class="italic">e</em><sub class="italic">ij</sub> for each decoder state <em class="italic">j</em>, which we then convert into a probability distribution by computing their corresponding softmax values <em class="italic">b</em><sub class="italic">ij</sub>. Finally, the attention context <em class="italic">c</em><sub class="italic">j</sub> is computed as the weighted sum of the encoder states <em class="italic">h</em><sub class="italic">i</sub> and their corresponding softmax weights <em class="italic">b</em><sub class="italic">ij</sub> over all <em class="italic">N</em> encoder time steps. The set of equations shown encapsulates this transformation for each decoder step <em class="italic">j</em>:</p>
<p class="center"><img alt="" height="250" src="../Images/B18331_05_013.png" style="height: 6.25em !important;" width="375"/></p>
<p class="normal">Multiple attention mechanisms have been proposed based on how the alignment is done. We will describe a few next. For notational convenience, we will indicate the state vector <em class="italic">h</em><sub class="subscript">i</sub> on the encoder side with <em class="italic">h</em>, and the state vector <em class="italic">s</em><sub class="italic">j</sub><sub class="subscript">-1</sub> on the decoder side with <em class="italic">s</em>.</p>
<p class="normal">The simplest formulation <a id="_idIndexMarker585"/>of alignment is <strong class="keyWord">content-based attention</strong>. It was proposed by Graves, Wayne, and Danihelka [27], and is just the cosine similarity between the encoder and decoder states. A precondition for using this formulation is that the hidden state vector on both the encoder and decoder must have the same dimensions:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_05_014.png" style="height: 1.25em !important;" width="275"/></p>
<p class="normal">Another formulation, known as <strong class="keyWord">additive</strong> or <strong class="keyWord">Bahdanau attention</strong>, was proposed by Bahdanau, Cho, and Bengio [28]. This involves<a id="_idIndexMarker586"/> combining the state vectors using learnable weights in a small neural network, given by the following equation. Here, the <em class="italic">s</em> and <em class="italic">h</em> vectors are concatenated and multiplied by the learned weights <em class="italic">W</em>, which is equivalent to using two learned weights <em class="italic">W</em><sub class="italic">s</sub> and <em class="italic">W</em><sub class="italic">h</sub> to multiply with <em class="italic">s</em> and <em class="italic">h</em>, and adding the results:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_05_015.png" style="height: 1.25em !important;" width="358"/></p>
<p class="normal">Luong, Pham, and Manning [29] proposed a set of three attention formulations (dot, general, and concat), of which the general formulation is also known as the <strong class="keyWord">multiplicative</strong> or <strong class="keyWord">Luong’s attention</strong>. </p>
<p class="normal">The <code class="inlineCode">dot</code> and <code class="inlineCode">concat</code> attention formulations are similar to the content-based and<a id="_idIndexMarker587"/> additive attention formulations discussed earlier. The multiplicative attention formulation is given by the following equation:</p>
<p class="center"><img alt="" height="42" src="../Images/B18331_05_016.png" style="height: 1.05em !important; vertical-align: 0.20em !important;" width="171"/></p>
<p class="normal">Finally, Vaswani, et al. [30] proposed a variation on content-based attention, called the <strong class="keyWord">scaled dot-product attention</strong>, which is given by<a id="_idIndexMarker588"/> the following equation. Here, <em class="italic">N</em> is the dimension of the <a id="_idIndexMarker589"/>encoder hidden state <em class="italic">h</em>. Scaled dot-product attention is used in transformer architecture, which we will learn about in the next chapter:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_05_017.png" style="height: 2.60em !important; vertical-align: 0.08em !important;" width="138"/></p>
<p class="normal">Attention mechanisms can also be categorized by what they attend to. Using this categorization scheme attention mechanisms can be self-attention, global or soft attention, and local or hard attention.</p>
<p class="normal">Self-attention is when the alignment is computed across different sections of the same sequence and has been found to be useful for applications such as machine reading, abstractive text summarization, and image caption generation.</p>
<p class="normal">Soft or global attention is<a id="_idIndexMarker590"/> when the alignment is computed over the entire input sequence, and hard or local attention is when the alignment is computed over part of the sequence. The advantage of soft attention is that it is differentiable; however, it can be expensive to compute. Conversely, hard attention is cheaper to compute at inference time but is non-differentiable and requires more complicated techniques during training.</p>
<p class="normal">In the next section, we will see how to integrate the attention mechanism with a seq2seq network and how it improves performance.</p>
<h2 class="heading-2" id="_idParaDest-141">Example ‒ seq2seq with attention for machine translation</h2>
<p class="normal">Let’s look at the same example of <a id="_idIndexMarker591"/>machine translation that we saw earlier in this chapter, except that the decoder will now attend to the encoder <a id="_idIndexMarker592"/>outputs using the additive attention mechanism proposed by Bahdanau, et al. [28], and the multiplicative one proposed by Luong, et al [29].</p>
<p class="normal">The first change is to the encoder. Instead of returning a single context or thought vector, it will return outputs at every time point, because the attention mechanism will need this information. Here is the revised encoder class with the change highlighted:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Encoder</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
     <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, vocab_size, num_timesteps,</span>
<span class="hljs-params">           embedding_dim, encoder_dim, **kwargs</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(Encoder, self).__init__(**kwargs)
        self.encoder_dim = encoder_dim
        self.embedding = tf.keras.layers.Embedding(
            vocab_size, embedding_dim, input_length=num_timesteps)
        self.rnn = tf.keras.layers.GRU(
            encoder_dim, return_sequences=<span class="hljs-literal">True</span>, return_state=<span class="hljs-literal">True</span>)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, x, state</span><span class="hljs-function">):</span>
        x = self.embedding(x)
        x, state = self.rnn(x, initial_state=state)
        <span class="hljs-keyword">return</span> x, state
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">init_state</span><span class="hljs-function">(</span><span class="hljs-params">self, batch_size</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">return</span> tf.zeros((batch_size, self.encoder_dim))
</code></pre>
<p class="normal">The decoder will have bigger changes. The biggest is the declaration of attention layers, which need to be defined, so let’s do that first. Let’s first consider the class definition for the additive attention proposed by Bahdanau. Recall that this combines the decoder hidden state at each time step with all the encoder hidden states to produce an input to the decoder at the next time step, which is given by the following equation:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_05_015.png" style="height: 1.25em !important;" width="358"/></p>
<p class="normal">The <em class="italic">W [s;h]</em> in the equation is shorthand for two separate linear transformations (of the form <em class="italic">y = Wx + b</em>), one on <em class="italic">s</em>, and the other on <em class="italic">h</em>. The two linear transformations are implemented as dense layers, as shown in the following implementation. We subclass a <code class="inlineCode">tf.keras</code> Layer object since <a id="_idIndexMarker593"/>our end goal is to use this as a layer in our network, but it is also acceptable to subclass a Model object. The <code class="inlineCode">call()</code> method takes the query (the decoder state) and values (the encoder states), computes the score, then the alignment as the corresponding softmax, and context vector as given by the equation, and then returns them. The shape of the context vector is given by (<code class="inlineCode">batch_size</code>, <code class="inlineCode">num_decoder_timesteps</code>), and the alignments have the shape (<code class="inlineCode">batch_size</code>, <code class="inlineCode">num_encoder_timesteps</code>, <code class="inlineCode">1</code>). The weights for the dense layer’s <code class="inlineCode">W1</code>, <code class="inlineCode">W2</code>, and <code class="inlineCode">V</code> tensors are learned during training:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">BahdanauAttention</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, num_units</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(num_units)
        self.W2 = tf.keras.layers.Dense(num_units)
        self.V = tf.keras.layers.Dense(<span class="hljs-number">1</span>)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, query, values</span><span class="hljs-function">):</span>
        <span class="hljs-comment"># query is the decoder state at time step j</span>
        <span class="hljs-comment"># query.shape: (batch_size, num_units)</span>
        <span class="hljs-comment"># values are encoder states at every timestep i</span>
        <span class="hljs-comment"># values.shape: (batch_size, num_timesteps, num_units)</span>
        <span class="hljs-comment"># add time axis to query: (batch_size, 1, num_units)</span>
        query_with_time_axis = tf.expand_dims(query, axis=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># compute score:</span>
        score = self.V(tf.keras.activations.tanh(
            self.W1(values) + self.W2(query_with_time_axis)))
        <span class="hljs-comment"># compute softmax</span>
        alignment = tf.nn.softmax(score, axis=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># compute attended output</span>
        context = tf.reduce_sum(
            tf.linalg.matmul(
                tf.linalg.matrix_transpose(alignment),
                values
            ), axis=<span class="hljs-number">1</span>
        )
        context = tf.expand_dims(context, axis=<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> context, alignment
</code></pre>
<p class="normal">The Luong attention is multiplicative, but the general implementation is similar. Instead of declaring three linear transformations <code class="inlineCode">W1</code>, <code class="inlineCode">W2</code>, and <code class="inlineCode">V</code>, we only have a single one <code class="inlineCode">W</code>. The steps in the <code class="inlineCode">call()</code> method follow the same general steps – first, we compute the scores according to the <a id="_idIndexMarker594"/>equation for Luong’s attention, as described in the last section. Then, we compute the alignments as the corresponding <a id="_idIndexMarker595"/>softmax version of the scores and then the context vector as the dot product of the alignment and the values. Like the weights in the Bahdanau attention class, the weight matrices represented by the dense layer <code class="inlineCode">W</code> are learned during training:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">LuongAttention</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, num_units</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(LuongAttention, self).__init__()
        self.W = tf.keras.layers.Dense(num_units)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, query, values</span><span class="hljs-function">):</span>
        <span class="hljs-comment"># add time axis to query</span>
        query_with_time_axis = tf.expand_dims(query, axis=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># compute score</span>
        score = tf.linalg.matmul(
            query_with_time_axis, self.W(values), transpose_b=<span class="hljs-literal">True</span>)
        <span class="hljs-comment"># compute softmax</span>
        alignment = tf.nn.softmax(score, axis=<span class="hljs-number">2</span>)
        <span class="hljs-comment"># compute attended output</span>
        context = tf.matmul(alignment, values)
        <span class="hljs-keyword">return</span> context, alignment
</code></pre>
<p class="normal">To verify that the two classes are drop-in replacements for each other, we run the following piece of throwaway <a id="_idIndexMarker596"/>code (commented out in the source code for this example). We just manufacture some random inputs and send them to both attention classes:</p>
<pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">64</span>
num_timesteps = <span class="hljs-number">100</span>
num_units = <span class="hljs-number">1024</span>
query = np.random.random(size=(batch_size, num_units))
values = np.random.random(size=(batch_size, num_timesteps, num_units))
<span class="hljs-comment"># check out dimensions for Bahdanau attention</span>
b_attn = BahdanauAttention(num_units)
context, alignments = b_attn(query, values)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Bahdanau: context.shape:"</span>, context.shape, 
    <span class="hljs-string">"alignments.shape:"</span>, alignments.shape)
<span class="hljs-comment"># check out dimensions for Luong attention</span>
l_attn = LuongAttention(num_units)
context, alignments = l_attn(query, values)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Luong: context.shape:"</span>, context.shape, 
    <span class="hljs-string">"alignments.shape:"</span>, alignments.shape)
</code></pre>
<p class="normal">The preceding code produces the following output and shows, as expected, that the two classes produce identically shaped outputs when given the same input. Hence, they are drop-in replacements for each other:</p>
<pre class="programlisting con"><code class="hljs-con">Bahdanau: context.shape: (64, 1024) alignments.shape: (64, 8, 1)
Luong: context.shape: (64, 1024) alignments.shape: (64, 8, 1)
</code></pre>
<p class="normal">Now that we have our attention classes, let’s look at the decoder. The difference in the <code class="inlineCode">init()</code> method is the addition of the attention class variable, which we have set to the <code class="inlineCode">BahdanauAttention</code> class. In <a id="_idIndexMarker597"/>addition, we have two additional transformations, <code class="inlineCode">Wc</code> and <code class="inlineCode">Ws</code>, that will be applied to the output of the decoder RNN. The first one has a <code class="inlineCode">tanh</code> activation to modulate the output between -1 and +1, and the next one is a standard linear transformation. Compared to the seq2seq network without an attention decoder component, this decoder takes an additional parameter <code class="inlineCode">encoder_output</code> in its <code class="inlineCode">call()</code> method and returns an <a id="_idIndexMarker598"/>additional context vector:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Decoder</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, vocab_size, embedding_dim, num_timesteps,</span>
<span class="hljs-params">            decoder_dim, **kwargs</span><span class="hljs-function">):</span>
        <span class="hljs-built_in">super</span>(Decoder, self).__init__(**kwargs)
        self.decoder_dim = decoder_dim
        self.attention = BahdanauAttention(embedding_dim)
        <span class="hljs-comment"># self.attention = LuongAttention(embedding_dim)</span>
        
        self.embedding = tf.keras.layers.Embedding(
            vocab_size, embedding_dim, input_length=num_timesteps)
        self.rnn = tf.keras.layers.GRU(
            decoder_dim, return_sequences=<span class="hljs-literal">True</span>, return_state=<span class="hljs-literal">True</span>)
        self.Wc = tf.keras.layers.Dense(decoder_dim, activation=<span class="hljs-string">"tanh"</span>)
        self.Ws = tf.keras.layers.Dense(vocab_size)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, x, state, encoder_out</span><span class="hljs-function">):</span>
        x = self.embedding(x)
        context, alignment = self.attention(x, encoder_out)
        x = tf.expand_dims(
                tf.concat([
                    x, tf.squeeze(context, axis=<span class="hljs-number">1</span>)
                ], axis=<span class="hljs-number">1</span>),
            axis=<span class="hljs-number">1</span>)
        x, state = self.rnn(x, state)
        x = self.Wc(x)
        x = self.Ws(x)
        <span class="hljs-keyword">return</span> x, state, alignment
</code></pre>
<p class="normal">The training loop is also a little different. Unlike the seq2seq without attention network, where we used teacher forcing to speed up training, using attention means that we now have to consume the<a id="_idIndexMarker599"/> decoder input one by one. This is because the decoder output at the previous step influences more strongly, through attention, the output at the current time step. Our new training loop is described by the <code class="inlineCode">train_step</code> function below and is significantly slower than the training<a id="_idIndexMarker600"/> loop on the seq2seq network without attention. However, this kind of training loop may be used on the former network as well, especially when we want to implement scheduled sampling strategies:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">train_step</span><span class="hljs-function">(</span><span class="hljs-params">encoder_in, decoder_in, decoder_out, encoder_state</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        encoder_out, encoder_state = encoder(encoder_in, encoder_state)
        decoder_state = encoder_state
        loss = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(decoder_out.shape[<span class="hljs-number">1</span>]):
            decoder_in_t = decoder_in[:, t]
            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,
                decoder_state, encoder_out)
            loss += loss_fn(decoder_out[:, t], decoder_pred_t)
    variables = (encoder.trainable_variables +
        decoder.trainable_variables)
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, variables))
    <span class="hljs-keyword">return</span> loss / decoder_out.shape[<span class="hljs-number">1</span>]
</code></pre>
<p class="normal">The <code class="inlineCode">predict()</code> and <code class="inlineCode">evaluate()</code> methods also have similar changes, since they also implement the new data flow on the decoder side that involves an extra <code class="inlineCode">encoder_out</code> parameter and an extra <code class="inlineCode">context</code> return value.</p>
<p class="normal">We trained two versions of the seq2seq network with attention, one with additive (Bahdanau) attention and one with multiplicative (Luong) attention. Both networks were trained for 50 epochs instead of 250. However, in both cases, translations were produced with a quality similar to that obtained from the seq2seq network without attention trained for 250 epochs. The training losses at the end of training for the seq2seq networks with either<a id="_idIndexMarker601"/> attention mechanism were marginally lower, and the BLEU scores on the test sets were slightly higher, compared with the seq2seq network without attention:</p>
<table class="table-container" id="table003-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Network Description</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Ending Loss (training set)</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Ending BLEU score (test set)</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">seq2seq without attention, trained for 250 epochs</p>
</td>
<td class="table-cell">
<p class="normal">0.0967</p>
</td>
<td class="table-cell">
<p class="normal">4.869e-02</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">seq2seq with additive attention, trained for 50 epochs</p>
</td>
<td class="table-cell">
<p class="normal">0.0893</p>
</td>
<td class="table-cell">
<p class="normal">5.508e-02</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">seq2seq with multiplicative attention, trained for 50 epochs</p>
</td>
<td class="table-cell">
<p class="normal">0.0706</p>
</td>
<td class="table-cell">
<p class="normal">5.563e-02</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 5.3: BLEU scores for the different methods</p>
<p class="normal">Here are some examples of the translations produced by the two networks. Epoch numbers and the type of <a id="_idIndexMarker602"/>attention used are mentioned with each example. Notice that even when the translations are not 100% the same as the labels, many of them are valid translations of the original:</p>
<table class="table-container" id="table004-1">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Attention Type</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Epoch-#</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">English</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">French (label)</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">French (predicted)</strong></p>
</td>
</tr>
<tr>
<td class="table-cell" rowspan="3">
<p class="normal">Bahdanau</p>
</td>
<td class="table-cell">
<p class="normal">20</p>
</td>
<td class="table-cell">
<p class="normal">your cat is fat.</p>
</td>
<td class="table-cell">
<p class="normal">ton chat est gras.</p>
</td>
<td class="table-cell">
<p class="normal">ton chat est mouille.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">25</p>
</td>
<td class="table-cell">
<p class="normal">i had to go back.</p>
</td>
<td class="table-cell">
<p class="normal">il m a fallu retourner.</p>
</td>
<td class="table-cell">
<p class="normal">il me faut partir.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">30</p>
</td>
<td class="table-cell">
<p class="normal">try to find it.</p>
</td>
<td class="table-cell">
<p class="normal">tentez de le trouver.</p>
</td>
<td class="table-cell">
<p class="normal">tentez de le trouver.</p>
</td>
</tr>
<tr>
<td class="table-cell" rowspan="3">
<p class="normal">Luong</p>
</td>
<td class="table-cell">
<p class="normal">20</p>
</td>
<td class="table-cell">
<p class="normal">that s peculiar.</p>
</td>
<td class="table-cell">
<p class="normal">c est etrange.</p>
</td>
<td class="table-cell">
<p class="normal">c est deconcertant.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">25</p>
</td>
<td class="table-cell">
<p class="normal">tom is athletic.</p>
</td>
<td class="table-cell">
<p class="normal">thomas est sportif.</p>
</td>
<td class="table-cell">
<p class="normal">tom est sportif.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">30</p>
</td>
<td class="table-cell">
<p class="normal">it s dangerous.</p>
</td>
<td class="table-cell">
<p class="normal">c est dangereux.</p>
</td>
<td class="table-cell">
<p class="normal">c est dangereux.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 5.4: Examples of English-to-French translations</p>
<p class="normal">The full code for the network <a id="_idIndexMarker603"/>described here is in the <code class="inlineCode">seq2seq_with_attn.py</code> file in the code folder for this chapter. To run the code from the command line, please use the following command. You <a id="_idIndexMarker604"/>can switch between Bahdanau (additive) or Luong (multiplicative) attention mechanisms by commenting out one or the other in the <code class="inlineCode">init()</code> method of the <code class="inlineCode">Decoder</code> class:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>python seq2seq_with_attn.py
</code></pre>
<h1 class="heading-1" id="_idParaDest-142">Summary</h1>
<p class="normal">In this chapter, we learned about RNNs, a class of networks that is specialized for dealing with sequences such as natural language, time series, speech, and so on. Just like CNNs exploit the geometry of images, RNNs exploit the sequential structure of their inputs. We learned about the basic RNN cell, how it handles state from previous time steps, and how it suffers from vanishing and exploding gradients because of inherent problems with BPTT. We saw how these problems lead to the development of novel RNN cell architectures such as LSTM, GRU, and peephole LSTMs. We also learned about some simple ways to make your RNN more effective, such as making it bidirectional or stateful.</p>
<p class="normal">We then looked at different RNN topologies and how each topology is adapted to a particular set of problems. After a lot of theory, we finally saw examples of three of these topologies. We then focused on one of these topologies, called seq2seq, which first gained popularity in the machine translation community, but has since been used in situations where the use case can be adapted to look like a machine translation problem.</p>
<p class="normal">From here, we looked at attention, which started as a way to improve the performance of seq2seq networks but has since been used very effectively in many situations where we want to compress the representation while keeping the data loss to a minimum. We looked at different kinds of attention and an example of using them in a seq2seq network with attention.</p>
<p class="normal">In the next chapter, you will learn about transformers, a state-of-the-art encoder-decoder architecture where the recurrent layers have been replaced by attention layers.</p>
<h1 class="heading-1" id="_idParaDest-143">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Jozefowicz, R., Zaremba, R. and Sutskever, I. (2015). <em class="italic">An Empirical Exploration of Recurrent Neural Network Architectures</em>. Journal of Machine Learning</li>
<li class="numberedList">Greff, K., et al. (July 2016). <em class="italic">LSTM: A Search Space Odyssey</em>. IEEE Transactions on Neural Networks and Learning Systems</li>
<li class="numberedList">Bernal, A., Fok, S., and Pidaparthi, R. (December 2012). <em class="italic">Financial Markets Time Series Prediction with Recurrent Neural Networks</em></li>
<li class="numberedList">Hadjeres, G., Pachet, F., and Nielsen, F. (August 2017). <em class="italic">DeepBach: a Steerable Model for Bach Chorales Generation</em>. Proceedings of the 34th International Conference on Machine Learning (ICML)</li>
<li class="numberedList">Karpathy, A. (2015). <em class="italic">The Unreasonable Effectiveness of Recurrent Neural Networks</em>. URL: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><span class="url">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</span></a></li>
<li class="numberedList">Karpathy, A., Li, F. (2015). <em class="italic">Deep Visual-Semantic Alignments for Generating Image Descriptions</em>. Conference on Pattern Recognition and Pattern Recognition (CVPR)</li>
<li class="numberedList">Socher, et al. (2013). <em class="italic">Recursive Deep Models for Sentiment Compositionality over a Sentiment Treebank</em>. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</li>
<li class="numberedList">Bahdanau, D., Cho, K., and Bengio, Y. (2015). <em class="italic">Neural Machine Translation by Jointly Learning to Align and Translate</em>. arXiv: 1409.0473 [cs.CL]</li>
<li class="numberedList">Wu, Y., et al. (2016). <em class="italic">Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</em>. arXiv 1609.08144 [cs.CL]</li>
<li class="numberedList">Vinyals, O., et al. (2015). <em class="italic">Grammar as a Foreign Language</em>. Advances in Neural Information Processing Systems (NIPS)</li>
<li class="numberedList">Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985). <em class="italic">Learning Internal Representations by Error Propagation</em>. Parallel Distributed Processing: Explorations in the Microstructure of Cognition</li>
<li class="numberedList">Britz, D. (2015). <em class="italic">Recurrent Neural Networks Tutorial, Part 3 - Backpropagation Through Time and Vanishing Gradients</em>: <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"><span class="url">http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/</span></a></li>
<li class="numberedList">Pascanu, R., Mikolov, T., and Bengio, Y. (2013). <em class="italic">On the difficulty of training Recurrent Neural Networks</em>. Proceedings of the 30th International Conference on Machine Learning (ICML)</li>
<li class="numberedList">Hochreiter, S., and Schmidhuber, J. (1997). <em class="italic">LSTM can solve hard long time lag problems</em>. Advances in Neural Information Processing Systems (NIPS)</li>
<li class="numberedList">Britz, D. (2015). <em class="italic">Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano</em>: <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"><span class="url">http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/</span></a></li>
<li class="numberedList">Olah, C. (2015). <em class="italic">Understanding LSTM Networks</em>: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"><span class="url">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</span></a></li>
<li class="numberedList">Cho, K., et al. (2014). <em class="italic">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em>. arXiv: 1406.1078 [cs.CL]</li>
<li class="numberedList">Shi, X., et al. (2015). <em class="italic">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</em>. arXiv: 1506.04214 [cs.CV]</li>
<li class="numberedList">Gers, F.A., and Schmidhuber, J. (2000). <em class="italic">Recurrent Nets that Time and Count</em>. Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN)</li>
<li class="numberedList">Kotzias, D. (2015). <em class="italic">Sentiment Labeled Sentences Dataset</em>, provided as part of “From Group to Individual Labels using Deep Features” (KDD 2015): <a href="https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences"><span class="url">https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences</span></a></li>
<li class="numberedList">Collobert, R., et al (2011). <em class="italic">Natural Language Processing (Almost) from Scratch</em>. Journal of Machine Learning Research (JMLR)</li>
<li class="numberedList">Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). <em class="italic">Building a large annotated corpus of English: the Penn Treebank</em>. Journal of Computational Linguistics</li>
<li class="numberedList">Bird, S., Loper, E., and Klein, E. (2009). <em class="italic">Natural Language Processing with Python, O’Reilly Media Inc</em>. Installation: <a href="https://www.nltk.org/install.xhtml"><span class="url">https://www.nltk.org/install.xhtml</span></a></li>
<li class="numberedList">Liu, C., et al. (2017). <em class="italic">MAT: A Multimodal Attentive Translator for Image Captioning</em>. arXiv: 1702.05658v3 [cs.CV]</li>
<li class="numberedList">Suilin, A. (2017). <em class="italic">Kaggle Web Traffic Time Series Forecasting</em>. GitHub repository: <a href="https://github.com/Arturus/kaggle-web-traffic"><span class="url">https://github.com/Arturus/kaggle-web-traffic</span></a></li>
<li class="numberedList">Tatoeba Project. (1997-2019). Tab-delimited Bilingual Sentence Pairs: <a href="https://tatoeba.org/en/"><span class="url">http://tatoeba.org </span></a>and <a href="http://www.manythings.org/anki"><span class="url">http://www.manythings.org/anki</span></a></li>
<li class="numberedList">Graves, A., Wayne, G., and Danihelka, I. (2014). <em class="italic">Neural Turing Machines</em>. arXiv: 1410.5401v2 [cs.NE]</li>
<li class="numberedList">Bahdanau, D., Cho, K., and Bengio, Y. (2015). <em class="italic">Neural Machine Translation by jointly learning to Align and Translate</em>. arXiv: 1409.0473v7 [cs.CL]</li>
<li class="numberedList">Luong, M., Pham, H., and Manning, C. (2015). <em class="italic">Effective Approaches to Attention-based Neural Machine Translation</em>. arXiv: 1508.04025v5 [cs.CL]</li>
<li class="numberedList">Vaswani, A., et al. (2017). <em class="italic">Attention Is All You Need</em>. 31st Conference on Neural Information Processing Systems (NeurIPS)</li>
<li class="numberedList">Zhang, A., Lipton, Z. C., Li, M., and Smola, A. J. (2019). <em class="italic">Dive into Deep Learning</em>: <a href="http://www.d2l.ai"><span class="url">http://www.d2l.ai</span></a></li>
<li class="numberedList">Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). <em class="italic">Layer Normalization</em>. arXiv: 1607.06450v1 [stat.ML]</li>
<li class="numberedList">Allamar, J. (2018). <em class="italic">The Illustrated Transformer</em>: <a href="http://jalammar.github.io/illustrated-transformer/"><span class="url">http://jalammar.github.io/illustrated-transformer/</span></a></li>
<li class="numberedList">Honnibal, M. (2016). <em class="italic">Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models</em>: <a href="https://explosion.ai/blog/deep-learning-formula-nlp"><span class="url">https://explosion.ai/blog/deep-learning-formula-nlp</span></a></li>
<li class="numberedList">Papineni, K., Roukos, S., Ward, T., and Zhu, W. (2002). <em class="italic">BLEU: A Method for Automatic Evaluation of Machine Translation</em>. Proceedings of the 40th Annual Meeting for the Association of Computational Linguistics (ACL)</li>
<li class="numberedList">Project Gutenberg (2019): <a href="https://www.gutenberg.org/"><span class="url">https://www.gutenberg.org/</span></a></li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>