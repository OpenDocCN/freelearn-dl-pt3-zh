<html><head></head><body>
		<div>
			<div id="_idContainer310" class="Content">
			</div>
		</div>
		<div id="_idContainer311" class="Content">
			<h1 id="_idParaDest-164"><a id="_idTextAnchor188"/>9. Recurrent Neural Networks</h1>
		</div>
		<div id="_idContainer351" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will learn how to handle real sequential data. You will extend your knowledge of <strong class="bold">artificial neural network</strong> (<strong class="bold">ANN</strong>) models and <strong class="bold">recurrent neural network</strong> (<strong class="bold">RNN</strong>) architecture for training sequential data. You will also learn how to build an RNN model with an LSTM layer for natural language processing. </p>
			<p class="callout">By the end of this chapter, you will have gained hands-on experience of applying multiple LSTM layers to build RNNs for stock price predictions.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor189"/>Introduction</h1>
			<p>Sequential data refers to datasets in which each data point is dependent on the previous ones. Think of it like a sentence, which is composed of a sequence of words that are related to each other. A verb will be linked to a subject and an adverb will be related to a verb. Another example is a stock price, where the price on a particular day is related to the price of the previous days. Traditional neural networks are not fit for processing this kind of data. There is a specific type of architecture that can ingest sequences of data. This chapter will introduce you to such models—known as <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>). </p>
			<p>An RNN model is a specific type of deep learning architecture in which the output of the model feeds back into the input. Models of this kind have their own challenges (known as vanishing and exploding gradients) that will be addressed later in the chapter.</p>
			<p>In many ways, an RNN is a representation of how a brain might work. RNNs use memory to help them learn. But how can they do this if information only flows in one direction? To understand this, you'll need to first review sequential data. This is a type of data that requires a working memory to process data effectively. Until now, you have only explored non-sequential models, such as a perceptron or CNN. In this chapter, you will look at sequential models such as RNN, LSTM, or GRU.</p>
			<div>
				<div id="_idContainer312" class="IMG---Figure">
					<img src="image/B16341_09_01.jpg" alt="Figure 9.1: Sequential versus non-sequential models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1: Sequential versus non-sequential models</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor190"/>Sequential Data</h1>
			<p>Sequential data is information that happens in a sequence and is related to past and future data. An example of sequential data is time series data; as you perceive it, time only travels in one direction.</p>
			<p>Suppose you have a ball (as in <em class="italic">Figure 9.2</em>), and you want to predict where this ball will travel next. If you have no prior information about the direction from which the ball was thrown, you will simply have to guess. However, if in addition to the ball's current location, you also had information about its previous location, the problem would be much simpler. To be able to predict the ball's next location, you need the previous location information in a sequential (or ordered) form to make a prediction about future events. </p>
			<div>
				<div id="_idContainer313" class="IMG---Figure">
					<img src="image/B16341_09_02.jpg" alt="Figure 9.2: Direction of the ball&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2: Direction of the ball</p>
			<p>RNNs function in a way that allows the sequence of the information to retain value with the help of internal memory.</p>
			<p>You'll take a look at some examples of sequential data in the following section. </p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor191"/>Examples of Sequential Data</h2>
			<p>Sequential data is a specific type of data where the order of each piece of information is important, and they all depend on each other.</p>
			<p>One example of sequential data is financial data, such as stock prices. If you want to predict future data values for a given stock, you need to use previous values in time. In fact, you will work on stock prediction in <em class="italic">Exercise 9.01</em>, <em class="italic">Training an ANN for Sequential Data – Nvidia Stock Prediction</em>.</p>
			<p>Audio and text can also be considered sequential data. Audio can be split up into a sequence of sound waves, and text can be split up into sequences of either characters or words. The sound waves or sequences of characters or words should be processed in order to convey the desired result. Beyond these two examples that you encounter every day, there are many more examples in which sequential processing may be useful, from analyzing medical signals such as EEGs, projecting stock prices, and inferring and understanding genomic sequences. There are three categories of sequential data:</p>
			<ul>
				<li><strong class="bold">Many-to-One</strong> produces one output from many inputs.</li>
				<li><strong class="bold">One-to-Many</strong> produces many outputs from one input.</li>
				<li><strong class="bold">Many-to-Many</strong> produces many outputs from many inputs.</li>
			</ul>
			<div>
				<div id="_idContainer314" class="IMG---Figure">
					<img src="image/B16341_09_03.jpg" alt="Figure 9.3: Categories of sequential data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3: Categories of sequential data</p>
			<p>Consider another example. Suppose you have a language model with a sentence or a phrase and you are trying to predict the word that comes next, as in the following figure:</p>
			<div>
				<div id="_idContainer315" class="IMG---Figure">
					<img src="image/B16341_09_04.jpg" alt="Figure 9.4: Sentence example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4: Sentence example</p>
			<p>Say you're given the words <strong class="source-inline">yesterday I took my car out for a…</strong>, and you want to try to predict the next word, <strong class="source-inline">drive</strong>. One way you could do this is by building a deep neural network such as a feed-forward neural network. However, you would immediately run into a problem. A feed-forward network can only take a fixed-length input vector as its input; you have to specify the size of that input right from the start.</p>
			<p>Because of this, your model needs a way to be able to handle variable-length inputs. One way you can do this is by using a fixed window. That means that you force your input vector to be just a certain length. For example, you can split the sentence into groups of two consecutive words (also called a <strong class="bold">bi-gram</strong>) and predict the next one. This means that no matter where you're trying to make that next prediction, your model will only be taking in the previous two words as its input. You need to consider how you can numerically represent this data. One way you can do this is by taking a fixed-length vector and allocating some space in that vector for the first word and some space in that vector for the second word. In those spaces, encode the identity of each word. However, this is problematic.</p>
			<p>Why? Because you're using only a portion of the information available (that is, two consecutive words only). You have access to a limited window of data that doesn't give enough context to accurately predict what will be the next word. That means you cannot effectively model long-term dependencies. This is important in sentences like the one in <em class="italic">Figure 9.5</em> where you clearly need information from much earlier in the sentence to be able to accurately predict the next word.</p>
			<div>
				<div id="_idContainer316" class="IMG---Figure">
					<img src="image/B16341_09_05.jpg" alt="Figure 9.5: Sentence example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5: Sentence example</p>
			<p>If you were only looking at the past two or three words, you wouldn't be able to make this next prediction, which you know is <strong class="source-inline">Italian</strong>. So, this means that you really need a way to integrate the information in the sentence from start to finish.</p>
			<p>To do this, you could use a set of counts as a fixed-length vector and use the entire sentence. This method is known as <strong class="bold">bag of words</strong>.</p>
			<p>You have a fixed-length vector regardless of the identity of the sentence, but what differs is adding the counts over this vocabulary. You can feed this into your model as an input to generate a prediction.</p>
			<p>However, there's another big problem with this. Using just the counts means that you lose all sequential information and all information about the prior history.</p>
			<p>Consider <em class="italic">Figure 9.6</em>. So, these two sentences, which have completely opposite semantic meanings would have the exact same representations in this bag of words format. This is because they have the exact same list of words, just in a different order. So, obviously, this isn't going to work. Another idea could be simply to extend the fixed window.</p>
			<div>
				<div id="_idContainer317" class="IMG---Figure">
					<img src="image/B16341_09_06.jpg" alt="Figure 9.6: Bag of words example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6: Bag of words example</p>
			<p>Now, consider <em class="italic">Figure 9.7</em>. You can represent your sentence in this way, feed the sentence into your model, and generate your prediction. The problem is that if you were to feed this vector into a feed-forward neural network, each of these inputs, <strong class="source-inline">yesterday I took my car</strong>, would have a separate weight connecting it to the network. So, if you were to repeatedly see the word <strong class="source-inline">yesterday</strong> at the beginning of the sentence, the network may be able to learn that <strong class="source-inline">yesterday</strong> represents a time or a setting. However, if <strong class="source-inline">yesterday</strong> were to suddenly appear later in that fixed-length vector, at the end of a sentence, the network may have difficulty understanding the meaning of <strong class="source-inline">yesterday</strong>. This is because the parameters that are at the end of a vector may never have seen the term <strong class="source-inline">yesterday</strong> before, and the parameters from the beginning of the sentence weren't shared across the entire sequence.</p>
			<div>
				<div id="_idContainer318" class="IMG---Figure">
					<img src="image/B16341_09_07.jpg" alt="Figure 9.7: Sentence example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7: Sentence example</p>
			<p>So, you need to be able to handle variable-length input and long-term dependencies, track sequential order, and have parameters that can be shared across the entirety of your sequence. Specifically, you need to develop models that can do the following:</p>
			<ul>
				<li>Handle variable-length input sequences.</li>
				<li>Track long-term dependencies in the data.</li>
				<li>Maintain information about the sequence's order.</li>
				<li>Share parameters across the entirety of the sequence.</li>
			</ul>
			<p>How can you do this with a model where information only flows in one direction? You need a different kind of neural network. You need a recursive model. You will practice processing sequential data in the following exercise.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor192"/>Exercise 9.01: Training an ANN for Sequential Data – Nvidia Stock Prediction</h2>
			<p>In this exercise, you will build a simple ANN model to predict the Nvidia stock price. But unlike examples from previous chapters, this time the input data is sequential. So, you need to manually do some processing to create a dataset that will contain the price of the stock for a given day as the target variable and the price for the previous 60 days as features. You are required to split the data into training and testing sets before and after the date <strong class="source-inline">2019-01-01</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the <strong class="source-inline">NVDA.csv</strong> dataset here: <a href="https://packt.link/Mxi80">https://packt.link/Mxi80</a>.</p>
			<ol>
				<li>Open a new Jupyter or Colab notebook.</li>
				<li>Import the libraries needed. Use <strong class="source-inline">numpy</strong> for computation, <strong class="source-inline">matplotlib</strong> for plotting visualization, <strong class="source-inline">pandas</strong> to help work with your dataset, and <strong class="source-inline">MinMaxScaler</strong> to scale the dataset between zero and one:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.preprocessing import StandardScaler, MinMaxScaler</p></li>
				<li>Use the <strong class="source-inline">read_csv()</strong> function to read in the CSV file and store your dataset in a pandas DataFrame, <strong class="source-inline">data</strong>, for manipulation:<p class="source-code">import io</p><p class="source-code">data = pd.read_csv('NVDA.csv')</p></li>
				<li>Call the <strong class="source-inline">head()</strong> function on your data to take a look at the first five rows of your DataFrame: <p class="source-code">data.head()</p><p>You should get the following output:</p><div id="_idContainer319" class="IMG---Figure"><img src="image/B16341_09_08.jpg" alt="Figure 9.8: First five rows of output&#13;&#10;"/></div><p class="figure-caption">Figure 9.8: First five rows of output</p><p>The preceding table shows the raw data. You can see that each row represents a day where you have information about the stock price when the market opened and closed, the highest price, the lowest price, and the adjusted close price of the stock (taking into account dividend or stock split, for instance).</p></li>
				<li>Now, split the training data. Use all data that is older than <strong class="source-inline">2019-01-01</strong> using the <strong class="source-inline">Date</strong> column for your training data. Save it as <strong class="source-inline">data_training</strong>. Save this in a separate file by using the <strong class="source-inline">copy()</strong> method:<p class="source-code">data_training = data[data['Date']&lt;'2019-01-01'].copy()</p></li>
				<li>Now, split the test data. Use all data that is more recent than or equal to <strong class="source-inline">2019-01-01</strong> using the <strong class="source-inline">Date</strong> column. Save it as <strong class="source-inline">data_test</strong>. Save this in a separate file by using the <strong class="source-inline">copy()</strong> method:<p class="source-code">data_test = data[data['Date']&gt;='2019-01-01'].copy()</p></li>
				<li>Use <strong class="source-inline">drop()</strong> to remove your <strong class="source-inline">Date</strong> and <strong class="source-inline">Adj Close</strong> columns in your DataFrame. Remember that you used the <strong class="source-inline">Date</strong> column to split your training and test sets, so the date information is not needed. Use <strong class="source-inline">axis = 1</strong> to specify that you also want to drop labels from your columns. To make sure it worked, call the <strong class="source-inline">head()</strong> function to take a look at the first five rows of the DataFrame:<p class="source-code">training_data = data_training.drop\</p><p class="source-code">                (['Date', 'Adj Close'], axis = 1)</p><p class="source-code">training_data.head()</p><p>You should get the following output:</p><p> </p><div id="_idContainer320" class="IMG---Figure"><img src="image/B16341_09_09.jpg" alt="Figure 9.9: New training data&#13;&#10;"/></div><p class="figure-caption">Figure 9.9: New training data</p><p>This is the output you should get after removing those two columns.</p></li>
				<li>Create a scaler from <strong class="source-inline">MinMaxScaler</strong> to scale <strong class="source-inline">training_data</strong> to numbers between zero and one. Use the <strong class="source-inline">fit_transform</strong> function to fit the model to the data and then transform the data according to the fitted model:<p class="source-code">scaler = MinMaxScaler()</p><p class="source-code">training_data = scaler.fit_transform(training_data)</p><p class="source-code">training_data</p><p>You should get the following output:</p><div id="_idContainer321" class="IMG---Figure"><img src="image/B16341_09_10.jpg" alt="Figure 9.10: Scaled training data&#13;&#10;"/></div><p class="figure-caption">Figure 9.10: Scaled training data</p></li>
				<li>Split your data into <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong> datasets:<p class="source-code">X_train = []</p><p class="source-code">y_train = []</p></li>
				<li>Check the shape of <strong class="source-inline">training_data</strong>:<p class="source-code">training_data.shape[0]</p><p>You should get the following output:</p><p class="source-code">868</p><p>You can see there are 868 observations in the training set.</p></li>
				<li>Create a training dataset that has the previous 60 days' stock prices so that you can predict the closing stock price for day 61. Here, <strong class="source-inline">X_train</strong> will have two columns. The first column will store the values from 0 to 59, and the second will store values from 1 to 60. In the first column of <strong class="source-inline">y_train</strong>, store the 61st value at index 60, and in the second column, store the 62nd value at index 61. Use a <strong class="source-inline">for</strong> loop to create data in 60 time steps:<p class="source-code">for i in range(60, training_data.shape[0]):</p><p class="source-code">    X_train.append(training_data[i-60:i])</p><p class="source-code">    y_train.append(training_data[i, 0])</p></li>
				<li>Convert <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong> into NumPy arrays:<p class="source-code">X_train, y_train = np.array(X_train), np.array(y_train)</p></li>
				<li>Call the <strong class="source-inline">shape()</strong> function on <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong>:<p class="source-code">X_train.shape, y_train.shape</p><p>You should get the following output:</p><p class="source-code">((808, 60, 5), (808,))</p><p>The preceding snippet shows that the prepared training set contains <strong class="source-inline">808</strong> observations with <strong class="source-inline">60</strong> days of data for the five features you kept (<strong class="source-inline">Open</strong>, <strong class="source-inline">Low</strong>, <strong class="source-inline">High</strong>, <strong class="source-inline">Close</strong>, and <strong class="source-inline">Volume</strong>).</p></li>
				<li>Transform the data into a 2D matrix with the shape of the sample (the number of samples and the number of features in each sample). Stack the features for all 60 days on top of each other to get an output size of <strong class="source-inline">(808, 300)</strong>. Use the following code for this purpose:<p class="source-code">X_old_shape = X_train.shape</p><p class="source-code">X_train = X_train.reshape(X_old_shape[0], \</p><p class="source-code">                          X_old_shape[1]*X_old_shape[2]) </p><p class="source-code">X_train.shape</p><p>You should get the following output:</p><p class="source-code">(808, 300)</p></li>
				<li>Now, build an ANN. You will need some additional libraries for this. Use <strong class="source-inline">Sequential</strong> to initialize the neural net, <strong class="source-inline">Input</strong> to add an input layer, <strong class="source-inline">Dense</strong> to add a dense layer, and <strong class="source-inline">Dropout</strong> to help prevent overfitting:<p class="source-code">from tensorflow.keras import Sequential</p><p class="source-code">from tensorflow.keras.layers import Input, Dense, Dropout</p></li>
				<li>Initialize the neural network by calling <strong class="source-inline">regressor_ann = Sequential()</strong>. <p class="source-code"><strong class="source-inline">regressor_ann = Sequential()</strong></p></li>
				<li>Add an input layer with <strong class="source-inline">shape</strong> as <strong class="source-inline">300</strong>: <p class="source-code">regressor_ann.add(Input(shape = (300,)))</p></li>
				<li>Then, add the first dense layer. Set it to <strong class="source-inline">512</strong> units, which will be your dimensionality for the output space. Use a ReLU activation function. Finally, add a dropout layer that will remove 20% of the units during training to prevent overfitting: <p class="source-code">regressor_ann.add(Dense(units = 512, activation = 'relu'))</p><p class="source-code">regressor_ann.add(Dropout(0.2))</p></li>
				<li>Add another dense layer with <strong class="source-inline">128</strong> units, ReLU as the activation function, and a dropout of <strong class="source-inline">0.3</strong>:<p class="source-code">regressor_ann.add(Dense(units = 128, activation = 'relu'))</p><p class="source-code">regressor_ann.add(Dropout(0.3))</p></li>
				<li>Add another dense layer with <strong class="source-inline">64</strong> units, ReLU as the activation function, and a dropout of <strong class="source-inline">0.4</strong>:<p class="source-code">regressor_ann.add(Dense(units = 64, activation = 'relu'))</p><p class="source-code">regressor_ann.add(Dropout(0.4))</p></li>
				<li>Again, add another dense layer with <strong class="source-inline">128</strong> units, ReLU as the activation function, and a dropout of <strong class="source-inline">0.3</strong>:<p class="source-code">regressor_ann.add(Dense(units = 16, activation = 'relu'))</p><p class="source-code">regressor_ann.add(Dropout(0.5))</p></li>
				<li>Add a final dense layer with one unit:<p class="source-code">regressor_ann.add(Dense(units = 1))</p></li>
				<li>Check the summary of the model: <p class="source-code">regressor_ann.summary()</p><p>You will get valuable information about your model layers and parameters.</p><div id="_idContainer322" class="IMG---Figure"><img src="image/B16341_09_11.jpg" alt="Figure 9.11: Model summary&#13;&#10;"/></div><p class="figure-caption">Figure 9.11: Model summary</p></li>
				<li>Use the <strong class="source-inline">compile()</strong> method to configure your model for training. Choose Adam as your optimizer and mean squared error to measure your loss function:<p class="source-code">regressor_ann.compile(optimizer='adam', \</p><p class="source-code">                      loss = 'mean_squared_error')</p></li>
				<li>Finally, fit your model and set it to run on <strong class="source-inline">10</strong> epochs. Set your batch size to <strong class="source-inline">32</strong>:<p class="source-code">regressor_ann.fit(X_train, y_train, epochs=10, batch_size=32)</p><p>You should get the following output:</p><p> </p><div id="_idContainer323" class="IMG---Figure"><img src="image/B16341_09_12.jpg" alt="Figure 9.12: Training the model&#13;&#10;"/></div><p class="figure-caption">Figure 9.12: Training the model</p></li>
				<li>Test and predict the stock price and prepare the dataset. Check your data by calling the <strong class="source-inline">head()</strong> method:<p class="source-code">data_test.head()</p><p>You should get the following output:</p><div id="_idContainer324" class="IMG---Figure"><img src="image/B16341_09_13.jpg" alt="Figure 9.13: First five rows of a DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 9.13: First five rows of a DataFrame</p></li>
				<li>Use the <strong class="source-inline">tail(60)</strong> method to create a <strong class="source-inline">past_60_days</strong> variable, which consists of the last 60 days of data in the training set. Add the <strong class="source-inline">past_60_days</strong> variable to the test data with the <strong class="source-inline">append()</strong> function. Assign <strong class="source-inline">True</strong> to <strong class="source-inline">ignore_index</strong>: <p class="source-code">past_60_days = data_training.tail(60)</p><p class="source-code">df = past_60_days.append(data_test, ignore_index = True)</p></li>
				<li>Now, prepare your test data for predictions by repeating what you did for the training data in <em class="italic">steps 8</em> to <em class="italic">15</em>: <p class="source-code">df = df.drop(['Date', 'Adj Close'], axis = 1)</p><p class="source-code">inputs = scaler.transform(df) </p><p class="source-code">X_test = []</p><p class="source-code">y_test = []</p><p class="source-code">for i in range(60, inputs.shape[0]):</p><p class="source-code">    X_test.append(inputs[i-60:i])</p><p class="source-code">    y_test.append(inputs[i, 0])</p><p class="source-code">X_test, y_test = np.array(X_test), np.array(y_test)</p><p class="source-code">X_old_shape = X_test.shape</p><p class="source-code">X_test = X_test.reshape(X_old_shape[0], \</p><p class="source-code">                        X_old_shape[1] * X_old_shape[2])</p><p class="source-code">X_test.shape, y_test.shape</p><p>You should get the following output:</p><p class="source-code">((391, 300), (391,))</p></li>
				<li>Test some predictions for your stock prices by calling the <strong class="source-inline">predict()</strong> method on <strong class="source-inline">X_test</strong>: <p class="source-code">y_pred = regressor_ann.predict(X_test)</p></li>
				<li>Before looking at the results, reverse the scaling you did earlier so that the number you get as output will be at the correct scale using the <strong class="source-inline">StandardScaler</strong> utility class that you imported with <strong class="source-inline">scaler.scale_</strong>:<p class="source-code">scaler.scale_</p><p>You should get the following output:</p><div id="_idContainer325" class="IMG---Figure"><img src="image/B16341_09_14.jpg" alt="Figure 9.14: Using StandardScaler&#13;&#10;"/></div><p class="figure-caption">Figure 9.14: Using StandardScaler</p></li>
				<li>Use the first value in the preceding array to set your scale in preparation for the multiplication of <strong class="source-inline">y_pred</strong> and <strong class="source-inline">y_test</strong>. Recall that you are converting your data back from your earlier scale, in which you converted all values to between zero and one:<p class="source-code">scale = 1/3.70274364e-03</p><p class="source-code">scale </p><p>You should get the following output:</p><p class="source-code">270.0700067909643</p></li>
				<li>Multiply <strong class="source-inline">y_pred</strong> and <strong class="source-inline">y_test</strong> by <strong class="source-inline">scale</strong> to convert your data back to the proper values:<p class="source-code">y_pred = y_pred*scale</p><p class="source-code">y_test = y_test*scale</p></li>
				<li>Review the real Nvidia stock price and your predictions:<p class="source-code">plt.figure(figsize=(14,5))</p><p class="source-code">plt.plot(y_test, color = 'black', label = "Real NVDA Stock Price")</p><p class="source-code">plt.plot(y_pred, color = 'gray',\</p><p class="source-code">         label = 'Predicted NVDA Stock Price')</p><p class="source-code">plt.title('NVDA Stock Price Prediction')</p><p class="source-code">plt.xlabel('time')</p><p class="source-code">plt.ylabel('NVDA Stock Price')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>You should get the following output:</p><div id="_idContainer326" class="IMG---Figure"><img src="image/B16341_09_15.jpg" alt="Figure 9.15: Real Nvidia stock price versus your predictions&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.15: Real Nvidia stock price versus your predictions</p>
			<p>In the preceding graph, you can see that your trained model is able to capture some of the trends of the Nvidia stock price. Observe that the predictions are quite different from the real values. It is evident from this result that ANNs are not suited for sequential data.</p>
			<p>In this exercise, you saw the inability of simple ANNs to deal with sequential data. In the next section, you will learn about recurrent neural networks, which are designed to learn from the temporal dimensionality of sequential data. Then, in <em class="italic">Exercise 9.02</em>, <em class="italic">Building an RNN with LSTM Layer Nvidia Stock Prediction</em>, you will perform predictions on the same Nvidia stock price dataset using RNNs and compare your results.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor193"/>Recurrent Neural Networks</h1>
			<p>The first formulation of a recurrent-like neural network was created by John Hopfield in 1982. He had two motivations for doing so:</p>
			<ul>
				<li>Sequential processing of data</li>
				<li>Modeling of neuronal connectivity</li>
			</ul>
			<p>Essentially, an RNN processes input data at each time step and stores information in its memory that will be used for the next step. Information is first transformed into vectors that can be processed by machines. The RNN then processes the vector sequence one at a time. As it processes each vector, it passes the previous hidden state. The hidden state retains information from the previous step, acting as a type of memory. It does this by combining the input and the previous hidden state with a tanh function that compresses the values between <strong class="source-inline">-1</strong> and <strong class="source-inline">1</strong>.</p>
			<p>Essentially, this is how the RNN functions. RNNs don't need a lot of computation and work well with short sequences.</p>
			<div>
				<div id="_idContainer327" class="IMG---Figure">
					<img src="image/B16341_09_16.jpg" alt="Figure 9.16: RNN data flow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.16: RNN data flow</p>
			<p>Now turn your attention to applying neural networks to problems that involve sequential processing of data. You've already learned a bit about why these sorts of tasks require a fundamentally different type of network architecture from what you've seen so far.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor194"/>RNN Architecture</h2>
			<p>This section will go through the key principles behind RNNs, how they are fundamentally different from what you've learned so far, and how RNN computation actually works.</p>
			<p>But before you do that, take one step back and consider the standard feed-forward neural network that was discussed previously.</p>
			<p>In feed-forward neural networks, data propagates in one direction only, that is, from input to output.</p>
			<p>Therefore, you need a different kind of network architecture to handle sequential data. RNNs are particularly well-suited to handling cases in which you have a sequence of inputs rather than a single input. These are great for problems in which a sequence of data is being propagated to give a single output.</p>
			<p>For example, imagine that you are training a model that takes a sequence of words as input and outputs an emotion associated with that sequence. Similarly, consider cases in which, instead of returning a single output, you could have a sequence of inputs and propagate them through your network, where each time step in the sequence generates an output.</p>
			<p>Simply put, RNNs are networks that offer a mechanism to persist previously processed data over time and use it to make future predictions.</p>
			<div>
				<div id="_idContainer328" class="IMG---Figure">
					<img src="image/B16341_09_17.jpg" alt="Figure 9.17: RNN computation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17: RNN computation</p>
			<p>In the preceding diagram, at some time step denoted by t, the RNN takes in <strong class="source-inline">X</strong><span class="subscript">t</span> as the input, and at that time step, it computes a prediction value, <strong class="source-inline">Y</strong><span class="subscript">t</span>, which is the output of the network.</p>
			<p>In addition to that output, it saved an internal state, called update, <strong class="source-inline">H</strong><span class="subscript">t</span>. This internal state from time step <strong class="source-inline">t</strong> can then be used to complement the input of the next time step <strong class="source-inline">t+1</strong>. So, basically, it provides information about the previous step to the next one. This mechanism is called <strong class="bold">recurrent</strong> because information is being passed from one time step to the next within the network.</p>
			<p>What's really happening here? This is done by using a simple recurrence relation to process the sequential data. RNNs maintain internal state, <strong class="source-inline">H</strong><span class="subscript">t</span>, and combine it with the next input data, <strong class="source-inline">X</strong><span class="subscript">t+1</span>, to make a prediction, <strong class="source-inline">Y</strong><span class="subscript">t+1</span>, and store the new internal state, <strong class="source-inline">H</strong><span class="subscript">t+1</span>. The key idea is that the state update is a combination of the previous state time step as well as the current input that the network is receiving.</p>
			<p>It's important to note that, in this computation, it's the same function <strong class="source-inline">f</strong> of <strong class="source-inline">W</strong> and the same set of parameters that are used at every time step, and it's those sets of parameters that you learn during the course of training. To get a better sense of how these networks work, step through the RNN algorithm:</p>
			<ol>
				<li value="1">You begin by initializing your RNN and the hidden state of that network. You can denote a sentence for which you are interested in predicting the next word. The RNN computation simply consists of them looping through the words in this sentence.</li>
				<li>At each time step, you feed both the current word that you're considering, as well as the previous hidden state of your RNN into the network. This can then generate a prediction for the next word in the sequence and use this information to update its hidden state.</li>
				<li>Finally, after you've looped through all the words in the sentence, your prediction for that missing word is simply the RNN's output at that final time step.</li>
			</ol>
			<p>As you can see in the following diagram, this RNN computation includes both the internal state update and the formal output vector.</p>
			<div>
				<div id="_idContainer329" class="IMG---Figure">
					<img src="image/B16341_09_18.jpg" alt="Figure 9.18: RNN data flow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.18: RNN data flow</p>
			<p>Given the input vector, <strong class="source-inline">X</strong><span class="subscript">t</span>, the RNN applies a function to update its hidden state. This function is simply a standard neural net operation. It consists of multiplication by a weight matrix and the application of a non-linearity activation function. The key difference is that, in this case, you're feeding in both the input vector, <strong class="source-inline">X</strong><span class="subscript">t</span>, and the previous state as inputs to this function, <strong class="source-inline">H</strong><span class="subscript">t-1</span>.</p>
			<p>Next, you apply a non-linearity activation function such as tanh to the previous step. You have these two weight matrices, and finally, your output, <strong class="source-inline">y</strong><span class="subscript">t</span>, at a given time step is then a modified, transformed version of this internal state.</p>
			<p>After you've looped through all the words in the sentence, your prediction for that missing word is simply the RNN's output at that final time step, after all the words have been fed through the model. So, as mentioned, RNN computation includes both internal state updates and formal output vectors.</p>
			<p>Another way you can represent RNNs is by unrolling their modules over time. You can think of RNNs as having multiple copies of the same network, where each passes a message on to its descendant.</p>
			<div>
				<div id="_idContainer330" class="IMG---Figure">
					<img src="image/B16341_09_19.jpg" alt="Figure 9.19: Computational graph with time&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.19: Computational graph with time</p>
			<p>In this representation, you can make your weight matrices explicit, beginning with the weights that transform the input to the <strong class="source-inline">H</strong> weights that are used to transform the previous hidden state to the current hidden state, and finally the hidden state to the output.</p>
			<p>It's important to note that you use the same weight matrices at every time step. From these outputs, you can compute a loss at each time step. The computation of the loss will then complete your forward propagation through the network. Finally, to define the total loss, you simply sum the losses from all of the individual time steps. Since your loss is dependent on each time step, this means that, in training the network, you will have to also involve time as a component.</p>
			<p>Now that you've got a bit of a sense of how these RNNs are constructed and how they function, you can walk through a simple example of how to implement an RNN from scratch in TensorFlow.</p>
			<p>The following snippet uses a simple RNN from <strong class="source-inline">keras.models.Sequential</strong>. You specify the number of units as <strong class="source-inline">1</strong> and set the first input dimension to <strong class="source-inline">None</strong> as an RNN can process any number of time steps. A simple RNN uses tanh activation by default:</p>
			<p class="source-code">model = keras.models.Sequential([</p>
			<p class="source-code">                                 keras.layers.SimpleRNN\</p>
			<p class="source-code">                                 (1, input_shape=[None, 1]) </p>
			<p class="source-code">])</p>
			<p>The preceding code creates a single layer with a single neuron.</p>
			<p>That was easy enough. Now you need to stack some additional recurrent layers. The code is similar, but there is a key difference here. You will notice <strong class="source-inline">return_sequences=True</strong> on all but the last layer. This is to ensure that the output is a 3D array. As you can see, the first two layers each have <strong class="source-inline">20</strong> units:</p>
			<p class="source-code">model = keras.models.Sequential\</p>
			<p class="source-code">        ([Keras.layers.SimpleRNN\</p>
			<p class="source-code">          (20, return_sequences=True, input_shape=[None, 1]), \</p>
			<p class="source-code">          Keras.layers.SimpleRNN(20, return_sequences=True), \</p>
			<p class="source-code">          Keras.layers.SimpleRNN(1)])</p>
			<p>The RNN is defined as a layer, and you can build it by inheriting it from the layer class. You can also initialize your weight matrices and the hidden state of your RNN cell to zero.</p>
			<p>The key step here is defining the call function, which describes how you make a forward pass through the network given an input <strong class="source-inline">X</strong>. And, to break down this call function, you would first update the hidden state according to the equation discussed previously.</p>
			<p>Take the previous hidden state and the input <strong class="source-inline">X</strong>, multiply them by the relevant weight matrices, add them together, and then pass them through a non-linearity, like a hyperbolic tangent (tanh).</p>
			<p>Then, the output is simply a transformed version of the hidden state, and at each time step, you return both the current output and the updated hidden state.</p>
			<p>TensorFlow has made it easy by having a built-in dense layer. The same applies to RNNs. TensorFlow has implemented these types of RNN cells with the simple RNN layer. But this type of layer has some limitations, such as vanishing gradients. You will look at this problem in the next section before exploring different types of recurrent layers.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor195"/>Vanishing Gradient Problem</h2>
			<p>If you take a closer look at how gradients flow in this chain of repeating modules, you can see that between each time step you need to perform matrix multiplication. That means that the computation of the gradient—that is, the derivative of the loss with respect to the parameters, tracing all the way back to your initial state—requires many repeated multiplications of this weight matrix, as well as repeated use of the derivative of your activation function.</p>
			<p>You can have one of two scenarios that could be particularly problematic: the exploding gradient problem or the vanishing gradient problem.</p>
			<p>The exploding gradients problem is when gradients become continuously larger and larger due to the matrix multiplication operation, and you can't optimize them anymore. One way you may be able to mitigate this is by performing what's called gradient clipping. This amounts to scaling back large gradients so that their values are smaller and closer to <strong class="source-inline">1</strong>.</p>
			<p>You can also have the opposite problem where your gradients are too small. This is what is known as the vanishing gradient problem. This is when gradients become increasingly smaller (close to <strong class="source-inline">0</strong>) as you make these repeated multiplications, and you can no longer train the network. This is a very real problem when it comes to training RNNs.</p>
			<p>For example, consider a scenario in which you keep multiplying a number by some number that's in between zero and one. As you keep doing this repeatedly, that number is constantly shrinking until, eventually, it vanishes and becomes 0. When this happens to gradients, it's hard to propagate errors further back into the past because the gradients are becoming smaller and smaller.</p>
			<p>Consider the earlier example from the language model where you were trying to predict the next word. If you're trying to predict the last word in the following phrase, it's relatively clear what the next word is going to be. There's not that much of a gap between the key relevant information, such as the word "fish," and the place where the prediction is needed.</p>
			<div>
				<div id="_idContainer331" class="IMG---Figure">
					<img src="image/B16341_09_20.jpg" alt="Figure 9.20: Word prediction &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.20: Word prediction </p>
			<p>However, there are other cases where more context is necessary, like in the following example. Information from early in the sentence, <strong class="source-inline">She lived in Spain</strong>, suggests that the next word of the sentence after <strong class="source-inline">she speaks fluent</strong> is most likely the name of a language, <strong class="source-inline">Spanish</strong>.</p>
			<div>
				<div id="_idContainer332" class="IMG---Figure">
					<img src="image/B16341_09_21.jpg" alt="Figure 9.21: Sentence example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.21: Sentence example</p>
			<p>But you need the context of <strong class="source-inline">Spain</strong>, which is located at a much earlier position in this sentence, to be able to fill in the relevant gaps and identify which language is correct. As this gap between words that are semantically important grows, RNNs become increasingly unable to connect the dots and link these relevant pieces of information together. That is due to the vanishing gradient problem.</p>
			<p>How can you alleviate this? The first trick is simple. You can choose either tanh or sigmoid as your activation function. Both of these functions have derivatives that are less than <strong class="source-inline">1</strong>.</p>
			<p>Another simple trick you can use is to initialize the weights for the parameters of your network. It turns out that initializing the weights to the identity matrix helps prevent them shrinking to zero too rapidly during back-propagation.</p>
			<p>But the final and most robust solution is to use a slightly more complex recurrent unit that can track long-term dependencies in the data more effectively. It can do this by controlling what information is passed through and what information is used to update its internal state. Specifically, this is the concept of a gated cell, like in the LSTM layer, which is the focus of the next section.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor196"/>Long Short-Term Memory Network</h2>
			<p>LSTMs are well-suited to learning long-term dependencies and overcoming the vanishing gradient problem. They are very performant models for sequential data, and they're widely used by the deep learning community.</p>
			<p>LSTMs have a chain-like structure. In an LSTM, the repeating unit contains different interacting layers. The key point is that these layers interact to selectively control the flow of information within the cell.</p>
			<p>The key building block of the LSTM is a structure called a gate, which functions to enable the LSTM to selectively add or remove information from its cell state. Gates consist of a neural net layer like a sigmoid.</p>
			<div>
				<div id="_idContainer333" class="IMG---Figure">
					<img src="image/B16341_09_22.jpg" alt="Figure 9.22: LSTM architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.22: LSTM architecture</p>
			<p>Take a moment to think about what a gate like this would do in an LSTM. In this case, the sigmoid function would force its input to be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. You can think of this mechanism as capturing how much of the information that's passed through the gate should be retained. It's between zero and one. This effectively gates the flow of information.</p>
			<p>LSTMs process information through four simple steps:</p>
			<ol>
				<li value="1">The first step in the LSTM is to decide what information is going to be thrown away from the cell state, to forget irrelevant history. This is a function of both the prior internal state, <strong class="source-inline">H</strong><span class="subscript">t-1</span>, and the input, <strong class="source-inline">X</strong><span class="subscript">t</span>, because some of that information may not be important.</li>
				<li>Next, the LSTM decides what part of the new information is relevant and uses this to store this information in its cell state.</li>
				<li>Then, it takes both the relevant parts of the prior information, as well as the current input, and uses this to selectively update its cell state.</li>
				<li>Finally, it returns an output, and this is known as the output gate, which controls what information encoded in the cell state is sent to the network.<div id="_idContainer334" class="IMG---Figure"><img src="image/B16341_09_23.jpg" alt="Figure 9.23: LSTM processing steps&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.23: LSTM processing steps</p>
			<p>The key takeaway here for LSTMs is the sequence of how they regulate information flow and storage. Once again, LSTMs operate as follows:</p>
			<ul>
				<li>Forgetting irrelevant history</li>
				<li>Storing what's new and what's important</li>
				<li>Using its internal memory to update the internal state </li>
				<li>Generating an output</li>
			</ul>
			<p>An important property of LSTMs is that all these different gating and update mechanisms work to create an internal cell state, <strong class="source-inline">C</strong>, which allows the uninterrupted flow of gradients through time. You can think of it as sort of a highway of cell states where gradients can flow uninterrupted. This enables you to alleviate and mitigate the vanishing gradient problem that's seen with standard RNNs.</p>
			<p>LSTMs are able to maintain this separate cell state independently of what is output, and they use gates to control the flow of information by forgetting irrelevant history, storing relevant new information, selectively updating their cell state, and then returning a filtered version as the output.</p>
			<p>The key point in terms of training and LSTMs is that maintaining the separate independent cell state allows the efficient training of an LSTM to backpropagate through time, which is discussed later.</p>
			<p>Now that you've gone through the fundamental workings of RNNs, the backpropagation through time algorithm, and a bit about the LSTM architecture, you can put some of these concepts to work in the following example.</p>
			<p>Consider the following LSTM model:</p>
			<p class="source-code">regressor = Sequential()</p>
			<p class="source-code">regressor.add(LSTM(units= 50, activation = 'relu', \</p>
			<p class="source-code">                   return_sequences = True, \</p>
			<p class="source-code">                   input_shape = (X_train.shape[1], 5)))</p>
			<p class="source-code">regressor.add(Dropout(0.2))</p>
			<p class="source-code">regressor.add(LSTM(units= 60, activation = 'relu', \</p>
			<p class="source-code">                   return_sequences = True))</p>
			<p class="source-code">regressor.add(Dropout(0.3))</p>
			<p class="source-code">regressor.add(LSTM(units= 80, activation = 'relu', \</p>
			<p class="source-code">                   return_sequences = True))</p>
			<p class="source-code">regressor.add(Dropout(0.4))</p>
			<p class="source-code">regressor.add(LSTM(units= 120, activation = 'relu'))</p>
			<p class="source-code">regressor.add(Dropout(0.5))</p>
			<p class="source-code">regressor.add(Dense(units = 1))</p>
			<p>First, you have initialized a neural network by calling <strong class="source-inline">regressor = Sequential()</strong>. Again, it's important to note that in the last line you omit <strong class="source-inline">return_sequences = True</strong> because it is the final output:</p>
			<p class="source-code">regressor = Sequential()</p>
			<p>Then, the LSTM layer is added. In the first instance, set the LSTM layer to <strong class="source-inline">50</strong> units. Use a relu activation function and specify the shape of the training set. Finally, the dropout layer is added with <strong class="source-inline">regressor.add(Dropout(0.2)</strong>. The <strong class="source-inline">0.2</strong> means that 20% of the layers will be removed. Set <strong class="source-inline">return_sequences = True</strong>, which allows the return of the last output.</p>
			<p>Similarly, add three more LSTM layers and one dense layer to the LSTM model.</p>
			<p>Now that you are familiar with the basic concepts surrounding working with sequential data, it's time to complete the following exercise using some real data.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor197"/>Exercise 9.02: Building an RNN with an LSTM Layer – Nvidia Stock Prediction</h2>
			<p>In this exercise, you will be working on the same dataset as for <em class="italic">Exercise 9.01</em>, <em class="italic">Training an ANN for Sequential Data – Nvidia Stock Prediction</em>. You will still try to predict the Nvidia stock price based on the data of the previous 60 days. But this time, you will be training an LSTM model. You will need to split the data into training and testing sets before and after the date <strong class="source-inline">2019-01-01</strong>.     </p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the <strong class="source-inline">NVDA.csv</strong> dataset here: <a href="https://packt.link/Mxi80">https://packt.link/Mxi80</a>.</p>
			<p>You will need to prepare the dataset like in <em class="italic">Exercise 9.01</em>, <em class="italic">Training an ANN for Sequential Data – Nvidia Stock Prediction</em> (<em class="italic">steps 1</em> to <em class="italic">15</em>) before applying the following code:</p>
			<ol>
				<li value="1">Start building the LSTM. You will need some additional libraries for this. Use <strong class="source-inline">Sequential</strong> to initialize the neural net, <strong class="source-inline">Dense</strong> to add a dense layer, <strong class="source-inline">LSTM</strong> to add an LSTM layer, and <strong class="source-inline">Dropout</strong> to help prevent overfitting:<p class="source-code">from tensorflow.keras import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense, LSTM, Dropout</p></li>
				<li>Initialize the neural network by calling <strong class="source-inline">regressor = Sequential()</strong>. Add four LSTM layers with <strong class="source-inline">50</strong>, <strong class="source-inline">60</strong>, <strong class="source-inline">80</strong>, and <strong class="source-inline">120</strong> units each. Use a ReLU activation function and assign <strong class="source-inline">True</strong> to <strong class="source-inline">return_sequences</strong> for all but the last LSTM layer. Provide the shape of your training set to the first LSTM layer. Finally, add dropout layers with 20%, 30%, 40%, and 50% dropouts:<p class="source-code">regressor = Sequential()</p><p class="source-code">regressor.add(LSTM(units= 50, activation = 'relu',\</p><p class="source-code">                   return_sequences = True,\</p><p class="source-code">                   input_shape = (X_train.shape[1], 5)))</p><p class="source-code">regressor.add(Dropout(0.2))</p><p class="source-code">regressor.add(LSTM(units= 60, activation = 'relu', \</p><p class="source-code">              return_sequences = True))</p><p class="source-code">regressor.add(Dropout(0.3))</p><p class="source-code">regressor.add(LSTM(units= 80, activation = 'relu', \</p><p class="source-code">              return_sequences = True))</p><p class="source-code">regressor.add(Dropout(0.4))</p><p class="source-code">regressor.add(LSTM(units= 120, activation = 'relu'))</p><p class="source-code">regressor.add(Dropout(0.5))</p><p class="source-code">regressor.add(Dense(units = 1))</p></li>
				<li>Check the summary of the model using the <strong class="source-inline">summary()</strong> method:<p class="source-code">regressor.summary()</p><p>You should get the following output:</p><div id="_idContainer335" class="IMG---Figure"><img src="image/B16341_09_24.jpg" alt="Figure 9.24: Model summary&#13;&#10;"/></div><p class="figure-caption">Figure 9.24: Model summary</p><p>As you can see from the preceding figure, the summary provides valuable information about all model layers and parameters. This is a good way to make sure that your layers are in the order you wish and that they have the proper output shapes and parameters.</p></li>
				<li>Use the <strong class="source-inline">compile()</strong> method to configure your model for training. Choose Adam as your optimizer and mean squared error to measure your loss function:<p class="source-code">regressor.compile(optimizer='adam', loss = 'mean_squared_error')</p></li>
				<li>Fit your model and set it to run on <strong class="source-inline">10</strong> epochs. Set your batch size equal to <strong class="source-inline">32</strong>:<p class="source-code">regressor.fit(X_train, y_train, epochs=10, batch_size=32)</p><p>You should get the following output:</p><div id="_idContainer336" class="IMG---Figure"><img src="image/B16341_09_25.jpg" alt="Figure 9.25: Training the model&#13;&#10;"/></div><p class="figure-caption">Figure 9.25: Training the model</p></li>
				<li>Test and predict the stock price and prepare the dataset. Check your data by calling the <strong class="source-inline">head()</strong> function:<p class="source-code">data_test.head()</p><p>You should get the following output:</p><div id="_idContainer337" class="IMG---Figure"><img src="image/B16341_09_26.jpg" alt="Figure 9.26: First five rows of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 9.26: First five rows of the DataFrame</p></li>
				<li>Call the <strong class="source-inline">tail(60)</strong> method to look at the last 60 days of data. You will use this information in the next step:<p class="source-code">data_training.tail(60)</p><p>You should get the following output:</p><div id="_idContainer338" class="IMG---Figure"><img src="image/B16341_09_27.jpg" alt="Figure 9.27: Last 10 rows of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 9.27: Last 10 rows of the DataFrame</p></li>
				<li>Use the <strong class="source-inline">tail(60)</strong> method to create the <strong class="source-inline">past_60_days</strong> variable:<p class="source-code">past_60_days = data_training.tail(60)</p></li>
				<li>Add the <strong class="source-inline">past_60_days</strong> variable to your test data with the <strong class="source-inline">append()</strong> function. Set <strong class="source-inline">True</strong> to <strong class="source-inline">ignore_index</strong>. Drop the <strong class="source-inline">Date</strong> and <strong class="source-inline">Adj Close</strong> columns as you will not need that information:<p class="source-code">df = past_60_days.append(data_test, ignore_index = True)</p><p class="source-code">df = df.drop(['Date', 'Adj Close'], axis = 1)</p></li>
				<li>Check the DataFrame to make sure that you successfully dropped <strong class="source-inline">Date</strong> and <strong class="source-inline">Adj Close</strong> by using the <strong class="source-inline">head()</strong> function:<p class="source-code">df.head()</p><p>You should get the following output:</p><div id="_idContainer339" class="IMG---Figure"><img src="image/B16341_09_28.jpg" alt="Figure 9.28: Checking the first five rows of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 9.28: Checking the first five rows of the DataFrame</p></li>
				<li>Use <strong class="source-inline">scaler.transform</strong> from <strong class="source-inline">StandardScaler</strong> to perform standardization on inputs:<p class="source-code">inputs = scaler.transform(df)</p><p class="source-code">inputs</p><p>You should get the following output:</p><div id="_idContainer340" class="IMG---Figure"><img src="image/B16341_09_29.jpg" alt="Figure 9.29: DataFrame standardization&#13;&#10;"/></div><p class="figure-caption">Figure 9.29: DataFrame standardization</p><p>From the preceding results, you can see that after standardization, all values are close to <strong class="source-inline">0</strong> now.</p></li>
				<li>Split your data into <strong class="source-inline">X_test</strong> and <strong class="source-inline">y_test</strong> datasets. Create a test dataset that has the previous 60 days' stock prices, so that you can test the closing stock price for the 61st day. Here, <strong class="source-inline">X_test</strong> will have two columns. The first column will store the values from 0 to 59. The second column will store values from 1 to 60. In the first column of <strong class="source-inline">y_test</strong>, store the 61st value at index 60, and in the second column, store the 62nd value at index 61. Use a <strong class="source-inline">for</strong> loop to create data in 60 time steps:<p class="source-code">X_test = []</p><p class="source-code">y_test = []</p><p class="source-code">for i in range(60, inputs.shape[0]):</p><p class="source-code">    X_test.append(inputs[i-60:i])</p><p class="source-code">    y_test.append(inputs[i, 0])</p></li>
				<li>Convert <strong class="source-inline">X_test</strong> and <strong class="source-inline">y_test</strong> into NumPy arrays:<p class="source-code">X_test, y_test = np.array(X_test), np.array(y_test)</p><p class="source-code">X_test.shape, y_test.shape</p><p>You should get the following output:</p><p class="source-code">((391, 60, 5), (391,))</p><p>The preceding result shows that there are <strong class="source-inline">391</strong> observations and for each of them you have the last <strong class="source-inline">60</strong> days' data for the following five features: <strong class="source-inline">Open</strong>, <strong class="source-inline">High</strong>, <strong class="source-inline">Low</strong>, <strong class="source-inline">Close</strong>, and <strong class="source-inline">Volume</strong>. The target variable, on the other hand, contains <strong class="source-inline">391</strong> values.</p></li>
				<li>Test some predictions for stock prices by calling <strong class="source-inline">regressor.predict(X_test)</strong>:<p class="source-code">y_pred = regressor.predict(X_test)</p></li>
				<li>Before looking at the results, reverse the scaling you did earlier so that the number you get as output will be at the correct scale using the <strong class="source-inline">StandardScaler</strong> utility class that you imported with <strong class="source-inline">scaler.scale_</strong>:<p class="source-code">scaler.scale_</p><p>You should get the following output:</p><div id="_idContainer341" class="IMG---Figure"><img src="image/B16341_09_30.jpg" alt="Figure 9.30: Using StandardScaler&#13;&#10;"/></div><p class="figure-caption">Figure 9.30: Using StandardScaler</p></li>
				<li>Use the first value in the preceding array to set your scale in preparation for the multiplication of <strong class="source-inline">y_pred</strong> and <strong class="source-inline">y_test</strong>. Recall that you are converting your data back from the scale you did earlier when converting all values to between zero and one:<p class="source-code">scale = 1/3.70274364e-03</p><p class="source-code">scale</p><p>You should get the following output:</p><p class="source-code">270.0700067909643</p></li>
				<li>Multiply <strong class="source-inline">y_pred</strong> and <strong class="source-inline">y_test</strong> by <strong class="source-inline">scale</strong> to convert your data back to the proper values:<p class="source-code">y_pred = y_pred*scale</p><p class="source-code">y_test = y_test*scale</p></li>
				<li>Use <strong class="source-inline">y_pred </strong>to view predictions for NVIDIA stock:<p class="source-code">y_pred</p><p>You should get the following output:</p><p> </p><div id="_idContainer342" class="IMG---Figure"><img src="image/B16341_09_31.jpg" alt="Figure 9.31: Checking prediction&#13;&#10;"/></div><p class="figure-caption">Figure 9.31: Checking prediction</p><p>The preceding results show the predicted Nvidia stock price for the future dates.</p></li>
				<li>Plot the real Nvidia stock price and your predictions:<p class="source-code">plt.figure(figsize=(14,5))</p><p class="source-code">plt.plot(y_test, color = 'black', label = "Real NVDA Stock Price")</p><p class="source-code">plt.plot(y_pred, color = 'gray',\</p><p class="source-code">         label = 'Predicted NVDA Stock Price')</p><p class="source-code">plt.title('NVDA Stock Price Prediction')</p><p class="source-code">plt.xlabel('time')</p><p class="source-code">plt.ylabel('NVDA Stock Price')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p><p>You should get the following output:</p><div id="_idContainer343" class="IMG---Figure"><img src="image/B16341_09_32.jpg" alt="Figure 9.32: NVIDIA stock price visualization&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.32: NVIDIA stock price visualization</p>
			<p>As you can see from the gray line in <em class="italic">Figure 9.32</em>, your prediction model is pretty accurate, when compared to the actual stock price, which is shown by the black line. </p>
			<p>In this exercise, you built an RNN with an LSTM layer for Nvidia stock prediction and completed the training, testing, and prediction steps.</p>
			<p>Now, test the knowledge you've gained so far in this chapter in the following activity. </p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor198"/>Activity 9.01: Building an RNN with Multiple LSTM Layers to Predict Power Consumption</h2>
			<p>The <strong class="source-inline">household_power_consumption.csv</strong> dataset contains information related to electric power consumption measurements for a household over 4 years with a 1-minute sampling rate. You are required to predict the power consumption of a given minute based on previous measurements.</p>
			<p>You are tasked with adapting an RNN model with additional LSTM layers to predict household power consumption at the minute level. You will be building an RNN model with three LSTM layers.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the dataset here: <a href="https://packt.link/qrloK">https://packt.link/qrloK</a>.</p>
			<p>Perform the following steps to complete this activity:</p>
			<ol>
				<li value="1">Load the data.</li>
				<li>Prepare the data by combining the <strong class="source-inline">Date</strong> and <strong class="source-inline">Time</strong> columns to form one single <strong class="source-inline">Datetime</strong> column that can be used then to sort the data and fill in missing values.</li>
				<li>Standardize the data and remove the <strong class="source-inline">Date</strong>, <strong class="source-inline">Time</strong>, <strong class="source-inline">Global_reactive_power</strong>, and <strong class="source-inline">Datetime</strong> columns as they won't be needed for the predictions.</li>
				<li>Reshape the data for a given minute to include the previous 60 minutes' values.</li>
				<li>Split the data into training and testing sets with, respectively, data before and after the index <strong class="source-inline">217440</strong>, which corresponds to the last month of data.</li>
				<li>Define and train an RNN model composed of three different layers of LSTM with <strong class="source-inline">20</strong>, <strong class="source-inline">40</strong>, and <strong class="source-inline">80</strong> units, followed by <strong class="source-inline">50%</strong> dropout and ReLU as the activation function.</li>
				<li>Make predictions on the testing set with the trained model.</li>
				<li>Compare the predictions against the actual values on the entire dataset.<p>You should get the following output:</p><div id="_idContainer344" class="IMG---Figure"><img src="image/B16341_09_33.jpg" alt="Figure 9.33: Expected output of Activity 9.01&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.33: Expected output of Activity 9.01</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor280">this link</a>.</p>
			<p>In the next section, you will learn how to apply RNNs to text.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor199"/>Natural Language Processing</h1>
			<p><strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) is a quickly growing field that is both challenging and rewarding. NLP takes valuable data that has traditionally been very difficult for machines to make sense of and turns it into information that can be used. This data can take the form of sentences, words, characters, text, and audio, to name a few. Why is this such a difficult task for machines? To answer that question, consider the following examples.</p>
			<p>Recall the two sentences: <em class="italic">it is what it is</em> and <em class="italic">is it what it is</em>. These two sentences, though they have completely opposite semantic meanings, would have the exact same representations in this bag of words format. This is because they have the exact same words, just in a different order. So, you know that you need to use a sequential model to process this, but what else? There are several tools and techniques that have been developed to solve these problems. But before you get to that, you need to learn how to preprocess sequential data.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor200"/>Data Preprocessing</h2>
			<p>As a quick review, preprocessing generally entails all the steps needed to train your model. Some common steps include data cleaning, data transformation, and data reduction. For natural language processing, more specifically, the steps could be all, some, or none of the following:</p>
			<ul>
				<li>Tokenization</li>
				<li>Padding</li>
				<li>Lowercase conversion</li>
				<li>Removing stop words</li>
				<li>Removing punctuation</li>
				<li>Stemming</li>
			</ul>
			<p>The following sections provide a more in-depth description of the steps that you will be using. For now, here's an overview of each step:</p>
			<ul>
				<li><strong class="bold">Dataset cleaning</strong> encompasses the conversion of case to lowercase, the removal of punctuation marks, and so on.</li>
				<li><strong class="bold">Tokenization</strong> is breaking up a character sequence into specified units called tokens.</li>
				<li><strong class="bold">Padding</strong> is a way to make input sentences of different sizes the same by padding them. Padding the sequences means ensuring that the sequences have a uniform length.</li>
				<li><strong class="bold">Stemming</strong> is truncating words down to their stem. For example, the words "rainy" and "raining" both have the stem "rain".</li>
			</ul>
			<h3 id="_idParaDest-177"><a id="_idTextAnchor201"/>Dataset Cleaning</h3>
			<p>Here, you create the <strong class="source-inline">clean_text</strong> function, which returns a list containing words once it has been cleaned. You will save all text as lowercase with <strong class="source-inline">lower()</strong> and encode it with <strong class="source-inline">utf8</strong> for character standardization: </p>
			<p class="source-code">def clean_text(txt):</p>
			<p class="source-code">    txt = "".join(v for v in txt if v not in string.punctuation)\</p>
			<p class="source-code">            .lower()</p>
			<p class="source-code">    txt = txt.encode("utf8").decode("ascii",'ignore')</p>
			<p class="source-code">    return txt </p>
			<p class="source-code">corpus = [clean_text(x) for x in all_headlines]</p>
			<h3 id="_idParaDest-178"><a id="_idTextAnchor202"/>Generating a Sequence and Tokenization</h3>
			<p>TensorFlow provides a dedicated class for generating a sequence of N-gram tokens – <strong class="source-inline">Tokenizer</strong> from <strong class="source-inline">keras.preprocessing.text</strong>: </p>
			<p class="source-code">from keras.preprocessing.text import Tokenizer</p>
			<p class="source-code">tokenizer = Tokenizer()</p>
			<p>Once you have instantiated a <strong class="source-inline">Tokenizer()</strong>, you can use the <strong class="source-inline">fit_on_texts()</strong> method to extract tokens from a corpus. This step will attribute an integer index to each unique word from the corpus:</p>
			<p class="source-code">tokenizer.fit_on_texts(corpus)</p>
			<p>After the tokenizer has been trained on a corpus, you can access the indexes allocated to each word from your corpus with the <strong class="source-inline">word_index</strong> attribute:</p>
			<p class="source-code">tokenizer.word_index</p>
			<p>You can convert a sentence into a tokenized version using the <strong class="source-inline">texts_to_sequences()</strong> method:</p>
			<p class="source-code">tokenizer.texts_to_sequences([sentence])</p>
			<p>You can create a function that will generate an N-gram sequence of tokenized sentences from an input corpus with the following snippet:</p>
			<p class="source-code">def get_seq_of_tokens(corpus):</p>
			<p class="source-code">    tokenizer.fit_on_texts(corpus)</p>
			<p class="source-code">    all_words = len(tokenizer.word_index) + 1</p>
			<p class="source-code">    </p>
			<p class="source-code">    input_sequences = []</p>
			<p class="source-code">    for line in corpus:</p>
			<p class="source-code">        token_list = tokenizer.texts_to_sequences([line])[0]</p>
			<p class="source-code">        for i in range(1, len(token_list)):</p>
			<p class="source-code">            n_gram_sequence = token_list[:i+1]</p>
			<p class="source-code">            input_sequences.append(n_gram_sequence)</p>
			<p class="source-code">    return input_sequences, all_words</p>
			<p class="source-code">inp_sequences, all_words = get_seq_of_tokens(corpus)</p>
			<p class="source-code">inp_sequences[:10]</p>
			<p>The <strong class="source-inline">get_seq_of_tokens()</strong> function trains a <strong class="source-inline">Tokenizer()</strong> on the given corpus. Then you need to iterate through each line of the corpus and convert them into their tokenized equivalents. Finally, for each tokenized sentence, you create the different sequences of N-gram from it.</p>
			<p>Next, you will see how you can deal with variable sentence length with padding.</p>
			<h3 id="_idParaDest-179"><a id="_idTextAnchor203"/>Padding Sequences</h3>
			<p>As discussed previously, deep learning models expect fixed-length input. But with text, the length of a sentence can vary. One way to overcome this is to transform all sentences to have the same length. You will need to set the maximum length of sentences. Then, for sentences that are shorter than this threshold, you can add padding, which will add a specific token value to fill the gap. On the other hand, longer sentences will be truncated to fit this constraint. You can use <strong class="source-inline">pad_sequences()</strong> to achieve this:</p>
			<p class="source-code">from keras.preprocessing.sequence import pad_sequences</p>
			<p>You can create the <strong class="source-inline">generate_padded_sequences</strong> function, which will take <strong class="source-inline">input_sequences</strong> and generate the padded version of it:</p>
			<p class="source-code">def generate_padded_sequences(input_sequences):</p>
			<p class="source-code">    max_sequence_len = max([len(x) for x in input_sequences])</p>
			<p class="source-code">    input_sequences = np.array(pad_sequences\</p>
			<p class="source-code">                               (input_sequences, \</p>
			<p class="source-code">                                maxlen=max_sequence_len, \</p>
			<p class="source-code">                                padding='pre'))</p>
			<p class="source-code">    predictors, label = input_sequences[:,:-1], \</p>
			<p class="source-code">                        input_sequences[:,-1]</p>
			<p class="source-code">    label = ku.to_categorical(label, num_classes=all_words)</p>
			<p class="source-code">    return predictors, label, max_sequence_len</p>
			<p class="source-code">predictors, label, max_sequence_len = generate_padded_sequences\</p>
			<p class="source-code">                                      (inp_sequences)</p>
			<p>Now that you know how to process raw text, have a look at the modeling step in the next section.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor204"/>Back Propagation Through Time (BPTT)</h1>
			<p>There are many types of sequential models. You've already used simple RNNs, deep RNNs, and LSTMs. Let's take a look at a couple of additional models used for NLP.</p>
			<p>Remember that you trained feed-forward models by first making a forward pass through the network that goes from input to output. This is the standard feed-forward model where the layers are densely connected. To train this kind of model, you can backpropagate the gradients through the network, taking the derivative of the loss of each weight parameter in the network. Then, you can adjust the parameters to minimize the loss.</p>
			<p>But in RNNs, as discussed earlier, your forward pass through the network also consists of going forward in time, updating the cell state based on the input and the previous state, and generating an output, <strong class="source-inline">Y</strong>. At that time step, computing a loss and then finally summing these losses from the individual time steps gets your total loss.</p>
			<p>This means that instead of backpropagating errors through a single feed-forward network at a single time step, errors are backpropagated at each individual time step, and then, finally, across all time steps—all the way from where you are currently, to the beginning of the sequence.</p>
			<p>This is why it's called backpropagation through time. As you can see, all errors are flowing back in time to the beginning of your data sequence.</p>
			<p>A great example of machine translation and one of the most powerful and widely used applications of RNNs in industry is Google Translate. In machine translation, you input a sequence in one language and the task is to train the RNN to output that sequence in a new language. This is done by employing a dual structure with an encoder that encodes the sentence in its original language into a state vector and a decoder. This then takes that encoded representation as input and decodes it into a new language.</p>
			<p>There's a key problem though in this approach: all content that is fed into the encoder structure must be encoded into a single vector. This can become a huge information bottleneck in practice because you may have a large body of text that you want to translate. To get around this problem the researchers at Google developed an extension of RNN called <strong class="bold">attention</strong>.</p>
			<p>Now, instead of the decoder only having access to the final encoded state, it can access the states of all the time steps in the original sentence. The weights of these vectors that connect the encoder states to the decoder are learned by the network during training. This is called attention because when the network learns, it places its attention on different parts of the input sentence.</p>
			<p>In this way, it effectively captures a sort of memory access to the important information in that original sentence. So, with building blocks such as attention and gated cells, like LSTMs, RNNs have really taken off in recent years and are being used in the real world quite successfully.</p>
			<p>You should have by now gotten a sense of how RNNs work and why they are so powerful for processing sequential data. You've seen why and how you can use RNNs to perform sequence modeling tasks by defining this recurrence relation. You also learned how you can train RNNs and looked at how gated cells such as LSTMs can help us model long-term dependencies. </p>
			<p>In the following exercise, you will see how to use an LSTM model for predicting the next word of a text.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor205"/>Exercise 9.03: Building an RNN with an LSTM Layer for Natural Language Processing</h2>
			<p>In this exercise, you will use an RNN with an LSTM layer to predict the final word of a news headline.</p>
			<p>The <strong class="source-inline">Articles.csv</strong> dataset contains raw text that consists of news titles. You will be training an LTSM model that will predict the next word of a given sentence.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the dataset here: <a href="https://packt.link/RQVoB">https://packt.link/RQVoB</a>.</p>
			<p>Perform the following steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the libraries needed:<p class="source-code">from keras.preprocessing.sequence import pad_sequences</p><p class="source-code">from keras.layers import Embedding, LSTM, Dense, Dropout</p><p class="source-code">from keras.preprocessing.text import Tokenizer</p><p class="source-code">from keras.callbacks import EarlyStopping</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">import keras.utils as ku </p><p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import string, os </p><p class="source-code">import warnings</p><p class="source-code">warnings.filterwarnings("ignore")</p><p class="source-code">warnings.simplefilter(action='ignore', category=FutureWarning)</p><p>You should get the following output:</p><p class="source-code">Using TensorFlow backend.</p></li>
				<li>Load the dataset locally by setting <strong class="source-inline">curr_dir</strong> to <strong class="source-inline">content</strong>. Create the <strong class="source-inline">all_headlines</strong> variable. Use a <strong class="source-inline">for</strong> loop to iterate over the files contained in the folder, and extract the headlines. Remove all headlines with the <strong class="source-inline">Unknown</strong> value. Print the length of <strong class="source-inline">all_headlines</strong>:<p class="source-code">curr_dir = '/content/'</p><p class="source-code">all_headlines = []</p><p class="source-code">for filename in os.listdir(curr_dir):</p><p class="source-code">    if 'Articles' in filename:</p><p class="source-code">        article_df = pd.read_csv(curr_dir + filename)</p><p class="source-code">        all_headlines.extend(list(article_df.headline.values))</p><p class="source-code">        break</p><p class="source-code">all_headlines = [h for h in all_headlines if h != "Unknown"]</p><p class="source-code">len(all_headlines)</p><p>The output will be as follows:</p><p class="source-code">831</p></li>
				<li>Create the <strong class="source-inline">clean_text</strong> method to return a list containing words once it has been cleaned. Save all text as lowercase with the <strong class="source-inline">lower()</strong> method and encode it with <strong class="source-inline">utf8</strong> for character standardization. Finally, output 10 headlines from your corpus:<p class="source-code">def clean_text(txt):</p><p class="source-code">    txt = "".join(v for v in txt \</p><p class="source-code">                  if v not in string.punctuation).lower()</p><p class="source-code">    txt = txt.encode("utf8").decode("ascii",'ignore')</p><p class="source-code">    return txt </p><p class="source-code">corpus = [clean_text(x) for x in all_headlines]</p><p class="source-code">corpus[:10]</p><p>You should get the following output:</p><div id="_idContainer345" class="IMG---Figure"><img src="image/B16341_09_34.jpg" alt="Figure 9.34: Corpus&#13;&#10;"/></div><p class="figure-caption">Figure 9.34: Corpus</p></li>
				<li>Use <strong class="source-inline">tokenizer.fit</strong> to extract tokens from the corpus. Each integer output corresponds with a specific word. With <strong class="source-inline">input_sequences</strong>, train features that will be a <strong class="source-inline">list []</strong>. With <strong class="source-inline">token_list = tokenizer.texts_to_sequences</strong>, convert each sentence into its tokenized equivalent. With <strong class="source-inline">n_gram_sequence = token_list</strong>, generate the N-gram sequences. Using <strong class="source-inline">input_sequences.append(n_gram_sequence)</strong>, append each N-gram sequence to the list of your features:<p class="source-code">tokenizer = Tokenizer()</p><p class="source-code">def get_seq_of_tokens(corpus):</p><p class="source-code">    tokenizer.fit_on_texts(corpus)</p><p class="source-code">    all_words = len(tokenizer.word_index) + 1</p><p class="source-code">    input_sequences = []</p><p class="source-code">    for line in corpus:</p><p class="source-code">        token_list = tokenizer.texts_to_sequences([line])[0]</p><p class="source-code">        for i in range(1, len(token_list)):</p><p class="source-code">            n_gram_sequence = token_list[:i+1]</p><p class="source-code">            input_sequences.append(n_gram_sequence)</p><p class="source-code">    return input_sequences, all_words</p><p class="source-code">inp_sequences, all_words = get_seq_of_tokens(corpus)</p><p class="source-code">inp_sequences[:10]</p><p>You should get the following output:</p><div id="_idContainer346" class="IMG---Figure"><img src="image/B16341_09_35.jpg" alt="Figure 9.35: N-gram tokens&#13;&#10;"/></div><p class="figure-caption">Figure 9.35: N-gram tokens</p></li>
				<li>Pad the sequences and obtain the <strong class="source-inline">predictors</strong> and <strong class="source-inline">target</strong> variables. Use <strong class="source-inline">pad_sequence</strong> to pad the sequences and make their lengths equal:<p class="source-code">def generate_padded_sequences(input_sequences):</p><p class="source-code">    max_sequence_len = max([len(x) for x in input_sequences])</p><p class="source-code">    input_sequences = np.array\</p><p class="source-code">                      (pad_sequences(input_sequences, \</p><p class="source-code">                                     maxlen=max_sequence_len, \</p><p class="source-code">                                     padding='pre'))</p><p class="source-code">    predictors, label = input_sequences[:,:-1], \</p><p class="source-code">                        input_sequences[:,-1]</p><p class="source-code">    label = ku.to_categorical(label, num_classes=all_words)</p><p class="source-code">    return predictors, label, max_sequence_len</p><p class="source-code">predictors, label, max_sequence_len = generate_padded_sequences\</p><p class="source-code">                                      (inp_sequences)</p></li>
				<li>Prepare your model for training. Add an input embedding layer with <strong class="source-inline">model.add(Embedding)</strong>. Add a hidden LSTM layer with <strong class="source-inline">100</strong> units and add a dropout of 10%. Then, add a dense layer with a softmax activation function. With the <strong class="source-inline">compile</strong> method, configure your model for training, setting your loss function to <strong class="source-inline">categorical_crossentropy</strong>, and use the Adam optimizer:<p class="source-code">def create_model(max_sequence_len, all_words):</p><p class="source-code">    input_len = max_sequence_len - 1</p><p class="source-code">    model = Sequential()</p><p class="source-code">    </p><p class="source-code">    model.add(Embedding(all_words, 10, input_length=input_len))</p><p class="source-code">    </p><p class="source-code">    model.add(LSTM(100))</p><p class="source-code">    model.add(Dropout(0.1))</p><p class="source-code">    </p><p class="source-code">    model.add(Dense(all_words, activation='softmax'))</p><p class="source-code">    model.compile(loss='categorical_crossentropy', \</p><p class="source-code">                  optimizer='adam')</p><p class="source-code">    return model</p><p class="source-code">model = create_model(max_sequence_len, all_words)</p><p class="source-code">model.summary()</p><p>You should get the following output:</p><div id="_idContainer347" class="IMG---Figure"><img src="image/B16341_09_36.jpg" alt="Figure 9.36: Model summary&#13;&#10;"/></div><p class="figure-caption">Figure 9.36: Model summary</p></li>
				<li>Fit your model with <strong class="source-inline">model.fit</strong> and set it to run on <strong class="source-inline">100</strong> epochs. Set <strong class="source-inline">verbose</strong> equal to <strong class="source-inline">5</strong>:<p class="source-code">model.fit(predictors, label, epochs=100, verbose=5)</p><p>You should get the following output:</p><div id="_idContainer348" class="IMG---Figure"><img src="image/B16341_09_37.jpg" alt="Figure 9.37: Training the model&#13;&#10;"/></div><p class="figure-caption">Figure 9.37: Training the model</p></li>
				<li>Write a function that will receive an input text, a model, and the number of next words to be predicted. This function will prepare the input text to be fed into the model that will predict the next word:<p class="source-code">def generate_text(seed_text, next_words, \</p><p class="source-code">                  model, max_sequence_len):</p><p class="source-code">    for _ in range(next_words):</p><p class="source-code">        token_list = tokenizer.texts_to_sequences\</p><p class="source-code">                     ([seed_text])[0]</p><p class="source-code">        token_list = pad_sequences([token_list], \</p><p class="source-code">                                   maxlen=max_sequence_len-1,\</p><p class="source-code">                                   padding='pre')</p><p class="source-code">        predicted = model.predict_classes(token_list, verbose=0)</p><p class="source-code">        output_word = ""</p><p class="source-code">        for word,index in tokenizer.word_index.items():</p><p class="source-code">            if index == predicted:</p><p class="source-code">                output_word = word</p><p class="source-code">                break</p><p class="source-code">        seed_text += " "+output_word</p><p class="source-code">    return seed_text.title()</p></li>
				<li>Output some of your generated text with the <strong class="source-inline">print</strong> function. Add your own words for the model to use and generate from. For example, in <strong class="source-inline">the hottest new</strong>, the integer <strong class="source-inline">5</strong> is the number of words output by the model:<p class="source-code">print (generate_text("the hottest new", 5, model,\</p><p class="source-code">                     max_sequence_len))</p><p class="source-code">print (generate_text("the stock market", 4, model,\</p><p class="source-code">                     max_sequence_len))</p><p class="source-code">print (generate_text("russia wants to", 3, model,\</p><p class="source-code">                     max_sequence_len))</p><p class="source-code">print (generate_text("french citizen", 4, model,\</p><p class="source-code">                     max_sequence_len))</p><p class="source-code">print (generate_text("the one thing", 15, model,\</p><p class="source-code">                     max_sequence_len))</p><p class="source-code">print (generate_text("the coronavirus", 5, model,\</p><p class="source-code">                     max_sequence_len))</p><p>You should get the following output:</p><div id="_idContainer349" class="IMG---Figure"><img src="image/B16341_09_38.jpg" alt="Figure 9.38: Generated text&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.38: Generated text</p>
			<p>In this result, you can see the text generated by your model for each sentence.</p>
			<p>In this exercise, you have successfully predicted some news headlines. Not surprisingly, some of them may not be very impressive, but some are not too bad. </p>
			<p>Now that you have all the essential knowledge about RNNs, try to test yourself by performing the next activity.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor206"/>Activity 9.02: Building an RNN for Predicting Tweets' Sentiment</h2>
			<p>The <strong class="source-inline">tweets.csv</strong> dataset contains a list of tweets related to an airline company. Each of the tweets has been classified as having positive, negative, or neutral sentiment.</p>
			<p>You have been tasked to analyze a sample of tweets for the company. Your goal is to build an RNN model that will be able to predict the sentiment of each tweet: either positive or negative.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find <strong class="source-inline">tweets.csv</strong> here: <a href="https://packt.link/dVUd2">https://packt.link/dVUd2</a>.</p>
			<p>Perform the following steps to complete this activity.</p>
			<ol>
				<li value="1">Import the necessary packages.</li>
				<li>Prepare the data (combine the <strong class="source-inline">Date</strong> and <strong class="source-inline">Time</strong> columns, name it <strong class="source-inline">datetime</strong>, sort the data, and fill in missing values).</li>
				<li>Prepare the text data (tokenize words and add padding).</li>
				<li>Split the dataset into training and testing sets with, respectively, the first 10,000 tweets and the remaining tweets.</li>
				<li>Define and train an RNN model composed of two different layers of LSTM with, respectively, <strong class="source-inline">50</strong> and <strong class="source-inline">100</strong> units followed by 20% dropout and ReLU as the activation function.</li>
				<li>Make predictions on the testing set with the trained model.<p>You should get the following output:</p><div id="_idContainer350" class="IMG---Figure"><img src="image/B16341_09_39.jpg" alt="Figure 9.39: Expected output of Activity 9.02&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.39: Expected output of Activity 9.02</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor281">this link</a>.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor207"/>Summary</h1>
			<p>In this chapter, you explored different recurrent models for sequential data. You learned that each sequential data point is dependent on the prior sequence of data points, such as natural language text. You also learned why you must use models that allow for the sequence of data to be used by the model, and sequentially generate the next output.</p>
			<p>This chapter introduced RNN models that can make predictions for sequential data. You observed the way RNNs can loop back on themselves, which allows the output of the model to feed back into the input. You reviewed the types of challenges that you face with these models, such as vanishing and exploding gradients, and how to address them.</p>
			<p>In the next chapter, you will learn how to utilize custom TensorFlow components to use within your models, including loss functions and layers.</p>
		</div>
		<div>
			<div id="_idContainer352" class="Content">
			</div>
		</div>
	</body></html>