- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we covered linear regression with TensorFlow, where we
    looked at both simple and multiple linear regression; we also explored various
    metrics for evaluating regression models. We concluded the chapter with a real-world
    use case, where we built a salary prediction model, and we used this to predict
    the salaries of new employees based on a set of features. In this chapter, we
    will continue with modeling in TensorFlow – this time, by exploring classification
    problems with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by looking at the concept of classification modeling, after which
    we will examine the various evaluation metrics for classification modeling and
    how we can apply them to various use cases. We will look at binary, multi-class,
    and multi-label classification modeling. Finally, we will walk through a case
    study, putting all we have learned into practice by building a binary classification
    model to predict whether a student will drop out of university or not.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should clearly understand what classification
    modeling in machine learning is and also be able to differentiate between binary,
    multi-class, and multi-label classification problems. You will be familiar with
    how to build, compile, train, predict, and evaluate classification models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification with TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A student dropout prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use Google Colab to run the coding exercise, and you
    will need to install Python >= 3.8.0, along with the following packages, which
    can be installed using the `pip` `install` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow >=` `2.7.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow-datasets ==` `4.4.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pillow ==` `8.4.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas ==` `1.3.4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy ==` `1.21.4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scipy ==` `1.7.3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code bundle for this book is available at the following GitHub link: [https://github.com/PacktPublishing/TensorFlow-Developer-Certificate](https://github.com/PacktPublishing/TensorFlow-Developer-Certificate).
    Solutions to all the exercises can also be found at this link.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B18118_01.xhtml#_idTextAnchor014), *Introduction to Machine
    Learning*, we talked about supervised learning and briefly talked about classification
    modeling. Classification modeling involves predicting classes in our target variable.
    When the classes we try to predict are binary (for example, trying to predict
    whether a pet is either a dog or a cat, whether an email is spam or not, or whether
    a patient has cancer or not), this type of classification scenario is referred
    to as **binary classification**.
  prefs: []
  type: TYPE_NORMAL
- en: Then again, we may be faced with a problem where we want to build an ML model
    to predict the different breeds of dogs. In this case, we have more than two classes,
    so this type of classification is called **multi-class classification**. Just
    like binary classification problems, in multi-class classification, our target
    variable can only belong to one class out of multiple classes – our model will
    select either a bulldog, a German shepherd, or a pit bull. Here, the classes are
    *mutually exclusive*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that you are building a movie classifier, and you want to classifier
    a blockbuster movie such as *Avengers: Endgame*. This movie belongs to the action,
    adventure, superhero, epic, fantasy, and science fiction genres. From the movie’s
    label, we can see that our target variable belongs to more than one genre; hence,
    this type of classification is called **multi-label classification**, where the
    output class has more than one target label.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike in multi-class classification, where each example can only belong to
    one class, in multi-label classification, each example can belong to multiple
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike binary and multi-class, where each example can only belong to one class,
    in multi-label classification, each example can belong to multiple classes, just
    like the *Avengers* movie. Now that we have looked at the three main types of
    classification problems, the next question is, how do we evaluate classification
    models? What are the key metrics we need to look out for? Let us look at this
    now and understand what they mean and how to best apply them to various classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike regression problems, where we have numeric values in our target variable,
    in classification modeling, we have established that our output is classes. Hence,
    we cannot use the same metrics we used to evaluate our regression models in [*Chapter
    3*](B18118_03.xhtml#_idTextAnchor065), *Linear Regression with TensorFlow*, since
    our output is not continuous numerical values but classes. For a classification
    problem, let’s say we build a spam filtering system to classify a client’s emails.
    The client has 250 emails that are not spam and another 250 emails that are spam.
    Using our spam filtering model, we are able to correctly flag 230 spam messages
    and also correctly identify 220 non-spam messages as not spam.
  prefs: []
  type: TYPE_NORMAL
- en: When our spam filter correctly identifies a spam message as spam (which is what
    we want), we call this a **true positive**, and when the model misclassifies a
    spam message as not spam, this is called a **false negative**. In a case where
    the model correctly identifies a non-spam email as not spam, this is called a
    **true negative**; however, we occasionally find important emails in our spam
    folder, and these messages were wrongly filtered as spam when they were not. This
    scenario is called a **false positive**. We can now use these details to evaluate
    the performance of our spam-filtering model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us list the important details we now know:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Total spam messages**: 250 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correctly predicted spam messages (true positives)**: 230 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrongly predicted spam messages (false negatives or a type 2 error)**: 20
    samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total non-spam messages**: 250 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correctly predicted non-spam messages (true negatives)**: 220 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrongly predicted spam messages (false positive or a type 1 error)**: 30
    samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have gathered the key details, let us now use them to learn how
    to evaluate classification models. To do this, we will have to talk about the
    confusion matrix next.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **confusion matrix** is an error matrix that displays the performance of
    a classification model in a tabular form, containing both the true values and
    the predicted values, as illustrated in *Figure 4**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The confusion matrix](img/B18118_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – The confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: Using the confusion matrix, we can calculate various classification evaluation
    metrics such as accuracy, precision, recall, and F1 score. In the confusion matrix,
    we can see the predicted class at the top, showing emails predicted as spam in
    the first column and those predicted as not spam in the second column, while the
    rows show us the true values. Here, we can see in the first row the true spam
    class and the true not-spam class. When we put it all together, we can see the
    true values and the wrong prediction in a tabular fashion, which gives us a quick
    view of the model and its performance across both classes. Let’s use these details
    to compute key performance metrics for our model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** is quite intuitive, as it is the sum of the correctly predicted
    labels over the total available data. We can represent this with the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy =  TP + TN _______________  (TP + FP + TN + FN)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add our values and see what our accuracy will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy =  230 + 220  ________________  (230 + 30 + 220 + 20)  = 0.90
  prefs: []
  type: TYPE_NORMAL
- en: We get an accuracy of 90%. This is potentially exciting, but let us be more
    realistic with our data. When it comes to spam emails, we will likely have more
    legitimate emails than spam emails coming into our mailbox.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine we have another client, B, with 500 emails, made up of the following
    details:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Total spam messages**: 40 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correctly predicted spam messages (true positives)**: 20 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrongly predicted spam messages (false negatives or a type 2 error)**: 20
    samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total non-spam messages**: 460 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correctly predicted non-spam messages (true negatives)**: 430 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wrongly predicted spam messages (false positive or a type 1 error)**: 30
    samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we compute the accuracy for client B, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy =  20 + 430 ______________  20 + 30 + 430 + 20  = 0.90
  prefs: []
  type: TYPE_NORMAL
- en: Again, we arrive at an accuracy of 90 percent, yet our model could only predict
    50 percent of the spam emails as spam. This shows us that accuracy may not always
    be the best measure, especially when we deal with a use case made up of imbalanced
    data such as email classification, fraud detection, or disease detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better sense of how our model is doing, we will now turn our attention
    to precision and recall. Referring back to *Figure 4**.2*, the ratio of the true
    positive to the positive class in the ground truth is called *sensitivity* or
    *recall*, or the *true positive rate* in ML lingo, and it is represented by the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Recall =  TP _ (TP + FN)
  prefs: []
  type: TYPE_NORMAL
- en: 'Whereas *precision* is the ratio of the true positive to the positive class
    predicted by the model, and we also represent it as an equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision =  TP _ (TP + FP)
  prefs: []
  type: TYPE_NORMAL
- en: 'Using client B, let us calculate our model’s performance using precision and
    recall:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision for case study 2 =  20 _ (20 + 30)  = 0.4
  prefs: []
  type: TYPE_NORMAL
- en: Recall for case study 2 =  20 _ (20 + 20)  = 0.5
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can see how badly our model is, although it has a high accuracy on
    the entirety of the data. Another important metric that we will come across for
    classification tasks is the F1 score. The *F1 score* combines recall and precision,
    and we arrive at it by computing the harmonic mean of precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: F1 Score = 2 *  precision * recall  _____________  (precision + recall)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us calculate the F1 score for the second case study:'
  prefs: []
  type: TYPE_NORMAL
- en: F1 Score = 2 *  0.4 * 0.5 _ (0.4 + 0.5)  = 0.44
  prefs: []
  type: TYPE_NORMAL
- en: From our evaluation of Client B’s emails using our spam-filtering model, we
    now know we need to build a more effective model, one with much better precision
    and recall for our target class. However, achieving high precision and recall
    may not always be possible. In such a scenario, we are left with a trade-off,
    which is known as the *precision/recall trade-off*. In the case of detecting spam
    emails, we know clients are unlikely to switch to a different service provider
    should a few spam messages find their way into their inbox; however, they will
    be upset if they fail to find important messages in their inbox. In this instance,
    we will aim to achieve a higher recall. Conversely, let’s say we build an early
    cancer detection system, where our focus will be on achieving high precision to
    minimize false positives. It is important to note that precision and recall are
    not mutually exclusive, and we can achieve both high precision and recall with
    a well-tuned model in many instances.
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered some important classification metrics. Now, let us look
    at a case study (a student dropout prediction) where we will build and evaluate
    our classification models using different modules from TensorFlow and scikit-learn.
    Let’s jump in.
  prefs: []
  type: TYPE_NORMAL
- en: A student dropout prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065), *Linear Regression with
    TensorFlow*, you began your journey using TensorFlow to build a salary prediction
    model. Your boss was impressed, and now that you are fully settled in the data
    team, your manager wants you to work with a new client. Your job is to help them
    build a model that will predict whether a student will drop out of university
    or not, as this will help them support such students, thus preventing them from
    dropping out of school. Your manager has given you authorization and the task
    is now yours. For this task, historical data was made available to you by your
    client. Just like in [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065), *Linear
    Regression with TensorFlow*, you had a rewarding chat with the client, and you
    identified the task as a binary classification problem. Let’s open the notebook
    labeled `Classification with TensorFlow` from the GitHub repository and get started.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by loading the historical data that we received from our client:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the TensorFlow libraries that we will use to execute
    our task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running the code, we can see the version of TensorFlow we will use. In
    my case, it’s 2.8.0 at the time of writing. You will most likely have a newer
    version, but it should work just as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, we will import some additional libraries that will help us simplify our
    workflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We previously discussed most of the libraries that we will use here, except
    for the last line of the code block, which we will use to import the confusion
    matrix and classification report from the scikit-learn library. We will use these
    functions to evaluate our model’s performance. If you are unclear about the other
    libraries, refer to [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065), *Linear Regression
    with TensorFlow*, before proceeding with this case study.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have loaded all the necessary libraries, let us make a DataFrame
    for easy processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we run the code, if everything works as expected, we should get the first
    five rows of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – A DataFrame showing the first five rows of our dataset](img/B18118_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – A DataFrame showing the first five rows of our dataset
  prefs: []
  type: TYPE_NORMAL
- en: From the output, we can see that our data is made up of numerical and categorical
    columns. Each of the rows represents a student. Upon inspection, we can see that
    we have 12 columns, namely, `Student ID`, `Student Name`, `Library`, `Resources`,
    `Finance`, `Scholarships`, `Study Time`, `Study Group`, `GPA`, `Test`, `Assignment`,
    and `Graduated`. To efficiently model our data, we need to do some data preparation,
    so let’s start with some exploratory data analysis and see what we can find.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to explore and analyze the data:'
  prefs: []
  type: TYPE_NORMAL
- en: We will begin the exploratory data analysis process using the `df.info()` function
    to check for `NULL` values as well as the data types in our dataset, as shown
    in *Figure 4**.3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Information about our dataset](img/B18118_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Information about our dataset
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that we have no missing values in our dataset, and yes, we
    will work with a much larger dataset than in our regression task. Here, we have
    25,000 data points representing students’ data collected from the university.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to drop the irrelevant columns. By inspecting the available
    columns, we drop the `student ID` and `student name` columns, as these columns
    should have no impact on whether a student will graduate or not. Let’s do that
    here, using the `drop` function from pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, let us use the `describe` function to generate key statistics of our dataset,
    as this will give us a sense of our data, as shown in *Figure 4**.4*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.4 – The summary statistics of the numerical columns](img/B18118_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – The summary statistics of the numerical columns
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 4**.4*, we can see that the mean GPA is `3.00`, the lowest GPA
    is `1.00`, and the highest GPA is `5.00`. Both the `Tes`t and `Assignment` columns
    have a minimum score of 5 and a maximum score of 15\. However, we have no sense
    of the distribution of our target column, as it is a categorical column; we will
    fix that shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us make a histogram plot of our categorical target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running this code produces the plot shown in *Figure 4**.5*. Here, we use `matplotlib`
    to plot the `Graduated` column, and we can see almost 17,500 students who successfully
    graduated and roughly 7,500 students who failed to graduate. Of course, it is
    only logical to expect a larger number of students will graduate. In ML terms,
    we have on our plate an unbalanced dataset. However, the good part is that we
    still have enough samples to train our model from the minority class. Anyway,
    don’t take my word for it; shortly, we will train our models after we complete
    the data preparation steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Summary statistics of the numeric columns](img/B18118_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Summary statistics of the numeric columns
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more plots in our notebook to explore, but we will keep it simple,
    since our main goal in this book is to focus on building models with TensorFlow.
    However, let’s look at one of the very important plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we create a scatterplot using `seaborn`, showing the `Library` column
    on the *x* axis and `GPA` on the *y* axis, and we use the `Graduate` column to
    color our data points, as shown in *Figure 4**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Library versus GPA](img/B18118_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Library versus GPA
  prefs: []
  type: TYPE_NORMAL
- en: 'From this plot, we can see that there is a good number of students with a GPA
    above 3.50 who graduated. However, don’t assume that everyone above a 3.50 GPA
    in the `Average`, `Good`, and `Excellent` columns graduated. In fact, let’s check
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: When we run this code, we get the total number of students who have a GPA of
    3.50 or over and dropped out. In total, we have 76 students who dropped out. Remember,
    our plot covers 25,000 data points, so don’t be surprised if you did not find
    these data points in the plot in *Figure 4**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us proceed to prepare our data for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065), *Linear Regression with
    TensorFlow*, we emphasized the need to put our data in the right form, handle
    missing data, drop irrelevant features, convert categorical values to numeric
    values, and so on. We will continue in that spirit here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start by converting our labels to numerical values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we assign a value of `1` to students who graduated and a value of `0`
    to students who dropped out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us examine the correlation between our numerical data and our target
    variable using the `corr()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we run the code, we get the correlation table shown in *Figure 4**.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Correlation table for our dataset](img/B18118_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Correlation table for our dataset
  prefs: []
  type: TYPE_NORMAL
- en: From the highlighted column, we can see that `GPA` has the strongest correlation
    with the `Graduated` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us convert our categorical variables to numerical values. We will
    stick to using dummy variables to one-hot encode our categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we drop the first column to avoid the dummy variable trap. When we run
    the code, we get a new DataFrame, as shown in *Figure 4**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – A DataFrame after one-hot encoding](img/B18118_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – A DataFrame after one-hot encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the attributes in numerical form, let’s see how correlated
    they are with our target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we run the code, it returns the correlation of all the columns with the
    target variable, as shown in *Figure 4**.9*. Our initial numerical variables are
    still the leading correlation values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – A correlation of the attributes with the target column](img/B18118_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – A correlation of the attributes with the target column
  prefs: []
  type: TYPE_NORMAL
- en: 'We have successfully converted our data into numerical values, so let us proceed
    to split the data into attributes (`X`) and a target (`y`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Don’t forget that we need to normalize our data. So, we bring all the attributes
    to scale for our modeling process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Again, we use `MinMaxScaler` from the scikit-learn library, after which we
    split our data into training and testing sets. For training, we use 80 percent
    of our data, and we keep 20 percent as our holdout data to test our model’s generalization
    capability. We set a random state to 10 to ensure that we can reproduce the same
    data split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Now we are done with data preparation, let us proceed to model building with
    TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build our model, we will start by creating a neural network architecture;
    here, we will use the sequential API to define the number of layers we want to
    connect sequentially. As shown in *Figure 4**.10*, we have only the input and
    output layers. Unlike in [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065), *Linear
    Regression with TensorFlow*, where we predicted numeric values, our output layer
    here has only one neuron because we are dealing with a binary classification problem.
    For the output layer, the activation function used depends on the task at hand.
    When we deal with a binary classification task, we typically use the *sigmoid
    activation function*; for multi-class classification problems, we commonly use
    the *softmax activation function*; and when dealing with multi-label classification,
    we commonly use sigmoid as our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Creating a classification model in TensorFlow](img/B18118_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Creating a classification model in TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us proceed to compile our model. We will use *binary cross entropy* for
    our loss function when we deal with binary classification and *categorical cross
    entropy* or *sparse categorical cross entropy* when we deal with multi-class classification
    problems. In [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification
    with Neural Networks*, our discussion will deep-dive into activation functions
    and more, as we continue to build our understanding and application of neural
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let us compile our model. Here, we will use accuracy as our evaluation
    metric. We will also look at other classification metrics, which we discussed
    earlier when we begin evaluating our model’s performance on test data. After we
    compile our model, the next step is to fit our model. In [*Chapter 1*](B18118_01.xhtml#_idTextAnchor014),
    *Introduction to Machine Learning*, we talked about training, validation, and
    test splits. Since we will deal with a much larger dataset, let’s use a validation
    set to assess our model’s performance at the end of each epoch, allowing us to
    monitor how our model performs on unseen data before we test it out on the hold-out
    test set. We set our `validation_split` argument to `0.2`; this signifies that
    we will use 20 percent of our training data for validation during the training
    process, which will run for 40 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 4**.11*, we can see the last five epochs of our model’s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Model training (the last five epochs)](img/B18118_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Model training (the last five epochs)
  prefs: []
  type: TYPE_NORMAL
- en: 'The model reaches a training accuracy of 99.35% and a validation accuracy of
    99.33%. Using just two layers and three simple steps in less than five minutes,
    we have arrived at almost 100 percent accuracy on both training and validation
    data. These results are impressive, yes; however, it is important to know this
    isn’t always the case, especially when we work with more complex datasets. They
    may require more complex architectures and longer training times to achieve good
    results. We will see this in *Section 2* of this book, where we will work with
    images. Before we proceed to evaluate our model, let us look at our model’s architecture
    using the `summary` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this line of code, we generate the model’s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The output shape shows us that the first `dense` layer (input layer) has 16
    neurons and 256 params, since we passed in 16 attributes (16 columns x 16 neurons
    = 256 params), while the `dense_1` layer (the output layer) has 1 neuron and 17
    params (17 columns x 1 neuron = 17 params). The total params are 273 and all the
    params are trainable, so we have 273 here, which means there will be zero non-trainable
    params. Now that we are done with model building, let’s shift our attention to
    evaluating our model. How well will it perform on the test data?
  prefs: []
  type: TYPE_NORMAL
- en: Classification performance evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate our model in TensorFlow, all we need is one line of code – using
    the `evaluate` function on our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We then generate our model’s performance on our holdout data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We arrive at an accuracy of 99.44% on our test data. This is good; however,
    let us look at the other classification metrics we talked about earlier in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate our model’s prediction on the test data. Then, we convert the probabilities
    using the `np.round()` function and convert the data type to integers. Then, we
    create a pandas DataFrame, after which we generate the number of the misclassified
    labels in our DataFrame. In our case, the model misclassified 28 out of 5,000
    data points in our test set. Now, we will generate a confusion matrix and classification
    reports to evaluate our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Running this code generates the confusion matrix shown in *Figure 4**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – A confusion matrix for our student dropout model](img/B18118_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – A confusion matrix for our student dropout model
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal arrows point in the direction of the true values, while the vertical
    arrows are in the direction of the predicted labels. Our ground truth had (5 +
    3,498) = 3,503 students in the graduated class, and our model predicted (3498
    + 23) = 3,521 in our graduate class. Meanwhile, in the dropout class, our model
    predicted (5 + 1474) = 1,479 students dropped out, as against the ground truth
    (1,474 + 23) = 1497\. From *Figure 4**.14*, we can see that the model wrongly
    predicted that 23 students who dropped out were graduates and 5 students who graduated
    were dropouts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us print out our classification report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have printed out our classification report, as shown in *Figure
    4**.15*, we can see our model’s precision, recall, and F1 score across both classes
    in our dataset, as well as the macro and weighted averages.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The classification report](img/B18118_04_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – The classification report
  prefs: []
  type: TYPE_NORMAL
- en: From the highlighted details in *Figure 4**.13*, we can see the model’s precision,
    recall, and F1 score. We have come a long way; this is a good result. If we wish
    to improve our result, we can try more experiments. Also, error analysis is very
    useful to help us understand misclassified data. We can drill down into the misclassified
    students, trying to understand patterns or common characteristics in cases the
    model failed to predict correctly. This can lead to further insights or help us
    identify issues regarding our data quality, among other possibilities. However,
    we will not delve into error analysis here. You have done a good job and achieved
    good results in both classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s save the model and present it to the manager using the `save` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We have completed our task, and we have a near-perfect model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you should be able to build a real-world classifier with TensorFlow for
    structured data problems, using what you learned from our case study in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed classification modeling and looked at the main
    types of classification problems. We also discussed the main types of metrics
    for the evaluation of classification models and how to best apply them to real-world
    use cases. Then, we looked at a real-world use case, where we learned how to build,
    compile, and train a classification model with TensorFlow for a binary classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned, hands-on, how to evaluate our classification models. We
    have now completed the first section of this book. Get ready for the next sections,
    where we will see the power of TensorFlow in its full glory as we work on unstructured
    data (image and text data).
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s test what we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What is classification modeling?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between multi-class and multi-label classification problems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You work for a streaming company offering interesting children content. Which
    metrics, between precision and recall, will you focus on improving, and why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your company is building a loan prediction system to offer loans to clients.
    Which metrics, between precision and recall, will you focus on improving, and
    why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, you can check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amr, T., 2020\. *Hands-On Machine Learning with scikit-learn and Scientific
    Python Toolkits*. [S.l.]: Packt Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beger, A., 2016\. *Precision-Recall Curves*. *SSRN* *Electronic Journal*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raschka, S. and Mirjalili, V., 2019\. *Python Machine Learning – Third Edition*.
    Packt Publishing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow* *guide*: [https://www.TensorFlow.org/guide](https://www.TensorFlow.org/guide).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2 – Image Classification with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will learn to build both binary and multiclass image classifiers
    with **convolutional neural networks** (**CNNs**), understand how to improve the
    model’s performance by tuning the hyperparameters, and how to handle the problem
    of overfitting. By the end, you should be comfortable with building real world
    image classifiers using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification With
    Neural Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18118_06.xhtml#_idTextAnchor129), *Improving the Model*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18118_07.xhtml#_idTextAnchor146), *Image Classification with
    Convolutional Neural Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18118_08.xhtml#_idTextAnchor186), *Handling Overfitting*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18118_09.xhtml#_idTextAnchor210), *Transfer Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
