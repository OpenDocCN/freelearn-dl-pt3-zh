- en: 'Appendix A: Mathematical Foundations and Advanced TensorFlow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we will discuss some concepts that will be useful for helping you to understand
    certain details provided in the chapters. First, we will discuss several mathematical
    data structures found throughout the book, followed by a description of the various
    operations performed on those data structures. After that, we will discuss the
    concept of probabilities. Probabilities play a vital role in machine learning,
    as they usually give insights into how uncertain a model is about its prediction.
    Finally, we will conclude this appendix with a guide on how to use TensorBoard
    as a visualization tool for word embeddings.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Basic data structures
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalar
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A scalar is a single number, unlike a matrix or a vector. For example, 1.3
    is a scalar. A scalar can be mathematically denoted as follows: ![](img/B14070_12_001.png).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Here, *R* is the real number space.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Vectors
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A vector is an array of numbers. Unlike a set, where there is no order to the
    elements, a vector has a certain order to the elements. An example vector is `[1.0,
    2.0, 1.4, 2.3]`. Mathematically, it can be denoted as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_002.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_003.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Here, *R* is the real number space and *n* is the number of elements in the
    vector.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Matrices
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A matrix can be thought of as a two-dimensional arrangement of a collection
    of scalars. In other words, a matrix can be thought of as a vector of vectors.
    An example matrix is shown as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_004.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: 'A more general matrix of size ![](img/B14070_12_005.png) can be mathematically
    defined like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_006.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'And:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_007.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Here, *m* is the number of rows of the matrix, *n* is the number of columns
    in the matrix, and *R* is the real number space.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Indexing of a matrix
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using zero-indexed notation (that is, indexes that start with 0).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'To index a single element from a matrix at the *(i, j)*^(th) position, we use
    the following notation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_008.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: 'Referring to the previously defined matrix, we get the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_004.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'We index an element from *A* like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_010.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'We denote a single row of any matrix *A* as shown here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_011.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: 'For our example matrix, we can denote the second row (indexed as 1) of the
    matrix as shown here:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_012.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'We denote the slice starting from the *(i, k)*^(th) index to the *(j, l)*^(th)
    index of any matrix *A* as shown here:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_013.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: 'In our example matrix, we can denote the slice from first row third column
    to second row fourth column as shown here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_014.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Special types of matrices
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identity matrix
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An identity matrix is a square matrix where values are equal to 1 on the diagonal
    of the matrix and 0 everywhere else. Mathematically, it can be shown as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_015.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: 'This would look like the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_016.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_12_017.png).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity matrix gives the following nice property when multiplied with
    another matrix *A*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_018.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Square diagonal matrix
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A square diagonal matrix is a more general case of the identity matrix, where
    the values along the diagonal can take any value and the off-diagonal values are
    zeros:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_019.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Tensors
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An *n*-dimensional matrix is called a **tensor**. In other words, a matrix
    with an arbitrary number of dimensions is called a tensor. For example, a four-dimensional
    tensor can be denoted as shown here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_020.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Here, *R* is the real number space.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Tensor/matrix operations
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transpose
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transpose is an important operation defined for matrices or tensors. For a
    matrix, the transpose is defined as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_021.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Here, *A*^T denotes the transpose of *A*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of the transpose operation can be illustrated as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_004.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'After the transpose operation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_023.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: 'For a tensor, transpose can be seen as permuting the dimensions order. For
    example, let’s define a tensor *S*, as shown here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_024.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: 'Now one transpose operation (out of many) can be defined as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_025.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Matrix multiplication
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matrix multiplication is another important operation that appears quite frequently
    in linear algebra.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the matrices ![](img/B14070_12_026.png) and ![](img/B14070_12_027.png),
    the multiplication of *A* and *B* is defined as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_028.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_12_029.png).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_030.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_031.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'This gives ![](img/B14070_12_032.png), and the value of *C* is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_033.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Element-wise multiplication
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Element-wise matrix multiplication (or the **Hadamard product**) is computed
    for two matrices that have the same shape. Given the matrices ![](img/B14070_12_034.png)
    and ![](img/B14070_12_035.png), the element-wise multiplication of *A* and *B*
    is defined as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_036.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_12_037.png).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_038.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: 'This gives ![](img/B14070_12_039.png), and the value of *C* is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_040.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: Inverse
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The inverse of the matrix *A* is denoted by *A*^(-1), where it satisfies the
    following condition:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_041.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'Inverse is very useful if we are trying to solve a system of linear equations.
    Consider this example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_042.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'We can solve for ![](img/B14070_12_043.png) like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_044.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: This can be written as ![](img/B14070_12_045.png), using the associative law
    – that is, ![](img/B14070_12_046.png).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will get, where ![](img/B14070_12_049.png) is the identity matrix.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, ![](img/B14070_12_050.png) because ![](img/B14070_12_051.png).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, polynomial regression, one of the regression techniques, uses
    a linear system of equations to solve the regression problem. Regression is similar
    to classification, but instead of outputting a class, regression models output
    a continuous value. Let’s look at an example problem: given the number of bedrooms
    in a house, we’ll calculate the real-estate value of the house. Formally, a polynomial
    regression problem can be written as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，多项式回归是回归技术之一，使用线性方程组来解决回归问题。回归类似于分类，但与分类输出一个类别不同，回归模型输出一个连续值。让我们看一个示例问题：给定一所房子的卧室数量，我们将计算这所房产的价值。形式上，一个多项式回归问题可以写成如下：
- en: '![](img/B14070_12_052.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_12_052.png)'
- en: 'Here,![](img/B14070_12_053.png) is the *i*^(th) data input, where ![](img/B14070_12_054.png)
    is the input, ![](img/B14070_12_055.png)is the label, and ![](img/B14070_12_056.png)
    is the noise in data. In our example, ![](img/B14070_12_057.png) is the number
    of bedrooms and ![](img/B14070_12_058.png) is the price of the house. This can
    be written as a system of linear equations as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B14070_12_053.png) 是第*i*个数据输入，其中 ![](img/B14070_12_054.png) 是输入，![](img/B14070_12_055.png)
    是标签，![](img/B14070_12_056.png) 是数据中的噪声。在我们的例子中，![](img/B14070_12_057.png) 是卧室的数量，![](img/B14070_12_058.png)
    是房子的价格。这可以写成如下的线性方程组：
- en: '![](img/B14070_12_059.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_12_059.png)'
- en: However, *A*^(-1) does not exist for all *A*. There are certain conditions that
    need to be satisfied in order for the inverse to exist for a matrix. For example,
    to define the inverse, *A* needs to be a square matrix (that is, ![](img/B14070_12_060.png)).
    Even when the inverse exists, we cannot always find it in the closed form; sometimes
    it can only be approximated with finite-precision computers. If the inverse exists,
    there are several algorithms for finding it, which we will discuss next.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有的*A*都存在逆。为了矩阵有逆，需要满足一定的条件。例如，为了定义逆矩阵，*A*需要是一个方阵（即，![](img/B14070_12_060.png)）。即使逆矩阵存在，我们也并不总能以封闭形式找到它；有时它只能通过有限精度计算机进行近似。如果逆矩阵存在，那么有几种算法可以找到它，我们将在接下来的内容中讨论。
- en: '**Note**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: When it is said that *A* needs to be a square matrix for the inverse to exist,
    we refer to the standard inversion. There exist variants of the inverse operation
    (for example, the **Moore-Penrose inverse**, also known as pseudoinverse) that
    can perform matrix inversion on general ![](img/B14070_12_061.png) matrices.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说*要*矩阵是方阵才能有逆时，指的是标准的逆运算。也存在逆运算的变种（例如，**摩尔-彭若斯逆**，也称为伪逆），它可以对一般的 ![](img/B14070_12_061.png)
    矩阵进行矩阵求逆操作。
- en: Finding the matrix inverse – Singular Value Decomposition (SVD)
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 求解矩阵逆——奇异值分解（SVD）
- en: 'Let’s now see how we can use SVD to find the inverse of a matrix *A*. SVD factorizes
    *A* into three different matrices, as shown here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用SVD求解矩阵*A*的逆。SVD将*A*分解为三个不同的矩阵，如下所示：
- en: '![](img/B14070_12_062.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_12_062.png)'
- en: 'Here the columns of *U* are known as left singular vectors, columns of *V*
    are known as right singular vectors, and diagonal values of *D* (a diagonal matrix)
    are known as singular values. Left singular vectors are the eigenvectors of ![](img/B14070_12_063.png)
    and the right singular vectors are the eigenvectors of ![](img/B14070_12_064.png).
    Finally, the singular values are the square roots of the eigenvalues of ![](img/B14070_12_065.png)
    and ![](img/B14070_12_066.png). The eigenvector ![](img/B14070_12_067.png) and
    its corresponding eigenvalue ![](img/B14070_12_068.png) of the square matrix *A*
    satisfy the following condition:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*U*的列被称为左奇异向量，*V*的列被称为右奇异向量，*D*（一个对角矩阵）的对角值被称为奇异值。左奇异向量是矩阵 ![](img/B14070_12_063.png)
    的特征向量，右奇异向量是矩阵 ![](img/B14070_12_064.png) 的特征向量。最后，奇异值是矩阵 ![](img/B14070_12_065.png)
    和 ![](img/B14070_12_066.png) 的特征值的平方根。矩阵*A*的特征向量 ![](img/B14070_12_067.png) 及其对应的特征值
    ![](img/B14070_12_068.png) 满足以下条件：
- en: '![](img/B14070_12_069.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_12_069.png)'
- en: 'Then, if the SVD exists, the inverse of *A* is given by this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果SVD存在，矩阵*A*的逆由以下公式给出：
- en: '![](img/B14070_12_070.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_12_070.png)'
- en: 'Since *D* is diagonal, *D*^(-1) is simply the element-wise reciprocal of the
    nonzero elements of *D*. SVD is an important matrix factorization technique that
    appears on many occasions in machine learning. For example, SVD is used for calculating
    **Principal Component Analysis** (**PCA**), which is a popular dimensionality
    reduction technique for data (a purpose similar to that of t-SNE, which we saw
    in *Chapter 4**, Advanced Word Vector Algorithms*). Another, more NLP-oriented
    application of SVD is document ranking. That is, when you want to get the most
    relevant documents (and rank them by relevance to some term, for example, *football*),
    SVD can be used to achieve this. To learn more about SVD, you can consult this
    blog post, which provides a geometric intuition on SVD, as well as showing how
    it’s applied in PCA: [https://gregorygundersen.com/blog/2018/12/10/svd/](https://gregorygundersen.com/blog/2018/12/10/svd/).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Norms
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A norm is used as a measure of the *size* of the vector (that is, of the values
    in the vector). The *p*^(th) norm is calculated and denoted as shown here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_071.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: 'For example, the *L2* norm would be this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_072.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Determinant
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The determinant of a square matrix is denoted by ![](img/B14070_12_073.png).
    The determinant is very useful in many ways. For example, *A* is invertible if,
    and only if, the determinant is nonzero. The determinant is also interpreted as
    the product of all the eigenvalues of the matrix. The determinant of a *2*x*2*
    matrix *A*,
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_075.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: is denoted as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_076.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: and computed as
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_077.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'The following equation shows the calculations for the determinant of a *3x3*
    matrix:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_078.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_079.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_080.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Probability
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we will discuss the terminology related to probability theory. Probability
    theory is a vital part of machine learning, as modeling data with probabilistic
    models allows us to draw conclusions about how uncertain a model is about some
    predictions. Consider a use case of sentiment analysis. We want to output a prediction
    (positive/negative) for a given movie review. Though the model outputs some value
    between 0 and 1 (0 for negative and 1 for positive) for any sample we input, the
    model doesn’t know how *uncertain* it is about its answer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand how uncertainty helps us to make better predictions. For example,
    a deterministic model (i.e. a model that outputs an exact value instead of a distribution
    for the value) might incorrectly say the positivity of the review *I never lost
    interest* is 0.25 (that is, it’s more likely to be a negative comment). However,
    a probabilistic model will give a mean value and a standard deviation for the
    prediction. For example, it will say, this prediction has a mean of 0.25 and a
    standard deviation of 0.5\. With the second model, we know that the prediction
    is likely to be wrong due to the high standard deviation. However, in the deterministic
    model, we don’t have this luxury. This property is especially valuable for critical
    machine systems (for example, a terrorism risk assessment model).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解不确定性如何帮助我们做出更好的预测。例如，一个确定性模型（即输出确切值而非值的分布的模型）可能会错误地说评论 “我从未失去兴趣” 的正向概率是
    0.25（也就是说，它更可能是负面评论）。然而，概率模型将为预测提供一个均值和一个标准差。例如，它可能会说，这个预测的均值为 0.25，标准差为 0.5。在第二种模型下，我们知道由于标准差较大，预测很可能是错误的。然而，在确定性模型中，我们没有这种奢侈的选择。这一特性对于关键的机器系统（例如，恐怖主义风险评估模型）尤其有价值。
- en: To develop such probabilistic machine learning models (for example, Bayesian
    logistic regression, Bayesian neural networks, or Gaussian processes), you should
    be familiar with basic probability theory. Therefore, we will provide some basic
    probability information here.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发这样的概率机器学习模型（例如，贝叶斯逻辑回归、贝叶斯神经网络或高斯过程），你应该熟悉基本的概率理论。因此，我们将在这里提供一些基本的概率信息。
- en: Random variables
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机变量
- en: 'A random variable is a variable that can take some value at random. Also, random
    variables are represented as *x*[1], *x*[2], and so on. Random variables can be
    of two types: discrete and continuous.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量是一个可以随机取值的变量。此外，随机变量通常表示为 *x*[1]、*x*[2] 等。随机变量可以分为两种类型：离散型和连续型。
- en: Discrete random variables
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离散随机变量
- en: A discrete random variable is a variable that can take discrete random values.
    For example, trials of flipping a coin can be modeled as a random variable; that
    is, the side a coin lands on when you flip it is a discrete variable as the value
    can only be *heads* or *tails*. Alternatively, the value you get when you roll
    a die is discrete, as well, as the values can only come from the set `{1,2,3,4,5,6}`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 离散随机变量是指可以取离散随机值的变量。例如，掷硬币的试验可以被建模为一个随机变量；即，硬币掷出的正面或反面是一个离散变量，因为结果只能是*正面*或*反面*。另外，掷骰子的结果也是离散的，因为其值只能来自集合
    `{1,2,3,4,5,6}`。
- en: Continuous random variables
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续随机变量
- en: 'A continuous random variable is a variable that can take any real value, that
    is, if *x* is a continuous random variable:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 连续随机变量是一个可以取任何实数值的变量，也就是说，如果 *x* 是一个连续随机变量：
- en: '![](img/B14070_12_081.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_12_081.png)'
- en: Here, *R* is the real number space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*R* 表示实数空间。
- en: For example, the height of a person is a continuous random variable as it can
    take any real value.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个人的身高是一个连续随机变量，因为它可以取任何实数值。
- en: The probability mass/density function
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率质量/密度函数
- en: 'The **probability mass function** (**PMF**) or the **probability density function**
    (**PDF**) is a way of showing the probability distribution over different values
    a random variable can take. For discrete variables, a PMF is defined, and for
    continuous variables, a PDF is defined. *Figure A.1* shows an example PMF:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率质量函数**（**PMF**）或**概率密度函数**（**PDF**）是一种展示随机变量在不同值上概率分布的方式。对于离散变量，定义了PMF；对于连续变量，定义了PDF。*图
    A.1* 显示了一个PMF的例子：'
- en: '![The probability mass/density function](img/B14070_12_01.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![概率质量/密度函数](img/B14070_12_01.png)'
- en: 'A.1: Probability mass function (PMF) discrete'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 'A.1: 概率质量函数（PMF）离散型'
- en: 'The preceding PMF might be achieved by a *biased* die. In this graph, we can
    see that there is a high probability of getting a 3 with this die. Such a graph
    can be obtained by running a number of trials (say, 100) and then counting the
    number of times each face fell on top. Finally, you would divide each count by
    the number of trials to obtain the normalized probabilities. Note that all the
    probabilities should add up to 1, as shown here:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的概率质量函数（PMF）可能是通过一个*偏*骰子实现的。在这张图中，我们可以看到，掷这个骰子时，出现 3 的概率很高。这样的图形可以通过进行多次试验（比如
    100 次）并统计每个面朝上的次数得到。最后，你需要将每个计数除以试验次数，以获得标准化后的概率。请注意，所有的概率总和应为 1，正如这里所示：
- en: '![](img/B14070_12_082.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_12_082.png)'
- en: 'The same concept is extended to a continuous random variable to obtain a PDF.
    Say that we are trying to model the probability of a certain height given a population.
    Unlike the discrete case, we do not have individual values to calculate the probability
    for, but rather a continuous spectrum of values (in the example, it extends from
    *0* to *2.4 m*). If we are to draw a graph for this example like the one in *Figure
    A.1*, we need to think of it in terms of infinitesimally small bins. For example,
    we find out the probability density of a person’s height being between *0.0 m-0.01
    m, 0.01-0.02 m, ..., 1.8 m-1.81 m, …*, and so on. The probability density can
    be calculated using the following formula:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_083.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Then, we will plot those bars close to each other to obtain a continuous curve,
    as shown in *Figure A.2*. Note that the probability density for a given bin can
    be greater than *1* (since it’s density), but the area under the curve must be
    1:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![The probability mass/density function](img/B14070_12_02.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.2: Probability density function (PDF) continuous'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The shape shown in *Figure A.2* is known as the normal (or Gaussian) distribution.
    It is also called the *bell curve*. We previously gave just an intuitive explanation
    of how to think about a continuous probability density function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, a continuous PDF of the normal distribution has an equation
    and is defined as follows. Let’s assume that a continuous random variable *X*
    has a normal distribution with mean ![](img/B14070_12_084.png) and standard deviation
    ![](img/B14070_12_085.png). The probability of *X = x* for any value of *x* is
    given by this formula:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_086.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'You should get the area (which needs to be 1 for a valid PDF) if you integrate
    this quantity over all possible infinitesimally small *dx* values, as denoted
    by this formula:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_087.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'The integral of the normal for the arbitrary *a*, *b* values is given by the
    following formula:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_088.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
- en: 'Using this, we can get the integral of the normal distribution, where ![](img/B14070_12_089.png)
    and ![](img/B14070_12_090.png):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_091.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: This gives the accumulation of all the probability values for all the values
    of *x* and gives you a value of 1\.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information at [http://mathworld.wolfram.com/GaussianIntegral.html](http://mathworld.wolfram.com/GaussianIntegral.html),
    or for a less complex discussion, refer to [https://en.wikipedia.org/wiki/Gaussian_integral](https://en.wikipedia.org/wiki/Gaussian_integral).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Conditional probability represents the probability of an event happening given
    the occurrence of another event. For example, given two random variables, *X*
    and *Y*, the conditional probability of *X = x*, given that *Y = y*, is denoted
    by this formula:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_092.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: 'A real-world example of such a probability would be as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_093.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: Joint probability
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given two random variables, *X* and *Y*, we will refer to the probability of
    *X = x* together with *Y = y* as the joint probability of *X = x* and *Y = y*.
    This is denoted by the following formula:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_094.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'If *X* and *Y* are mutually exclusive events, this expression reduces to this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_095.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'A real-world example of this is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_096.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: Marginal probability
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A marginal probability distribution is the probability distribution of a subset
    of random variables, given the joint probability distribution of all variables.
    For example, consider that two random variables, *X* and *Y*, exist, and we already
    know ![](img/B14070_12_097.png) and we want to calculate *P(x)*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_098.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Intuitively, we are taking the sum over all possible values of *Y*, effectively
    making the probability of *Y = 1*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ rule
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bayes’ rule gives us a way to calculate ![](img/B14070_12_099.png) if we already
    know ![](img/B14070_12_100.png), and ![](img/B14070_12_101.png). We can easily
    arrive at Bayes’ rule as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_102.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'Now let’s take the middle and right parts:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_103.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_104.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'This is Bayes’ rule. Let’s put it simply, as shown here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_105.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Visualizing word embeddings with TensorBoard
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we wanted to visualize word embeddings in *Chapter 3*, *Word2vec – Learning
    Word Embeddings,* we manually implemented the visualization with the t-SNE algorithm.
    However, you also could use TensorBoard to visualize word embeddings. TensorBoard
    is a visualization tool provided with TensorFlow. You can use TensorBoard to visualize
    the TensorFlow variables in your program. This allows you to see how different
    variables behave over time (for example, model loss/accuracy), so you can identify
    potential issues in your model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard enables you to visualize scalar values (e.g. loss values over training
    iterations) and vectors as histograms (e.g. model’s layer node activations). Apart
    from this, TensorBoard also allows you to visualize word embeddings. Therefore,
    it takes all the required code implementation away from you, if you need to analyze
    what the embeddings look like. Next, we will see how we can use TensorBoard to
    visualize word embeddings. The code for this exercise is provided in `tensorboard_word_embeddings.ipynb`
    in the `Appendix` folder.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Starting TensorBoard
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will list the steps for starting TensorBoard. TensorBoard acts as
    a service and runs on a specific port (by default, on `6006`). To start TensorBoard,
    you will need to follow these steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Open up Command Prompt (Windows) or Terminal (Ubuntu/macOS).
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go into the project home directory.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are using the python `virtualenv`, activate the virtual environment where
    you have installed TensorFlow.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure that you can see the TensorFlow library through Python. To do this,
    follow these steps:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type in `python3`; you will get a `>>>` looking prompt
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Try `import tensorflow as tf`
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If you can run this successfully, you are fine
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Exit the `python` prompt (that is, `>>>`) by typing `exit()`
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type in `tensorboard --logdir=models`:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `--logdir` option points to the directory where you will create data to
    visualize
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, you can use `--port=<port_you_like>` to change the port TensorBoard
    runs on
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should now get the following message:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Enter the `<url>:6006` into the web browser. You should be able to see an orange
    dashboard at this point. You won’t have anything to display because we haven’t
    generated any data.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Saving word embeddings and visualizing via TensorBoard
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will download and load the 50-dimensional GloVe embeddings file (`glove.6B.zip`)
    from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
    and place it in the `Appendix` folder. We will load the first 50,000 word vectors
    in the file and later use these to initialize a TensorFlow variable. We will also
    record the word strings of each word, as we will later provide these as labels
    for each point to display on TensorBoard:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have defined our embeddings as a pandas DataFrame. It has the vector values
    as columns and words as the index.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_03.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.3: GloVe vectors presented as a pandas DataFrame'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to define TensorFlow-related variables and operations. Before
    doing this, we will create a directory called `embeddings`, which will be used
    to store the variables:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we will define a variable that will be initialized with the word embeddings
    we copied from the text file earlier:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We also need to save a metadata file. A metadata file contains labels/images
    or other types of information associated with the word embeddings, so that when
    you hover over the embedding visualization, the corresponding points will show
    the word/label they represent. The metadata file should be of the `.tsv` (tab-separated
    values) format and should contain `vocabulary_size` rows in it, where each row
    contains a word in the order they appear in the embeddings matrix:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we will need to tell TensorFlow where it can find the metadata for the
    embedding data we saved to the disk. For this, we need to create a `ProjectorConfig`
    object, which maintains various configuration details about the embedding we want
    to display. The details stored in the `ProjectorConfig` folder will be saved to
    a file called `projector_config.pbtxt` in the `models` directory:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we will populate the required fields of the `ProjectorConfig` object
    we created. First, we will tell it the name of the variable we’re interested in
    visualizing. Then, we will tell it where it can find the metadata corresponding
    to that variable:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that we are adding the suffix `/.ATTRIBUTES/VARIABLE_VALUE` to the name
    `embedding`. This is required for TensorBoard to find this tensor. TensorBoard
    will read the necessary files at startup:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now if you load TensorBoard, you should see something similar to *Figure A.4*:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_04.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.4: TensorBoard view of the embeddings'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'When you hover over the displayed point cloud, it will show the label of the
    word you’re currently hovering over, as we provided this information in the `metadata.tsv`
    file. Furthermore, you have several options. The first option (shown with a dotted
    line and marked as **1**) will allow you to select a subset of the full embedding
    space. You can draw a bounding box over the area of the embedding space you’re
    interested in, and it will look as shown in *Figure A.5*. I have selected the
    embeddings from the right side of the visualization. You can see the full list
    of selected words on the right:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_05.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.5: Selecting a subset of the embedding space'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option you have is the ability to view words themselves, instead of
    dots. You can do this by selecting the second option in *Figure A.4* (shown inside
    a solid box and marked as **2**). This would look as shown in *Figure A.6*. Additionally,
    you can pan/zoom/rotate the view to your liking. If you click on the help button
    (shown within a solid box and marked as **1** in *Figure A.6*), it will show you
    a guide for controlling the view:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_06.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.6: Embedding vectors displayed as words instead of dots'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can change the visualization algorithm from the panel on the left-hand
    side (shown with a dashed line and marked with **3** in *Figure A.4*).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we discussed some of the mathematical background as well as some implementations
    we did not cover in the other chapters. First, we discussed the mathematical notation
    for scalars, vectors, matrices, and tensors. Then, we discussed various operations
    performed on these data structures such as matrix multiplication and inversion.
    After that, we discussed various terminology that is useful for understanding
    probabilistic machine learning, such as probability density functions, joint probability,
    marginal probability, and Bayes’ rule. Finally, we ended the appendix with a guide
    to visualizing word embeddings using TensorBoard, a visualization platform that
    comes with TensorFlow.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/New_Packt_Logo1.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: packt.com
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve your learning with Skill Plans built especially for you
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At www.packt.com, you can also read a collection of free technical articles,
    sign up for a range of free newsletters, and receive exclusive discounts and offers
    on Packt books and eBooks.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Other Books You May Enjoy
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/9781803247335.png)](https://www.packtpub.com/product/transformers-for-natural-language-processing/9781803247335?_ga=2.5602675.1586621222.1658751433-1060321437.1657688636)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers for Natural Language Processing, Second Edition**'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Denis Rothman
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 9781803247335'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Find out how ViT and CLIP label images (including blurry ones!) and create images
    from a sentence using DALL-E
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover new techniques to investigate complex language problems
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare and contrast the results of GPT-3 against T5, GPT-2, and BERT-based
    transformers
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carry out sentiment analysis, text summarization, casual speech analysis, machine
    translations, and more using TensorFlow, PyTorch, and GPT-3
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measure the productivity of key transformers to define their scope, potential,
    and limits in production
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/9781801819312.png)](https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312?_ga=2.204407376.1586621222.1658751433-1060321437.1657688636)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine Learning with PyTorch and Scikit-Learn**'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Sebastian Raschka
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Yuxi (Hayden) Liu
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Vahid Mirjalili
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 9781801819312'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Explore frameworks, models, and techniques for machines to ‘learn’ from data
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use scikit-learn for machine learning and PyTorch for deep learning
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train machine learning classifiers on images, text, and more
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and train neural networks, transformers, and boosting algorithms
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover best practices for evaluating and tuning models
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict continuous target outcomes using regression analysis
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dig deeper into textual and social media data using sentiment analysis
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packt is searching for authors like you
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re interested in becoming an author for Packt, please visit authors.packtpub.com
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you’ve finished *Natural Language Processing with TensorFlow, Second Edition*,
    we’d love to hear your thoughts! If you purchased the book from Amazon, please
    [click here to go straight to the Amazon review page](https://packt.link/r/1838641351)
    for this book and share your feedback or leave a review on the site that you purchased
    it from.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
