- en: 'Appendix A: Mathematical Foundations and Advanced TensorFlow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we will discuss some concepts that will be useful for helping you to understand
    certain details provided in the chapters. First, we will discuss several mathematical
    data structures found throughout the book, followed by a description of the various
    operations performed on those data structures. After that, we will discuss the
    concept of probabilities. Probabilities play a vital role in machine learning,
    as they usually give insights into how uncertain a model is about its prediction.
    Finally, we will conclude this appendix with a guide on how to use TensorBoard
    as a visualization tool for word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Basic data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A scalar is a single number, unlike a matrix or a vector. For example, 1.3
    is a scalar. A scalar can be mathematically denoted as follows: ![](img/B14070_12_001.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *R* is the real number space.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A vector is an array of numbers. Unlike a set, where there is no order to the
    elements, a vector has a certain order to the elements. An example vector is `[1.0,
    2.0, 1.4, 2.3]`. Mathematically, it can be denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_002.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *R* is the real number space and *n* is the number of elements in the
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: Matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A matrix can be thought of as a two-dimensional arrangement of a collection
    of scalars. In other words, a matrix can be thought of as a vector of vectors.
    An example matrix is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A more general matrix of size ![](img/B14070_12_005.png) can be mathematically
    defined like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *m* is the number of rows of the matrix, *n* is the number of columns
    in the matrix, and *R* is the real number space.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing of a matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using zero-indexed notation (that is, indexes that start with 0).
  prefs: []
  type: TYPE_NORMAL
- en: 'To index a single element from a matrix at the *(i, j)*^(th) position, we use
    the following notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Referring to the previously defined matrix, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We index an element from *A* like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We denote a single row of any matrix *A* as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our example matrix, we can denote the second row (indexed as 1) of the
    matrix as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We denote the slice starting from the *(i, k)*^(th) index to the *(j, l)*^(th)
    index of any matrix *A* as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our example matrix, we can denote the slice from first row third column
    to second row fourth column as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_014.png)'
  prefs: []
  type: TYPE_IMG
- en: Special types of matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identity matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An identity matrix is a square matrix where values are equal to 1 on the diagonal
    of the matrix and 0 everywhere else. Mathematically, it can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_016.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_12_017.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The identity matrix gives the following nice property when multiplied with
    another matrix *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_018.png)'
  prefs: []
  type: TYPE_IMG
- en: Square diagonal matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A square diagonal matrix is a more general case of the identity matrix, where
    the values along the diagonal can take any value and the off-diagonal values are
    zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_019.png)'
  prefs: []
  type: TYPE_IMG
- en: Tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An *n*-dimensional matrix is called a **tensor**. In other words, a matrix
    with an arbitrary number of dimensions is called a tensor. For example, a four-dimensional
    tensor can be denoted as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *R* is the real number space.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor/matrix operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transpose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transpose is an important operation defined for matrices or tensors. For a
    matrix, the transpose is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_021.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *A*^T denotes the transpose of *A*.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of the transpose operation can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the transpose operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For a tensor, transpose can be seen as permuting the dimensions order. For
    example, let’s define a tensor *S*, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now one transpose operation (out of many) can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_025.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matrix multiplication is another important operation that appears quite frequently
    in linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the matrices ![](img/B14070_12_026.png) and ![](img/B14070_12_027.png),
    the multiplication of *A* and *B* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_028.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_12_029.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_030.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives ![](img/B14070_12_032.png), and the value of *C* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_033.png)'
  prefs: []
  type: TYPE_IMG
- en: Element-wise multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Element-wise matrix multiplication (or the **Hadamard product**) is computed
    for two matrices that have the same shape. Given the matrices ![](img/B14070_12_034.png)
    and ![](img/B14070_12_035.png), the element-wise multiplication of *A* and *B*
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_036.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B14070_12_037.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives ![](img/B14070_12_039.png), and the value of *C* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_040.png)'
  prefs: []
  type: TYPE_IMG
- en: Inverse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The inverse of the matrix *A* is denoted by *A*^(-1), where it satisfies the
    following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inverse is very useful if we are trying to solve a system of linear equations.
    Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can solve for ![](img/B14070_12_043.png) like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_044.png)'
  prefs: []
  type: TYPE_IMG
- en: This can be written as ![](img/B14070_12_045.png), using the associative law
    – that is, ![](img/B14070_12_046.png).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will get, where ![](img/B14070_12_049.png) is the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, ![](img/B14070_12_050.png) because ![](img/B14070_12_051.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, polynomial regression, one of the regression techniques, uses
    a linear system of equations to solve the regression problem. Regression is similar
    to classification, but instead of outputting a class, regression models output
    a continuous value. Let’s look at an example problem: given the number of bedrooms
    in a house, we’ll calculate the real-estate value of the house. Formally, a polynomial
    regression problem can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_052.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here,![](img/B14070_12_053.png) is the *i*^(th) data input, where ![](img/B14070_12_054.png)
    is the input, ![](img/B14070_12_055.png)is the label, and ![](img/B14070_12_056.png)
    is the noise in data. In our example, ![](img/B14070_12_057.png) is the number
    of bedrooms and ![](img/B14070_12_058.png) is the price of the house. This can
    be written as a system of linear equations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_059.png)'
  prefs: []
  type: TYPE_IMG
- en: However, *A*^(-1) does not exist for all *A*. There are certain conditions that
    need to be satisfied in order for the inverse to exist for a matrix. For example,
    to define the inverse, *A* needs to be a square matrix (that is, ![](img/B14070_12_060.png)).
    Even when the inverse exists, we cannot always find it in the closed form; sometimes
    it can only be approximated with finite-precision computers. If the inverse exists,
    there are several algorithms for finding it, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: When it is said that *A* needs to be a square matrix for the inverse to exist,
    we refer to the standard inversion. There exist variants of the inverse operation
    (for example, the **Moore-Penrose inverse**, also known as pseudoinverse) that
    can perform matrix inversion on general ![](img/B14070_12_061.png) matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the matrix inverse – Singular Value Decomposition (SVD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now see how we can use SVD to find the inverse of a matrix *A*. SVD factorizes
    *A* into three different matrices, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_062.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here the columns of *U* are known as left singular vectors, columns of *V*
    are known as right singular vectors, and diagonal values of *D* (a diagonal matrix)
    are known as singular values. Left singular vectors are the eigenvectors of ![](img/B14070_12_063.png)
    and the right singular vectors are the eigenvectors of ![](img/B14070_12_064.png).
    Finally, the singular values are the square roots of the eigenvalues of ![](img/B14070_12_065.png)
    and ![](img/B14070_12_066.png). The eigenvector ![](img/B14070_12_067.png) and
    its corresponding eigenvalue ![](img/B14070_12_068.png) of the square matrix *A*
    satisfy the following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_069.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, if the SVD exists, the inverse of *A* is given by this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_070.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since *D* is diagonal, *D*^(-1) is simply the element-wise reciprocal of the
    nonzero elements of *D*. SVD is an important matrix factorization technique that
    appears on many occasions in machine learning. For example, SVD is used for calculating
    **Principal Component Analysis** (**PCA**), which is a popular dimensionality
    reduction technique for data (a purpose similar to that of t-SNE, which we saw
    in *Chapter 4**, Advanced Word Vector Algorithms*). Another, more NLP-oriented
    application of SVD is document ranking. That is, when you want to get the most
    relevant documents (and rank them by relevance to some term, for example, *football*),
    SVD can be used to achieve this. To learn more about SVD, you can consult this
    blog post, which provides a geometric intuition on SVD, as well as showing how
    it’s applied in PCA: [https://gregorygundersen.com/blog/2018/12/10/svd/](https://gregorygundersen.com/blog/2018/12/10/svd/).'
  prefs: []
  type: TYPE_NORMAL
- en: Norms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A norm is used as a measure of the *size* of the vector (that is, of the values
    in the vector). The *p*^(th) norm is calculated and denoted as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_071.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, the *L2* norm would be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_072.png)'
  prefs: []
  type: TYPE_IMG
- en: Determinant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The determinant of a square matrix is denoted by ![](img/B14070_12_073.png).
    The determinant is very useful in many ways. For example, *A* is invertible if,
    and only if, the determinant is nonzero. The determinant is also interpreted as
    the product of all the eigenvalues of the matrix. The determinant of a *2*x*2*
    matrix *A*,
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_075.png)'
  prefs: []
  type: TYPE_IMG
- en: is denoted as
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_076.png)'
  prefs: []
  type: TYPE_IMG
- en: and computed as
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_077.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following equation shows the calculations for the determinant of a *3x3*
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_078.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_079.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_080.png)'
  prefs: []
  type: TYPE_IMG
- en: Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we will discuss the terminology related to probability theory. Probability
    theory is a vital part of machine learning, as modeling data with probabilistic
    models allows us to draw conclusions about how uncertain a model is about some
    predictions. Consider a use case of sentiment analysis. We want to output a prediction
    (positive/negative) for a given movie review. Though the model outputs some value
    between 0 and 1 (0 for negative and 1 for positive) for any sample we input, the
    model doesn’t know how *uncertain* it is about its answer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand how uncertainty helps us to make better predictions. For example,
    a deterministic model (i.e. a model that outputs an exact value instead of a distribution
    for the value) might incorrectly say the positivity of the review *I never lost
    interest* is 0.25 (that is, it’s more likely to be a negative comment). However,
    a probabilistic model will give a mean value and a standard deviation for the
    prediction. For example, it will say, this prediction has a mean of 0.25 and a
    standard deviation of 0.5\. With the second model, we know that the prediction
    is likely to be wrong due to the high standard deviation. However, in the deterministic
    model, we don’t have this luxury. This property is especially valuable for critical
    machine systems (for example, a terrorism risk assessment model).
  prefs: []
  type: TYPE_NORMAL
- en: To develop such probabilistic machine learning models (for example, Bayesian
    logistic regression, Bayesian neural networks, or Gaussian processes), you should
    be familiar with basic probability theory. Therefore, we will provide some basic
    probability information here.
  prefs: []
  type: TYPE_NORMAL
- en: Random variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A random variable is a variable that can take some value at random. Also, random
    variables are represented as *x*[1], *x*[2], and so on. Random variables can be
    of two types: discrete and continuous.'
  prefs: []
  type: TYPE_NORMAL
- en: Discrete random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A discrete random variable is a variable that can take discrete random values.
    For example, trials of flipping a coin can be modeled as a random variable; that
    is, the side a coin lands on when you flip it is a discrete variable as the value
    can only be *heads* or *tails*. Alternatively, the value you get when you roll
    a die is discrete, as well, as the values can only come from the set `{1,2,3,4,5,6}`.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A continuous random variable is a variable that can take any real value, that
    is, if *x* is a continuous random variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_081.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *R* is the real number space.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the height of a person is a continuous random variable as it can
    take any real value.
  prefs: []
  type: TYPE_NORMAL
- en: The probability mass/density function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **probability mass function** (**PMF**) or the **probability density function**
    (**PDF**) is a way of showing the probability distribution over different values
    a random variable can take. For discrete variables, a PMF is defined, and for
    continuous variables, a PDF is defined. *Figure A.1* shows an example PMF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The probability mass/density function](img/B14070_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A.1: Probability mass function (PMF) discrete'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding PMF might be achieved by a *biased* die. In this graph, we can
    see that there is a high probability of getting a 3 with this die. Such a graph
    can be obtained by running a number of trials (say, 100) and then counting the
    number of times each face fell on top. Finally, you would divide each count by
    the number of trials to obtain the normalized probabilities. Note that all the
    probabilities should add up to 1, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_082.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The same concept is extended to a continuous random variable to obtain a PDF.
    Say that we are trying to model the probability of a certain height given a population.
    Unlike the discrete case, we do not have individual values to calculate the probability
    for, but rather a continuous spectrum of values (in the example, it extends from
    *0* to *2.4 m*). If we are to draw a graph for this example like the one in *Figure
    A.1*, we need to think of it in terms of infinitesimally small bins. For example,
    we find out the probability density of a person’s height being between *0.0 m-0.01
    m, 0.01-0.02 m, ..., 1.8 m-1.81 m, …*, and so on. The probability density can
    be calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_083.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we will plot those bars close to each other to obtain a continuous curve,
    as shown in *Figure A.2*. Note that the probability density for a given bin can
    be greater than *1* (since it’s density), but the area under the curve must be
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The probability mass/density function](img/B14070_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.2: Probability density function (PDF) continuous'
  prefs: []
  type: TYPE_NORMAL
- en: The shape shown in *Figure A.2* is known as the normal (or Gaussian) distribution.
    It is also called the *bell curve*. We previously gave just an intuitive explanation
    of how to think about a continuous probability density function.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, a continuous PDF of the normal distribution has an equation
    and is defined as follows. Let’s assume that a continuous random variable *X*
    has a normal distribution with mean ![](img/B14070_12_084.png) and standard deviation
    ![](img/B14070_12_085.png). The probability of *X = x* for any value of *x* is
    given by this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_086.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You should get the area (which needs to be 1 for a valid PDF) if you integrate
    this quantity over all possible infinitesimally small *dx* values, as denoted
    by this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_087.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The integral of the normal for the arbitrary *a*, *b* values is given by the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_088.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this, we can get the integral of the normal distribution, where ![](img/B14070_12_089.png)
    and ![](img/B14070_12_090.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_091.png)'
  prefs: []
  type: TYPE_IMG
- en: This gives the accumulation of all the probability values for all the values
    of *x* and gives you a value of 1\.
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information at [http://mathworld.wolfram.com/GaussianIntegral.html](http://mathworld.wolfram.com/GaussianIntegral.html),
    or for a less complex discussion, refer to [https://en.wikipedia.org/wiki/Gaussian_integral](https://en.wikipedia.org/wiki/Gaussian_integral).
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Conditional probability represents the probability of an event happening given
    the occurrence of another event. For example, given two random variables, *X*
    and *Y*, the conditional probability of *X = x*, given that *Y = y*, is denoted
    by this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_092.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A real-world example of such a probability would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_093.png)'
  prefs: []
  type: TYPE_IMG
- en: Joint probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given two random variables, *X* and *Y*, we will refer to the probability of
    *X = x* together with *Y = y* as the joint probability of *X = x* and *Y = y*.
    This is denoted by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_094.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If *X* and *Y* are mutually exclusive events, this expression reduces to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_095.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A real-world example of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_096.png)'
  prefs: []
  type: TYPE_IMG
- en: Marginal probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A marginal probability distribution is the probability distribution of a subset
    of random variables, given the joint probability distribution of all variables.
    For example, consider that two random variables, *X* and *Y*, exist, and we already
    know ![](img/B14070_12_097.png) and we want to calculate *P(x)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_098.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, we are taking the sum over all possible values of *Y*, effectively
    making the probability of *Y = 1*.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bayes’ rule gives us a way to calculate ![](img/B14070_12_099.png) if we already
    know ![](img/B14070_12_100.png), and ![](img/B14070_12_101.png). We can easily
    arrive at Bayes’ rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_102.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let’s take the middle and right parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_103.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_12_104.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is Bayes’ rule. Let’s put it simply, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_105.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing word embeddings with TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we wanted to visualize word embeddings in *Chapter 3*, *Word2vec – Learning
    Word Embeddings,* we manually implemented the visualization with the t-SNE algorithm.
    However, you also could use TensorBoard to visualize word embeddings. TensorBoard
    is a visualization tool provided with TensorFlow. You can use TensorBoard to visualize
    the TensorFlow variables in your program. This allows you to see how different
    variables behave over time (for example, model loss/accuracy), so you can identify
    potential issues in your model.
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard enables you to visualize scalar values (e.g. loss values over training
    iterations) and vectors as histograms (e.g. model’s layer node activations). Apart
    from this, TensorBoard also allows you to visualize word embeddings. Therefore,
    it takes all the required code implementation away from you, if you need to analyze
    what the embeddings look like. Next, we will see how we can use TensorBoard to
    visualize word embeddings. The code for this exercise is provided in `tensorboard_word_embeddings.ipynb`
    in the `Appendix` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Starting TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will list the steps for starting TensorBoard. TensorBoard acts as
    a service and runs on a specific port (by default, on `6006`). To start TensorBoard,
    you will need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up Command Prompt (Windows) or Terminal (Ubuntu/macOS).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go into the project home directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are using the python `virtualenv`, activate the virtual environment where
    you have installed TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure that you can see the TensorFlow library through Python. To do this,
    follow these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type in `python3`; you will get a `>>>` looking prompt
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Try `import tensorflow as tf`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If you can run this successfully, you are fine
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Exit the `python` prompt (that is, `>>>`) by typing `exit()`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type in `tensorboard --logdir=models`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `--logdir` option points to the directory where you will create data to
    visualize
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, you can use `--port=<port_you_like>` to change the port TensorBoard
    runs on
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should now get the following message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Enter the `<url>:6006` into the web browser. You should be able to see an orange
    dashboard at this point. You won’t have anything to display because we haven’t
    generated any data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Saving word embeddings and visualizing via TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will download and load the 50-dimensional GloVe embeddings file (`glove.6B.zip`)
    from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
    and place it in the `Appendix` folder. We will load the first 50,000 word vectors
    in the file and later use these to initialize a TensorFlow variable. We will also
    record the word strings of each word, as we will later provide these as labels
    for each point to display on TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We have defined our embeddings as a pandas DataFrame. It has the vector values
    as columns and words as the index.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.3: GloVe vectors presented as a pandas DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to define TensorFlow-related variables and operations. Before
    doing this, we will create a directory called `embeddings`, which will be used
    to store the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will define a variable that will be initialized with the word embeddings
    we copied from the text file earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to save a metadata file. A metadata file contains labels/images
    or other types of information associated with the word embeddings, so that when
    you hover over the embedding visualization, the corresponding points will show
    the word/label they represent. The metadata file should be of the `.tsv` (tab-separated
    values) format and should contain `vocabulary_size` rows in it, where each row
    contains a word in the order they appear in the embeddings matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will need to tell TensorFlow where it can find the metadata for the
    embedding data we saved to the disk. For this, we need to create a `ProjectorConfig`
    object, which maintains various configuration details about the embedding we want
    to display. The details stored in the `ProjectorConfig` folder will be saved to
    a file called `projector_config.pbtxt` in the `models` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will populate the required fields of the `ProjectorConfig` object
    we created. First, we will tell it the name of the variable we’re interested in
    visualizing. Then, we will tell it where it can find the metadata corresponding
    to that variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are adding the suffix `/.ATTRIBUTES/VARIABLE_VALUE` to the name
    `embedding`. This is required for TensorBoard to find this tensor. TensorBoard
    will read the necessary files at startup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if you load TensorBoard, you should see something similar to *Figure A.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.4: TensorBoard view of the embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you hover over the displayed point cloud, it will show the label of the
    word you’re currently hovering over, as we provided this information in the `metadata.tsv`
    file. Furthermore, you have several options. The first option (shown with a dotted
    line and marked as **1**) will allow you to select a subset of the full embedding
    space. You can draw a bounding box over the area of the embedding space you’re
    interested in, and it will look as shown in *Figure A.5*. I have selected the
    embeddings from the right side of the visualization. You can see the full list
    of selected words on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.5: Selecting a subset of the embedding space'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option you have is the ability to view words themselves, instead of
    dots. You can do this by selecting the second option in *Figure A.4* (shown inside
    a solid box and marked as **2**). This would look as shown in *Figure A.6*. Additionally,
    you can pan/zoom/rotate the view to your liking. If you click on the help button
    (shown within a solid box and marked as **1** in *Figure A.6*), it will show you
    a guide for controlling the view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure A.6: Embedding vectors displayed as words instead of dots'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can change the visualization algorithm from the panel on the left-hand
    side (shown with a dashed line and marked with **3** in *Figure A.4*).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we discussed some of the mathematical background as well as some implementations
    we did not cover in the other chapters. First, we discussed the mathematical notation
    for scalars, vectors, matrices, and tensors. Then, we discussed various operations
    performed on these data structures such as matrix multiplication and inversion.
    After that, we discussed various terminology that is useful for understanding
    probabilistic machine learning, such as probability density functions, joint probability,
    marginal probability, and Bayes’ rule. Finally, we ended the appendix with a guide
    to visualizing word embeddings using TensorBoard, a visualization platform that
    comes with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/New_Packt_Logo1.png)'
  prefs: []
  type: TYPE_IMG
- en: packt.com
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve your learning with Skill Plans built especially for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At www.packt.com, you can also read a collection of free technical articles,
    sign up for a range of free newsletters, and receive exclusive discounts and offers
    on Packt books and eBooks.
  prefs: []
  type: TYPE_NORMAL
- en: Other Books You May Enjoy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/9781803247335.png)](https://www.packtpub.com/product/transformers-for-natural-language-processing/9781803247335?_ga=2.5602675.1586621222.1658751433-1060321437.1657688636)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers for Natural Language Processing, Second Edition**'
  prefs: []
  type: TYPE_NORMAL
- en: Denis Rothman
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 9781803247335'
  prefs: []
  type: TYPE_NORMAL
- en: Find out how ViT and CLIP label images (including blurry ones!) and create images
    from a sentence using DALL-E
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover new techniques to investigate complex language problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare and contrast the results of GPT-3 against T5, GPT-2, and BERT-based
    transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carry out sentiment analysis, text summarization, casual speech analysis, machine
    translations, and more using TensorFlow, PyTorch, and GPT-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measure the productivity of key transformers to define their scope, potential,
    and limits in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/9781801819312.png)](https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312?_ga=2.204407376.1586621222.1658751433-1060321437.1657688636)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine Learning with PyTorch and Scikit-Learn**'
  prefs: []
  type: TYPE_NORMAL
- en: Sebastian Raschka
  prefs: []
  type: TYPE_NORMAL
- en: Yuxi (Hayden) Liu
  prefs: []
  type: TYPE_NORMAL
- en: Vahid Mirjalili
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 9781801819312'
  prefs: []
  type: TYPE_NORMAL
- en: Explore frameworks, models, and techniques for machines to ‘learn’ from data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use scikit-learn for machine learning and PyTorch for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train machine learning classifiers on images, text, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and train neural networks, transformers, and boosting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover best practices for evaluating and tuning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict continuous target outcomes using regression analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dig deeper into textual and social media data using sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packt is searching for authors like you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re interested in becoming an author for Packt, please visit authors.packtpub.com
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you’ve finished *Natural Language Processing with TensorFlow, Second Edition*,
    we’d love to hear your thoughts! If you purchased the book from Amazon, please
    [click here to go straight to the Amazon review page](https://packt.link/r/1838641351)
    for this book and share your feedback or leave a review on the site that you purchased
    it from.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
