<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer081">
<h1 class="chapterNumber">1</h1>
<h1 class="chapterTitle" id="_idParaDest-14">Neural Network Foundations with TF</h1>
<p class="normal">In this chapter, we learn the basics of TensorFlow, an open-source library developed by Google for machine learning and deep learning. In addition, we introduce the basics of neural networks and deep learning, two areas of machine learning that have had incredible Cambrian growth during the last few years. The idea behind this chapter is to provide all the tools needed to do basic but fully hands-on deep learning.</p>
<p class="normal">We will learn:</p>
<ul>
<li class="bulletList">What TensorFlow and Keras are</li>
<li class="bulletList">An introduction to neural networks</li>
<li class="bulletList">What the perceptron and multi-layer perceptron are</li>
<li class="bulletList">A real example: recognizing handwritten digits</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp1"><span class="url">https://packt.link/dltfchp1</span></a>.</p>
</div>
<p class="normal">Let’s begin!</p>
<h1 class="heading-1" id="_idParaDest-15">What is TensorFlow (TF)?</h1>
<p class="normal">TensorFlow is a powerful open-source software<a id="_idIndexMarker000"/> library developed by the Google Brain Team for deep neural networks, the topic covered in this book. It was first made available under the Apache 2.0 License in November 2015 and has since grown rapidly; as of May 2022, its GitHub repository (<a href="https://github.com/tensorflow/tensorflow"><span class="url">https://github.com/tensorflow/tensorflow</span></a>) has more than 129,000 commits, with roughly 3,100 contributors. This in itself provides a measure of the popularity of TensorFlow.</p>
<p class="normal">Let us first learn what exactly TensorFlow<a id="_idIndexMarker001"/> is and why it is so popular among deep neural network researchers and engineers. Google calls it “an open-source software library for machine intelligence,” but since there are so many other<a id="_idIndexMarker002"/> deep learning libraries like PyTorch (<a href="https://pytorch.org/"><span class="url">https://pytorch.org/</span></a>), Caffe (<a href="https://caffe.berkeleyvision.org/"><span class="url">https://caffe.berkeleyvision.org/</span></a>), and MXNet (<a href="https://mxnet.apache.org/"><span class="url">https://mxnet.apache.org/</span></a>), what makes TensorFlow special? Most<a id="_idIndexMarker003"/> other deep<a id="_idIndexMarker004"/> learning libraries, like TensorFlow, have auto-differentiation (a useful mathematical tool used for optimization), many are open-source platforms. Most of them support the CPU/GPU option, have pretrained models, and support commonly used NN architectures like recurrent neural networks, convolutional neural networks, and deep belief networks. So, what<a id="_idIndexMarker005"/> else is there in TensorFlow? Let me list the top features:</p>
<ul>
<li class="bulletList">It works with all popular languages such as Python, C++, Java, R, and Go. TensorFlow provides stable Python and C++ APIs, as well as a non-guaranteed backward-compatible API for other languages. </li>
<li class="bulletList">Keras – a high-level neural network API that has been integrated with TensorFlow (in 2.0 Keras became the standard API for interacting with TensorFlow). This API specifies how software components should interact.</li>
<li class="bulletList">TensorFlow allows model deployment and ease of use in production.</li>
<li class="bulletList">Most importantly, TensorFlow has very good community support. </li>
</ul>
<p class="normal">The number of stars on GitHub (see <em class="italic">Figure 1.1</em>) is a measure of popularity for all open-source projects. As of May 2022, TensorFlow, Keras, and PyTorch have 165K, 55K, and 56K stars respectively, which makes TensorFlow the most popular framework for machine learning:</p>
<figure class="mediaobject"><img alt="" height="547" src="../Images/B18331_01_01.png" width="875"/></figure>
<p class="packt_figref">Figure 1.1: Number of stars for various deep learning projects on GitHub</p>
<h1 class="heading-1" id="_idParaDest-16">What is Keras?</h1>
<p class="normal">Keras is a beautiful API for composing building<a id="_idIndexMarker006"/> blocks to create and train deep learning models. Keras can be integrated with multiple deep learning engines including Google TensorFlow, Microsoft CNTK, Amazon MXNet, and Theano. Starting with TensorFlow 2.0, Keras, the API developed by François Chollet, has been adopted as the standard high-level API, largely simplifying coding and making programming more intuitive.</p>
<h1 class="heading-1" id="_idParaDest-17">Introduction to neural networks</h1>
<p class="normal">Artificial neural networks (briefly, “nets” or ANNs) represent a class of machine<a id="_idIndexMarker007"/> learning models loosely inspired by studies about the central nervous systems of mammals. Each ANN is made up of several interconnected “neurons,” organized in “layers.” Neurons<a id="_idIndexMarker008"/> in one layer pass messages to neurons in the next layer (they “fire,” in jargon terms) and this is how the network computes things. Initial studies were started in the early 1950s with the introduction of the “perceptron” [1], a two-layer network<a id="_idIndexMarker009"/> used for simple operations, and further expanded in the late 1960s with the introduction of the “back-propagation” algorithm used for efficient<a id="_idIndexMarker010"/> multi-layer network training (according to [2] and [3]). Some studies argue that these techniques have roots dating further back than normally cited [4].</p>
<p class="normal">Neural networks<a id="_idIndexMarker011"/> were a topic of intensive academic studies up until the 1980s, at which point other simpler approaches became more relevant. However, there has been a resurgence of interest since the mid 2000s, mainly thanks to three factors: a breakthrough fast learning algorithm proposed by G. Hinton [3], [5], and [6]; the introduction of GPUs around 2011 for massive numeric computation; and the availability of big collections of data for training.</p>
<p class="normal">These improvements<a id="_idIndexMarker012"/> opened the route for modern “deep learning,” a class of neural networks characterized by a significant number of layers of neurons that are able to learn rather sophisticated models, based on progressive levels of abstraction. People began referring to it as “deep” when it started utilizing 3–5 layers a few years ago. Now, networks with more than 200 layers are commonplace!</p>
<p class="normal">This learning via progressive abstraction resembles vision models that have evolved over millions of years within the human brain. Indeed, the human visual system is organized into different layers. First, our eyes are connected to an area of the brain named the visual cortex (V1), which is located in the lower posterior part of our brain. This area is common to many mammals and has the role of discriminating basic properties like small changes in visual orientation, spatial frequencies, and colors.</p>
<p class="normal">It has been estimated that V1 consists of about 140 million neurons, with tens of billions of connections between them. V1 is then connected to other areas, V2, V3, V4, V5, and V6 doing progressively more complex image processing and recognition of more sophisticated concepts, such as shapes, faces, animals, and many more. It has been estimated that there are ~16 billion human cortical neurons and about 10–25% of the human cortex is devoted to vision [7]. Deep learning has taken some inspiration from this layer-based organization of the human visual system: early artificial neuron layers learn basic properties of images while deeper layers learn more sophisticated concepts.</p>
<p class="normal">This book covers several major aspects of neural networks by providing working nets in TensorFlow. So, let’s start!</p>
<h1 class="heading-1" id="_idParaDest-18">Perceptron</h1>
<p class="normal">The “perceptron” is a simple algorithm<a id="_idIndexMarker013"/> that, given an input<a id="_idIndexMarker014"/> vector <em class="italic">x</em> of <em class="italic">m</em> values (<em class="italic">x</em><sub class="subscript">1</sub>, <em class="italic">x</em><sub class="subscript">2</sub>,..., and <em class="italic">x</em><sub class="italic">m</sub>), often called input features or simply<a id="_idIndexMarker015"/> features, outputs either a <em class="italic">1</em> (“yes”) or a <em class="italic">0</em> (“no”). Mathematically, we define a function:</p>
<p class="center"><img alt="" height="83" src="../Images/B18331_01_001.png" style="height: 2.08em !important;" width="408"/></p>
<p class="normal">Where <em class="italic">w</em> is a vector of weights, <img alt="" height="42" src="../Images/B18331_01_002.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="83"/> is the dot product <img alt="" height="54" src="../Images/B18331_01_003.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="142"/>, and <em class="italic">b</em> is the bias. If you remember elementary geometry, <em class="italic">wx</em> + <em class="italic">b</em> defines a boundary hyperplane that changes position according to the values assigned to <em class="italic">w</em> and <em class="italic">b</em>. Note that a hyperplane is a subspace whose dimension is one fewer than that of its ambient space. See (<em class="italic">Figure 1.2</em>) for an example:</p>
<figure class="mediaobject"><img alt="" height="154" src="../Images/B18331_01_02.png" width="158"/></figure>
<p class="packt_figref">Figure 1.2: An example of a hyperplane</p>
<p class="normal">In other words, this is a very simple but effective algorithm! For example, given three input features, the amounts of red, green, and blue in a color, the perceptron could try to decide whether the color is “white” or not.</p>
<p class="normal">Note that the perceptron cannot express a “<em class="italic">maybe</em>” answer. It can answer “yes” (1) or “no” (0) if we understand how to define <em class="italic">w</em> and <em class="italic">b</em>. This is the “training” process that will be discussed in the following sections.</p>
<h2 class="heading-2" id="_idParaDest-19">Our first example of TensorFlow code</h2>
<p class="normal">There are three ways of creating<a id="_idIndexMarker016"/> a model in <code class="inlineCode">tf.keras</code>: sequential API, functional API, and model subclassing. In this chapter, we will use the simplest one, <code class="inlineCode">Sequential()</code>, while the other two are discussed in <em class="chapterRef">Chapter 2</em>, <em class="italic">Regression and Classification</em>. A <code class="inlineCode">Sequential()</code> model is a linear pipeline (a stack) of neural<a id="_idIndexMarker017"/> network layers. This code fragment defines a single<a id="_idIndexMarker018"/> layer with 10 artificial neurons that expect 784 input<a id="_idIndexMarker019"/> variables (also known as features). Note that the net is “dense,” meaning that each neuron in a layer is connected to all neurons located in the previous layer, and to all the neurons in the following layer:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
NB_CLASSES = <span class="hljs-number">10</span>
RESHAPED = <span class="hljs-number">784</span>
model = tf.keras.models.Sequential()
model.add(keras.layers.Dense(NB_CLASSES,
            input_shape=(RESHAPED,), kernel_initializer=<span class="hljs-string">'zeros'</span>,
            name=<span class="hljs-string">'dense_layer'</span>, activation=<span class="hljs-string">'softmax'</span>))
</code></pre>
<p class="normal">Each neuron can be initialized with specific weights via the <code class="inlineCode">'kernel_initializer'</code> parameter. There are a few choices, the most common of which are listed below: </p>
<ul>
<li class="bulletList"><code class="inlineCode">random_uniform</code>: Weights are initialized to uniformly random small values in the range (-0.05, 0.05).</li>
<li class="bulletList"><code class="inlineCode">random_normal</code>: Weights are initialized according to a Gaussian distribution, with zero mean and a small standard deviation of 0.05. For those of you who are not familiar with a Gaussian distribution, think about a symmetric “bell curve” shape.</li>
<li class="bulletList"><code class="inlineCode">zero</code>: All weights are initialized to zero.</li>
</ul>
<p class="normal">A full list<a id="_idIndexMarker020"/> is available online (<a href="https://www.tensorflow.org/api_docs/python/tf/keras/initializers"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/initializers</span></a>).</p>
<h1 class="heading-1" id="_idParaDest-20">Multi-layer perceptron: our first example of a network</h1>
<p class="normal">In this chapter, we present<a id="_idIndexMarker021"/> our first example of a network with multiple dense layers. Historically, “perceptron” was the name given to the model having one single linear layer, and as a consequence, if it has multiple layers, we call it a <strong class="keyWord">Multi-Layer Perceptron</strong> (<strong class="keyWord">MLP</strong>). Note that the input and the output layers are visible from the outside, while all the other layers in the middle are hidden – hence the name <em class="italic">hidden layers</em>. In this context, a single layer is simply a linear function and the MLP is therefore obtained by stacking<a id="_idIndexMarker022"/> multiple single layers one after the other:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="291" src="../Images/Image5021.png" width="379"/></figure>
<p class="packt_figref">Figure 1.3: An example of multiple layer perceptron</p>
<p class="normal">In <em class="italic">Figure 1.3</em> each node in the first hidden layer receives an input and “fires” (0,1) according to the values of the associated linear function. Then the output of the first hidden layer is passed to the second layer where another linear function is applied, the results of which are passed to the final output layer consisting of one single neuron. It is interesting to note that this layered organization vaguely resembles the organization of the human vision system, as we discussed earlier.</p>
<h2 class="heading-2" id="_idParaDest-21">Problems in training the perceptron and solution</h2>
<p class="normal">Let’s consider a single neuron; what <a id="_idIndexMarker023"/>are the best choices for the weight <em class="italic">w</em> and the bias <em class="italic">b</em>? Ideally, we would like to provide a set of training examples and let the computer adjust the weight and the bias in such a way that the errors produced in the output are minimized.</p>
<p class="normal">In order to make this a bit more concrete, let’s suppose that we have a set of images of cats and another separate set of images not containing cats. Suppose that each neuron receives input from the value of a single pixel in the images. While the computer processes those images, we would like our neuron to adjust its weights and its bias so that we have fewer and fewer images wrongly recognized.</p>
<p class="normal">This approach seems<a id="_idIndexMarker024"/> very intuitive, but it requires that a small change in the weights (or the bias) causes only a small change in the outputs. Think about it: if we have a big output jump, we cannot learn <em class="italic">progressively</em>. After all, kids learn little by little. Unfortunately, the perceptron does not show this “little-by-little” behavior. A perceptron is either a 0 or 1, and that’s a big jump that will not help in learning (see <em class="italic">Figure 1.4</em>):</p>
<figure class="mediaobject"> <img alt="" height="401" src="../Images/B18331_01_04.png" width="569"/></figure>
<p class="packt_figref">Figure 1.4: Example of a perceptron – either a 0 or 1</p>
<p class="normal">We need something different, something smoother. We need a function that progressively changes from 0 to 1 with no discontinuity. Mathematically, this means that we need a continuous function that allows us to compute the derivative. You might remember that in mathematics the derivative is the amount by which a function is changing at a given point. For functions with input given by real numbers, the derivative is the slope of the tangent line at a point on a graph. Later in this chapter we will see why derivatives are important for learning, when we will talk about gradient descent. </p>
<h2 class="heading-2" id="_idParaDest-22">Activation function: sigmoid</h2>
<p class="normal">The sigmoid<a id="_idIndexMarker025"/> function, defined as <img alt="" height="71" src="../Images/B18331_01_004.png" style="height: 1.77em !important; vertical-align: -0.60em !important;" width="217"/> and represented<a id="_idIndexMarker026"/> in the image below, has small output changes in the range (0, 1) when the input varies in the range <img alt="" height="46" src="../Images/B18331_01_005.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="142"/>. Mathematically the function is continuous. A typical sigmoid function is represented in <em class="italic">Figure 1.5</em>:</p>
<figure class="mediaobject"><img alt="" height="181" src="../Images/B18331_01_05.png" width="650"/></figure>
<p class="packt_figref">Figure 1.5: A sigmoid function with output in the range (0,1)</p>
<p class="normal">A neuron can use the sigmoid for computing the nonlinear function <img alt="" height="46" src="../Images/B18331_01_006.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="250"/>. Note that if <em class="italic">z</em> = <em class="italic">wx</em> + <em class="italic">b</em> is very large and positive, then <img alt="" height="42" src="../Images/B18331_01_007.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="133"/> so <img alt="" height="50" src="../Images/B18331_01_008.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="154"/>, while if <em class="italic">z</em> = <em class="italic">wx</em> + <em class="italic">b</em> is very large and negative, then <img alt="" height="42" src="../Images/B18331_01_009.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="146"/> so <img alt="" height="50" src="../Images/B18331_01_010.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="154"/>. In other words, a neuron with sigmoid activation has a behavior similar to the perceptron, but the changes are gradual and output values such as 0.5539 or 0.123191 are perfectly legitimate. In this sense a sigmoid neuron can answer “maybe.”</p>
<h2 class="heading-2" id="_idParaDest-23">Activation function: tanh</h2>
<p class="normal">Another useful activation<a id="_idIndexMarker027"/> function is<a id="_idIndexMarker028"/> tanh. It is defined as <img alt="" height="75" src="../Images/B18331_01_011.png" style="height: 1.88em !important; vertical-align: -0.60em !important;" width="292"/> whose shape is shown in <em class="italic">Figure 1.6</em>. Its outputs range from -1 to 1:</p>
<figure class="mediaobject"><img alt="" height="240" src="../Images/B18331_01_06.png" width="706"/></figure>
<p class="packt_figref">Figure 1.6: Tanh activation function</p>
<h2 class="heading-2" id="_idParaDest-24">Activation function: ReLU</h2>
<p class="normal">The “sigmoid” is not the only kind<a id="_idIndexMarker029"/> of smooth activation function<a id="_idIndexMarker030"/> used for neural networks. Recently, a very simple function named <strong class="keyWord">ReLU</strong> (<strong class="keyWord">REctified Linear Unit</strong>) became very popular because it helps address some problems of optimizations observed with sigmoids. We will discuss these problems in more detail when we talk about vanishing gradient in <em class="chapterRef">Chapter 5</em>, <em class="italic">Recurrent Neural Networks</em>. </p>
<p class="normal">A ReLU is simply defined as <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">max</em>(0, <em class="italic">x</em>) and the nonlinear function is represented in <em class="italic">Figure 1.7</em>. As we can see, the function is zero for negative values and it grows linearly for positive values. The ReLU is also very simple to implement (generally three instructions are enough), while the sigmoid is a few orders of magnitude more. This helps to squeeze the neural networks onto an early GPU:</p>
<figure class="mediaobject"><img alt="" height="480" src="../Images/B18331_01_07.png" width="481"/></figure>
<p class="packt_figref">Figure 1.7: A ReLU function</p>
<h2 class="heading-2" id="_idParaDest-25">Two additional activation functions: ELU and Leaky ReLU</h2>
<p class="normal">Sigmoid and ReLU are not the only activation<a id="_idIndexMarker031"/> functions used for learning.</p>
<p class="normal"><strong class="keyWord">Exponential Linear Unit</strong> (<strong class="keyWord">ELU</strong>) is defined as <img alt="" height="88" src="../Images/B18331_01_012.png" style="height: 2.20em !important; vertical-align: -0.75em !important;" width="525"/> for <img alt="" height="42" src="../Images/B18331_01_013.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="100"/> and<a id="_idIndexMarker032"/> its plot is represented in <em class="italic">Figure 1.8</em>:</p>
<figure class="mediaobject"> <img alt="" height="301" src="../Images/B18331_01_08.png" width="476"/></figure>
<p class="packt_figref">Figure 1.8: An ELU function</p>
<p class="normal">LeakyReLU is<a id="_idIndexMarker033"/> defined<a id="_idIndexMarker034"/> as <img alt="" height="83" src="../Images/B18331_01_014.png" style="height: 2.08em !important; vertical-align: -0.75em !important;" width="413"/> for <img alt="" height="42" src="../Images/B18331_01_013.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="100"/> and its plot is represented in <em class="italic">Figure 1.9</em>:</p>
<figure class="mediaobject"><img alt="" height="371" src="../Images/B18331_01_09.png" width="374"/></figure>
<p class="packt_figref">Figure 1.9: A LeakyReLU function</p>
<p class="normal">Both the functions allow small updates if <em class="italic">x</em> is negative, which might be useful in certain conditions.</p>
<h2 class="heading-2" id="_idParaDest-26">Activation functions</h2>
<p class="normal">Sigmoid, Tanh, ELU, Leaky ReLU, and ReLU are<a id="_idIndexMarker035"/> generally called <em class="italic">activation functions</em> in neural network jargon. In the gradient<a id="_idIndexMarker036"/> descent section, we will see that those gradual changes typical of sigmoid and ReLU functions are the basic building blocks to developing a learning algorithm that adapts little by little, by progressively reducing the mistakes made by our nets. An example of using the activation function <img alt="" height="42" src="../Images/B18331_01_016.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/> with the (<em class="italic">x</em><sub class="subscript">1</sub>, <em class="italic">x</em><sub class="subscript">2</sub>,..., <em class="italic">x</em><sub class="italic">m</sub>) input vector, the (<em class="italic">w</em><sub class="subscript">1</sub>, <em class="italic">w</em><sub class="subscript">2</sub>,..., <em class="italic">w</em><sub class="italic">m</sub>) weight vector, the <em class="italic">b</em> bias, and the <img alt="" height="42" src="../Images/B18331_01_017.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> summation is given in <em class="italic">Figure 1.10</em> (note that TensorFlow supports many activation functions, a full list of which is available online):</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="376" src="../Images/Image5194.png" width="620"/></figure>
<p class="packt_figref">Figure 1.10: An example of an activation function applied after a linear function </p>
<h2 class="heading-2" id="_idParaDest-27">In short: what are neural networks after all?</h2>
<p class="normal">In one sentence, machine learning<a id="_idIndexMarker037"/> models are a way to compute a function that maps some inputs to their corresponding outputs. The function is nothing more than a number of addition and multiplication operations. However, when combined with a nonlinear activation and stacked in multiple layers, these functions can learn almost anything [8]. We also need a meaningful metric capturing what we want to optimize (this being the so-called loss function that we will cover later in the book), enough data to learn from, and sufficient computational power.</p>
<p class="normal">Now, it might be beneficial to stop<a id="_idIndexMarker038"/> one moment and ask ourselves what “learning” really is? Well, we can say for our purposes that learning is essentially a process aimed at generalizing established observations [9] to predict future results. So, in short, this is exactly the goal we want to achieve with neural networks.</p>
<h1 class="heading-1" id="_idParaDest-28">A real example: recognizing handwritten digits</h1>
<p class="normal">In this section we will build a network<a id="_idIndexMarker039"/> that can recognize handwritten numbers. To achieve this goal, we use MNIST (<a href="http://yann.lecun.com/exdb/mnist/"><span class="url">http://yann.lecun.com/exdb/mnist/</span></a>), a database of handwritten digits made up of a training set of 60,000 examples, and a test set of 10,000 examples. The training examples are annotated by humans with the correct answer. For instance, if the handwritten digit is the number “3,” then 3 is simply the label associated with that example.</p>
<p class="normal">In machine learning, when a dataset with correct<a id="_idIndexMarker040"/> answers is available, we say that we can perform a form of <em class="italic">supervised learning</em>. In this case we can use training examples for improving our net. Testing examples also have the correct answer associated with each digit. In this case, however, the idea is to pretend that the label is unknown, let the network do the prediction, and then later on reconsider the label to evaluate how well our neural network has learned to recognize digits. Unsurprisingly, testing examples are just used to test the performance of our net.</p>
<p class="normal">Each MNIST image is in grayscale and consists of 28 x 28 pixels. A subset of these images of numbers is shown in <em class="italic">Figure 1.11</em>:</p>
<figure class="mediaobject"> <img alt="mnist.png" height="153" src="../Images/B18331_01_11.png" width="201"/></figure>
<p class="packt_figref">Figure 1.11: A collection of MNIST images</p>
<h2 class="heading-2" id="_idParaDest-29">One hot-encoding (OHE)</h2>
<p class="normal">We will use OHE<a id="_idIndexMarker041"/> as a simple tool to encode information used inside<a id="_idIndexMarker042"/> neural networks. In many applications, it is convenient to transform categorical (non-numerical) features into numerical variables. For instance, the categorical feature <em class="italic">digit</em> with value <em class="italic">d</em> in [0–9] can be encoded into a binary vector with 10 positions, which has always a 0 value except the <em class="italic">d - th</em> position where a 1 is present. For example, the digit 3 can be encoded as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. </p>
<p class="normal">This type of representation is called <strong class="keyWord">One-Hot-Encoding</strong> (<strong class="keyWord">OHE</strong>) or sometimes simply one-hot, and is very common in data mining when the learning algorithm is specialized in dealing with numerical functions.</p>
<h2 class="heading-2" id="_idParaDest-30">Defining a simple neural net in TensorFlow</h2>
<p class="normal">In this section we use TensorFlow<a id="_idIndexMarker043"/> to define a network that recognizes<a id="_idIndexMarker044"/> MNIST handwritten digits. We start with a very simple neural network and then progressively improve it.</p>
<p class="normal">Following Keras’ style, TensorFlow<a id="_idIndexMarker045"/> provides suitable libraries (<a href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/datasets</span></a>) for loading the dataset and splits it into training sets, <code class="inlineCode">X_train</code><em class="italic">,</em> used for fine-tuning our net, and test sets, <code class="inlineCode">X_test</code><em class="italic">,</em> used for assessing the performance. Later in the chapter, we are going to formally define what a training set, a validation set, and a test set are. For now, we just need to know that a training set is the dataset used to let our neural network learn from data examples. Data is converted into <code class="inlineCode">float32</code> to use 32-bit precision when training a neural network and normalized to the range [0,1]. In addition, we load the true labels into <code class="inlineCode">Y_train</code> and <code class="inlineCode">Y_test</code> respectively, and perform one-hot encoding on them. Let’s see the code.</p>
<p class="normal">For now, do not focus too much on understanding why certain parameters have specific assigned values, as these choices will be discussed throughout the rest of the book. Intuitively, an epoch defines how long the training should last, <code class="inlineCode">BATCH_SIZE</code> is the number of samples you feed in your network at a time, and the validation sample is the amount of data reserved for checking or proving the validity of the training process. The reason why we picked <code class="inlineCode">EPOCHS = 200</code>, <code class="inlineCode">BATCH_SIZE = 128</code>, <code class="inlineCode">VALIDATION_SPLIT=0.2</code>, and <code class="inlineCode">N_HIDDEN = 128</code> will be clearer later in this chapter when we will explore different values and discuss hyperparameters optimization. Let’s see our first code fragment of a neural network<a id="_idIndexMarker046"/> in TensorFlow. Reading is intuitive<a id="_idIndexMarker047"/> but you will find a detailed explanation in the upcoming pages:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-comment"># Network and training parameters.</span>
EPOCHS = <span class="hljs-number">200</span>
BATCH_SIZE = <span class="hljs-number">128</span>
VERBOSE = <span class="hljs-number">1</span>
NB_CLASSES = <span class="hljs-number">10</span>   <span class="hljs-comment"># number of outputs = number of digits</span>
N_HIDDEN = <span class="hljs-number">128</span>
VALIDATION_SPLIT = <span class="hljs-number">0.2</span> <span class="hljs-comment"># how much TRAIN is reserved for VALIDATION</span>
<span class="hljs-comment"># Loading MNIST dataset.</span>
<span class="hljs-comment"># verify</span>
<span class="hljs-comment"># You can verify that the split between train and test is 60,000, and 10,000 respectively. </span>
<span class="hljs-comment"># Labels have one-hot representation.is automatically applied</span>
mnist = keras.datasets.mnist
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
<span class="hljs-comment"># X_train is 60000 rows of 28x28 values; we  --&gt; reshape it to 60000 x 784.</span>
RESHAPED = <span class="hljs-number">784</span>
<span class="hljs-comment">#</span>
X_train = X_train.reshape(<span class="hljs-number">60000</span>, RESHAPED)
X_test = X_test.reshape(<span class="hljs-number">10000</span>, RESHAPED)
X_train = X_train.astype(<span class="hljs-string">'float32'</span>)
X_test = X_test.astype(<span class="hljs-string">'float32'</span>)
<span class="hljs-comment"># Normalize inputs to be within in [0, 1].</span>
X_train /= <span class="hljs-number">255</span>
X_test /= <span class="hljs-number">255</span>
<span class="hljs-built_in">print</span>(X_train.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'train samples'</span>)
<span class="hljs-built_in">print</span>(X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'test samples'</span>)
<span class="hljs-comment"># One-hot representation of the labels.</span>
Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)
Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)
</code></pre>
<p class="normal">You can see from the above code that the input layer has a neuron associated to each pixel in the image for a total of 28 x 28=784 neurons, one for each pixel in the MNIST images.</p>
<p class="normal">Typically, the values<a id="_idIndexMarker048"/> associated with each pixel are normalized<a id="_idIndexMarker049"/> in the range [0,1] (which means that the intensity of each pixel is divided by 255, the maximum intensity value). The output can be one of ten classes, with one class for each digit.</p>
<p class="normal">The final layer is a single neuron with the activation function <code class="inlineCode">'</code><code class="inlineCode">softmax'</code>, which is a generalization of the sigmoid function. As discussed earlier, a sigmoid function output is in the range (0, 1) when the input varies in the range <img alt="" height="46" src="../Images/B18331_01_005.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="142"/>. Similarly, a softmax “squashes” a K-dimensional vector of arbitrary real values into a K-dimensional vector of real values in the range (0, 1), so that they all add up to 1. In our case, it aggregates ten answers provided by the previous layer with ten neurons. What we have just described is implemented with the following code:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Build the model.</span>
model = tf.keras.models.Sequential()
model.add(keras.layers.Dense(NB_CLASSES,
            input_shape=(RESHAPED,),
            name=<span class="hljs-string">'dense_layer'</span>, 
            activation=<span class="hljs-string">'softmax'</span>))
</code></pre>
<p class="normal">Once we define the model, we have to compile it so that it can be executed by TensorFlow. There are a few choices to be made during compilation. Firstly, we need to select an <em class="italic">optimizer</em>, which is the specific <a id="_idIndexMarker050"/>algorithm used to update weights while we train our model. </p>
<p class="normal">A complete<a id="_idIndexMarker051"/> list of optimizers is at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers</span></a>. Second, we need to select an <em class="italic">objective function</em>,<em class="italic"> </em>which is used by the optimizer<a id="_idIndexMarker052"/> to navigate the space of weights (frequently objective functions<a id="_idIndexMarker053"/> are called either <em class="italic">loss functions </em>or <em class="italic">cost functions</em> and the process of optimization<a id="_idIndexMarker054"/> is defined<a id="_idIndexMarker055"/> as a process of loss <em class="italic">minimization</em>). Third, we need to evaluate the trained model. </p>
<p class="normal">Some common choices for objective functions (a complete list<a id="_idIndexMarker056"/> of loss functions is at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/losses</span></a>) are: </p>
<ul>
<li class="bulletList"><code class="inlineCode">mse</code>, which defines the mean squared error<a id="_idIndexMarker057"/> between the predictions and the true values. Mathematically if <em class="italic">d</em> is a vector of predictions and <em class="italic">y</em> is the vector of <em class="italic">n</em> observed values, then <img alt="" height="71" src="../Images/B18331_01_019.png" style="height: 1.77em !important; vertical-align: -0.60em !important;" width="375"/>. Note that this objective function is the average of all the mistakes made in each prediction. If a prediction is far off from the true value, then this distance is made more evident by the squaring operation. In addition, the square can add up the error regardless of whether a given value is positive or negative.</li>
<li class="bulletList"><code class="inlineCode">binary_crossentropy</code>, which defines the binary logarithmic<a id="_idIndexMarker058"/> loss. Suppose that our model predicts <em class="italic">p</em> while the target is <em class="italic">c</em>, then the binary cross-entropy is defined as <img alt="" height="50" src="../Images/B18331_01_020.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="638"/>. Note that this objective function is suitable for binary labels prediction.</li>
<li class="bulletList"><code class="inlineCode">categorical_crossentropy</code>, which defines the multiclass<a id="_idIndexMarker059"/> logarithmic loss. Categorical cross-entropy compares the distribution of the predictions with the true distribution, with the probability of the true class set to 1 and 0 for the other classes. If the true class is <em class="italic">c</em> and the prediction is <em class="italic">y</em>, then the categorical cross-entropy is defined as:
    <p class="center"><img alt="" height="108" src="../Images/B18331_01_021.png" style="height: 2.70em !important;" width="392"/></p>
<p class="normal">One way to think about multi-class logarithm loss is to consider the true class represented as a one-hot encoded vector, and the closer the model’s outputs are to that vector, the lower the loss. Note that this objective function is suitable for multi-class label predictions. It is also the default choice with softmax activation. A complete<a id="_idIndexMarker060"/> list of loss functions is at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/losses</span></a>.</p>
</li>
</ul>
<p class="normal">Some common choices<a id="_idIndexMarker061"/> for metrics (a complete list of metrics<a id="_idIndexMarker062"/> is at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/metrics</span></a>) are:</p>
<ul>
<li class="bulletList">Accuracy, defined as the proportion<a id="_idIndexMarker063"/> of correct predictions with respect to the total number of predictions.</li>
<li class="bulletList">Precision, defined as the proportion<a id="_idIndexMarker064"/> of correct positive predictions with respect to the number of correct and incorrect positive predictions.</li>
<li class="bulletList">Recall, defined as the proportion<a id="_idIndexMarker065"/> of correct positive predictions with respect to the actual number of positive predictions.</li>
</ul>
<p class="normal">A complete<a id="_idIndexMarker066"/> list of metrics is at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/metrics</span></a>. Metrics are similar to objective<a id="_idIndexMarker067"/> functions, with the only difference being that they are not used for training a model, only for evaluating the model. However, it is important to understand the difference between metrics and objective functions. As discussed, the loss function is used to optimize your network. This is the function minimized by the selected optimizer. Instead, a metric is used to judge the performance of your network. This is only for you to run an evaluation, and it should be separated from the optimization process. On some occasions, it would be ideal to directly optimize for a specific metric. However, some metrics are not differentiable with respect to their inputs, which precludes them from being used directly.</p>
<p class="normal">When compiling a model<a id="_idIndexMarker068"/> in TensorFlow, it is possible to select the optimizer, the loss function, and the metric used together with a given model:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Compiling the model.</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'SGD'</span>, 
              loss=<span class="hljs-string">'categorical_crossentropy'</span>,
              metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
<p class="normal"><strong class="keyWord">Stochastic Gradient Descent</strong> (<strong class="keyWord">SGD</strong>) is a particular kind of optimization<a id="_idIndexMarker069"/> algorithm used to reduce the mistakes made by neural networks after each training epoch. We will review SGD and other optimization algorithms in the next chapters. Once the model is compiled, it can then be trained with the <code class="inlineCode">fit()</code> method, which specifies a few parameters:</p>
<ul>
<li class="bulletList"><code class="inlineCode">epochs</code> is the number of times the model is exposed to the training set. At each iteration the optimizer tries to adjust the weights so that the objective function is minimized.</li>
<li class="bulletList"><code class="inlineCode">batch_size</code> is the number of training instances observed before the optimizer performs a weight update; there are usually many batches per epoch.</li>
</ul>
<p class="normal">Training a model in TensorFlow is very simple:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Training the model.</span>
model.fit(X_train, Y_train,
          batch_size=BATCH_SIZE, epochs=EPOCHS,
          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)
</code></pre>
<p class="normal">Note that we’ve reserved part of the training set for validation. The key idea is that we reserve a part of the training data for measuring the performance on the validation while training. This is a good practice to follow for any machine learning task, and one that we will adopt in all of our examples. Please note that we will return to validation later in this chapter when we will talk about overfitting.</p>
<p class="normal">Once the model is trained, we can evaluate it on the test set that contains new examples never seen by the model during the training phase. </p>
<p class="normal">Note that, of course, the training<a id="_idIndexMarker070"/> set and the test set are rigorously separated. There is no point in evaluating a model on an example that was already used for training. In TF we can use the method <code class="inlineCode">evaluate(X_test, Y_test)</code> to compute the <code class="inlineCode">test_loss</code> and the <code class="inlineCode">test_acc</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#evaluate the model</span>
test_loss, test_acc = model.evaluate(X_test, Y_test)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Test accuracy:'</span>, test_acc)
</code></pre>
<p class="normal">Congratulations! You have just defined your first neural network in TensorFlow. A few lines of code and your computer should be able to recognize handwritten numbers. Let’s run the code and see what the performance is.</p>
<h2 class="heading-2" id="_idParaDest-31">Running a simple TensorFlow net and establishing a baseline</h2>
<p class="normal">So, let’s see<a id="_idIndexMarker071"/> what happens when we run<a id="_idIndexMarker072"/> the code: </p>
<pre class="programlisting con"><code class="hljs-con">Model: "sequential"
_________________________________________________________________
Layer (type)                Output Shape              Param #    
=================================================================
dense_layer (Dense)         (None, 10)                7850       
                                                                 
=================================================================
Total params: 7,850
Trainable params: 7,850
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/200
48000/48000 [==============================] - 1s 31us/sample - loss: 2.1276 - accuracy: 0.2322 - val_loss: 1.9508 - val_accuracy: 0.3908
Epoch 2/200
48000/48000 [==============================] - 1s 23us/sample - loss: 1.8251 - accuracy: 0.5141 - val_loss: 1.6848 - val_accuracy: 0.6277
Epoch 3/200
48000/48000 [==============================] - 1s 25us/sample - loss: 1.5992 - accuracy: 0.6531 - val_loss: 1.4838 - val_accuracy: 0.7150
Epoch 4/200
48000/48000 [==============================] - 1s 27us/sample - loss: 1.4281 - accuracy: 0.7115 - val_loss: 1.3304 - val_accuracy: 0.7551
Epoch 5/200
</code></pre>
<p class="normal">First the net architecture<a id="_idIndexMarker073"/> is dumped and we can see the different types of layers used, their output shape, how many<a id="_idIndexMarker074"/> parameters (i.e., how many weights) they need to optimize, and how they are connected. Then, the network is trained on 48K samples, and 12K are reserved for validation. Once the neural model is built, it is then tested on 10K samples. For now we won’t go into the internals of how the training happens, but we can see that the program runs for 200 iterations and each time accuracy improves. When the training ends, we test our model on the test set and we achieve about 89.96% accuracy on the training dataset, 90.70% on validation, and 90.71% on test: </p>
<pre class="programlisting con"><code class="hljs-con">Epoch 199/200
48000/48000 [==============================] - 1s 22us/sample - loss: 0.3684 - accuracy: 0.8995 - val_loss: 0.3464 - val_accuracy: 0.9071
Epoch 200/200
48000/48000 [==============================] - 1s 23us/sample - loss: 0.3680 - accuracy: 0.8996 - val_loss: 0.3461 - val_accuracy: 0.9070
10000/10000 [==============================] - 1s 54us/sample - loss: 0.3465 - accuracy: 0.9071
Test accuracy: 0.9071
</code></pre>
<p class="normal">This means that nearly 1 in 10 images are incorrectly classified. We can certainly do better than that.</p>
<h2 class="heading-2" id="_idParaDest-32">Improving the simple net in TensorFlow with hidden layers</h2>
<p class="normal">Okay, we have a baseline<a id="_idIndexMarker075"/> of accuracy of 89.96% on the training<a id="_idIndexMarker076"/> dataset, 90.70% on validation, and 90.71% on test. It is a good starting point, but we can improve it. Let’s see how.</p>
<p class="normal">An initial improvement is to add additional layers to our network because these additional neurons might intuitively help to learn more complex patterns in the training data. In other words, additional layers add more parameters, potentially allowing a model to memorize more complex patterns. So, after the input layer, we have a first dense layer with <code class="inlineCode">N_HIDDEN</code> neurons and an activation function <code class="inlineCode">'relu'</code>. This additional layer is considered <em class="italic">hidden</em> because it is not directly connected either with the input or with the output. After the first hidden layer we have a second hidden layer, again with <code class="inlineCode">N_HIDDEN</code> neurons, followed by an output layer with ten neurons, each one of which will fire when the relative digit is recognized. The following code defines this new network:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-comment"># Network and training.</span>
EPOCHS = <span class="hljs-number">50</span>
BATCH_SIZE = <span class="hljs-number">128</span>
VERBOSE = <span class="hljs-number">1</span>
NB_CLASSES = <span class="hljs-number">10</span>   <span class="hljs-comment"># number of outputs = number of digits</span>
N_HIDDEN = <span class="hljs-number">128</span>
VALIDATION_SPLIT = <span class="hljs-number">0.2</span> <span class="hljs-comment"># how much TRAIN is reserved for VALIDATION</span>
<span class="hljs-comment"># Loading MNIST dataset.</span>
<span class="hljs-comment"># Labels have one-hot representation.</span>
mnist = keras.datasets.mnist
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
<span class="hljs-comment"># X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.</span>
RESHAPED = <span class="hljs-number">784</span>
<span class="hljs-comment">#</span>
X_train = X_train.reshape(<span class="hljs-number">60000</span>, RESHAPED)
X_test = X_test.reshape(<span class="hljs-number">10000</span>, RESHAPED)
X_train = X_train.astype(<span class="hljs-string">'float32'</span>)
X_test = X_test.astype(<span class="hljs-string">'float32'</span>)
<span class="hljs-comment"># Normalize inputs to be within in [0, 1].</span>
X_train, X_test = X_train / <span class="hljs-number">255.0</span>, X_test / <span class="hljs-number">255.0</span>
<span class="hljs-built_in">print</span>(X_train.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'train samples'</span>)
<span class="hljs-built_in">print</span>(X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'test samples'</span>)
<span class="hljs-comment"># Labels have one-hot representation.</span>
Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)
Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)
<span class="hljs-comment"># Build the model.</span>
model = tf.keras.models.Sequential()
model.add(keras.layers.Dense(N_HIDDEN,
             input_shape=(RESHAPED,),
             name=<span class="hljs-string">'dense_layer'</span>, activation=<span class="hljs-string">'relu'</span>))
model.add(keras.layers.Dense(N_HIDDEN,
             name=<span class="hljs-string">'dense_layer_2'</span>, activation=<span class="hljs-string">'relu'</span>))
model.add(keras.layers.Dense(NB_CLASSES,
             name=<span class="hljs-string">'dense_layer_3'</span>, activation=<span class="hljs-string">'softmax'</span>))
<span class="hljs-comment"># Summary of the model.</span>
model.summary()
<span class="hljs-comment"># Compiling the model.</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'</span><span class="hljs-string">SGD'</span>, 
              loss=<span class="hljs-string">'categorical_crossentropy'</span>,
              metrics=[<span class="hljs-string">'accuracy'</span>])
<span class="hljs-comment"># Training the model.</span>
model.fit(X_train, Y_train,
          batch_size=BATCH_SIZE, epochs=EPOCHS,
          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)
<span class="hljs-comment"># Evaluating the model.</span>
test_loss, test_acc = model.evaluate(X_test, Y_test)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Test accuracy:'</span>, test_acc)
</code></pre>
<p class="normal">Note that <code class="inlineCode">to_categorical(Y_train, NB_CLASSES)</code> converts the array <code class="inlineCode">Y_train</code> into a matrix with as many<a id="_idIndexMarker077"/> columns as there are classes. The number<a id="_idIndexMarker078"/> of rows stays the same. So, for instance, if we have:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt; </span>labels
array([0, 2, 1, 2, 0])
</code></pre>
<p class="normal">then:</p>
<pre class="programlisting con"><code class="hljs-con">to_categorical(labels)
array([[ 1.,  0.,  0.],
       [ 0.,  0.,  1.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.],
       [ 1.,  0.,  0.]], dtype=float32)
</code></pre>
<p class="normal">Let’s run the code and see what results we get with this multi-layer network: </p>
<pre class="programlisting con"><code class="hljs-con">_________________________________________________________________
Layer (type)                Output Shape              Param #    
=================================================================
dense_layer (Dense)         (None, 128)               100480     
                                                                 
dense_layer_2 (Dense)       (None, 128)               16512      
                                                                 
dense_layer_3 (Dense)       (None, 10)                1290       
                                                                 
=================================================================
Total params: 118,282
Trainable params: 118,282
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/50
48000/48000 [==============================] - 3s 63us/sample - loss: 2.2507 - accuracy: 0.2086 - val_loss: 2.1592 - val_accuracy: 0.3266
</code></pre>
<p class="normal">The previous output shows<a id="_idIndexMarker079"/> the initial steps of the run while the following output shows the conclusion. Not bad. As seen in the following output, by adding two hidden layers we reached 90.81% on the training dataset, 91.40% on validation, and 91.18% on test. This means that we have increased accuracy on the test dataset with respect to the previous network, and we have reduced the number of iterations from 200 to 50. That’s good, but we want more.</p>
<p class="normal">If you want, you can play by yourself and see what happens if you add only one hidden layer instead of two or if you add more than two layers. I leave this experiment as an exercise:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 49/50
48000/48000 [==============================] - 1s 30us/sample - loss: 0.3347 - accuracy: 0.9075 - val_loss: 0.3126 - val_accuracy: 0.9136
Epoch 50/50
48000/48000 [==============================] - 1s 28us/sample - loss: 0.3326 - accuracy: 0.9081 - val_loss: 0.3107 - val_accuracy: 0.9140
10000/10000 [==============================] - 0s 40us/sample - loss: 0.3164 - accuracy: 0.9118
Test accuracy: 0.9118
</code></pre>
<p class="normal">Note that improvement<a id="_idIndexMarker080"/> stops (or it become almost imperceptible) after<a id="_idIndexMarker081"/> a certain number<a id="_idIndexMarker082"/> of epochs. In machine learning this is a phenomenon called <em class="italic">convergence</em>.</p>
<h2 class="heading-2" id="_idParaDest-33">Further improving the simple net in TensorFlow with dropout</h2>
<p class="normal">Now our baseline is 90.81% on training<a id="_idIndexMarker083"/> set, 91.40% on validation, and 91.18% on test. A second improvement<a id="_idIndexMarker084"/> is very simple. We decide to randomly drop – with the <code class="inlineCode">DROPOUT</code> probability – some of the values propagated inside our internal dense network of hidden layers during training. In machine learning this is a well-known form of regularization. Surprisingly enough, this idea of randomly dropping a few values can improve our performance. The idea behind this improvement is that random dropouts <em class="italic">force</em> the network to learn redundant patterns that are useful for better generalization:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-comment"># Network and training.</span>
EPOCHS = <span class="hljs-number">200</span>
BATCH_SIZE = <span class="hljs-number">128</span>
VERBOSE = <span class="hljs-number">1</span>
NB_CLASSES = <span class="hljs-number">10</span>   <span class="hljs-comment"># number of outputs = number of digits</span>
N_HIDDEN = <span class="hljs-number">128</span>
VALIDATION_SPLIT = <span class="hljs-number">0.2</span> <span class="hljs-comment"># how much TRAIN is reserved for VALIDATION</span>
DROPOUT = <span class="hljs-number">0.3</span>
<span class="hljs-comment"># Loading MNIST dataset.</span>
<span class="hljs-comment"># Labels have one-hot representation.</span>
mnist = keras.datasets.mnist
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
<span class="hljs-comment"># X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.</span>
RESHAPED = <span class="hljs-number">784</span>
<span class="hljs-comment">#</span>
X_train = X_train.reshape(<span class="hljs-number">60000</span>, RESHAPED)
X_test = X_test.reshape(<span class="hljs-number">10000</span>, RESHAPED)
X_train = X_train.astype(<span class="hljs-string">'float32'</span>)
X_test = X_test.astype(<span class="hljs-string">'float32'</span>)
<span class="hljs-comment"># Normalize inputs within [0, 1].</span>
X_train, X_test = X_train / <span class="hljs-number">255.0</span>, X_test / <span class="hljs-number">255.0</span>
<span class="hljs-built_in">print</span>(X_train.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'train samples'</span>)
<span class="hljs-built_in">print</span>(X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-string">'test samples'</span>)
<span class="hljs-comment"># One-hot representations for labels.</span>
Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)
Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)
<span class="hljs-comment"># Building the model.</span>
model = tf.keras.models.Sequential()
model.add(keras.layers.Dense(N_HIDDEN,
              input_shape=(RESHAPED,),
              name=<span class="hljs-string">'dense_layer'</span>, activation=<span class="hljs-string">'relu'</span>))
model.add(keras.layers.Dropout(DROPOUT))
model.add(keras.layers.Dense(N_HIDDEN,
              name=<span class="hljs-string">'dense_layer_2'</span>, activation=<span class="hljs-string">'relu'</span>))
model.add(keras.layers.Dropout(DROPOUT))
model.add(keras.layers.Dense(NB_CLASSES,
              name=<span class="hljs-string">'dense_layer_3'</span>, activation=<span class="hljs-string">'softmax'</span>))
<span class="hljs-comment"># Summary of the model.</span>
model.summary()
<span class="hljs-comment"># Compiling the model.</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'SGD'</span>, 
              loss=<span class="hljs-string">'categorical_crossentropy'</span>,
              metrics=[<span class="hljs-string">'accuracy'</span>])
<span class="hljs-comment"># Training the model.</span>
model.fit(X_train, Y_train,
          batch_size=BATCH_SIZE, epochs=EPOCHS,
          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)
<span class="hljs-comment"># Evaluating the model.</span>
test_loss, test_acc = model.evaluate(X_test, Y_test)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Test accuracy:'</span>, test_acc)
</code></pre>
<p class="normal">Let’s run the code for 200 iterations<a id="_idIndexMarker085"/> as before and we see that this<a id="_idIndexMarker086"/> net achieves an accuracy of 91.70% on training, 94.42% on validation, and 94.15% on testing:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 199/200
48000/48000 [==============================] - 2s 45us/sample - loss: 0.2850 - accuracy: 0.9177 - val_loss: 0.1922 - val_accuracy: 0.9442
Epoch 200/200
48000/48000 [==============================] - 2s 42us/sample - loss: 0.2845 - accuracy: 0.9170 - val_loss: 0.1917 - val_accuracy: 0.9442
10000/10000 [==============================] - 1s 61us/sample - loss: 0.1927 - accuracy: 0.9415
Test accuracy: 0.9415
</code></pre>
<p class="normal">Note that it has been frequently observed that networks with random dropouts in internal hidden layers can “generalize” better on unseen examples contained in test sets. Intuitively we can consider this phenomenon as each neuron becoming more capable because it knows it cannot depend on its neighbors. Also, it forces information to be stored in a redundant way. During testing there is no dropout, so we are now using all our highly tuned neurons. In short, it is generally a good approach to test how a net performs when some dropout function is adopted.</p>
<p class="normal">Besides that, note that training accuracy should still be above test accuracy; otherwise, we might be not training for long enough. This is the case in our example and therefore we should increase the number of epochs. However, before performing this attempt we need to introduce a few other concepts<a id="_idIndexMarker087"/> that allow the training<a id="_idIndexMarker088"/> to converge faster. Let’s talk about optimizers.</p>
<h2 class="heading-2" id="_idParaDest-34">Testing different optimizers in TensorFlow</h2>
<p class="normal">Now that we have defined<a id="_idIndexMarker089"/> and used a network, it is useful<a id="_idIndexMarker090"/> to start developing some intuition about how networks are trained, using an analogy. Let us focus<a id="_idIndexMarker091"/> on one popular training technique known as <strong class="keyWord">gradient descent</strong> (<strong class="keyWord">GD</strong>). Imagine a generic cost function <em class="italic">C</em>(<em class="italic">w</em>) in one single variable <em class="italic">w</em> like in <em class="italic">Figure 1.12</em>:</p>
<figure class="mediaobject"><img alt="" height="420" src="../Images/B18331_01_12.png" width="407"/></figure>
<p class="packt_figref">Figure 1.12: An example of GD optimization</p>
<p class="normal">GD can be seen as a hiker<a id="_idIndexMarker092"/> who needs to navigate down a steep slope and aims to enter a ditch. The slope represents the function <em class="italic">C</em> while the ditch represents the minimum <em class="italic">C</em><sub class="italic">min</sub>. The hiker has a starting point <em class="italic">w</em><sub class="subscript">0</sub>. The hiker moves little by little; imagine that there is almost zero visibility, so the hiker cannot see where to go automatically, and they proceed in a zigzag. At each step <em class="italic">r</em>, the gradient is the direction of maximum increase.</p>
<p class="normal">Mathematically this direction is the value of the partial derivative <img alt="" height="71" src="../Images/B18331_01_022.png" style="height: 1.77em !important; vertical-align: -0.55em !important;" width="46"/> evaluated at point <em class="italic">w</em><sub class="italic">r</sub>, reached at step <em class="italic">r</em>. Therefore, by taking the opposite direction <img alt="" height="71" src="../Images/B18331_01_023.png" style="height: 1.77em !important; vertical-align: -0.55em !important;" width="158"/> the hiker can move toward the ditch.</p>
<p class="normal">At each step the hiker can decide how big a stride to take<a id="_idIndexMarker093"/> before the next stop. This is the so-called “learning rate” <img alt="" height="46" src="../Images/B18331_01_024.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="113"/> in GD jargon. Note that if <img alt="" height="46" src="../Images/B18331_01_025.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> is too small, then the hiker will move slowly. However, if <img alt="" height="46" src="../Images/B18331_01_025.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> is too high, then the hiker will possibly miss the ditch by stepping over it.</p>
<p class="normal">Now you should remember that a sigmoid is a continuous function and it is possible to compute the derivative. It can be proven that the sigmoid <img alt="" height="71" src="../Images/B18331_01_027.png" style="height: 1.77em !important; vertical-align: -0.60em !important;" width="217"/> has the derivative <img alt="" height="75" src="../Images/B18331_01_028.png" style="height: 1.88em !important; vertical-align: -0.70em !important;" width="392"/>.</p>
<p class="normal">ReLU is not differentiable at 0. We can however extend the first derivative at 0 to a function over the whole domain by defining it to be either a 0 or 1. </p>
<p class="normal">The piecewise derivative of ReLU <img alt="" height="50" src="../Images/B18331_01_029.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="246"/> is <img alt="" height="83" src="../Images/B18331_01_030.png" style="height: 2.08em !important; vertical-align: -0.69em !important;" width="258"/>. </p>
<p class="normal">Once we have the derivative, it is possible to optimize the nets with a GD technique. TensorFlow computes the derivative on our behalf so we don’t need to worry about implementing or computing it.</p>
<p class="normal">A neural network<a id="_idIndexMarker094"/> is essentially a composition<a id="_idIndexMarker095"/> of multiple derivable functions with thousands and sometimes millions of parameters. Each network layer computes a function, the error of which should be minimized in order to improve the accuracy observed during the learning phase. When we discuss backpropagation, we will discover that the minimization game is a bit more complex than our toy example. However, it is still based on the same intuition of descending a slope to reach a ditch.</p>
<p class="normal">TensorFlow implements a fast variant of GD known as <strong class="keyWord">Stochastic Gradient Descent</strong> (<strong class="keyWord">SGD</strong>) and many more advanced optimization<a id="_idIndexMarker096"/> techniques such as RMSProp and Adam. RMSProp and Adam include the concept of momentum (a velocity component) in addition to the acceleration component that SGD has. This allows faster convergence at the cost of more computation. Think about a hiker who starts to move in one direction and then decides to change direction but remembers previous choices. It can be proven that momentum helps accelerate SGD in the relevant direction and dampens oscillations [10]. </p>
<p class="normal">SGD was our default <a id="_idIndexMarker097"/>choice so far. So now let’s try <a id="_idIndexMarker098"/>the other two. It is very simple; we just need to change a few lines:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Compiling the model.</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'RMSProp'</span>, 
              loss=<span class="hljs-string">'categorical_crossentropy'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
<p class="normal">That’s it. Let’s test it. </p>
<pre class="programlisting con"><code class="hljs-con">_________________________________________________________________
Layer (type)                Output Shape              Param #    
=================================================================
dense_layer (Dense)         (None, 128)               100480     
                                                                 
dropout_2 (Dropout)         (None, 128)               0          
                                                                 
dense_layer_2 (Dense)       (None, 128)               16512      
                                                                 
dropout_3 (Dropout)         (None, 128)               0          
                                                                 
dense_layer_3 (Dense)       (None, 10)                1290       
                                                                 
=================================================================
Total params: 118,282
Trainable params: 118,282
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
48000/48000 [==============================] - 2s 48us/sample - loss: 0.4715 - accuracy: 0.8575 - val_loss: 0.1820 - val_accuracy: 0.9471
Epoch 2/10
48000/48000 [==============================] - 2s 36us/sample - loss: 0.2215 - accuracy: 0.9341 - val_loss: 0.1268 - val_accuracy: 0.9361
Epoch 3/10
48000/48000 [==============================] - 2s 39us/sample - loss: 0.1684 - accuracy: 0.9497 - val_loss: 0.1198 - val_accuracy: 0.9651
Epoch 4/10
48000/48000 [==============================] - 2s 43us/sample - loss: 0.1459 - accuracy: 0.9569 - val_loss: 0.1059 - val_accuracy: 0.9710
Epoch 5/10
48000/48000 [==============================] - 2s 39us/sample - loss: 0.1273 - accuracy: 0.9623 - val_loss: 0.1059 - val_accuracy: 0.9696
Epoch 6/10
48000/48000 [==============================] - 2s 36us/sample - loss: 0.1177 - accuracy: 0.9659 - val_loss: 0.0941 - val_accuracy: 0.9731
Epoch 7/10
48000/48000 [==============================] - 2s 35us/sample - loss: 0.1083 - accuracy: 0.9671 - val_loss: 0.1009 - val_accuracy: 0.9715
Epoch 8/10
48000/48000 [==============================] - 2s 35us/sample - loss: 0.0971 - accuracy: 0.9706 - val_loss: 0.0950 - val_accuracy: 0.9758
Epoch 9/10
48000/48000 [==============================] - 2s 35us/sample - loss: 0.0969 - accuracy: 0.9718 - val_loss: 0.0985 - val_accuracy: 0.9745
Epoch 10/10
48000/48000 [==============================] - 2s 35us/sample - loss: 0.0873 - accuracy: 0.9743 - val_loss: 0.0966 - val_accuracy: 0.9762
10000/10000 [==============================] - 1s 2ms/sample - loss: 0.0922 - accuracy: 0.9764
Test accuracy: 0.9764
</code></pre>
<p class="normal">As you can see, RMSProp is faster<a id="_idIndexMarker099"/> than SDG since we are able<a id="_idIndexMarker100"/> to achieve in only 10 epochs an accuracy of 97.43% on the training dataset, 97.62% on validation, and 97.64% on test. That’s a significant improvement on SDG. Now that we have a very fast optimizer, let us try to increase significantly the number of epochs up to 250, and we get 98.99% accuracy on the training dataset, 97.66% on validation, and 97.77% on test:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 248/250
48000/48000 [==============================] - 2s 40us/sample - loss: 0.0506 - accuracy: 0.9904 - val_loss: 0.3465 - val_accuracy: 0.9762
Epoch 249/250
48000/48000 [==============================] - 2s 40us/sample - loss: 0.0490 - accuracy: 0.9905 - val_loss: 0.3645 - val_accuracy: 0.9765
Epoch 250/250
48000/48000 [==============================] - 2s 39us/sample - loss: 0.0547 - accuracy: 0.9899 - val_loss: 0.3353 - val_accuracy: 0.9766
10000/10000 [==============================] - 1s 58us/sample - loss: 0.3184 - accuracy: 0.9779
Test accuracy: 0.9779
</code></pre>
<p class="normal">It is useful to observe<a id="_idIndexMarker101"/> how accuracy increases on training<a id="_idIndexMarker102"/> and test sets when the number of epochs increases (see <em class="italic">Figure 1.13</em>). As you can see, these two curves touch at about 15 epochs and therefore there is no need to train further after that point:</p>
<figure class="mediaobject"><img alt="" height="738" src="../Images/B18331_01_13.png" width="580"/></figure>
<p class="packt_figref">Figure 1.13: An example of accuracy and loss with RMSProp</p>
<p class="normal">Okay, let’s try<a id="_idIndexMarker103"/> the other optimizer, <code class="inlineCode">Adam()</code>. It’s pretty<a id="_idIndexMarker104"/> simple to implement:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Compiling the model.</span>
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'Adam'</span>, 
              loss=<span class="hljs-string">'categorical_crossentropy'</span>,
              metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
<p class="normal">As we see, <code class="inlineCode">Adam()</code> is slightly better. With Adam we achieve 98.94% accuracy on the training dataset, 97.89% on validation, and 97.82% on test with 50 iterations:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 49/50
48000/48000 [==============================] - 3s 55us/sample - loss: 0.0313 - accuracy: 0.9894 - val_loss: 0.0868 - val_accuracy: 0.9808
Epoch 50/50
48000/48000 [==============================] - 2s 51s/sample - loss: 0.0321 - accuracy: 0.9894 - val_loss: 0.0983 - val_accuracy: 0.9789
10000/10000 [==============================] - 1s 66us/step - loss: 0.0964 - accuracy: 0.9782
Test accuracy: 0.9782
</code></pre>
<p class="normal">One more time, let’s plot how accuracy<a id="_idIndexMarker105"/> increases on training and test sets<a id="_idIndexMarker106"/> when the number of epochs increases (see <em class="italic">Figure 1.14</em>). You’ll notice that by choosing Adam as an optimizer we are able to stop after just about 12 epochs or steps:</p>
<figure class="mediaobject"><img alt="" height="726" src="../Images/B18331_01_14.png" width="564"/></figure>
<p class="packt_figref">Figure 1.14: An example of accuracy and loss with Adam</p>
<p class="normal">Note that this is our fifth<a id="_idIndexMarker107"/> variant and remember that our initial baseline<a id="_idIndexMarker108"/> was at 90.71% on the test dataset. So far, we’ve made progressive improvements. However, gains are now more and more difficult to obtain. Note that we are optimizing with a dropout of 30%. For the sake of completeness, it could be useful to report the accuracy of the test dataset for different dropout values (see <em class="italic">Figure 1.15</em>). In this example, we selected Adam as the optimizer. Note that the choice of optimizer isn’t a rule of thumb and we can get different performance depending on the problem-optimizer combination:</p>
<figure class="mediaobject"><img alt="Chart" height="313" src="../Images/B18331_01_15.png" width="507"/></figure>
<p class="packt_figref">Figure 1.15: An example of changes in accuracy for different dropout values</p>
<h2 class="heading-2" id="_idParaDest-35">Increasing the number of epochs</h2>
<p class="normal">Let’s make another attempt<a id="_idIndexMarker109"/> and increase the number of epochs used for training from 20 to 200. Unfortunately, this choice increases our computation time tenfold, yet gives us no gain. The experiment is unsuccessful, but we have learned that if we spend more time learning, we will not necessarily improve the result. Learning is more about adopting smart techniques and not necessarily about the time spent in computations. Let’s keep track of our five variants in the following graph:</p>
<figure class="mediaobject"><img alt="Chart" height="403" src="../Images/B18331_01_16.png" width="670"/></figure>
<p class="packt_figref">Figure 1.16: Accuracy for different models and optimizers</p>
<h2 class="heading-2" id="_idParaDest-36">Controlling the optimizer learning rate</h2>
<p class="normal">There is another approach<a id="_idIndexMarker110"/> we can take that involves changing<a id="_idIndexMarker111"/> the learning parameter for our optimizer. As you can see in <em class="italic">Figure 1.17</em>, the best value reached by our three experiments [lr=0.1, lr=0.01, and lr=0.001] is 0.1, which is the default learning rate for the optimizer. Good! Adam works well out of the box:</p>
<figure class="mediaobject"><img alt="Chart" height="383" src="../Images/B18331_01_17.png" width="621"/></figure>
<p class="packt_figref">Figure 1.17: Accuracy for different learning rates</p>
<h2 class="heading-2" id="_idParaDest-37">Increasing the number of internal hidden neurons</h2>
<p class="normal">Yet another approach involves changing<a id="_idIndexMarker112"/> the number of internal hidden neurons. We report the results of the experiments with an increasing number of hidden neurons. We see that by increasing the complexity of the model, the runtime increases significantly because there are more and more parameters to optimize. However, the gains that we are getting by increasing the size of the network decrease more and more as the network grows (see <em class="italic">Figure 1.18</em>, <em class="italic">Figure 1.19</em>, and <em class="italic">Figure 1.20</em>): </p>
<figure class="mediaobject"><img alt="Chart" height="406" src="../Images/B18331_01_18.png" width="625"/></figure>
<p class="packt_figref">Figure 1.18: Number of parameters for the increasing values of internal hidden neurons</p>
<p class="normal">On the other hand, the time needed<a id="_idIndexMarker113"/> increases as the size of the internal network increases (see <em class="italic">Figure 1.19</em>):</p>
<figure class="mediaobject"><img alt="Chart" height="388" src="../Images/B18331_01_19.png" width="634"/></figure>
<p class="packt_figref">Figure 1.19: Seconds of computation time for the increasing values of internal hidden neurons</p>
<p class="normal">Note that increasing<a id="_idIndexMarker114"/> the number of hidden neurons after a certain value can reduce the accuracy because the network might not be able to generalize well (as shown in <em class="italic">Figure 1.20</em>):</p>
<figure class="mediaobject"><img alt="Chart" height="409" src="../Images/B18331_01_20.png" width="667"/></figure>
<p class="packt_figref">Figure 1.20: Test accuracy for the increasing values of internal hidden neurons</p>
<h2 class="heading-2" id="_idParaDest-38">Increasing the size of batch computation</h2>
<p class="normal">GD tries to minimize the cost function<a id="_idIndexMarker115"/> on all the examples provided in the training sets and, at the same time, for all the features provided as input. SGD is a much less expensive variant that considers only <code class="inlineCode">BATCH_SIZE</code> examples. So, let us see how it behaves when we change this parameter. As you can see, the best accuracy value is reached for a <code class="inlineCode">BATCH_SIZE=64</code> in our four experiments (see <em class="italic">Figure 1.21</em>):</p>
<figure class="mediaobject"><img alt="Chart" height="429" src="../Images/B18331_01_21.png" width="695"/></figure>
<p class="packt_figref">Figure 1.21: Test accuracy for different batch values</p>
<h2 class="heading-2" id="_idParaDest-39">Summarizing experiments run to recognizing handwritten digits</h2>
<p class="normal">So, let’s summarize: with five different<a id="_idIndexMarker116"/> variants, we were able to improve our performance from 90.71% to 97.82%. First, we defined a simple layer network in TensorFlow. Then, we improved the performance by adding some hidden layers. After that, we improved the performance on the test set by adding a few random dropouts in our network, and then by experimenting with different types of optimizers:</p>
<table class="table-container" id="table001">
<tbody>
<tr>
<td class="table-cell"/>
<td class="table-cell" colspan="3">
<p class="normal"><strong class="keyWord">Accuracy</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Model</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Training</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Validation</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Test</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Simple</strong></p>
</td>
<td class="table-cell">
<p class="normal">89.96%</p>
</td>
<td class="table-cell">
<p class="normal">90.70%</p>
</td>
<td class="table-cell">
<p class="normal">90.71%</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Two hidden layers (128)</strong></p>
</td>
<td class="table-cell">
<p class="normal">90.81%</p>
</td>
<td class="table-cell">
<p class="normal">91.40%</p>
</td>
<td class="table-cell">
<p class="normal">91.18%</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Dropout (30%)</strong></p>
</td>
<td class="table-cell">
<p class="normal">91.70%</p>
</td>
<td class="table-cell">
<p class="normal">94.42%</p>
</td>
<td class="table-cell">
<p class="normal">94.15% (200 epochs)</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">RMSProp</strong></p>
</td>
<td class="table-cell">
<p class="normal">97.43%</p>
</td>
<td class="table-cell">
<p class="normal">97.62%</p>
</td>
<td class="table-cell">
<p class="normal">97.64% (10 epochs)</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Adam</strong></p>
</td>
<td class="table-cell">
<p class="normal">98.94%</p>
</td>
<td class="table-cell">
<p class="normal">97.89%</p>
</td>
<td class="table-cell">
<p class="normal">97.82% (10 epochs)</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 1.1: Summary of experiments with various levels of accuracy</p>
<p class="normal">However, the next two experiments (not shown in <em class="italic">Table 1.1</em>) were not providing significant improvements. Increasing <a id="_idIndexMarker117"/>the number of internal neurons creates more complex models and requires more expensive computations, but it provides only marginal gains. We have the same experience if we increase the number of training epochs. A final experiment consisted of changing the <code class="inlineCode">BATCH_SIZE</code> for our optimizer. This also provided marginal results.</p>
<h1 class="heading-1" id="_idParaDest-40">Regularization</h1>
<p class="normal">In this section we will review a few best practices for improving the training phase. In particular, regularization and batch normalization will be discussed.</p>
<h2 class="heading-2" id="_idParaDest-41">Adopting regularization to avoid overfitting</h2>
<p class="normal">Intuitively, a good machine learning<a id="_idIndexMarker118"/> model should achieve low error on training data. Mathematically this is equivalent to minimizing the loss function on the training data given the model:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_01_031.png" style="height: 1.25em !important;" width="625"/></p>
<p class="normal">However, this might not be enough. A model can become excessively complex in order to capture all the relations inherently expressed by the training data. This increase in complexity might have two negative consequences. First, a complex model might require a significant amount of time to be executed. Second, a complex model might achieve very good performance on training data but perform quite badly on validation data. This is because the model is able to contrive relationships between many parameters in the specific training context, but these relationships in fact do not exist within a more generalized context. Causing a model to lose its ability<a id="_idIndexMarker119"/> to generalize in this manner is termed “overfitting. “ Again, learning is more about generalization than memorization. Another <a id="_idIndexMarker120"/>phenomenon to consider is “underfitting.” </p>
<p class="normal">This happens<a id="_idIndexMarker121"/> when a data model cannot capture the relationship between the input and output variables accurately, with a high error rate on both the training set and new unseen data:</p>
<figure class="mediaobject"><img alt="" height="401" src="../Images/B18331_01_22.png" width="474"/></figure>
<p class="packt_figref">Figure 1.22: Loss function and overfitting</p>
<p class="normal">As a rule of thumb, if during the training we see that the loss increases on validation, after an initial decrease, then we have a problem of model complexity that overfits the training data.</p>
<p class="normal">In order to solve the overfitting problem, we need a way to capture the complexity of a model, i.e. how complex a model can be. What could the solution be? Well, a model is nothing more than a vector of weights. Each weight affects the output, except for those which are zero, or very close to it. Therefore, the complexity of a model can be conveniently represented as the number of non-zero weights. In other words, if we have two models M1 and M2 achieving pretty much the same performance in terms of a loss function, then we should choose the simplest model, the one which has the minimum number of non-zero weights. We can use a hyperparameter <img alt="" height="42" src="../Images/B18331_01_032.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="121"/> for controlling the importance<a id="_idIndexMarker122"/> of having a simple model, as in this formula:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_01_033.png" style="height: 1.25em !important;" width="1063"/></p>
<p class="normal">There are three different types of regularization<a id="_idIndexMarker123"/> used in machine learning:</p>
<ul>
<li class="bulletList">L1 regularization (also known as LASSO). The complexity<a id="_idIndexMarker124"/> of the model is expressed<a id="_idIndexMarker125"/> as the sum of the absolute values of the weights.</li>
<li class="bulletList">L2 regularization (also known as Ridge). The complexity<a id="_idIndexMarker126"/> of the model<a id="_idIndexMarker127"/> is expressed as the sum of the squares of the weights.</li>
<li class="bulletList">ElasticNet regularization. The complexity of the<a id="_idIndexMarker128"/> model is captured by a combination of the two techniques above. </li>
</ul>
<p class="normal">Note that playing with regularization can be a good way to increase the generalization performance of a network, particularly when there is an evident situation of overfitting. This set of experiments is left as an exercise to the interested reader.</p>
<p class="normal">Also note that TensorFlow supports L1, L2, and ElasticNet regularization. A complete list of regularizers<a id="_idIndexMarker129"/> is at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/regularizers"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/regularizers</span></a>. Adding regularization is easy: </p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tf.keras.regularizers <span class="hljs-keyword">import</span> l2, activity_l2
model.add(Dense(<span class="hljs-number">64</span>, input_dim=<span class="hljs-number">64</span>, W_regularizer=l2(<span class="hljs-number">0.01</span>),
    activity_regularizer=activity_l2(<span class="hljs-number">0.01</span>)))
</code></pre>
<h2 class="heading-2" id="_idParaDest-42">Understanding batch normalization</h2>
<p class="normal">Batch normalization is another<a id="_idIndexMarker130"/> form of regularization and one of the most effective improvements<a id="_idIndexMarker131"/> proposed during the last few years. Batch normalization enables us to accelerate training, in some cases by halving the training epochs, and it offers some regularization. During training, the weights in early layers naturally change and therefore the inputs of later layers can significantly change. In other words, each layer must continuously re-adjust its weights to the different distribution for every batch. This may slow down the model’s training greatly. The key idea is to make layer inputs more similar in distribution, batch after batch and epoch after epoch.</p>
<p class="normal">Another issue is that the sigmoid activation function works very well close to zero but tends to “get stuck” when values get sufficiently far away from zero. If, occasionally, neuron outputs fluctuate far away from the sigmoid zero, then said neuron becomes unable to update its own weights. </p>
<p class="normal">The other key idea is therefore to transform the layer outputs into a Gaussian distribution unit close to zero. This way, layers will have significantly less variation from batch to batch. Mathematically, the formula is very simple. The activation input x is centered around zero by subtracting the batch mean <img alt="" height="46" src="../Images/B18331_01_034.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="25"/> from it. Then the result is divided by <img alt="" height="42" src="../Images/B18331_01_035.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="88"/>, the sum of batch variance <img alt="" height="42" src="../Images/B18331_01_016.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/>, and a small number <img alt="" height="42" src="../Images/B18331_01_037.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> to prevent division by zero. Then, we use a linear transformation <img alt="" height="50" src="../Images/B18331_01_038.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="221"/> to make sure that the normalizing effect is applied during training. </p>
<p class="normal">In this way, <img alt="" height="42" src="../Images/B18331_01_039.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> and <img alt="" height="46" src="../Images/B18331_01_040.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> are parameters<a id="_idIndexMarker132"/> that get optimized during the training phase in a way<a id="_idIndexMarker133"/> similar to any other layer. Batch normalization has been proven to be a very effective way to increase both the speed of training and accuracy, because it helps to prevent activations becoming either too small and vanishing or too big and exploding.</p>
<h1 class="heading-1" id="_idParaDest-43">Playing with Google Colab: CPUs, GPUs, and TPUs</h1>
<p class="normal">Google offers a truly intuitive<a id="_idIndexMarker134"/> tool for training neural networks and for playing with TensorFlow<a id="_idIndexMarker135"/> at no cost. You can find an actual Colab, which can be freely accessed, at <a href="https://colab.research.google.com/"><span class="url">https://colab.research.google.com/</span></a> and if you are familiar with Jupyter notebooks you will find <a id="_idIndexMarker136"/>a very familiar web-based environment here. <strong class="keyWord">Colab</strong> stands for <strong class="keyWord">Colaboratory</strong> and is a Google research project created to help disseminate machine learning education and research. We will see the difference between CPUs, GPUs, and TPUs in <em class="chapterRef">Chapter 15</em>, <em class="italic">Tensor Processing Unit</em>. </p>
<p class="normal">For now, it’s important to know that CPUs are generic processing units, while GPUs and TPUs are accelerators, specific processing units suitable for deep learning. Let’s see how it works, starting with the screenshot shown in <em class="italic">Figure 1.23</em>:</p>
<figure class="mediaobject"><img alt="" height="431" src="../Images/B18331_01_23.png" width="878"/></figure>
<p class="packt_figref">Figure 1.23: An example of notebooks in Colab</p>
<p class="normal">By accessing Colab, we can<a id="_idIndexMarker137"/> either check a listing of notebooks generated in the past or we can create a new notebook. Different versions of Python are supported.</p>
<p class="normal">When we create a new notebook, we can also select if we want to run it on CPUs, GPUs, or in Google’s TPUs as shown in <em class="italic">Figure 1.24</em>: </p>
<figure class="mediaobject"><img alt="" height="430" src="../Images/B18331_01_24.png" width="879"/></figure>
<p class="packt_figref">Figure 1.24: Selecting the desired hardware accelerator (None, GPUs, or TPUs) – the first step</p>
<p class="normal">By accessing the <strong class="screenText">Notebook settings</strong> option contained in the <strong class="screenText">Edit</strong> menu (see <em class="italic">Figure 1.24</em> and <em class="italic">Figure 1.25</em>), we can select<a id="_idIndexMarker138"/> the desired hardware accelerator (<strong class="screenText">None</strong>, <strong class="screenText">GPUs</strong>, or <strong class="screenText">TPUs</strong>). Google will allocate the resources at no cost, although they can be withdrawn at any time, for example during periods of a particularly heavy load. In my experience, this is a very rare event, and you can access Colab pretty much any time. However, be polite and do not do something like start mining bitcoins at no cost – you will almost certainly get evicted!</p>
<figure class="mediaobject"><img alt="" height="265" src="../Images/B18331_01_25.png" width="438"/></figure>
<p class="packt_figref">Figure 1.25: Selecting the desired hardware accelerator (None, GPUs, or TPUs) – the second step</p>
<p class="normal">The next step is to insert<a id="_idIndexMarker139"/> your code (see <em class="italic">Figure 1.26</em>) in the appropriate Colab notebook cells and <em class="italic">voila!</em> You are good to go. Execute the code and happy deep learning without the hassle of buying very expensive hardware to start your experiments! <em class="italic">Figure 1.26</em> contains an example of code in a Google notebook:</p>
<figure class="mediaobject"><img alt="" height="528" src="../Images/B18331_01_26.png" width="677"/></figure>
<p class="packt_figref">Figure 1.26: An example of code in a notebook</p>
<h1 class="heading-1" id="_idParaDest-44">Sentiment analysis</h1>
<p class="normal">What is the code we used to test Colab? It is an example<a id="_idIndexMarker140"/> of sentiment analysis developed on top of the IMDB dataset. The IMDB dataset contains the text of 50,000 movie reviews from the Internet Movie Database. Each review is either positive or negative (for example, thumbs up or thumbs down). The dataset is split into 25,000 reviews for training and 25,000 reviews for testing. Our goal is to build a classifier that can predict the binary judgment given the text. We can easily load IMDB via <code class="inlineCode">tf.keras</code> and the sequences of words in the reviews have been converted to sequences of integers, where each integer represents a specific word in a dictionary. We also have a convenient way of padding sentences to <code class="inlineCode">max_len</code>, so that we can use all sentences, whether short or long, as inputs to a neural network with an input vector of fixed size: </p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models, preprocessing
<span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
max_len = <span class="hljs-number">200</span>
n_words = <span class="hljs-number">10000</span>
dim_embedding = <span class="hljs-number">256</span>
EPOCHS = <span class="hljs-number">20</span>
BATCH_SIZE = <span class="hljs-number">500</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span>():
    <span class="hljs-comment"># Load data.</span>
    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)
    <span class="hljs-comment"># Pad sequences with max_len.</span>
    X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)
    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)
    <span class="hljs-keyword">return</span> (X_train, y_train), (X_test, y_test)
</code></pre>
<p class="normal">Now let’s build a model. We are going<a id="_idIndexMarker141"/> to use a few layers that will be explained in detail in <em class="chapterRef">Chapter 4</em>, <em class="italic">Word Embeddings</em>. For now, let’s assume that the <code class="inlineCode">embedding()</code> layer will map the sparse space of words contained in the reviews into a denser space. This will make computation easier. In addition, we will use a <code class="inlineCode">GlobalMaxPooling1D()</code> layer, which takes the maximum value of either feature vector from each of the <code class="inlineCode">n_words</code> features. In addition, we have two <code class="inlineCode">Dense()</code> layers. The last one is made up of a single neuron with a sigmoid activation function for making the final binary estimation:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>():
    model = models.Sequential()
    <span class="hljs-comment"># Input: - eEmbedding Layer.</span>
    <span class="hljs-comment"># The model will take as input an integer matrix of size (batch, input_length).</span>
    <span class="hljs-comment"># The model will output dimension (input_length, dim_embedding).</span>
    <span class="hljs-comment"># The largest integer in the input should be no larger</span>
    <span class="hljs-comment"># than n_words (vocabulary size).</span>
    model.add(layers.Embedding(n_words, 
        dim_embedding, input_length=max_len))
    model.add(layers.Dropout(<span class="hljs-number">0.3</span>))
    <span class="hljs-comment"># Takes the maximum value of either feature vector from each of the n_words features.</span>
    model.add(layers.GlobalMaxPooling1D())
    model.add(layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'</span><span class="hljs-string">relu'</span>))
    model.add(layers.Dropout(<span class="hljs-number">0.5</span>))
    model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>))
    <span class="hljs-keyword">return</span> model
</code></pre>
<p class="normal">Now we need to train<a id="_idIndexMarker142"/> our model and this piece of code is very similar to what we have done with MNIST. Let’s see:</p>
<pre class="programlisting code"><code class="hljs-code">(X_train, y_train), (X_test, y_test) = load_data()
model = build_model()
model.summary()
model.<span class="hljs-built_in">compile</span>(optimizer = <span class="hljs-string">"adam"</span>, loss = <span class="hljs-string">"binary_crossentropy"</span>,
 metrics = [<span class="hljs-string">"accuracy"</span>]
)
score = model.fit(X_train, y_train,
 epochs = EPOCHS,
 batch_size = BATCH_SIZE,
 validation_data = (X_test, y_test)
)
score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nTest score:"</span>, score[<span class="hljs-number">0</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Test accuracy:'</span>, score[<span class="hljs-number">1</span>])
</code></pre>
<p class="normal">Let’s see the network and then run a few iterations:</p>
<pre class="programlisting con"><code class="hljs-con">___________________________________________________________________
Layer (type)                  Output Shape              Param #    
===================================================================
embedding (Embedding)         (None, 200, 256)          2560000    
                                                                   
dropout (Dropout)             (None, 200, 256)          0          
                                                                   
global_max_pooling1d (Global  (None, 256)               0          
                                                                   
dense (Dense)                 (None, 128)               32896      
                                                                   
dropout_1 (Dropout)           (None, 128)               0          
                                                                   
dense_1 (Dense)               (None, 1)                 129        
                                                                   
===================================================================
Total params: 2,593,025
Trainable params: 2,593,025
Non-trainable params: 0
</code></pre>
<p class="normal">As shown in the following output,<a id="_idIndexMarker143"/> we reach accuracy of 85%, which is not bad at all for a simple network:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 20/20
25000/25000 [==============================] - 23s 925ms/sample - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.4993 - val_accuracy: 0.8503
25000/25000 [==============================] - 2s 74us/sample - loss: 0.4993 - accuracy: 0.88503
Test score: 0.4992710727453232
Test accuracy: 0.85028
</code></pre>
<p class="normal">The next section is devoted to tuning hyperparameters and AutoML.</p>
<h2 class="heading-2" id="_idParaDest-45">Hyperparameter tuning and AutoML</h2>
<p class="normal">The experiments<a id="_idIndexMarker144"/> defined above give some opportunities for fine-tuning<a id="_idIndexMarker145"/> a net. However, what works<a id="_idIndexMarker146"/> for this example<a id="_idIndexMarker147"/> will not necessarily work for other examples. For a given neural network, there are indeed multiple parameters that can be optimized (such as the number of hidden neurons, batch size, number of epochs, and many more according to the complexity of the net itself). These<a id="_idIndexMarker148"/> parameters are called “hyperparameters” to distinguish them from the parameters of the network itself, i.e. the values of the weights and biases.</p>
<p class="normal">Hyperparameter tuning is the process of finding the optimal combination of those hyperparameters that minimize cost functions. The key idea is that if we have <em class="italic">n</em> hyperparameters, then we can imagine that they define a space with <em class="italic">n</em> dimensions, and the goal is to find the point in this space that corresponds to an optimal value for the cost function. One way to achieve this goal is to create a grid in this space and systematically check the value assumed by the cost function for each grid vertex. In other words, the hyperparameters are divided into buckets and different combinations of values are checked via a brute-force approach.</p>
<p class="normal">If you think that this process<a id="_idIndexMarker149"/> of fine-tuning the hyperparameters<a id="_idIndexMarker150"/> is manual and expensive then you are absolutely<a id="_idIndexMarker151"/> right! However, during the last few years, we have seen<a id="_idIndexMarker152"/> significant results in AutoML, a set of research techniques aimed at both automatically tuning hyperparameters and searching automatically for optimal network architecture. We will discuss more about this in <em class="chapterRef">Chapter 13</em>, <em class="italic">An Introduction to AutoML</em>. </p>
<h1 class="heading-1" id="_idParaDest-46">Predicting output</h1>
<p class="normal">Once a net is trained, it can of course<a id="_idIndexMarker153"/> be used for making predictions. In TensorFlow, this is very simple. We can use this method:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Making predictions.</span>
predictions = model.predict(X)
</code></pre>
<p class="normal">For a given input, several types of output can be computed including a method <code class="inlineCode">model.evaluate()</code> used to compute the loss values, a method <code class="inlineCode">model.predict_classes()</code> used to compute category outputs, and a method <code class="inlineCode">model.predict_proba()</code> used to compute class probabilities.</p>
<h1 class="heading-1" id="_idParaDest-47">A practical overview of backpropagation</h1>
<p class="normal">Multi-layer perceptrons learn<a id="_idIndexMarker154"/> from training data through a process called backpropagation. In this paragraph we will give an intuition while more details are in <em class="chapterRef">Chapter 14</em>, <em class="italic">The Math Behind Deep Learning</em>. The process can be described as a way of progressively correcting mistakes as soon as they are detected. Let’s see how this works.</p>
<p class="normal">Remember that each neural network layer has an associated set of weights that determine the output values for a given set of inputs. Additionally, remember that a neural network can have multiple hidden layers.</p>
<p class="normal">At the beginning, all the weights have some random assignment. Then the neural network is activated for each input in the training set: values are propagated <em class="italic">forward</em> from the input stage through the hidden stages to the output stage where a prediction is made. </p>
<p class="normal">Note that we keep <em class="italic">Figure 1.27</em> below simple by only representing a few values with green dotted lines but in reality, all the values are propagated forward through the network:</p>
<figure class="mediaobject"><img alt="" height="272" src="../Images/B18331_01_27.png" width="343"/></figure>
<p class="packt_figref">Figure 1.27: Forward step in backpropagation</p>
<p class="normal">Since we know the true observed value<a id="_idIndexMarker155"/> in the training set, it is possible to calculate the error made in the prediction. The key intuition for backtracking is to propagate the error back (see <em class="italic">Figure 1.28</em>), using an appropriate optimizer algorithm such as a GD to adjust the neural network weights with the goal of reducing the error (again for the sake of simplicity only a few error values are represented here):</p>
<figure class="mediaobject"><img alt="" height="262" src="../Images/B18331_01_28.png" width="343"/></figure>
<p class="packt_figref">Figure 1.28: Backward step in backpropagation</p>
<p class="normal">The process of forward propagation<a id="_idIndexMarker156"/> from input to output and backward propagation of errors is repeated several times until the error gets below a predefined threshold. The whole process is represented in <em class="italic">Figure 1.29</em>:</p>
<figure class="mediaobject"><img alt="" height="208" src="../Images/B18331_01_29.png" width="598"/></figure>
<p class="packt_figref">Figure 1.29: Forward propagation and backward propagation</p>
<p class="normal">The features represent the input, and the labels are used here to drive the learning process. The model is updated in such a way that the loss function is progressively minimized. In a neural network, what really matters is not the output of a single neuron but the collective weights adjusted in each layer. Therefore, the network progressively adjusts its internal weights in such a way<a id="_idIndexMarker157"/> that the prediction increases the number of correctly forecasted labels. Of course, using the right set of features and having quality labeled data is fundamental to minimizing the bias during the learning process.</p>
<h1 class="heading-1" id="_idParaDest-48">What have we learned so far?</h1>
<p class="normal">In this chapter, we have learned the basics of neural networks. More specifically, we have learned what a perceptron is and what a multi-layer perceptron is, how to define neural networks in TensorFlow, how to progressively improve metrics once a good baseline is established, and how to fine-tune the hyperparameter space. In addition to that, we also have a good idea of useful activation functions (sigmoid and ReLU) available, and how to train a network with backpropagation algorithms based on either GD, SGD, or more sophisticated approaches, such as Adam and RMSProp.</p>
<h1 class="heading-1" id="_idParaDest-49">Toward a deep learning approach</h1>
<p class="normal">While playing with handwritten<a id="_idIndexMarker158"/> digit recognition, we came to the conclusion that the closer we get to an accuracy of 99%, the more difficult it is to improve. If we want more improvement, we definitely need a new idea. What are we missing? Think about it.</p>
<p class="normal">The fundamental intuition is that in our examples so far, we are not making use of the local spatial structure of images, which means we will use the fact that an image can be described as a matrix with data locality. In particular, this piece of code transforms the bitmap representing each written digit into a flat vector where the local spatial structure (the fact that some pixels are closer to each other) is gone:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># X_train is 60000 rows of 28x28 values; we  --&gt; reshape it as in 60000 x 784.</span>
X_train = X_train.reshape(<span class="hljs-number">60000</span>, <span class="hljs-number">784</span>)
X_test = X_test.reshape(<span class="hljs-number">10000</span>, <span class="hljs-number">784</span>)
</code></pre>
<p class="normal">However, this is not how our brain works. Remember that our vision is based on multiple cortex levels, each one recognizing more and more structured information while still preserving the locality. First, we see single pixels, then from that, we recognize simple geometric forms and then more and more sophisticated elements such as objects, faces, human bodies, animals, and so on. </p>
<p class="normal">In <em class="chapterRef">Chapter 3</em>, we will see that a particular<a id="_idIndexMarker159"/> type of deep learning network known as the <strong class="keyWord">Convolutional Neural Network</strong> (<strong class="keyWord">CNN</strong>) has been developed by taking into account both the idea of preserving the local spatial structure in images (and more generally in any type of information that has a spatial structure) and the idea of learning via progressive levels of abstraction: with one layer you can only learn simple patterns; with more than one layer you can learn multiple patterns. Before discussing CNN, we need to discuss some aspects of TensorFlow architecture and have a practical introduction to a few additional machine learning concepts.</p>
<h1 class="heading-1" id="_idParaDest-50">Summary</h1>
<p class="normal">In this chapter we learned what TensorFlow and Keras are and introduced neural networks with the perceptron and the multi-layer perceptron. Then, we saw a real example of recognizing handwritten digits with several optimizations.</p>
<p class="normal">The next chapter is devoted to regression and classification.</p>
<h1 class="heading-1" id="_idParaDest-51">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Rosenblatt, F. (1958). <em class="italic">The perceptron: a probabilistic model for information storage and organization in the brain</em>.<em class="italic"> </em>Psychol. Rev, vol. 65, pp. 386–408.</li>
<li class="numberedList">Werbos, P. J. (1990). <em class="italic">Backpropagation through time: what it does and how to do it</em>. Proc. IEEE, vol. 78, pp. 1550–1560.</li>
<li class="numberedList">Hinton, G. E., Osindero, S., and Teh, Y. W. (2006). <em class="italic">A fast learning algorithm for deep belief nets</em>. Neural Comput, vol. 18, pp. 1527–1554.</li>
<li class="numberedList">Schmidhuber, J. (2015). <em class="italic">Deep learning in neural networks: an overview</em>.<em class="italic"> </em>Neural Networks : Off. J. Int. Neural Netw. Soc., vol. 61, pp. 85–117.</li>
<li class="numberedList">Leven, S. (1996). <em class="italic">The roots of backpropagation: From ordered derivatives to neural networks and political forecasting</em>.<em class="italic"> </em>Neural Networks, vol. 9.</li>
<li class="numberedList">Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). <em class="italic">Learning representations by back-propagating errors</em>.<em class="italic"> </em>Nature, vol. 323.</li>
<li class="numberedList">Herculano-Houzel, S. (2009). <em class="italic">The human brain in numbers: a linearly scaled-up primate brain</em>. Front. Hum. Neurosci., vol. 3.</li>
<li class="numberedList">Hornick, K., Stinchcombe, M., and White, H. (1989). <em class="italic">Multilayer feedforward networks are universal approximators</em>. Neural Networks Volume 2, Issue 5. Pages 359–366.</li>
<li class="numberedList">Vapnik, V. N. (2013). <em class="italic">The nature of statistical learning theory</em>.</li>
<li class="numberedList">Sutskever, I., Martens, J., Dahl, G., Hinton, G., (2013). <em class="italic">On the importance of initialization and momentum in deep learning</em>. 30th International Conference on Machine Learning, ICML.</li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>