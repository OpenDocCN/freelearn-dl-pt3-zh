<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Meta Learning</h1>
                </header>
            
            <article>
                
<p style="padding-left: 30px" class="mce-root">In <a href="66956576-0f67-49a6-9ba8-1a782baa6b24.xhtml">Chapter 9</a>, <em>Emerging Neural Network Designs</em>, we introduced new <strong>neural network</strong> (<strong>NN</strong>) architectures to tackle some of the limitations of existing <strong>deep learning</strong> (<strong>DL</strong>) algorithms. We discussed graph neural networks that are used to process structured data<span>, represented as graphs</span>. We also introduced memory augmented neural networks, which allow networks to use external memory. In this chapter, we'll look at how to improve DL algorithms by giving them the ability to learn more information using fewer training samples.</p>
<p style="padding-left: 30px" class="mce-root">Let's illustrate this problem with an example. Imagine that a person has never seen a certain type of object, say a car (I know—highly unlikely). They will only need to see a car once to be able to recognize other cars as well. But this is not the case with DL algorithms. A DNN needs a lot of training samples (and sometimes data augmentation as well), to be able to recognize a certain class of object. Even the relatively small CIFAR-10 (<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>) dataset contains 50,000 training images for only 10 classes of objects, the equivalent of 5,000 images per class.</p>
<p style="padding-left: 30px" class="mce-root">Meta learning, also referred to as learning to learn, allows <strong>machine learning</strong> (<strong>ML</strong>) algorithms to leverage and channel knowledge, gained over multiple training tasks, to improve its training efficiency over a new task. Hopefully, in this way, the algorithm will require fewer training samples to learn the new task. The ability to train with fewer samples has two advantages: reduced training time and good performance when there is not enough training data. <span>In that regard, the goals of meta learning are similar to the </span>transfer learning<span> mechanism that we introduced in <a href="d94e220f-820e-40da-8bb5-9593e0790b21.xhtml">Chapter 2</a>,<em> </em></span><em>Understanding Convolutional Networks</em><span>. In fact, we can think of transfer learning as a meta learning algorithm. But there are multiple approaches to meta learning. In this chapter, we'll discuss some of them.</span></p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Introduction to meta learning</li>
<li>Metric-based meta learning</li>
<li><span><span>Optimization-based meta learning</span></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to meta learning</h1>
                </header>
            
            <article>
                
<p><span>As we mentioned in the introduction, the goal of meta learning is to allow an ML algorithm (in our case, NN) to learn from relatively fewer training samples compared to standard supervised training. Some meta learning algorithms try to achieve this goal by finding a mapping between their existing knowledge of the domain of a well-known task to the domain of a new task. Other algorithms are simply designed from scratch to learn from fewer training samples. Yet another category of algorithms introduce new optimization training techniques, designed specifically with meta learning in mind. But before we discuss these topics, let's introduce some basic meta learning paradigms. In a standard ML supervised learning task, we aim to minimize the cost function </span><em>J(θ)</em> <span>across a training dataset</span> <em>D</em><span> by updating the model parameters </span><em>θ </em><span>(network weights, in the case of NNs). As we mentioned in the introduction, in meta learning we usually work with multiple datasets. Therefore, in a meta learning scenario, we can extend this definition by saying that we aim to minimize</span><span> </span><em>J(θ) </em><span>over a distribution of these datasets</span> <em>P(D)</em><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8fb8d948-a381-4a5c-a510-044386944a60.png" style="width:12.33em;height:1.67em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/7c90bf15-eefe-4035-9a88-d268ac605628.png" style="width:1.00em;height:1.00em;"/> is the optimal model parameters and <img class="fm-editor-equation" src="assets/031dc6dc-63e5-4e8e-a5da-207139d104da.png" style="width:2.42em;height:1.08em;"/> is the cost function, which now depends on the current dataset as well as the model parameters. In other words, the goal is to find model parameters <img class="fm-editor-equation" src="assets/43cc2369-d0da-4210-a0a1-3b6f68952bfc.png" style="width:1.00em;height:1.00em;"/> such that the expected value (as described in <a href="b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml">Chapter 1</a>, <em>The Nuts and Bolts of Neural Networks</em>, in the <em>Random variables and probability distributions</em> section) of the cost across all datasets <img class="fm-editor-equation" src="assets/8348ecdc-8752-4afb-8f4d-4607fa921b82.png" style="width:7.42em;height:1.42em;"/> is minimized. We can think of this scenario as training over a single dataset whose training samples are themselves datasets. </p>
<p>Next, let's continue by expanding the expression <em><span>fewer training samples</span></em> <span>that we used in the introduction. In supervised training, we can refer to this scenario of scarce training data as <strong><em>k</em>-shot learning</strong>, where <em>k</em> can be 0, 1, 2, and so on. Let's assume that our training dataset consists of labeled samples distributed among <em>n</em> classes. In <em>k</em>-shot learning, we have <em>k</em> labeled training samples for each of the <em>n</em> classes (the total number of labeled samples is <em>n × k</em>). We refer to this dataset as the <strong>support set</strong>, and we'll denote it with <em>S</em>. </span><span>We also have a <strong>query set</strong> <em>Q</em>, which contains unlabeled samples that belong to one of the <em>n</em> classes. Our goal is to correctly classify the samples of the query set. There are three types of <em>k</em>-shot learning: zero-shot, one-shot, and few-shot. Let's start with zero-shot learning.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Zero-shot learning</h1>
                </header>
            
            <article>
                
<p>We'll begin with zero-shot learning (<em>k</em> = 0), where we know that a particular class exists, but we don't have any labeled samples of that class (that is, there is no support set). At first, this sounds impossible—how can we classify something we have never seen before? But in meta learning, this is not exactly the case. Recall that we leverage knowledge of previously learned tasks (let's denote them with <em>a</em>) over the task at hand (<em>b</em>). In that regard, zero-shot learning is a form of transfer learning. To understand how this works, let's imagine that a person has never seen an elephant (another highly unlikely example), yet they have to recognize one when they see a picture of it (new task <em>b</em>). However, the person has read in a book that the elephant is large, gray, has four legs, large ears, and a trunk (previous task <em>a</em>). Given this description, they'll easily recognize an elephant when they see it. In this example, the person applied their knowledge from the domain of a previously learned task (reading a book) to the domain of the new task (image classification).</p>
<p>In the context of ML, these features can be encoded as nonhuman readable embedding vectors. <span>We can replicate the elephant recognition example in the NN realm by using language-modeling techniques, such as word2vec or transformers to encode a context-based embedding vector</span><span> of the word <em>elephant</em></span><span>. We can also use a convolutional network (CNN)</span> <span>to produce an embedding vector</span><span> </span><strong>h</strong><sub><em>b</em></sub><span> of an image of an elephant. Let's look at how to implement this step by step:</span></p>
<ol>
<li>Apply encoders <em>f</em> and g (NNs) over labeled and unlabeled samples <em>a</em> and <em>b </em>to produce embeddings <strong>h</strong><sub><em>a</em></sub> and <strong>h</strong><sub><em>b</em></sub> respectively.</li>
<li><span>Use a mapping function to transform <strong>h</strong><sub><em>b</em></sub> to the vector space of the embeddings <strong>h</strong><sub><em>a*</em></sub> of the known samples. The mapping function could be an NN as well. Furthermore, the encoders and the mapping could be combined in a single model and learned jointly. </span></li>
</ol>
<ol start="3">
<li>Once we have the transformed representation of the query sample, we can compare it to all representations <strong>h</strong><sub><em>a</em></sub>* using a similarity measure (for example, cosine similarity). We then assume that the query sample's class is the same as the class of the support sample most closely related to the query. The following diagram illustrates this scenario:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1140 image-border" src="assets/8e41022d-9fb4-492d-bb62-63c299a64fe8.png" style="width:48.75em;height:13.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Zero-shot learning is possible thanks to transfer learning. Inspired by Chapter 15 of http://www.deeplearningbook.org/</div>
<p>Let's formalize the zero-shot learning scenario. In a traditional classification task with a single dataset, the NN represents the conditional probability <img class="fm-editor-equation" src="assets/d7cc052e-9841-434d-923f-3b137dd1f47f.png" style="width:3.25em;height:1.17em;"/>, where <em>y</em> is the label of input sample <strong>x</strong> and <em>θ</em> is the model parameters. In meta learning, <strong>x</strong> and <em>y</em><span> </span>belong to the traditional dataset, but we introduce a random variable <em>T</em> that describes the new task we're interested in. <span>In our example, </span><strong>x</strong><span> would be the context (surrounding words) of the word <em>elephant</em>, and the label </span><em>y</em><span> is a one-hot encoding of the class elephant. On the other hand, <em>T</em> will be an image we're interested in;</span> therefore, the meta learning model represents a new conditional probability <img class="fm-editor-equation" src="assets/20ce23f4-8e8d-43a4-a4ff-aca3bbf6a943.png" style="width:4.50em;height:1.17em;"/>. The zero-shot scenario we just described is part of so-called metric-based meta learning (we'll see more examples of this later in the chapter). For now, let's move on to one-shot learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One-shot learning</h1>
                </header>
            
            <article>
                
<p>In this section, we'll be looking at <strong>one-shot learning</strong> (<em>k = 1</em>) and its generalization <strong>few-shot learning</strong> (<em>k &gt; 1</em>). In this case, the support set is not empty and we have one or more labeled samples of each class. This is an advantage over the zero-shot scenario because we can rely on labeled samples from the same domain instead of using a mapping from the labeled samples of another domain. Therefore, we have a single encoder <em>f</em> and no need for additional mapping.</p>
<p>An example of a one-shot learning task is a company's facial recognition system. This system should be able to recognize the identity of an employee based on a single photo. It should be possible to add new employees with a single <span>photo</span> as well. <span>Let's note that in this scenario, adding a new employee is equivalent to adding a new class that has already been seen (the photo itself), but which is otherwise unknown. This is in contrast to zero-shot learning, where we had unseen, but known classes.</span> A naive way to solve this task is with a classification <strong>feed-forward network</strong> (<strong>FFN</strong>), which takes the photo as input and ends with a softmax output, where each class represents one employee. This system will have two major disadvantages. First, every time we add a new employee, we have to retrain the whole model using the full dataset of employees. And second, we need multiple images per employee to train the model.</p>
<div class="packt_infobox"><span>The following description is based on the method introduced in </span><em>Matching Networks for One Shot Learning</em><span> (</span><a href="https://arxiv.org/abs/1606.04080">https://arxiv.org/abs/1606.04080</a><span>). The paper has two major contributions: a novel one-shot training procedure and a special network architecture. In this section, we'll discuss the training procedure and we'll describe the network architecture in the <em>Matching networks</em> section.</span></div>
<p>We can also solve this task within the one-shot learning framework. The first thing we'll need is a pretrained network that can produce embedding vectors of the employee images. We'll assume that the pretraining allows the network to produce a sufficiently unique embedding <strong>h</strong> for each photo. We'll also store all employee photos in some external database. For performance reasons, we can apply the network to all photos and then store the embedding of each image as well. Let's focus on the use case where the system has to identify an existing employee when he or she tries to authenticate with a new photo. We'll use the network to produce an embedding of that photo and then we'll compare it to the embeddings in the database. We'll identify the employee by taking the database embedding that most closely matches the embedding of the current photo.</p>
<p>Next, let's look at the use case when a new employee is added to the system. Here, we'll simply take a photo of that employee and store it in the database. In this way, every time the employee tries to authenticate, their current photo will be compared to the initial one (along with all other photos). In this way, we have added a new class (the employee) without any changes to the network. We can think of the employee photo/<span>identification</span> database as a support set <img class="fm-editor-equation" src="assets/3697813c-caa0-4554-9008-6da67efc3a5f.png" style="width:16.33em;height:1.17em;"/>. The goal of the task is to map this support set to a classifier <img class="fm-editor-equation" src="assets/a86cb797-f60e-4e0d-b95f-b79c0d2c5c18.png" style="width:2.42em;height:1.17em;"/>, which outputs a probability distribution over the labels <img class="fm-editor-equation" src="assets/c54b34bd-26a6-4570-aa0a-778c3befbdfe.png" style="width:0.58em;height:1.17em;"/>, given a previously unseen query sample <img class="fm-editor-equation" src="assets/2c4ff6d5-f273-40d9-a492-11c0bd853084.png" style="width:0.67em;height:1.00em;"/>. In our case, the <img class="fm-editor-equation" src="assets/654a154a-a2e7-4893-8094-7a99365c1c59.png" style="width:2.42em;height:1.17em;"/> pairs represent new employees (that is, new query samples and new classes) that were not part of the system before.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In other words, we want to be able to predict never before seen classes with the help of the existing support set. We'll define the mapping <img class="fm-editor-equation" src="assets/f3fa7d27-d701-4c0e-867e-2532b9c61574.png" style="width:4.08em;height:1.00em;"/> as a conditional probability <img class="fm-editor-equation" src="assets/a3e6dc4c-5998-418e-a878-f7af427e9d09.png" style="width:3.58em;height:0.92em;"/>, implemented by a neural network with weights <em>θ</em>. Additionally, we can also plug a new support set <img class="fm-editor-equation" src="assets/e3146769-fbf0-4f1a-9a2a-35757e5babac.png" style="width:0.92em;height:0.92em;"/> into the same network, which would lead to a new probability distribution <img class="fm-editor-equation" src="assets/ee987cf4-2602-45a3-88a7-8a6349fca99a.png" style="width:4.58em;height:1.17em;"/>. In this way, we can condition the outputs over the new training data without changing the network weights <em>θ</em>.</p>
<p>Now that we are familiar with <em>k</em>-shot learning, let's look at how to train an algorithm with few-shot datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Meta-training and meta-testing</h1>
                </header>
            
            <article>
                
<p>The scenarios we described in the <em>Zero-shot learning</em> and <em>One-shot learning</em> <span>sections </span><span>are referred to as</span> <strong>meta-testing</strong> <strong>phases</strong><span>. In this phase, we leverage the knowledge of a pretrained network and apply it to predict previously unseen labels with the help of only a small support set (or no support set at all). We also have a </span><strong>meta-training phase</strong><span>, where we train a network from scratch in a few-shot context. The authors of <em>Matching Networks for One Shot Learning</em> introduce a meta-training algorithm that closely matches the meta-testing. This is necessary so that we can train the model under the same conditions that we expect it to work in the testing phase. Since we train the network from scratch, the training set (denoted with</span> <strong><em>D</em></strong><span>) is not a few-shot dataset, and instead contains a sufficient number of labeled examples of each class. Nevertheless, the training process simulates a few-shot dataset.</span></p>
<p>Here's how it works:</p>
<ol>
<li>Sample a set of labels <img class="fm-editor-equation" src="assets/0f33b03f-3446-4a31-97fb-7857d111590b.png" style="width:2.67em;height:0.83em;"/>, where <em>T</em> is the set of all labels in <em>D</em>. To clarify, <em>L</em> contains only part of all labels <em>T</em>. In this way, the training mimics the testing when the model sees just a couple of samples<span>. For example, adding a new employee to the facial recognition system requires a single image and a label.</span></li>
<li>Sample a support set <img class="fm-editor-equation" src="assets/aa4bc1ea-b5c3-4afd-84af-822b26901b15.png" style="width:3.17em;height:1.00em;"/>, where the labels of all samples in <img class="fm-editor-equation" src="assets/f8a6dbad-5521-4176-9c67-7d093f01da14.png" style="width:1.25em;height:1.08em;"/> are only part of L <img class="fm-editor-equation" src="assets/7fc335ba-2d84-4a81-85e1-5021be3694c6.png" style="width:3.33em;height:1.17em;"/>. The support set contains <em>k</em> samples of each label.</li>
<li>Sample a training batch <img class="fm-editor-equation" src="assets/97335221-af7f-4e5c-8804-f9a15409fb29.png" style="width:3.00em;height:0.92em;"/>, where <img class="fm-editor-equation" src="assets/13951691-7fcb-47c8-b849-1d6a6c15df90.png" style="width:3.58em;height:1.17em;"/> (the same as the support set). <span>The combination of </span><img class="fm-editor-equation" src="assets/780e0957-6fd5-415d-8921-da00cad5e2ac.png" style="width:1.17em;height:1.00em;"/><span> and </span><img class="fm-editor-equation" src="assets/464ecc9a-6411-46fb-b8ad-4a48dd54cc34.png" style="width:1.33em;height:1.08em;"/><span> represents one training <strong>episode</strong>. We can think of the episode as a separate learning <strong>task</strong> with its corresponding dataset. Alternatively, in supervised learning, one episode is simply a single training sample.</span></li>
</ol>
<ol start="4">
<li>Optimize the network weights over the episode. The network represents the probability <img class="fm-editor-equation" src="assets/fce68f2a-12ac-4a4f-aa9d-bd1229ed21b1.png" style="width:4.17em;height:1.08em;"/> and uses<span> both</span><span> </span><img class="fm-editor-equation" src="assets/780e0957-6fd5-415d-8921-da00cad5e2ac.png" style="width:1.33em;height:1.17em;"/><span> and </span><img class="fm-editor-equation" src="assets/464ecc9a-6411-46fb-b8ad-4a48dd54cc34.png" style="width:1.50em;height:1.17em;"/><span> as inputs</span>. To clarify, the set <img class="fm-editor-equation" src="assets/bc78000b-ce6c-45b3-b80e-6ce86ee4ff22.png" style="width:1.25em;height:1.00em;"/> consists of the <img class="fm-editor-equation" src="assets/8a63630d-66b4-4e85-aa2b-b4e96e56969e.png" style="width:2.50em;height:1.17em;"/> tuples, conditioned on the support set <img class="fm-editor-equation" src="assets/4c0b25dd-e5ba-49e3-816a-84287b8da55a.png" style="width:1.33em;height:1.17em;"/>. This is the "meta" part of the training process, because the model learns to learn from a support set to minimize the loss over the full batch. The model uses the following cross-entropy objective:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9da8b85a-03fa-4a0e-85d6-780c27e5ee2a.png" style="width:28.00em;height:3.08em;"/></p>
<p style="padding-left: 60px">Here, <img class="fm-editor-equation" src="assets/ea5bddf9-9d3e-49be-a5a7-73a24bb0ec6f.png" style="width:2.42em;height:1.00em;"/> and <img class="fm-editor-equation" src="assets/6c29607a-e8a2-4892-b43e-0215125e28b2.png" style="width:4.92em;height:1.25em;"/> reflect the sampling of labels and examples respectively. Let's compare this to the same task, but in a classic supervised learning scenario. In this case, we sample mini batches <em>B</em> from the dataset <em>D</em> and there is no support set. The sampling is random and doesn't depend on the labels. Then, the preceding formula would transform to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/971e0294-1966-44bc-93e6-5b948cb613fb.png" style="width:19.17em;height:3.00em;"/></p>
<p>Meta learning algorithms can be classified into three main categories: metric-based, model-based, and optimization-based. In this chapter, we'll focus on the metric- and optimization- based approaches (excluding model-based). Model-based meta learning algorithms don't impose any restrictions on the type of ML algorithm that implements the probability <img class="fm-editor-equation" src="assets/732a9825-aa4b-4429-9700-d77602b5935d.png" style="width:3.92em;height:1.00em;"/>. That is, there is no requirement for encoder and mapping functions. Instead, they rely on network architectures specifically adapted to work with a small number of labeled samples. You may recall that in <a href="66956576-0f67-49a6-9ba8-1a782baa6b24.xhtml">Chapter 9</a>, <em>Emerging Neural Network Designs</em>, <span>we introduced one such model when</span> <span>we looked at the</span> <em>One-shot Learning with Memory-Augmented Neural Networks</em><span> paper(</span><a href="https://arxiv.org/abs/1605.06065">https://arxiv.org/abs/1605.06065</a><span>). As the name suggests, the paper demonstrates the use of memory-augmented neural networks in a one-shot learning framework. Since we have already discussed the network architecture, and the training process is similar to the one we described in this section, we won't include another model-based example in this chapter.</span></p>
<p>Now that we've introduced the basics of meta learning, in the following section we'll focus on metric-based learning algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Metric-based meta learning</h1>
                </header>
            
            <article>
                
<p>We mentioned a metric-based approach when we discussed the one-shot scenario in the <em>Introduction to meta learning </em><span>section</span><span>, but this approach applies to</span> <em>k</em><span>-shot learning in general. The idea is to measure the similarity between the unlabeled query sample </span><img style="font-size: 1em;width:0.75em;height:1.08em;" class="fm-editor-equation" src="assets/0737f559-b5df-47e5-8f94-61c64b233435.png"/><span> and all other samples</span> <img style="font-size: 1em;width:0.67em;height:0.67em;" class="fm-editor-equation" src="assets/428554e4-a2f5-4329-bb51-48ba85405fa2.png"/><span> of the support set. Using these similarity scores, we can compute a probability distribution</span> <img style="font-size: 1em;width:0.50em;height:1.00em;" class="fm-editor-equation" src="assets/e3047114-b1db-449c-91ad-4a8c07c441ad.png"/><span>. The following formula reflects this mechanism:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/843c91d4-f4f7-4593-bb4d-d806d7dfdb94.png" style="width:10.17em;height:2.92em;"/></p>
<p>Here, <em>α</em> is the similarity measure between the query samples and <img class="fm-editor-equation" src="assets/eabcb9e2-b8e4-439c-bf46-8a7522301878.png" style="width:1.33em;height:1.25em;"/> is the size of the support set with <em>n</em> classes and <em>k</em> samples of each class. To clarify, the label of the query sample is simply a linear combination of all samples of the support set. The classes of the samples with higher similarities will have higher contributions to the distribution of the label of the query sample. We can implement <em>α</em><span> as a clustering algorithm (for example, <em>k</em>-nearest neighbors) or an attention model (as we'll see later in the upcoming section).</span> In the case of zero-shot learning, this process has two formal steps: compute sample embeddings and then compute the similarity between the embeddings. But the preceding formula is a generalized combination of the two steps, and computes the <span>similarity</span> directly from the query samples (although internally, the steps could still be separate). The two-step metric-based learning (including the encoders <em>f</em> and <em>g</em>) is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1141 image-border" src="assets/c1ee8ed7-cf32-4dba-a869-a43efc4ee63c.png" style="width:30.92em;height:15.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Generic metric-based learning algorithm</div>
<p>In the next few sections, we'll discuss some of the more popular metric meta-learning algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Matching networks for one-shot learning</h1>
                </header>
            
            <article>
                
<p>We already discussed the training procedure that was introduced alongside matching networks in the <em>Introduction to</em> <em>meta learning</em> sectio<span>n</span><span>. Now, let's focus on the actual model, starting with the similarity measure, which we outlined in the </span><em>Metric-based meta learning </em><span>section</span><em>. </em><span>One way to implement this is with cosine similarity</span><span> (denoted with </span><em>c</em><span>)</span><span>, followed by softmax:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3f743c02-83f5-46ed-9fe9-dc2607309fc4.png" style="width:14.50em;height:2.67em;"/></p>
<p class="mce-root">Here, <em>f</em> and <em>g</em> are encoders of the samples of the new task and the support set respectively (as we discussed, it's possible that <em>f</em> and <em>g</em> are the same function). The encoders could be CNNs for image inputs or word embeddings, such as word2vec in the case of natural language processing tasks. This formula is very similar to the<span> attention mechanism that we introduced in <a href="0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml">Chapter 8</a>,<em> </em></span><em>Sequence-to-Sequence Models and Attention. </em></p>
<p class="mce-root">With the current definition, the encoder <em>g</em> only encodes one support sample at a time, independently of the other samples of the support set. However, it's possible that the embeddings <img class="fm-editor-equation" src="assets/129f9541-1bbb-43b2-bebe-4310d53a8b00.png" style="width:2.08em;height:1.08em;"/> and <img class="fm-editor-equation" src="assets/b2aa5d9c-f3c1-4932-8a0a-187f78564751.png" style="width:2.42em;height:1.25em;"/> of two samples <em>i</em> and <em>j</em> are very close in the embedding feature space, but that the two samples have different labels. The authors of the paper propose modifying <em>g</em> to take the whole support set <em>S</em> as additional input: <img class="fm-editor-equation" src="assets/8b93e6d0-a0b6-4e9f-ac1a-850ab142a1b6.png" style="width:3.17em;height:1.08em;"/>. In this way, the encoder could condition the embedding vector of <img class="fm-editor-equation" src="assets/2af7ad11-148a-4894-8d30-c181bd5585f5.png" style="width:0.83em;height:0.67em;"/> on <em>S</em> and avoid this problem. We can apply similar logic to the encoder <em>f</em> as well. The paper refers to new embedding functions as <strong>full context embeddings</strong>.</p>
<p>Let's look at how to implement full context embeddings over <em>f</em>. First, we'll introduce a new function <img class="fm-editor-equation" src="assets/c46e1950-117a-43cf-84ce-592498727f9a.png" style="width:2.00em;height:1.08em;"/>, which is similar to the old encoder (before including <em>S</em> as input)—that is, <em>f'</em> could be a CNN or word-embedding model, which creates sample embedding, independently of the support set. The result of <img class="fm-editor-equation" src="assets/c46e1950-117a-43cf-84ce-592498727f9a.png" style="width:2.08em;height:1.08em;"/> will serve as the input for the full embedding function <img class="fm-editor-equation" src="assets/ca72fc39-4d7a-448e-a67d-2c7336d2977b.png" style="width:3.92em;height:1.00em;"/>. We'll treat the support set as a sequence, which allows us to embed it using long short-term memory (LSTM). Because of this, computing the embedding vector is a sequential process of multiple steps.</p>
<p>However, <em>S</em> is a set, which implies that the order of samples in the sequence is not relevant. To reflect this, the algorithm also uses a special attention mechanism over the elements of the support set. In this way, the embedding function can attend to all previous elements of the sequence, regardless of their order.</p>
<p>Let's see how one step of the encoder works:</p>
<ol>
<li><img class="fm-editor-equation" src="assets/fd51771f-facb-41bb-8b25-e16bc4ec5776.png" style="width:15.58em;height:1.25em;"/>, where <em>t</em> is the current element of the input sequence, <img class="fm-editor-equation" src="assets/a9ea670b-72b1-4af1-be30-1fe93e02d555.png" style="width:0.83em;height:1.08em;"/> is an intermediate hidden state, <img class="fm-editor-equation" src="assets/5da17d1b-16a9-4da0-a99d-e363b302f1c4.png" style="width:1.50em;height:0.83em;"/> is the hidden state at step <em>t-1</em>, and <img class="fm-editor-equation" src="assets/d172852d-a434-4c56-b861-395e3685bf5b.png" style="width:1.83em;height:0.83em;"/> is the cell state. The attention mechanism is implemented with a vector <img class="fm-editor-equation" src="assets/17b4899b-1150-432e-b153-34bbc012cc11.png" style="width:2.17em;height:1.00em;"/>, which is concatenated to the hidden state <img class="fm-editor-equation" src="assets/91a1c38c-e597-47da-8840-8f6e1c369c71.png" style="width:1.75em;height:1.00em;"/>.</li>
</ol>
<ol start="2">
<li><img class="fm-editor-equation" src="assets/0bbe5c21-503b-4a53-b89c-db4eb7ca5db4.png" style="width:5.08em;height:1.00em;"/>, where <img class="fm-editor-equation" src="assets/1d7d2444-6565-4fe0-aeef-6b627dd6bb21.png" style="width:1.17em;height:1.25em;"/> is the final hidden state at step <em>t</em>.</li>
<li><img class="fm-editor-equation" src="assets/2211a28c-c0fa-4804-898c-f10d7e2304cd.png" style="width:9.92em;height:2.50em;"/>, where <img class="fm-editor-equation" src="assets/d0a84215-84c6-4555-ad04-18d6f3fab826.png" style="width:1.17em;height:1.08em;"/> is the size of the support set, <em>g</em> is an embedding function for the support set, and α is a similarity measure, which is defined as multiplicative attention, followed by a softmax:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f482152a-6913-4870-ac20-eb0219de356b.png" style="width:17.75em;height:1.58em;"/></p>
<p>The process continues for <em>T</em> steps (<em>T</em> is a parameter). We can summarize it with the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/16204eed-bf27-42d7-a7c3-c5c673afb6b9.png" style="width:16.42em;height:1.25em;"/></p>
<p>Next, let's focus on the full context embeddings of <em>g</em>. Like <em>f</em>, <span>we'll introduce a new function, <img class="fm-editor-equation" src="assets/3f28a031-ed39-4bab-a7a5-d88d8d243ebf.png" style="width:2.67em;height:1.25em;"/></span><span>, which is similar to the old encoder (before including </span><em>S</em><span> as input). The authors propose to use a bidirectional LSTM encoder, defined as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9bfdab3d-e575-4900-ae76-2a37fc18c20e.png" style="width:14.08em;height:1.92em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/20547e60-5396-43af-8168-185a072ca9e1.png" style="width:1.42em;height:1.75em;"/> and <img class="fm-editor-equation" src="assets/40fe9ba8-571b-4e0c-98f0-3232fac5e237.png" style="width:1.33em;height:1.67em;"/> are the cell hidden states in both directions. We can define them as follows:</p>
<p style="padding-left: 180px"><img src="assets/04b0f06b-7947-4297-ad2d-8eb11f6e736a.png" style="width:17.50em;height:3.83em;"/></p>
<p>In the next section, we'll discuss another metric-based learning approach called Siamese networks.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Siamese networks</h1>
                </header>
            
            <article>
                
<p>In this section, we'll discuss the <em>Siamese Neural Networks for One-shot Image Recognition</em> paper (<a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf">https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf</a>). A Siamese network is a system of two identical base networks, as illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1142 image-border" src="assets/a472a2ae-9a98-44be-9c15-e7c2018a8144.png" style="width:21.42em;height:7.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Siamese networks</div>
<p>The two networks are identical in the sense that they share the same architecture and the same parameters (weights). Each network is fed a single input sample and the last hidden layer produces an embedding vector of that sample. The two embeddings are fed to a distance measure. The distance is further processed to produce the final output of the system, which is binary and represents a verification of whether the two samples are from the same class. The distance measure itself is differentiable, which allows us to train the networks as a single system. The authors of the paper <span>recommend using</span> <em>L1</em> distance:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bf11009d-9829-442f-87e0-b5733908126e.png" style="width:10.17em;height:1.25em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/c5191441-03d4-4710-be85-b879e8b75f25.png" style="width:0.92em;height:1.08em;"/> is the base network. Using Siamese networks in a one-shot learning scenario follows the same general idea we described in the <em>Meta-training and meta-testing </em><span>section,</span><span> but in this case, the task is simplified because we always have only two classes (same or not same), regardless of the actual number of classes in the dataset. In the meta-training phase, we train the system with a large labeled dataset. </span><span>We do this by generating samples of image pairs and binary labels with either the same or a different class</span><span>. In the meta-testing phase, we have a single query sample and a support set. We then create multiple pairs of images, where each pair contains the query sample and a single sample of the support set. </span><span>We have as many image pairs as the size of the support set.</span><span> Then, we feed all pairs to the Siamese system and we pick the pair with the smallest distance. The class of the query image is determined by the class of the support sample of that pair.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing Siamese networks</h1>
                </header>
            
            <article>
                
<p>In this section, we'll use Keras to implement a simple example of Siamese networks, which will verify whether two MNIST images are from the same class or not. It is partially based on <a href="https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py">https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py</a>.</p>
<p>Let's look at how to do this step by step:</p>
<ol>
<li>We'll start with the import statements:</li>
</ol>
<pre style="padding-left: 60px"><span>import </span>random<br/><br/><span>import </span>numpy <span>as </span>np<br/><span>import </span>tensorflow <span>as </span>tf</pre>
<ol start="2">
<li>Next, we'll implement the <kbd>create_pairs</kbd> function to create the train/test dataset (both for training and testing):</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>create_pairs</span>(inputs: np.ndarray<span>, </span>labels: np.ndarray):<br/><span>    </span>num_classes = <span>10<br/></span><span><br/></span><span>    </span>digit_indices = [np.where(labels == i)[<span>0</span>] <span>for </span>i <span>in </span><span>range</span>(num_classes)]<br/>    pairs = <span>list</span>()<br/>    labels = <span>list</span>()<br/>    n = <span>min</span>([<span>len</span>(digit_indices[d]) <span>for </span>d <span>in </span><span>range</span>(num_classes)]) - <span>1<br/></span><span>    </span><span>for </span>d <span>in </span><span>range</span>(num_classes):<br/>        <span>for </span>i <span>in </span><span>range</span>(n):<br/>            z1<span>, </span>z2 = digit_indices[d][i]<span>, </span>digit_indices[d][i + <span>1</span>]<br/>            pairs += [[inputs[z1]<span>, </span>inputs[z2]]]<br/>            inc = random.randrange(<span>1</span><span>, </span>num_classes)<br/>            dn = (d + inc) % num_classes<br/>            z1<span>, </span>z2 = digit_indices[d][i]<span>, </span>digit_indices[dn][i]<br/>            pairs += [[inputs[z1]<span>, </span>inputs[z2]]]<br/>            labels += [<span>1</span><span>, </span><span>0</span>]<br/><br/>    <span>return </span>np.array(pairs)<span>, </span>np.array(labels<span>, </span><span>dtype</span>=np.float32)</pre>
<p style="padding-left: 60px">Each dataset sample consists of an input pair of two MNIST images and a binary label, which indicates whether they are from the same class. The function creates an equal number of true/false samples distributed over all classes (digits).</p>
<ol start="3">
<li>Next, let's implement the <kbd>create_base_network</kbd> function, which defines one branch of the Siamese network:</li>
</ol>
<pre style="padding-left: 60px"><span>def </span><span>create_base_network</span>():<br/><span>    </span><span>return </span>tf.keras.models.Sequential([<br/>        tf.keras.layers.Flatten()<span>,<br/></span><span>        </span>tf.keras.layers.Dense(<span>128</span><span>, </span><span>activation</span>=<span>'relu'</span>)<span>,<br/></span><span>        </span>tf.keras.layers.Dropout(<span>0.1</span>)<span>,<br/></span><span>        </span>tf.keras.layers.Dense(<span>128</span><span>, </span><span>activation</span>=<span>'relu'</span>)<span>,<br/></span><span>        </span>tf.keras.layers.Dropout(<span>0.1</span>)<span>,<br/></span><span>        </span>tf.keras.layers.Dense(<span>64</span><span>, </span><span>activation</span>=<span>'relu'</span>)<span>,<br/></span><span>    </span>])</pre>
<p style="padding-left: 60px">The branch represents the base network that starts from the input and goes to the last hidden layer, before the distance measure. We'll use a simple NN of three fully connected layers. </p>
<ol start="4">
<li>Next, let's build the whole training system, starting from the MNIST dataset:</li>
</ol>
<pre style="padding-left: 60px">(x_train<span>, </span>y_train)<span>, </span>(x_test<span>, </span>y_test) = tf.keras.datasets.mnist.load_data()<br/>x_train = x_train.astype(np.float32)<br/>x_test = x_test.astype(np.float32)<br/>x_train /= <span>255<br/></span>x_test /= <span>255<br/></span>input_shape = x_train.shape[<span>1</span>:]</pre>
<ol start="5">
<li>We'll use the raw dataset to create the actual train and test verification datasets:</li>
</ol>
<pre style="padding-left: 60px">train_pairs<span>, </span>tr_labels = create_pairs(x_train<span>, </span>y_train)<br/>test_pairs<span>, </span>test_labels = create_pairs(x_test<span>, </span>y_test)</pre>
<ol start="6">
<li>Then, we'll build the base portion of the Siamese network:</li>
</ol>
<pre style="padding-left: 60px">base_network = create_base_network()</pre>
<p style="padding-left: 60px">The <kbd>base_network</kbd> object is shared between the two forks of the Siamese system. In this way, we ensure that the weights are the same in the two branches.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="7">
<li>Next, let's create the two branches:</li>
</ol>
<pre style="padding-left: 60px"><span># Create first half of the siamese system<br/></span>input_a = tf.keras.layers.Input(<span>shape</span>=input_shape)<br/><br/><span># Note how we reuse the base_network in both halfs<br/></span>encoder_a = base_network(input_a)<br/><br/><span># Create the second half of the siamese system<br/></span>input_b = tf.keras.layers.Input(<span>shape</span>=input_shape)<br/>encoder_b = base_network(input_b)</pre>
<ol start="8">
<li>Next, we'll create the L1 distance, which uses the outputs of <kbd>encoder_a</kbd> and <kbd>encoder_b</kbd>. It is implemented as a <kbd>tf.keras.layers.Lambda</kbd> layer:</li>
</ol>
<pre style="padding-left: 60px">l1_dist = tf.keras.layers.Lambda(<br/>    <span>lambda </span>embeddings: tf.keras.backend.abs(embeddings[<span>0</span>] - embeddings[<span>1</span>])) \<br/>    ([encoder_a<span>, </span>encoder_b])</pre>
<ol start="9">
<li>Then, we'll create the final fully connected layer, which takes the output of the distance and compresses it to a single sigmoid output:</li>
</ol>
<pre style="padding-left: 60px">flattened_weighted_distance = tf.keras.layers.Dense(<span>1</span><span>, </span><span>activation</span>=<span>'sigmoid'</span>) \<br/>    (l1_dist)</pre>
<ol start="10">
<li>Finally, we can build the model and initiate the training for 20 epochs:</li>
</ol>
<pre style="padding-left: 60px"><span># Build the model<br/></span>model = tf.keras.models.Model([input_a<span>, </span>input_b]<span>, </span>flattened_weighted_distance)<br/><br/><span># Train<br/></span>model.compile(<span>loss</span>=<span>'binary_crossentropy'</span><span>,<br/></span><span>              </span><span>optimizer</span>=tf.keras.optimizers.Adam()<span>,<br/></span><span>              </span><span>metrics</span>=[<span>'accuracy'</span>])<br/><br/>model.fit([train_pairs[:<span>, </span><span>0</span>]<span>, </span>train_pairs[:<span>, </span><span>1</span>]]<span>, </span>tr_labels<span>,<br/></span><span>          </span><span>batch_size</span>=<span>128</span><span>,<br/></span><span>          </span><span>epochs</span>=<span>20</span><span>,<br/></span><span>          </span><span>validation_data</span>=([test_pairs[:<span>, </span><span>0</span>]<span>, </span>test_pairs[:<span>, </span><span>1</span>]]<span>, </span>test_labels))</pre>
<p>If everything goes alright, the model will achieve around 98% accuracy.</p>
<p>Next, we'll discuss yet another metric learning method called prototypical networks. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prototypical networks</h1>
                </header>
            
            <article>
                
<p>In a few short learning scenarios, it would be very easy for a high-capacity model (an NN with many layers and parameters) to overfit. Prototypical networks (as discussed in the <em>Prototypical Networks for Few-shot Learning</em> paper, <a href="https://arxiv.org/abs/1703.05175">https://arxiv.org/abs/1703.05175</a>) address this issue by computing a special prototype vector of each label, which is based on all samples of that label. The same prototypical network computes an embedding of the query samples as well. Then, we measure the distance between the query embedding and the prototypes and assign the query class accordingly (more details on this later in the section).</p>
<p>Prototypical networks work for both zero-shot and few-shot learning, as illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1143 image-border" src="assets/58be1cd6-d9e8-438d-a162-e36d75404460.png" style="width:28.67em;height:7.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Left: Few-shot learning; Right: Zero-shot learning. Source: https://arxiv.org/abs/1703.05175<a href="https://arxiv.org/abs/1703.05175"/></div>
<p>Let's start with the few-shot learning scenario, where the prototype vector <img class="fm-editor-equation" src="assets/e72f4c55-df9a-4d9e-95c8-81f9244a499d.png" style="width:1.00em;height:0.75em;"/> of each class <em>k</em> is computed as the element-wise mean value of all samples of that class:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/16491fe9-1061-41e5-81c4-1a0731c13384.png" style="width:13.92em;height:3.83em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/30bb12e9-f333-4761-adbd-0d6528c3606e.png" style="width:1.67em;height:1.17em;"/> is the number of samples in the support set of class <em>k</em> and <img class="fm-editor-equation" src="assets/97739005-895a-4f7e-a9b0-91cc57aaa616.png" style="width:0.92em;height:1.08em;"/> is the prototypical network with parameters <em>θ</em>. In the zero-shot learning scenario, the prototype is computed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/05302326-8610-4da4-b8e0-5e6ccb3a58e4.png" style="width:6.08em;height:1.33em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/770cd824-65a0-4b46-b1cb-ce9cf587643c.png" style="width:1.33em;height:0.92em;"/>is a metadata vector, which gives a high-level description of the label, and <img class="fm-editor-equation" src="assets/6d0952ea-11c9-4b62-90cb-5a891c014b8f.png" style="width:1.58em;height:1.25em;"/> is the embedding function (encoder) of that vector. The metadata vectors could be given in advance or computed. </p>
<p>Each new query sample is classified as a softmax over the distance between the sample embedding and all prototypes: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0af436ea-87c0-4572-a362-0249a8fa36ee.png" style="width:18.42em;height:2.75em;"/></p>
<p>Here, <em>d</em> is a distance measure (for example, the linear Euclidean distance).  </p>
<p>Now that we have an overview of the main idea behind prototype networks, let's focus on how to train them (the procedure is similar to the training we outlined in the <em>Introduction to meta learning </em><span>section)</span><span>.</span></p>
<p>Before we start, we'll introduce some notations:</p>
<ul>
<li><em>D</em> is the few-shot training set.</li>
<li><em><span> </span>D<sub>k</sub></em> is the training samples of <em>D</em> of class <em>k</em>.</li>
<li><em>T</em> is the total number of classes in the dataset.</li>
<li><img class="fm-editor-equation" src="assets/aeebc6f9-55ef-4356-a4c8-aa68957472ca.png" style="width:2.92em;height:0.92em;"/> is the subset of labels, selected for each training episode.</li>
<li><em>N<sub>S</sub></em> is the number of support samples per class per episode.</li>
<li><em>N<sub>Q</sub></em><span> </span>is the number of query samples per episode.</li>
</ul>
<p>The algorithm starts with the training set <em>D</em> and outputs the result of the cost function <em>J.</em> Let's look at how it works step by step:</p>
<ol>
<li><span>Sample a set of labels </span><img class="fm-editor-equation" src="assets/aeebc6f9-55ef-4356-a4c8-aa68957472ca.png" style="width:2.67em;height:0.83em;"/>.</li>
</ol>
<ol start="2">
<li>For each class <em>k</em> in <em>L</em>, do the following:
<ol>
<li>Sample support set <img class="fm-editor-equation" src="assets/f281ab2d-23e9-4cae-8c30-b2108591b259.png" style="width:3.00em;height:0.83em;"/>, where <img class="fm-editor-equation" src="assets/fc7488e4-b34b-4096-8a86-9de46e6b63df.png" style="width:4.67em;height:1.25em;"/>.</li>
<li>Sample query set <img class="fm-editor-equation" src="assets/dd31ea70-5b05-4b41-8f2f-f8c618b9f6eb.png" style="width:7.75em;height:1.08em;"/>, where <img class="fm-editor-equation" src="assets/0a4e6495-2547-4855-9112-13020afe2afc.png" style="width:3.42em;height:0.92em;"/>.</li>
<li>Compute the class prototype from the support set:</li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/16491fe9-1061-41e5-81c4-1a0731c13384.png" style="width:10.92em;height:3.00em;"/></p>
<ol start="3">
<li>Initialize the cost function <img class="fm-editor-equation" src="assets/e65dadec-23f7-4f15-b24d-34dc8a1d25b2.png" style="width:3.83em;height:1.17em;"/>.</li>
<li><span>For each class </span><em>k</em><span> in </span><em>L</em>, do the following:
<ol>
<li>For each query sample <img class="fm-editor-equation" src="assets/41a130c0-a158-4900-8769-5e1fc8adf9f0.png" style="width:5.42em;height:1.25em;"/>, update the cost function as follows:</li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9c041ae9-5e63-4a39-a70e-aa10926261f9.png" style="width:27.33em;height:2.75em;"/></p>
<p style="padding-left: 120px">Intuitively, the first component (in the square braces) minimizes the distance between the query and its corresponding prototype of the same class. The second term maximizes the sum of the distance between the query and the prototypes of the other classes.</p>
<p>The authors of the paper demonstrated their work on the Omniglot dataset (<a href="https://github.com/brendenlake/omniglot">https://github.com/brendenlake/omniglot</a>), which contains 1,623 images of handwritten characters collected from 50 alphabets. There are 20 examples associated with each character, where each example is drawn by a different human subject. The goal is to classify a new character as one of the 1,623 classes. They trained prototypical networks using Euclidean distance, one-shot, and five-shot scenarios, and training episodes with 60 classes and 5 query points per class. The following screenshot shows a <em>t</em>-SNE (<a href="https://lvdmaaten.github.io/tsne/">https://lvdmaaten.github.io/tsne/</a>) visualization of the embeddings of a subset of similar (but not the same) characters of the same alphabet, learned by the prototypical network. </p>
<p>Even though the visualized characters are minor variations of each other, the network is able to cluster the hand-drawn characters closely around the class prototypes. Several misclassified characters are highlighted in rectangles,<span> </span><span>along with arrows pointing to the correct prototype:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1307 image-border" src="assets/28516082-2f11-4246-bb25-2bee44a03d0c.png" style="width:37.25em;height:35.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A <span><em>t</em>-SNE </span><span><span>visualization of the embeddings of a subset of similar characters, learned by the network; source: https://arxiv.org/abs/1703.05175</span></span></div>
<p>This concludes our description of prototypical networks and metric-based meta learning as well. Next, we'll focus on model-based methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimization-based learning</h1>
                </header>
            
            <article>
                
<p>So far, we have discussed metric-based learning, which uses a special similarity measure (which is hard to overfit) to adapt the representational power of NNs with the ability to learn from datasets with few training samples. Alternatively, model-based approaches rely on improved network architectures (for example, memory augmented networks) to solve the same issue. In this section, we'll discuss optimization-based approaches, which adjust the training framework to adapt to the few-shot learning requirements. More specifically, we'll focus on a particular algorithm called <strong>model-agnostic meta learning</strong> (MAML; <em>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</em>, <a href="https://arxiv.org/abs/1703.03400">https://arxiv.org/abs/1703.03400</a>). As the name suggests, MAML can be applied over any learning problem and model that is trained with gradient descent.</p>
<p>To quote the original paper:</p>
<div class="packt_quote">The key idea underlying our method is to train the model’s initial parameters such that the model has maximal performance on a new task after the parameters have been updated through one or more gradient steps computed with a small amount of data from that new task.<br/>
...<br/>
The process of training a model’s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying the top layer weights in a feedforward model) can produce good results. In effect, our procedure optimizes for models that are easy and fast to fine-tune, allowing the adaptation to happen in the right space for fast learning. From a dynamical systems standpoint, our learning process can be viewed as maximizing the sensitivity of the loss functions of new tasks with respect to the parameters: when the sensitivity is high, small local changes to the parameters can lead to large improvements in the task loss.<br/>
The primary contribution of this work is a simple model and task-agnostic algorithm for meta-learning that trains a model’s parameters such that a small number of gradient updates will lead to fast learning on a new task. We demonstrate the algorithm on different model types, including fully connected and convolutional networks, and in several distinct domains, including few-shot regression, image classification, and reinforcement learning. Our evaluation shows that our meta-learning algorithm compares favorably to state-of-the-art one-shot learning methods designed specifically for supervised classification, while using fewer parameters, but that it can also be readily applied to regression and can accelerate reinforcement learning in the presence of task variability, substantially outperforming direct pretraining as initialization.</div>
<p class="mce-root"/>
<p>To understand MAML, we'll introduce some paper-specific notations (some of them overlap with notations from the preceding sections, but I prefer to preserve the originals from the paper):</p>
<ul>
<li>We'll denote the model (<span>neural network) with</span> <em><img class="fm-editor-equation" src="assets/bd356c54-ccf7-4925-a927-44ad2593e014.png" style="width:1.08em;height:1.25em;"/>,</em> which maps inputs <strong>x</strong> to outputs <strong>a</strong>.</li>
<li>We'll denote the full training set with <img class="fm-editor-equation" src="assets/fccdc6d6-2215-4a4a-9a04-679fc9fcf47a.png" style="width:0.92em;height:1.08em;"/> (equivalent to the dataset <em>D</em>). Similar to the meta-training of the <em>Meta-training and meta-testing </em>section, we sample tasks <img class="fm-editor-equation" src="assets/405c5e5e-1ef4-4528-95f0-f7408e991380.png" style="width:0.83em;height:1.00em;"/>(equivalent to episodes) from <img class="fm-editor-equation" src="assets/57f3e407-f1aa-423a-92fe-e7b52b477953.png" style="width:0.83em;height:1.00em;"/>during training. This process is defined as a distribution over tasks <img class="fm-editor-equation" src="assets/fe7b2420-8146-4563-95d5-e219560c8b13.png" style="width:1.58em;height:0.83em;"/>.</li>
<li>We'll denote one task (an episode) with <img class="fm-editor-equation" src="assets/54b1269a-0a1a-424c-ae52-2bc0e9ac97f0.png" style="width:20.08em;height:1.00em;"/>. It is defined by a loss function <img class="fm-editor-equation" src="assets/0431e13d-3789-4612-b937-aa14bed7d0cd.png" style="width:8.25em;height:1.00em;"/> (equivalent to the loss <em>J</em>), a distribution over the initial observations <img class="fm-editor-equation" src="assets/ad65abcb-0d83-4ffe-aae4-b8e50ef72d3d.png" style="width:2.42em;height:1.17em;"/>, a transition distribution <img class="fm-editor-equation" src="assets/1aaba41c-f1ed-4a83-9010-848ce3f9438e.png" style="width:5.92em;height:1.17em;"/>, and length <em>H</em>. </li>
</ul>
<p><span>To understand some components of the MAML task definition, </span>let's note that besides supervised problems, MAML can be applied to <strong>reinforcement learning</strong> (<strong>RL</strong>) tasks as well. In the RL framework, we have an environment and an agent, which continuously interact with each other. At each step, the agent takes an action (from a number of possible actions) and the environment provides it with feedback. The feedback consists of a reward (which could be negative) and the new state of the environment after the agent's action. Then the agent takes a new action, and so on, as illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1144 image-border" src="assets/db858706-4511-4407-8c5e-5fbd55b4bf0f.png" style="width:18.08em;height:6.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The RL framework</div>
<p>Many real-world tasks can be represented as RL problems, including games, where the agent is the player and the environment is the game universe.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can view task <img class="fm-editor-equation" src="assets/56d99c2b-8167-4080-9483-ed465ed1f3d3.png" style="width:0.92em;height:1.08em;"/> in both supervised and RL contexts. With a supervised task, we have labeled training samples <img class="fm-editor-equation" src="assets/df638237-5175-4d54-bbda-b4eafb34a6a5.png" style="width:2.92em;height:1.08em;"/> in no particular order. But in an RL context, we can view the inputs <strong>x</strong> as the environment state and the outputs <strong>a</strong> as the agent's action. In this scenario, the task is sequential—state <strong>x</strong><em><sub>1</sub></em> leads to action <strong>a</strong><sub><em>1</em></sub>, which in turn leads to state <strong>x</strong><sub><em>2</em></sub>, and so on. The initial state of the environment is denoted as <img class="fm-editor-equation" src="assets/ad65abcb-0d83-4ffe-aae4-b8e50ef72d3d.png" style="width:2.42em;height:1.17em;"/>. This means that <img class="fm-editor-equation" src="assets/1aaba41c-f1ed-4a83-9010-848ce3f9438e.png" style="width:4.67em;height:0.92em;"/> is the probability of a new environment state <img class="fm-editor-equation" src="assets/d1ac7d91-b821-4f28-a9af-0ac7326c59b7.png" style="width:1.75em;height:0.75em;"/>, given the previous state <strong>x</strong><em><sub>t</sub></em> and the agent's action <strong>a</strong><sub><em>t</em></sub>. The loss <img class="fm-editor-equation" src="assets/aded729e-1ffc-4689-9b56-719691cfaa4b.png" style="width:0.83em;height:1.00em;"/> can be viewed in both contexts as well: a misclassification loss in the supervised scenario and a cost function (the one that provides rewards) in the RL scenario.</p>
<p>Now that we're familiar with the notation, let's focus on the MAML algorithm. To understand how it works, we'll look at another quote from the original paper:</p>
<div class="packt_quote">We propose a method that can learn the parameters of any standard model via meta-learning in such a way as to prepare that model for fast adaptation. The intuition behind this approach is that some internal representations are more transferable than others. For example, a neural network might learn internal features that are broadly applicable to all tasks in <img class="fm-editor-equation" src="assets/8d889142-b146-4f41-a47e-dd57238882d0.png" style="width:2.08em;height:1.08em;"/>, rather than a single individual task. How can we encourage the emergence of such general-purpose representations? We take an explicit approach to this problem: since the model will be fine-tuned using a gradient-based learning rule on a new task, we will aim to learn a model in such a way that this gradient-based learning rule can make rapid progress on new tasks drawn from <img class="fm-editor-equation" src="assets/231436d9-6ab9-4ff4-a63f-62fda7143215.png" style="width:2.25em;height:1.17em;"/>, without overfitting. In effect, we will aim to find model parameters that are sensitive to changes in the task, such that small changes in the parameters will produce large improvements on the loss function of any task drawn from <img class="fm-editor-equation" src="assets/1be30fb7-8c51-469d-8e47-c2ffb6895e1c.png" style="width:2.25em;height:1.17em;"/>, when altered in the direction of the gradient of that loss.</div>
<p>After all this suspense, let's check the MAML algorithm, illustrated by the following pseudocode:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1145 image-border" src="assets/8ed9c91a-6d4d-4dbd-8a5d-bb0c9013be39.png" style="width:26.33em;height:16.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The MAML algorithm: source: https://arxiv.org/abs/1703.03400</div>
<p>The algorithm has an outer (line 2) and an inner loop (line 4). We'll start with the inner loop, which iterates over a number of tasks, sampled from the task distribution <img class="fm-editor-equation" src="assets/17f4abf2-e718-48f2-b619-f07ca66f52ff.png" style="width:2.58em;height:0.67em;"/>. Let's focus on a single loop iteration, which handles a single task <img class="fm-editor-equation" src="assets/07a0cc41-5a51-4412-bf2c-6eee6e86c686.png" style="width:1.00em;height:1.17em;"/> with <img class="fm-editor-equation" src="assets/bdd38bab-6c83-4fbb-8f26-a4ffcdb2b52e.png" style="width:1.58em;height:1.17em;"/> training samples, where <img class="fm-editor-equation" src="assets/95d50486-9933-4695-ab69-4170bc0a56ba.png" style="width:1.25em;height:1.33em;"/> is the support set of the task. The training samples are processed as batches in the following steps (lines 4 through 7 in the preceding screenshot):</p>
<ol>
<li>Propagate the samples through the model and compute<span> the loss </span><img class="fm-editor-equation" src="assets/b6d1c143-b5c5-4d67-adbc-d3353ecb6fc8.png" style="width:3.33em;height:1.25em;"/>.</li>
<li>Compute the error gradient <img class="fm-editor-equation" src="assets/2ce06d60-e158-48ce-8c69-1aa1ecf91ac2.png" style="width:4.08em;height:1.08em;"/> with respect to the initial parameters <em>θ.</em></li>
<li>Propagate the gradient backward and compute the updated model parameters <img class="fm-editor-equation" src="assets/3949829b-221a-4175-851d-8f886214d0b5.png" style="width:7.83em;height:1.08em;"/>, where α is the learning rate. Note that the parameters <img class="fm-editor-equation" src="assets/02942f5e-041d-42b8-af9a-d45b069de7d8.png" style="width:0.75em;height:1.08em;"/> are auxiliary and are specific for each task. To clarify, whenever the inner loop starts a new iteration for a new task <img class="fm-editor-equation" src="assets/79232bbc-2d38-44cd-aca8-30c1f1fe9df1.png" style="width:2.08em;height:1.33em;"/>, the model always starts with the same initial parameters<span> </span><em>θ</em><em>.</em> At the same time, each task stores its updated weights as an additional variable <img class="fm-editor-equation" src="assets/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png" style="width:1.00em;height:1.50em;"/> without actually modifying the initial parameters<span> </span><em>θ</em> (we'll update the original model in the outer loop).</li>
<li>We can perform such gradient updates multiple times over the same task. Think of it as training over multiple batches, implemented with an additional nested loop in the inner loop. In this scenario, the algorithm starts each iteration <em>i</em> with the weights <img class="fm-editor-equation" src="assets/5744edb7-fe78-47a9-97df-9f4b87dc23bc.png" style="width:1.92em;height:1.42em;"/> of the last iteration and not with the initial parameters <em>θ</em>, as shown in the following formula:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fcd96c3f-47ce-4d6a-b59b-084e6d9fa2ca.png" style="width:15.00em;height:8.83em;"/></p>
<p style="padding-left: 60px">In the case of multiple iterations, only the latest weights <img class="fm-editor-equation" src="assets/bb684ec6-d41a-4ca2-bee9-b765b06e8689.png" style="width:1.25em;height:1.17em;"/> are preserved.</p>
<p>Only after the inner loop is done can we proceed to update the initial parameters<span> </span><em>θ</em> of the original model, based on the feedback of all tasks <img class="fm-editor-equation" src="assets/cb429e50-f798-426e-9792-5557e2699e3c.png" style="width:0.83em;height:1.00em;"/>. To understand why this is necessary, let's take a look at the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1146 image-border" src="assets/185047d5-b9ac-4aa4-897c-6285fe0566d0.png" style="width:18.25em;height:11.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">MAML that optimizes for parameters <em>θ</em> that can quickly adapt to new tasks: source: https://arxiv.org/abs/1703.03400</div>
<p>It shows the error gradients of three tasks <img class="fm-editor-equation" src="assets/f67b0a3a-58bc-4c2c-be06-8adfbb56484e.png" style="width:7.25em;height:1.00em;"/> along the global error gradient. Let's assume that, instead of an inner/outer loop mechanism, we iterate sequentially over each task and simply update the original model parameters <em>θ</em> after each mini batch<em>.</em> We can see that the gradients of the different loss functions would push the model in completely different directions; for example, the error gradient of task 2 will contradict the gradient of task 1. MAML solves this problem by aggregating (but not applying) the updated weights for each task from the inner loop (the auxiliary parameters <img class="fm-editor-equation" src="assets/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png" style="width:0.83em;height:1.25em;"/>). Then, we can compute the outer loop cost function (referred to as the meta-objective), combining the <span>updated weights of all tasks all at once</span> (this is a meta optimization across all tasks):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0ece93d6-c678-40b6-8b7d-9fd721f250f9.png" style="width:24.42em;height:2.83em;"/></p>
<p>We use the following formula for the weight update of the main model parameters:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b4d82576-378d-4629-a4f6-b3fa3259db10.png" style="width:13.67em;height:2.83em;"/></p>
<p>Here, β is the learning rate. The outer loop tasks <img class="fm-editor-equation" src="assets/cafe3eca-c354-4d70-9cdc-77c5f072b84d.png" style="width:1.00em;height:1.17em;"/> (line 8 of the MAML pseudocode program) are not the same as the ones from the inner loop (line 3). We can think of inner loop tasks as the training set and the tasks of the outer loop as the validation set. Note that we use task-specific parameters <img class="fm-editor-equation" src="assets/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png" style="width:0.92em;height:1.33em;"/> to compute the losses, but we compute the loss gradient with respect to the initial parameters <em>θ</em>. To clarify, this means that we backpropagate through the outer loop and the inner loop as well. This is referred to as a second-order gradient, because we compute a gradient over the gradient (second derivative). This makes it possible to learn parameter that can generalize over a task even after a number of updates.</p>
<p class="mce-root"/>
<p><span>One disadvantage of MAML is that backpropagation through the full computational graph (the outer loop and inner loop) is computationally expensive. In addition, because of the large number of backpropagation steps, it can suffer from vanishing or exploding gradients. To better understand this, let's assume that we have a single task <img class="fm-editor-equation" src="assets/cafe3eca-c354-4d70-9cdc-77c5f072b84d.png" style="width:0.75em;height:0.92em;"/> (we'll omit it from the formulas); we perform a single gradient step (one inner loop iteration) for that task, and the inner loop's updated parameters are <img class="fm-editor-equation" src="assets/b5d8c89a-4eb0-42b2-8562-58268f1bc207.png" style="width:0.67em;height:0.83em;"/>. That is, we change the notation of the loss function from <img class="fm-editor-equation" src="assets/14233372-0ec2-4bdf-a55a-62c8d16a6764.png" style="width:4.92em;height:1.33em;"/> to <img class="fm-editor-equation" src="assets/cdb3b688-6555-483c-a3c6-eb1e13d32ca0.png" style="width:1.92em;height:1.00em;"/>. Then, t</span>he parameter <span>update rule</span> of the outer loop becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f64fb8b8-0784-453e-ae82-c4712fb0b196.png" style="width:14.42em;height:2.83em;"/></p>
<p>We can compute the gradient of the loss with respect to the initial parameters <em>θ</em> with the help of the chain rule (see <a href="b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml">Chapter 1</a>, <em>The Nuts and Bolts Of Neural Networks</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dedd5d9b-fbff-41a2-9aad-e9fefa467305.png" style="width:19.50em;height:2.67em;"/></p>
<p>We can see that the formula includes second-degree derivative <img class="fm-editor-equation" src="assets/01fbf8b5-14a9-4822-badf-d77723f72555.png" style="width:6.75em;height:1.00em;"/>. The authors of MAML have proposed the so-called <strong>first-order MAML</strong> (<strong>FOMAML</strong>), which simply ignores the term <img class="fm-editor-equation" src="assets/9a4d1637-422f-4a4e-8b17-ebebecac8549.png" style="width:3.33em;height:1.00em;"/>. With this, we have <img class="fm-editor-equation" src="assets/30598cc8-79a9-41cb-9d04-bcea6c45d59f.png" style="width:4.17em;height:1.08em;"/> and the FOMAML gradient becomes: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c3fd34a5-d556-428d-b7e1-e7d316a681e4.png" style="width:7.83em;height:1.08em;"/></p>
<p>This simplified formula excludes the computationally expensive second-order gradient.</p>
<p>So far, we have looked at the generic MAML algorithm, which applies for both supervised and RL settings. Next, let's focus on the supervised version. Let's recall that in the supervised case, each task is a list of unrelated input/label pairs and the episode length <em>H</em> is <em>1</em>. We can see the MAML algorithm for few-shot supervised learning in the following pseudocode (it's similar to the generic algorithm):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1147 image-border" src="assets/66cf802b-e94d-417b-a945-c2c575b9ccfe.png" style="width:26.00em;height:19.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>MAML for few-shot supervised learning: source: https://arxiv.org/abs/1703.03400</span></div>
<p>In the preceding code, equations 2 and 3 both refer to cross-entropy losses for classification tasks or mean square errors for regression tasks, <img class="fm-editor-equation" src="assets/84101fed-0c4c-4d2d-b671-b4cd6dd43ee5.png" style="width:0.92em;height:1.08em;"/> refers to the inner loop training set, and <img class="fm-editor-equation" src="assets/fb149d2e-5e1d-4262-8e82-0e463ff8033e.png" style="width:1.25em;height:1.08em;"/> refers to the validation set of the outer loop.</p>
<p>Finally, let's discuss the RL scenario, as illustrated by the following pseudocode:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1148 image-border" src="assets/2a79b67c-b4c1-4ea9-88e9-14cb933aa8dc.png" style="width:26.83em;height:19.83em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>MAML for few-shot reinforcement learning: source: https://arxiv.org/abs/1703.03400</span></span></div>
<p>Each sample <img class="fm-editor-equation" src="assets/22fd3afd-a08e-4a6e-8351-81ea3a82d65e.png" style="width:10.08em;height:1.00em;"/> represents a trajectory of one game episode, where the environment presents the agent with its current state <img class="fm-editor-equation" src="assets/6995f43b-5308-45bb-a6f7-ee41a2a6ce75.png" style="width:1.17em;height:0.92em;"/> <span>at step </span><em>t</em>. In turn, the agent (NN) samples use <strong>policy</strong> <img class="fm-editor-equation" src="assets/5365dd0a-8fb5-43fa-be6e-98e72aace46f.png" style="width:1.08em;height:1.25em;"/> to map states <img class="fm-editor-equation" src="assets/cbf56b62-9c7a-463d-881c-0c27324811d0.png" style="width:1.17em;height:0.92em;"/> to a distribution over actions <img class="fm-editor-equation" src="assets/b0c3bb4e-5d4e-4a33-8e1a-e878aabb91d0.png" style="width:4.33em;height:1.00em;"/>. The model uses a special type of loss function, which aims to train the network to maximize rewards over all steps of the episode. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the field of meta learning, which can be described as learning to learn. We started with an introduction to meta learning. More specifically, we talked about zero-shot and few-shot learning, as well as meta training and meta testing. Then, we focused on several metric-based learning approaches. We looked at matching networks, implemented an example of a Siamese network, and we introduced prototypical networks. Next, we focused on optimization-based learning, where we introduced the MAML algorithm.</p>
<p>In the next chapter, we'll learn about an exciting topic: automated vehicles. </p>


            </article>

            
        </section>
    </body></html>