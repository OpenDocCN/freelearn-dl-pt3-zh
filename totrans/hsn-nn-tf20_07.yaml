- en: Efficient Data Input Pipelines and Estimator API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at two of the most common modules of the TensorFlow
    API: `tf.data` and `tf.estimator`.'
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow 1.x design was so good that almost nothing changed in TensorFlow
    2.0; in fact, `tf.data` and `tf.estimator` were the first two high-level modules
    introduced during the life cycle of TensorFlow 1.x.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.data` module is a high-level API that allows you to define high-efficiency
    input pipelines without worrying about threads, queues, synchronization, and distributed
    filesystems. The API was designed with simplicity in mind to overcome the usability
    issues of the previous low-level API.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.estimator` API was designed to simplify and standardize machine learning
    programming, allowing to train, evaluate, run inference, and export for serving
    a parametric model, letting the user focus on the model and input definition only.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.data` and `tf.estimator` APIs are fully compatible, and it is highly
    encouraged to use them together. Moreover, as we will see in the next sections,
    every Keras model, the whole eager execution, and even AutoGraph are fully compatible
    with the `tf.data.Dataset` object. This compatibility speeds up the training and
    evaluation phases by defining and using high-efficiency data input pipelines in
    a few lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient data input pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tf.estimator` API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient data input pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is the most critical part of every machine learning pipeline; the model
    learns from it, and its quantity and quality are game-changers of every machine
    learning application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feeding data to a Keras model has so far seemed natural: we can fetch the dataset
    as a NumPy array, create the batches, and feed the batches to the model to train
    it using mini-batch gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the way of feeding the input shown so far is, in fact, hugely inefficient
    and error-prone, for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete dataset can weight several thousands of GBs: no single standard
    computer or even a deep learning workstation has the memory required to load huge
    datasets in memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually creating the input batches means taking care of the slicing indexes
    manually; errors can happen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing data augmentation, applying random perturbations to each input sample,
    slows down the model training phase since the augmentation process needs to complete
    before feeding the data to the model. Parallelizing these operations means you
    worry about synchronization problems among threads and many other common issues
    related to parallel computing. Moreover, the boilerplate code increases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feeding a model whose parameters are on a GPU/TPU from the main Python process
    that resides on the CPU involves loading/unloading data, and this is a process
    that can make the computation suboptimal: the hardware utilization can be below
    100% and is a complete waste.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TensorFlow implementation of the Keras API specification, `tf.keras`, has
    native support for feeding models via the `tf.data` API, as it is possible and
    suggested to use them while using, eager execution, AutoGraph, and estimator API.
  prefs: []
  type: TYPE_NORMAL
- en: Defining an input pipeline is a common practice that can be framed as an ETL
    (Extract Transform and Load) process.
  prefs: []
  type: TYPE_NORMAL
- en: Input pipeline structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Defining a training input pipeline is a standard process; the steps to follow
    can be framed as an **Extract Transform and Load** (**ETL**) process: that is,
    the procedure of copying the data from a data source to a destination system that
    will use it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ETL process consists of the following three steps that the `tf.data.Dataset` object
    allows us to implement easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extract**: Read the data from the data source. It can be either local (persistent
    storage, already loaded in memory) or remote (cloud storage, remote filesystem).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transform**: Apply transformations to the data to clean, augment (random
    crop image, flip, color distortion, add noise), make it interpretable by the model.
    Conclude the transformation by shuffling and batching the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Load**: Load the transformed data into the device that better fits the training
    needs (GPUs or TPUs) and execute the training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These ETL steps can be performed not only during the training phases but also
    during the inference.
  prefs: []
  type: TYPE_NORMAL
- en: If the target device for the training/inference is not the CPU but a different
    device, the `tf.data` API effectively utilizes the CPU, reserving the target device
    for the inference/training of the model; in fact, target devices such as GPUs
    or TPUs make it possible to train parametric models faster, while the CPU is heavily
    utilized for the sequential processing of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: This process, however, is prone to becoming the bottleneck of the whole training
    process since target devices could consume the data at a faster rate than the
    CPU produces it.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.data` API, through its `tf.data.Dataset` class, allows us to easily
    define data input pipelines that transparently solve all the previous issues while
    adding powerful high-level features that make using them a pleasure. Special attention
    has to be given to performance optimization since it is still up to the developer
    to define the ETL process correctly to have 100% usage of the target devices,
    manually removing any bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: The tf.data.Dataset object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `tf.data.Dataset` object represents an input pipeline as a collection of elements
    accompanied by an ordered set of transformations that act on those elements.
  prefs: []
  type: TYPE_NORMAL
- en: Each element contains one or more `tf.Tensor` objects. For example, for an image
    classification problem, the `tf.data.Dataset` elements might be single training
    examples with a pair of tensors representing the image and its associated label.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways of creating a dataset object, depending on the *data
    source*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the data position and format, the `tf.data.Dataset` class offers
    many static methods to use to create a dataset easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensors in memory**: `tf.data.Dataset.from_tensors` or `tf.data.Dataset.from_tensor_slices`.
    In this case, the tensors can be NumPy arrays or `tf.Tensor` objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From a Python generator**: `tf.data.Dataset.from_generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From a list of files that matches a pattern**: `tf.data.Dataset.list_files`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, there are two specializations of the `tf.data.Dataset` object created
    for working with two commonly used file formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.data.TFRecordDataset` to work with the `TFRecord` files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.data.TextLineDataset` to work with text files, reading them line by line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A description of the `TFRecord` file format is presented in the optimization
    section that follows.
  prefs: []
  type: TYPE_NORMAL
- en: Once the dataset object has been constructed, it is possible to transform it
    into a new `tf.data.Dataset` object by chaining method calls. The `tf.data` API
    extensively uses method chaining to naturally express the set of transformations
    applied to the data as a sequence of actions.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow 1.x, it was required to create an iterator node since the input
    pipeline was a member of the computational graph, too. From version 2.0 onward,
    the `tf.data.Dataset` object is iterable, which means you can either enumerate
    its elements using a `for` loop or create a Python iterator using the `iter` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that being iterable does not imply being a Python iterator.
  prefs: []
  type: TYPE_NORMAL
- en: You can loop in a dataset by using a `for` loop, `for value in dataset`, but
    you can't extract elements by using `next(dataset)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, it is possible to use `next(iterator)` after creating an iterator
    by using the Python `iter` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '`iterator = iter(dataset)` `value = next(iterator)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A dataset object is a very flexible data structure that allows creating a dataset
    not only of numbers or a tuple of numbers but of every Python data structure.
    As shown in the next snippet, it is possible to mix Python dictionaries with TensorFlow
    generated values efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The set of transformations the `tf.data.Dataset` object offers through its methods
    supports datasets of any structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we want to define a dataset that produces an unlimited number of
    vectors, each one with 100 elements, of random values (we will do so in the chapter
    dedicated to the GANs, [Chapter 9](66948c53-131c-43ef-a7fc-3d242d1e0664.xhtml),
    *Generative Adversarial Networks*); using `tf.data.Dataset.from_generator`, it
    is possible to do so in a few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The only peculiarity of the `from_generator` method is the need to pass the
    type of the parameters (`tf.float32`, in this case) as the second parameter; this
    is required since to build a graph we need to know the type of the parameters
    in advance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using method chaining, it is possible to create new dataset objects, transforming
    the one just built to get the data our machine learning model expects as input.
    For example, if we want to sum 10 to every component of the noise vector, shuffle
    the dataset content, and create batches of 32 vectors each, we can do so by calling
    just three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `map` method is the most widely used method of the `tf.data.Dataset` object since
    it allows us to apply a function to every element of the input dataset, producing
    a new, transformed dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The `shuffle` method is used in every training pipeline since this transformation
    randomly shuffles the input dataset using a fixed-sized buffer; this means that
    the shuffled data first fetches the `buffer_size` element from its input, then
    shuffles them and produces the output.
  prefs: []
  type: TYPE_NORMAL
- en: The `batch` method gathers the `batch_size` elements from its input and creates
    a batch as output. The only constraint of this transformation is that all elements
    of the batch must have the same shape.
  prefs: []
  type: TYPE_NORMAL
- en: To train a model, it has to be fed with all the elements of the training set
    for multiple epochs. The `tf.data.Dataset` class offers the `repeat(num_epochs)`
    method to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the input data pipeline can be summarized as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/640c3fdc-3c31-47b8-8380-1f321542659d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The diagram shows the typical data input pipeline: the transformation from
    raw data to data ready to be used by the model, just by chaining method calls.
    Prefetching and caching are optimization tips that are explained in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that until not a single word has been said about the concept of
    thread, synchronization, or remote filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: 'All this is hidden by the `tf.data` API:'
  prefs: []
  type: TYPE_NORMAL
- en: The input paths (for example, when using the `tf.data.Dataset.list_files` method)
    can be remote. TensorFlow internally uses the `tf.io.gfile` package, which is
    a file input/output wrapper without thread locking. This module makes it possible
    to read from a local filesystem or a remote filesystem in the same way. For instance,
    it is possible to read from a Google Cloud Storage bucket by using its address
    in the `gs://bucket/` format, without the need to worry about authentication,
    remote requests, and all the boilerplate required to work with a remote filesystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every transformation applied to the data is executed using all the CPU resources
    efficiently—a number of threads equal to the number of CPU cores are created together
    with the dataset object and are used to process the data sequentially and in parallel
    whenever parallel transformation is possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synchronization among these threads is all managed by the `tf.data` API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the transformations described by chaining method calls are executed by threads
    on the CPU that `tf.data.Dataset` instantiates to perform operations that can
    be executed in parallel automatically, which is a great performance boost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, `tf.data.Dataset` is high-level enough to make invisible all the
    threads execution and synchronization, but the automated solution can be suboptimal:
    the target device could be not completely used, and it is up to the user to remove
    the bottlenecks to reach the 100% usage of the target devices.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tf.data` API as shown so far describes a sequential data input pipeline
    that transforms the data from a raw to a useful format by applying transformations.
  prefs: []
  type: TYPE_NORMAL
- en: All these operations are executed on the CPU while the target device (CPUs,
    TPUs, or, in general, the consumer) waits for the data. If the target device consumes
    the data faster than it is produced, there will be moments of 0% utilization of
    the target devices.
  prefs: []
  type: TYPE_NORMAL
- en: In parallel programming, this problem has been solved by using prefetching.
  prefs: []
  type: TYPE_NORMAL
- en: Prefetching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the consumer is working, the producer shouldn't be idle but must work in
    the background to produce the data the consumer will need in the next iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.data` API offers the `prefetch(n)` method to apply a transformation
    that allows overlapping the work of the producer and the consumer. The best practice
    is adding `prefetch(n)` at the end of the input pipeline to overlap the transformation
    performed on the CPU with the computation done on the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing `n` is easy: `n` is the number of elements consumed by a training
    step, and since the vast majority of models are trained using batches of data,
    one batch per training step, then `n=1`.'
  prefs: []
  type: TYPE_NORMAL
- en: The process of reading from disks, especially if reading big files, reading
    from slow HDDs, or using remote filesystems can be time-consuming. Caching is
    often used to reduce this overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Cache elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `cache` transformation can be used to cache the data in memory, completely removing the
    accesses to the data sources. This can bring huge benefits when using remote filesystems,
    or when the reading process is slow. Caching data after the first epoch is only
    possible if the data can fit into memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cache` method acts as a barrier in the transformation pipeline: everything
    executed before the `cache` method is executed only once, thus placing this transformation
    in the pipeline can bring immense benefits. In fact, it can be applied after a
    computationally intensive transformation or after any slow process to speed up
    everything that comes next.'
  prefs: []
  type: TYPE_NORMAL
- en: Using TFRecords
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reading data is a time-intensive process. Often, data can't be read as it is
    stored on the disk linearly, but the files have to be processed and transformed
    to be correctly read.
  prefs: []
  type: TYPE_NORMAL
- en: The `TFRecord` format is a binary and language-agnostic format (defined using
    `protobuf`) for storing a sequence of binary records. TensorFlow allows reading
    and writing `TFRecord` files that are composed of a series of `tf.Example` messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `tf.Example` is a flexible message type that represents a `{"key": value}`
    mapping where `key` is the feature name, and `value` is its binary representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, `tf.Example` could be the dictionary (in pseudocode):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Where a row of the dataset (image, label, together with additional information)
    is serialized as an example and stored inside a `TFRecord` file, in particular,
    the image is not stored using a compression format but directly using its binary
    representation. This allows reading the image linearly, as a sequence of bytes,
    without the need to apply any image decoding algorithm on it, saving time (but
    using disk space).
  prefs: []
  type: TYPE_NORMAL
- en: Before the introduction of `tfds` (TensorFlow Datasets), reading and writing `TFRecord`
    files was a repetitive and tedious process since we had to take care of how to
    serialize and deserialize the input features to be compatible with the `TFRecord`
    binary format. TensorFlow Datasets, that is, a high-level API built over the `TFRecord`
    file specification, standardized the process of high-efficiency dataset creation,
    forcing the creation of the `TFRecord` representation of any dataset. Furthermore, `tfds` already
    contains a lot of ready-to-use datasets correctly stored in the `TFRecord` format,
    and its official guide explains perfectly how to build a dataset, by describing
    its features, to create the `TFRecord` representation of the dataset ready to
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Since the `TFRecord` description and usage goes beyond the scope of this book,
    in the next sections we will cover only the utilization of TensorFlow Datasets.
    For a complete guide on the creation of a TensorFlow Dataset Builder see [Chapter
    8](51f4dcda-add6-4e58-a660-75f34a7e5593.xhtml), *Semantic Segmentation and Custom
    Dataset* *Builder*. If you are interested in the `TFRecord` representation please
    refer to the official documentation:[ https://www.tensorflow.org/beta/tutorials/load_data/tf_records](https://www.tensorflow.org/beta/tutorials/load_data/tf_records).
  prefs: []
  type: TYPE_NORMAL
- en: Building your dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following example shows how to build a `tf.data.Dataset` object using the
    Fashion-MNIST dataset. This is the first complete example of a dataset that uses
    all the best practices described previously; please take the time to understand
    why the method chaining is performed in this way and where the performance optimizations
    have been applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we define the `train_dataset` function, which returns
    the `tf.data.Dataset` object ready to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A training dataset, however, should contain augmented data in order to address
    the overfitting problem. Applying data augmentation on image data is straightforward
    using the TensorFlow `tf.image` package.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ETL process defined so far only transforms the raw data, applying transformations
    that do not change the image content. Data augmentation, instead, requires to
    apply meaningful transformation the raw data with the aim of creating a bigger
    dataset and train, thus, a model more robust to these kinds of variations.
  prefs: []
  type: TYPE_NORMAL
- en: Working with images, it is possible to use the whole API offered by the `tf.image`
    package to augment the dataset. The augmentation step consists in the definition
    of a function and its application to the training set, using the dataset `map`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: The set of valid transformations depends on the dataset—if we were using the
    MNIST dataset, for instance, flipping the input image upside down won't be a good
    idea (nobody wants to feed an image of the number 6 labeled as 9), but since we
    are using the fashion-MNIST dataset we can flip and rotate the input image as
    we like (a pair of trousers remains a pair of trousers, even if randomly flipped
    or rotated).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tf.image` package already contains functions with stochastic behavior,
    designed for data augmentation. These functions apply the transformation to the
    input image with a 50% chance; this is the desired behavior since we want to feed
    the model with both original and augmented images. Thus, a function that applies
    meaningful transformations to the input data can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Applying this augmentation function to the dataset, using the dataset `map`
    method, is left as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: Although it is easy, thanks to the `tf.data` API, building your own datasets
    to benchmark every new algorithm on a standard task (classification, object detection,
    or semantic segmentation) can be a repetitive and, therefore, error-prone process.
    The TensorFlow developers, together with the TensorFlow developer community, standardized
    the *extraction* and *transformation* process of the ETL pipeline, developing
    TensorFlow Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data augmentation functions offered by TensorFlow sometimes are not enough,
    especially when working with small datasets that require a lot of argumentation
    to become useful. There are many data augmentation libraries written in Python
    that can be easily integrated into the dataset augmentation step. Two of the most
    common are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '- **imgaug**: [https://github.com/aleju/imgaug](https://github.com/aleju/imgaug)'
  prefs: []
  type: TYPE_NORMAL
- en: '- **albumentations**: [https://github.com/albu/albumentations](https://github.com/albu/albumentations)'
  prefs: []
  type: TYPE_NORMAL
- en: Using `tf.py_function` it is possible to execute Python code inside the `map`
    method, and thus use these libraries to generate a rich set of transformations
    (not offered by the `tf.image` package).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Datasets – tfds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow Datasets is a collection of ready-to-use datasets that handle the
    downloading and preparation phases of the ETL process, constructing a `tf.data.Dataset` object.
  prefs: []
  type: TYPE_NORMAL
- en: The significant advantage this project has brought to machine learning practitioners
    is the extreme simplification of the data download and preparation of the most
    commonly used benchmark dataset.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Datasets (`tfds`) not only downloads and converts the dataset to
    a standard format but also locally converts the dataset to its `TFRecord` representation,
    making the reading from disk highly efficient and giving the user a `tf.data.Dataset`
    object that reads from `TFRecord` and is ready to use. The API comes with the
    concept of a builder***. ***Every builder is an available dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Different from the `tf.data` API, TensorFlow Datasets comes as a separate package
    that needs to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Being a Python package, installing it using `pip` is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: That's it. The package is lightweight since all the datasets are downloaded
    only when needed.
  prefs: []
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The package comes with two main methods: `list_builders()` and `load()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`list_builders()` returns the list of the available datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load(name, split)` accepts the name of an available builder and the desired
    split. The split value depends on the builder since every builder carries its
    information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using `tfds` to load the train and test splits of MNIST, in the list of the
    available builders, is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In a single line of code, we downloaded, processed, and converted the dataset
    to TFRecord, and created two `tf.data.Dataset` objects to read them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this single line of code, we don''t have any information about the dataset
    itself: no clue about the data type of the returned objects, the shape of the
    images and labels, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To gain a complete description of the whole dataset, it is possible to use
    the builder associated with the dataset and print the `info` property; this property
    contains all the information required to work with the dataset, from the academic
    citation to the data format:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing it, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That's all we need.
  prefs: []
  type: TYPE_NORMAL
- en: Using `tfds` is highly encouraged; moreover, since the `tf.data.Dataset` objects
    are returned, there is no need to learn how to use another fancy API as the `tf.data` API
    is the standard, and we can use it everywhere in TensorFlow 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Keras integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset objects are natively supported by the TensorFlow implementation of the
    Keras `tf.keras` specification. This means that using NumPy arrays or using a `tf.data.Dataset`
    object is the same when it comes to training/evaluating a model. The classification
    model defined in [Chapter 4](655b734e-1636-4e11-b944-a71fafacb977.xhtml), *TensorFlow
    2.0 Architecture,* using the `tf.keras.Sequential` API, can be trained more quickly
    using the `tf.data.Dataset` object created by the `train_dataset` function previously
    defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we just use the standard `.compile` and `.fit` method
    calls, to compile (define the training loop) and fit the dataset (that is a `tf.data.Dataset`):'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow 2.0, being eager by default, natively allows iterating over a `tf.data.Dataset`
    object to build our own custom training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Eager integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `tf.data.Dataset` object is iterable, which means one can either enumerate
    its elements using a for loop or create a Python iterator using the `iter` keyword.
    Please note that being iterable does not imply being a Python iterator as pointed
    out at the beginning of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Iterating over a dataset object is extremely easy: we can use the standard
    Python `for` loop to extract a batch at each iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the input pipeline by using a dataset object is a better solution
    than the one used so far.
  prefs: []
  type: TYPE_NORMAL
- en: The manual process of extracting elements from a dataset by computing the indices
    is error-prone and inefficient, while the `tf.data.Dataset` objects are highly-optimized.
    Moreover, the dataset objects are fully compatible with `tf.function`, and therefore
    the whole training loop can be graph-converted and accelerated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the lines of code get reduced a lot, increasing the readability.
    The following code block represents the graph-accelerated (via `@tf.function`)
    custom training loop from the previous chapter, [Chapter 4](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=28&action=edit#post_27), *TensorFlow
    2.0 Architecture*; the loop uses the `train_dataset` function defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You are invited to read the source code carefully and compare it with the custom
    training loop from the previous chapter, [Chapter 4](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=28&action=edit#post_27), *TensorFlow
    2.0 Architecture*.
  prefs: []
  type: TYPE_NORMAL
- en: Estimator API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw how the `tf.data` API simplifies and standardizes
    the input pipeline definition. Also, we saw that the `tf.data` API is completely
    integrated into the TensorFlow Keras implementation and the eager or graph-accelerated
    version of a custom training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as for the input data pipelines, there are a lot of repetitive parts in
    the whole machine learning programming. In particular, after defining the first
    version of the machine learning model, the practitioner is interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After many iterations of these points, exporting the trained model for serving
    is the natural consequence.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, defining a training loop, the evaluation process, and the predicting
    process are very similar for each machine learning process. For example, for a
    predictive model, we are interested in training the model for a certain number
    of epochs, measuring a metric on the training and validation set at the end of
    the process, and repeating this process, changing the hyperparameters until the
    results are satisfactory.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify machine learning programming and help the developer to focus on
    the nonrepetitive parts of the process, TensorFlow introduced the concept of Estimator
    through the `tf.estimator` API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tf.estimator` API is a high-level API that encapsulates the repetitive
    and standard processes of the machine learning pipeline. For more information
    on estimators, see the official documentation ([https://www.tensorflow.org/guide/estimators](https://www.tensorflow.org/guide/estimators)).
    Here are the main advantages estimators bring:'
  prefs: []
  type: TYPE_NORMAL
- en: You can run Estimator-based models on a local host or a distributed multiserver
    environment without changing your model. Furthermore, you can run Estimator-based
    models on CPUs, GPUs, or TPUs without recoding your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimators simplify sharing implementations between model developers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can develop a state-of-the-art model with high-level, intuitive code. In
    short, it is generally much easier to create models with Estimators than with
    the low-level TensorFlow APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimators are themselves built on `tf.keras.layers`, which simplifies customization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimators build the graph for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Estimators provide a safely distributed training loop that controls how and
    when to:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the graph
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize variables
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Load data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle exceptions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create checkpoint files and recover from failures
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Save summaries for TensorBoard
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/afe1dc5d-85d3-4366-8fae-faff6b191706.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Estimator API is built upon the TensorFlow mid-level layers; in particular,
    estimators themselves are built using the Keras layers in order to simplify the
    customization. Image credits: tensorflow.org'
  prefs: []
  type: TYPE_NORMAL
- en: The standardization process of the machine learning pipeline passes through
    the definition of a class that describes it: `tf.estimator.Estimator`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this class, you need to use a well-defined programming model that is
    enforced by the public methods of the `tf.estimator.Estimator` object, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61f4769c-873f-4ca0-9a5a-299c1cbe50d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The estimator programming model is enforced by the Estimator object public
    methods; the API itself takes care of the checkpoint saving and reloading; the
    user must implement only the input function and the model itself; the standard
    processes of training, evaluate, and predict are implemented by the API. Image
    credits: tensorflow.org'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to use the Estimator API in two different ways: building custom
    Estimators or using premade Estimators.'
  prefs: []
  type: TYPE_NORMAL
- en: Premade and custom Estimators follow the same programming model; the only difference
    is that in custom Estimators the user must write a `model_fn` model function,
    while in the premade Estimator the model definition comes for free (at the cost
    of lower flexibility).
  prefs: []
  type: TYPE_NORMAL
- en: 'The programming model the Estimator API forces you to use consists of the implementation
    of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of the data input pipeline, implementing the `input_fn` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (optional) The implementation of the model, handling the training, evaluation,
    and predict cases, and implementing the `model_fn` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the documentation talks about graphs**. **In fact, to guarantee
    high performance, the Estimator API is built upon the (hidden) graph representation.
    Even if TensorFlow 2.0 defaults on the eager execution paradigm, neither `model_fn`
    and `input_fn` are executed eagerly, the Estimator switches to graph mode before
    calling these functions, which is why the code has to be compatible with graph
    mode execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, the Estimator API is a standardization of the good practice of separating
    the data from the model. This is well highlighted by the constructor of the `tf.estimator.Estimator`
    object, which is the subject of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It is worth noticing that there is no mention of `input_fn` in the constructor,
    and this makes sense since the input can change during the estimator's lifetime,
    whereas the model can't.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how the `input_fn` function should be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Data input pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Firstly, let''s look at the standard ETL process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extract**: Read the data from the data source. It can be either local (persistent
    storage, already loaded in memory) or remote (Cloud Storage, remote filesystem).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transform**: Apply transformations to the data to clean, augment (random
    crop image, flip, color distortion, adding noise), and make the data interpretable
    by the model. Conclude the transformation by shuffling and batching the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load**: Load the transformed data into the device that better fits the training
    needs (GPUs or TPUs) and execute the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tf.estimator.Estimator` API merges the first two phases in the implementation
    of the `input_fn` function passed to the `train` and `evaluate` methods.
  prefs: []
  type: TYPE_NORMAL
- en: The `input_fn` function is a Python function that returns a `tf.data.Dataset`
    object, which yields the `features` and `labels` objects consumed by the model,
    that's all.
  prefs: []
  type: TYPE_NORMAL
- en: 'As known from the theory presented in [Chapter 1](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml),
    *What is machine learning?, *the correct way of using a dataset is to split it
    into three non-overlapping parts: training, validation, and test set.'
  prefs: []
  type: TYPE_NORMAL
- en: To correctly implement it, it is suggested to define an input function that
    accepts an input parameter able to change the returned `tf.data.Dataset` object,
    returning a new function to pass as input to the Estimator object. The estimator
    API comes with the concept of *m**ode*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model, and dataset too, can be in a different mode, depending on which
    phases of the pipeline we are at. The mode is implemented in the `enum` type `tf.estimator.ModeKeys`,
    which contains the three standard keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TRAIN`: Training mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EVAL`: Evaluation mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PREDICT`: Inference mode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is thus possible to use a `tf.estimator.ModeKeys` input variable to change
    the returned dataset (the fact that this is not required by the Estimator API
    is something that comes in handy).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we are interested in defining the correct input pipeline for a classification
    model of the fashion-MNIST dataset, we just have to get the data, split the dataset
    (since the evaluation set is not provided, we halve the test set), and build the
    dataset object we need.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input signature of the input function is completely up to the developer;
    this freedom allows us to define the dataset objects parametrically by passing
    every dataset parameter as function inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining the input function, the programming model introduced by the
    Estimator API gives us two choices: create our own custom estimator by manually
    defining the model to train, or use the so-called canned or premade estimators.'
  prefs: []
  type: TYPE_NORMAL
- en: Custom estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Premade and custom estimators share a common architecture: both aim to build
    a `tf.estimator.EstimatorSpec` object that fully defines the model to be run by `tf.estimator.Estimator`;
    the return value of any `model_fn` is, therefore, the Estimator specification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `model_fn` function follows this signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The function parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`features` is the first item returned from `input_fn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` is the second item returned from `input_fn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` is the `tf.estimator.ModeKeys` object that specifies the status of the
    model, if it is in the training, evaluation, or prediction phase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params` is a dictionary of hyperparameters that can be used to tune the model
    easily'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` is a `tf.estimator.RunConfig` object that allows you to configure
    parameters related to the runtime execution, such as the model parameters directory
    and the number of distributed nodes to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `features`, `labels`, and `mode` are the most important part of the `model_fn`
    definition, and that the signature of `model_fn` must use these parameter names;
    otherwise, a `ValueError` exception is raised.
  prefs: []
  type: TYPE_NORMAL
- en: The requirement of having a complete match with the input signature is proof
    that estimators must be used in standard scenarios when the whole machine learning
    pipeline can get a huge speedup from this standardization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goals of `model_fn` are twofold: it has to define the model using Keras,
    and define its behavior during the various `mode`. The way to specify the behavior
    is to return a correctly built `tf.estimator.EstimatorSpec`.'
  prefs: []
  type: TYPE_NORMAL
- en: Since even writing the model function is straightforward using the Estimator
    API, a complete implementation of a classification problem solution using the
    Estimator API follows. The model definition is pure Keras, and the function used
    is the `make_model(num_classes)` previously defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are invited to look carefully at how the behavior of the model changes
    when the `mode` parameter changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Important**: The Estimator API, although present in TensorFlow 2.0, still
    works in graph mode. `model_fn`, thus, can use Keras to build the model, but the
    training and summary logging operation must be defined using the `tf.compat.v1` compatibility
    module.'
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [Chapter 3](f62be9d4-c8e0-4590-8299-2fdad139830f.xhtml), *TensorFlow
    Graph Architecture*, for a better understanding of the graph definition.`(tf2)`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `model_fn` function works exactly like a standard graph model of TensorFlow
    1.x; the whole model behavior (three possible scenarios) is encoded inside the
    function, inside the Estimator specification the function returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few lines of code are required to train and evaluate the model performance
    at the end of every training epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The loop for 50 epochs shows that the estimator API takes care of restoring
    the model parameters and saving them at the end of each `.train` invocation, without
    any user intervention, all automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'By running `TensorBoard --logdir log`, it is possible to see the loss and accuracy
    trends. The orange is the training run while the blue is the validation run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59a37b54-2e87-46d8-9110-3fc4e2f94fa7.png)'
  prefs: []
  type: TYPE_IMG
- en: The validation accuracy and the training and validation loss values as shown
    in TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: Writing custom estimators requires you to think about the TensorFlow graph architecture
    and use them as in the 1.x version.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow 2.0, as in the 1.x version, it is possible to define computational
    graphs by using premade estimators that define the `model_fn` function automatically,
    without the need to think in the graph-way.
  prefs: []
  type: TYPE_NORMAL
- en: Premade estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow 2.0 has two different kinds of premade Estimators: the one automatically
    created from the Keras model definition, and the canned-estimators built upon
    the TensorFlow 1.x API.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a Keras model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recommended way of constructing an Estimator object in TensorFlow 2.0 is
    to use a Keras model itself.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.keras.estimator` package offers all the tools required to automatically
    convert a `tf.keras.Model` object to its Estimator counterpart. In fact, when
    a Keras model is compiled, the whole training and evaluation loops are defined;
    it naturally follows that the `compile` method almost defines an Estimator-like
    architecture that the `tf.keras.estimator` package is able to use.
  prefs: []
  type: TYPE_NORMAL
- en: Even when using Keras, you must always define the `tf.estimator.EstimatorSpec`
    objects that define the `input_fn` function to use during the training and evaluation
    phases.
  prefs: []
  type: TYPE_NORMAL
- en: There is no need to define a single `EstimatorSpec` object for both cases, but
    it is possible and recommended to use `tf.estimator.TrainSpec` and `tf.estimator.EvalSpec`
    to define the behavior of the model separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, given the usual `make_model(num_classes)` function, which creates
    a Keras model, it is really easy to define the specs and convert the model to
    an estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Using a canned estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Model architectures are pretty much standard: convolutional neural networks
    are made of convolutional layers interleaved by pooling layers; fully connected
    neural networks are made by a stack of dense layers, each with a different number
    of hidden units, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.estimator` package comes with a huge list of premade models, ready to
    use. The full list is available in the documentation: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator)[.](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator)
  prefs: []
  type: TYPE_NORMAL
- en: The process of the input function definition is pretty similar to what has been
    described so far; the main difference is that instead of feeding the model with
    the data as is, the canned Estimator requires an input description using feature
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: Feature columns are intermediaries between the `tf.data.Dataset` object and
    the Estimator. In fact, they can be used to apply standard transformations to
    the input data, working exactly like an additional `.map` method added to the
    input pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the `tf.estimator` API was added to TensorFlow 2.0 because of
    the popularity of the Estimator-based solution in 1.x, but this package lacks
    many features that a Keras-based or pure TensorFlow with eager execution plus
    AutoGraph offers. When TensorFlow 1.x was the standard, it was tough and time-consuming
    to experiment with many standard solutions and to manually define several standard
    computational graphs; that's why the Estimator package gained popularity quickly.
    Using TensorFlow 2.0 in eager mode and defining models using Keras, instead, allows
    you to prototype and experiment with many different solutions easily. Moreover,
    the `tf.data` API is so flexible that correctly defining the input pipeline is
    straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, canned Estimators are only cited in this book. This knowledge
    is not mandatory, and there is a high chance that in future versions of TensorFlow
    the, `tf.estimator` package will be removed or moved to a separate project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, two of the most widely used high-level APIs were presented.
    `tf.estimator` and `tf.data` APIs have maintained almost the same structure they
    had in TensorFlow 1.x since they were designed with simplicity in mind.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.data` API, through `tf.data.Dataset`, allows you to define a high-efficiency
    data input pipeline by chaining transformations in an ETL fashion, using the method
    chaining paradigm. `tf.data.Dataset` objects are integrated with every part of
    TensorFlow, from eager execution to AutoGraph, passing through the training methods
    of Keras models and the Estimator API. The ETL process is made easy and the complexity
    is hidden.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Datasets is the preferred way of creating a new `tf.data.Dataset` object,
    and is the perfect tool to use when a machine learning model has been developed,
    and it is time to measure the performance on every publicly available benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: The Estimator API standardized machine learning programming but reduces flexibility
    while increasing productivity. In fact, it is the perfect tool to use to define
    the input pipeline once and to test with different standard models if a solution
    to the problem can be easily found.
  prefs: []
  type: TYPE_NORMAL
- en: The custom estimators, on the other hand, are the perfect tool to use when a
    non-standard architecture could solve the problem, but the training process is
    the standard one. Instead of wasting time rewriting the training loops, the metrics
    measurements, and all the standard machine learning training pipeline, you can
    focus only on the model definition. The `tf.estimator` and `tf.data` APIs are
    two powerful tools TensorFlow offers, and using them together speeds up the development
    a lot. The whole path from the development to the production is handled by these
    tools, making putting a model into production almost effortless.
  prefs: []
  type: TYPE_NORMAL
- en: This is the last chapter dedicated to the TensorFlow framework architecture.
    In the following chapters, we will look at several machine learning tasks, all
    of them with an end-to-end TensorFlow 2.0 solution. During the hands-on solution,
    we will use other features of TensorFlow 2.0, such as the integration of TensorFlow
    Hub with the Keras framework. The following chapters are a complete tutorial on
    how to use TensorFlow 2.0 to solve a certain machine learning task using neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once again, you are invited to answer all the following questions. You will struggle
    to find answers when the problems are hard, since this is the only way to master
    Estimator and Data APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an ETL process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is an ETL process related to the `tf.data` API?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why can't a `tf.data.Dataset` object can't be manipulated directly, but every
    non-static method returns a new dataset object that's the result of the transformation
    applied?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which are the most common optimizations in the context of the `tf.data` API?
    Why is prefetching so important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the two datasets of the next question, which one loops faster? Explain
    your response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Given the following two datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Can functions `l1` and `l2` be converted to their graph representations using
    `@tf.function`? Analyze the resulting code using the `tf.autograph` module to
    explain the answer.
  prefs: []
  type: TYPE_NORMAL
- en: When should the `tf.data.Dataset.cache` method be used?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `tf.io.gfile` package to store an uncompressed copy of the fashion-MNIST
    dataset locally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `tf.data.Dataset` object reading the files created in the previous
    point; use the `tf.io.gfile` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the complete example of the previous chapter to `tf.data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the complete example of the previous chapter to `tf.Estimator`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use `tfds` to load the `"cat_vs_dog"` dataset. Look at its builder information:
    it''s a single split dataset. Split it in three non-overlapping parts: the training
    set, the validation set, and the test set, using the `tf.data.Dataset.skip` and `tf.data.dataset.take`
    methods. Resize every image to `32x32x3`, and swap the labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the three datasets created previously to define `input_fn`, which chooses
    the correct split when the `mode` changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a custom `model_fn` function using a simple convolutional neural network
    to classify cats and dogs (with swapped labels). Log the results on TensorBoard
    and measure the accuracy, the loss value, and the distribution of the output neuron
    on the validation set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a canned estimator to solve question 11\. Is it possible to reproduce the
    same solution developed using a custom `model_fn` function with a premade Estimator?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the accuracy and validation loss curves shown in the section dedicated
    to the custom Estimator, it is possible to see that the model is not behaving
    correctly; what is the name of this pathological condition and how can it be mitigated?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to reduce the pathological condition of the model (referred to in the previous
    question) by tweaking the `loss` and/or changing the model architecture. Your
    solution should reach at least a validation accuracy value of 0.96.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
