["```\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\nn_classes = 10\nmodel = tf.keras.Sequential([\n tf.keras.layers.Conv2D(\n 32, (5, 5), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n tf.keras.layers.MaxPool2D((2, 2), (2, 2)),\n tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n tf.keras.layers.MaxPool2D((2, 2), (2, 2)),\n tf.keras.layers.Flatten(),\n tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n tf.keras.layers.Dropout(0.5),\n tf.keras.layers.Dense(n_classes)\n])\n\nmodel.summary()\n\n(train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n# Scale input in [-1, 1] range\ntrain_x = train_x / 255\\. * 2 - 1\ntest_x = test_x / 255\\. * 2 - 1\ntrain_x = tf.expand_dims(train_x, -1).numpy()\ntest_x = tf.expand_dims(test_x, -1).numpy()\n\nmodel.compile(\n optimizer=tf.keras.optimizers.Adam(1e-5),\n loss='sparse_categorical_crossentropy',\n metrics=['accuracy'])\n\nmodel.fit(train_x, train_y, epochs=10)\nmodel.evaluate(test_x, test_y)\n```", "```\nModel: \"sequential\"\n__________________________________________________\nLayer   (type)     Output Shape         Param #\n==================================================\nconv2d (Conv2D) (None, 24, 24, 32) 832 \n__________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 12, 12, 32) 0 \n__________________________________________________\nconv2d_1 (Conv2D) (None, 10, 10, 64) 18496 \n__________________________________________________\nmax_pooling2d_1 (MaxPooling2D) (None, 5, 5, 64) 0 \n__________________________________________________\nflatten (Flatten) (None, 1600) 0 \n__________________________________________________\ndense (Dense) (None, 1024) 1639424 \n__________________________________________________\ndropout (Dropout) (None, 1024) 0 \n__________________________________________________\ndense_1 (Dense) (None, 10) 10250 \n==================================================\nTotal params: 1,669,002\nTrainable params: 1,669,002\nNon-trainable params: 0\n```", "```\nEpoch 1/10\n60000/60000 [================] - 126s 2ms/sample - loss: 1.9142 - accuracy: 0.4545\nEpoch 2/10\n60000/60000 [================] - 125s 2ms/sample - loss: 1.3089 - accuracy: 0.6333\nEpoch 3/10\n60000/60000 [================] - 129s 2ms/sample - loss: 1.1676 - accuracy: 0.6824\n[ ... ]\nEpoch 10/10\n60000/60000 [================] - 130s 2ms/sample - loss: 0.8645 - accuracy: 0.7618\n\n10000/10000 [================] - 6s 644us/sample - loss: 0.7498 - accuracy: 0.7896\n```", "```\nimport tensorflow as tf\n\ninput_shape = (100,)\ninputs = tf.keras.layers.Input(input_shape)\nnet = tf.keras.layers.Dense(units=64, activation=tf.nn.elu, name=\"fc1\")(inputs)\nnet = tf.keras.layers.Dense(units=64, activation=tf.nn.elu, name=\"fc2\")(net)\nnet = tf.keras.layers.Dense(units=1, name=\"G\")(net)\nmodel = tf.keras.Model(inputs=inputs, outputs=net)\n```", "```\nimport tensorflow as tf\n\nclass Generator(tf.keras.Model):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.dense_1 = tf.keras.layers.Dense(\n            units=64, activation=tf.nn.elu, name=\"fc1\")\n        self.dense_2 = f.keras.layers.Dense(\n            units=64, activation=tf.nn.elu, name=\"fc2\")\n        self.output = f.keras.layers.Dense(units=1, name=\"G\")\n\n    def call(self, inputs):\n        # Build the model in functional style here\n        # and return the output tensor\n        net = self.dense_1(inputs)\n        net = self.dense_2(net)\n        net = self.output(net)\n        return net\n```", "```\nimport tensorflow as tf\n\nA = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\nx = tf.constant([[0, 10], [0, 0.5]])\nb = tf.constant([[1, -1]], dtype=tf.float32)\ny = tf.add(tf.matmul(A, x), b, name=\"result\") #y = Ax + b\n\nwith tf.Session() as sess:\n    print(sess.run(y))\n```", "```\n[[ 1\\. 10.]\n [ 1\\. 31.]]\n```", "```\nimport tensorflow as tf\n\nA = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\nx = tf.constant([[0, 10], [0, 0.5]])\nb = tf.constant([[1, -1]], dtype=tf.float32)\ny = tf.add(tf.matmul(A, x), b, name=\"result\")\nprint(y)\n```", "```\ntf.Tensor(\n[[ 1\\. 10.]\n [ 1\\. 31.]], shape=(2, 2), dtype=float32)\n```", "```\nprint(y.numpy())\n```", "```\nimport tensorflow as tf\n\ndef multiply(x, y):\n    \"\"\"Matrix multiplication.\n    Note: it requires the input shape of both input to match.\n    Args:\n        x: tf.Tensor a matrix\n        y: tf.Tensor a matrix\n    Returns:\n        The matrix multiplcation x @ y\n    \"\"\"\n\n    assert x.shape == y.shape\n    return tf.matmul(x, y)\n\ndef add(x, y):\n    \"\"\"Add two tensors.\n    Args:\n        x: the left hand operand.\n        y: the right hand operand. It should be compatible with x.\n    Returns:\n        x + y\n    \"\"\"\n    return x + y\n\ndef main():\n    \"\"\"Main program.\"\"\"\n    A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n    x = tf.constant([[0, 10], [0, 0.5]])\n    b = tf.constant([[1, -1]], dtype=tf.float32)\n\n    z = multiply(A, x)\n    y = add(z, b)\n    print(y)\n\nif __name__ == \"__main__\":\n    main()\n```", "```\nimport tensorflow as tf\n\ndef count_op():\n    \"\"\"Print the operations define in the default graph\n    and returns their number.\n    Returns:\n        number of operations in the graph\n    \"\"\"\n    ops = tf.get_default_graph().get_operations()\n    print([op.name for op in ops])\n    return len(ops)\n\nA = tf.constant([[1, 2], [3, 4]], dtype=tf.float32, name=\"A\")\nx = tf.constant([[0, 10], [0, 0.5]], name=\"x\")\nb = tf.constant([[1, -1]], dtype=tf.float32, name=\"b\")\n\nassert count_op() == 3\ndel A\ndel x\ndel b\nassert count_op() == 0 # FAIL!\n```", "```\nA = tf.get_default_graph().get_tensor_by_name(\"A:0\")\nx = tf.get_default_graph().get_tensor_by_name(\"x:0\")\nb = tf.get_default_graph().get_tensor_by_name(\"b:0\")\n```", "```\nimport tensorflow as tf\n\ndef get_y():\n    A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32, name=\"A\")\n    x = tf.constant([[0, 10], [0, 0.5]], name=\"x\")\n    b = tf.constant([[1, -1]], dtype=tf.float32, name=\"b\")\n    # I don't know what z is: if is a constant or a variable\n    z = tf.get_default_graph().get_tensor_by_name(\"z:0\")\n    y = A @ x + b - z\n    return y\n\ntest = tf.Variable(10., name=\"z\")\ndel test\ntest = tf.constant(10, name=\"z\")\ndel test\n\ny = get_y()\n\nwith tf.Session() as sess:\n    print(sess.run(y))\n```", "```\nimport tensorflow as tf\n\nx = tf.Variable(1, dtype=tf.int32)\ny = tf.Variable(2, dtype=tf.int32)\n\nassign_op = tf.assign_add(y, 1)\nout = x * y\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for _ in range(5):\n        print(sess.run(out))\n```", "```\nwith tf.control_dependencies([assign_op]):\n    out = x * y\n```", "```\nimport tensorflow as tf\n\nx = tf.Variable(1, dtype=tf.int32)\ny = tf.Variable(2, dtype=tf.int32)\n\nfor _ in range(5):\n    y.assign_add(1)\n    out = x * y\n    print(out)\n```", "```\nx = tf.constant(4.0)\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    y = tf.pow(x, 2)\n# Will compute 8 = 2*x, x = 8\ndy_dx = tape.gradient(y, x)\n```", "```\nx = tf.Variable(4.0)\ny = tf.Variable(2.0)\nwith tf.GradientTape(persistent=True) as tape:\n    z = x + y\n    w = tf.pow(x, 2)\ndz_dy = tape.gradient(z, y)\ndz_dx = tape.gradient(z, x)\ndw_dx = tape.gradient(w, x)\nprint(dz_dy, dz_dx, dw_dx) # 1, 1, 8\n# Release the resources\ndel tape\n```", "```\ngradients = gradients * threshold / l2(gradients)\n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\ndef make_model(n_classes):\n return tf.keras.Sequential([\n   tf.keras.layers.Conv2D(\n     32, (5, 5), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n   tf.keras.layers.MaxPool2D((2, 2), (2, 2)),\n   tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n   tf.keras.layers.MaxPool2D((2, 2), (2, 2)),\n   tf.keras.layers.Flatten(),\n   tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n   tf.keras.layers.Dropout(0.5),\n   tf.keras.layers.Dense(n_classes)\n ])\n```", "```\ndef load_data():\n    (train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n    # Scale input in [-1, 1] range\n    train_x = tf.expand_dims(train_x, -1)\n    train_x = (tf.image.convert_image_dtype(train_x, tf.float32) - 0.5) * 2\n    train_y = tf.expand_dims(train_y, -1)\n\n    test_x = test_x / 255\\. * 2 - 1\n    test_x = (tf.image.convert_image_dtype(test_x, tf.float32) - 0.5) * 2\n    test_y = tf.expand_dims(test_y, -1)\n\n    return (train_x, train_y), (test_x, test_y)\n```", "```\ndef train():\n    # Define the model\n    n_classes = 10\n    model = make_model(n_classes)\n\n    # Input data\n    (train_x, train_y), (test_x, test_y) = load_data()\n\n    # Training parameters\n    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n    step = tf.Variable(1, name=\"global_step\")\n    optimizer = tf.optimizers.Adam(1e-3)\n    accuracy = tf.metrics.Accuracy()\n```", "```\n    # Train step function\n    def train_step(inputs, labels):\n        with tf.GradientTape() as tape:\n            logits = model(inputs)\n            loss_value = loss(labels, logits)\n\n        gradients = tape.gradient(loss_value, model.trainable_variables)\n        # TODO: apply gradient clipping here\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        step.assign_add(1)\n\n        accuracy_value = accuracy(labels, tf.argmax(logits, -1))\n        return loss_value, accuracy_value\n\n    epochs = 10\n    batch_size = 32\n    nr_batches_train = int(train_x.shape[0] / batch_size)\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Number of batches per epoch: {nr_batches_train}\")\n\n    for epoch in range(epochs):\n        for t in range(nr_batches_train):\n            start_from = t * batch_size\n            to = (t + 1) * batch_size\n            features, labels = train_x[start_from:to], train_y[start_from:to]\n            loss_value, accuracy_value = train_step(features, labels)\n            if t % 10 == 0:\n                print(\n                    f\"{step.numpy()}: {loss_value} - accuracy: {accuracy_value}\"\n                )\n        print(f\"Epoch {epoch} terminated\")\n\nif __name__ == \"__main__\":\n    train()\n```", "```\nckpt = tf.train.Checkpoint(step=step, optimizer=optimizer, model=model)\nmanager = tf.train.CheckpointManager(ckpt, './checkpoints', max_to_keep=3)\nckpt.restore(manager.latest_checkpoint)\nif manager.latest_checkpoint:\n    print(f\"Restored from {manager.latest_checkpoint}\")\nelse:\n    print(\"Initializing from scratch.\")\n```", "```\nsave_path = manager.save()\nprint(f\"Checkpoint saved: {save_path}\")\n```", "```\nsummary_writer = tf.summary.create_file_writer(path)\n```", "```\nmean_loss = tf.metrics.Mean(name='loss')\n```", "```\ndef train():\n    # Define the model\n    n_classes = 10\n    model = make_model(n_classes)\n\n    # Input data\n    (train_x, train_y), (test_x, test_y) = load_data()\n\n    # Training parameters\n    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n    step = tf.Variable(1, name=\"global_step\")\n    optimizer = tf.optimizers.Adam(1e-3)\n\n    ckpt = tf.train.Checkpoint(step=step, optimizer=optimizer, model=model)\n    manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)\n    ckpt.restore(manager.latest_checkpoint)\n    if manager.latest_checkpoint:\n        print(f\"Restored from {manager.latest_checkpoint}\")\n    else:\n        print(\"Initializing from scratch.\")\n\n    accuracy = tf.metrics.Accuracy()\n    mean_loss = tf.metrics.Mean(name='loss')\n```", "```\n     # Train step function\n     def train_step(inputs, labels):\n         with tf.GradientTape() as tape:\n             logits = model(inputs)\n             loss_value = loss(labels, logits)\n\n         gradients = tape.gradient(loss_value, model.trainable_variables)\n         # TODO: apply gradient clipping here\n         optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n         step.assign_add(1)\n\n         accuracy.update_state(labels, tf.argmax(logits, -1))\n         return loss_value, accuracy.result()\n\n    epochs = 10\n    batch_size = 32\n    nr_batches_train = int(train_x.shape[0] / batch_size)\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Number of batches per epoch: {nr_batches_train}\")\n    train_summary_writer = tf.summary.create_file_writer('./log/train')\n    with train_summary_writer.as_default():\n        for epoch in range(epochs):\n            for t in range(nr_batches_train):\n                start_from = t * batch_size\n                to = (t + 1) * batch_size\n\n                features, labels = train_x[start_from:to], train_y[start_from:to]\n\n                loss_value, accuracy_value = train_step(features, labels)\n                mean_loss.update_state(loss_value)\n\n                if t % 10 == 0:\n                    print(f\"{step.numpy()}: {loss_value} - accuracy: {accuracy_value}\")\n                    save_path = manager.save()\n                    print(f\"Checkpoint saved: {save_path}\")\n                    tf.summary.image(\n                        'train_set', features, max_outputs=3, step=step.numpy())\n                    tf.summary.scalar(\n                        'accuracy', accuracy_value, step=step.numpy())\n                    tf.summary.scalar(\n                        'loss', mean_loss.result(), step=step.numpy())\n                    accuracy.reset_states()\n                    mean_loss.reset_states()\n            print(f\"Epoch {epoch} terminated\")\n            # Measuring accuracy on the whole training set at the end of the epoch\n            for t in range(nr_batches_train):\n                start_from = t * batch_size\n                to = (t + 1) * batch_size\n                features, labels = train_x[start_from:to], train_y[start_from:to]\n                logits = model(features)\n                accuracy.update_state(labels, tf.argmax(logits, -1))\n            print(f\"Training accuracy: {accuracy.result()}\")\n            accuracy.reset_states()\n```", "```\nValueError: tf.function-decorated function tried to create variables on non-first call.\n```", "```\ndef f():\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    b = tf.Variable(12.)\n    y = tf.matmul(a, x) + b\n    return y\n```", "```\n@tf.function\ndef f(b):\n    a = tf.constant([[10,10],[11.,1.]])\n    x = tf.constant([[1.,0.],[0.,1.]])\n    y = tf.matmul(a, x) + b\n    return y\n\nvar = tf.Variable(12.)\nf(var)\nf(15)\nf(tf.constant(1))\n```", "```\nb = None\n\n@tf.function\ndef f():\n    a = tf.constant([[10, 10], [11., 1.]])\n    x = tf.constant([[1., 0.], [0., 1.]])\n    global b\n    if b is None:\n        b = tf.Variable(12.)\n    y = tf.matmul(a, x) + b\n    return y\n\nf()\n```", "```\nclass F():\n    def __init__(self):\n        self._b = None\n\n    @tf.function\n    def __call__(self):\n        a = tf.constant([[10, 10], [11., 1.]])\n        x = tf.constant([[1., 0.], [0., 1.]])\n        if self._b is None:\n            self._b = tf.Variable(12.)\n        y = tf.matmul(a, x) + self._b\n        return y\n\nf = F()\nf()\n```", "```\n@tf.function\ndef train_step(inputs, labels):\n# function body\n```", "```\nimport tensorflow as tf\n\n@tf.function\ndef f():\n    x = 0\n    for i in range(10):\n        print(i)\n        x += i\n    return x\n\nf()\nprint(tf.autograph.to_code(f.python_function))\n```", "```\ndef tf__f():\n  try:\n    with ag__.function_scope('f'):\n      do_return = False\n      retval_ = None\n      x = 0\n\n      def loop_body(loop_vars, x_1):\n        with ag__.function_scope('loop_body'):\n          i = loop_vars\n          with ag__.utils.control_dependency_on_returns(ag__.print_(i)):\n            x, i_1 = ag__.utils.alias_tensors(x_1, i)\n            x += i_1\n            return x,\n      x, = ag__.for_stmt(ag__.range_(10), None, loop_body, (x,))\n      do_return = True\n      retval_ = x\n      return retval_\n  except:\n    ag__.rewrite_graph_construction_error(ag_source_map__)\n```", "```\ntf_upgrade_v2 --infile file.py --outfile file-migrated.py\n```", "```\ntf_upgrade_v2 --intree project --outtree project-migrated\n```", "```\nAdded keyword 'input' to reordered function 'tf.argmax'\nRenamed keyword argument from 'dimension' to 'axis'\n\n    Old: tf.argmax([[1, 2, 2]], dimension=0))\n                                        ~~~~~~~~~~\n    New: tf.argmax(input=[[1, 2, 2]], axis=0))\n```", "```\n# Define your model\ntrainer = Trainer(model)\n# Get features and labels as numpy arrays (explore the dataset available in the keras module)\ntrainer.train(features, labels)\n# measure the accuracy\ntrainer.evaluate(test_features, test_labels)\n```", "```\ndef output():\n    for i in range(10):\n        tf.print(i)\n```", "```\ndef output():\n    for i in tf.range(10):\n        print(i)\n```", "```\ndef output():\n    for i in tf.range(10):\n        tf.print(f\"{i}\", i)\n        print(f\"{i}\", i)\n```", "```\n@tf.function\ndef train(model, optimizer):\n  train_ds = mnist_dataset()\n  step = 0\n  loss = 0.0\n  accuracy = 0.0\n  for x, y in train_ds:\n    step += 1\n    loss = train_one_step(model, optimizer, x, y)\n    if tf.equal(step % 10, 0):\n      tf.print('Step', step, ': loss', loss, '; accuracy', compute_accuracy.result())\n  return step, loss, accuracy\n```"]