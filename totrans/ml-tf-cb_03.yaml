- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the high-level TensorFlow API named Keras.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you should have a better understanding of:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The Keras Sequential API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras Functional API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras Subclassing API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras Preprocessing API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered TensorFlow's fundamentals, and we are now
    able to set up a computational graph. This chapter will introduce Keras, a high-level
    neural network API written in Python with multiple backends. TensorFlow is one
    of them. François Chollet, a French software engineer and AI researcher currently
    working at Google, created Keras for his own personal use before it was open-sourced
    in 2015\. Keras's primary goal is to provide an easy-to-use and accessible library
    to enable fast experiments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow v1 suffers from usability issues; in particular, a sprawling and
    sometimes confusing API. For example, TensorFlow v1 offers two high-level APIs:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The Estimator API (added in release 1.1) is used for training models on localhost
    or distributed environments
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras API was then added later (release 1.4.0) and intended to be used for
    fast prototyping
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With TensorFlow v2, Keras became the official high-level API. Keras can scale
    and suit various user profiles, from research to application development and from
    model training to deployment. Keras provides four key advantages: it''s user-friendly
    (without sacrificing flexibility and performance), modular, composable, and scalable.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Keras APIs are the same as the Keras API. However, the implementation
    of Keras in its TensorFlow version of the backend has been optimized for TensorFlow.
    It integrates TensorFlow-specific functionality, such as eager execution, data
    pipelines, and Estimators.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The difference between Keras, the independent library, and Keras' implementation
    as integrated with TensorFlow is only the way to import it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the command to import the Keras API specification:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is TensorFlow''s implementation of the Keras API specification:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, let's start by discovering the basic building blocks of Keras.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Keras layers
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras layers are the fundamental building blocks of Keras models. Each layer
    receives data as input, does a specific task, and returns an output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras includes a wide range of built-in layers:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**Core layers:** Dense, Activation, Flatten, Input, Reshape, Permute, RepeatVector,
    SpatialDropOut, and many more.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional layers for Convolutional Neural Networks:** Conv1D, Conv2D,
    SeparableConv1D, Conv3D, Cropping2D, and many more.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pooling** layers that perform a downsampling operation to reduce feature
    maps: MaxPooling1D, AveragePooling2D, and GlobalAveragePooling3D.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent layers for recurrent neural networks to process recurrent or sequence
    data:** RNN, SimpleRNN, GRU, LSTM, ConvLSTM2D, etc.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The embedding layer**, only used as the first layer in a model and turns
    positive integers into dense vectors of fixed size.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Merge layers:** Add, Subtract, Multiply, Average, Maximum, Minimum, and many
    more.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced activation layers:** LeakyReLU, PReLU, Softmax, ReLU, etc.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The batch normalization layer**, which normalizes the activation of the previous
    layer at each batch.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise layers:** GausianNoise, GausianDropout, and AlphaDropout.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer wrappers:** TimeDistributed applies a layer to every temporal slice
    of an input and bidirectional wrapper for RNNs.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locally-connected layers:** LocallyConnected1D and LocallyConnected2D. They
    work like Conv1D or Conv2D without sharing their weights.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also write our Keras layers as explained in the Keras Subclassing API
    section of this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start, we'll review some methods that are common in all Keras layers. These
    methods are very useful to know the configuration and the state of a layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with the layer''s weights. The weights are possibly the most essential
    concept in a layer; it decides how much influence the input will have on the output.
    It represents the state of a layer. The `get_weights()` function returns the weights
    of the layer as a list of NumPy arrays:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `set_weights()` method fixes the weights of the layer from a list of Numpy
    arrays:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we''ll explain in the Keras Functional API recipe, sometimes neural network
    topology isn''t linear. In this case, a layer can be used several times in the
    network (shared layer). We can easily get the inputs and outputs of a layer by
    using this command if the layer is a single node (no shared layer):'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Or this one, if the layer has multiple nodes:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can also easily get the layer''s input and output shapes by using this command
    if a layer is a single node (no shared layer):'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Or this one, if the layer has multiple nodes:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we''ll be discussing the layer''s configuration. As the same layer could
    be instantiating several times, the configuration doesn''t include the weights
    or connectivity information. The `get_config()` function returns a dictionary
    containing the configuration of the layer:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `from_config()` method instantiates a layer''s configuration:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the layer configuration is stored in an associative array (Python
    dictionary), a data structure that maps keys to values.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The layers are the building blocks of the models. Keras offers a wide range
    of building layers and useful methods to know more about what's happening and
    get inside the models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'With Keras, we can build models in three ways: with the Sequential, the Functional,
    or the Subclassing API. We''ll later see that only the last two APIs allow access
    to the layers.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Layers API, see the following documentation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras layers API documentation: [https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow Keras layers API documentation: [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Keras Sequential API
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main goal of Keras is to make it easy to create deep learning models. The
    Sequential API allows us to create Sequential models, which are a linear stack
    of layers. Models that are connected layer by layer can solve many problems. To
    create a Sequential model, we have to create an instance of a Sequential class,
    create some model layers, and add them to it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: We will go from the creation of our Sequential model to its prediction via the
    compilation, training, and evaluation steps. By the end of this recipe, you will
    have a Keras model ready to be deployed in production.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will cover the main ways of creating a Sequential model and assembling
    layers to build a model with the Keras Sequential API.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we load TensorFlow and NumPy, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We are ready to proceed with an explanation of how to do it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we will create a Sequential model. Keras offers two equivalent ways of
    creating a Sequential model. Let's start by passing a list of layer instances
    as an array to the constructor. We'll build a multi-class classifier (10 categories)
    fully connected model, aka a multi-layer perceptron, by entering the following
    code.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Another way to create a Sequential model is to instantiate a Sequential class
    and then add layers via the `.add()` method.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s take a closer look at the layer configuration. The `tf.keras.layers`
    API offers a lot of built-in layers and also provides an API to create our layers.
    In most of them, we can set these parameters to the layer''s constructor:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can add an activation function by specifying the name of a built-in function
    or as a callable object. This function decides whether a neuron should be activated
    or not. By default, a layer has no activation function. Below are the two ways
    to create a layer with an activation function. Note that you don't have to run
    the following code; these layers are not assigned to variables.
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can also specify an initialization strategy for the initial weights (kernel
    and bias) by passing the string identifier of built-in initializers or a callable
    object. The kernel is by default set to the "Glorot uniform" initializer, and
    the bias is set to zeros.
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can also specify regularizers for kernel and bias, such as L1 (also called
    Lasso) or L2 (also called Ridge) regularization. By default, no regularization
    is applied. A regularizer aims to prevent overfitting by penalizing a model for
    having large weights. These penalties are incorporated in the loss function that
    the network optimizes.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In Keras, it's strongly recommended to set the input shape for the first layer.
    Yet, contrary to appearances, the input layer isn't a layer but a tensor. Its
    shape must be the same as our training data. The following layers perform automatic
    shape inference; their shapes are calculated based on the unit of the previous
    layer.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each type of layer requires input with a certain number of dimensions, so there
    are different ways to specify the input shape depending on the kind of layer.
    Here, we''ll focus on the Dense layer, so we''ll use the `input_dim` parameter.
    Since the shape of the weights depends on the input size, if the input shape isn''t
    specified in advance, the model has no weights: the model is not built. In this
    case, you can''t call any methods of the `Layer` class such as `summary`, `layers`,
    `weights`, and so on.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this recipe, we'll create datasets with 64 features, and we'll process batches
    of 10 samples. The shape of our input data is (10,64), aka (`batch_size`, `number_of_features`).
    By default, a Keras model is defined to support any batch size, so the batch size
    isn't mandatory. We just have to specify the number of features through the `input_dim`
    parameter to our first layer.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: However, we can force the batch size for efficiency reasons with the `batch_size`
    argument.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Before the learning phase, our model needs to be configured. This is done by
    the `compile` method. We have to specify:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An optimization algorithm for the training of our neural network. We can pass
    an optimizer instance from the `tf.keras.optimizers` module. For example, we can
    use an instance of `tf.keras.optimizers.RMSprop` or `'RMSprop'`, which is an optimizer
    that implements the `RMSprop` algorithm.
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A loss function called an objective function or optimization score function
    aims at minimizing the model. It can be the name of an existing loss function
    (such as `categorical_crossentropy` or `mse`), a symbolic TensorFlow loss function
    (`tf.keras.losses.MAPE`), or a custom loss function, which takes as input two
    tensors (true tensors and predicted tensors) and returns a scalar for each data
    point.
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of metrics used to judge our model's performance that aren't used in
    the model training process. We can either pass the string names or callables from
    the `tf.keras.metrics` module.
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to be sure that the model trains and evaluates eagerly, we can set
    the argument `run_eagerly` to true.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the graph is finalized with the `compile` method.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we'll compile the model using the Adam optimizer for categorical cross-entropy
    loss and display the accuracy metric.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, we'll generate three toy datasets of 64 features with random values. One
    will be used to train the model (2,000 samples), another one to validate (500
    samples), and the last one to test (500 samples).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After the model has been configured, the learning phase begins by calling the
    `fit` method. The training configuration is done by these three arguments:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have to set the number of epochs, aka the number of iterations over the entire
    input data.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to specify the number of samples per gradient, called the `batch_size`
    argument. Note that the last batch may be smaller if the total number of samples
    is not divisible by the batch size.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can specify a validation dataset by setting the `validation_data` argument
    (a tuple of inputs and labels). This dataset makes it easy to monitor the performance
    of the model. The loss and metrics are computed in inference mode at the end of
    each epoch.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过设置`validation_data`参数（一个包含输入和标签的元组）来指定验证数据集。这个数据集可以方便地监控模型的性能。在每个训练周期结束时，损失和度量会在推理模式下计算。
- en: 'Now, we''ll train the model on our toy datasets by calling the `fit` method:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将通过调用`fit`方法在我们的玩具数据集上训练模型：
- en: '[PRE20]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we''ll evaluate our model on the test dataset. We''ll call the `model.evaluate`
    function, which predicts the loss value and the metric values of the model in
    test mode. Computation is done in batches. It has three important arguments: the
    input data, the target data, and the batch size. This function predicts the output
    for a given input. Then, it computes the `metrics` function (specified in the
    `model.compile` based on the target data) and the model''s prediction and returns
    the computed metric value as the output.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在测试数据集上评估我们的模型。我们将调用`model.evaluate`函数，它预测模型在测试模式下的损失值和度量值。计算是按批次进行的。它有三个重要参数：输入数据、目标数据和批次大小。此函数为给定输入预测输出，然后计算`metrics`函数（在`model.compile`中根据目标数据指定），并返回计算后的度量值作为输出。
- en: '[PRE21]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can also just use the model to make a prediction. The `tf.keras.Model.predict`
    method takes as input only data and returns a prediction. And here''s how to predict
    the output of the last layer of inference for the data provided, as a NumPy array:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以仅使用模型进行预测。`tf.keras.Model.predict`方法仅接受数据作为输入并返回预测结果。以下是如何预测提供数据的最后一层推理输出，结果以
    NumPy 数组形式呈现：
- en: '[PRE22]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Analyzing this model's performance is of no interest in this recipe because
    we randomly generated a dataset.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分析这个模型的性能在这个示例中并不重要，因为我们使用的是随机生成的数据集。
- en: Now, let's move on to an analysis of this recipe.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们继续分析这个示例。
- en: How it works...
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Keras provides the Sequential API to create models composed of a linear stack
    of layers. We can either pass a list of layer instances as an array to the constructor
    or use the `add` method.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了 Sequential API 来创建由一系列线性堆叠的层组成的模型。我们可以将层实例的列表作为数组传递给构造函数，或者使用`add`方法。
- en: Keras provides different kinds of layers. Most of them share some common constructor
    arguments such as `activation`, `kernel_initializer` and `bias_initializer`, and
    `kernel_regularizer` and `bias_regularizer`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了不同种类的层。它们大多数共享一些常见的构造参数，如`activation`、`kernel_initializer`和`bias_initializer`，以及`kernel_regularizer`和`bias_regularizer`。
- en: 'Take care with the delayed-build pattern: if no input shape is specified on
    the first layer, the model gets built the first time the model is called on some
    input data or when methods such as `fit`, `eval`, `predict`, and `summary` are
    called. The graph is finalized with the `compile` method, which configures the
    model before the learning phase. Then, we can evaluate the model or make predictions.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意延迟构建模式：如果没有在第一层指定输入形状，当第一次将模型应用于输入数据时，或者调用`fit`、`eval`、`predict`和`summary`等方法时，模型才会被构建。图形在调用`compile`方法时最终确定，该方法在学习阶段之前配置模型。然后，我们可以评估模型或进行预测。
- en: See also
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For some references on the Keras Sequential API, visit the following websites:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Keras Sequential API 的一些参考资料，请访问以下网站：
- en: 'tf.keras.Sequential model API documentation: [https://www.tensorflow.org/api_docs/python/tf/keras/Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential
    )'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'tf.keras.Sequential 模型 API 文档: [https://www.tensorflow.org/api_docs/python/tf/keras/Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)'
- en: 'Keras Sequential model API documentation: [https://keras.io/models/sequential/](https://keras.io/models/sequential/
    )'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Keras Sequential 模型 API 文档: [https://keras.io/models/sequential/](https://keras.io/models/sequential/)'
- en: Using the Keras Functional API
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 函数式 API
- en: The Keras Sequential API is great for developing deep learning models in most
    situations. However, this API has some limitations, such as a linear topology,
    that could be overcome with the Functional API. Note that many high-performing
    networks are based on a non-linear topology such as Inception, ResNet, etc.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Keras Sequential API 在大多数情况下非常适合开发深度学习模型。然而，这个 API 有一些限制，例如线性拓扑结构，可以通过函数式 API
    来克服。需要注意的是，许多高效的网络都基于非线性拓扑结构，如 Inception、ResNet 等。
- en: The Functional API allows defining complex models with a non-linear topology,
    multiple inputs, multiple outputs, residual connections with non-sequential flows,
    and shared and reusable layers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式 API 允许定义具有非线性拓扑结构、多个输入、多个输出、残差连接的复杂模型，以及具有非顺序流动的共享和可重用层。
- en: The deep learning model is usually a directed acyclic graph (DAG). The Functional
    API is a way to build a graph of layers and create more flexible models than the
    `tf.keras.Sequential` API.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will cover the main ways of creating a Functional model, using callable
    models, manipulating complex graph topologies, sharing layers, and finally introducing
    the concept of the layer "node" with the Keras Sequential API.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we just need to import TensorFlow as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are ready to proceed with an explanation of how to do it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's go and make a Functional model for recognizing the MNIST dataset of handwritten
    digits. We will predict the handwritten digits from grayscale images.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Functional model
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we will load the MNIST dataset.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then, we will create an input node with a 28x28 dimensional shape. Remember
    that in Keras, the input layer is not a layer but a tensor, and we have to specify
    the input shape for the first layer. This tensor must have the same shape as our
    training data. By default, a Keras model is defined to support any batch size,
    so the batch size isn't mandatory. `Input()` is used to instantiate a Keras tensor.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Then, we will flatten the images of size (28,28) using the following command.
    This will produce an array of 784 pixels.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We''ll add a new node in the graph of layers by calling `the flatten_layer`
    on the `inputs` object:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The "layer call" action is like drawing an arrow from `inputs` to the `flatten_layer`.
    We're "passing" the inputs to the flatten layer, and as a result, it produces
    outputs. A layer instance is callable (on a tensor) and returns a tensor.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we''ll create a new layer instance:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We''ll add a new node:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To build a model, multiple layers are stacked. In this example, we will add
    another `dense` layer to do a classification task between 10 classes:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Input tensor(s) and output tensor(s) are used to define a model. The model is
    a function of one or more input layers and one or more output layers. The model
    instance formalizes the computational graph on how the data flows from input(s)
    to output(s).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, we'll print the summary.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This results in the following output:![](img/B16254_03_01.png)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.1: Summary of the model'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Such a model can be trained and evaluated by the same `compile, fit`, `evaluate`,
    and `predict` methods used in the Keras Sequential model.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In this recipe, we have built a model using the Functional API.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Using callable models like layers
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's go into the details of the Functional API with callable models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'With the Functional API, it is easy to reuse trained models: any model can
    be treated as a layer, by calling it on a tensor. We will reuse the model defined
    in the previous section as a layer to see this in action. It''s a classifier for
    10 categories. This model returns 10 probabilities: 1 for each category. It''s
    called a 10-way softmax. So, by calling the model defined above, the model will
    predict for each input one of the 10 classes.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that by calling a model, we are not just reusing the model architecture,
    we are also reusing its weights.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，通过调用一个模型，我们不仅仅是重用模型架构，我们还在重用它的权重。
- en: If we're facing a sequence problem, creating a model will become very easy with
    the Functional API. For example, instead of processing one image, we want to process
    a video composed of many images. We could turn an image classification model into
    a video classification model in just one line using the `TimeDistributed` layer
    wrapper. This wrapper applies our previous model to every temporal slice of the
    input sequence, or in other words, to each image of our video.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们面临一个序列问题，使用功能性 API 创建模型将变得非常简单。例如，假设我们不是处理一张图片，而是处理由多张图片组成的视频。我们可以通过使用 `TimeDistributed`
    层包装器，将图像分类模型转变为视频分类模型，仅需一行代码。这个包装器将我们的前一个模型应用于输入序列的每一个时间切片，换句话说，就是应用于视频的每一帧图像。
- en: '[PRE35]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We have seen that models are callable like layers. Now, we'll learn how to create
    complex models with a non-linear topology.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，模型像层一样是可以调用的。现在，我们将学习如何创建具有非线性拓扑的复杂模型。
- en: Creating a model with multiple inputs and outputs
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个具有多个输入和输出的模型
- en: The Functional API makes it easy to manipulate a large number of intertwined
    datastreams with multiple inputs and outputs and non-linear connectivity topologies.
    These cannot be handled with the Sequential API, which isn't able to create a
    model with layers that aren't connected sequentially or with multiple inputs or
    outputs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 功能性 API 使得操作大量交织的数据流变得简单，具有多个输入输出和非线性连接拓扑。这些是顺序 API 无法处理的，顺序 API 无法创建具有非顺序连接或多个输入输出的模型。
- en: Let's go with an example. We're going to build a system for predicting the price
    of a specific house and the elapsed time before its sale.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。我们将构建一个系统，用于预测特定房子的价格和销售前的经过时间。
- en: 'The model will have two inputs:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将有两个输入：
- en: Data about the house such as the number of bedrooms, house size, air conditioning,
    fitted kitchen, etc.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于房子的资料，例如卧室数量、房屋大小、空调、内置厨房等。
- en: A recent picture of the house
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 房子的最新照片
- en: 'This model will have two outputs:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型将有两个输出：
- en: The elapsed time before the sale (two categories – slow or fast)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 销售前的经过时间（两个类别——慢或快）
- en: The predicted price
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测价格
- en: To build this system, we'll start by building the first block to process tabular
    data about the house.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了构建这个系统，我们将从构建第一个模块开始，用于处理关于房子的表格数据。
- en: '[PRE36]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Then, we'll build the second block to process the house image data.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将构建第二个模块来处理房子的图像数据。
- en: '[PRE37]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now, we'll merge all available features into a single large vector via concatenation.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将通过拼接将所有可用特征合并为一个大的向量。
- en: '[PRE38]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Then, we'll stick a logistic regression for price prediction on top of the features.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将在特征上加一个用于价格预测的逻辑回归。
- en: '[PRE39]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: And, we'll stick a time classifier on top of the features.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们将在特征上加一个时间分类器。
- en: '[PRE40]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, we'll build the model.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将构建模型。
- en: '[PRE41]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now, we'll plot the model.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将绘制模型图。
- en: '[PRE42]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This results in the following output:![](img/B16254_03_02.png)
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：![](img/B16254_03_02.png)
- en: 'Figure 3.2: Plot of a model with multiple inputs and outputs'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.2：具有多个输入和输出的模型图
- en: In this recipe, we have created a complex model using the Functional API with
    multiple inputs and outputs that predicts the price of a specific house and the
    elapsed time before its sale. Now, we'll introduce the concept of shared layers.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用功能性 API 创建了一个复杂模型，具有多个输入输出，用于预测特定房子的价格和销售前的经过时间。现在，我们将引入共享层的概念。
- en: Shared layers
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享层
- en: Some models reuse the same layer multiple times inside their architecture. These
    layer instances learn features that correspond to multiple paths in the graph
    of layers. Shared layers are often used to encode inputs from similar spaces.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型在其架构内多次重用相同的层。这些层实例学习与层图中多个路径对应的特征。共享层通常用于对来自相似空间的输入进行编码。
- en: To share a layer (weights and all) across different inputs, we only need to
    instantiate the layer once and call it on as many inputs as we want.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不同的输入之间共享一个层（包括权重），我们只需实例化该层一次，并将其应用于我们需要的多个输入。
- en: Let's consider two different sequences of text. We will apply the same embedding
    layer to these two sequences, which feature similar vocabulary.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑两种不同的文本序列。我们将对这两个具有相似词汇的序列应用相同的嵌入层。
- en: '[PRE43]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In this recipe, we have learned how to reuse a layer multiple times in the same
    model. Now, we'll introduce the concept of extracting and reusing a layer.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们已经学习了如何在同一个模型中多次重用一个层。现在，我们将介绍提取和重用层的概念。
- en: Extracting and reusing nodes in the graph of layers
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在层的图中提取和重用节点
- en: In the first recipe of this chapter, we saw that a layer is an instance that
    takes a tensor as an argument and returns another tensor. A model is composed
    of several layer instances. These layer instances are objects that are chained
    one to another by their layer input and output tensors. Each time we instantiate
    a layer, the output of the layer is a new tensor. By adding a "node" to the layer,
    we link the input to the output tensor.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一个例子中，我们看到一个层是一个实例，它以一个张量作为参数并返回另一个张量。一个模型是由多个层实例组成的。这些层实例是通过它们的输入和输出张量相互连接的对象。每次我们实例化一个层时，该层的输出是一个新的张量。通过向层添加一个“节点”，我们将输入和输出张量连接起来。
- en: The graph of layers is a static data structure. With the Keras Functional API,
    we can easily access and inspect the model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 层的图是一个静态数据结构。通过 Keras 函数式 API，我们可以轻松访问和检查模型。
- en: The `tf.keras.application` module contains canned architectures with pre-trained
    weights.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.application` 模块包含具有预训练权重的现成架构。'
- en: Let's go to download the ResNet 50 pre-trained model.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们去下载 ResNet 50 预训练模型。
- en: '[PRE44]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we''ll display the intermediate layers of the model by querying the graph
    data structure:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将通过查询图数据结构来显示模型的中间层：
- en: '[PRE45]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we''ll display the top 10 intermediate layers of the model by querying
    the graph data structure:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将通过查询图数据结构来显示模型的前 10 个中间层：
- en: '[PRE46]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This results in the following output:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE47]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now, we'll select all the feature layers. We'll go into the details in the convolution
    neural network chapter.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将选择所有特征层。我们将在卷积神经网络章节中详细讲解。
- en: '[PRE48]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Then, we'll reuse the nodes in order to create our feature-extraction model.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将重用这些节点来创建我们的特征提取模型。
- en: '[PRE49]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'One of the interesting benefits of a deep learning model is that it can be
    reused partly or wholly on similar predictive modeling problems. This technique
    is called "transfer learning": it significantly improves the training phase by
    decreasing the training time and the model''s performance on a related problem.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的一个有趣的好处是，它可以在类似的预测建模问题上部分或完全重复使用。这种技术称为“迁移学习”：通过减少训练时间，它显著提高了训练阶段的效果，并增强了模型在相关问题上的表现。
- en: The new model architecture is based on one or more layers from a pre-trained
    model. The weights of the pre-trained model may be used as the starting point
    for the training process. They can be either fixed or fine-tuned, or totally adapted
    during the learning phase. The two main approaches to implement transfer learning
    are weight initialization and feature extraction. Don't worry, we'll go into the
    details later in this book.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 新的模型架构基于预训练模型中的一个或多个层。预训练模型的权重可以作为训练过程的起点。它们可以是固定的、微调的，或者在学习阶段完全适应。实现迁移学习的两种主要方法是权重初始化和特征提取。别担心，我们稍后会在本书中详细讲解。
- en: In this recipe, we have loaded a pretrained model based on the VGG19 architecture.
    We have extracted nodes from this model and reused them in a new model.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们加载了基于 VGG19 架构的预训练模型。我们从这个模型中提取了节点，并在新模型中重用了它们。
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The Keras Sequential API is appropriate in the vast majority of cases but is
    limited to creating layer-by-layer models. The Functional API is more flexible
    and allows extracting and reusing nodes, sharing layers, and creating non-linear
    models with multiple inputs and multiple outputs. Note that many high-performing
    networks are based on a non-linear topology.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 顺序 API 在绝大多数情况下是合适的，但仅限于创建逐层模型。函数式 API 更加灵活，允许提取和重用节点、共享层，并创建具有多个输入和多个输出的非线性模型。需要注意的是，许多高性能的网络基于非线性拓扑结构。
- en: In this recipe, we have learned how to build models using the Keras Functional
    API. These models are trained and evaluated by the same `compile`, `fit`, `evaluate`,
    and `predict` methods used by the Keras Sequential model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们已经学习了如何使用 Keras 函数式 API 构建模型。这些模型的训练和评估使用与 Keras 顺序模型相同的 `compile`、`fit`、`evaluate`
    和 `predict` 方法。
- en: We have also viewed how to reuse trained models as a layer, how to share layers,
    and also how to extract and reuse nodes. This last approach is used in transfer
    learning techniques that speed up training and improve performance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we can access every layer, models built with the Keras Functional API have
    specific features such as model plotting, whole-model saving, etc.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Models built with the Functional API could be complex, so here are some tips
    to consider to avoid pulling your hair out during the process:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Name the layers: It will be quite useful when we display summaries and plots
    of the model graph.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Separate submodels: Consider each submodel as being like a Lego brick that
    we will combine together with the others at the end.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Review the layer summary: Use the `summary` method to check the outputs of
    each layer.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Review graph plots: Use the `plot` method to display and check the connection
    between the layers.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consistent variable names: Use the same variable name for the input and output
    layers. It avoids copy-paste mistakes.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Functional API, visit the following websites:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras Functional API documentation: [https://keras.io/getting-started/functional-api-guide/](https://keras.io/getting-started/functional-api-guide/)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.keras.Model` API: [https://www.tensorflow.org/api_docs/python/tf/keras/Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model
    )'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine Learning Mastery: [https://machinelearningmastery.com/keras-functional-api-deep-learning/](https://machinelearningmastery.com/keras-functional-api-deep-learning/)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside TensorFlow: tf.Keras by François Chollet (part1) [https://www.youtube.com/watch?v=UYRBHFAvLSs](https://www.youtube.com/watch?v=UYRBHFAvLSs)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside TensorFlow: tf.Keras (part2) [https://www.youtube.com/watch?v=uhzGTijaw8A](https://www.youtube.com/watch?v=uhzGTijaw8A)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Keras Subclassing API
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is based on object-oriented design principles. So, we can subclass the
    `Model` class and create our model architecture definition.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The Keras Subclassing API is the third way proposed by Keras to build deep neural
    network models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: This API is fully customizable, but this flexibility also brings complexity!
    So, hold on to your hats, it's harder to use than the Sequential or Functional
    API.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: But you're probably wondering why we need this API if it's so hard to use. Some
    model architectures and some custom layers can be extremely challenging. Some
    researchers and some developers hope to have full control of their models and
    the way to train them. The Subclassing API provides these features. Let's go into
    the details.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will cover the main ways of creating a custom layer and a custom model
    using the Keras Subclassing API.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we load TensorFlow, as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We are ready to proceed with an explanation of how to do it.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by creating our layer.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom layer
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As explained in the *Understanding Keras layers* section, Keras provides various
    built-in layers such as dense, convolutional, recurrent, and normalization layers
    through its layered API.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'All layers are subclasses of the `Layer` class and implement these methods:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The `build` method, which defines the weights of the layer.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `call` method, which specifies the transformation from inputs to outputs
    done by the layer.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `compute_output_shape` method, if the layer modifies the shape of its input.
    This allows Keras to perform automatic shape inference.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `get_config` and `from_config` methods, if the layer is serialized and deserialized.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s put the theory into action. First, we''ll make a subclass layer for
    a custom dense layer:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we''ll create a model using the `MyCustomDense` layer created in the
    previous step:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, we will reload the model from the config:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In this recipe, we have created our `Layer` class. Now, we'll create our model.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom model
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By subclassing the `tf.keras.Model` class, we can build a fully customizable
    model.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: We define our layers in the `__init__` method, and we can have full, complete
    control over the forward pass of the model by implementing the `call` method.
    The `training` Boolean argument can be used to specify different behavior during
    the training or inference phase.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load the MNIST dataset and normalize the grayscale:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s go and make a subclass `Model` for recognizing MNIST data:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, we are going to instantiate the model and process the training:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How it works...
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Subclassing API is a way for deep learning practitioners to build their
    layers or models using object-oriented Keras design principles. We recommend using
    this API only if your model cannot be achieved using the Sequential or the Functional
    API. Although this way can be complicated to implement, it remains useful in a
    few cases, and it is interesting for all developers and researchers to know how
    layers and models are implemented in Keras.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Subclassing API, see the following tutorials,
    papers, and articles:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing custom layers and models with Keras: [https://www.tensorflow.org/guide/keras/custom_layers_and_models](https://www.tensorflow.org/guide/keras/custom_layers_and_models)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Writing your own Keras layers: [https://keras.io/layers/writing-your-own-keras-layers/](https://keras.io/layers/writing-your-own-keras-layers/)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Keras Preprocessing API
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Keras Preprocessing API gathers modules for data processing and data augmentation.
    This API provides utilities for working with sequence, text, and image data. Data
    preprocessing is an essential step in machine learning and deep learning. It converts,
    transforms, or encodes raw data into an understandable, useful, and efficient
    format for learning algorithms.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will cover some preprocessing methods provided by Keras for sequence,
    text, and image data.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we just need to import TensorFlow as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We are ready to proceed with an explanation of how to do it.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start with the sequence data.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Sequence preprocessing
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sequence data is data where the order matters, such as text or a time series.
    So, a time series is defined by a series of data points ordered by time.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Time series generator
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Keras provides utilities for preprocessing sequence data such as time series
    data. It takes in consecutive data points and applies transformations using time
    series parameters such as stride, length of history, etc., to return a TensorFlow
    dataset instance.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go with a toy time series dataset of 10 integer values:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This results in the following output:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We want to predict the next value from the last five lag observations. So,
    we''ll define a generator with the `length` argument set to 5\. This argument
    specifies the length of the output sequences in a number of timesteps:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We want to generate samples composed of 5 lag observations for one prediction
    and the toy time series dataset contains 10 values. So, the number of samples
    generated is 5:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Then, we''ll display the inputs and output of each sample and check that the
    data is well prepared:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'This results in the following output:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, we''ll create and compile a model:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'And we''ll train the model by giving the generator as input data:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Preparing time series data for modeling with deep learning methods can be very
    challenging. But fortunately, Keras provides a generator that will help us transform
    a univariate or multivariate time series dataset into a data structure ready to
    train models. This generator offers many options to prepare the data, such as
    the shuffle, the sampling rate, the start and end offsets, etc. We recommend consulting
    the official Keras API to get more details.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'll focus on how to prepare data for variable-length input sequences.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Padding sequences
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When processing sequence data, each sample often has different lengths. In order
    for all the sequences to fit the desired length, the solution is to pad them.
    Sequences shorter than the defined sequence length are padded with values at the
    end (by default) or the beginning of each sequence. Otherwise, if the sequence
    is greater than the desired length, the sequence is truncated.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with four sentences:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: First, we'll build the vocabulary lookup table. We'll create two dictionaries
    to go from the words to integer identifiers and vice versa.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Then after building the vocabulary lookup table, we'll encode the sentences
    as integer arrays.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This results in the following output:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Now, we'll use the `pad_sequences` function to truncate and pad sequences to
    a common length easily. The pre-sequence padding is activated by default.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This results in the following output:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Then, we'll activate the post-sequence padding and set the `maxlen` argument
    to the desired length – here, 7.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This results in the following output:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The length of the sequence can also be trimmed to the desired length – here,
    3\. By default, this function removes timesteps from the beginning of each sequence.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This results in the following output:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Set the truncating argument to `post` to remove timesteps from the end of each
    sequence.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'This results in the following output:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Padding is very useful when we want all sequences in a list to have the same
    length.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover a very popular technique for preprocessing
    text.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Skip-grams
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Skip-grams is one of the unsupervised learning techniques in natural language
    processing. It finds the most related words for a given word and predicts the
    context word for this given word.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Keras provides the `skipgrams` pre-processing function, which takes in an integer-encoded
    sequence of words and returns the relevance for each pair of words in the defined
    window. If the pair of words is relevant, the sample is positive, and the associated
    label is set to 1\. Otherwise, the sample is considered negative, and the label
    is set to 0.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: An example is better than thousands of words. So, let's take this sentence,
    `"I like coconut and apple,"` select the first word as our "context word," and
    use a window size of two. We make pairs of the context word "I" with the word
    covered in the specified window. So, we have two pairs of words `(I, like)` and
    `(I, coconut)`, both of which equal `1`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put the theory into action:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll encode a sentence as a list of word indices:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Then, we''ll call the `skipgrams` function with a window size of 1:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now, we''ll print the results:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'This results in the following output:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Note that the non-word is defined by index 0 in the vocabulary and will be skipped.
    We recommend that readers consult the Keras API to find more details about padding.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's introduce some tips to preprocess text data.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Text preprocessing
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In deep learning, we cannot feed raw text directly into our network. We have
    to encode our text as numbers and provide integers as input. Our model will generate
    integers as output. This module provides utilities for preprocessing text input.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Split text to word sequence
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Keras provides the `text_to_word_sequence` method, which transforms a sequence
    into a list of words or tokens.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go with this sentence:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Then, we'll call the method that converts a sentence into a list of words. By
    default, this method splits the text on whitespace.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'This results in the following output:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now, we''ll set the `lower` argument to `True`, and the text will be converted
    to lower case:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'This results in the following output:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Note that by default, the `filter` argument filters out a list of characters
    such as punctuation. In our last code execution, we removed all the predefined
    filters.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue with a method to encode words or categorical features.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `Tokenizer` class is the utility class for text tokenization. It's the preferred
    approach for preparing text in deep learning.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'This class takes as inputs:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of words to keep. Only the most common words will be kept
    based on word frequency.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of characters to filter out.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A boolean to convert the text into lower case, or not.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The separator for word splitting.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s go with this sentence:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now, we will create a `Tokenizer` instance and fit it on the previous sentences:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The tokenizer creates several pieces of information about the document. We can
    get a dictionary containing the count for each word.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'This results in the following outputs:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We can also get a dictionary containing, for each word, the number of documents
    in which it appears:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'This results in the following outputs:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'A dictionary contains, for each word, its unique integer identifier:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'This results in the following outputs:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: The number of unique documents that were used to fit the `Tokenizer`.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'This results in the following outputs:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Now, we are ready to encode our documents, thanks to the `texts_to_matrix` function.
    This function provides four different document encoding schemes to compute the
    coefficient for each token.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start with the binary mode, which returns whether or not each token is
    present in the document.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'This results in the following outputs:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The `Tokenizer` API offers another mode based on word count – it returns the
    count of each word in the document:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'This results in the following outputs:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Note that we can also use the `tfidf` mode or the frequency mode. The first
    returns the term frequency-inverse document frequency score for each word, and
    the second returns the frequency of each word in the document related to the total
    number of words in the document.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: The `Tokenizer` API can fit the training dataset and encode text data in the
    training, validation, and test datasets.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have covered a few techniques to prepare text data before
    training and prediction.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's go on to prepare and augment images.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Image preprocessing
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data preprocessing module provides a set of tools for real-time data augmentation
    on image data.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, the performance of a neural network is often improved by the
    number of examples available in the training dataset.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: The `ImageDataGenerator` class in the Keras preprocessing API allows the creation
    of new data from the training dataset. It isn't applied to the validation or test
    dataset because it aims to expand the number of examples in the training datasets
    with plausible new images. This technique is called data augmentation. Beware
    not to confuse data preparation with data normalization or image resizing, which
    is applied to all data in interaction with the model. Data augmentation includes
    many transformations from the field of image manipulation, such as rotation, horizontal
    and vertical shift, horizontal and vertical flip, brightness, and much more.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: The strategy may differ depending on the task to realize. For example, in the
    MNIST dataset, which contains images of handwritten digits, applying a horizontal
    flip doesn't make sense. Except for the figure 8, this transformation isn't appropriate.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: While in the case of a baby picture, applying this kind of transformation makes
    sense because the image could have been taken from the left or right.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Let's put the theory into action and perform a data augmentation on the `CIFAR10`
    dataset. We will start by downloading the `CIFAR` dataset.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Now, we'll create an image data generator that applies a horizontal flip, a
    random rotation between 0 and 15, and a shift of 3 pixels on the width and on
    the height.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Create an iterator on the train dataset.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Create a model and compile it.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: And process the training by calling the `fit` method. Take care to set the `step_per_epoch`
    argument, which specifies the number of sample batches comprising an epoch.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: With the image data generator, we have extended the size of our original dataset
    by creating new images. With more images, the training of a deep learning model
    can be improved.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Keras Preprocessing API allows transforming, encoding, and augmenting data
    for neural networks. It makes it easier to work with sequence, text, and image
    data.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: First, we introduced the Keras Sequence Preprocessing API. We used the time
    series generator to transform a univariate or multivariate time series dataset
    into a data structure ready to train models. Then, we focused on the data preparation
    for variable-length input sequences, aka padding. And we finished this first part
    with the skip-gram technique, which finds the most related words for a given word
    and predicts the context word for that given word.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Then, we covered the Keras Text Preprocessing API, which offers a complete turnkey
    solution to process natural language. We learned how to split text into words
    and tokenize the words using binary, word count, `tfidf`, or frequency mode.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we focused on the Image Preprocessing API using the `ImageDataGenerator`,
    which is a real advantage to increase the size of your training dataset and to
    work with images.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Preprocessing API, visit the following websites:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence Preprocessing Keras API: [http://keras.io/preprocessing/sequence/](http://keras.io/preprocessing/sequence/)'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text Processing Keras API: [https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More about syntactic and semantic word similarities: Tomas Mikolov and Kai
    Chen and Greg Corrado and Jeffrey Dean. (2013). Efficient Estimation of Word Representations
    in Vector Space [https://arxiv.org/pdf/1301.3781v3.pdf](https://arxiv.org/pdf/1301.3781v3.pdf)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image Preprocessing Keras API: [http://keras.io/preprocessing/image/](http://keras.io/preprocessing/image/)'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More examples of data image augmentation: [https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
