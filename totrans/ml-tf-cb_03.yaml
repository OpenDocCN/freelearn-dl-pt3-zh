- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the high-level TensorFlow API named Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you should have a better understanding of:'
  prefs: []
  type: TYPE_NORMAL
- en: The Keras Sequential API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras Functional API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras Subclassing API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras Preprocessing API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered TensorFlow's fundamentals, and we are now
    able to set up a computational graph. This chapter will introduce Keras, a high-level
    neural network API written in Python with multiple backends. TensorFlow is one
    of them. François Chollet, a French software engineer and AI researcher currently
    working at Google, created Keras for his own personal use before it was open-sourced
    in 2015\. Keras's primary goal is to provide an easy-to-use and accessible library
    to enable fast experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow v1 suffers from usability issues; in particular, a sprawling and
    sometimes confusing API. For example, TensorFlow v1 offers two high-level APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: The Estimator API (added in release 1.1) is used for training models on localhost
    or distributed environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras API was then added later (release 1.4.0) and intended to be used for
    fast prototyping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With TensorFlow v2, Keras became the official high-level API. Keras can scale
    and suit various user profiles, from research to application development and from
    model training to deployment. Keras provides four key advantages: it''s user-friendly
    (without sacrificing flexibility and performance), modular, composable, and scalable.'
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Keras APIs are the same as the Keras API. However, the implementation
    of Keras in its TensorFlow version of the backend has been optimized for TensorFlow.
    It integrates TensorFlow-specific functionality, such as eager execution, data
    pipelines, and Estimators.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between Keras, the independent library, and Keras' implementation
    as integrated with TensorFlow is only the way to import it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the command to import the Keras API specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is TensorFlow''s implementation of the Keras API specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's start by discovering the basic building blocks of Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Keras layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras layers are the fundamental building blocks of Keras models. Each layer
    receives data as input, does a specific task, and returns an output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras includes a wide range of built-in layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core layers:** Dense, Activation, Flatten, Input, Reshape, Permute, RepeatVector,
    SpatialDropOut, and many more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional layers for Convolutional Neural Networks:** Conv1D, Conv2D,
    SeparableConv1D, Conv3D, Cropping2D, and many more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pooling** layers that perform a downsampling operation to reduce feature
    maps: MaxPooling1D, AveragePooling2D, and GlobalAveragePooling3D.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent layers for recurrent neural networks to process recurrent or sequence
    data:** RNN, SimpleRNN, GRU, LSTM, ConvLSTM2D, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The embedding layer**, only used as the first layer in a model and turns
    positive integers into dense vectors of fixed size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Merge layers:** Add, Subtract, Multiply, Average, Maximum, Minimum, and many
    more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced activation layers:** LeakyReLU, PReLU, Softmax, ReLU, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The batch normalization layer**, which normalizes the activation of the previous
    layer at each batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise layers:** GausianNoise, GausianDropout, and AlphaDropout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer wrappers:** TimeDistributed applies a layer to every temporal slice
    of an input and bidirectional wrapper for RNNs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locally-connected layers:** LocallyConnected1D and LocallyConnected2D. They
    work like Conv1D or Conv2D without sharing their weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also write our Keras layers as explained in the Keras Subclassing API
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start, we'll review some methods that are common in all Keras layers. These
    methods are very useful to know the configuration and the state of a layer.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with the layer''s weights. The weights are possibly the most essential
    concept in a layer; it decides how much influence the input will have on the output.
    It represents the state of a layer. The `get_weights()` function returns the weights
    of the layer as a list of NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `set_weights()` method fixes the weights of the layer from a list of Numpy
    arrays:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we''ll explain in the Keras Functional API recipe, sometimes neural network
    topology isn''t linear. In this case, a layer can be used several times in the
    network (shared layer). We can easily get the inputs and outputs of a layer by
    using this command if the layer is a single node (no shared layer):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or this one, if the layer has multiple nodes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also easily get the layer''s input and output shapes by using this command
    if a layer is a single node (no shared layer):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or this one, if the layer has multiple nodes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll be discussing the layer''s configuration. As the same layer could
    be instantiating several times, the configuration doesn''t include the weights
    or connectivity information. The `get_config()` function returns a dictionary
    containing the configuration of the layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `from_config()` method instantiates a layer''s configuration:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the layer configuration is stored in an associative array (Python
    dictionary), a data structure that maps keys to values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The layers are the building blocks of the models. Keras offers a wide range
    of building layers and useful methods to know more about what's happening and
    get inside the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Keras, we can build models in three ways: with the Sequential, the Functional,
    or the Subclassing API. We''ll later see that only the last two APIs allow access
    to the layers.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Layers API, see the following documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras layers API documentation: [https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow Keras layers API documentation: [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Keras Sequential API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main goal of Keras is to make it easy to create deep learning models. The
    Sequential API allows us to create Sequential models, which are a linear stack
    of layers. Models that are connected layer by layer can solve many problems. To
    create a Sequential model, we have to create an instance of a Sequential class,
    create some model layers, and add them to it.
  prefs: []
  type: TYPE_NORMAL
- en: We will go from the creation of our Sequential model to its prediction via the
    compilation, training, and evaluation steps. By the end of this recipe, you will
    have a Keras model ready to be deployed in production.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will cover the main ways of creating a Sequential model and assembling
    layers to build a model with the Keras Sequential API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we load TensorFlow and NumPy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to proceed with an explanation of how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we will create a Sequential model. Keras offers two equivalent ways of
    creating a Sequential model. Let's start by passing a list of layer instances
    as an array to the constructor. We'll build a multi-class classifier (10 categories)
    fully connected model, aka a multi-layer perceptron, by entering the following
    code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Another way to create a Sequential model is to instantiate a Sequential class
    and then add layers via the `.add()` method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a closer look at the layer configuration. The `tf.keras.layers`
    API offers a lot of built-in layers and also provides an API to create our layers.
    In most of them, we can set these parameters to the layer''s constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can add an activation function by specifying the name of a built-in function
    or as a callable object. This function decides whether a neuron should be activated
    or not. By default, a layer has no activation function. Below are the two ways
    to create a layer with an activation function. Note that you don't have to run
    the following code; these layers are not assigned to variables.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: We can also specify an initialization strategy for the initial weights (kernel
    and bias) by passing the string identifier of built-in initializers or a callable
    object. The kernel is by default set to the "Glorot uniform" initializer, and
    the bias is set to zeros.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: We can also specify regularizers for kernel and bias, such as L1 (also called
    Lasso) or L2 (also called Ridge) regularization. By default, no regularization
    is applied. A regularizer aims to prevent overfitting by penalizing a model for
    having large weights. These penalties are incorporated in the loss function that
    the network optimizes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: In Keras, it's strongly recommended to set the input shape for the first layer.
    Yet, contrary to appearances, the input layer isn't a layer but a tensor. Its
    shape must be the same as our training data. The following layers perform automatic
    shape inference; their shapes are calculated based on the unit of the previous
    layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each type of layer requires input with a certain number of dimensions, so there
    are different ways to specify the input shape depending on the kind of layer.
    Here, we''ll focus on the Dense layer, so we''ll use the `input_dim` parameter.
    Since the shape of the weights depends on the input size, if the input shape isn''t
    specified in advance, the model has no weights: the model is not built. In this
    case, you can''t call any methods of the `Layer` class such as `summary`, `layers`,
    `weights`, and so on.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this recipe, we'll create datasets with 64 features, and we'll process batches
    of 10 samples. The shape of our input data is (10,64), aka (`batch_size`, `number_of_features`).
    By default, a Keras model is defined to support any batch size, so the batch size
    isn't mandatory. We just have to specify the number of features through the `input_dim`
    parameter to our first layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: However, we can force the batch size for efficiency reasons with the `batch_size`
    argument.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before the learning phase, our model needs to be configured. This is done by
    the `compile` method. We have to specify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An optimization algorithm for the training of our neural network. We can pass
    an optimizer instance from the `tf.keras.optimizers` module. For example, we can
    use an instance of `tf.keras.optimizers.RMSprop` or `'RMSprop'`, which is an optimizer
    that implements the `RMSprop` algorithm.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A loss function called an objective function or optimization score function
    aims at minimizing the model. It can be the name of an existing loss function
    (such as `categorical_crossentropy` or `mse`), a symbolic TensorFlow loss function
    (`tf.keras.losses.MAPE`), or a custom loss function, which takes as input two
    tensors (true tensors and predicted tensors) and returns a scalar for each data
    point.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of metrics used to judge our model's performance that aren't used in
    the model training process. We can either pass the string names or callables from
    the `tf.keras.metrics` module.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to be sure that the model trains and evaluates eagerly, we can set
    the argument `run_eagerly` to true.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the graph is finalized with the `compile` method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we'll compile the model using the Adam optimizer for categorical cross-entropy
    loss and display the accuracy metric.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we'll generate three toy datasets of 64 features with random values. One
    will be used to train the model (2,000 samples), another one to validate (500
    samples), and the last one to test (500 samples).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the model has been configured, the learning phase begins by calling the
    `fit` method. The training configuration is done by these three arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have to set the number of epochs, aka the number of iterations over the entire
    input data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to specify the number of samples per gradient, called the `batch_size`
    argument. Note that the last batch may be smaller if the total number of samples
    is not divisible by the batch size.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can specify a validation dataset by setting the `validation_data` argument
    (a tuple of inputs and labels). This dataset makes it easy to monitor the performance
    of the model. The loss and metrics are computed in inference mode at the end of
    each epoch.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we''ll train the model on our toy datasets by calling the `fit` method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll evaluate our model on the test dataset. We''ll call the `model.evaluate`
    function, which predicts the loss value and the metric values of the model in
    test mode. Computation is done in batches. It has three important arguments: the
    input data, the target data, and the batch size. This function predicts the output
    for a given input. Then, it computes the `metrics` function (specified in the
    `model.compile` based on the target data) and the model''s prediction and returns
    the computed metric value as the output.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also just use the model to make a prediction. The `tf.keras.Model.predict`
    method takes as input only data and returns a prediction. And here''s how to predict
    the output of the last layer of inference for the data provided, as a NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Analyzing this model's performance is of no interest in this recipe because
    we randomly generated a dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's move on to an analysis of this recipe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras provides the Sequential API to create models composed of a linear stack
    of layers. We can either pass a list of layer instances as an array to the constructor
    or use the `add` method.
  prefs: []
  type: TYPE_NORMAL
- en: Keras provides different kinds of layers. Most of them share some common constructor
    arguments such as `activation`, `kernel_initializer` and `bias_initializer`, and
    `kernel_regularizer` and `bias_regularizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take care with the delayed-build pattern: if no input shape is specified on
    the first layer, the model gets built the first time the model is called on some
    input data or when methods such as `fit`, `eval`, `predict`, and `summary` are
    called. The graph is finalized with the `compile` method, which configures the
    model before the learning phase. Then, we can evaluate the model or make predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Sequential API, visit the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: 'tf.keras.Sequential model API documentation: [https://www.tensorflow.org/api_docs/python/tf/keras/Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keras Sequential model API documentation: [https://keras.io/models/sequential/](https://keras.io/models/sequential/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Keras Functional API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Keras Sequential API is great for developing deep learning models in most
    situations. However, this API has some limitations, such as a linear topology,
    that could be overcome with the Functional API. Note that many high-performing
    networks are based on a non-linear topology such as Inception, ResNet, etc.
  prefs: []
  type: TYPE_NORMAL
- en: The Functional API allows defining complex models with a non-linear topology,
    multiple inputs, multiple outputs, residual connections with non-sequential flows,
    and shared and reusable layers.
  prefs: []
  type: TYPE_NORMAL
- en: The deep learning model is usually a directed acyclic graph (DAG). The Functional
    API is a way to build a graph of layers and create more flexible models than the
    `tf.keras.Sequential` API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will cover the main ways of creating a Functional model, using callable
    models, manipulating complex graph topologies, sharing layers, and finally introducing
    the concept of the layer "node" with the Keras Sequential API.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we just need to import TensorFlow as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to proceed with an explanation of how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's go and make a Functional model for recognizing the MNIST dataset of handwritten
    digits. We will predict the handwritten digits from grayscale images.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Functional model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we will load the MNIST dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we will create an input node with a 28x28 dimensional shape. Remember
    that in Keras, the input layer is not a layer but a tensor, and we have to specify
    the input shape for the first layer. This tensor must have the same shape as our
    training data. By default, a Keras model is defined to support any batch size,
    so the batch size isn't mandatory. `Input()` is used to instantiate a Keras tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we will flatten the images of size (28,28) using the following command.
    This will produce an array of 784 pixels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll add a new node in the graph of layers by calling `the flatten_layer`
    on the `inputs` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The "layer call" action is like drawing an arrow from `inputs` to the `flatten_layer`.
    We're "passing" the inputs to the flatten layer, and as a result, it produces
    outputs. A layer instance is callable (on a tensor) and returns a tensor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we''ll create a new layer instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll add a new node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To build a model, multiple layers are stacked. In this example, we will add
    another `dense` layer to do a classification task between 10 classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Input tensor(s) and output tensor(s) are used to define a model. The model is
    a function of one or more input layers and one or more output layers. The model
    instance formalizes the computational graph on how the data flows from input(s)
    to output(s).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we'll print the summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This results in the following output:![](img/B16254_03_01.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.1: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Such a model can be trained and evaluated by the same `compile, fit`, `evaluate`,
    and `predict` methods used in the Keras Sequential model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this recipe, we have built a model using the Functional API.
  prefs: []
  type: TYPE_NORMAL
- en: Using callable models like layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's go into the details of the Functional API with callable models.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the Functional API, it is easy to reuse trained models: any model can
    be treated as a layer, by calling it on a tensor. We will reuse the model defined
    in the previous section as a layer to see this in action. It''s a classifier for
    10 categories. This model returns 10 probabilities: 1 for each category. It''s
    called a 10-way softmax. So, by calling the model defined above, the model will
    predict for each input one of the 10 classes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that by calling a model, we are not just reusing the model architecture,
    we are also reusing its weights.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we're facing a sequence problem, creating a model will become very easy with
    the Functional API. For example, instead of processing one image, we want to process
    a video composed of many images. We could turn an image classification model into
    a video classification model in just one line using the `TimeDistributed` layer
    wrapper. This wrapper applies our previous model to every temporal slice of the
    input sequence, or in other words, to each image of our video.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have seen that models are callable like layers. Now, we'll learn how to create
    complex models with a non-linear topology.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model with multiple inputs and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Functional API makes it easy to manipulate a large number of intertwined
    datastreams with multiple inputs and outputs and non-linear connectivity topologies.
    These cannot be handled with the Sequential API, which isn't able to create a
    model with layers that aren't connected sequentially or with multiple inputs or
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go with an example. We're going to build a system for predicting the price
    of a specific house and the elapsed time before its sale.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model will have two inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: Data about the house such as the number of bedrooms, house size, air conditioning,
    fitted kitchen, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A recent picture of the house
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This model will have two outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The elapsed time before the sale (two categories – slow or fast)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predicted price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To build this system, we'll start by building the first block to process tabular
    data about the house.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we'll build the second block to process the house image data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we'll merge all available features into a single large vector via concatenation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we'll stick a logistic regression for price prediction on top of the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And, we'll stick a time classifier on top of the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we'll build the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we'll plot the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This results in the following output:![](img/B16254_03_02.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.2: Plot of a model with multiple inputs and outputs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this recipe, we have created a complex model using the Functional API with
    multiple inputs and outputs that predicts the price of a specific house and the
    elapsed time before its sale. Now, we'll introduce the concept of shared layers.
  prefs: []
  type: TYPE_NORMAL
- en: Shared layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some models reuse the same layer multiple times inside their architecture. These
    layer instances learn features that correspond to multiple paths in the graph
    of layers. Shared layers are often used to encode inputs from similar spaces.
  prefs: []
  type: TYPE_NORMAL
- en: To share a layer (weights and all) across different inputs, we only need to
    instantiate the layer once and call it on as many inputs as we want.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider two different sequences of text. We will apply the same embedding
    layer to these two sequences, which feature similar vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In this recipe, we have learned how to reuse a layer multiple times in the same
    model. Now, we'll introduce the concept of extracting and reusing a layer.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and reusing nodes in the graph of layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the first recipe of this chapter, we saw that a layer is an instance that
    takes a tensor as an argument and returns another tensor. A model is composed
    of several layer instances. These layer instances are objects that are chained
    one to another by their layer input and output tensors. Each time we instantiate
    a layer, the output of the layer is a new tensor. By adding a "node" to the layer,
    we link the input to the output tensor.
  prefs: []
  type: TYPE_NORMAL
- en: The graph of layers is a static data structure. With the Keras Functional API,
    we can easily access and inspect the model.
  prefs: []
  type: TYPE_NORMAL
- en: The `tf.keras.application` module contains canned architectures with pre-trained
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go to download the ResNet 50 pre-trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll display the intermediate layers of the model by querying the graph
    data structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll display the top 10 intermediate layers of the model by querying
    the graph data structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we'll select all the feature layers. We'll go into the details in the convolution
    neural network chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we'll reuse the nodes in order to create our feature-extraction model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'One of the interesting benefits of a deep learning model is that it can be
    reused partly or wholly on similar predictive modeling problems. This technique
    is called "transfer learning": it significantly improves the training phase by
    decreasing the training time and the model''s performance on a related problem.'
  prefs: []
  type: TYPE_NORMAL
- en: The new model architecture is based on one or more layers from a pre-trained
    model. The weights of the pre-trained model may be used as the starting point
    for the training process. They can be either fixed or fine-tuned, or totally adapted
    during the learning phase. The two main approaches to implement transfer learning
    are weight initialization and feature extraction. Don't worry, we'll go into the
    details later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we have loaded a pretrained model based on the VGG19 architecture.
    We have extracted nodes from this model and reused them in a new model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Keras Sequential API is appropriate in the vast majority of cases but is
    limited to creating layer-by-layer models. The Functional API is more flexible
    and allows extracting and reusing nodes, sharing layers, and creating non-linear
    models with multiple inputs and multiple outputs. Note that many high-performing
    networks are based on a non-linear topology.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we have learned how to build models using the Keras Functional
    API. These models are trained and evaluated by the same `compile`, `fit`, `evaluate`,
    and `predict` methods used by the Keras Sequential model.
  prefs: []
  type: TYPE_NORMAL
- en: We have also viewed how to reuse trained models as a layer, how to share layers,
    and also how to extract and reuse nodes. This last approach is used in transfer
    learning techniques that speed up training and improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we can access every layer, models built with the Keras Functional API have
    specific features such as model plotting, whole-model saving, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Models built with the Functional API could be complex, so here are some tips
    to consider to avoid pulling your hair out during the process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Name the layers: It will be quite useful when we display summaries and plots
    of the model graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Separate submodels: Consider each submodel as being like a Lego brick that
    we will combine together with the others at the end.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Review the layer summary: Use the `summary` method to check the outputs of
    each layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Review graph plots: Use the `plot` method to display and check the connection
    between the layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consistent variable names: Use the same variable name for the input and output
    layers. It avoids copy-paste mistakes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Functional API, visit the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras Functional API documentation: [https://keras.io/getting-started/functional-api-guide/](https://keras.io/getting-started/functional-api-guide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.keras.Model` API: [https://www.tensorflow.org/api_docs/python/tf/keras/Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine Learning Mastery: [https://machinelearningmastery.com/keras-functional-api-deep-learning/](https://machinelearningmastery.com/keras-functional-api-deep-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside TensorFlow: tf.Keras by François Chollet (part1) [https://www.youtube.com/watch?v=UYRBHFAvLSs](https://www.youtube.com/watch?v=UYRBHFAvLSs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside TensorFlow: tf.Keras (part2) [https://www.youtube.com/watch?v=uhzGTijaw8A](https://www.youtube.com/watch?v=uhzGTijaw8A)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Keras Subclassing API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is based on object-oriented design principles. So, we can subclass the
    `Model` class and create our model architecture definition.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras Subclassing API is the third way proposed by Keras to build deep neural
    network models.
  prefs: []
  type: TYPE_NORMAL
- en: This API is fully customizable, but this flexibility also brings complexity!
    So, hold on to your hats, it's harder to use than the Sequential or Functional
    API.
  prefs: []
  type: TYPE_NORMAL
- en: But you're probably wondering why we need this API if it's so hard to use. Some
    model architectures and some custom layers can be extremely challenging. Some
    researchers and some developers hope to have full control of their models and
    the way to train them. The Subclassing API provides these features. Let's go into
    the details.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will cover the main ways of creating a custom layer and a custom model
    using the Keras Subclassing API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we load TensorFlow, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to proceed with an explanation of how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by creating our layer.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As explained in the *Understanding Keras layers* section, Keras provides various
    built-in layers such as dense, convolutional, recurrent, and normalization layers
    through its layered API.
  prefs: []
  type: TYPE_NORMAL
- en: 'All layers are subclasses of the `Layer` class and implement these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: The `build` method, which defines the weights of the layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `call` method, which specifies the transformation from inputs to outputs
    done by the layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `compute_output_shape` method, if the layer modifies the shape of its input.
    This allows Keras to perform automatic shape inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `get_config` and `from_config` methods, if the layer is serialized and deserialized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s put the theory into action. First, we''ll make a subclass layer for
    a custom dense layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll create a model using the `MyCustomDense` layer created in the
    previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will reload the model from the config:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this recipe, we have created our `Layer` class. Now, we'll create our model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By subclassing the `tf.keras.Model` class, we can build a fully customizable
    model.
  prefs: []
  type: TYPE_NORMAL
- en: We define our layers in the `__init__` method, and we can have full, complete
    control over the forward pass of the model by implementing the `call` method.
    The `training` Boolean argument can be used to specify different behavior during
    the training or inference phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load the MNIST dataset and normalize the grayscale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s go and make a subclass `Model` for recognizing MNIST data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are going to instantiate the model and process the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Subclassing API is a way for deep learning practitioners to build their
    layers or models using object-oriented Keras design principles. We recommend using
    this API only if your model cannot be achieved using the Sequential or the Functional
    API. Although this way can be complicated to implement, it remains useful in a
    few cases, and it is interesting for all developers and researchers to know how
    layers and models are implemented in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Subclassing API, see the following tutorials,
    papers, and articles:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing custom layers and models with Keras: [https://www.tensorflow.org/guide/keras/custom_layers_and_models](https://www.tensorflow.org/guide/keras/custom_layers_and_models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Writing your own Keras layers: [https://keras.io/layers/writing-your-own-keras-layers/](https://keras.io/layers/writing-your-own-keras-layers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Keras Preprocessing API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Keras Preprocessing API gathers modules for data processing and data augmentation.
    This API provides utilities for working with sequence, text, and image data. Data
    preprocessing is an essential step in machine learning and deep learning. It converts,
    transforms, or encodes raw data into an understandable, useful, and efficient
    format for learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will cover some preprocessing methods provided by Keras for sequence,
    text, and image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we just need to import TensorFlow as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to proceed with an explanation of how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start with the sequence data.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sequence data is data where the order matters, such as text or a time series.
    So, a time series is defined by a series of data points ordered by time.
  prefs: []
  type: TYPE_NORMAL
- en: Time series generator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Keras provides utilities for preprocessing sequence data such as time series
    data. It takes in consecutive data points and applies transformations using time
    series parameters such as stride, length of history, etc., to return a TensorFlow
    dataset instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go with a toy time series dataset of 10 integer values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We want to predict the next value from the last five lag observations. So,
    we''ll define a generator with the `length` argument set to 5\. This argument
    specifies the length of the output sequences in a number of timesteps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We want to generate samples composed of 5 lag observations for one prediction
    and the toy time series dataset contains 10 values. So, the number of samples
    generated is 5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll display the inputs and output of each sample and check that the
    data is well prepared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll create and compile a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And we''ll train the model by giving the generator as input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Preparing time series data for modeling with deep learning methods can be very
    challenging. But fortunately, Keras provides a generator that will help us transform
    a univariate or multivariate time series dataset into a data structure ready to
    train models. This generator offers many options to prepare the data, such as
    the shuffle, the sampling rate, the start and end offsets, etc. We recommend consulting
    the official Keras API to get more details.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'll focus on how to prepare data for variable-length input sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Padding sequences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When processing sequence data, each sample often has different lengths. In order
    for all the sequences to fit the desired length, the solution is to pad them.
    Sequences shorter than the defined sequence length are padded with values at the
    end (by default) or the beginning of each sequence. Otherwise, if the sequence
    is greater than the desired length, the sequence is truncated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with four sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, we'll build the vocabulary lookup table. We'll create two dictionaries
    to go from the words to integer identifiers and vice versa.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then after building the vocabulary lookup table, we'll encode the sentences
    as integer arrays.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we'll use the `pad_sequences` function to truncate and pad sequences to
    a common length easily. The pre-sequence padding is activated by default.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we'll activate the post-sequence padding and set the `maxlen` argument
    to the desired length – here, 7.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The length of the sequence can also be trimmed to the desired length – here,
    3\. By default, this function removes timesteps from the beginning of each sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set the truncating argument to `post` to remove timesteps from the end of each
    sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Padding is very useful when we want all sequences in a list to have the same
    length.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover a very popular technique for preprocessing
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Skip-grams
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Skip-grams is one of the unsupervised learning techniques in natural language
    processing. It finds the most related words for a given word and predicts the
    context word for this given word.
  prefs: []
  type: TYPE_NORMAL
- en: Keras provides the `skipgrams` pre-processing function, which takes in an integer-encoded
    sequence of words and returns the relevance for each pair of words in the defined
    window. If the pair of words is relevant, the sample is positive, and the associated
    label is set to 1\. Otherwise, the sample is considered negative, and the label
    is set to 0.
  prefs: []
  type: TYPE_NORMAL
- en: An example is better than thousands of words. So, let's take this sentence,
    `"I like coconut and apple,"` select the first word as our "context word," and
    use a window size of two. We make pairs of the context word "I" with the word
    covered in the specified window. So, we have two pairs of words `(I, like)` and
    `(I, coconut)`, both of which equal `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put the theory into action:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll encode a sentence as a list of word indices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll call the `skipgrams` function with a window size of 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the non-word is defined by index 0 in the vocabulary and will be skipped.
    We recommend that readers consult the Keras API to find more details about padding.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's introduce some tips to preprocess text data.
  prefs: []
  type: TYPE_NORMAL
- en: Text preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In deep learning, we cannot feed raw text directly into our network. We have
    to encode our text as numbers and provide integers as input. Our model will generate
    integers as output. This module provides utilities for preprocessing text input.
  prefs: []
  type: TYPE_NORMAL
- en: Split text to word sequence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Keras provides the `text_to_word_sequence` method, which transforms a sequence
    into a list of words or tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go with this sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we'll call the method that converts a sentence into a list of words. By
    default, this method splits the text on whitespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll set the `lower` argument to `True`, and the text will be converted
    to lower case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that by default, the `filter` argument filters out a list of characters
    such as punctuation. In our last code execution, we removed all the predefined
    filters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue with a method to encode words or categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `Tokenizer` class is the utility class for text tokenization. It's the preferred
    approach for preparing text in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This class takes as inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of words to keep. Only the most common words will be kept
    based on word frequency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of characters to filter out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A boolean to convert the text into lower case, or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The separator for word splitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s go with this sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will create a `Tokenizer` instance and fit it on the previous sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The tokenizer creates several pieces of information about the document. We can
    get a dictionary containing the count for each word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also get a dictionary containing, for each word, the number of documents
    in which it appears:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A dictionary contains, for each word, its unique integer identifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The number of unique documents that were used to fit the `Tokenizer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we are ready to encode our documents, thanks to the `texts_to_matrix` function.
    This function provides four different document encoding schemes to compute the
    coefficient for each token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start with the binary mode, which returns whether or not each token is
    present in the document.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `Tokenizer` API offers another mode based on word count – it returns the
    count of each word in the document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we can also use the `tfidf` mode or the frequency mode. The first
    returns the term frequency-inverse document frequency score for each word, and
    the second returns the frequency of each word in the document related to the total
    number of words in the document.
  prefs: []
  type: TYPE_NORMAL
- en: The `Tokenizer` API can fit the training dataset and encode text data in the
    training, validation, and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have covered a few techniques to prepare text data before
    training and prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's go on to prepare and augment images.
  prefs: []
  type: TYPE_NORMAL
- en: Image preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data preprocessing module provides a set of tools for real-time data augmentation
    on image data.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, the performance of a neural network is often improved by the
    number of examples available in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The `ImageDataGenerator` class in the Keras preprocessing API allows the creation
    of new data from the training dataset. It isn't applied to the validation or test
    dataset because it aims to expand the number of examples in the training datasets
    with plausible new images. This technique is called data augmentation. Beware
    not to confuse data preparation with data normalization or image resizing, which
    is applied to all data in interaction with the model. Data augmentation includes
    many transformations from the field of image manipulation, such as rotation, horizontal
    and vertical shift, horizontal and vertical flip, brightness, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy may differ depending on the task to realize. For example, in the
    MNIST dataset, which contains images of handwritten digits, applying a horizontal
    flip doesn't make sense. Except for the figure 8, this transformation isn't appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: While in the case of a baby picture, applying this kind of transformation makes
    sense because the image could have been taken from the left or right.
  prefs: []
  type: TYPE_NORMAL
- en: Let's put the theory into action and perform a data augmentation on the `CIFAR10`
    dataset. We will start by downloading the `CIFAR` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we'll create an image data generator that applies a horizontal flip, a
    random rotation between 0 and 15, and a shift of 3 pixels on the width and on
    the height.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create an iterator on the train dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a model and compile it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And process the training by calling the `fit` method. Take care to set the `step_per_epoch`
    argument, which specifies the number of sample batches comprising an epoch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the image data generator, we have extended the size of our original dataset
    by creating new images. With more images, the training of a deep learning model
    can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Keras Preprocessing API allows transforming, encoding, and augmenting data
    for neural networks. It makes it easier to work with sequence, text, and image
    data.
  prefs: []
  type: TYPE_NORMAL
- en: First, we introduced the Keras Sequence Preprocessing API. We used the time
    series generator to transform a univariate or multivariate time series dataset
    into a data structure ready to train models. Then, we focused on the data preparation
    for variable-length input sequences, aka padding. And we finished this first part
    with the skip-gram technique, which finds the most related words for a given word
    and predicts the context word for that given word.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we covered the Keras Text Preprocessing API, which offers a complete turnkey
    solution to process natural language. We learned how to split text into words
    and tokenize the words using binary, word count, `tfidf`, or frequency mode.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we focused on the Image Preprocessing API using the `ImageDataGenerator`,
    which is a real advantage to increase the size of your training dataset and to
    work with images.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the Keras Preprocessing API, visit the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence Preprocessing Keras API: [http://keras.io/preprocessing/sequence/](http://keras.io/preprocessing/sequence/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text Processing Keras API: [https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More about syntactic and semantic word similarities: Tomas Mikolov and Kai
    Chen and Greg Corrado and Jeffrey Dean. (2013). Efficient Estimation of Word Representations
    in Vector Space [https://arxiv.org/pdf/1301.3781v3.pdf](https://arxiv.org/pdf/1301.3781v3.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image Preprocessing Keras API: [http://keras.io/preprocessing/image/](http://keras.io/preprocessing/image/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More examples of data image augmentation: [https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/](https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
