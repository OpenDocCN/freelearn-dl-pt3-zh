- en: TensorFlow in Action - Some Basic Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ',In this chapter, we will explain the main computational concept behind TensorFlow,
    which is the computational graph model, and demonstrate how to get you on track
    by implementing linear regression and logistic regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Capacity of a single neuron and activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed-forward neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for a multilayer network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow terminologies—recap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression model—building and training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression model—building and training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start by explaining what a single neuron can actually do/model, and
    based on this, the need for a multilayer network will arise. Next up, we will
    do more elaboration of the main concepts and tools that are used/available within
    TensorFlow and how to use these tools to build up simple examples such as linear
    regression and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity of a single neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **neural network**  is a computational model that is mainly inspired by the
    way the biological neural networks of the human brain process the incoming information.
    Neural networks made a huge breakthrough in machine learning research (deep learning,
    specifically) and industrial applications, such as breakthrough results in computer
    vision, speech recognition, and text processing. In this chapter, we will try
    to develop an understanding of a particular type of neural network called the **multi-layer
    Perceptron**.
  prefs: []
  type: TYPE_NORMAL
- en: Biological motivation and connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic computational unit of our brains is called a **neuron**, and we have
    approximately 86 billion neurons in our nervous system, which are connected with
    approximately ![](img/d63a8baf-bbe2-4bae-ad11-1657839aa8bf.png) to ![](img/c889ca1f-940a-4aa4-b33d-d9631a8aa4c0.png)
    synapses.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1* shows a biological neuron. *Figure 2* shows the corresponding mathematical
    model. In the drawing of the biological neuron, each neuron receives incoming
    signals from its dendrites and then produces output signals along its axon, where
    the axon gets split out and connects via synapses to other neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: In the corresponding mathematical computational model of a neuron, the signals
    that travel along the axons ![](img/613ebbfe-8820-436c-914e-9fcf1212b25f.png)
    interact with a multiplication operation ![](img/12164cb0-d15a-430c-94a1-ba0adabcfa27.png)
    with the dendrites of the other neuron in the system based on the synaptic strength
    at that synapse, which is represented by ![](img/b060c83c-50ec-4aa1-9eec-425f11521da3.png).
    The idea is that the synaptic weights/strength ![](img/9ed5a5b6-6d41-4348-8719-8ee0f4812468.png)
    gets learned by the network and they're the ones that control the influence of
    a specific neuron on another.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in the basic computational model in *Figure 2*, the dendrites carry the
    signal to the main cell body where it sums them all. If the final result is above
    a certain threshold, the neuron can fire in the computational model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, it is worth mentioning that we need to control the frequency of the output
    spikes along the axon, so we use something called an **activation function**.
    Practically, a common choice of activation function is the sigmoid function σ,
    since it takes a real-valued input (the signal strength after the sum) and squashes
    it to be between 0 and 1\. We will see the details of these activation functions
    later in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/903ae559-7cca-47b8-a502-97797b4a994c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Computational unit of the brain (http://cs231n.github.io/assets/nn1/neuron.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is the corresponding basic mathematical model for the biological one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c94c95ce-1f72-4a7b-9f55-c17a2f00ad10.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Mathematical modeling of the Brain''s computational unit (http://cs231n.github.io/assets/nn1/neuron_model.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: The basic unit of computation in a neural network is the neuron, often called
    a **node** or **unit**. It receives input from some other nodes or from an external
    source and computes an output. Each input has an associated **weight** (**w**),
    which is assigned on the basis of its importance relative to other inputs. The
    node applies a function *f* (we've defined it later) to the weighted sum of its
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: So, the basic computational unit of neural networks in general is called **neuron**/**node**/**unit.**
  prefs: []
  type: TYPE_NORMAL
- en: This neuron receives its input from previous neurons or even an external source
    and then it does some processing on this input to produce a so-called activation.
    Each input to this neuron is associated with its own weight ![](img/c0332e36-48b3-4716-b43a-177ce42e409a.png),
    which represents the strength of this connection and hence the importance of this
    input.
  prefs: []
  type: TYPE_NORMAL
- en: So, the final output of this basic building block of the neural network is a
    summed version of the inputs weighted by their importance *w*, and then the neuron
    passes the summed output through an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cb94f65-1a2f-497f-9668-800a94d2ae6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A single neuron'
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The output from the neuron is computed as shown in *Figure 3*, and passed through
    an activation function that introduces non-linearity to the output. This *f* is
    called an **activation function**. The main purposes of the activation functions
    are to:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduce nonlinearity into the output of a neuron. This is important because
    most real-world data is nonlinear and we want neurons to learn these nonlinear
    representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Squash the output to be in a specific range.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every activation function (or nonlinearity) takes a single number and performs
    a certain fixed mathematical operation on it. There are several activation functions
    you may encounter in practice.
  prefs: []
  type: TYPE_NORMAL
- en: So, we are going to briefly cover the most common activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Historically, the sigmoid activation function is widely used among researchers.
    This function accepts a real-valued input and squashes it to a range between 0
    and 1, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '*σ(x) = 1 / (1 + exp(−x))*![](img/cb589e92-9a77-4fa2-b417-d69f20ea718a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Sigmoid activation function'
  prefs: []
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tanh is another activation function that tolerates some negative values. Tanh
    accepts a real-valued input and squashes them to [-1, 1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh(x) = 2σ(2x) − 1*![](img/feef9554-16ff-4f6b-9047-cdc9b679729f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Tanh activation function'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rectified linear unit** (**ReLU**) does not tolerate negative values as it
    accepts a real-valued input and thresholds it at zero (replaces negative values
    with zero):'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x) = max(0, x)*![](img/f1c98eda-2e60-4fb1-b43d-094110178204.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Relu activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importance of bias**: The main function of bias is to provide every node
    with a trainable constant value (in addition to the normal inputs that the node
    receives). See this link at [https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)
    to learn more about the role of bias in a neuron.'
  prefs: []
  type: TYPE_NORMAL
- en: Feed-forward neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The feed-forward neural network was the first and simplest type of artificial
    neural network devised. It contains multiple neurons (nodes) arranged in layers.
    Nodes from adjacent layers have connections or edges between them. All these connections
    have weights associated with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a feed-forward neural network is shown in *Figure 7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b47331f-bfc0-4af2-8fd1-f08ddf2ec754.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An example feed-forward neural network'
  prefs: []
  type: TYPE_NORMAL
- en: In a feed-forward network, the information moves in only one direction—forward—from
    the input nodes, through the hidden nodes (if any), and to the output nodes. There
    are no cycles or loops in the network (this property of feed-forward networks
    is different from recurrent neural networks, in which the connections between
    nodes form a cycle).
  prefs: []
  type: TYPE_NORMAL
- en: The need for multilayer networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **multi-layer perceptron** (**MLP**) contains one or more hidden layers (apart
    from one input and one output layer). While a single layer perceptron can learn only linear
    functions, a MLP can also learn non-linear functions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7* shows MLP with a single hidden layer. Note that all connections
    have weights associated with them, but only three weights (*w0*, *w1*, and *w2*)
    are shown in the figure.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Layer**: The Input layer has three nodes. The bias node has a value
    of 1\. The other two nodes take X1 and X2 as external inputs (which are numerical
    values depending upon the input dataset). As discussed before, no computation,
    is performed in the **Input Layer**, so the outputs from nodes in the **Input
    Layer** are **1**, **X1**, and **X2** respectively, which are fed into the **Hidden
    Layer**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hidden Layer**: The **Hidden Layer** also has three nodes, with the bias
    node having an output of 1\. The output of the other two nodes in the **Hidden
    Layer** depends on the outputs from the **Input Layer** (**1**, **X1**, and **X2**)
    as well as the weights associated with the connections (edges). Remember that
    *f* refers to the activation function. These outputs are then fed to the nodes
    in the **Output Layer**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38718db0-2454-42d0-be89-545151286aba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A multi-layer perceptron having one hidden layer'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output Layer:** The **Output Layer** has two nodes; they take inputs from
    the **Hidden Layer** and perform similar computations as shown for the highlighted
    hidden node. The values calculated (**Y1** and **Y2**) as a result of these computations
    act as outputs of the multi-layer perceptron.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of features *X = (x1, x2, …)* and a target *y*, a multi-layer perceptron
    can learn the relationship between the features and the target for either classification
    or regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example to understand multi-layer perceptrons better. Suppose
    we have the following student marks dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1 – Sample student marks dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hours studied** | **Mid term marks** | **Final term results** |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | 67 | Pass |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 75 | Fail |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 89 | Pass |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | 56 | Pass |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 90 | Fail |'
  prefs: []
  type: TYPE_TB
- en: The two input columns show the number of hours the student has studied and the
    mid term marks obtained by the student. The **Final Result** column can have two
    values, **1** or **0**, indicating whether the student passed in the final term
    or not. For example, we can see that if the student studied 35 hours and had obtained
    67 marks in the mid term, he/she ended up passing the final term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, suppose we want to predict whether a student studying 25 hours and having
    70 marks in the mid term will pass the final term:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 2 – Sample student with unknown final term result**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hours studied** | **Mid term marks** | **Final term result** |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 70 | ? |'
  prefs: []
  type: TYPE_TB
- en: This is a binary classification problem, where a MLP can learn from the given
    examples (training data) and make an informed prediction given a new data point.
    We will soon see how a MLP learns such relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Training our MLP – the backpropagation algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process by which a multi-layer perceptron learns is called the **backpropagation**
    algorithm. I would recommend reading this Quora answer by Hemanth Kumar, [https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri)
    (quoted later), which explains backpropagation clearly.
  prefs: []
  type: TYPE_NORMAL
- en: '"**Backward Propagation of Errors**, often abbreviated as BackProp is one of
    the several ways in which an artificial neural network (ANN) can be trained. It
    is a supervised training scheme, which means, it learns from labeled training
    data (there is a supervisor, to guide its learning).'
  prefs: []
  type: TYPE_NORMAL
- en: To put in simple terms, BackProp is like "**learning from mistakes"**. The supervisor
    corrects the ANN whenever it makes mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: An ANN consists of nodes in different layers; input layer, intermediate hidden
    layer(s) and the output layer. The connections between nodes of adjacent layers
    have "weights" associated with them. The goal of learning is to assign correct
    weights for these edges. Given an input vector, these weights determine what the
    output vector is.
  prefs: []
  type: TYPE_NORMAL
- en: In supervised learning, the training set is labeled. This means, for some given
    inputs, we know the desired/expected output (label).
  prefs: []
  type: TYPE_NORMAL
- en: 'BackProp Algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially all the edge weights are randomly assigned. For every input in the
    training dataset, the ANN is activated and its output is observed. This output
    is compared with the desired output that we already know, and the error is "propagated"
    back to the previous layer. This error is noted and the weights are "adjusted"
    accordingly. This process is repeated until the output error is below a predetermined
    threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Once the above algorithm terminates, we have a "learned" ANN which, we consider
    is ready to work with "new" inputs. This ANN is said to have learned from several
    examples (labeled data) and from its mistakes (error propagation)."
  prefs: []
  type: TYPE_NORMAL
- en: —Hemanth Kumar.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an idea of how backpropagation works, let's go back to our
    student marks dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The MLP shown in *Figure 8* has two nodes in the input layer, which take the
    inputs hours studied and mid term marks. It also has a hidden layer with two nodes.
    The output layer has two nodes as well; the upper node outputs the probability
    of *pass* while the lower node outputs the probability of *fail*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In classification applications, we widely use a softmax function ([http://cs231n.github.io/linear-classify/#softmax](http://cs231n.github.io/linear-classify/#softmax))
    as the activation function in the output layer of the MLP to ensure that the outputs
    are probabilities and they add up to 1\. The softmax function takes a vector of
    arbitrary real-valued scores and squashes it to a vector of values between 0 and
    1 that sum up to 1\. So, in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e46f34df-4448-4998-aba9-9ce537e465ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 1 – forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All weights in the network are randomly initialized. Let's consider a specific
    hidden layer node and call it *V*. Assume that the weights of the connections
    from the inputs to that node are **w1**, **w2**, and **w3** (as shown).
  prefs: []
  type: TYPE_NORMAL
- en: 'The network then takes the first training samples as input (we know that for
    inputs 35 and 67, the probability of passing is 1):'
  prefs: []
  type: TYPE_NORMAL
- en: Input to the network = [35, 67]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Desired output from the network (target) = [1, 0]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, output *V* from the node in consideration, which can be calculated as
    follows (f is an activation function such as sigmoid):'
  prefs: []
  type: TYPE_NORMAL
- en: '*V = f (1*w1 + 35*w2 + 67*w3)*'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, outputs from the other node in the hidden layer are also calculated.
    The outputs of the two nodes in the hidden layer act as inputs to the two nodes
    in the output layer. This enables us to calculate output probabilities from the
    two nodes in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the output probabilities from the two nodes in the output layer are
    0.4 and 0.6, respectively (since the weights are randomly assigned, outputs will
    also be random). We can see that the calculated probabilities (0.4 and 0.6) are
    very far from the desired probabilities (1 and 0 respectively); hence the network
    is said to have an *incorrect output*.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – backpropagation and weight updation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We calculate the total error at the output nodes and propagate these errors
    back through the network using backpropagation to calculate the gradients. Then,
    we use an optimization method such as gradient descent to adjust all weights in
    the network with an aim of reducing the error at the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the new weights associated with the node in consideration are *w4*,
    *w5*, and *w6* (after backpropagation and adjusting weights).
  prefs: []
  type: TYPE_NORMAL
- en: If we now feed the same sample as an input to the network, the network should
    perform better than the initial run since the weights have now been optimized
    to minimize the error in prediction. The errors at the output nodes now reduce
    to [0.2, -0.2] as compared to [0.6, -0.4] earlier. This means that our network
    has learned to correctly classify our first training sample.
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process with all other training samples in our dataset. Then,
    our network is said to have learned those examples.
  prefs: []
  type: TYPE_NORMAL
- en: If we now want to predict whether a student studying 25 hours and having 70
    marks in the mid term will pass the final term, we go through the forward propagation
    step and find the output probabilities for pass and fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have avoided mathematical equations and explanation of concepts such as gradient
    descent here and have rather tried to develop an intuition for the algorithm.
    For a more mathematically involved discussion of the backpropagation algorithm,
    refer to this link: [http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html](http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html).'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow terminologies – recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide an overview of the TensorFlow library as well
    as the structure of a basic TensorFlow application. TensorFlow is an open source
    library for creating large-scale machine learning applications; it can model computations
    on a wide variety of hardware, ranging from android devices to heterogeneous multi-gpu
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow uses a special structure in order to execute code on different devices
    such as CPUs and GPUs. Computations are defined as a graph and each graph is made
    up of operations, also known as **ops**, so whenever we work with TensorFlow,
    we define the series of operations in a graph.
  prefs: []
  type: TYPE_NORMAL
- en: To run these operations, we need to launch the graph into a session. The session
    translates the operations and passes them to a device for execution.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the following image represents a graph in TensorFlow. *W*, *x*,
    and *b* are tensors over the edges of this graph. *MatMul* is an operation over
    the tensors *W* and *x*; after that, *Add* is called and we add the result of
    the previous operator with *b*. The resultant tensors of each operation cross
    the next one until the end, where it's possible to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2f35a77-296a-4058-832f-d70cfdfbea53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Sample TensorFlow computational graph'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use TensorFlow, we need to import the library; we''ll give it the
    name `tf` so that we can access a module by writing `tf` dot and then the module''s
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To create our first graph, we will start by using source operations, which do
    not require any input. These source operations or source ops will pass their information
    to other operations, which will actually run computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create two source operations that will output numbers. We will define
    them as `A` and `B`, which you can see in the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we''ll define a simple computational operation `tf.add()`, used
    to sum two elements. You can also use `C = A + B`, as shown in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Since graphs need to be executed in the context of a session, we need to create
    a session object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To watch the graph, let''s run the session to get the result from the previously
    defined `C` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You're probably thinking that it was a lot of work just to add two numbers together,
    but it's extremely important that you understand the basic structure of TensorFlow.
    Once you do so, you can define any computations that you want; again, TensorFlow's
    structure allows it to handle computations on different devices (CPU or GPU),
    and even in clusters. If you want to learn more about this, you can run the method
    `tf.device()`.
  prefs: []
  type: TYPE_NORMAL
- en: Also feel free to experiment with the structure of TensorFlow in order to get
    a better idea of how it works. If you want a list of all the mathematical operations
    that TensorFlow supports, you can check out the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should understand the structure of TensorFlow and how to create
    a basic applications.
  prefs: []
  type: TYPE_NORMAL
- en: Defining multidimensional arrays using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will try to define such arrays using TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that you understand these data structures, I encourage you to play with
    them using some previous functions to see how they will behave, according to their
    structure types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'With the regular symbol definition and also the `tensorflow` function, we were
    able to get an element-wise multiplication, also known as **Hadamard product**.
    But what if we want the regular matrix product? We need to use another TensorFlow
    function called `tf.matmul()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can also define this multiplication ourselves, but there is a function that
    already does that, so no need to reinvent the wheel!
  prefs: []
  type: TYPE_NORMAL
- en: Why tensors?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tensor structure helps us by giving us the freedom to shape the dataset
    the way we want.
  prefs: []
  type: TYPE_NORMAL
- en: This is particularly helpful when dealing with images, due to the nature of
    how information in images are encoded.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about images, it's easy to understand that it has a height and width,
    so it would make sense to represent the information contained in it with a two-dimensional
    structure (a matrix)... until you remember that images have colors. To add information
    about the colors, we need another dimension, and that's when Tensors become particularly
    helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Images are encoded into color channels; image data is represented in each color''s
    intensity in a color channel at a given point, the most common one being RGB (which
    means red, blue, and green). The information contained in an image is the intensity
    of each channel color in the width and height of the image, just like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37850946-f7a2-47ba-8780-ae325f94f654.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Different color channels for a specific image'
  prefs: []
  type: TYPE_NORMAL
- en: So, the intensity of the red channel at each point with width and height can
    be represented in a matrix; the same goes for the blue and green channels. So,
    we end up having three matrices, and when these are combined, they form a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are more familiar with the structure of data, we will take a look
    at how TensorFlow handles variables.
  prefs: []
  type: TYPE_NORMAL
- en: To define variables, we use the command `tf.variable()`. To be able to use variables
    in a computation graph, it is necessary to initialize them before running the
    graph in a session. This is done by running `tf.global_variables_initializer()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To update the value of a variable, we simply run an assign operation that assigns
    a value to the variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s first create a simple counter, a variable that increases one unit at
    a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Variables must be initialized by running an initialization operation after
    having launched the graph. We first have to add the initialization operation to
    the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We then start a session to run the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first initialize the variables, then print the initial value of the state
    variable, and finally run the operation of updating the state variable and printing
    the result after each update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Placeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we know how to manipulate variables inside TensorFlow, but what about feeding
    data outside of a TensorFlow model?
  prefs: []
  type: TYPE_NORMAL
- en: If you want to feed data to a TensorFlow model from outside a model, you will
    need to use placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: So, what are these placeholders and what do they do? Placeholders can be seen
    as *holes* in your model, *holes* that you will pass the data to. You can create
    them using `tf.placeholder(datatype)`, where `datatype` specifies the type of
    data (integers, floating points, strings, and Booleans) along with its precision
    (8, 16, 32, and 64) bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of each data type with the respective Python syntax is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3 – Definition of different TensorFlow data types**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data type** | **Python type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_FLOAT` | `tf.float32` | 32-bits floating point. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_DOUBLE` | `tf.float64` | 64-bits floating point |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_INT8` | `tf.int8` | 8-bits signed integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_INT16` | `tf.int16` | 16-bits signed integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_INT32` | `tf.int32` | 32-bits signed integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_INT64` | `tf.int64` | 64-bits signed integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_UINT8` | `tf.uint8` | 8-bits unsigned integer. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_STRING` | `tf.string` | Variable length byte arrays. Each element of
    a Tensor is a byte array. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_BOOL` | `tf.bool` | Boolean. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_COMPLEX64` | `tf.complex64` | Complex number made of two 32-bits floating
    points: real and imaginary parts. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_COMPLEX128` | `tf.complex128` | Complex number made of two 64-bits floating
    points: real and imaginary parts. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_QINT8` | `tf.qint8` | 8-bits signed integer used in quantized ops. |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_QINT32` | `tf.qint32` | 32-bits signed integer used in quantized ops.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `DT_QUINT8` | `tf.quint8` | 8-bits unsigned integer used in quantized ops.
    |'
  prefs: []
  type: TYPE_TB
- en: 'So let''s create a placeholder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And define a simple multiplication operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to define and run the session, but since we created a *hole* in
    the model to pass the data, when we initialize the session. We are obliged to
    pass an argument with the data; otherwise we get an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'To pass the data to the model, we call the session with an extra argument,
    `feed_dict`, in which we should pass a dictionary with each placeholder name followed
    by its respective data, just like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Since data in TensorFlow is passed in the form of multidimensional arrays,
    we can pass any kind of tensor through the placeholders to get the answer to the
    simple multiplication operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operations are nodes that represent mathematical operations over the tensors
    on a graph. These operations can be any kind of functions, like add and subtract
    tensors, or maybe an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.matmul`, `tf.add`, and  `tf.nn.sigmoid` are some of the operations in TensorFlow.
    These are like functions in Python, but operate directly over tensors and each
    one does a specific thing.'
  prefs: []
  type: TYPE_NORMAL
- en: Other operations can be easily found at: [https://www.tensorflow.org/api_guides/python/math_ops](https://www.tensorflow.org/api_guides/python/math_ops).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s play around with some of these operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.nn.sigmoid` is an activation function: it''s a little more complicated,
    but this function helps learning models to evaluate what kind of information is
    good or not good.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression model – building and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to our explanation of linear regression in the [Chapter 2](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml),
    *Data Modeling in Action - The Titanic Example* we are going to rely on this definition
    to build a simple linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start off by importing the necessary packages for this implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define an independent variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8e33a510-0612-4242-8cc9-7229a1ae9265.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Visualization of the dependent variable versus the independent one'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how this gets interpreted into a TensorFlow code.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the first part, we will generate random data points and define a linear
    relation; we''ll use TensorFlow to adjust and get the right parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The equation for the model used in this example is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bc580b1-9609-40b2-9c9c-f3cffa70bd0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Nothing special about this equation, it is just a model that we use to generate
    our data points. In fact, you can change the parameters to whatever you want,
    as you will do later. We add some Gaussian noise to the points to make it a bit
    more interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a sample of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we initialize the variables ![](img/7beac794-06f0-4f0c-95d2-8f5344ad94e2.png)
    and ![](img/323cad53-6532-4e69-8234-50b2b8511bc7.png) with any random guess, and
    then we define the linear function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In a typical linear regression model, we minimize the squared error of the equation
    that we want to adjust minus the target values (the data that we have), so we
    define the equation to be minimized as a loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find loss''s value, we use `tf.reduce_mean()`. This function finds the mean
    of a multidimensional tensor, and the result can have a different dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Then, we define the optimizer method. Here, we will use a simple gradient descent
    with a learning rate of 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will define the training method of our graph, but what method will we use
    for minimize the loss? It's `tf.train.GradientDescentOptimizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.minimize()` function will minimize the error function of our optimizer,
    resulting in a better model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to initialize the variables before executing a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to start the optimization and run the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize the training process to fit the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1840b114-b57e-4f87-b90b-6a8082c02b13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Visualization of the data points fitted by a regression line'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression model – building and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Also based on our explanation of logistic regression in [Chapter 2](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml),
    *Data Modeling in Action - The Titanic Example*, we are going to implement the
    logistic regression algorithm in TensorFlow. So, briefly, logistic regression
    passes the input through the logistic/sigmoid but then treats the result as a
    probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06e797f4-9584-46f6-8915-d1177426c2bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Discriminating between two linearly separable classes, 0''s and
    1''s'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing logistic regression in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For us to utilize logistic regression in TensorFlow, we first need to import
    whatever libraries we are going to use. To do so, you can run this code cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will load the dataset we are going to use. In this case, we are utilizing
    the iris dataset, which is inbuilt. So, there''s no need to do any preprocessing
    and we can jump right into manipulating it. We separate the dataset into *x*''s
    and *y*''s, and then into training *x*''s and *y*''s and testing *x*''s and *y*''s,
    (pseudo) randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define *x* and *y*. These placeholders will hold our iris data (both
    the features and label matrices) and help pass them along to different parts of
    the algorithm. You can consider placeholders as empty shells into which we insert
    our data. We also need to give them shapes that correspond to the shape of our
    data. Later, we will insert data into these placeholders by feeding the placeholders
    the data via a `feed_dict` (feed dictionary):'
  prefs: []
  type: TYPE_NORMAL
- en: Why use placeholders?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This feature of TensorFlow allows us to create an algorithm that accepts data
    and knows something about the shape of the data without knowing the amount of
    data going in. When we insert *batches* of data in training, we can easily adjust
    how many examples we train on in a single step without changing the entire algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Set model weights and bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much like linear regression, we need a shared variable weight matrix for logistic
    regression. We initialize both *W* and *b* as tensors full of zeros. Since we
    are going to learn *W* and *b*, their initial value doesn't matter too much. These
    variables are the objects that define the structure of our regression model, and
    we can save them after they've been trained so that we can reuse them later.
  prefs: []
  type: TYPE_NORMAL
- en: We define two TensorFlow variables as our parameters. These variables will hold
    the weights and biases of our logistic regression and they will be continually
    updated during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that *W* has a shape of [4, 3] because we want to multiply the 4-dimensional
    input vectors by it to produce 3-dimensional vectors of evidence for the difference
    classes. *b* has a shape of [3], so we can add it to the output. Moreover, unlike
    our placeholders (which are essentially empty shells waiting to be fed data),
    TensorFlow variables need to be initialized with values, say, with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now define our operations in order to properly run the logistic regression.
    Logistic regression is typically thought of as a single equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5e9a90e-75b4-4675-878e-4be59ea25db9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, for the sake of clarity, we can have it broken into its three main
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: A weight times features matrix multiplication operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summation of the weighted features and a bias term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the application of a sigmoid function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As such, you will find these components defined as three separate operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: As we have seen previously, the function we are going to use is the logistic
    function, which is fed the input data after applying weights and bias. In TensorFlow,
    this function is implemented as the `nn.sigmoid` function. Effectively, it fits
    the weighted input with bias into a 0-100 percent curve, which is the probability
    function we want.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The learning algorithm is how we search for the best weight vector (*w*). This
    search is an optimization problem looking for the hypothesis that optimizes an
    error/cost measure.
  prefs: []
  type: TYPE_NORMAL
- en: So, the cost or the loss function of the model is going to tell us our model
    is bad, and we need to minimize this function. There are different loss or cost
    criteria that you can follow. In this implementation, we are going to use **mean ****squared
    error** (**MSE**) as a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish the task of minimizing the loss function, we are going to use
    the gradient descent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Cost function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before defining our cost function, we need to define how long we are going
    to train and how we should define the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now, it's time to execute our computational graph through the session variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'So first off, we need to initialize our weights and biases with zeros or random
    values using `tf.initialize_all_variables()`. This initialization step will become
    a node in our computational graph, and when we put the graph into a session, the
    operation will run and create the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to see how our trained model performs on the `iris` dataset,
    so let''s test our trained model against the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Getting 0.9 accuracy on the test set is really good and you can try to get better
    results by changing the number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through a basic explanation of neural networks and
    and the need for multi-layer neural networks. We also covered the TensorFlow computational
    graph model with some basic examples, such as linear regression and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we will go through more advanced examples and demonstrate how TensorFlow
    can be used to build something like handwritten character recognition. We will
    also tackle the core idea of architecture engineering that has replaced feature
    engineering in traditional machine learning.
  prefs: []
  type: TYPE_NORMAL
