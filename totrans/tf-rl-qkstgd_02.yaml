- en: Temporal Difference, SARSA, and Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at the basics of RL. In this chapter, we
    will cover **temporal difference** (**TD**) learning, SARSA, and Q-learning, which
    were very widely used algorithms in RL before deep RL became more common. Understanding
    these older-generation algorithms is essential if you want to master the field,
    and will also lay the foundation for delving into deep RL. We will therefore spend
    this chapter looking at examples using these older generation algorithms. In addition,
    we will also code some of these algorithms using Python. We will not be using
    TensorFlow for this chapter, as the problems do not involve any deep neural networks
    under study. However, this chapter will lay the groundwork for more advanced topics
    that we will cover in the subsequent chapters, and will also be our first coding
    experience of an RL algorithm. Specifically, this chapter will be our first deep
    dive into a standard RL algorithm, and how you can use it to train RL agents for
    a specific task. It will also be our first hands-on effort at RL, including both
    theory and practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the topics that will be covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding TD learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning SARSA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cliff walking with SARSA and Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid world with SARSA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Knowledge of the following will help you to better understand the concepts
    presented in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Python (version 2 or 3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (version 1.4 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding TD learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first learn about TD learning. This is a very fundamental concept in
    RL. In TD learning, the learning of the agent is attained by experience. Several
    trial episodes are undertaken of the environment, and the rewards accrued are
    used to update the value functions. Specifically, the agent will keep an update
    of the state-action value functions as it experiences new states/actions. The
    Bellman equation is used to update this state-action value function, and the goal
    is to minimize the TD error. This essentially means the agent is reducing its
    uncertainty of which action is the optimal action in a given state; it gains confidence
    on the optimal action in a given state by lowering the TD error.
  prefs: []
  type: TYPE_NORMAL
- en: Relation between the value functions and state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The value function is an agent's estimate of how good a given state is. For
    instance, if a robot is near the edge of a cliff and may fall, that state is bad
    and must have a low value. On the other hand, if the robot/agent is near its final
    goal, that state is a good state to be in, as the rewards they will soon receive
    are high, and so that state will have a higher value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value function, *V*, is updated after reaching a *s[t] *state and receiving
    a *r[t]* reward from the environment. The simplest TD learning algorithm is called
    *TD(0)* and performs an update using the following equation where *α* is the learning
    rate and *0 ≤ α ≤ 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5af87999-0b2c-43b2-bb23-aaff79a6b0e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in some reference papers or books, the preceding formula will have
    *r[t]* instead of *r[t+1]*. This is just a difference in convention and is not
    an error; *r[t+1]* here denotes the reward received from *s[t]* stateand transitioning
    to *s[t+1]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also another TD learning variant called *TD(λ)* that used eligibility
    traces *e(s),* which are a record of visiting a state. More formally, we perform
    a *TD(λ)* update as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79783f9a-d161-4dae-9041-68ded414b74a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The eligibility traces are given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcb459a2-8c15-4304-8038-4473ebaef015.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *e(s) = 0* at *t = 0*. For each step the agent takes, the eligibility
    trace decreases by *γλ* for all states, and is incremented by *1* for the state
    visited in the current time step. Here, *0 ≤ λ ≤ 1*, and it is a parameter that
    decides how much of the credit from a reward is to be assigned to distant states.
    Next, we will look at the theory behind our next two RL algorithms, SARSA and
    Q-learning, both of which are quite popular in the RL community.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SARSA and Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about SARSA and Q-Learning and how can they are
    coded with Python. Before we go further, let's find out what SARSA and Q-Learning
    are. SARSA is an algorithm that uses the state-action Q values to update. These
    concepts are derived from the computer science field of dynamic programming, while Q-learning
    is an off-policy algorithm that was first proposed by Christopher Watkins in 1989,
    and is a widely used RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Learning SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SARSA** is another on-policy algorithm that was very popular, particularly
    in the 1990s. It is an extension of TD-learning, which we saw previously, and
    is an on-policy algorithm. SARSA keeps an update of the state-action value function,
    and as new experiences are encountered, this state-action value function is updated
    using the Bellman equation of dynamic programming. We extend the preceding TD
    algorithm to state-action value function, *Q(s[t],a[t])*, and this approach is
    called SARSA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54c7d075-6c67-403c-8d81-bb5c4e19cb39.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, from a given state *s[t]*, we take action *a[t]*, receive a reward *r[t+1]*,
    transition to a new state *s[t+1]*, and thereafter take an action *a[t+1]* that
    then continues on and on. This quintuple (*s[t]*, *a[t]*, *r[t+1]*, *s[t+1]*,
    *a[t+1]*) gives the algorithm the name SARSA. SARSA is an on-policy algorithm,
    as the same policy is updated as was used to estimate Q. For exploration, you
    can use, say, *ε*-greedy.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is an off-policy algorithm that was first proposed by Christopher
    Watkins in 1989, and is a widely used RL algorithm. Q-learning, such as SARSA,
    keeps an update of the state-action value function for each state-action pair,
    and recursively updates it using the Bellman equation of dynamic programming as
    new experiences are collected. Note that it is an off-policy algorithm as it uses
    the state-action value function evaluated at the action, which will maximize the
    value. Q-learning is used for problems where the actions are discrete – for example,
    if we have the actions move north, move south, move east, move west, and we are
    to decide the optimum action in a given state, then Q-learning is applicable in
    such settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the classical Q-learning approach, the update is given as follows, where
    the max is performed over actions, that is, we choose the action a corresponding to
    the maximum value of Q at state *s[t+1]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3536e18c-a044-4baa-a15b-c7703069b46a.png)'
  prefs: []
  type: TYPE_IMG
- en: The *α* is the learning rate, which is a hyper-parameter that the user can specify.
  prefs: []
  type: TYPE_NORMAL
- en: Before we code the algorithms in Python, let's find out what kind of problems
    will be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Cliff walking and grid world problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider cliff walking and grid world problems. First, we will introduce
    these problems to you, then we will proceed on to the coding part. For both problems,
    we consider a rectangular grid with `nrows` (number of rows) and `ncols` (number
    of columns). We start from one cell to the south of the bottom left cell, and
    the goal is to reach the destination, which is one cell to the south of the bottom
    right cell.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the start and destination cells are not part of the `nrows` x `ncols`
    grid of cells. For the cliff walking problem, the cells to the south of the bottom
    row of cells, except for the start and destination cells, form a cliff where,
    if the agent enters, the episode ends with catastrophic fall into the cliff. Likewise,
    if the agent tries to leave the left, top, or right boundaries of the grid of
    cells, it is placed back in the same cell, that is, it is equivalent to taking
    no action.
  prefs: []
  type: TYPE_NORMAL
- en: For the grid world problem, we do not have a cliff, but we have obstacles inside
    the grid world. If the agent tries to enter any of these obstacle cells, it is
    bounced back to the same cell from which it came. In both these problems, the
    goal is to find the optimum path from the start to the destination.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's dive on in!
  prefs: []
  type: TYPE_NORMAL
- en: Cliff walking with SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now learn how to code the aforementioned equations in Python and implement
    the cliff walking problem with SARSA. First, let''s import the `numpy`, `sys`,
    and `matplotlib` packages in Python. If you have not used these packages in the
    past, there are several Packt books on these topics to help you come up to speed.
    Type the following command to install the required packages in a Linux Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now summarize the code involved to solve the grid world problem. In
    a Terminal, use your favorite editor (for example, gedit, emacs, or vi) to code
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use a 3 x 12 grid for the cliff walking problem, that is, `3` rows
    and `12` columns. We also have `4` actions to take at any cell. You can go north,
    east, south, or west:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will consider `100000` episodes in total. For exploration, we will use ε-greedy
    with a value of *ε* = `0.1.` We will consider a constant value for *ε*, although
    the interested reader is encouraged to consider variable values for *ε* as well
    with its value slowly annealed to zero over the course of the episodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning rate, *α*, is chosen as `0.1`, and the discount factor *γ* = `0.95`
    is used, which are typical values for this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will next assign values for the rewards. For any normal action that does
    not fall into the cliff, the reward is `-1`; if the agent falls down the cliff,
    the reward is `-100`; for reaching the destination, the reward is also `-1`. Feel
    free to explore other values for these rewards later, and investigate its effect
    on the final Q values and path taken from start to destination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The Q values for state-action pairs are initialized to zero. We will use a
    NumPy array for Q, which is `nrows` x `ncols` x `nact`, that is, we have a `nact`
    number of entries for each cell, and we have `nrows` x `ncols` total number of
    cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define a function to make the agent go to the start location, which
    has (*x, y*) co-ordinates of (`x=0`, `y=nrows`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a function to take a random action, where we define `a = 0`
    for moving `top/north`, `a = 1` for moving `right/east`, `a = 2` for moving `bottom/south`,
    and `a = 4` for moving `left/west`. Specifically, we will use NumPy''s `random.randint()`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define the `move` function, which will take a given (*x, y*) location
    of the agent and the current action, `a`, and then will perform the action. It
    will return the new location of the agent after taking the action, (*x1, y1*),
    as well as the state of the agent, which we define as `state = 0` for the agent
    to be `OK` after taking the action; `state = 1` for reaching the destination;
    and `state = 2` for falling into the cliff. If the agent leaves the domain through
    the left, top, or right, it is sent back to the same grid (equivalent to taking
    no action):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will next define the `exploit` function, which will take the (*x, y*) location
    of the agent and take the greedy action based on the Q values, that is, it will
    take the `a` action that has the highest Q value at that (*x, y*) location. We
    will do this using NumPy''s `np.argmax()`. If we are at the start location, we
    go north (`a = 0`); if we are one step away from the destination, we go south
    (`a = 2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will perform the Bellman update using the following `bellman()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then define a function to explore or exploit, depending on a random
    number less than *ε*, the parameter we use in the ε-greedy exploration strategy.
    For this, we will use NumPy''s `np.random.uniform()`, which will output a real
    random number between zero and one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have all the functions required for the cliff walking problem. So, we
    will loop over the episodes and for each episode we start at the starting location,
    then explore or exploit, then we move the agent one step depending on the action.
    Here is the Python code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform the Bellman update based on the rewards received; note that this
    is based on the equations presented earlier in this chapter in the theory section.
    We stop the episode if we reach the destination or fall down the cliff; if not,
    we continue the exploration or exploitation strategy for one more step, and this
    goes on and on. The `state` variable in the following code takes the `1` value
    for reaching the destination, takes the value `2` for falling down the cliff,
    and is `0` otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will complete all the episodes, and we now have the converged
    values of Q. We will now plot this using `matplotlib` for each of the actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will do a path planning using the preceding converged Q values.
    That is, we will plot the exact path the agent will take from start to finish
    using the final converged Q values. For this, we will create a variable called
    `path`, and store values for it tracing the `path`. We will then plot it using
    `matplotlib` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: That's it. We have completed the coding required for the cliff walking problem
    with SARSA. We will now view the results. In the following screenshot, we present
    the Q values for each of the actions (going north, east, south, or west) at each
    of the locations in the grid. As per the legend, yellow represents high Q values
    and violet represents low Q values.
  prefs: []
  type: TYPE_NORMAL
- en: 'SARSA clearly tries to avoid the cliff by choosing to not go south when the
    agent is just one step to the north of the cliff, as is evident from the large
    negative Q values for the south action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/505d9930-d182-4645-a9a4-014be72a8096.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Q values for the cliff walking problem using SARSA'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will next plot the path taken by the agent from start to finish in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65270e16-dedb-4ed8-88e7-d5c04a3c5497.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Path taken by the agent using SARSA'
  prefs: []
  type: TYPE_NORMAL
- en: The same cliff walking problem will now be investigated using Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Cliff walking with Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now repeat the same cliff walking problem, albeit using Q-learning in
    lieu of SARSA. Most of the code is the same as was used for SARSA, except for
    a few differences that will be summarized here. Since Q-learning uses a greedy
    action selection strategy, we will use a function for this as follows to compute
    the value of the maximum value of Q at a given location. Most of the code is the
    same as in the previous section, so we will only specify the changes to be made.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's code cliff walking with Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `max_Q()` function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will compute the Q value at the new state using the `max_Q()` function defined
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the action choosing whether it is exploration or exploitation
    is done inside the `while` loop as we choose actions greedily when exploiting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it for coding Q-learning. We will now apply this to solve the cliff
    walking problem and present the Q values for each of the actions, and the path
    traced by the agent to go from start to finish, which are shown in the following
    screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5369a08-d1bf-4c24-a6cd-402953bf2d65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Q values for the cliff walking problem using Q-learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'As evident, the path traced is now different for Q-learning vis-à-vis SARSA.
    Since Q-learning is a greedy strategy, the agent now takes a path close to the
    cliff at the bottom of the following screenshot (*Figure 4*), as it is the shortest
    path:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1129e881-0d54-4ac3-9dc7-bf086e65ba38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Path traced by the agent for the cliff walking problem using Q-learning'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, since SARSA is more far-sighted, and so chooses the safe
    but longer path that is the top row of cells (see *Figure 2*).
  prefs: []
  type: TYPE_NORMAL
- en: Our next problem is the grid world problem, where we must navigate a grid. We
    will code this in SARSA.
  prefs: []
  type: TYPE_NORMAL
- en: Grid world with SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will next consider the grid world problem, and we will use SARSA to solve
    it. We will introduce obstacles in place of a cliff. The goal of the agent is
    to navigate the grid world from start to destination by avoiding the obstacles.
    We will store the coordinates of the obstacle cells in the `obstacle_cells` list,
    where each entry is the (*x, y*) coordinate of the obstacle cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the steps involved in this task:'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the code is the same as previously used, the differences will be summarized
    here
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Place obstacles in the grid
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `move()` function has to also look for obstacles in the grid
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot Q values and the path traced by the agent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we will start coding the algorithm in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `move()` function will now change, as we have to also look for obstacles.
    If the agent ends up in one of the obstacle cells it is pushed back to where it
    came from, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it for coding grid world with SARSA. The Q-values and the path taken
    are shown in following diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fea0ff6-2606-4c75-9d97-0e2b04560463.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Q-values for each of the actions for the grid world problem using
    SARSA'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following diagram, the agent navigates around the obstacles
    to reach its destination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce653ae1-9920-48d1-87d8-7de42b1930c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Path traced by the agent for the grid world problem using SARSA'
  prefs: []
  type: TYPE_NORMAL
- en: Grid world with Q-learning is not a straightforward problem to attempt, as the
    greedy strategy used will not enable the agent to avoid repeated actions easily
    at a given state. Convergence is typically very slow, and so it will be avoided
    for now.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we looked at the concept of TD. We also learned about our
    first two RL algorithms: Q-learning and SARSA. We saw how you can code these two
    algorithms in Python and use them to solve the cliff walking and grid world problems.
    These two algorithms give us a good understanding of the basics of RL and how
    to transition from theory to code. These two algorithms were very popular in the
    1990s and early 2000s, before deep RL gained prominence. Despite that, Q-learning
    and SARSA still find use in the RL community today.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the use of deep neural networks in RL that
    gives rise to deep RL. We will see a variant of Q-learning called **Deep Q-Networks**
    (**DQNs**) that will use a neural network instead of a tabular state-action value
    function, which we saw in this chapter. Note that only problems with small number
    of states and actions are suited to Q-learning and SARSA. When we have a large
    number of states and/or actions, we encounter what is called as the Curse of Dimensionality,
    where a tabular approach will be unfeasible due to excessive memory use; in these
    problems, DQN is better suited, and will be the crux of the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Reinforcement Learning: an Introduction* by *Richard Sutton and Andrew Barto*,
    2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
