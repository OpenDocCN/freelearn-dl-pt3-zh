["```\nALGO = \"DDQN\" #\"DQN\" # DDQN\n```", "```\n# calculate q values and targets \n\nif (ALGO == 'DQN'): \n\n   q_values_next = target_net.predict(sess, next_states_batch)\n   greedy_q = np.amax(q_values_next, axis=1) \n   targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * gamma * greedy_q\n\nelif (ALGO == 'DDQN'):\n\n   q_values_next = q_net.predict(sess, next_states_batch)\n   greedy_q = np.argmax(q_values_next, axis=1)\n   q_values_next_target = target_net.predict(sess, next_states_batch)\n   targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * gamma * q_values_next_target[np.arange(batch_size), greedy_q]\n```", "```\nDUELING = True # False\n```", "```\nif (not DUELING):\n\n     # Q(s,a)\n     self.predictions = tf.contrib.layers.fully_connected(fc1, len(self.VALID_ACTIONS), activation_fn=None, weights_initializer=winit)\n\n else:\n\n     # Deuling network\n     # branch out into two streams using flattened (i.e., ignore fc1 for Dueling DQN)\n\n     valuestream = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu, weights_initializer=winit) \n     advantagestream = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu, weights_initializer=winit) \n```", "```\n# A(s,a)\nself.advantage = tf.contrib.layers.fully_connected(advantagestream, len(self.VALID_ACTIONS), activation_fn=None, weights_initializer=winit)\n\n# V(s)\nself.value = tf.contrib.layers.fully_connected(valuestream, 1, activation_fn=None, weights_initializer=winit)\n\n# Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\nself.predictions = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keep_dims=True))\n```", "```\ngit clone https://github.com/google/dopamine.git\n```", "```\ncd dopamine\nexport PYTHONPATH=${PYTHONPATH}:.\npython tests/atari_init_test.py\n```", "```\n2018-10-27 23:08:17.810679: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-10-27 23:08:18.079916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-10-27 23:08:18.080741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \nname: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.48\npciBusID: 0000:01:00.0\ntotalMemory: 5.93GiB freeMemory: 5.54GiB\n2018-10-27 23:08:18.080783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\n2018-10-27 23:08:24.476173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-10-27 23:08:24.476247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958] 0 \n2018-10-27 23:08:24.476273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0: N \n2018-10-27 23:08:24.476881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5316 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)\n.\n.\n.\nRan 2 tests in 8.475s\n\nOK\n```", "```\npython -um dopamine.atari.train --agent_name=rainbow --base_dir=/tmp/dopamine --gin_files='dopamine/agents/rainbow/configs/rainbow.gin'\n```", "```\ndopamine/dopamine/agents/rainbow/configs/rainbow.gin\n```", "```\n# Hyperparameters follow Hessel et al. (2018), except for sticky_actions,\n# which was False (not using sticky actions) in the original paper.\nimport dopamine.agents.rainbow.rainbow_agent\nimport dopamine.atari.run_experiment\nimport dopamine.replay_memory.prioritized_replay_buffer\nimport gin.tf.external_configurables\n\nRainbowAgent.num_atoms = 51\nRainbowAgent.vmax = 10.\nRainbowAgent.gamma = 0.99\nRainbowAgent.update_horizon = 3\nRainbowAgent.min_replay_history = 20000 # agent steps\nRainbowAgent.update_period = 4\nRainbowAgent.target_update_period = 8000 # agent steps\nRainbowAgent.epsilon_train = 0.01\nRainbowAgent.epsilon_eval = 0.001\nRainbowAgent.epsilon_decay_period = 250000 # agent steps\nRainbowAgent.replay_scheme = 'prioritized'\nRainbowAgent.tf_device = '/gpu:0' # use '/cpu:*' for non-GPU version\nRainbowAgent.optimizer = @tf.train.AdamOptimizer()\n\n# Note these parameters are different from C51's.\ntf.train.AdamOptimizer.learning_rate = 0.0000625\ntf.train.AdamOptimizer.epsilon = 0.00015\n\nRunner.game_name = 'Pong'\n# Sticky actions with probability 0.25, as suggested by (Machado et al., 2017).\nRunner.sticky_actions = True\nRunner.num_iterations = 200\nRunner.training_steps = 250000 # agent steps\nRunner.evaluation_steps = 125000 # agent steps\nRunner.max_steps_per_episode = 27000 # agent steps\n\nWrappedPrioritizedReplayBuffer.replay_capacity = 1000000\nWrappedPrioritizedReplayBuffer.batch_size = 32\n```"]