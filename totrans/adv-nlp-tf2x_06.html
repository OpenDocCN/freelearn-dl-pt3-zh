<html><head></head><body>
  <div id="_idContainer141">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-97" class="chapterTitle">Text Summarization with Seq2seq Attention and Transformer Networks</h1>
    <p class="normal">Summarizing a piece of text challenges a deep learning model's understanding of language. Summarization can be considered a uniquely human ability, where the gist of a piece of text needs to be understood and phrased. In the previous chapters, we have built components that can help in summarization. First, we used BERT to encode text and perform sentiment analysis. Then, we used a decoder architecture with GPT-2 to generate text. Putting the Encoder and Decoder together yields a summarization model. In this chapter, we will implement a seq2seq Encoder-Decoder with Bahdanau Attention. Specifically, we will cover the following topics:</p>
    <ul>
      <li class="bullet">Overview of extractive and abstractive text summarization</li>
      <li class="bullet">Building a seq2seq model with attention to summarize text</li>
      <li class="bullet">Improving summarization with beam search</li>
      <li class="bullet">Addressing beam search issues with length normalizations</li>
      <li class="bullet">Measuring the performance of summarization with ROUGE metrics</li>
      <li class="bullet">A review of state-of-the-art summarization</li>
    </ul>
    <p class="normal">The first step of this journey begins with understanding the main ideas behind text summarization. It is important to understand the task before building a model.</p>
    <h1 id="_idParaDest-98" class="title">Overview of text summarization</h1>
    <p class="normal">The core idea in <a id="_idIndexMarker380"/>summarization is to condense long-form text or articles into a short representation. The shorter representation should contain the main idea of crucial information from the longer form. A single document can be summarized. This document could be long or may contain just a couple of sentences. An example of a short document summarization is generating a headline from the first <a id="_idIndexMarker381"/>few sentences of an article. This is called <strong class="keyword">sentence compression</strong>. When multiple documents are being summarized, they are usually related. They could <a id="_idIndexMarker382"/>be the financial reports of a company or news reports about an event. The generated summary could itself be long or short. A shorter summary would be desirable when generating a headline. A lengthier summary would be something like an abstract and could have multiple sentences.</p>
    <p class="normal">There are two main approaches when summarizing text:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Extractive summarization</strong>: Phrases or sentences from the articles are selected and <a id="_idIndexMarker383"/>put together to create a summary. A mental model for this approach is using a highlighter on the long-form text, and the summary is the highlights put together. Extractive summarization is a more straightforward approach as sentences from the source text can be copied, which leads to fewer grammatical issues. The quality of the summarization is also easier to measure using metrics such as ROUGE. This metric is detailed later in this chapter. Extractive summarization was the predominant approach before deep learning and neural networks.</li>
      <li class="bullet"><strong class="keyword">Abstractive summarization</strong>: A person may use the full vocabulary available in a language while summarizing an article. They are not restricted to only using words <a id="_idIndexMarker384"/>from the article. The mental model is that the person is penning a new piece of text. The model must have some understanding of the meaning of different words so that the model can use them in the summary. Abstractive summarization is quite hard to implement and evaluate. The advent of the seq2seq architecture made significant improvements to the quality of abstractive summarization models.</li>
    </ul>
    <p class="normal">This chapter <a id="_idIndexMarker385"/>focuses on abstractive summarization. Here are some examples of summaries that our model can generate:</p>
    <table id="table001-4">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Source text</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Generated summary</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">american airlines group inc said on sunday it plans to raise ## billion by selling shares and convertible senior notes , to improve the airline's liquidity as it grapples with travel restrictions caused by the coronavirus .</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">american airlines to raise ## <strong class="keyword">bln</strong> convertible <strong class="keyword">bond issue</strong></p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">sales of newly-built single-family houses occurred at a seasonally adjusted annual rate of ## in may , that represented a #.#% increase from the downwardly revised pace of ## in april .</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><strong class="keyword">new home</strong> sales <strong class="keyword">rise</strong> in may</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">jc penney will close another ## stores for good . the department store chain , which filed for bankruptcy last month , is inching toward its target of closing ## stores .</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">jc penney to close <strong class="keyword">more</strong> stores</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The source text was pre-processed to be all in lowercase, and numbers were replaced with placeholder tokens to prevent the model from inventing numbers in the summary. The generated <a id="_idIndexMarker386"/>summaries have some words highlighted. Those words were not present in the source text. The model was able to propose these words in the summary. Thus, the model is an abstractive summarization model. So, how can such a model be built?</p>
    <p class="normal">One way of looking at the summarization problem is that the model is <em class="italic">translating</em> an input sequence of tokens into a smaller set of output tokens. The model learns the output lengths based on the supervised examples provided. Another well-known problem is mapping an input sequence to an output sequence – the problem of Neural Machine Translation or NMT. In NMT, the input sequence could be a sentence from the source language, and the output could be a sequence of tokens in the target language. The process for translation is as follows:</p>
    <ol>
      <li class="numbered">Convert the input text into tokens</li>
      <li class="numbered">Learn embeddings for these tokens</li>
      <li class="numbered">Pass the token embeddings through an encoder to calculate the hidden states and outputs</li>
      <li class="numbered">Use the hidden states with the attention mechanism for generating a context vector for the inputs</li>
      <li class="numbered">Pass encoder outputs, hidden states, and context vectors to the decoder part of the network</li>
      <li class="numbered">Generate the outputs from left to right using an autoregressive model</li>
    </ol>
    <p class="normal">Google AI published <a id="_idIndexMarker387"/>a tutorial on NMT using a seq2seq attention model in July 2017. This model uses a left-to-right encoder with GRU cells. The Decoder also uses GRU cells. In summarization, the piece of text to be summarized is a prerequisite. This may or may not be valid for machine translation. In some cases, the translation is performed on the fly. In that case, a left-to-right encoder is useful. However, if the entire text to be translated or summarized is available from the outset, a bi-directional Encoder can encode context from both sides of a given token. BiRNN in the Encoder leads to much better performance of the overall model. The NMT tutorial code serves as inspiration for the seq2seq attention model and the attention tutorial referenced previously. Before we work on the model, let's look at the datasets that are used for this purpose.</p>
    <h1 id="_idParaDest-99" class="title">Data loading and pre-processing</h1>
    <p class="normal">There are several summarization-related datasets available for training. These datasets are available through the TensorFlow Datasets or <code class="Code-In-Text--PACKT-">tfds</code> package, which we have used in the previous chapters as well. The datasets that are available differ in <a id="_idIndexMarker388"/>length and style. The CNN/DailyMail dataset is one of the most commonly used datasets. It was published in 2015, with approximately a total of 1 million news articles. Articles from CNN, starting in 2007, and Daily Mail, starting in 2010, were collected until 2015. The summaries are usually multi-sentence. The Newsroom dataset, available from <a href="https://summari.es"><span class="url">https://summari.es</span></a>, contains over 1.3 million news articles from 38 publications. However, this dataset requires<a id="_idIndexMarker389"/> that you register to download it, which is why it is not used in this book. The wikiHow data set contains full Wikipedia article pages and the summary sentences for those articles. The LCSTS data set contains Chinese language data collected from Sina Weibo with paragraphs and their one-sentence summaries.</p>
    <p class="normal">Another popular dataset is the Gigaword dataset. It provides the first one or two sentences of a news story and has the headline of the story as the summary. This dataset is quite large, with just under 4 million rows. This dataset was published in a paper titled <em class="italic">Annotated Gigaword</em> by Napoles et al. in 2011. It is quite easy to import this dataset using <code class="Code-In-Text--PACKT-">tfds</code>. Given the large size of the dataset and long training times for the model, the training code is stored in Python files, while the inference code is in an IPython notebook. This pattern was used in the previous chapter as well. The code for training is in the <code class="Code-In-Text--PACKT-">s2s-training.py</code> file. The top part of the file contains the imports and a method called <code class="Code-In-Text--PACKT-">setupGPU()</code> to initialize the GPU. The file contains a main function, which provides the control flow, and several functions that perform specific actions. </p>
    <p class="normal">The dataset needs to be loaded first. The code for loading the data is in the <code class="Code-In-Text--PACKT-">load_data()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">load_data</span><span class="hljs-functio">():</span>
    print(<span class="hljs-string">"Loading the dataset"</span>)
    (ds_train, ds_val, ds_test), ds_info = tfds.load(
        <span class="hljs-string">'gigaword'</span>,
        split=[<span class="hljs-string">'train'</span>, <span class="hljs-string">'validation'</span>, <span class="hljs-string">'test'</span>],
        shuffle_files=<span class="hljs-literal">True</span>,
        as_supervised=<span class="hljs-literal">True</span>,
        with_info=<span class="hljs-literal">True</span>,
    )
    <span class="hljs-keyword">return</span> ds_train, ds_val, ds_test 
</code></pre>
    <p class="normal">The corresponding section in the main function looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    setupGPU()  <span class="hljs-comment"># OPTIONAL – only if using GPU</span>
    ds_train, _, _ = load_data()
</code></pre>
    <p class="normal">Only the training dataset is being loaded. The validation dataset contains approximately 190,000 examples, while the<a id="_idIndexMarker390"/> test split contains over 1,900 examples. In contrast, the training set contains over 3.8 million examples. Depending on the internet connection, downloading the dataset may<a id="_idIndexMarker391"/> take a while:</p>
    <pre class="programlisting con"><code class="hljs-con">Downloading and preparing dataset gigaword/1.2.0 (download: 551.61 MiB, generated: Unknown size, total: 551.61 MiB) to /xxx/tensorflow_datasets/gigaword/1.2.0...
/xxx/anaconda3/envs/tf21g/lib/python3.7/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning, 
  InsecureRequestWarning,
Shuffling and writing examples to /xxx/tensorflow_datasets/gigaword/1.2.0.incomplete1FP5M4/gigaword-train.tfrecord
100%
&lt;snip/&gt;
100%
1950/1951 [00:00&lt;00:00, 45393.40 examples/s]
Dataset gigaword downloaded and prepared to /xxx/tensorflow_datasets/gigaword/1.2.0. Subsequent calls will reuse this data.
</code></pre>
    <p class="normal">The warning about insecure requests can be safely ignored. The data is now ready to be tokenized and vectorized.</p>
    <h1 id="_idParaDest-100" class="title">Data tokenization and vectorization</h1>
    <p class="normal">The Gigaword dataset has been already cleaned, normalized, and tokenized using the StanfordNLP tokenizer. All the data is converted into lowercase and normalized using the StanfordNLP tokenizer, as seen in the preceding examples. The main task in this step is to create a vocabulary. A word-based tokenizer is the most common choice in summarization. However, we will use a<a id="_idIndexMarker392"/> subword tokenizer in this chapter. A subword tokenizer provides the benefit of limiting the size of the vocabulary while minimizing the number of unknown words. <em class="chapterRef">Chapter 3</em>, <em class="italic">Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding</em>, on BERT, described different types of <a id="_idIndexMarker393"/>tokenizers. Consequently, models such specifically the part as BERT and GPT-2 use some variant of a subword tokenizer. The <code class="Code-In-Text--PACKT-">tfds</code> package provides a way for us to create a subword tokenizer, initialized from a corpus of text. Since generating the vocabulary requires running it over all of the training data, this process can be slow. After initialization, the tokenizer can be persisted to disk for future use. The code for this process is defined in the <code class="Code-In-Text--PACKT-">get_tokenizer()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">get_tokenizer</span><span class="hljs-functio">(</span><span class="hljs-params">data, file=</span><span class="hljs-string">"gigaword32k.enc"</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> os.path.exists(file+.subwords):
        <span class="hljs-comment"># data has already been tokenized - just load and return</span>
        tokenizer = \
tfds.features.text.SubwordTextEncoder.load_from_file(file)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-comment"># This takes a while</span>
        tokenizer = \
tfds.features.text.SubwordTextEncoder.build_from_corpus(
        ((art.numpy() + <span class="hljs-string">b" "</span> + smm.numpy()) <span class="hljs-keyword">for</span> art, smm <span class="hljs-keyword">in</span> data),
        target_vocab_size=<span class="hljs-number">2</span>**<span class="hljs-number">15</span>
        )  <span class="hljs-comment"># End tokenizer construction</span>
        tokenizer.save_to_file(file)  <span class="hljs-comment"># save for future iterations</span>
   
   print(<span class="hljs-string">"Tokenizer ready. Total vocabulary size: "</span>, tokenizer.vocab_size)
   <span class="hljs-keyword">return</span> tokenizer
</code></pre>
    <p class="normal">This method checks to see if a subword tokenizer is saved and loads it. If no tokenizer exists on disk, it creates one by feeding in the articles and summaries combined. Note that creating a new tokenizer took over 20 minutes on my machine. </p>
    <p class="normal">Hence, it is a good idea to do this process only once and persist the results for future use. The GitHub folder for this chapter contains a saved version of the tokenizer to save some of your time.</p>
    <p class="normal">Two additional tokens that denote the start and end of a sequence are added to the vocabulary after its creation. These tokens help the model start and end the inputs and outputs. The end <a id="_idIndexMarker394"/>of sequence token provides a way for the Decoder, which generates the summary, to signal the end of the summary. The main method at this point looks like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    setupGPU()  <span class="hljs-comment"># OPTIONAL - only if using GPU</span>
    ds_train, _, _ = load_data()
    tokenizer = get_tokenizer(ds_train)
    <span class="hljs-comment"># Test tokenizer</span>
    txt = <span class="hljs-string">"Coronavirus spread surprised everyone"</span>
    print(txt, <span class="hljs-string">" =&gt; "</span>, tokenizer.encode(txt.lower()))
    <span class="hljs-keyword">for</span> ts <span class="hljs-keyword">in</span> tokenizer.encode(txt.lower()):
        <span class="hljs-built_in">print</span> (<span class="hljs-string">'{} ----&gt; {}'</span>.<span class="hljs-built_in">format</span>(ts, tokenizer.decode([ts])))
    <span class="hljs-comment"># add start and end of sentence tokens</span>
    start = tokenizer.vocab_size + <span class="hljs-number">1</span> 
    end = tokenizer.vocab_size
    vocab_size = end + <span class="hljs-number">2</span>
</code></pre>
    <p class="normal">Articles and their summaries can be tokenized using the tokenizer. Articles can be of varying lengths <a id="_idIndexMarker395"/>and will need to be truncated at a maximum length. A maximum token length of 128 has been chosen as the Gigaword dataset only contains a few sentences from the article. Note that 128 tokens are not the same as 128 words due to the subword tokenizer. Using a subword tokenizer minimizes the presence of unknown tokens during summary generation.</p>
    <p class="normal">Once the tokenizer is ready, both the article and summary texts need to be tokenized. Since the summary will be fed to the Decoder one token at a time, the provided summary text will be shifted right by adding a <code class="Code-In-Text--PACKT-">start</code> token, as shown previously. An <code class="Code-In-Text--PACKT-">end</code> token will be appended to the summary to let the Decoder learn how to signal the end of the summary's generation. The <code class="Code-In-Text--PACKT-">encode()</code> method in the file <code class="Code-In-Text--PACKT-">seq2seq.py</code> defines the vectorization step:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">encode</span><span class="hljs-functio">(</span><span class="hljs-params">article, summary, start=start, end=end, </span>
<span class="hljs-params">           tokenizer=tokenizer, art_max_len=</span><span class="hljs-number">128</span><span class="hljs-params">, </span>
<span class="hljs-params">           smry_max_len=</span><span class="hljs-number">50</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># vectorize article</span>
    tokens = tokenizer.encode(article.numpy())
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) &gt; art_max_len:
        tokens = tokens[:art_max_len]
    art_enc = sequence.pad_sequences([tokens], padding=<span class="hljs-string">'post'</span>,
                                 maxlen=art_max_len).squeeze()
    <span class="hljs-comment"># vectorize summary</span>
    tokens = [start] + tokenizer.encode(summary.numpy())
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) &gt; smry_max_len:
        tokens = tokens[:smry_max_len]
    <span class="hljs-keyword">else</span>:
        tokens = tokens + [end]
    
    smry_enc = sequence.pad_sequences([tokens], padding=<span class="hljs-string">'post'</span>,
                                 maxlen=smry_max_len).squeeze()
    <span class="hljs-keyword">return</span> art_enc, smry_enc
</code></pre>
    <p class="normal">Since this is <a id="_idIndexMarker396"/>a Python function working on the contents of the text of tensors, another function needs to be defined. This can be passed to the <a id="_idIndexMarker397"/>dataset to be applied to all the rows of the data. This function is also defined in the same file as the <code class="Code-In-Text--PACKT-">encode</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">tf_encode</span><span class="hljs-functio">(</span><span class="hljs-params">article, summary</span><span class="hljs-functio">):</span>
    art_enc, smry_enc = tf.py_function(encode, [article, summary],
                                     [tf.int64, tf.int64])
    art_enc.set_shape([<span class="hljs-literal">None</span>])
    smry_enc.set_shape([<span class="hljs-literal">None</span>])
    <span class="hljs-keyword">return</span> art_enc, smry_enc
</code></pre>
    <p class="normal">Going back to the main function in the <code class="Code-In-Text--PACKT-">s2s-training.py</code> file, the dataset can be vectorized with the help of the preceding functions like so:</p>
    <pre class="programlisting code"><code class="hljs-code">BUFFER_SIZE = <span class="hljs-number">1500000</span>  <span class="hljs-comment"># dataset is 3.8M samples, using less</span>
BATCH_SIZE = <span class="hljs-number">64</span>  <span class="hljs-comment"># try bigger batch for faster training</span>
train = ds_train.take(BUFFER_SIZE)  <span class="hljs-comment"># 1.5M samples</span>
print(<span class="hljs-string">"Dataset sample taken"</span>)
train_dataset = train.<span class="hljs-built_in">map</span>(s2s.tf_encode)
<span class="hljs-comment"># train_dataset = train_dataset.shuffle(BUFFER_SIZE) – optional </span>
train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=<span class="hljs-literal">True</span>)
print(<span class="hljs-string">"Dataset batching done"</span>)
</code></pre>
    <p class="normal">Note that shuffling the dataset is recommended. By shuffling the dataset, it is easier for the model to converge and not overfit to batches. However, this adds to the training time. This has <a id="_idIndexMarker398"/>been commented out here as this is an <a id="_idIndexMarker399"/>optional step. Shuffling records in batches while training models for production use cases is recommended. The last step in preparing the data is batching it, as shown in the last step here. Now, we are ready to build the model and train it.</p>
    <h1 id="_idParaDest-101" class="title">Seq2seq model with attention</h1>
    <p class="normal">The summarization model has an Encoder part with a bidirectional RNN and a unidirectional decoder part. There <a id="_idIndexMarker400"/>is an attention layer that helps the Decoder focus on specific<a id="_idIndexMarker401"/> parts of the input while generating an output token. The overall architecture is shown in the following diagram:</p>
    <figure class="mediaobject"><img src="image/B16252_06_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.1: Seq2seq and attention model</p>
    <p class="normal">These layers are detailed in the following subsections. All the code for these parts of the model are in the file <code class="Code-In-Text--PACKT-">seq2seq.py</code>. All the layers use common hyperparameters specified in the main function in the <code class="Code-In-Text--PACKT-">s2s-training.py</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code">embedding_dim = <span class="hljs-number">128</span>
units = <span class="hljs-number">256</span>  <span class="hljs-comment"># from pointer generator paper</span>
</code></pre>
    <p class="normal">The code and architecture for this section have been inspired by the paper titled <em class="italic">Get To The Point: Summarization with Pointer-Generator Networks</em> by Abigail See, Peter Liu, and Chris Manning, published in<a id="_idIndexMarker402"/> April 2017. The fundamental architecture is easy to follow and provides impressive performance for a model that can be trained on a desktop with a commodity GPU.</p>
    <h2 id="_idParaDest-102" class="title">Encoder model</h2>
    <p class="normal">The detailed architecture of the <a id="_idIndexMarker403"/>Encoder layer is shown in the following diagram. Tokenized and vectorized input is fed through an embedding layer. Embeddings for the tokens generated by the tokenizer are <a id="_idIndexMarker404"/>learned from scratch. It is possible to use a set of pre-trained embeddings like GloVe and use the corresponding tokenizer. While using a pre-trained set of embeddings can help with the accuracy of the model, a word-based vocabulary would have many unknown tokens, as we saw in the IMDb example and GloVe vectors earlier. The unknown tokens would impact the ability of the model to create summaries with words it hasn't seen before. If the summarization model is used on daily news, there can be several unknown words, like names of people, places, or new products:</p>
    <figure class="mediaobject"><img src="image/B16252_06_02.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.2: Encoder architecture</p>
    <p class="normal">The embedding layer has a dimension of 128, as configured in the hyperparameters. These hyperparameters have been chosen to resemble those in the paper. We then create an embedding singleton that can be used by both the Encoder and the Decoder. The code for the class is in the <code class="Code-In-Text--PACKT-">seq2seq.py</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Embedding</span><span class="hljs-class">(</span><span class="hljs-built_in">object</span><span class="hljs-class">):</span>
    embedding = <span class="hljs-literal">None</span>  <span class="hljs-comment"># singleton</span>
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">get_embedding</span><span class="hljs-functio">(</span><span class="hljs-params">self, vocab_size, embedding_dim</span><span class="hljs-functio">):</span>
        <span class="hljs-keyword">if</span> self.embedding <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            self.embedding = tf.keras.layers.Embedding(vocab_size,
                                                      embedding_dim,
                                                      mask_zero=<span class="hljs-literal">True</span>)
        <span class="hljs-keyword">return</span> self.embedding
</code></pre>
    <p class="normal">Input sequences will be padded to a fixed length of 128. Hence, a masking parameter is<a id="_idIndexMarker405"/> passed to the embedding layer so that the <a id="_idIndexMarker406"/>embedding layer ignores the mask tokens. Next, let's define an <code class="Code-In-Text--PACKT-">Encoder</code> class and instantiate the embedding layer in the constructor:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Encoder</span>
<span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Encoder</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, vocab_size, embedding_dim, enc_units, batch_size</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(Encoder, self).__init__()
        self.batch_size = batch_size
        self.enc_units = enc_units
        <span class="hljs-comment"># Shared embedding layer</span>
        self.embedding = Embedding.get_embedding(vocab_size, 
                                                 embedding_dim)
</code></pre>
    <p class="normal">The constructor <a id="_idIndexMarker407"/>takes a number of parameters:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Size of the vocabulary</strong>: In the present case, this is 32,899 tokens.</li>
      <li class="bullet"><strong class="keyword">Embedding dimensions</strong>: This is 128 dimensions. Feel free to experiment with a larger or smaller embedding dimension. Smaller dimensions would reduce the model's size and memory required for training the model.</li>
      <li class="bullet"><strong class="keyword">Encoder units</strong>: The number of forward and backward units in the bidirectional layer. 256 units will be used for a total of 512 units.</li>
      <li class="bullet"><strong class="keyword">Batch size</strong>: The size of the input batches. 64 records will be in one batch. A larger batch would make training go faster but would need more memory on the GPU. So, this number can be adjusted based on the capacity of the training hardware.</li>
    </ul>
    <p class="normal">The output of the embedding layer is fed to a bidirectional RNN layer. There are 256 GRU units in each direction. The bidirectional layer in Keras provides options on how to combine the output of the forward and backward layer. In this case, we concatenate the outputs of the <a id="_idIndexMarker408"/>forward and backward GRU cells. Hence, the output will be 512-dimensional. Furthermore, the hidden states are also needed for the <a id="_idIndexMarker409"/>attention mechanism to work, so a parameter is passed to retrieve the output states. The bidirectional GRU layer is configured like so:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.bigru = Bidirectional(GRU(self.enc_units,
                          return_sequences=<span class="hljs-literal">True</span>,
                          return_state=<span class="hljs-literal">True</span>,
                          recurrent_initializer=<span class="hljs-string">'glorot_uniform'</span>),
                          merge_mode=<span class="hljs-string">'concat'</span>
                        )
        self.relu = Dense(self.enc_units, activation=<span class="hljs-string">'relu'</span>)
</code></pre>
    <p class="normal">A dense layer with ReLU activation is also set up. The two layers return their hidden layers. However, the Decoder and attention layers require one vector of hidden states. We pass the hidden <a id="_idIndexMarker410"/>states through the dense layer and convert the dimensions from 512 into 256, which is expected by the Decoder and attention modules. This completes the constructor for the Encoder class. Given this is a custom model with specific ways to compute the model, a <code class="Code-In-Text--PACKT-">call()</code> method is defined that operates on a batch of inputs to produce the output and hidden states. This method takes in hidden states to seed the bidirectional layer:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, x, hidden</span><span class="hljs-functio">):</span>
        x = self.embedding(x)  <span class="hljs-comment"># We are using a mask</span>
        output, forward_state, backward_state = self.bigru(x, initial_state = hidden)
        <span class="hljs-comment"># now, concat the hidden states through the dense ReLU layer</span>
        hidden_states = tf.concat([forward_state, backward_state], 
                                  axis=<span class="hljs-number">1</span>)
        output_state = self.relu(hidden_states)
        
        <span class="hljs-keyword">return</span> output, output_state
</code></pre>
    <p class="normal">First, the input is passed through the embedding layer. The output is fed to the bidirectional layer, and the output and hidden states are retrieved. The two hidden states are concatenated and fed through the dense layer to create the output hidden state. Lastly, a utility method to return initial hidden states is defined:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">initialize_hidden_state</span><span class="hljs-functio">(</span><span class="hljs-params">self</span><span class="hljs-functio">):</span>
        <span class="hljs-keyword">return</span> [tf.zeros((self.batch_size, self.enc_units)) 
                 <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
</code></pre>
    <p class="normal">This completes the <a id="_idIndexMarker411"/>code for the Encoder. Before going into the Decoder, an attention layer needs to be defined, which will be used <a id="_idIndexMarker412"/>in the Decoder. Bahdanau's attention formulation will be used for this. Note that TensorFlow/Keras does not provide an attention layer out of the box. However, this simple attention layer code should be entirely reusable.</p>
    <h2 id="_idParaDest-103" class="title">Bahdanau attention layer</h2>
    <p class="normal">Bahdanau et al. published this form of <a id="_idIndexMarker413"/>global attention in 2015. It has been widely used in Transformer models, as we saw in the previous chapters. Now, we are going to implement an <a id="_idIndexMarker414"/>attention layer from scratch. This part of the code is inspired by the NMT tutorial published by the TensorFlow team.</p>
    <p class="normal">The core idea behind attention is to let the Decoder see all the inputs and focus on the most relevant inputs while predicting the output token. A global attention mechanism allows the Decoder to see all the inputs. This global version of the attention mechanism will be implemented. At an abstract level, the purpose of the attention mechanism maps a set of values to a given query. It does this by providing a relevance score of each of these values for a given query.</p>
    <p class="normal">In our case, the query is the Decoder's hidden state, and the values are the Encoder outputs. We are interested in figuring out which inputs can best help in generating the next token from the Decoder. The first step is computing a score using the Encoder output and the Decoder's previous hidden state. If this is the first step of decoding, then the hidden states from the Encoder are used to seed the Decoder. A corresponding weight matrix is multiplied by the Encoder's output and Decoder's hidden state. The output is passed through a <em class="italic">tanh</em> activation function and multiplied by another weight matrix to produce the final score. The following equation shows this formulation:</p>
    <figure class="mediaobject"><img src="image/B16252_06_001.png" alt="" style="max-height:20px;"/></figure>
    <p class="normal">Matrices <em class="italic">V</em>, <em class="italic">W</em><sub class="" style="font-style: italic;">1</sub>, and <em class="italic">W</em><sub class="" style="font-style: italic;">2</sub> are trainable. Then, to understand the alignment between the Decoder output and the Encoder outputs, a softmax is computed:</p>
    <figure class="mediaobject"><img src="image/B16252_06_002.png" alt="" style="max-height:20px;"/></figure>
    <p class="normal">The last step is to produce a context vector. The context vector is produced by multiplying the attention weights by the Encoder outputs:</p>
    <figure class="mediaobject"><img src="image/B16252_06_003.png" alt="" style="max-height:20px;"/></figure>
    <p class="normal">These are <a id="_idIndexMarker415"/>all the computations in the attention layer.</p>
    <p class="normal">The first <a id="_idIndexMarker416"/>step is setting up the constructor for the attention class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">BahdanauAttention</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, units</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">call()</code> method of the <code class="Code-In-Text--PACKT-">BahdanauAttention</code> class implements the equations shown previously with some additional code to manage the tensor shapes. This is shown here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, decoder_hidden, enc_output</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># decoder hidden state shape == (64, 256) </span>
<span class="hljs-comment">    # [batch size, decoder units]</span>
    <span class="hljs-comment"># encoder output shape == (64, 128, 256) </span>
    <span class="hljs-comment"># which is [batch size, max sequence length, encoder units]</span>
    query = decoder_hidden <span class="hljs-comment"># to map our code to generic </span>
<span class="hljs-comment">    # form of attention</span>
    values = enc_output
    
    <span class="hljs-comment"># query_with_time_axis shape == (batch_size, 1, hidden size)</span>
    <span class="hljs-comment"># we are doing this to broadcast addition along the time axis</span>
    query_with_time_axis = tf.expand_dims(query, <span class="hljs-number">1</span>)
    <span class="hljs-comment"># score shape == (batch_size, max_length, 1)</span>
    score = self.V(tf.nn.tanh(
        self.W1(query_with_time_axis) + self.W2(values)))
    <span class="hljs-comment"># attention_weights shape == (batch_size, max_length, 1)</span>
    attention_weights = tf.nn.softmax(score, axis=<span class="hljs-number">1</span>)
    <span class="hljs-comment"># context_vector shape after sum == (batch_size, hidden_size)</span>
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> context_vector, attention_weights
</code></pre>
    <p class="normal">The only <a id="_idIndexMarker417"/>thing we have left to do <a id="_idIndexMarker418"/>is implement the Decoder model.</p>
    <h2 id="_idParaDest-104" class="title">Decoder model</h2>
    <p class="normal">The detailed <a id="_idIndexMarker419"/>Decoder model is<a id="_idIndexMarker420"/> shown in the following diagram:</p>
    <figure class="mediaobject"><img src="image/B16252_06_03.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.3: Detailed decoder architecture</p>
    <p class="normal">Hidden states from the Encoder are<a id="_idIndexMarker421"/> used to initialize the hidden states of the Decoder. The start token initiates the summaries being generated. The hidden states of the Decoder, along with the Encoder output, are used to compute the attention weights and the context vector. The context vector, along with the embeddings of the output token, are concatenated <a id="_idIndexMarker422"/>and passed through the unidirectional GRU cell. The output of the GRU cell is passed through a dense layer, with a softmax activation function to get the output token. This process is repeated token by token.</p>
    <p class="normal">Note that the Decoder functions differently during training and inference. During training, the output token from the Decoder is used to calculate the loss but is not fed back into the Decoder to produce the next token. Instead, the next token from the ground truth is fed <a id="_idIndexMarker423"/>into the Decoder at each time step. This process is called <strong class="keyword">teacher forcing</strong>. The output tokens generated by the Decoder are only fed back in during inference when summaries are being generated.</p>
    <p class="normal">A <code class="Code-In-Text--PACKT-">Decoder</code> class is defined in the <code class="Code-In-Text--PACKT-">seq2seq.py</code> file. The constructor for this class sets up the dimensions and the various layers:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Decoder</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">__init__</span><span class="hljs-functio">(</span><span class="hljs-params">self, vocab_size, embedding_dim, dec_units, batch_sz</span><span class="hljs-functio">):</span>
        <span class="hljs-built_in">super</span>(Decoder, self).__init__()
        self.batch_sz = batch_sz
        self.dec_units = dec_units
        <span class="hljs-comment"># Unique embedding layer</span>
        self.embedding = tf.keras.layers.Embedding(vocab_size, 
                                                   embedding_dim,
                                                   mask_zero=<span class="hljs-literal">True</span>)
        <span class="hljs-comment"># Shared embedding layer</span>
        <span class="hljs-comment"># self.embedding = Embedding.get_embedding(vocab_size, </span>
<span class="hljs-comment">        # embedding_dim)</span>
        self.gru = tf.keras.layers.GRU(self.dec_units,
                                       return_sequences=<span class="hljs-literal">True</span>,
                                       return_state=<span class="hljs-literal">True</span>,
                                       recurrent_initializer=\
                                       <span class="hljs-string">'glorot_uniform'</span>)
        self.fc1 = tf.keras.layers.Dense(vocab_size, 
                               activation=<span class="hljs-string">'softmax'</span>)
        <span class="hljs-comment"># used for attention</span>
        self.attention = BahdanauAttention(self.dec_units)
</code></pre>
    <p class="normal">The embedding layer in the Decoder is not shared with the Encoder. This is a design choice. It is common in summarization to use a shared embedding layer. The structure of the articles and their summaries is slightly different in the Gigaword dataset as news headlines are not <a id="_idIndexMarker424"/>proper sentences but fragments of sentences. During training, using different embedding layers gave better results than shared embeddings. It is possible that, on the CNN/DailyMail dataset, shared embeddings <a id="_idIndexMarker425"/>give better results than on the Gigaword dataset. In the case of machine translation, the Encoder and Decoder are seeing different languages, so having separate embedding layers is a best practice. You are encouraged to try out both versions on different datasets and build your own intuition. The preceding commented code makes it easy to switch back and forth between shared and separate embeddings between the Encoder and Decoder.</p>
    <p class="normal">The next part of the Decoder is the computation that calculates the output:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">call</span><span class="hljs-functio">(</span><span class="hljs-params">self, x, hidden, enc_output</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># enc_output shape == (batch_size, max_length, hidden_size)</span>
    context_vector, attention_weights = self.attention(hidden,
                                                       enc_output)
    <span class="hljs-comment"># x shape after passing through embedding</span>
    <span class="hljs-comment"># == (batch_size, 1, embedding_dim)</span>
    x = self.embedding(x)
    x = tf.concat([tf.expand_dims(context_vector, <span class="hljs-number">1</span>), x], axis=<span class="hljs-number">-1</span>)
    <span class="hljs-comment"># passing the concatenated vector to the GRU</span>
    output, state = self.gru(x)
    output = tf.reshape(output, (<span class="hljs-number">-1</span>, output.shape[<span class="hljs-number">2</span>]))
    
    x = self.fc1(output)
    
    <span class="hljs-keyword">return</span> x, state, attention_weights
</code></pre>
    <p class="normal">The computation <a id="_idIndexMarker426"/>is fairly <a id="_idIndexMarker427"/>straightforward. The model looks like this:</p>
    <pre class="programlisting con"><code class="hljs-con">Model: "encoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding (Embedding)        multiple                  4211072
_________________________________________________________________
bidirectional (Bidirectional multiple                  592896
_________________________________________________________________
dense (Dense)                multiple                  131328
=================================================================
Total params: 4,935,296
Trainable params: 4,935,296
Non-trainable params: 0
_________________________________________________________________
Model: "decoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding_1 (Embedding)      multiple                  4211072
_________________________________________________________________
gru_1 (GRU)                  multiple                  689664
_________________________________________________________________
dense_1 (Dense)              multiple                  8455043
_________________________________________________________________
bahdanau_attention (Bahdanau multiple                  197377
=================================================================
Total params: 13,553,156
Trainable params: 13,553,156
Non-trainable params: 0
</code></pre>
    <p class="normal">The Encoder <a id="_idIndexMarker428"/>model contains 4.9M parameters, while the Decoder model contains 13.5M parameters <a id="_idIndexMarker429"/>for a total of 18.4M parameters. Now, we are ready to train the model.</p>
    <h1 id="_idParaDest-105" class="title">Training the model</h1>
    <p class="normal">There are a number of steps to be performed in training that require a custom training loop. First, let's define <a id="_idIndexMarker430"/>a method that executes one step of the <a id="_idIndexMarker431"/>training loop. This method is defined in the <code class="Code-In-Text--PACKT-">s2s-training.py</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@tf.function</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">train_step</span><span class="hljs-functio">(</span><span class="hljs-params">inp, targ, enc_hidden, max_gradient_norm=</span><span class="hljs-number">5</span><span class="hljs-functio">):</span>
    loss = <span class="hljs-number">0</span>
    
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        <span class="hljs-comment"># print("inside gradient tape")</span>
        enc_output, enc_hidden = encoder(inp, enc_hidden)
               
        dec_hidden = enc_hidden
        dec_input = tf.expand_dims([start] * BATCH_SIZE, <span class="hljs-number">1</span>)
        
        <span class="hljs-comment"># Teacher forcing - feeding the target as the next input</span>
        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, targ.shape[<span class="hljs-number">1</span>]):
            <span class="hljs-comment"># passing enc_output to the decoder</span>
            predictions, dec_hidden, _ = decoder(dec_input,   
                                           dec_hidden, enc_output)
            
            loss += s2s.loss_function(targ[:, t], predictions)
            <span class="hljs-comment"># using teacher forcing</span>
            dec_input = tf.expand_dims(targ[:, t], <span class="hljs-number">1</span>)
            
    batch_loss = (loss / <span class="hljs-built_in">int</span>(targ.shape[<span class="hljs-number">1</span>]))
    
    variables = encoder.trainable_variables + \
decoder.trainable_variables
    gradients = tape.gradient(loss, variables)
    <span class="hljs-comment"># Gradient clipping</span>
    clipped_gradients, _ = tf.clip_by_global_norm(
                                    gradients, max_gradient_norm)
    optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(clipped_gradients, variables))
    <span class="hljs-keyword">return</span> batch_loss
</code></pre>
    <p class="normal">This is a custom training loop that uses <code class="Code-In-Text--PACKT-">GradientTape</code>, which tracks the different variables of the <a id="_idIndexMarker432"/>model and calculates the gradients. The preceding <a id="_idIndexMarker433"/>function runs once for each batch of inputs. Inputs are passed through the Encoder to get the final encoding and the last hidden state. The Decoder is initialized with the last Encoder hidden state, and summaries are generated one token at a time. However, the generated token is not fed back into the Decoder. Instead, the actual token is fed back. This method is known as <strong class="keyword">Teacher Forcing</strong>. A custom loss function is defined in the <code class="Code-In-Text--PACKT-">seq2seq.py</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code">loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
                    from_logits=<span class="hljs-literal">False</span>, reduction=<span class="hljs-string">'none'</span>)
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">loss_function</span><span class="hljs-functio">(</span><span class="hljs-params">real, pred</span><span class="hljs-functio">):</span>
    mask = tf.math.logical_not(tf.math.equal(real, <span class="hljs-number">0</span>))
    loss_ = loss_object(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    <span class="hljs-keyword">return</span> tf.reduce_mean(loss_)
</code></pre>
    <p class="normal">The key to the loss function is to use a mask to handle summaries of varying lengths. The last part of the model is using an optimizer. The Adam optimizer is being used here, with a learning rate schedule that reduces the learning rate over epochs of training. The concept of learning rate annealing was covered in previous chapters. The code for the optimizer is inside the main function in the <code class="Code-In-Text--PACKT-">s2s-training.py</code> file:</p>
    <pre class="programlisting code"><code class="hljs-code">steps_per_epoch = BUFFER_SIZE // BATCH_SIZE
embedding_dim = <span class="hljs-number">128</span>
units = <span class="hljs-number">256</span>  <span class="hljs-comment"># from pointer generator paper</span>
EPOCHS = <span class="hljs-number">16</span>
 
encoder = s2s.Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)
decoder = s2s.Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)
<span class="hljs-comment"># Learning rate scheduler</span>
lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
                   <span class="hljs-number">0.001</span>,
                   decay_steps=steps_per_epoch*(EPOCHS/<span class="hljs-number">2</span>),
                   decay_rate=<span class="hljs-number">2</span>,
                   staircase=<span class="hljs-literal">False</span>)
optimizer = tf.keras.optimizers.Adam(lr_schedule)
</code></pre>
    <p class="normal">Since the model is going to be trained for a long time, it is important to set up checkpoints that <a id="_idIndexMarker434"/>can be used to restart training in case issues occur. Checkpoints also provide us with an opportunity to adjust some of the training parameters <a id="_idIndexMarker435"/>across runs. The next part of the main function sets up the checkpointing system. We looked at checkpoints in the previous chapter. We will extend what we've learned and set up an optional command-line argument that specifies if training needs to be restarted from a specific checkpoint:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> args.checkpoint <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
    dt = datetime.datetime.today().strftime(<span class="hljs-string">"%Y-%b-%d-%H-%M-%S"</span>)
    checkpoint_dir = <span class="hljs-string">'./training_checkpoints-'</span> + dt
<span class="hljs-keyword">else</span>:
    checkpoint_dir = args.checkpoint
checkpoint_prefix = os.path.join(checkpoint_dir, <span class="hljs-string">"ckpt"</span>)
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)
<span class="hljs-keyword">if</span> args.checkpoint <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
    <span class="hljs-comment"># restore last model</span>
    print(<span class="hljs-string">"Checkpoint being restored: "</span>,
 tf.train.latest_checkpoint(checkpoint_dir))
    chkpt_status = checkpoint.restore(
tf.train.latest_checkpoint(checkpoint_dir))
    <span class="hljs-comment"># to check loading worked</span>
 chkpt_status.assert_existing_objects_matched()  
<span class="hljs-keyword">else</span>:
    print(<span class="hljs-string">"Starting new training run from scratch"</span>)
print(<span class="hljs-string">"New checkpoints will be stored in: "</span>, checkpoint_dir)
</code></pre>
    <p class="normal">If training needs to be restarted from a checkpoint, then a command-line argument in the form <code class="Code-In-Text--PACKT-">–-checkpoint &lt;dir&gt;</code> can be specified while invoking the training script. If no argument is supplied, then a new checkpoint directory will be created. Training with 1.5M records takes <a id="_idIndexMarker436"/>over 3 hours. Running 10 iterations will <a id="_idIndexMarker437"/>take over a day and a half. The Pointer-Generator model we referenced earlier in this chapter was trained for 33 epochs, which took over 4 days of training. However, it is possible to see some results after 4 epochs of training.</p>
    <p class="normal">Now, the last part of the main function is to start the training process:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">"Starting Training. Total number of steps / epoch: "</span>, steps_per_epoch)
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):
        start_tm = time.time()
        enc_hidden = encoder.initialize_hidden_state()
        total_loss = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> (batch, (art, smry)) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataset.take(steps_per_epoch)):
            batch_loss = train_step(art, smry, enc_hidden)
            total_loss += batch_loss
            <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
                ts = datetime.datetime.now().\
strftime(<span class="hljs-string">"%d-%b-%Y (%H:%M:%S)"</span>)
                print(<span class="hljs-string">'[{}] Epoch {} Batch {} Loss {:.6f}'</span>.\
<span class="hljs-built_in">                        format</span>(ts,epoch + <span class="hljs-number">1</span>, batch,
<span class="hljs-built_in">                       </span> batch_loss.numpy())) <span class="hljs-comment"># end print</span>
        <span class="hljs-comment"># saving (checkpoint) the model every 2 epochs</span>
        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:
            checkpoint.save(file_prefix = checkpoint_prefix)
        print(<span class="hljs-string">'Epoch {} Loss {:.6f}'</span>.\
                <span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>, total_loss / steps_per_epoch))
        
        print(<span class="hljs-string">'Time taken for 1 epoch {} sec\n'</span>.\
                  <span class="hljs-built_in">format</span>(time.time() - start_tm))
</code></pre>
    <p class="normal">The training loop prints the loss every 100 batches and saves a checkpoint every second epoch. Feel free <a id="_idIndexMarker438"/>to adjust these settings as needed. The following <a id="_idIndexMarker439"/>command can be used to start training:</p>
    <pre class="programlisting con"><code class="hljs-con">$ python s2s-training.py
</code></pre>
    <p class="normal">The output of this script should be something similar to:</p>
    <pre class="programlisting con"><code class="hljs-con">Loading the dataset
Tokenizer ready. Total vocabulary size:  32897
Coronavirus spread surprised everyone  =&gt;  [16166, 2342, 1980, 7546, 21092]
16166 ----&gt; corona
2342 ----&gt; virus
1980 ----&gt; spread
7546 ----&gt; surprised
21092 ----&gt; everyone
Dataset sample taken
Dataset batching done
Starting new training run from scratch
New checkpoints will be stored in:  ./training_checkpoints-2021-Jan-04-04-33-42
Starting Training. Total number of steps / epoch:  31
[04-Jan-2021 (04:34:45)] Epoch 1 Batch 0 Loss 2.063991
<span class="hljs-co -meta">...</span>
Epoch 1 Loss 1.921176
Time taken for 1 epoch 83.241370677948 sec
[04-Jan-2021 (04:35:06)] Epoch 2 Batch 0 Loss 1.487815
Epoch 2 Loss 1.496654
Time taken for 1 epoch 21.058568954467773 sec
</code></pre>
    <p class="normal">This sample run used only 2,000 samples since we edited this line:</p>
    <pre class="programlisting code"><code class="hljs-code">BUFFER_SIZE = <span class="hljs-number">2000</span>  <span class="hljs-comment"># 3500000 takes 7hr/epoch</span>
</code></pre>
    <p class="normal">If training is being restarted from a checkpoint, then the command line will be:</p>
    <pre class="programlisting con"><code class="hljs-con">$ python s2s-trainingo.py --checkpoint training_checkpoints-2021-Jan-04-04-33-42
</code></pre>
    <p class="normal">With this comment, the model is hydrated from the checkpoint directory we used in the training step. Training continues from that point. Once the model has finished training, we are ready to <a id="_idIndexMarker440"/>generate the summaries. Note that the model we'll <a id="_idIndexMarker441"/>be using in the next section was trained for 8 epochs with 1.5M records. Using all 3.8M records and training for more epochs would give better results.</p>
    <h1 id="_idParaDest-106" class="title">Generating summaries</h1>
    <p class="normal">The critical thing to note while generating summaries is that a new inference loop will need to be built. Recall that <em class="italic">teacher forcing</em> was used during training, and the output of the Decoder was not <a id="_idIndexMarker442"/>used in predicting the next token. While generating summaries, we would like to use the generated tokens in predicting the next token. Since we would like to play with various input texts and generate summaries, we will use the code in the <code class="Code-In-Text--PACKT-">generating-summaries.ipynb</code> IPython notebook. After importing and setting everything up, the tokenizer needs to be instantiated. The <em class="italic">Setup Tokenization</em> section of the notebook loads the tokenizers and sets up the vocabulary by adding start and end token IDs. Similar to when we loaded the data, the data encoding method is set up to encode the input articles.</p>
    <p class="normal">Now, we must hydrate the model from the saved checkpoint. All of the model objects are created first:</p>
    <pre class="programlisting code"><code class="hljs-code">BATCH_SIZE = <span class="hljs-number">1</span>  <span class="hljs-comment"># for inference</span>
embedding_dim = <span class="hljs-number">128</span>
units = <span class="hljs-number">256</span>  <span class="hljs-comment"># from pointer generator paper</span>
vocab_size = end + <span class="hljs-number">2</span>
<span class="hljs-comment"># Create encoder and decoder objects</span>
encoder = s2s.Encoder(vocab_size, embedding_dim, units, 
                        BATCH_SIZE)
decoder = s2s.Decoder(vocab_size, embedding_dim, units, 
                        BATCH_SIZE)
optimizer = tf.keras.optimizers.Adam()
</code></pre>
    <p class="normal">Next, a checkpoint <a id="_idIndexMarker443"/>with the appropriate checkpoint directory is defined:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Hydrate the model from saved checkpoint</span>
checkpoint_dir = <span class="hljs-string">'training_checkpoints-2021-Jan-25-09-26-31'</span>
checkpoint_prefix = os.path.join(checkpoint_dir, <span class="hljs-string">"ckpt"</span>)
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)
</code></pre>
    <p class="normal">Then, the last checkpoint is checked:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># The last training checkpoint</span>
tf.train.latest_checkpoint(checkpoint_dir)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'training_checkpoints-2021-Jan-25-09-26-31/ckpt-11'
</code></pre>
    <p class="normal">Since checkpoints are stored after every alternate epoch, this checkpoint corresponds to 8 epochs of training. Checkpoints can be loaded and tested with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">chkpt_status = checkpoint.restore(
                        tf.train.latest_checkpoint(checkpoint_dir))
chkpt_status.assert_existing_objects_matched()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">&lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f603ae03c90&gt;
</code></pre>
    <p class="normal">That's it! The model is now ready for inference.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-"><strong class="scree Text">Checkpoints and variable names</strong></p>
      <p class="Tip--PACKT-">It is possible that the second command may give an error if it cannot match the names of the variables in the checkpoint with the names in the model. This can happen as we did not explicitly name the layers when they were instantiated in the model. TensorFlow will provide a dynamically generated name for the layer when the model is instantiated:</p>
      <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> decoder.layers:
    print(layer.name)
</code></pre>
      <pre class="programlisting con"><code class="hljs-con">embedding_1
gru_1
fc1
</code></pre>
      <p class="Tip--PACKT-">Variable names in the checkpoint can be inspected with:</p>
      <pre class="programlisting code"><code class="hljs-code">tf.train.list_variables(
       tf.train.latest_checkpoint(<span class="hljs-string">'./&lt;chkpt_dir&gt;/'</span>)
)
</code></pre>
      <p class="Tip--PACKT-">If the model is instantiated again, these names may change, and restore from checkpoint may fail. There are two solutions to prevent this. A quick fix is to restart the notebook kernel. A better fix is to edit the code and add names to each layer in the Encoder and Decoder constructors before training. This ensures that checkpoints will always find the variables. An example of this approach is shown for the <code class="Code-In-Text--PACKT-">fc1</code> layer in the Decoder:</p>
      <pre class="programlisting code"><code class="hljs-code">self.fc1 = tf.keras.layers.Dense(
                vocab_size, activation=<span class="hljs-string">'softmax'</span>, 
                name=<span class="hljs-string">'fc1'</span>)
</code></pre>
    </div>
    <p class="normal">Inference can be done via the greedy search or beam search algorithms. Both of these methods <a id="_idIndexMarker444"/>will be demonstrated here. Before going into the code for generating summaries, a convenience method for plotting attention weights will be defined. This helps in providing some intuition on what inputs contributed to a given token being generated in the summary:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># function for plotting the attention weights</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">plot_attention</span><span class="hljs-functio">(</span><span class="hljs-params">attention, article, summary</span><span class="hljs-functio">):</span>
    fig = plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>))
    ax = fig.add_subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
    <span class="hljs-comment"># https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html </span>
    <span class="hljs-comment"># for scales</span>
    ax.matshow(attention, cmap=<span class="hljs-string">'cividis'</span>)
    fontdict = {<span class="hljs-string">'fontsize'</span>: <span class="hljs-number">14</span>}
    ax.set_xticklabels([<span class="hljs-string">''</span>] + article, fontdict=fontdict, rotation=<span class="hljs-number">90</span>)
    ax.set_yticklabels([<span class="hljs-string">''</span>] + summary, fontdict=fontdict)
    ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="hljs-number">1</span>))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="hljs-number">1</span>))
    plt.show()
</code></pre>
    <p class="normal">A plot is configured with the input sequence as the columns and the output summary tokens as the rows. Feel free to play with different color scales to get a better idea of the strength of <a id="_idIndexMarker445"/>the association between the tokens.</p>
    <p class="normal">We have covered much ground and possibly trained a network for hours. It is time to see the fruits of our labor!</p>
    <h2 id="_idParaDest-107" class="title">Greedy search</h2>
    <p class="normal">Greedy search uses the highest probability token at each time step to construct the sequence. The predicted <a id="_idIndexMarker446"/>token is fed back into the model to generate the next token. This is the same model that was used in the previous chapter while generating characters in the char-RNN model:</p>
    <pre class="programlisting code"><code class="hljs-code">art_max_len = <span class="hljs-number">128</span>
smry_max_len = <span class="hljs-number">50</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">greedy_search</span><span class="hljs-functio">(</span><span class="hljs-params">article</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># To store attention plots of the output</span>
    attention_plot = np.zeros((smry_max_len, art_max_len))
    tokens = tokenizer.encode(article) 
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) &gt; art_max_len:
        tokens = tokens[:art_max_len]
    inputs = sequence.pad_sequences([tokens], padding=<span class="hljs-string">'post'</span>,
                                 maxlen=art_max_len).squeeze()
    inputs = tf.expand_dims(tf.convert_to_tensor(inputs), <span class="hljs-number">0</span>)
    
    <span class="hljs-comment"># output summary tokens will be stored in this</span>
    summary = <span class="hljs-string">"</span>
    hidden = [tf.zeros((<span class="hljs-number">1</span>, units)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)] <span class="hljs-comment">#BiRNN</span>
    enc_out, enc_hidden = encoder(inputs, hidden)
    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([start], <span class="hljs-number">0</span>)
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(smry_max_len):
        predictions, dec_hidden, attention_weights = \
decoder(dec_input, dec_hidden, enc_out)
        predicted_id = tf.argmax(predictions[<span class="hljs-number">0</span>]).numpy()
        <span class="hljs-keyword">if</span> predicted_id == end:
            <span class="hljs-keyword">return</span> summary, article, attention_plot
        <span class="hljs-comment"># storing the attention weights to plot later on</span>
        attention_weights = tf.reshape(attention_weights, (<span class="hljs-number">-1</span>, ))
        attention_plot[t] = attention_weights.numpy()
        
        summary += tokenizer.decode([predicted_id])
        <span class="hljs-comment"># the predicted ID is fed back into the model</span>
        dec_input = tf.expand_dims([predicted_id], <span class="hljs-number">0</span>)
    <span class="hljs-keyword">return</span> summary, article, attention_plot
</code></pre>
    <p class="normal">The first part of the code encodes the inputs the same way they were encoded during training. These inputs are passed through the Encoder to the final encoder output and the last hidden state. The Decoder's initial hidden state is set to the last hidden state of the Encoder. Now, the process of generating the output tokens begins. First, the inputs <a id="_idIndexMarker447"/>are fed to the Decoder, which generates a prediction, the hidden state, and the attention weights. Attention weights are added to a running list of attention weights per time step. This generation continues until whichever comes earlier; producing an end-of-sequence token, or producing 50 tokens. The resulting summary and attention plot are returned. A summarization method is defined, which calls this greedy search algorithm, plots the attention weights, and converts the generated tokens into proper words:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Summarize</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">summarize</span><span class="hljs-functio">(</span><span class="hljs-params">article, algo=</span><span class="hljs-string">'greedy'</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> algo == <span class="hljs-string">'greedy'</span>:
        summary, article, attention_plot = greedy_search(article)
    <span class="hljs-keyword">else</span>:
        print(<span class="hljs-string">"Algorithm {} not implemented"</span>.<span class="hljs-built_in">format</span>(algo))
        <span class="hljs-keyword">return</span>
    
    print(<span class="hljs-string">'Input: %s'</span> % (article))
    print(<span class="hljs-string">'** Predicted Summary: {}'</span>.<span class="hljs-built_in">format</span>(summary))
    attention_plot = \
attention_plot[:<span class="hljs-built_in">len</span>(summary.split(<span class="hljs-string">' '</span>)), :<span class="hljs-built_in">len</span>(article.split(<span class="hljs-string">' '</span>))]
    plot_attention(attention_plot, article.split(<span class="hljs-string">' '</span>), 
                     summary.split(<span class="hljs-string">' '</span>))
</code></pre>
    <p class="normal">The preceding method has a spot where we can plug in beam search later. Let's test the<a id="_idIndexMarker448"/> model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Test Summarization</span>
txt = <span class="hljs-string">"president georgi parvanov summoned france 's ambassador on wednesday in a show of displeasure over comments from french president jacques chirac chiding east european nations for their support of washington on the issue of iraq ."</span>
summarize(txt.lower())
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Input: president georgi parvanov summoned france's ambassador on wednesday in a show of displeasure over comments from french president jacques chirac chiding east european nations for their support of washington on the issue of iraq .
** Predicted Summary: <span class="code-highlight"><strong class="hljs-con-slc">bulgarian president summons french ambassador over remarks on iraq</strong></span>
</code></pre>
    <figure class="mediaobject"><img src="image/B16252_06_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.4: Attention plot for an example summary</p>
    <p class="normal">Let's take a look at the generated summary:</p>
    <p class="normal"><em class="italic">bulgarian president summons french ambassador over remarks on iraq</em></p>
    <p class="normal">It is a pretty good summary! The most surprising part is that the model was able to identify the Bulgarian president, even though Bulgaria is not mentioned anywhere in the source text. It contains other words not found in the original text. These are highlighted in the preceding output. The model was able to change the tense of the word <em class="italic">summoned</em> to <em class="italic">summons</em>. The word <em class="italic">remarks</em> never appears in the source text. The model was able to infer this from a number of input tokens. The notebook contains many examples, both good and bad, of summaries generated by the model.</p>
    <p class="normal">Here is an example of a piece of challenging text for the model:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Input</strong>: <em class="italic">charles kennedy , leader of britain's third-ranked liberal democrats , announced saturday he was quitting with immediate effect and would not stand in a new leadership election . us president george w. bush on saturday called for extending tax cuts adopted in his first term , which he said had bolstered economic growth.</em></li>
      <li class="bullet"><strong class="keyword">Predicted summary</strong>: <em class="italic">kennedy quits to be a step toward new term</em></li>
    </ul>
    <p class="normal">In this article, there are two seemingly unrelated sentences. The model is trying to make sense of them but messes it up. There are other examples where the model doesn't do so well:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Input</strong>: <em class="italic">jc penney will close another ## stores for good . the department store chain , which filed for bankruptcy last month , is inching toward its target of closing ## stores.</em></li>
      <li class="bullet"><strong class="keyword">Predicted summary</strong>: <em class="italic">jc penney to close another ## stores for #nd stores</em></li>
    </ul>
    <p class="normal">In this example, the model repeats itself, attending to the same positions. In fact, this is a common problem with summarization models. One solution to prevent repetition is to add coverage loss. Coverage loss keeps a running total of the attention weights across time steps and feeds it back to the attention mechanism, as a way to clue it in to previously attended positions. Furthermore, coverage loss terms are added to the overall loss equation to penalize repetition. Training the model for much longer would also help<a id="_idIndexMarker449"/> in this particular case. Note that Transformer-based models suffer a little less from repetition.</p>
    <p class="normal">The second example is the model inventing something:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Input</strong>: <em class="italic">the german engineering giant siemens is working on a revamped version of its defective tram car , of which the ### units sold so far worldwide are being recalled owing to a technical fault , a company spokeswoman said on tuesday.</em></li>
      <li class="bullet"><strong class="keyword">Predicted summary</strong>: <em class="italic">siemens to launch reb-made cars</em></li>
    </ul>
    <p class="normal">The model invents <em class="italic">reb-made</em>, which is incorrect:</p>
    <figure class="mediaobject"><img src="image/B16252_06_05.jpg" alt=""/></figure>
    <p class="packt_figref">Figure 6.5: Model invents the word "reb-made"</p>
    <p class="normal">Looking at the preceding attention plot, the new word is being generated by attending to <em class="italic">revamped</em>, <em class="italic">version</em>, <em class="italic">defective</em>, and <em class="italic">tram</em>. This made-up word garbles the summary generated.</p>
    <p class="normal">As noted earlier, using beam <a id="_idIndexMarker450"/>search can help in further improving the accuracy of the translations. We will try some of these challenging examples after implementing the beam search algorithm.</p>
    <h2 id="_idParaDest-108" class="title">Beam search</h2>
    <p class="normal">Beam search uses multiple paths or beams to generate tokens and tries to minimize the<a id="_idIndexMarker451"/> overall conditional probability. At each time step, all the options are evaluated, and the cumulative conditional probabilities are evaluated over all the time steps so far. Only the top <em class="italic">k</em> beams, where <em class="italic">k</em> is the beam width, are kept; the rest are pruned for the next time step. Greedy search is a special case of beam search with a beam width of 1. In fact, this property serves as a test case for the beam search algorithm. The code for this section can be found in the <em class="italic">Beam Search</em> section of the IPython notebook.</p>
    <p class="normal">A new method called <code class="Code-In-Text--PACKT-">beam_search()</code> is defined. The first part of this method is similar to greedy search, where inputs are tokenized and passed through the Encoder. The main difference between this algorithm and the greedy search algorithm is the core loop, which processes one token at a time. In beam search, a token needs to be generated for every beam. This makes beam search slower than greedy search, and running time increases in proportion to beam width. At each time step, for each of the <em class="italic">k</em> beams, the top <em class="italic">k</em> tokens are generated, sorted, and pruned back to <em class="italic">k</em> items. This step is performed until each beam generates an end of sequence token or has generated the maximum number of tokens. If there are <em class="italic">m</em> tokens to be generated, then beam search would require <em class="italic">k * m</em> runs of the Decoder to generate the output sequence. The main loop is shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># initial beam with (tokens, last hidden state, attn, score)</span>
start_pt = [([start], dec_hidden, attention_plot, <span class="hljs-number">0.0</span>)]  <span class="hljs-comment"># initial beam </span>
<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(smry_max_len):
    options = <span class="hljs-built_in">list</span>() <span class="hljs-comment"># empty list to store candidates</span>
    <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> start_pt:
        <span class="hljs-comment"># handle beams emitting end signal</span>
        allend = <span class="hljs-literal">True</span>
        dec_input = row[<span class="hljs-number">0</span>][<span class="hljs-number">-1</span>]
        <span class="hljs-keyword">if</span> dec_input != end_tk:
              <span class="hljs-comment"># last token</span>
            dec_input = tf.expand_dims([dec_input], <span class="hljs-number">0</span>)  
            dec_hidden = row[<span class="hljs-number">1</span>]  <span class="hljs-comment"># second item is hidden states</span>
            attn_plt = np.zeros((smry_max_len, art_max_len)) +\
                       row[<span class="hljs-number">2</span>] <span class="hljs-comment"># new attn vector</span>
            
            predictions, dec_hidden, attention_weights = \
decoder(dec_input, dec_hidden, enc_out)
            <span class="hljs-comment"># storing the attention weights to plot later on</span>
            attention_weights = tf.reshape(attention_weights, (<span class="hljs-number">-1</span>, ))
            attn_plt[t] = attention_weights.numpy() 
            
            <span class="hljs-comment"># take top-K in this beam</span>
            values, indices = tf.math.top_k(predictions[<span class="hljs-number">0</span>],
                                               k=beam_width)
            <span class="hljs-keyword">for</span> tokid, scre <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(indices, values):
                score = row[<span class="hljs-number">3</span>] - np.log(scre)
                options.append((row[<span class="hljs-number">0</span>]+[tokid], dec_hidden, 
                                   attn_plt, score))
            allend=<span class="hljs-literal">False</span>
        <span class="hljs-keyword">else</span>:
            options.append(row)  <span class="hljs-comment"># add ended beams back in</span>
    
    <span class="hljs-keyword">if</span> allend:
        <span class="hljs-keyword">break</span> <span class="hljs-comment"># end for loop as all sequences have ended</span>
    start_pt = <span class="hljs-built_in">sorted</span>(options, key=<span class="hljs-keyword">lambda</span> tup:tup[<span class="hljs-number">3</span>])[:beam_width]
</code></pre>
    <p class="normal">At the start, there is only one beam within the start token. A list to keep track of the beams generated is then defined. The list of tuples stores the attention plots, tokens, last hidden state, and the overall cost of the beam. Conditional probability requires a product of all probabilities. Given that all probabilities are numbers between 0 and 1, the conditional probability could become very small. Instead, logs of the probabilities <a id="_idIndexMarker452"/>are added together, as shown in the preceding highlighted code. The best beams minimize this score. Finally, a small section is inserted that prints all the top beams with their scores once the function completes its execution. This part is optional and can be removed:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> verbose:  <span class="hljs-comment"># to control output</span>
        <span class="hljs-comment"># print all the final summaries</span>
        <span class="hljs-keyword">for</span> idx, row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(start_pt):
            tokens = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> row[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> x &lt; end_tk]
            print(<span class="hljs-string">"Summary {} with {:5f}: {}"</span>.<span class="hljs-built_in">format</span>(idx, row[<span class="hljs-number">3</span>], 
                                        tokenizer.decode(tokens)))
</code></pre>
    <p class="normal">At the end, the function returns the best beam:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-comment"># return final sequence</span>
    summary = tokenizer.decode([x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> start_pt[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> x &lt; end_tk])
    attention_plot = start_pt[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>] <span class="hljs-comment"># third item in tuple</span>
    <span class="hljs-keyword">return</span> summary, article, attention_plot
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">summarize()</code> method is extended so that you can generate greedy and beam search, like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Summarize</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">summarize</span><span class="hljs-functio">(</span><span class="hljs-params">article, algo=</span><span class="hljs-string">'greedy'</span><span class="hljs-params">, beam_width=</span><span class="hljs-number">3</span><span class="hljs-params">, verbose=</span><span class="hljs-literal">True</span><span class="hljs-functio">):</span>
    <span class="hljs-keyword">if</span> algo == <span class="hljs-string">'greedy'</span>:
        summary, article, attention_plot = greedy_search(article)
    <span class="hljs-keyword">elif</span> algo==<span class="hljs-string">'beam'</span>:
        summary, article, attention_plot = beam_search(article, 
                                                beam_width=beam_width,
                                                verbose=verbose)
    <span class="hljs-keyword">else</span>:
        print(<span class="hljs-string">"Algorithm {} not implemented"</span>.<span class="hljs-built_in">format</span>(algo))
        <span class="hljs-keyword">return</span>
    
    print(<span class="hljs-string">'Input: %s'</span> % (article))
    print(<span class="hljs-string">'** Predicted Summary: {}'</span>.<span class="hljs-built_in">format</span>(summary))
    attention_plot = attention_plot[:<span class="hljs-built_in">len</span>(summary.split(<span class="hljs-string">' '</span>)), 
                                    :<span class="hljs-built_in">len</span>(article.split(<span class="hljs-string">' '</span>))]
    plot_attention(attention_plot, article.split(<span class="hljs-string">' '</span>), 
                   summary.split(<span class="hljs-string">' '</span>))
</code></pre>
    <p class="normal">Let's re-run the Siemens tram car example:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Greedy search summary</strong>: <em class="italic">siemens to launch reb-made cars</em></li>
      <li class="bullet"><strong class="keyword">Beam search summary</strong>: <em class="italic">siemens working on revamped european tram car</em></li>
    </ul>
    <p class="normal">The beam search summary contains more detail and represents the text better. It introduces a new word, <em class="italic">european</em>, which may or<a id="_idIndexMarker453"/> may not be accurate in the current context. Contrast the following attention plot with the one shown previously:</p>
    <figure class="mediaobject"><img src="image/B16252_06_06.jpg" alt=""/></figure>
    <p class="packt_figref">Figure 6.6: Attention plot of summary generated by beam search</p>
    <p class="normal">The summary generated by beam search covers more concepts from the source text. For the JC Penney example, beam search makes the output better:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Greedy search summary</strong>: <em class="italic">jc penney to close another ## stores stores for #nd stores</em></li>
      <li class="bullet"><strong class="keyword">Beam search summary</strong>: <em class="italic">jc penney to close ## more stores</em></li>
    </ul>
    <p class="normal">The beam search summary is more concise and grammatically correct. These examples were generated with a beam width of 3. The notebook contains several other examples for you to play with. You will notice that generally, beam search improves the results, but it<a id="_idIndexMarker454"/> reduces the length of the output. Beam search suffers from issues where the score of sequences is not normalized for the sequence length, and repeatedly attending to the same input tokens has no penalty.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The most significant improvement at this point will come from training the model for longer and on more examples. The model that was used for these examples was trained for 22 epochs on 1.5M samples out of 3.8M from the Gigaword dataset. However, it is important to have beam search and various penalties in your back pocket to improve the quality of your model.</p>
    </div>
    <p class="normal">There are two specific penalties that address these issues, both of which will be discussed in the next section.</p>
    <h2 id="_idParaDest-109" class="title">Decoding penalties with beam search</h2>
    <p class="normal">Wu et al. proposed <a id="_idIndexMarker455"/>two penalties in the seminal <a id="_idIndexMarker456"/>paper <em class="italic">Google's Neural Machine Translation System</em>, published in 2016. These penalties are:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Length normalization</strong><strong class="keyword"><a id="_idIndexMarker457"/></strong>: Aimed at encouraging longer or short summaries.</li>
      <li class="bullet"><strong class="keyword">Coverage normalization</strong>: Aimed at penalizing generation if the output focuses too much <a id="_idIndexMarker458"/>on the same part of the input sequence. As per the pointer-generator paper, this is best added during training for the last few iterations of training. This will not be implemented in this section.</li>
    </ul>
    <p class="normal">These methods are inspired by NMT and must be adapted for the needs of summarization. At a high level, the score can be represented by the following formula:</p>
    <figure class="mediaobject"><img src="image/B16252_06_004.png" alt="" style="max-height:40px;"/></figure>
    <p class="normal">For example, the beam search algorithm naturally produces shorter sequences. The length penalty is important for NMT as the output sequence should address the input text. This is different from summarization, where shorter outputs are preferred. Length normalization computes a factor based on a parameter and the current token number. The cost of the beam is divided by this factor to calculate a length-normalized score. The paper proposes the following empirical formula:</p>
    <figure class="mediaobject"><img src="image/B16252_06_005.png" alt="" style="max-height:40px;"/></figure>
    <p class="normal">Smaller values of alpha produce shorter sequences, and larger values produce longer sequences. Values of <img src="image/B16252_06_006.png" alt=""/> are between 0 and 1. The conditional probability score is divided by the preceding quantity to give the normalized score for a beam. The <code class="Code-In-Text--PACKT-">length_wu()</code> method normalizes the score using this parameter. </p>
    <p class="normal">Note that all the code for this part is in the <em class="italic">Beam Search with Length Normalizations</em> section of the notebook:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">length_wu</span><span class="hljs-functio">(</span><span class="hljs-params">step, score, alpha=</span><span class="hljs-number">0.</span><span class="hljs-functio">):</span>
    <span class="hljs-comment"># NMT length re-ranking score from</span>
    <span class="hljs-comment"># "Google's Neural Machine Translation System" paper by Wu et al</span>
    modifier = (((<span class="hljs-number">5</span> + step) ** alpha) /
                ((<span class="hljs-number">5</span> + <span class="hljs-number">1</span>) ** alpha))
    <span class="hljs-keyword">return</span> (score / modifier)
</code></pre>
    <p class="normal">It is easy to implement in the code. A new beam search method with normalizations is created. Most of the code is the same as in the previous implementation. The key change for enabling length normalization involves adding an alpha parameter to the method signature <a id="_idIndexMarker459"/>and updating the computation <a id="_idIndexMarker460"/>of the score so that it uses the aforementioned method:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Beam search implementation with normalization</span>
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">beam_search_norm</span><span class="hljs-functio">(</span><span class="hljs-params">article, beam_width=</span><span class="hljs-number">3</span><span class="hljs-params">, </span>
<span class="hljs-params">                         art_max_len=</span><span class="hljs-number">128</span><span class="hljs-params">, </span>
<span class="hljs-params">                         smry_max_len=</span><span class="hljs-number">50</span><span class="hljs-params">,</span>
<span class="hljs-params">                         end_tk=end,</span>
<span class="hljs-params">                         alpha=</span><span class="hljs-number">0.</span><span class="hljs-params">,</span>
<span class="hljs-params">                         verbose=</span><span class="hljs-literal">True</span><span class="hljs-functio">)</span>
</code></pre>
    <p class="normal">Next, the score is normalized like so (around line 60 in the code):</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">for</span> tokid, scre <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(indices, values):
            score = row[<span class="hljs-number">3</span>] - np.log(scre) 
            score = length_wu(t, score, alpha)
</code></pre>
    <p class="normal">Let's try the settings out on some examples. First, we will try placing a length normalization penalty on the Siemens example:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Greedy search</strong>: <em class="italic">siemens to launch reb-made cars</em></li>
      <li class="bullet"><strong class="keyword">Beam search</strong>: <em class="italic">siemens working on revamped european tram car</em></li>
      <li class="bullet"><strong class="keyword">Beam search with length penalties</strong>: <em class="italic">siemens working on new version of defective tram car</em></li>
    </ul>
    <p class="normal">A beam size of 5 and an alpha of 0.8 was used to generate the preceding example. Length normalization generates longer summaries, which corrects some of the challenges that are faced by the summaries generated purely by beam search:</p>
    <figure class="mediaobject"><img src="image/B16252_06_07.jpg" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.7: Beam search with length normalization produces a great summary</p>
    <p class="normal">Now, let's take a look at a more complex contemporary example, which is not in the training set at all:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Input</strong>: <em class="italic">the uk on friday said that it would allow a quarantine-free international travel to some low-risk countries falling in its green zone list of an estimated ## nations . uk transport secretary said that the us will fall within the red zone</em> </li>
      <li class="bullet"><strong class="keyword">Greedy search</strong>: <em class="italic">uk to allow free travel to low-risk countries</em></li>
      <li class="bullet"><strong class="keyword">Beam search</strong>: <em class="italic">britain to allow free travel to low-risk countries</em></li>
      <li class="bullet"><strong class="keyword">Beam search with length normalization</strong>: <em class="italic">britain to allow quarantines free travel to low-risk countries</em></li>
    </ul>
    <p class="normal">The best summary uses beam search and length normalization. Note that beam search alone was removing <a id="_idIndexMarker461"/>a very important word, "quarantines", before "free travel." This changed the meaning of the summary. With length <a id="_idIndexMarker462"/>normalization, the summary contains all the right details. Note that the Gigaword dataset has very short summaries in general, and beam search is making them even shorter. Hence, we use larger values of alpha. Generally, smaller values of alpha are used for summarization and larger values for NMT. You can try different values of the length normalization parameter and beam width to build some intuition. Note that the formulation for the length penalty was empirical. It should also be experimented with.</p>
    <p class="normal">The penalty adds a new parameter that needs to be tuned in addition to beam size. Selecting the right parameters requires a better way of evaluating summaries than human inspection. This is the focus of the next section.</p>
    <h1 id="_idParaDest-110" class="title">Evaluating summaries</h1>
    <p class="normal">When people write summaries, they use inventive language. Human-written summaries often use words that are not present in the vocabulary of the text being summarized. When models generate abstractive summaries, they may also use words that are different from the words used in the ground truth <a id="_idIndexMarker463"/>summaries provided. There is no real way to do an effective semantic comparison of the ground truth summary and the generated summary. In summarization problems, a human evaluation step is often involved, which is where a qualitative check of the generated summaries is done. This method is both unscalable and expensive. There are approximations that uses n-gram overlaps and the longest common subsequence matches after stemming and lemmatization. The hope is that such pre-processing helps bring ground truth and generated <a id="_idIndexMarker464"/>summaries closer together for evaluation. The most common metric used for evaluating summaries is <strong class="keyword">Recall-Oriented Understudy for Gisting Evaluation</strong>, also referred to as <strong class="keyword">ROUGE</strong>. In <a id="_idIndexMarker465"/>machine translation, metrics such as <strong class="keyword">Bilingual Evaluation Understudy</strong> (<strong class="keyword">BLEU</strong>) and <strong class="keyword">Metric for Evaluation of Translation with Explicit Ordering</strong> (<strong class="keyword">METEOR</strong>) are used. BLEU relies mainly on <a id="_idIndexMarker466"/>precision, as precision is very important for translation. In summarization, recall is more important. Consequently, ROUGE is the metric of choice for evaluating summarization models. It was proposed by Chin-Yew Lin in 2004 in a paper titled <em class="italic">Rouge: A Package for Automatic Evaluation of Summaries</em>.</p>
    <h1 id="_idParaDest-111" class="title">ROUGE metric evaluation</h1>
    <p class="normal">A summary that's generated by a model should be readable, coherent, and factually correct. In addition, it should <a id="_idIndexMarker467"/>be grammatically correct. Human evaluation of summaries can be a mammoth task. If a person took 30 seconds to evaluate one summary in the Gigaword dataset, then it would take over 26 hours for one person to check the validation set. Since abstractive summaries are being generated, this human evaluation work will need to be done every time summaries are produced. The ROUGE metric tries to measure various aspects of an abstractive summary. It is a collection of four metrics:</p>
    <ul>
      <li class="bullet"><strong class="keyword">ROUGE-N</strong> is the n-gram <a id="_idIndexMarker468"/>recall between a generated summary and the ground truth or reference summary. "N" at the end of the name specifies the length of the n-gram. It is common to report ROUGE-1 and ROUGE-2. The metric is calculated as the ratio of matching n-grams between the ground truth summary and the generated summary, divided by the total number of n-grams in the ground truth. This formulation is oriented toward recall. If multiple reference summaries exist, the ROUGE-N metric is calculated pairwise for each reference summary, and the maximum score is taken. In our example, only one reference summary exists.</li>
      <li class="bullet"><strong class="keyword">ROUGE-L</strong> uses <a id="_idIndexMarker469"/>the <strong class="keyword">longest common subsequence</strong> (<strong class="keyword">LCS</strong>) between the generated summary and the ground truth to calculate the metric. Often, the sequences are stemmed prior to computing the LCS. Once the length of the LCS is known, precision is calculated by dividing it by <a id="_idIndexMarker470"/>the length of the reference summary; recall is calculated by dividing by the length of the generated score. The F1 score, which is the harmonic mean of precision and recall, is also calculated and reported. The F1 score provides a way for us to balance precision and recall. Since the LCS already includes common n-grams, choosing an n-gram length is not required. This particular version of ROUGE-L is called the sentence-level LCS score. There is a summary-level score as well, for cases when the summary contains more than one sentence. It is used for the CNN and DailyMail datasets, among others. The summary-level score matches each sentence in the ground truth with all the generated sentences to calculate the union LCS precision and recall. Details of the method can be found in the paper referenced previously. </li>
      <li class="bullet"><strong class="keyword">ROUGE-W</strong> is a weighted <a id="_idIndexMarker471"/>version of the previous metric, where contiguous matches in the LCS are weighted higher than if the tokens were separated by some other tokens in the middle.</li>
      <li class="bullet"><strong class="keyword">ROUGE-S</strong> uses skip-bigram <a id="_idIndexMarker472"/>co-occurrence statistics. A skip-bigram allows there to be arbitrary gaps between two tokens. Precision and recall are calculated using this measure.</li>
    </ul>
    <p class="normal">The paper that proposed these metrics also contained code, in Perl, for calculating these metrics. This requires <a id="_idIndexMarker473"/>generating text files with references and generating summaries. Google Research has published a full Python implementation that is available from their GitHub repository: <a href="https://github.com/google-research/google-research"><span class="url">https://github.com/google-research/google-research</span></a>. The <code class="Code-In-Text--PACKT-">rouge/</code> directory contains the code for these metrics. Please follow the installation instructions from the repository. Once installed, we can evaluate greedy search, beam search, and beam search with length normalization to judge their quality using the ROUGE-L metric. The code for this part is in the ROUGE Evaluation section.</p>
    <p class="normal">The scorer library can be imported and initialized like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> rouge_score <span class="hljs-keyword">import</span> rouge_scorer <span class="hljs-keyword">as</span> rs
scorer = rs.RougeScorer([<span class="hljs-string">'rougeL'</span>], use_stemmer=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">A version of the <code class="Code-In-Text--PACKT-">summarize()</code> method, called <code class="Code-In-Text--PACKT-">summarize_quietly()</code>, is used to summarize pieces of text without printing any outputs like attention plots. Random samples from the validation test will be used to measure the performance. The code for loading the <a id="_idIndexMarker474"/>data and the quiet summarization method can be found in the notebook and should be run prior to running metrics. Evaluation can be run using a greedy search, as shown in the following code fragment:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># total eval size: 189651</span>
articles = <span class="hljs-number">1000</span>
f1 = <span class="hljs-number">0.</span>
prec = <span class="hljs-number">0.</span>
rec = <span class="hljs-number">0.</span>
beam_width = <span class="hljs-number">1</span>
<span class="hljs-keyword">for</span> art, smm <span class="hljs-keyword">in</span> ds_val.take(articles):
    summ = summarize_quietly(<span class="hljs-built_in">str</span>(art.numpy()), algo=<span class="hljs-string">'beam-norm'</span>, 
                             beam_width=<span class="hljs-number">1</span>, verbose=<span class="hljs-literal">False</span>)
    score = scorer.score(<span class="hljs-built_in">str</span>(smm.numpy()), summ)
    f1 += score[<span class="hljs-string">'rougeL'</span>].fmeasure / articles
    prec += score[<span class="hljs-string">'rougeL'</span>].precision / articles
    rec += score[<span class="hljs-string">'rougeL'</span>].recall / articles
    <span class="hljs-comment"># see if a sample needs to be printed</span>
    <span class="hljs-keyword">if</span> random.choices((<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>), [<span class="hljs-number">1</span>, <span class="hljs-number">99</span>])[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>: 
<span class="hljs-keyword">    </span><span class="hljs-comment"># 1% samples printed out</span>
        print(<span class="hljs-string">"Article: "</span>, art.numpy())
        print(<span class="hljs-string">"Ground Truth: "</span>, smm.numpy())
        print(<span class="hljs-string">"Greedy Summary: "</span>, summarize_quietly(<span class="hljs-built_in">str</span>(art.numpy()),
              algo=<span class="hljs-string">'beam-norm'</span>, 
              beam_width=<span class="hljs-number">1</span>, verbose=<span class="hljs-literal">False</span>))
        print(<span class="hljs-string">"Beam Search Summary :"</span>, summ, <span class="hljs-string">"\n"</span>)
print(<span class="hljs-string">"Precision: {:.6f}, Recall: {:.6f}, F1-Score: {:.6f}"</span>.<span class="hljs-built_in">format</span>(prec, rec, f1))
</code></pre>
    <p class="normal">While the validation set contains close to 190,000 records, the preceding code runs metrics on 1,000 records. The code also randomly prints out summaries for about 1% of the samples. The results of this evaluation should look similar to these:</p>
    <pre class="programlisting con"><code class="hljs-con">Precision: 0.344725, Recall: 0.249029, F1-Score: 0.266480
</code></pre>
    <p class="normal">This is not a bad start since we have high precision, but the recall is low. The current leaderboard for the Gigaword dataset has 36.74 as the highest ROUGE-L F1 score, as per paperswithcode.com. Let's run the same test with beam search and see the results. The code here is identical to the preceding code, with the only difference being that a beam width of 3 is being used:</p>
    <pre class="programlisting con"><code class="hljs-con">Precision: 0.382001, Recall: 0.226766, F1-Score: 0.260703
</code></pre>
    <p class="normal">It seems that the precision has improved considerably at the expense of recall. Overall, the F1 score shows a slight decrease. Beam search does produce shorter summaries, which could be the reason for the decrease in recall. Adjusting length normalization could help with this. Another hypothesis could be to try bigger beams. Trying a bigger beam size of 5 produces this result:</p>
    <pre class="programlisting con"><code class="hljs-con">Precision: 0.400730, Recall: 0.219472, F1-Score: 0.258531
</code></pre>
    <p class="normal">There is a significant <a id="_idIndexMarker475"/>improvement in precision and a further decrease in recall. Now, let's try some length normalization. Running beam search with an alpha of 0.7 gives us the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Precision: 0.356155, Recall: 0.253459, F1-Score: 0.271813
</code></pre>
    <p class="normal">By running a larger beam width of 5 with the same alpha, we obtain this result:</p>
    <pre class="programlisting con"><code class="hljs-con">Precision: 0.356993, Recall: 0.252384, F1-Score: 0.273171
</code></pre>
    <p class="normal">There is a considerable increase in recall due to there being a decline in precision. Overall, for a basic model trained only on a slice of data, the performance is quite good. A score of 27.3 would yield a spot on the top 20 of the leaderboard.</p>
    <p class="normal">Seq2seq-based text summarization was the main approach prior to the advent of Transformer-based models. Now, Transformer-based models, which include both the Encoder and Decoder parts, are used for summarization. The next section reviews state-of-the-art approaches to summarization.</p>
    <h1 id="_idParaDest-112" class="title">Summarization – state of the art</h1>
    <p class="normal">Today, the predominant <a id="_idIndexMarker476"/>approach to summarization uses the full Transformer architecture. Such models are quite big, often ranging from 223M parameters to over a billion in the case of GPT-3. Google Research published a paper at ICML in June 2020 titled <em class="italic">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</em>. This paper sets the benchmark for state-of-the-art results as of the time of writing. The key innovation proposed by this model is a specific pre-training objective for summarization. Recall that BERT was pre-trained using a <strong class="keyword">masked language model</strong> (<strong class="keyword">MLM</strong>) objective, where <a id="_idIndexMarker477"/>tokens were randomly masked and the model had to predict them. The PEGASUS model proposed a <strong class="keyword">Gap Sentence Generation</strong> (<strong class="keyword">GSG</strong>) pre-training objective, where important sentences are completely <a id="_idIndexMarker478"/>replaced with a special masking token, and the model has to generate the sequence. </p>
    <p class="normal">The importance of the sentence is judged using the ROUGE1-F1 score of a given score compared to the entire document. A certain <a id="_idIndexMarker479"/>number of top-scoring sentences are masked from the input, and the model needs to predict them. Additional details can be found in the aforementioned paper. The base Transformer model is very similar to the BERT configurations. The pre-training objective makes a significant difference to the ROUGE1/2/L-F1 scores and sets new records on many of the datasets.</p>
    <p class="normal">These models are quite large and training them on a desktop is not realistic. Often, the models are pre-trained on humongous datasets for several days at a time. Thankfully, pre-trained versions of such models are available through libraries like HuggingFace.</p>
    <h1 id="_idParaDest-113" class="title">Summary</h1>
    <p class="normal">Summarizing text is considered a uniquely human trait. Deep learning NLP models have made great strides in this area in the past 2-3 years. Summarization remains a very hot area of research within many applications. In this chapter, we built a seq2seq model from scratch that can summarize sentences from news articles and generate a headline. This model obtains fairly good results due to its simplicity. We were able to train the model for a long period of time due to learning rate annealing. By checkpointing the model, training was made resilient as it could be restarted from the last checkpoint in case of failure. Post-training, we improved our generated summaries through a custom implementation of beam search. As beam search has a tendency to provide short summaries, length normalization techniques were used to make the summaries even better.</p>
    <p class="normal">Measuring the quality of generated summaries is a challenge in abstractive summarization. Here is a random example from the validation dataset:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Input</strong>: <em class="italic">the french soccer star david ginola on saturday launched his anti-land mines campaign on behalf of the international committee for the red cross which has taken him on as a sort of poster boy for the cause .</em></li>
      <li class="bullet"><strong class="keyword">Ground truth</strong>: <em class="italic">soccer star joins red cross effort against land mines</em></li>
      <li class="bullet"><strong class="keyword">Beam search (5/0.7)</strong>: <em class="italic">former french star ginola launches anti-land mine campaign</em></li>
    </ul>
    <p class="normal">The generated summary is very comparable to the ground truth. However, matching token by token would give us a very low score. ROUGE metrics that use n-grams and the LCS allow us to measure the quality of the summaries.</p>
    <p class="normal">Finally, we took a quick look at the current state-of-the-art models for summarization. Large models that are pre-trained on even larger datasets are ruling the roost. Unfortunately, training a model of such size is often beyond the resources of a single individual.</p>
    <p class="normal">Now, we will move on to a very new and exciting area of research – multi-modal networks. Thus far, we have only treated text in isolation. But is a picture really worth a thousand words? We shall find out when we try to caption images and answer questions about them in the next chapter.</p>
  </div>
</body></html>