- en: Speech to Text and Topic Extraction Using NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recognizing and understanding spoken language is a challenging problem due to
    the complexity and variety of speech data. There have been several different technologies
    deployed to recognize spoken words in the past. Most of those approaches were
    very limited in their scope, as they were unable to recognize a wide variety of
    words, accents, and tones, and aspects of spoken language, such as a pause between
    spoken words. Some of the prevalent modeling technique for speech recognition
    include **Hidden Markov Models** (**HMM**), **Dynamic Time Warping** (**DTW**),
    **Long Short-Term Memory Network**s (**LSTM**), and **Connectionist Temporal Classification**
    (**CTC**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we shall learn about various options for speech to text and
    the prebuilt model from Google''s TensorFlow team, using the Speech Commands Dataset.
    We shall cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Speech to text frameworks and toolkits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Speech Commands Dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN based architecture for speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A TensorFlow speech commands example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download and follow the code for this chapter from [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/).
  prefs: []
  type: TYPE_NORMAL
- en: Speech-to-text frameworks and toolkits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many cloud-based AI providers offer speech to text as a service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon''s offering for speech recognition is known as **Amazon Transcribe**.
    Amazon Transcribe allows transcription of the audio files stored in Amazon S3
    in four different formats: `.flac`, `.wav`, `.mp4`, and `.mp3`. It allows an audio
    file with a maximum of two hours in length and 1 GB in size. The results of the
    transcription are created as a JSON file in an Amazon S3 bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google offers speech to text as part of its Google Cloud ML Services. Google
    Cloud Speech to Text supports `FLAC`, `Linear16`, `MULAW`, `AMR`, `AMR_WB`, and
    `OGG_OPUS` file formats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft offers a speech to text API as part of its Azure Cognitive Services
    platform, known as Speech Service SDK. The Speech Service SDK integrates with
    rest of the Microsoft APIs to transcribe recorded audio. It only allows the WAV
    or PCM file format with a single channel and sample rate of 6 kHz.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IBM offers a speech to text API as part if its Watson platform. Watson Speech
    to Text supports eight audio formats: BASIC, FLAC, L16, MP3, MULAW, OGG, WAV,
    and WEBM. The maximum size and length of the audio files vary depending on the
    format used. The results of transcription are returned as a JSON file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apart from the support for various international spoken languages and an extensive
    global vocabulary, these cloud services support the following features to different
    extents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multichannel recognition**: Identifying multiple participants recorded in
    multiple channels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speaker diarization**: Prediction of speech of a certain speaker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom models and model selection**: Plug in your own models and select from
    a plethora of pre-built models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inappropriate content filtering and noise filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also many open source toolkits for speech recognition, such as Kaldi.
  prefs: []
  type: TYPE_NORMAL
- en: Kaldi ([http:/kaldi-asr.org](http:/kaldi-asr.org)) is a popular open source
    speech to text recognition library. It is written in C++ and is available from [https://github.com/kaldi-asr/kaldi](https://github.com/kaldi-asr/kaldi).
    Kaldi can be integrated into your applications using its C++ API. It also supports
    Android using NDK, clang++, and OpenBLAS.
  prefs: []
  type: TYPE_NORMAL
- en: Google Speech Commands Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Google Speech Commands Dataset was created by the TensorFlow and AIY teams
    to showcase the speech recognition example using the TensorFlow API. The dataset
    has 65,000 clips of one-second-long duration. Each clip contains one of the 30
    different words spoken by thousands of different subjects.
  prefs: []
  type: TYPE_NORMAL
- en: The Google Speech Commands Dataset is available from the following link: [http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz](http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz).
  prefs: []
  type: TYPE_NORMAL
- en: 'The clips were recorded in realistic environments with phones and laptops.
    The 35 words contained noise words and the ten command words most useful in a
    robotics environment, and are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'No'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Off'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More details on how the speech dataset is prepared can be found in the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1804.03209.pdf](https://arxiv.org/pdf/1804.03209.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this dataset, thus the problem that shown in the example in this chapter
    is known as Keyword Spotting Task.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The network used for this example has three modules:'
  prefs: []
  type: TYPE_NORMAL
- en: A feature extraction module that processes the audio clips into feature vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep neural network module that produces softmax probabilities for each word
    in the input frame of feature vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A posterior handling module that combines the frame-level posterior scores into
    a single score for each keyword
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to make the computation easy, the incoming audio signal is run through
    a voice-activity detection system and the signal is divided into speech and non-speech
    parts of the signals. The voice activity detector uses a 30-component diagonal
    covariance GMM model. The input to this model is 13-dimensional PLP features,
    their deltas, and double deltas. The output of GMM is passed to a State Machine
    that does temporal smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: The output of this GMM-SM module is speech and non-speech parts of the signal.
  prefs: []
  type: TYPE_NORMAL
- en: The speech parts of the signal are further processed to generate the features.
    The acoustic features are generated based on 40-dimensional log-filterbank energies
    computed every 10 ms over a window of 25 ms. 10 Future and 30 Pas frames are added
    to the signal.
  prefs: []
  type: TYPE_NORMAL
- en: More details on feature extractor can be obtained from the original papers,
    links provided in the further readings section.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural network module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DNN module is implemented with the Convolutional Neural Network (CNN) architecture.
    The code implements multiple variations of ConvNet, each variation producing different
    levels of accuracy and taking a different amount of time to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for building the model is provided in the `models.py` file. It allows
    the creation of four different models, depending on the parameter passed at the
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '`single_fc`: This model has only one fully connected layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv`: This model is a full CNN architecture with two pairs of Convolution
    and MaxPool layers, followed by a fully connected layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_latency_conv`: This model has one convolutional layer, followed by three
    fully connected layers. As the name suggests, it has a lesser number of parameters
    and computations compared with the `conv` architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_latency_svdf`: This model follows the architecture and layers from the
    paper titled *Compressing Deep Neural*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Networks using a Rank-Constrained Topology* available from [https://research.google.com/pubs/archive/43813.pdf](https://research.google.com/pubs/archive/43813.pdf).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`tiny_conv`: This model has only one convolutional and one fully connected
    layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The default architecture is `conv`, if the architecture is not passed from
    the command line. In our runs, the architectures showed the following accuracies
    for training, validation, and test sets when running the models with default accuracy
    and default number of steps of 18,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Accuracy (in %) |'
  prefs: []
  type: TYPE_TB
- en: '| Train set | Validation set | Test set |'
  prefs: []
  type: TYPE_TB
- en: '| `conv` (default) | 90 | 88.5 | 87.7 |'
  prefs: []
  type: TYPE_TB
- en: '| `single_fc` | 50 | 48.5 | 48.2 |'
  prefs: []
  type: TYPE_TB
- en: '| `low_latenxy_conv` | 22 | 21.6 | 23.6 |'
  prefs: []
  type: TYPE_TB
- en: '| `low_latency_svdf` | 7 | 8.9 | 8.6 |'
  prefs: []
  type: TYPE_TB
- en: '| `tiny_conv` | 55 | 65.7 | 65.4 |'
  prefs: []
  type: TYPE_TB
- en: Since the network architecture uses CNN layers that are more suitable for image
    data, the speech files are converted to a single-channel image by converting the
    audio signal of a short segment into vectors of frequency strengths.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the preceding observations, the shortened architectures give
    lower accuracy for same hyper-parameters, but they run faster. Hence, they can
    be run for a higher number of epochs, or the learning rate could be increased
    to get higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how to train and use this model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Move to the folder where you cloned the code from the repository, and train
    the model with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will start seeing the output of the training as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training iterations start, the code prints out the learning rate,
    along with accuracy and the cross entropy loss on the training set, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The code also keeps saving the model every 100 steps, so that if training is
    interrupted, then it can be restarted from the latest checkpoint saved:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The training runs for several hours for 18,000 steps and at the end prints
    the final training learning rate, accuracy, loss, and confusion matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: One observation from this output is that, although the code starts with a learning
    rate of 0.001, it reduces the learning rate to 0.001 towards the end. Since there
    are 12 command words, it also prints out a 12 x 12 confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code also prints the accuracy and confusion matrix on the validation set
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the code prints the test set accuracy as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: That's it. The model has been trained and can be exported for serving through
    TensorFlow, or embedding in another desktop, web, or mobile app.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about a project for converting audio data to text.
    There are many open source SDKs and commercial paid cloud services that allow
    us to convert from audio recordings and files to text data. As an example project,
    we took Google's Speech Commands Dataset and TensorFlow's deep learning-based
    example to convert audio files to spoken words in order to recognize the commands.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we continue this journey to build a project for predicting
    stock prices using Gaussian Processes, which is a popular algorithm for forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the interpretation of the confusion matrix provided at the end of the
    training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dataset of your own recorded voices with your friends and family members.
    Run the model on this data and observe what the accuracy is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrain the model on your own dataset and check the accuracy for your own train,
    validation and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with different options from train.py and share your findings in a
    blog.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add different architectures to the models.py file and see if you can create
    a better architecture for the speech dataset or your own recorded dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following links are helpful in learning more about speech to text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/pdf/1804.03209.pdf](https://arxiv.org/pdf/1804.03209.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html](https://arxiv.org/pdf/1804.03209.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://research.google.com/pubs/archive/43813.pdf](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/speech-to-text/docs/](https://research.google.com/pubs/archive/43813.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html](https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/](https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://nlp.stanford.edu/projects/speech.shtml](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
