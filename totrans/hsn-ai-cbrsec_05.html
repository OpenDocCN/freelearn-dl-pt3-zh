<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Ham or Spam? </span><span class="koboSpan" id="kobo.1.2">Detecting Email Cybersecurity Threats with AI</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Most security threats use email as an attack vector. </span><span class="koboSpan" id="kobo.2.2">Since the amount of traffic conveyed in this way is particularly large, it is necessary to use automated detection procedures that exploit </span><strong><span class="koboSpan" id="kobo.3.1">machine learning</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">ML</span></strong><span class="koboSpan" id="kobo.6.1">) algorithms. </span><span class="koboSpan" id="kobo.6.2">In this chapter, different detection strategies ranging from linear classifiers and Bayesian filters to more sophisticated solutions such as decision trees, logistic regression, and </span><strong><span class="koboSpan" id="kobo.7.1">natural language processing</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong><span class="koboSpan" id="kobo.9.1">NLP</span></strong><span class="koboSpan" id="kobo.10.1">) will be illustrated.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">This chapter will cover the following topics:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.12.1">How to detect spam with Perceptrons</span></li>
<li><span class="koboSpan" id="kobo.13.1">Image spam detection with </span><strong><span class="koboSpan" id="kobo.14.1">support vector machines</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><strong><span class="koboSpan" id="kobo.16.1">SVMs</span></strong><span class="koboSpan" id="kobo.17.1">)</span></li>
<li><span class="koboSpan" id="kobo.18.1">Phishing detection with logistic regression and decision trees</span></li>
<li><span class="koboSpan" id="kobo.19.1">Spam detection with Naive Bayes</span></li>
<li><span class="koboSpan" id="kobo.20.1">Spam detection adopting NLP</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Detecting spam with Perceptrons</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the first concrete and successful applications of AI in the field of cybersecurity was spam detection, and one of the most famous open source tools is </span><strong><span class="koboSpan" id="kobo.3.1">SpamAssassin</span></strong><span class="koboSpan" id="kobo.4.1">.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The strategies that can be implemented for effective spam detection are different, as we will see in the course of the chapter, but the most common and simpler one uses </span><strong><span class="koboSpan" id="kobo.6.1">Neural Networks</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong><span class="koboSpan" id="kobo.8.1">NNs</span></strong><span class="koboSpan" id="kobo.9.1">) in the most basic form; that is, the Perceptron.</span></p>
<p><span class="koboSpan" id="kobo.10.1">Spam detection also provides us with the opportunity to introduce theoretical concepts related to NNs in a gradual and accessible way, starting with the Perceptron.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Meet NNs at their purest – the Perceptron</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The peculiar characteristic that unites all NNs (regardless of their implementation complexity) is that they </span><span><span class="koboSpan" id="kobo.3.1">conceptually</span></span><span class="koboSpan" id="kobo.4.1"> mimic the behavior of the human brain. </span><span class="koboSpan" id="kobo.4.2">The most basic structure we encounter when we analyze the behavior of the brain, is undoubtedly the neuron.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The Perceptron is one of the first successful implementations of a neuron in the field of </span><strong><span class="koboSpan" id="kobo.6.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong><span class="koboSpan" id="kobo.8.1">AI</span></strong><span class="koboSpan" id="kobo.9.1">). </span><span class="koboSpan" id="kobo.9.2">Just like a neuron in the human brain, it is characterized by a layered structure, aimed at associating a result in output to certain input levels, as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.10.1"><img src="assets/8ed6924f-b57b-4920-916d-6bf7aca65538.png" style="width:28.58em;height:16.92em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.11.1">In the same way, the artificial representation of the neuron implemented through the Perceptron model is structured in such a way as to associate a given output value to one or more levels of input data:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.12.1"><img style="font-size: 1em;width:30.75em;height:14.58em;" src="assets/ce0d6325-06b4-42af-bf86-736180df18a3.png"/></span></p>
<p><span><span class="koboSpan" id="kobo.13.1">The mechanism that transforms the input data into an output value is implemented by making use of an appropriate weighing of the values of an input, which are synthesized and forwarded to an activation function, which, when exceeding a certain threshold, produces a value of output that is forwarded to the remaining components of the NN.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">It's all about finding the right weight!</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the differences in the approach between the statistical models and the AI algorithms is that the algorithms implement an optimization strategy based on the iteration. </span><span class="koboSpan" id="kobo.2.2">At each iteration, in fact, the algorithm tries to adjust its own estimate of the values, attributing to them a greater or lesser weight depending on the cost function that we must minimize. </span><span class="koboSpan" id="kobo.2.3">One of the aims of the algorithm is to identify precisely an optimal weight vector to be applied to the estimated values in order to obtain reliable future predictions on unknown future data.</span></p>
<p><span class="koboSpan" id="kobo.3.1">To fully understand the power of AI algorithms applied to spam detection, we must first clarify the ideas on which tasks we should perform a spam filter.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Spam filters in a nutshell</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1"> To understand the tasks performed by a spam filter, let's look at an example. </span><span class="koboSpan" id="kobo.2.2">Imagine separating the emails we receive, categorizing them based on the presence or the absence of particular keywords occurring within the text of the emails with a certain frequency. </span><span class="koboSpan" id="kobo.2.3">To this end, we could list all the messages we receive in our inbox within a table. </span><span class="koboSpan" id="kobo.2.4">But how will we proceed with classifying our messages as ham or spam?</span></p>
<p><span class="koboSpan" id="kobo.3.1">As we said, we will look for the number of occurrences of the suspicious keywords within the text of the email messages. </span><span class="koboSpan" id="kobo.3.2">We will then assign a score to the individual messages identified as spam, based on the number of occurrences of identified keywords. </span><span class="koboSpan" id="kobo.3.3">This score will also provide us with a reference to classify subsequent email messages.</span></p>
<p><span class="koboSpan" id="kobo.4.1">We will identify a threshold value that allows us to separate spam messages. </span><span class="koboSpan" id="kobo.4.2">If the calculated score exceeds the threshold value, the email will automatically be classified as spam; otherwise, it will be accepted as a legitimate message, and thus classified as ham. </span><span class="koboSpan" id="kobo.4.3">This threshold value (as well as the assigned scores) will be constantly redetermined to take into account the new series of spam messages that we will meet in the future.</span></p>
<p><span class="koboSpan" id="kobo.5.1">Even from the abstract description of our spam detection algorithm, we notice some important features that must be kept in mind: we must proceed to identify a certain number of suspicious keywords that allow us to classify the messages as potential spam emails, assigning to each email a score based on the number of occurrences of identified keywords.</span></p>
<p><span class="koboSpan" id="kobo.6.1">We need to set a threshold value for the score assigned to the individual emails above which the emails will automatically be classified as spam. </span><span class="koboSpan" id="kobo.6.2">We must also correctly weigh the significance of the keywords present in the text of the emails in order to adequately represent the degree of probability that the message that contains them represents spam (the keywords, in fact, taken individually, could even be harmless, but put together, they are more likely to represent junk mail).</span></p>
<p><span class="koboSpan" id="kobo.7.1">We must consider that the spammers are well aware of our attempt to filter unwanted messages, and therefore they'll try their best to adopt new strategies to deceive us and our spam filters. </span><span class="koboSpan" id="kobo.7.2">This translates into a process of continuous and iterative learning, which lends itself well to being implemented using an AI algorithm.</span></p>
<p><span class="koboSpan" id="kobo.8.1">From what we have said, it is clear that it is no coincidence that spam detection represents a first test in the adoption of AI in the cybersecurity field. </span><span class="koboSpan" id="kobo.8.2">The first spam detection solution, in fact, made use of static rules, using regular expressions to identify predefined patterns of suspicious words in the email text.</span></p>
<p><span class="koboSpan" id="kobo.9.1">These static rules quickly proved to be ineffective as a result of the ever-new deception strategies implemented by spammers to deceive the anti-spam filters. </span><span class="koboSpan" id="kobo.9.2">It was therefore necessary to adopt a dynamic approach, which allowed the spam filter to learn based on the continuous innovations introduced by spammers, also taking advantage of the decisions made by the user in classifying their emails. </span><span class="koboSpan" id="kobo.9.3">This way, it was possible to effectively manage the explosive spread of the spam phenomenon.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Spam filters in action</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">How does an anti-spam algorithm actually behave in the classification of emails? </span><span class="koboSpan" id="kobo.2.2">First of all, let's classify the emails based on suspicious keywords. </span><span class="koboSpan" id="kobo.2.3">Let's imagine, for the sake of simplicity, that the list of the most representative suspicious keywords is thus reduced to only two words: buy and sex.</span></p>
<p><span class="koboSpan" id="kobo.3.1">At this point, we will classify the email messages within a table, showing the number of occurrences of the individual keywords identified within the text of the emails, indicating the messages as spam or ham:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.4.1"><img src="assets/1d2b3161-314c-406a-9866-ed9e0b87d485.png" style="width:47.67em;height:8.50em;"/></span></p>
<p><span class="koboSpan" id="kobo.5.1">At this point, we will assign a score to every single email message.</span></p>
<p><span class="koboSpan" id="kobo.6.1">This score will be calculated using a scoring function that takes into account the number of occurrences of suspicious keywords contained within the text.</span></p>
<p><span class="koboSpan" id="kobo.7.1">A possible scoring function could be the sum of the occurrences of our two keywords, represented in this case by the </span><em><span class="koboSpan" id="kobo.8.1">B</span></em><span class="koboSpan" id="kobo.9.1"> variable instead of the word buy, and the </span><em><span class="koboSpan" id="kobo.10.1">S</span></em><span class="koboSpan" id="kobo.11.1"> variable instead of the word sex.</span></p>
<p><span class="koboSpan" id="kobo.12.1">The scoring function therefore becomes the following:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><q><span class="koboSpan" id="kobo.13.1"><img class="fm-editor-equation" src="assets/7fe320e9-9583-4ddf-b21b-50309382d3ff.png" style="width:4.42em;height:1.00em;"/></span></q></p>
<p><span class="koboSpan" id="kobo.14.1">We can also attribute different weights to the representative variables of the respective keywords, based on the fact that, for example, the keyword sex contained within the message is indicative of a greater probability of spam than the word buy.</span></p>
<p><span class="koboSpan" id="kobo.15.1">It is clear that if both words are present in the text of the email, the probability of it being spam increases. </span><span class="koboSpan" id="kobo.15.2">Therefore, we will attribute a lower weight of </span><em><span class="koboSpan" id="kobo.16.1">2</span></em><span class="koboSpan" id="kobo.17.1"> to the </span><em><span class="koboSpan" id="kobo.18.1">B</span></em><span class="koboSpan" id="kobo.19.1"> variable and a greater weight of </span><em><span class="koboSpan" id="kobo.20.1">3</span></em><span class="koboSpan" id="kobo.21.1"> to the </span><em><span class="koboSpan" id="kobo.22.1">S</span></em><span class="koboSpan" id="kobo.23.1"> variable.</span></p>
<p><span class="koboSpan" id="kobo.24.1">Our scoring function, corrected with the relative weights assigned to the variables/keywords, therefore becomes the following:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.25.1"><img class="fm-editor-equation" src="assets/f1080b19-8eb9-4a55-8151-2e18a348d0c4.png" style="width:5.33em;height:1.00em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.26.1">Now let's try to reclassify our emails, calculating the relative scores with our scoring function:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.27.1"><img src="assets/7a298cdb-ef25-4c2f-97f8-f51ebf195e7b.png" style="width:47.67em;height:7.58em;"/></span></p>
<p><span class="koboSpan" id="kobo.28.1">At this point, we must try to identify a threshold value that effectively separates spam from ham. </span><span class="koboSpan" id="kobo.28.2">Indeed, a threshold value between </span><strong><span class="koboSpan" id="kobo.29.1">4</span></strong><span class="koboSpan" id="kobo.30.1"> and </span><strong><span class="koboSpan" id="kobo.31.1">5</span></strong><span class="koboSpan" id="kobo.32.1"> allows us to properly separate the spam from the ham. </span><span class="koboSpan" id="kobo.32.2">In other words, in the event that a new email message scores a value equal to or greater than </span><strong><span class="koboSpan" id="kobo.33.1">4</span></strong><span class="koboSpan" id="kobo.34.1">, we would most likely be faced with spam rather than ham.</span></p>
<p><span class="koboSpan" id="kobo.35.1">How can we effectively translate the concepts we have just seen into mathematical formulas that can be used in our algorithms?</span></p>
<p><span class="koboSpan" id="kobo.36.1">To this end, linear algebra (as we mentioned in </span><a href="fbb9686a-7359-4659-bcdf-d0bf5e4e6af8.xhtml"><span class="koboSpan" id="kobo.37.1">Chapter 2</span></a><span class="koboSpan" id="kobo.38.1">, </span><em><span class="koboSpan" id="kobo.39.1">Setting Your AI for Cybersecurity Arsenal</span></em><span class="koboSpan" id="kobo.40.1">, when we talked about the matrix implementation offered by the </span><kbd><span class="koboSpan" id="kobo.41.1">numpy</span></kbd><span class="koboSpan" id="kobo.42.1"> library) comes to our aid.</span></p>
<p><span class="koboSpan" id="kobo.43.1">We will discuss further the implementation of Perceptrons, but first, we will introduce the concept of a linear classifier, useful for mathematically representing the task performed by a common spam detection algorithm.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Detecting spam with linear classifiers</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As known from linear algebra, the equation that represents the function used to determine the score to be associated with every single email message is as follows:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img class="fm-editor-equation" src="assets/ebf6f6ad-2011-4d6c-b4b0-be0c9587e690.png" style="width:5.75em;height:1.08em;"/></span></p>
<p><span class="koboSpan" id="kobo.4.1">This identifies a straight line in the Cartesian plane; therefore, the classifier used by our spam filter to classify emails is called a </span><strong><span class="koboSpan" id="kobo.5.1">linear classifier</span></strong><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">Using the known mathematical formalization commonly adopted in statistics, it is possible to redefine the previous equation in a more compact form by introducing the sum operator </span><span class="koboSpan" id="kobo.7.1"><img class="fm-editor-equation" src="assets/e17c9d51-ab4d-4c35-a5ac-dc51b490b4fc.png" style="width:1.00em;height:1.08em;"/></span><span class="koboSpan" id="kobo.8.1">, substituting in place of the </span><em><span class="koboSpan" id="kobo.9.1">B</span></em><span class="koboSpan" id="kobo.10.1"> and </span><em><span class="koboSpan" id="kobo.11.1">S</span></em><span class="koboSpan" id="kobo.12.1"> variables a matrix of indexed values </span><span class="koboSpan" id="kobo.13.1"><img class="fm-editor-equation" src="assets/18e041cb-9379-4929-995d-7ad8488cbe71.png" style="width:1.25em;height:1.00em;"/></span><span class="koboSpan" id="kobo.14.1">, and a vector of weights </span><span class="koboSpan" id="kobo.15.1"><img class="fm-editor-equation" src="assets/cabcde63-c1a9-4098-bfca-ce82f3b97179.png" style="width:1.50em;height:1.08em;"/></span><span class="koboSpan" id="kobo.16.1"> associated with it:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.17.1"><img class="fm-editor-equation" src="assets/e2c5ccfa-dd01-462f-86b1-74ba10310214.png" style="width:6.33em;height:1.83em;"/></span></p>
<p><span class="koboSpan" id="kobo.18.1">With the index </span><em><span class="koboSpan" id="kobo.19.1">i,</span></em><span class="koboSpan" id="kobo.20.1"> which takes the values from </span><em><span class="koboSpan" id="kobo.21.1">1</span></em><span class="koboSpan" id="kobo.22.1"> to </span><em><span class="koboSpan" id="kobo.23.1">n, </span></em><span class="koboSpan" id="kobo.24.1">this formalization is nothing more than the compact form of the previous summation between the variables and our relative weights:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.25.1"><img class="fm-editor-equation" src="assets/014da7e8-db01-451b-bf0f-d46ed7176999.png" style="width:18.92em;height:1.33em;"/></span></p>
<p><span class="koboSpan" id="kobo.26.1">This way, we have generalized our linear classifier to an unspecified number of variables, </span><em><span class="koboSpan" id="kobo.27.1">n</span></em><span class="koboSpan" id="kobo.28.1">, rather than limiting ourselves to </span><em><span class="koboSpan" id="kobo.29.1">2</span></em><span class="koboSpan" id="kobo.30.1"> as in the previous case. </span><span class="koboSpan" id="kobo.30.2">This compact representation is also useful for exploiting linear algebra formulas in the implementation of our algorithms.</span></p>
<p><span class="koboSpan" id="kobo.31.1">In fact, our function translates into a sum of products (between individual weights and variables) that can easily be represented as a product of matrices and vectors:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.32.1"><img class="fm-editor-equation" src="assets/8f0458cf-0f6c-4264-8ad9-20a99cf8e4c6.png" style="width:4.25em;height:1.17em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.33.1">Here, </span></span><em><span class="koboSpan" id="kobo.34.1">wT</span></em><span><span class="koboSpan" id="kobo.35.1"> stands for the transposed weights carrier, necessary calculating the product of the matrices and vectors.</span></span></p>
<p><span class="koboSpan" id="kobo.36.1">As we have seen, to adequately classify email messages, we need to identify an appropriate threshold value that correctly splits spam messages from ham messages: if the score associated with a single email message is equal to or higher than the threshold value, the message email will be classified as spam (and we will assign it the value </span><kbd><span class="koboSpan" id="kobo.37.1">+1</span></kbd><span class="koboSpan" id="kobo.38.1">); otherwise, it will be classified as ham (to which we will assign the value </span><kbd><span class="koboSpan" id="kobo.39.1">-1</span></kbd><span class="koboSpan" id="kobo.40.1">).</span></p>
<p><span class="koboSpan" id="kobo.41.1">In formal terms, we represent this condition as follows (where </span><em><span class="koboSpan" id="kobo.42.1">θ</span></em><span class="koboSpan" id="kobo.43.1"> represents the threshold value):</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.44.1"><img class="fm-editor-equation" src="assets/443b7012-87c0-4c7e-abec-3367a3abb7be.png" style="width:55.75em;height:3.58em;"/></span></p>
<p><span class="koboSpan" id="kobo.45.1">The preceding conditions are nothing but the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.46.1"><img class="fm-editor-equation" src="assets/1a3e4c31-ab06-452a-8774-8f2f7845186b.png" style="width:55.75em;height:3.33em;"/></span></p>
<p><span class="koboSpan" id="kobo.47.1">It is a consolidated habit to further generalize this formalization by shifting the </span><em><span class="koboSpan" id="kobo.48.1">θ</span></em><span class="koboSpan" id="kobo.49.1"> threshold value on the left side of the equation, associating it with the </span><em><span class="koboSpan" id="kobo.50.1">x</span><sub><span class="koboSpan" id="kobo.51.1">0</span></sub></em><span class="koboSpan" id="kobo.52.1"> variable (thus introducing the </span><em><span class="koboSpan" id="kobo.53.1">i = 0</span></em><span><span class="koboSpan" id="kobo.54.1"> positional index</span></span><span class="koboSpan" id="kobo.55.1"> of the summation) to which we attribute the conventional value of 1, and a weight </span><em><span class="koboSpan" id="kobo.56.1">w</span><sub><span class="koboSpan" id="kobo.57.1">0</span></sub></em><span class="koboSpan" id="kobo.58.1"> equal to </span><em><span class="koboSpan" id="kobo.59.1">-θ</span></em><span class="koboSpan" id="kobo.60.1"> (that is, the threshold value taken with the negative sign, following the displacement of </span><em><span class="koboSpan" id="kobo.61.1">θ</span></em><span class="koboSpan" id="kobo.62.1"> on the left side of the equation). </span><span class="koboSpan" id="kobo.62.2">Therefore, with </span><em><span class="koboSpan" id="kobo.63.1">θ</span></em><span class="koboSpan" id="kobo.64.1"> we replace the product:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.65.1"><img class="fm-editor-equation" src="assets/091ecebf-33c3-4e1a-b7fc-56971aec261b.png" style="width:19.17em;height:1.42em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.66.1">This way, our compact formulation of the linear classifier takes its definitive form:</span></span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.67.1"><img class="fm-editor-equation" src="assets/ed16fec0-b6bc-4fa0-b2f3-fa888f826544.png" style="width:34.67em;height:2.17em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.68.1">Here, the</span></span> <span><span class="koboSpan" id="kobo.69.1">index</span></span> <span class="koboSpan" id="kobo.70.1"><img class="fm-editor-equation" src="assets/85b1f65e-49c9-4197-96c2-1e0fba296044.png" style="width:0.50em;height:1.33em;"/></span><span class="koboSpan" id="kobo.71.1"> </span><span><span class="koboSpan" id="kobo.72.1">now assumes the values from</span></span> <em><span class="koboSpan" id="kobo.73.1">0</span></em><span class="koboSpan" id="kobo.74.1"> to </span><em><span class="koboSpan" id="kobo.75.1">n</span></em><span><span class="koboSpan" id="kobo.76.1">.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How the Perceptron learns</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The approach followed by Rosenblatt </span><span><span class="koboSpan" id="kobo.3.1">Perceptron </span></span><span class="koboSpan" id="kobo.4.1">model, which we have described so far in this chapter, is based on a simplified description of the neuron of the human brain. </span><span class="koboSpan" id="kobo.4.2">Just as the brain's neurons activate in the case of a positive signal, and remain inert otherwise, the Perceptron uses the threshold value via an activation function, which assigns a </span><kbd><span class="koboSpan" id="kobo.5.1">+1</span></kbd><span class="koboSpan" id="kobo.6.1"> value (in case of excitement of the Perceptron, which indicates the pre-established threshold value has been exceeded), or a </span><kbd><span class="koboSpan" id="kobo.7.1">-1</span></kbd><span class="koboSpan" id="kobo.8.1"> value (in other words, indicating a failure to exceed the threshold value).</span></p>
<p><span class="koboSpan" id="kobo.9.1">Taking up the previous mathematical expression that determines the conditions of activation of the Perceptron:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.10.1"><img class="fm-editor-equation" src="assets/8db10016-290d-4dd5-8186-11841ebeac4f.png" style="width:55.75em;height:3.58em;"/></span></p>
<p><span class="koboSpan" id="kobo.11.1">We see that it is the product of the </span><span class="koboSpan" id="kobo.12.1"><img class="fm-editor-equation" src="assets/ab8d4619-eaa0-4f07-aaa6-634ba22ef1e2.png" style="width:2.00em;height:1.00em;"/></span><span class="koboSpan" id="kobo.13.1"> values (that is, the input data for the corresponding weights) that has to overcome the </span><em><span class="koboSpan" id="kobo.14.1">θ</span></em><span class="koboSpan" id="kobo.15.1"> threshold to determine the activation of the Perceptron. </span><span class="koboSpan" id="kobo.15.2">Since the </span><em><span class="koboSpan" id="kobo.16.1">x</span><sub><span class="koboSpan" id="kobo.17.1">i</span></sub></em><span class="koboSpan" id="kobo.18.1"> input data is by definition prefixed, it is the value of the corresponding weights that helps to determine if the Perceptron has to activate itself or not.</span></p>
<p><span class="koboSpan" id="kobo.19.1">But how are weights updated in practice, thus determining the Perceptron learning process?</span></p>
<p><span class="koboSpan" id="kobo.20.1">The Perceptron learning process can be synthesized in the following three phases:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.21.1">Initializing the weights to a predefined value (usually equal to </span><em><span class="koboSpan" id="kobo.22.1">0</span></em><span><span class="koboSpan" id="kobo.23.1">)</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Calculating the output value, </span><span class="koboSpan" id="kobo.25.1"><img class="fm-editor-equation" src="assets/6bf7d5f2-776c-4897-87d9-093052675e4f.png" style="width:1.17em;height:1.17em;"/></span><span class="koboSpan" id="kobo.26.1">, </span><span><span class="koboSpan" id="kobo.27.1">for each corresponding training sample,</span></span><span class="koboSpan" id="kobo.28.1"> </span><span class="koboSpan" id="kobo.29.1"><img class="fm-editor-equation" src="assets/d41005e9-d267-4216-bfb0-8794327e9c79.png" style="width:1.25em;height:1.00em;"/></span></li>
<li><span class="koboSpan" id="kobo.30.1">Updating the weights on the basis of the distance between the expected output value (that is, the </span><span class="koboSpan" id="kobo.31.1"><img class="fm-editor-equation" src="assets/19a2dde2-3988-4d4e-8097-65ce1b3bad83.png" style="width:0.58em;height:0.92em;"/></span><span><span class="koboSpan" id="kobo.32.1"> value</span></span><span class="koboSpan" id="kobo.33.1"> </span><span><span class="koboSpan" id="kobo.34.1">associated with the original class label of the corresponding input data,</span></span> <span class="koboSpan" id="kobo.35.1"><img class="fm-editor-equation" src="assets/caa3cac9-acc6-44e0-9e39-83ce74d8ad8b.png" style="width:1.25em;height:1.00em;"/></span><span><span class="koboSpan" id="kobo.36.1">) and the predicted value (the</span></span><span class="koboSpan" id="kobo.37.1"> </span><span class="koboSpan" id="kobo.38.1"><img class="fm-editor-equation" src="assets/f30fa8fa-3141-4c99-9118-a3fb1ef950a5.png" style="width:1.17em;height:1.17em;"/></span><span><span class="koboSpan" id="kobo.39.1">value</span></span><span class="koboSpan" id="kobo.40.1"> </span><span><span class="koboSpan" id="kobo.41.1">estimated by the Perceptron)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.42.1">In practice, the individual weights are updated according to the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.43.1"><img class="fm-editor-equation" src="assets/2ffa93f5-2b2a-48d6-a6ef-bc9dda3c258d.png" style="width:9.50em;height:1.42em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.44.1">Here, the</span></span><span class="koboSpan" id="kobo.45.1"> </span><span class="koboSpan" id="kobo.46.1"><img class="fm-editor-equation" src="assets/b5c79404-c080-4be4-ae11-a978b4912c51.png" style="width:2.58em;height:1.42em;"/></span> <span><span class="koboSpan" id="kobo.47.1">value represents the deviation between the expected (</span></span><strong><em><span class="koboSpan" id="kobo.48.1">y</span></em></strong><span><span class="koboSpan" id="kobo.49.1">) value and the predicted value (</span><span class="koboSpan" id="kobo.50.1"><img class="fm-editor-equation" src="assets/70392b2e-fb5a-4bd5-b2fb-f404b267f785.png" style="width:1.17em;height:1.17em;"/></span></span><span><span class="koboSpan" id="kobo.51.1">):</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.52.1"><img class="fm-editor-equation" src="assets/53423643-515b-4686-b32f-bb24a7e4bae1.png" style="width:11.58em;height:1.67em;"/></span></p>
<p><span class="koboSpan" id="kobo.53.1">As is evident from the preceding formula, the deviation between the expected </span><strong><em><span class="koboSpan" id="kobo.54.1">y</span></em></strong><span class="koboSpan" id="kobo.55.1"> value and predicted </span><span class="koboSpan" id="kobo.56.1"><img class="fm-editor-equation" src="assets/3a2ca1aa-1975-4a19-bbfe-ac56a659697f.png" style="width:1.17em;height:1.17em;"/></span> <span><span class="koboSpan" id="kobo.57.1">value</span></span><span class="koboSpan" id="kobo.58.1"> is multiplied by the value of input </span><span class="koboSpan" id="kobo.59.1"><img class="fm-editor-equation" src="assets/d338e333-09e9-49cb-a2d2-824e58873414.png" style="width:1.25em;height:1.00em;"/></span><span class="koboSpan" id="kobo.60.1">, and by the </span><span class="koboSpan" id="kobo.61.1"><img class="fm-editor-equation" src="assets/15dc2b1d-f3df-4f4d-a0ac-cd4218c3e8c7.png" style="width:0.83em;height:1.25em;"/></span><span class="koboSpan" id="kobo.62.1"> constant, which represents the learning rate assigned to the Perceptron. </span><span class="koboSpan" id="kobo.62.2">The </span><span class="koboSpan" id="kobo.63.1"><img class="fm-editor-equation" src="assets/c2622b38-1fd9-402e-afc9-12e4c79d7e84.png" style="width:0.83em;height:1.25em;"/></span><span class="koboSpan" id="kobo.64.1"> constant usually assumes a value between </span><em><span class="koboSpan" id="kobo.65.1">0.0</span></em><span class="koboSpan" id="kobo.66.1"> and </span><em><span class="koboSpan" id="kobo.67.1">1.0</span></em><span class="koboSpan" id="kobo.68.1">, a value that is assigned at the Perceptron initialization phase.</span></p>
<p><span class="koboSpan" id="kobo.69.1">As we will see, the value of the learning rate is crucial for the learning of the Perceptron, and it is therefore necessary to carefully evaluate (even by trial and error) the value to be attributed to the </span><span class="koboSpan" id="kobo.70.1"><img class="fm-editor-equation" src="assets/64f04147-11a8-43bf-9f96-79cff0f45531.png" style="width:0.83em;height:1.25em;"/></span><span class="koboSpan" id="kobo.71.1"> constant to optimize the results returned from the Perceptron.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">A simple Perceptron-based spam filter</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We will now see a concrete example of the use of the Perceptron. </span><span class="koboSpan" id="kobo.2.2">We will use the </span><kbd><span class="koboSpan" id="kobo.3.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.4.1"> library to create a simple spam filter based on the Perceptron. </span><span class="koboSpan" id="kobo.4.2">The dataset we will use to test our spam filter is based on the sms spam messages collection, available at </span><a href="https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"><span class="koboSpan" id="kobo.5.1">https://archive.ics.uci.edu/ml/datasets/sms+spam+collection</span></a></p>
<p><span class="koboSpan" id="kobo.6.1">The original dataset can be downloaded in CSV format; we proceeded to process the data contained in the CSV file, transforming it into numerical values to make it manageable by the Perceptron. </span><span class="koboSpan" id="kobo.6.2">Moreover, we have selected only the messages containing the buy and sex keywords (according to our previous description), counting for each message (be it spam or ham) the number of occurrences of the keywords present in the text of the message.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The result of our preprocessing is available in the </span><kbd><span class="koboSpan" id="kobo.8.1">sms_spam_perceptron.csv</span></kbd><span class="koboSpan" id="kobo.9.1"> file (attached to the source code repository that comes with this book).</span></p>
<p><span class="koboSpan" id="kobo.10.1">Then proceed with the loading of data from the </span><kbd><span class="koboSpan" id="kobo.11.1">sms_spam_perceptron.csv</span></kbd><span class="koboSpan" id="kobo.12.1"> file, through the </span><kbd><span class="koboSpan" id="kobo.13.1">pandas</span></kbd><span class="koboSpan" id="kobo.14.1"> library, extracting from the </span><kbd><span class="koboSpan" id="kobo.15.1">DataFrame</span></kbd><span class="koboSpan" id="kobo.16.1"> of pandas the respective values, referenced through the </span><kbd><span class="koboSpan" id="kobo.17.1">iloc()</span></kbd><span class="koboSpan" id="kobo.18.1"> method:</span></p>
<pre><span class="koboSpan" id="kobo.19.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.20.1">import numpy as np</span><br/><br/><span class="koboSpan" id="kobo.21.1">df = pd.read_csv('../datasets/sms_spam_perceptron.csv')</span><br/><span class="koboSpan" id="kobo.22.1">y = df.iloc[:, 0].values</span><br/><span class="koboSpan" id="kobo.23.1">y = np.where(y == 'spam', -1, 1)</span><br/><span class="koboSpan" id="kobo.24.1">X = df.iloc[:, [1, 2]].values</span></pre>
<p><span class="koboSpan" id="kobo.25.1">We have, therefore, assigned the class labels </span><kbd><span class="koboSpan" id="kobo.26.1">ham</span></kbd><span class="koboSpan" id="kobo.27.1"> and </span><kbd><span class="koboSpan" id="kobo.28.1">spam</span></kbd><span class="koboSpan" id="kobo.29.1"> (present in the </span><kbd><span class="koboSpan" id="kobo.30.1">.csv</span></kbd><span class="koboSpan" id="kobo.31.1"> file in the first column of the </span><kbd><span class="koboSpan" id="kobo.32.1">DataFrame</span></kbd><span class="koboSpan" id="kobo.33.1">) to the </span><kbd><span class="koboSpan" id="kobo.34.1">y</span></kbd><span class="koboSpan" id="kobo.35.1"> variable (which represents the vector of the expected values) using the </span><kbd><span class="koboSpan" id="kobo.36.1">iloc()</span></kbd><span class="koboSpan" id="kobo.37.1"> method. </span><span class="koboSpan" id="kobo.37.2">Moreover, we have converted the previously mentioned class labels into the numerical values of </span><kbd><span class="koboSpan" id="kobo.38.1">-1</span></kbd><span class="koboSpan" id="kobo.39.1"> (in the case of spam) and </span><kbd><span class="koboSpan" id="kobo.40.1">+1</span></kbd><span class="koboSpan" id="kobo.41.1"> (in the case of ham) using the </span><kbd><span class="koboSpan" id="kobo.42.1">where()</span></kbd><span class="koboSpan" id="kobo.43.1"> method of NumPy, to allow us to manage the class labels with the Perceptron.</span></p>
<p><span class="koboSpan" id="kobo.44.1">In the same way, we assigned to the </span><kbd><span class="koboSpan" id="kobo.45.1">X</span></kbd><span class="koboSpan" id="kobo.46.1"> matrix the values corresponding to the </span><kbd><span class="koboSpan" id="kobo.47.1">sex</span></kbd><span class="koboSpan" id="kobo.48.1"> and </span><kbd><span class="koboSpan" id="kobo.49.1">buy</span></kbd><span class="koboSpan" id="kobo.50.1"> columns of the </span><kbd><span class="koboSpan" id="kobo.51.1">DataFrame</span></kbd><span class="koboSpan" id="kobo.52.1">, containing the number of occurrences corresponding to the two keywords within the message text. </span><span class="koboSpan" id="kobo.52.2">These values are also in numerical format, so it is possible to feed them to our Perceptron.</span></p>
<p><span class="koboSpan" id="kobo.53.1">Before proceeding with the creation of the Perceptron, we divide the input data between training data and test data:</span></p>
<div>
<pre><span class="koboSpan" id="kobo.54.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.55.1">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span></pre>
<p><span class="koboSpan" id="kobo.56.1">Using the </span><kbd><span class="koboSpan" id="kobo.57.1">train_test_split()</span></kbd><span class="koboSpan" id="kobo.58.1"> method applied to the </span><kbd><span class="koboSpan" id="kobo.59.1">X</span></kbd><span class="koboSpan" id="kobo.60.1"> and </span><kbd><span class="koboSpan" id="kobo.61.1">y</span></kbd><span class="koboSpan" id="kobo.62.1"> variables, we split the dataset into two subsets, assigning a percentage of 30% of the original dataset (using the parameter </span><kbd><span class="koboSpan" id="kobo.63.1">test_size = 0.3</span></kbd><span class="koboSpan" id="kobo.64.1">) to the test values, and the remaining 70% to the training values.</span></p>
<p><span class="koboSpan" id="kobo.65.1">At this point, we can define our Perceptron by instantiating the </span><kbd><span class="koboSpan" id="kobo.66.1">Perceptron</span></kbd><span class="koboSpan" id="kobo.67.1"> class of the </span><kbd><span class="koboSpan" id="kobo.68.1">sklearn.linear_model</span></kbd><span class="koboSpan" id="kobo.69.1"> package:</span></p>
<pre><span class="koboSpan" id="kobo.70.1">from sklearn.linear_model import Perceptron</span><br/><span class="koboSpan" id="kobo.71.1">p = Perceptron(max_iter=40, eta0=0.1, random_state=0)</span><br/><span class="koboSpan" id="kobo.72.1">p.fit(X_train, y_train)</span></pre></div>
<p><span class="koboSpan" id="kobo.73.1">During the initialization phase of the </span><kbd><span class="koboSpan" id="kobo.74.1">p</span></kbd><span class="koboSpan" id="kobo.75.1"> Perceptron, we assigned a maximum number of iterations equal to </span><kbd><span class="koboSpan" id="kobo.76.1">40</span></kbd><span class="koboSpan" id="kobo.77.1"> (with the </span><kbd><span class="koboSpan" id="kobo.78.1">max_iter = 40parameter</span></kbd><span class="koboSpan" id="kobo.79.1"> initialization) and a learning rate equal to </span><kbd><span class="koboSpan" id="kobo.80.1">0.1</span></kbd><span class="koboSpan" id="kobo.81.1"> (</span><kbd><span class="koboSpan" id="kobo.82.1">eta0 = 0.1</span></kbd><span class="koboSpan" id="kobo.83.1">). </span><span class="koboSpan" id="kobo.83.2">Finally, we invoked the </span><kbd><span class="koboSpan" id="kobo.84.1">fit()</span></kbd><span class="koboSpan" id="kobo.85.1"> method of the Perceptron, training the </span><kbd><span class="koboSpan" id="kobo.86.1">p</span></kbd><em><span class="koboSpan" id="kobo.87.1"> </span></em><span class="koboSpan" id="kobo.88.1">object with the training data.</span></p>
<p><span class="koboSpan" id="kobo.89.1">We can now proceed to estimate the values on the test data, invoking the </span><kbd><span class="koboSpan" id="kobo.90.1">predict()</span></kbd><span class="koboSpan" id="kobo.91.1"> method of the Perceptron:</span></p>
<pre><span class="koboSpan" id="kobo.92.1">y_pred = p.predict(X_test)</span></pre>
<p><span class="koboSpan" id="kobo.93.1">As a consequence of the training phase on the sample data (which accounts for 70% of the original dataset), the Perceptron should now be able to correctly estimate the expected values of the test data subset (equal to the remaining 30% of the original dataset).</span></p>
<p><span class="koboSpan" id="kobo.94.1">We can verify the accuracy of the estimated values returned by the Perceptron using the </span><span><kbd><span class="koboSpan" id="kobo.95.1">sklearn.metrics</span></kbd><span class="koboSpan" id="kobo.96.1"> package of </span><kbd><span class="koboSpan" id="kobo.97.1">scikit-learn</span></kbd></span><span class="koboSpan" id="kobo.98.1"> as follows:</span></p>
<pre><span class="koboSpan" id="kobo.99.1">from sklearn.metrics import accuracy_score</span><br/><span class="koboSpan" id="kobo.100.1">print('Misclassified samples: %d' % (y_test != y_pred).sum())</span><br/><span class="koboSpan" id="kobo.101.1">print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))</span><br/><span class="koboSpan" id="kobo.102.1">Misclassified samples: 3</span><br/><span class="koboSpan" id="kobo.103.1">Accuracy: 0.90</span></pre>
<p><span class="koboSpan" id="kobo.104.1">By comparing the test data (</span><kbd><span class="koboSpan" id="kobo.105.1">y_test</span></kbd><span class="koboSpan" id="kobo.106.1">) with the predicted values (</span><kbd><span class="koboSpan" id="kobo.107.1">y_pred</span></kbd><span class="koboSpan" id="kobo.108.1">), and summing up the overall number of mismatches, we are now able to evaluate the accuracy of the predictions provided by the Perceptron.</span></p>
<p><span class="koboSpan" id="kobo.109.1">In our example, the percentage of accuracy is quite good (90%), since the total number of cases of incorrect classifications amounts to only three.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Pros and cons of Perceptrons</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Despite the relative simplicity of the implementation of the Perceptron (simplicity here constitutes the strength of the algorithm, if compared to the accuracy of the predictions provided), it suffers from some important limitations. </span><span class="koboSpan" id="kobo.2.2">Being essentially a binary linear classifier, the Perceptron is able to offer accurate results only if the analyzed data can be linearly separable; that is, it is possible to identify a straight line (or a hyperplane, in case of multidimensional data) that completely bisects the data in the Cartesian plane:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img class="aligncenter size-full wp-image-635 image-border" src="assets/77014850-7866-4ff8-a0a2-2bad99558bef.png" style="width:29.67em;height:23.33em;"/></span></p>
<p><span class="koboSpan" id="kobo.4.1">I</span><span><span class="koboSpan" id="kobo.5.1">f instead (and this is so in the majority of real cases) the analyzed data was not linearly separable, the Perceptron learning algorithm would oscillate indefinitely around the data, looking for a possible vector of weights that can linearly separate the data (without, however, being able to find it):</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img src="assets/8cfba7dd-ad73-4b1a-b4e9-20d90b614b91.png" style="width:26.92em;height:21.42em;"/></span></p>
<p><span class="koboSpan" id="kobo.7.1">Therefore, the convergence of the Perceptron is only possible in the presence of linearly separable data, and in the case of a small learning rate. </span><span class="koboSpan" id="kobo.7.2">If the classes of data are not linearly separable, it is of great importance to set a maximum number of iterations (corresponding to the </span><kbd><span class="koboSpan" id="kobo.8.1">max_iter</span></kbd><span class="koboSpan" id="kobo.9.1"> parameter) in order to prevent the algorithm from oscillating indefinitely in search of an (nonexistent) optimal solution.</span></p>
<p><span class="koboSpan" id="kobo.10.1">One way to overcome the Perceptron's practical limitations is to accept a </span><strong><span class="koboSpan" id="kobo.11.1">wider margin</span></strong><span class="koboSpan" id="kobo.12.1"> of data separation between them. </span><span class="koboSpan" id="kobo.12.2">This is the strategy followed by SVMs, a topic we'll encounter in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Spam detection with SVMs</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">SVMs are an example of </span><em><span class="koboSpan" id="kobo.3.1">supervised</span></em><span class="koboSpan" id="kobo.4.1"> algorithms (as well as the Perceptron), whose task is to identify the hyperplane that best separates classes of data that can be represented in a </span><strong><span class="koboSpan" id="kobo.5.1">multidimensional space</span></strong><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">It is possible, however, to identify different hyperplanes that correctly separate the data from each other; in this case, the choice falls on the hyperplane that </span><strong><span class="koboSpan" id="kobo.7.1">optimizes the prefixed margin</span></strong><span class="koboSpan" id="kobo.8.1">, that is, the distance between the hyperplane and the data.</span></p>
<p><span class="koboSpan" id="kobo.9.1">One of the advantages of the SVM is that the identified hyperplane is </span><strong><span class="koboSpan" id="kobo.10.1">not limited</span></strong><span class="koboSpan" id="kobo.11.1"> to the linear model (unlike the Perceptron), as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.12.1"><img src="assets/57633dad-9b33-4d6a-97ed-408eda75ecae.png" style="width:42.42em;height:19.92em;"/></span></p>
<p><span class="koboSpan" id="kobo.13.1">The SVM can be considered as an extension of the Perceptron, however. </span><span class="koboSpan" id="kobo.13.2">While in the case of the Perceptron, our goal was to </span><strong><span class="koboSpan" id="kobo.14.1">minimize</span></strong><span class="koboSpan" id="kobo.15.1"> classification errors, in the case of SVM, our goal instead is to </span><strong><span class="koboSpan" id="kobo.16.1">maximize</span></strong><span class="koboSpan" id="kobo.17.1"> the margin, that is, the </span><strong><span class="koboSpan" id="kobo.18.1">distance</span></strong><span class="koboSpan" id="kobo.19.1"> between the hyperplane and the training data </span><em><span class="koboSpan" id="kobo.20.1">closest</span></em><span class="koboSpan" id="kobo.21.1"> to the hyperplane (the nearest training data is thus known as a </span><strong><span class="koboSpan" id="kobo.22.1">support vector</span></strong><span class="koboSpan" id="kobo.23.1">).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">SVM optimization strategy</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Why choose the hyperplane that maximizes the margin in the first place? </span><span class="koboSpan" id="kobo.2.2">The reason lies in the fact that </span><strong><span class="koboSpan" id="kobo.3.1">wider margins</span></strong><span class="koboSpan" id="kobo.4.1"> correspond to fewer classification errors, while with </span><strong><span class="koboSpan" id="kobo.5.1">narrower margins</span></strong><span class="koboSpan" id="kobo.6.1"> we risk incurring the phenomenon known as </span><strong><span class="koboSpan" id="kobo.7.1">overfitting</span></strong><span class="koboSpan" id="kobo.8.1"> (a real disaster that we may incur when dealing with </span><em><span class="koboSpan" id="kobo.9.1">iterative</span></em><span class="koboSpan" id="kobo.10.1"> algorithms, as we will see when we will discuss verification and optimization strategies for our AI solutions).</span></p>
<p><span class="koboSpan" id="kobo.11.1">We can translate the SVM optimization strategy in mathematical terms, similar to what we have done in the case of the Perceptron (which remains our starting point). </span><span class="koboSpan" id="kobo.11.2">We define the condition that must be met to assure that the SVM correctly identifies the best hyperplane that separates the classes of data:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.12.1"><img class="fm-editor-equation" src="assets/ef752721-9f69-4f15-a213-e9a48c28e73f.png" style="width:12.83em;height:2.17em;"/></span></p>
<p><span class="koboSpan" id="kobo.13.1">Here, the </span><em><span class="koboSpan" id="kobo.14.1">β</span></em><span class="koboSpan" id="kobo.15.1"> constant represents the </span><em><span class="koboSpan" id="kobo.16.1">bias</span></em><span class="koboSpan" id="kobo.17.1">, while </span><em><span class="koboSpan" id="kobo.18.1">µ</span></em><span class="koboSpan" id="kobo.19.1"> represents our </span><em><span class="koboSpan" id="kobo.20.1">margin</span></em><span class="koboSpan" id="kobo.21.1"> (which assumes the maximum possible positive value in order to obtain the best separation between the classes of values).</span></p>
<p><span class="koboSpan" id="kobo.22.1">In practice, to the algebraic multiplication (represented by </span><span class="koboSpan" id="kobo.23.1"><img class="fm-editor-equation" src="assets/d31c5f8b-b344-4e90-b3dd-54e4eeece6a4.png" style="width:5.00em;height:2.17em;"/></span><span class="koboSpan" id="kobo.24.1">) we add the value of the </span><em><span class="koboSpan" id="kobo.25.1">β</span></em><span class="koboSpan" id="kobo.26.1"> bias, which allows us to obtain a value greater than or equal to zero, in the presence of values ​​that fall in the same </span><strong><span class="koboSpan" id="kobo.27.1">class label</span></strong><span class="koboSpan" id="kobo.28.1"> (remember that </span><em><span class="koboSpan" id="kobo.29.1">y</span></em><span class="koboSpan" id="kobo.30.1"> can only assume the values of ​​</span><kbd><span class="koboSpan" id="kobo.31.1">-1</span></kbd><span class="koboSpan" id="kobo.32.1"> or </span><kbd><span class="koboSpan" id="kobo.33.1">+1</span></kbd><span class="koboSpan" id="kobo.34.1"> to distinguish between the corresponding classes to which the samples belong, as we have already seen in the case of the Perceptron).</span></p>
<p><span class="koboSpan" id="kobo.35.1">At this point, the value calculated in this way is compared with the </span><span class="koboSpan" id="kobo.36.1"><img class="fm-editor-equation" src="assets/0b961c0e-f1c9-427f-82ce-c80f17a8b870.png" style="width:0.67em;height:1.75em;"/></span><span class="koboSpan" id="kobo.37.1"> </span><span><span class="koboSpan" id="kobo.38.1">margin </span></span><span class="koboSpan" id="kobo.39.1">in order to ensure that the distance between each sample and the separating hyperplane we identified (thus constituting our decision boundary) is greater or at most equal to our margin (which, as we have seen, is identified as the maximum possible positive value, in order to obtain the </span><em><span class="koboSpan" id="kobo.40.1">best</span></em><span class="koboSpan" id="kobo.41.1"> separation between the classes of values).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">SVM spam filter example</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's go back to our sample spam filter, and replace the Perceptron with an SVM, as we have seen in the identification of the hyperplane that we are not limited to using linear classifier models only (being able to choose between classifiers characterized by greater complexity).</span></p>
<p><span class="koboSpan" id="kobo.3.1">However, to compare the results obtained previously with the Perceptron, which represents a strictly linear classifier, we will also choose a linear classifier in the case of the SVM.</span></p>
<p><span class="koboSpan" id="kobo.4.1">This time, however, our dataset (stored in the </span><kbd><span class="koboSpan" id="kobo.5.1">sms_spam_svm.csv</span></kbd><span class="koboSpan" id="kobo.6.1"> file, and derived from the collection of SMS spam messages we found earlier in the chapter, in which the total occurrences of the various suspicious keywords were extracted and compared to the total number of harmless words appearing within the messages) is not strictly linearly separable.</span></p>
<p><span class="koboSpan" id="kobo.7.1">In the same way as in the case of the Perceptron, we will proceed to load the data with </span><kbd><span class="koboSpan" id="kobo.8.1">pandas</span></kbd><span class="koboSpan" id="kobo.9.1">, associating the class labels with the corresponding ​​</span><kbd><span class="koboSpan" id="kobo.10.1">-1</span></kbd><span class="koboSpan" id="kobo.11.1"> values (in the case of spam) and </span><kbd><span class="koboSpan" id="kobo.12.1">1</span></kbd><span class="koboSpan" id="kobo.13.1"> (in the case of ham):</span></p>
<pre><span class="koboSpan" id="kobo.14.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.15.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.16.1">df = pd.read_csv('../datasets/sms_spam_svm.csv')</span><br/><span class="koboSpan" id="kobo.17.1">y = df.iloc[:, 0].values</span><br/><span class="koboSpan" id="kobo.18.1">y = np.where(y == 'spam', -1, 1)</span></pre>
<p><span><span class="koboSpan" id="kobo.19.1">Once the data has been loaded, we proceed to split the original dataset into 30% test data and 70% training data:</span></span></p>
<pre><span class="koboSpan" id="kobo.20.1">from sklearn.model_selection import train_test_split</span><br/><br/><span class="koboSpan" id="kobo.21.1">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span></pre>
<p><span><span class="koboSpan" id="kobo.22.1">At this point, we can thus proceed to instantiate our SVM, importing the</span></span> <kbd><span class="koboSpan" id="kobo.23.1">SVC</span></kbd> <span><span class="koboSpan" id="kobo.24.1">class (which stands for </span><strong><span class="koboSpan" id="kobo.25.1">support vector classifier</span></strong><span class="koboSpan" id="kobo.26.1">) from the</span></span><span class="koboSpan" id="kobo.27.1"> </span><kbd><span class="koboSpan" id="kobo.28.1">sklearn.svm</span></kbd><span class="koboSpan" id="kobo.29.1"> </span><span><span class="koboSpan" id="kobo.30.1">package, choosing th</span></span><span class="koboSpan" id="kobo.31.1">e linear classifier </span><span><span class="koboSpan" id="kobo.32.1">(</span><kbd><span class="koboSpan" id="kobo.33.1">kernel = 'linear'</span></kbd><span class="koboSpan" id="kobo.34.1">), then proceeding to the model training by invoking the</span></span> <kbd><span class="koboSpan" id="kobo.35.1">fit()</span></kbd> <span><span class="koboSpan" id="kobo.36.1">method, and finally estimating the test data by invoking the</span></span> <kbd><span class="koboSpan" id="kobo.37.1">predict()</span></kbd> <span><span class="koboSpan" id="kobo.38.1">method:</span></span></p>
<pre><span class="koboSpan" id="kobo.39.1">from sklearn.svm import SVC</span><br/><br/><span class="koboSpan" id="kobo.40.1">svm = SVC(kernel='linear', C=1.0, random_state=0)</span><br/><span class="koboSpan" id="kobo.41.1">svm.fit(X_train, y_train)</span><br/><span class="koboSpan" id="kobo.42.1">y_pred = svm.predict(X_test)</span></pre>
<p><span><span class="koboSpan" id="kobo.43.1">We can now evaluate the accuracy of the predictions returned by the SVM algorithm, making use of the</span></span> <kbd><span class="koboSpan" id="kobo.44.1">sklearn.metrics</span></kbd> <span><span class="koboSpan" id="kobo.45.1">package as we did with the Perceptron:</span></span></p>
<pre><span class="koboSpan" id="kobo.46.1">from sklearn.metrics import accuracy_score</span><br/><br/><span class="koboSpan" id="kobo.47.1">print('Misclassified samples: %d' % (y_test != y_pred).sum())</span><br/><span class="koboSpan" id="kobo.48.1">print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))</span><br/><span class="koboSpan" id="kobo.49.1">Misclassified samples: 7</span><br/><span class="koboSpan" id="kobo.50.1">Accuracy: 0.84</span></pre>
<p><span><span class="koboSpan" id="kobo.51.1">Even in the presence of non-linearly separable data, we see how well the SVM algorithm behaves, since the level of accuracy of the predictions accounts to 84%, with the number of incorrect classifications accounting to only 7 cases.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Image spam detection with SVMs</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The versatility of the SVM algorithm allows us to deal with even more complex real-world classification cases, such as in the case of spam messages represented by images, instead of simple text.</span></p>
<p><span class="koboSpan" id="kobo.3.1">As we have seen, spammers are well aware of our detection attempts, and therefore try to adopt all possible solutions to deceive our filters. </span><span class="koboSpan" id="kobo.3.2">One of the evasion strategies is to use images as a vehicle for spreading spam, instead of simple text.</span></p>
<p><span class="koboSpan" id="kobo.4.1">For some time, however, viable image-based spam detection solutions have been available. </span><span class="koboSpan" id="kobo.4.2">Among these, we can distinguish detection strategies based on the following:</span></p>
<ul>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.5.1">Content-based filtering</span></strong><span><span class="koboSpan" id="kobo.6.1">: The approach consists of trying to identify the suspect keywords that are most commonly used in textual spam messages even within images; to this end, pattern recognition techniques leveraging optical character recognition</span></span><span class="koboSpan" id="kobo.7.1"> (</span><span><strong><span class="koboSpan" id="kobo.8.1">OCR</span></strong><span class="koboSpan" id="kobo.9.1">) technology are implemented in order to extract text from images (this is the solution that</span></span><span class="koboSpan" id="kobo.10.1"> SpamAssassin </span><span><span class="koboSpan" id="kobo.11.1">adopts).</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.12.1">Non content-based filtering</span></strong><span><span class="koboSpan" id="kobo.13.1">: In this case, we try to identify specific features of spam images (such as color features and so on), on the grounds that spam images, being computer-generated, show different characteristics compared to natural images; for the extraction of the features, we make use of advanced recognition techniques based on NNs and </span><strong><span class="koboSpan" id="kobo.14.1">deep learning</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><strong><span class="koboSpan" id="kobo.16.1">DL</span></strong><span class="koboSpan" id="kobo.17.1">).</span></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How did SVM come into existence?</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Once the salient features of the images have been extracted, and the corresponding samples have been classified within their respective classes (spam or ham), it is possible to exploit an SVM to perform model training on these features.</span></p>
<p><span class="koboSpan" id="kobo.3.1">One of the most recent projects on this subject is </span><em><span class="koboSpan" id="kobo.4.1">Image Spam Analysis</span></em><span class="koboSpan" id="kobo.5.1"> by Annapurna Sowmya </span><span><span class="koboSpan" id="kobo.6.1">Annadatha</span></span><span class="koboSpan" id="kobo.7.1"> (</span><a href="http://scholarworks.sjsu.edu/etd_projects/486"><span class="koboSpan" id="kobo.8.1">http://scholarworks.sjsu.edu/etd_projects/486</span></a><span class="koboSpan" id="kobo.9.1">), which is characterized by the innovative approach adopted, based on the assumption that the features that characterize a spam image, being computer generated, are different to those associated with an image generated by a camera; and the selective use of SVM, which leads to high accuracy of results compared to a reduced cost in computational terms.</span></p>
<p><span class="koboSpan" id="kobo.10.1">The approach consists of the following steps:</span></p>
<ol>
<li class="mce-root"><span><span class="koboSpan" id="kobo.11.1">Train the classifier using the linear SVM and the feature set</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.12.1">Compute the SVM weights for all the features</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.13.1">Select the first one with the largest weights</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.14.1">Create a model based on the subset</span></li>
</ol>
<p><span class="koboSpan" id="kobo.15.1">For further information, refer to the project reference mentioned in the previous paragraph.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Phishing detection with logistic regression and decision trees</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">After having analyzed the Perceptron and the SVM, we now deal with alternative classification strategies that make use of logistic regression and decision trees.</span></p>
<p><span class="koboSpan" id="kobo.3.1">But before continuing, we will discover the distinctive features of these algorithms and their use for spam detection and phishing, starting with regression models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Regression models</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Regression models are undoubtedly the most used of all learning algorithms. </span><span class="koboSpan" id="kobo.2.2">Developed from statistical analysis, regression models have quickly spread in ML and in AI in general. </span><span class="koboSpan" id="kobo.2.3">The most known and used regression model is linear regression, thanks to the simplicity of its implementation and the good predictive capacity that it allows us to achieve in many practical cases (such as estimating the level of house prices in relation to changes in interest rates).</span></p>
<p><span class="koboSpan" id="kobo.3.1">Alongside the linear model, there is also the logistic regression model, especially useful in the most complex cases, where the linear model proves to be too rigid for the data to be treated. </span><span class="koboSpan" id="kobo.3.2">Both models, therefore, represent the tools of choice for analysts and algorithm developers.</span></p>
<p><span class="koboSpan" id="kobo.4.1">In the next section, we will analyze the characteristics and advantages of regression models, and their possible uses in the field of spam detection. </span><span class="koboSpan" id="kobo.4.2">Let's start our analysis with the simplest model, the linear regression model, which will help us make comparisons with the logistic regression model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introducing linear regression models</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The linear regression model is characterized by the fact that the data is represented as sums of features, leading to a straight line in the Cartesian plane.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In formal terms, linear regression can be described by the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.4.1"><img class="fm-editor-equation" src="assets/61f944ad-cec4-4a1f-8dbd-4f4c901079b9.png" style="width:5.33em;height:1.08em;"/></span></p>
<p><span class="koboSpan" id="kobo.5.1">Here, </span><em><span class="koboSpan" id="kobo.6.1">y</span></em><span class="koboSpan" id="kobo.7.1"> represents the predicted values, which are the result of the linear combination of the single features (represented by the </span><em><span class="koboSpan" id="kobo.8.1">X</span></em><span class="koboSpan" id="kobo.9.1"> matrix) to which a weight vector is applied (represented by the </span><em><span class="koboSpan" id="kobo.10.1">w</span><span><span class="koboSpan" id="kobo.11.1"> vector</span></span></em><span class="koboSpan" id="kobo.12.1">), and by the addition of a constant (</span><em><span class="koboSpan" id="kobo.13.1">β</span></em><span class="koboSpan" id="kobo.14.1">), which represents the default predicted value when all features assume the value of zero (or simply are missing).</span></p>
<p><span class="koboSpan" id="kobo.15.1">The </span><em><span class="koboSpan" id="kobo.16.1">β</span></em><span class="koboSpan" id="kobo.17.1"> constant can also be interpreted as the systematic distortion of the model, and corresponds graphically with the intercept value on the vertical axis of the Cartesian plane (that is to say, the point where the regression line meets the vertical axis).</span></p>
<p><span class="koboSpan" id="kobo.18.1">Obviously, the linear model can be extended to cases in which there is more than just one feature. </span><span class="koboSpan" id="kobo.18.2">In this case, the mathematical formalization assumes the following aspect:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.19.1"><img class="fm-editor-equation" src="assets/1707e517-6e40-432d-8e07-1b998b2aa02d.png" style="width:5.67em;height:1.17em;"/></span></p>
<p><span class="koboSpan" id="kobo.20.1">The geometric representation of the previous formula will correspond to a hyperplane in the </span><em><span class="koboSpan" id="kobo.21.1">n</span></em><span class="koboSpan" id="kobo.22.1">-dimensional space, rather than a straight line in the Cartesian plane. </span><span class="koboSpan" id="kobo.22.2">We have mentioned the importance of the </span><span class="koboSpan" id="kobo.23.1"><img class="fm-editor-equation" src="assets/ee6a30be-f904-4e97-8f92-ca8517a68fa8.png" style="width:0.58em;height:1.00em;"/></span><span class="koboSpan" id="kobo.24.1"> constant as the default predictive value of the model in the case in which the features assume a value equal to zero.</span></p>
<p><span class="koboSpan" id="kobo.25.1">The individual ​​</span><span class="koboSpan" id="kobo.26.1"><img class="fm-editor-equation" src="assets/6dffb83c-d6d6-4145-9f31-e6ec4acba0a6.png" style="width:1.50em;height:1.08em;"/></span> <span><span class="koboSpan" id="kobo.27.1">values </span></span><span class="koboSpan" id="kobo.28.1">within the vector of the weights, </span><span class="koboSpan" id="kobo.29.1"><img class="fm-editor-equation" src="assets/aa889ef4-0ff3-4e3f-9b5b-05c300af8b09.png" style="width:0.83em;height:0.67em;"/></span><span class="koboSpan" id="kobo.30.1">, can be interpreted as a measure of the intensity of the corresponding features, </span><span class="koboSpan" id="kobo.31.1"><img class="fm-editor-equation" src="assets/1cd520fc-8894-40a4-b160-ca820d1ac691.png" style="width:1.25em;height:1.00em;"/></span><sub><span class="koboSpan" id="kobo.32.1">.</span></sub></p>
<p><span class="koboSpan" id="kobo.33.1">In practice, if the value of the </span><span class="koboSpan" id="kobo.34.1"><img class="fm-editor-equation" src="assets/5c0cd86a-07c6-4abd-b14a-4a395f6a092f.png" style="width:1.50em;height:1.08em;"/></span> <span><span class="koboSpan" id="kobo.35.1">weight</span></span><span class="koboSpan" id="kobo.36.1"> is close to zero, the corresponding </span><span class="koboSpan" id="kobo.37.1"><img class="fm-editor-equation" src="assets/37084f2a-9bfa-4559-b9b5-38177aebd8c1.png" style="width:1.25em;height:1.00em;"/></span> <span><span class="koboSpan" id="kobo.38.1">f</span></span><span class="koboSpan" id="kobo.39.1">eature assumes a minimum importance (or none at all) in the determination of predicted values. </span><span class="koboSpan" id="kobo.39.2">If, instead, the </span><span class="koboSpan" id="kobo.40.1"><img class="fm-editor-equation" src="assets/d5561866-a060-4c5f-984e-3f1a36a87a7b.png" style="width:1.50em;height:1.08em;"/></span> <span><span class="koboSpan" id="kobo.41.1">weight </span></span><span class="koboSpan" id="kobo.42.1">assumes positive values, it will amplify the final value returned by the regression model.</span></p>
<p><span class="koboSpan" id="kobo.43.1">If, on the other hand, </span><span class="koboSpan" id="kobo.44.1"><img class="fm-editor-equation" src="assets/89ba77ce-4e92-4de7-a30e-42d0f1230e70.png" style="width:1.50em;height:1.08em;"/></span><span class="koboSpan" id="kobo.45.1"> assumes negative values, it will help to reverse the direction of the model's predictions, as the value of the </span><em><span class="koboSpan" id="kobo.46.1"><img class="fm-editor-equation" src="assets/f4aae63f-6229-4f1f-a898-807f7044571d.png" style="width:1.08em;height:0.92em;"/></span></em><span class="koboSpan" id="kobo.47.1"> </span><span><span class="koboSpan" id="kobo.48.1">feature </span></span><span class="koboSpan" id="kobo.49.1">increases, it will correspond to a decrease in the value estimated by the regression. </span><span class="koboSpan" id="kobo.49.2">Hence, it is important to consider the impacts of the weights on the </span><em><span class="koboSpan" id="kobo.50.1"><img class="fm-editor-equation" src="assets/9c016c68-d6c7-4054-9d11-5616f3db9da9.png" style="width:1.08em;height:0.92em;"/></span></em><span class="koboSpan" id="kobo.51.1"> </span><span><span class="koboSpan" id="kobo.52.1">features, </span></span><span class="koboSpan" id="kobo.53.1">as they are determinant in the correctness of the predictions that we can derive from the regression model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Linear regression with scikit-learn</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">In the following code snippet, we will see how to implement a simple predictive model based on linear regression, using the </span><kbd><span class="koboSpan" id="kobo.3.1">linear_model</span></kbd><span class="koboSpan" id="kobo.4.1"> module of </span><kbd><span class="koboSpan" id="kobo.5.1">scikit-learn</span></kbd><span class="koboSpan" id="kobo.6.1">, which we will feed with one of the previously used spam message datasets:</span></span></p>
<pre><span class="koboSpan" id="kobo.7.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.8.1">import numpy as np</span><br/><br/><span class="koboSpan" id="kobo.9.1">df = pd.read_csv('../datasets/sms_spam_perceptron.csv')</span><br/><span class="koboSpan" id="kobo.10.1">X = df.iloc[:, [1, 2]].values</span><br/><span class="koboSpan" id="kobo.11.1">y = df.iloc[:, 0].values</span><br/><span class="koboSpan" id="kobo.12.1">y = np.where(y == 'spam', -1, 1)</span><br/><br/><span class="koboSpan" id="kobo.13.1">from sklearn.linear_model import LinearRegression</span><br/><br/><span class="koboSpan" id="kobo.14.1">linear_regression = LinearRegression()</span><br/><span class="koboSpan" id="kobo.15.1">linear_regression.fit(X,y)</span><br/><span class="koboSpan" id="kobo.16.1">print (linear_regression.score(X,y))</span></pre>
<p><span class="koboSpan" id="kobo.17.1">To verify the accuracy of the predictions provided by the linear regression model, we can use the </span><kbd><span class="koboSpan" id="kobo.18.1">score()</span></kbd><span class="koboSpan" id="kobo.19.1"> method, which gives us the measurement of the coefficient of the R</span><sup><span class="koboSpan" id="kobo.20.1">2</span></sup><span class="koboSpan" id="kobo.21.1"> determination.</span></p>
<p><span class="koboSpan" id="kobo.22.1">This coefficient varies between </span><kbd><span class="koboSpan" id="kobo.23.1">0</span></kbd><span class="koboSpan" id="kobo.24.1"> and </span><kbd><span class="koboSpan" id="kobo.25.1">1</span></kbd><span class="koboSpan" id="kobo.26.1">, and measures how much better the predictions returned by the linear model are, when compared to the simple mean.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Linear regression – pros and cons</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As we have seen, the simplicity of implementation represents an undoubted advantage of the linear regression model. </span><span class="koboSpan" id="kobo.2.2">However, the limitations of the model are rather important.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In fact, the linear regression model can only be used to manage quantitative data, whereas in the case where the predictive analysis used categorical data, we have to resort to the logistic regression model. </span><span class="koboSpan" id="kobo.3.2">Furthermore, the major limitation of linear regression is that the model assumes that features are mostly unrelated; that is, they do not influence each other. </span><span class="koboSpan" id="kobo.3.3">This assumption legitimizes the representation of the products between the features and their respective weights as sums of independent terms.</span></p>
<p><span class="koboSpan" id="kobo.4.1">There are, however, real cases in which this assumption is unrealistic (for example, the possible relationship between variables such as the age and the weight of a person, which are related to each other, as weight varies according to age). </span><span class="koboSpan" id="kobo.4.2">The negative side effect of this assumption consists in the fact that we risk adding the same information several times, failing to correctly predict the effect of the combination of the variables on the final result.</span></p>
<p><span class="koboSpan" id="kobo.5.1">In technical terms, the linear regression model is characterized by a greater bias in the predictions, instead of greater variance (we will have the opportunity to face the trade-off between bias and variance later on).</span></p>
<p><span class="koboSpan" id="kobo.6.1">In other words, when the data being analyzed exhibits complex relationships, the linear regression model leads us to systematically distorted predictions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Logistic regression</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have seen that one of the limits of linear regression is that it cannot be used to solve classification problems:</span></p>
<p><span class="koboSpan" id="kobo.3.1">In fact, in case we wanted to use linear regression to classify the samples within two classes (as is the case in spam detection) whose labels are represented by numerical values ​​(for example, </span><kbd><span class="koboSpan" id="kobo.4.1">-1</span></kbd><span class="koboSpan" id="kobo.5.1"> for </span><strong><span class="koboSpan" id="kobo.6.1">spam</span></strong><span class="koboSpan" id="kobo.7.1">, and </span><kbd><span class="koboSpan" id="kobo.8.1">+1</span></kbd><span class="koboSpan" id="kobo.9.1"> for </span><strong><span class="koboSpan" id="kobo.10.1">ham</span></strong><span class="koboSpan" id="kobo.11.1">), the linear regression model will try to identify the result that is closest to the target value (that is, linear regression has the purpose of minimizing forecasting errors). </span><span class="koboSpan" id="kobo.11.2">The negative side effect of this behavior is that it leads to greater classification errors. </span><span class="koboSpan" id="kobo.11.3">With respect to the Perceptron, linear regression does not give us good results in terms of classification accuracy, precisely because linear regression works better with continuous intervals of values, rather than with classes of discrete values ​​(as is the case in classification).</span></p>
<p><span class="koboSpan" id="kobo.12.1">An alternative strategy, most useful for the purposes of classification, consists of estimating the probability of the samples belonging to individual classes. </span><span class="koboSpan" id="kobo.12.2">This is the strategy adopted by logistic regression (which, in spite of the name, constitutes a classification algorithm, rather than a regression model).</span></p>
<p><span class="koboSpan" id="kobo.13.1">The mathematical formulation of logistic regression is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.14.1"><img class="fm-editor-equation" src="assets/a127c2e4-f02b-4988-abb2-688c11c6e204.png" style="width:13.50em;height:3.42em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.15.1">Here, </span><span class="koboSpan" id="kobo.16.1"><img class="fm-editor-equation" src="assets/dc3f4c7d-604d-4827-977a-748627caf92e.png" style="width:5.50em;height:1.58em;"/></span></span><span><span class="koboSpan" id="kobo.17.1">. </span><span class="koboSpan" id="kobo.18.1"><img class="fm-editor-equation" src="assets/2760b21d-2534-48ee-bdb2-9e6f36a3a9d7.png" style="width:6.42em;height:1.67em;"/></span></span> <span><span class="koboSpan" id="kobo.19.1">therefore measures the conditional probability that a given sample falls into the </span><span class="koboSpan" id="kobo.20.1"><img class="fm-editor-equation" src="assets/00e35927-6dee-4570-ab02-1fd300f2007b.png" style="width:0.50em;height:0.67em;"/></span><span class="koboSpan" id="kobo.21.1"> class, given the </span><span class="koboSpan" id="kobo.22.1"><img class="fm-editor-equation" src="assets/28a16344-441e-46fb-9f56-43417e767bb3.png" style="width:1.25em;height:1.00em;"/></span><span class="koboSpan" id="kobo.23.1"> features.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">A phishing detector with logistic regression</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We can then use logistic regression to implement a phishing detector, exploiting the fact that logistic regression is particularly useful for solving classification problems. </span><span class="koboSpan" id="kobo.2.2">Like spam detection, phishing detection is nothing more than a sample classification task.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In our example, we will use the dataset available on the UCI machine learning repository website (</span><a href="https://archive.ics.uci.edu/ml/datasets/Phishing+Websites"><span class="koboSpan" id="kobo.4.1">https://archive.ics.uci.edu/ml/datasets/Phishing+Websites</span></a><span class="koboSpan" id="kobo.5.1">).</span></p>
<p><span class="koboSpan" id="kobo.6.1">The dataset has been converted into CSV format starting from the original </span><kbd><span class="koboSpan" id="kobo.7.1">.arff</span></kbd><span class="koboSpan" id="kobo.8.1"> format, using the data wrangling technique known as </span><strong><span class="koboSpan" id="kobo.9.1">one-hot encoding</span></strong><span class="koboSpan" id="kobo.10.1"> (</span><a href="https://en.wikipedia.org/wiki/One-hot"><span class="koboSpan" id="kobo.11.1">https://en.wikipedia.org/wiki/One-hot</span></a><span class="koboSpan" id="kobo.12.1">), and consists of records containing 30 features that characterize phishing websites.</span></p>
<p><span class="koboSpan" id="kobo.13.1">Find the source code of our detector in the following code block:</span></p>
<pre><span class="koboSpan" id="kobo.14.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.15.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.16.1">from sklearn import *</span><br/><span class="koboSpan" id="kobo.17.1">from sklearn.linear_model import LogisticRegression</span><br/><span class="koboSpan" id="kobo.18.1">from sklearn.metrics import accuracy_score</span><br/><br/><span class="koboSpan" id="kobo.19.1">phishing_dataset = np.genfromtxt('../datasets/phishing_dataset.csv',</span><br/><span class="koboSpan" id="kobo.20.1">delimiter=',', dtype=np.int32)</span><br/><br/><span class="koboSpan" id="kobo.21.1">samples = phishing_dataset[:,:-1]</span><br/><br/><span class="koboSpan" id="kobo.22.1">targets = phishing_dataset[:, -1]</span><br/><br/><span class="koboSpan" id="kobo.23.1">from sklearn.model_selection import train_test_split</span><br/><br/><span class="koboSpan" id="kobo.24.1">training_samples, testing_samples, training_targets, testing_targets = train_test_split(samples, targets, test_size=0.2, random_state=0)</span><br/><br/><span class="koboSpan" id="kobo.25.1">log_classifier = LogisticRegression()</span><br/><br/><span class="koboSpan" id="kobo.26.1">log_classifier.fit(training_samples, training_targets)</span><br/><br/><span class="koboSpan" id="kobo.27.1">predictions = log_classifier.predict(testing_samples)</span><br/><span class="koboSpan" id="kobo.28.1">accuracy = 100.0 * accuracy_score(testing_targets, predictions)</span><br/><br/><span class="koboSpan" id="kobo.29.1">print ("Logistic Regression accuracy: " + str(accuracy))</span><br/><br/><span class="koboSpan" id="kobo.30.1">Logistic Regression accuracy: 91.72320217096338</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.31.1">As we can see, the level of accuracy of the logistic regression classifier is quite good, as the model is able to correctly detect over 90% of URLs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Logistic regression pros and cons</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The advantages of adopting logistic regression can be summarized as follows:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.3.1">The model can be trained very efficiently</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.4.1">It can be used effectively even in the presence of a large number of features</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.5.1">The algorithm has a high degree of scalability, due to the simplicity of its scoring function</span></li>
</ul>
<p><span class="koboSpan" id="kobo.6.1">At the same time, however, logistic regression suffers from some important limitations, deriving from the basic assumptions that characterize it, such as the need for the features to be linearly independent (a rule that translates in technical terms as the absence of multicollinearity), as well as requiring more training samples on average than other competing algorithms, as the maximum likelihood criterion adopted in logistic regression is known to be less powerful than, say, the least squares method used in linear regression to minimize prediction errors.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Making decisions with trees</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As we saw in the previous paragraphs, when we have to choose which algorithm to use to perform a given task, we must consider the type of features that characterize our data. </span><span class="koboSpan" id="kobo.2.2">The features can in fact be made up of quantitative values ​​or qualitative data.</span></p>
<p><span class="koboSpan" id="kobo.3.1">ML algorithms </span><span><span class="koboSpan" id="kobo.4.1">are </span></span><span class="koboSpan" id="kobo.5.1">obviously more at ease when dealing with quantitative values; however, most of the real cases involve the use of data expressed in a qualitative form (such as descriptions, labels, words, and so on) that imply information expressed in non-numerical form.</span></p>
<p><span class="koboSpan" id="kobo.6.1">As in the case of spam detection, we have seen how the translation in numerical form (a practice known as </span><strong><span class="koboSpan" id="kobo.7.1">numeric encoding</span></strong><span class="koboSpan" id="kobo.8.1">) of qualitative features (such as the spam and ham labels, to which we assigned the numerical values of ​​</span><kbd><span class="koboSpan" id="kobo.9.1">-1</span></kbd><span class="koboSpan" id="kobo.10.1"> and </span><kbd><span class="koboSpan" id="kobo.11.1">+1</span></kbd><span class="koboSpan" id="kobo.12.1">, respectively) only partially solve the classification problems.</span></p>
<p><span class="koboSpan" id="kobo.13.1">It is not by chance that the paper entitled </span><em><span class="koboSpan" id="kobo.14.1">Induction of Decision Trees</span></em><span class="koboSpan" id="kobo.15.1"> (</span><a href="http://dl.acm.org/citation.cfm?id=637969"><span class="koboSpan" id="kobo.16.1">http://dl.acm.org/citation.cfm?id=637969</span></a><span class="koboSpan" id="kobo.17.1">), in which John Ross Quinlan described the decision trees algorithm, takes into consideration information conveyed in qualitative form. </span><span class="koboSpan" id="kobo.17.2">The object of the paper by Quinlan (whose contribution was significant for the development of decision trees) is in fact the choice of whether to play tennis outside, based on features such as outlook (sunny, overcast, or rain), temperatures (cool, mild, or hot), humidity (high or normal), windy (true or false).</span></p>
<p><span class="koboSpan" id="kobo.18.1">How can we instruct a machine to process information presented both in quantitative and qualitative forms?</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Decision trees rationales</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Decision trees use binary trees to analyze and process data, thus succeeding in formulating predictions concerning both values ​​expressed in numerical and categorical form, accepting both numerical values ​​and qualitative information as input data.</span></p>
<p><span class="koboSpan" id="kobo.3.1">To intuitively </span><span><span class="koboSpan" id="kobo.4.1">understand </span></span><span class="koboSpan" id="kobo.5.1">the strategy adopted by decision trees, let's see the typical steps involving their implementation:</span></p>
<ol>
<li class="mce-root"><span><span class="koboSpan" id="kobo.6.1">The first step consists in subdividing the original dataset into two child subsets,</span></span><span class="koboSpan" id="kobo.7.1"> after having verified a binary condition</span><span><span class="koboSpan" id="kobo.8.1">, following the first subdivision, we will have two child subsets as a result, in which the binary condition is verified or falsified.</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.9.1">The child subsets will be further subdivided on the basis of further conditions; at each step, the condition that provides the best bipartition of the original subset is chosen (for this purpose, appropriate metrics are used to measure the quality of the subdivision).</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.10.1">The division proceeds in a recursive manner. </span><span class="koboSpan" id="kobo.10.2">It is therefore necessary to define a stopping condition (such as the achievement of a maximum depth).</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.11.1">At each iteration, the algorithm generates a tree structure in which the child nodes represent the choices taken at each step, with each leaf contributing to the overall classification of the input data.</span></li>
</ol>
<p><span class="koboSpan" id="kobo.12.1">Take a look at the following diagram, which depicts the decision tree for the Iris dataset:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.13.1"><img src="assets/003c40cf-dfa5-4005-9e8a-ac54f87b6aa1.png" style="width:39.50em;height:30.92em;"/></span></p>
<p><span class="koboSpan" id="kobo.14.1">Decision trees are also very efficient in the elaboration of large datasets. </span><span class="koboSpan" id="kobo.14.2">In fact, the characteristics of the tree data structures allow us to limit the complexity of the algorithm to an order of magnitude equal to </span><em><span class="koboSpan" id="kobo.15.1">0</span></em><span class="koboSpan" id="kobo.16.1"> (</span><em><span class="koboSpan" id="kobo.17.1">log n</span></em><span class="koboSpan" id="kobo.18.1">).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Phishing detection with decision trees</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We will now see the use of decision trees in the task of phishing detection. </span><span class="koboSpan" id="kobo.2.2">As we said in the previous paragraphs, phishing detection (as well as spam filtering) basically involves the classification of input data:</span></p>
<pre><span class="koboSpan" id="kobo.3.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.4.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.5.1">from sklearn import *</span><br/><span class="koboSpan" id="kobo.6.1">from sklearn.linear_model import LogisticRegression</span><br/><span class="koboSpan" id="kobo.7.1">from sklearn.metrics import accuracy_score</span><br/><br/><span class="koboSpan" id="kobo.8.1">phishing_dataset = np.genfromtxt('../datasets/phishing_dataset.csv', delimiter=',', dtype=np.int32)</span><br/><br/><span class="koboSpan" id="kobo.9.1">samples = phishing_dataset[:,:-1]</span><br/><span class="koboSpan" id="kobo.10.1">targets = phishing_dataset[:, -1]</span><br/><br/><span class="koboSpan" id="kobo.11.1">from sklearn.model_selection import train_test_split</span><br/><br/><span class="koboSpan" id="kobo.12.1">training_samples, testing_samples, training_targets, testing_targets =</span><br/><span class="koboSpan" id="kobo.13.1">train_test_split(samples, targets, test_size=0.2, random_state=0)</span><br/><br/><span class="koboSpan" id="kobo.14.1">from sklearn import tree</span><br/><br/><span class="koboSpan" id="kobo.15.1">tree_classifier = tree.DecisionTreeClassifier()</span><br/><br/><span class="koboSpan" id="kobo.16.1">tree_classifier.fit(training_samples, training_targets)</span><br/><br/><span class="koboSpan" id="kobo.17.1">predictions = tree_classifier.predict(testing_samples)</span><br/><span class="koboSpan" id="kobo.18.1">accuracy = 100.0 * accuracy_score(testing_targets, predictions)</span><br/><br/><span class="koboSpan" id="kobo.19.1">print ("Decision Tree accuracy: " + str(accuracy))</span><br/><br/><span class="koboSpan" id="kobo.20.1">Decision Tree accuracy: 96.33649932157394</span></pre>
<p><span class="koboSpan" id="kobo.21.1">We can see how the decision tree classifier further enhances the already excellent performance obtained previously with logistic regression.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Decision trees – pros and cons</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In addition to the advantages already described, we must remember the possible disadvantages related to decision trees; these are essentially associated with the phenomenon of overfitting, which is due to the complexity of the tree data structures (it is in fact necessary to proceed in a systematic manner with the pruning of the tree, in order to reduce its overall complexity).</span></p>
<p><span class="koboSpan" id="kobo.3.1">One of the undesirable consequences of the complexity is the high sensitivity of the algorithm to even the smallest changes in the training dataset, which can lead to sensible impacts on the prediction model. </span><span class="koboSpan" id="kobo.3.2">Therefore, decision trees are not the best fit for incremental learning.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Spam detection with Naive Bayes</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the advantages associated with the use of Naive Bayes is the fact that it requires little starting data to begin classifying input data; moreover, the information that progressively adds up contributes to dynamically updating the previous estimates, incrementally improving the forecasting model (unlike, as we saw in the previous paragraph, the algorithm based on decision trees).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Advantages of Naive Bayes for spam detection</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The aforementioned features are well suited to the task of spam detection. </span><span class="koboSpan" id="kobo.2.2">Without the need to resort to large datasets, in fact, the spam detection algorithms based on Naive Bayes can exploit the emails already present in the inbox, constantly updating the probability estimations on the base of new email messages that are progressively added to those already existing.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The constant process of </span><span><span class="koboSpan" id="kobo.4.1">updating the </span></span><span class="koboSpan" id="kobo.5.1">probability estimates is based on the well-known Bayes rule:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img class="fm-editor-equation" src="assets/6283b99d-def2-4213-8f43-85cd788579df.png" style="width:13.00em;height:3.08em;"/></span></p>
<p><span class="koboSpan" id="kobo.7.1">The preceding equation describes the relationship between the probability of the occurrence of an event, </span><em><span class="koboSpan" id="kobo.8.1">A,</span></em><span class="koboSpan" id="kobo.9.1"> conditioned to the evidence, </span><em><span class="koboSpan" id="kobo.10.1">E</span></em><span class="koboSpan" id="kobo.11.1">.</span></p>
<p><span class="koboSpan" id="kobo.12.1">This relationship depends on the probability of </span><em><span class="koboSpan" id="kobo.13.1">A</span></em><span class="koboSpan" id="kobo.14.1"> (prior probability) and the likelihood, </span><span class="koboSpan" id="kobo.15.1"><img class="fm-editor-equation" src="assets/834b7330-4413-44d0-a70a-be766c67857f.png" style="width:3.08em;height:0.92em;"/></span><span class="koboSpan" id="kobo.16.1">, of evidence, </span><em><span class="koboSpan" id="kobo.17.1">E</span></em><span class="koboSpan" id="kobo.18.1">, which determines the probability estimate, </span><span class="koboSpan" id="kobo.19.1"><img class="fm-editor-equation" src="assets/cf3e7f1e-cb2f-4e29-b54b-85a51147b8b1.png" style="width:2.67em;height:0.92em;"/></span><span class="koboSpan" id="kobo.20.1"> (the posterior probability).</span></p>
<p><span class="koboSpan" id="kobo.21.1">An important feature of the Bayes rule probability update is that the probability </span><span class="koboSpan" id="kobo.22.1"><img class="fm-editor-equation" src="assets/fb696743-a298-4c7f-9d4f-1f72d190f1eb.png" style="width:2.58em;height:0.92em;"/></span><span class="koboSpan" id="kobo.23.1"> (the posterior probability), as a result of the updating process, becomes the new prior probability, thus contributing to dynamically updating the existing probability estimates.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Why Naive Bayes?</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the basic assumptions of the Bayes rule is that it postulates the independence of events. </span><span class="koboSpan" id="kobo.2.2">This assumption is not always realistic.</span></p>
<p><span class="koboSpan" id="kobo.3.1">However, in most cases, it is a reasonable condition that leads to good forecasts, while at the same time simplifying the application of the Bayes rule, especially in the presence of several competing events, thus reducing the calculations to a simple multiplication of the probabilities associated with each event.</span></p>
<p><span class="koboSpan" id="kobo.4.1">Before seeing Naive Bayes in action, applying the algorithm in spam detection, we need to analyze the text analysis techniques to allow Naive Bayes to dynamically recognize the suspect keywords used by spammers (rather than choosing them in a fixed fashion, as we did in the previous examples).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">NLP to the rescue</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the most exciting areas of AI is certainly NLP, which consists of the analysis and automated understanding of human language.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The purpose of NLP is to try to extract sensible information from unstructured data (such as email messages, tweets, and Facebook posts).</span></p>
<p><span class="koboSpan" id="kobo.4.1">The fields of application of NLP are huge, and vary from simultaneous translations to sentiment analysis speech recognition.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">NLP steps</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The phases that characterize NPL are as follows:</span></p>
<ol>
<li class="mce-root"><span><span class="koboSpan" id="kobo.3.1">Identification of the words (tokens) constituting the language</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.4.1">Analysis of the structure of the text</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.5.1">Identification of the relationships between words (in paragraphs, sentences, and so on)</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.6.1">Semantic analysis of the text</span></li>
</ol>
<p class="mceNonEditable"/>
<p><span class="koboSpan" id="kobo.7.1">One of the best known Python libraries for NLP is the </span><strong><span class="koboSpan" id="kobo.8.1">Natural Language Toolkit</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong><span class="koboSpan" id="kobo.10.1">NLTK</span></strong><span class="koboSpan" id="kobo.11.1">), often used for spam detection.</span></p>
<p><span class="koboSpan" id="kobo.12.1">In the following example, we will see how to take advantage of NLTK combined with Naive Bayes to create a spam detector.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">A Bayesian spam detector with NLTK</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As a concluding example, we will show the use of a classifier based on Naive Bayes, using </span><kbd><span class="koboSpan" id="kobo.3.1">MultinomialNB</span></kbd><span class="koboSpan" id="kobo.4.1"> from the </span><kbd><span class="koboSpan" id="kobo.5.1">sklearn.naive_bayes</span></kbd><span class="koboSpan" id="kobo.6.1"> module. </span><span class="koboSpan" id="kobo.6.2">As usual, we will divide the original dataset consisting of the spam message archive in CSV format, assigning a percentage equal to 30% to the test data subset, and the remaining 70% to the training data subset.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The data will be treated with the </span><strong><span class="koboSpan" id="kobo.8.1">bag of words</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong><span class="koboSpan" id="kobo.10.1">BoW</span></strong><span class="koboSpan" id="kobo.11.1">) technique, which assigns a number to each identified word in the text using </span><span><kbd><span class="koboSpan" id="kobo.12.1">CountVectorizer</span></kbd><span class="koboSpan" id="kobo.13.1"> of </span></span><kbd><span class="koboSpan" id="kobo.14.1">sklearn</span></kbd><span class="koboSpan" id="kobo.15.1">, to which we will pass the </span><kbd><span class="koboSpan" id="kobo.16.1">get_lemmas()</span></kbd><span class="koboSpan" id="kobo.17.1"> method, which returns the individual tokens extracted from the text of the messages.</span></p>
<p><span class="koboSpan" id="kobo.18.1">Finally, we will proceed to normalize and weigh the data using </span><kbd><span class="koboSpan" id="kobo.19.1">TfidfTransformer</span></kbd><span class="koboSpan" id="kobo.20.1">, which transforms a count matrix to a normalized </span><kbd><span class="koboSpan" id="kobo.21.1">tf</span></kbd><span class="koboSpan" id="kobo.22.1"> or </span><kbd><span class="koboSpan" id="kobo.23.1">tf-idf</span></kbd><span class="koboSpan" id="kobo.24.1"> representation.</span></p>
<p><span class="koboSpan" id="kobo.25.1">In the scikit-learn documentation for </span><kbd><span class="koboSpan" id="kobo.26.1">TfidfTransformer</span></kbd><span class="koboSpan" id="kobo.27.1"> (</span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html"><span class="koboSpan" id="kobo.28.1">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html</span></a><span class="koboSpan" id="kobo.29.1">), we can find the following:</span></p>
<div class="packt_quote"><span class="koboSpan" id="kobo.30.1">"Tf means term frequency, while tf-idf means term-frequency times inverse document frequency. </span><span class="koboSpan" id="kobo.30.2">This is a common term weighting scheme in information retrieval that has also found good use in document classification. </span><span class="koboSpan" id="kobo.30.3">The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus</span><q><span class="koboSpan" id="kobo.31.1">."</span></q></div>
<p><span class="koboSpan" id="kobo.32.1">Let's get into the source code:</span></p>
<pre><span class="koboSpan" id="kobo.33.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.34.1">import csv</span><br/><span class="koboSpan" id="kobo.35.1">from textblob import TextBlob</span><br/><span class="koboSpan" id="kobo.36.1">import pandas</span><br/><span class="koboSpan" id="kobo.37.1">import sklearn</span><br/><span class="koboSpan" id="kobo.38.1">import numpy as np</span><br/><br/><span class="koboSpan" id="kobo.39.1">import nltk</span><br/><br/><span class="koboSpan" id="kobo.40.1">from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer</span><br/><span class="koboSpan" id="kobo.41.1">from sklearn.naive_bayes import MultinomialNB</span><br/><span class="koboSpan" id="kobo.42.1">from sklearn.metrics import classification_report, accuracy_score</span><br/><span class="koboSpan" id="kobo.43.1">from sklearn.model_selection import train_test_split </span><br/><br/><span class="koboSpan" id="kobo.44.1">from defs import get_tokens</span><br/><span class="koboSpan" id="kobo.45.1">from defs import get_lemmas</span><br/><br/><span class="koboSpan" id="kobo.46.1">sms = pandas.read_csv('../datasets/sms_spam_no_header.csv', sep=',', names=["type", "text"])</span><br/><br/><span class="koboSpan" id="kobo.47.1">text_train, text_test, type_train, type_test = train_test_split(sms['text'], sms['type'], test_size=0.3)</span><br/><br/><span class="koboSpan" id="kobo.48.1"># bow stands for "Bag of Words"</span><br/><span class="koboSpan" id="kobo.49.1">bow = CountVectorizer(analyzer=get_lemmas).fit(text_train)</span><br/><br/><span class="koboSpan" id="kobo.50.1">sms_bow = bow.transform(text_train)</span><br/><br/><span class="koboSpan" id="kobo.51.1">tfidf = TfidfTransformer().fit(sms_bow)</span><br/><br/><span class="koboSpan" id="kobo.52.1">sms_tfidf = tfidf.transform(sms_bow)</span><br/><br/><span class="koboSpan" id="kobo.53.1">spam_detector = MultinomialNB().fit(sms_tfidf, type_train)</span></pre>
<p><span><span class="koboSpan" id="kobo.54.1">We can check if </span><kbd><span class="koboSpan" id="kobo.55.1">spam_detector</span></kbd><span class="koboSpan" id="kobo.56.1"> works well by trying to run a prediction on a random message (in our example, we chose the 26</span><sup><span class="koboSpan" id="kobo.57.1">th</span></sup><span class="koboSpan" id="kobo.58.1"> message from the dataset), and checking that the detector correctly classifies the type of message (spam or ham) by comparing the predicted value with the corresponding </span><kbd><span class="koboSpan" id="kobo.59.1">type</span></kbd><span class="koboSpan" id="kobo.60.1"> label associated with the message:</span></span></p>
<pre><span class="koboSpan" id="kobo.61.1">msg = sms['text'][25]</span><br/><span class="koboSpan" id="kobo.62.1">msg_bow = bow.transform([msg])</span><br/><span class="koboSpan" id="kobo.63.1">msg_tfidf = tfidf.transform(msg_bow)</span><br/><br/><span class="koboSpan" id="kobo.64.1">print ('predicted:', spam_detector.predict(msg_tfidf)[0])</span><br/><span class="koboSpan" id="kobo.65.1">print ('expected:', sms.type[25])</span><br/><br/><span class="koboSpan" id="kobo.66.1">predicted: ham</span><br/><span class="koboSpan" id="kobo.67.1">expected: ham</span></pre>
<p><span><span class="koboSpan" id="kobo.68.1">At this point, once the correct functioning has been verified, we proceed to the prediction on the whole dataset:</span></span></p>
<pre><span class="koboSpan" id="kobo.69.1">predictions = spam_detector.predict(sms_tfidf)</span><br/><span class="koboSpan" id="kobo.70.1">print ('accuracy', accuracy_score(sms['type'][:len(predictions)], predictions))</span><br/><span class="koboSpan" id="kobo.71.1">accuracy 0.7995385798513202</span></pre>
<p><span class="koboSpan" id="kobo.72.1">The preceding commands generate the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.73.1"><img src="assets/1782eb55-8ee7-4a9c-b80d-23c1110c6e34.png" style="width:41.33em;height:9.08em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.74.1">As can be seen in the preceding screenshot, the level of accuracy of Naive Bayes is already quite high (equal to 80%) with the advantage, unlike the other algorithms, that this accuracy can improve further still as the number of messages analyzed increases.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, several supervised learning algorithms have been explained, and we have seen their concrete application in solving common tasks in the field of cybersecurity, such as spam detection and phishing detection.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The knowledge acquired in this chapter contributes to forming the right mindset to face increasingly complex tasks, such as those we will face in the next chapters, leading to a greater awareness of the advantages and disadvantages associated with each AI algorithm.</span></p>
<p><span class="koboSpan" id="kobo.4.1">In the next chapter, we will learn about malware analysis and</span><span><span class="koboSpan" id="kobo.5.1"> advanced malware detection with DL.</span></span></p>


            </article>

            
        </section>
    </body></html>