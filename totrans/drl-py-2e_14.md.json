["```\ndef get_support(N, V_min, V_max):\n    dz = (V_max – V_min) / (N-1)\n    return [V_min + i * dz for i in range(N)] \n```", "```\nm = np.zeros(num_support)\nfor j in range(num_support):\n    Tz = min(v_max,max(v_min,r+gamma * z[j]))\n    bj = (Tz - v_min) / delta_z\n    l,u = math.floor(bj),math.ceil(bj)\n    pj = p[j] \n    m[int(l)] += pj * (u - bj)\n    m[int(u)] += pj * (bj - l) \n```", "```\nimport numpy as np\nimport random\nfrom collections import deque\nimport math\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport gym\nfrom tensorflow.python.framework import ops \n```", "```\nv_min = 0\nv_max = 1000 \n```", "```\natoms = 51 \n```", "```\ngamma = 0.99 \n```", "```\nbatch_size = 64 \n```", "```\nupdate_target_net = 50 \n```", "```\nepsilon = 0.5 \n```", "```\nbuffer_length = 20000 \n```", "```\nreplay_buffer = deque(maxlen=buffer_length) \n```", "```\ndef sample_transitions(batch_size):\n    batch = np.random.permutation(len(replay_buffer))[:batch_size]\n    trans = np.array(replay_buffer)[batch]\n    return trans \n```", "```\nclass Categorical_DQN(): \n```", "```\n def __init__(self,env): \n```", "```\n self.sess = tf.InteractiveSession() \n```", "```\n self.v_max = v_max\n        self.v_min = v_min \n```", "```\n self.atoms = atoms \n```", "```\n self.epsilon = epsilon \n```", "```\n self.state_shape = env.observation_space.shape \n```", "```\n self.action_shape = env.action_space.n \n```", "```\n self.time_step = 0 \n```", "```\n target_state_shape = [1]\n        target_state_shape.extend(self.state_shape) \n```", "```\n self.state_ph = tf.placeholder(tf.float32,target_state_shape) \n```", "```\n self.action_ph = tf.placeholder(tf.int32,[1,1]) \n```", "```\n self.m_ph = tf.placeholder(tf.float32,[self.atoms]) \n```", "```\n self.delta_z = (self.v_max - self.v_min) / (self.atoms - 1) \n```", "```\n self.z = [self.v_min + i * self.delta_z for i in range(self.atoms)] \n```", "```\n self.build_categorical_DQN() \n```", "```\n self.sess.run(tf.global_variables_initializer()) \n```", "```\n def build_network(self, state, action, name, units_1, units_2, weights, bias): \n```", "```\n with tf.variable_scope('conv1'):\n            conv1 = conv(state, [5, 5, 3, 6], [6], [1, 2, 2, 1], weights, bias) \n```", "```\n with tf.variable_scope('conv2'):\n            conv2 = conv(conv1, [3, 3, 6, 12], [12], [1, 2, 2, 1], weights, bias) \n```", "```\n with tf.variable_scope('flatten'):\n            flatten = tf.layers.flatten(conv2) \n```", "```\n with tf.variable_scope('dense1'):\n            dense1 = dense(flatten, units_1, [units_1], weights, bias) \n```", "```\n with tf.variable_scope('dense2'):\n            dense2 = dense(dense1, units_2, [units_2], weights, bias) \n```", "```\n with tf.variable_scope('concat'):\n            concatenated = tf.concat([dense2, tf.cast(action, tf.float32)], 1) \n```", "```\n with tf.variable_scope('dense3'):\n            dense3 = dense(concatenated, self.atoms, [self.atoms], weights, bias) \n        return tf.nn.softmax(dense3) \n```", "```\n def build_categorical_DQN(self): \n```", "```\n with tf.variable_scope('main_net'):\n            name = ['main_net_params',tf.GraphKeys.GLOBAL_VARIABLES]\n            weights = tf.random_uniform_initializer(-0.1,0.1)\n            bias = tf.constant_initializer(0.1)\n            self.main_p = self.build_network(self.state_ph,self.action_ph,name,24,24,weights,bias) \n```", "```\n with tf.variable_scope('target_net'):\n            name = ['target_net_params',tf.GraphKeys.GLOBAL_VARIABLES]\n            weights = tf.random_uniform_initializer(-0.1,0.1)\n            bias = tf.constant_initializer(0.1)\n            self.target_p = self.build_network(self.state_ph,self.action_ph,name,24,24,weights,bias) \n```", "```\n self.main_Q = tf.reduce_sum(self.main_p * self.z) \n```", "```\n self.target_Q = tf.reduce_sum(self.target_p * self.z) \n```", "```\n self.cross_entropy_loss = -tf.reduce_sum(self.m_ph * tf.log(self.main_p)) \n```", "```\n self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.cross_entropy_loss) \n```", "```\n main_net_params = tf.get_collection(\"main_net_params\") \n```", "```\n target_net_params = tf.get_collection('target_net_params') \n```", "```\n self.update_target_net = [tf.assign(t, e) for t, e in zip(target_net_params, main_net_params)] \n```", "```\n def train(self,s,r,action,s_,gamma): \n```", "```\n self.time_step += 1 \n```", "```\n list_q_ = [self.sess.run(self.target_Q,feed_dict={self.state_ph:[s_],self.action_ph:[[a]]}) for a in range(self.action_shape)] \n```", "```\n a_ = tf.argmax(list_q_).eval() \n```", "```\n m = np.zeros(self.atoms) \n```", "```\n p = self.sess.run(self.target_p,feed_dict = {self.state_ph:[s_],self.action_ph:[[a_]]})[0] \n```", "```\n for j in range(self.atoms):\n            Tz = min(self.v_max,max(self.v_min,r+gamma * self.z[j]))\n            bj = (Tz - self.v_min) / self.delta_z \n            l,u = math.floor(bj),math.ceil(bj) \n            pj = p[j]\n            m[int(l)] += pj * (u - bj)\n            m[int(u)] += pj * (bj - l) \n```", "```\n self.sess.run(self.optimizer,feed_dict={self.state_ph:[s] , self.action_ph:[action], self.m_ph: m }) \n```", "```\n if self.time_step % update_target_net == 0:\n            self.sess.run(self.update_target_net) \n```", "```\n def select_action(self,s): \n```", "```\n if random.random() <= self.epsilon:\n            return random.randint(0, self.action_shape - 1)\n        else: \n            return np.argmax([self.sess.run(self.main_Q,feed_dict={self.state_ph:[s],self.action_ph:[[a]]}) for a in range(self.action_shape)]) \n```", "```\nenv = gym.make(\"Tennis-v0\") \n```", "```\nagent = Categorical_DQN(env) \n```", "```\nnum_episodes = 800 \n```", "```\nfor i in range(num_episodes): \n```", "```\n done = False \n```", "```\n Return = 0 \n```", "```\n state = env.reset() \n```", "```\n while not done: \n```", "```\n env.render() \n```", "```\n action = agent.select_action(state) \n```", "```\n next_state, reward, done, info = env.step(action) \n```", "```\n Return = Return + reward \n```", "```\n replay_buffer.append([state, reward, [action], next_state]) \n```", "```\n if len(replay_buffer) >= batch_size:\n            trans = sample_transitions(batch_size)\n            for item in trans:\n                agent.train(item[0],item[1], item[2], item[3],gamma) \n```", "```\n state = next_state \n```", "```\n print(\"Episode:{}, Return: {}\".format(i,Return)) \n```", "```\ndef huber_loss(target,predicted, kappa=1):\n    #compute u as difference between target and predicted value\n    u = target – predicted\n    #absolute value of u\n    abs_u = abs(u)\n    #compute quadratic loss\n    quad_loss = 0.5 * (abs_u ** 2) \n    #compute linear loss\n    linear_loss = kappa * (abs_u - 0.5 * kappa)\n    #true where the absolute value is less than or equal to kappa\n    flag = abs_u <= kappa\n    #Loss is the quadratic loss where the absolute value is less than kappa \n    #else it is linear loss\n    loss = (flag) * quad_loss + (~flag) * linear_loss\n\n    return loss \n```"]