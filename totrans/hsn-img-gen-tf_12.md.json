["```\nffmpeg  -i video.mp4 /images/image_%04d.png\n```", "```\nimport cv2\ncap = cv2.VideoCapture('video.mp4')\ncount = 0 \nwhile cap.isOpened():\n    ret,frame = cap.read()\n    cv2.imwrite(“images/image_%04d.png” % count, frame)\n    count += 1\n```", "```\nimport face_recognition\ncoords = face_recognition.face_locations(image, model='cnn')[0]\n```", "```\ndef _css_to_rect(css):\n    return dlib.rectangle(css[3], css[0], css[1], css[2])\nface_coords = _css_to_rect(coords)\n```", "```\ndef crop_face(image, coords, pad=0):\n    x_min = coords.left() - pad\n    x_max = coords.right() + pad\n    y_min = coords.top() - pad\n    y_max = coords.bottom() + pad\n    return image[y_min:y_max, x_min:x_max]\n```", "```\npredictor = dlib.shape_predictor( \t\t    'shape_predictor_68_face_landmarks.dat') \nface_shape = predictor(face_image, face_coords)\n```", "```\ndef shape_to_np(shape):\n    coords = []\n    for i in range(0, shape.num_parts):        \n        coords.append((shape.part(i).x, shape.part(i).y))\n    return np.array(coords)\nface_shape = shape_to_np(face_shape)\n```", "```\nfrom umeyama import umeyama def get_align_mat(face_landmarks):    return umeyama(face_landmarks[17:], \\  \t\t\t   mean_landmarks, False)[0:2]affine_matrix = get_align_mat(face_image)\n```", "```\ndef align_face(face_image, affine_matrix, size, padding=50):\n    affine_matrix = affine_matrix * \\ \t\t\t\t(size[0] - 2 * padding) \n    affine_matrix[:, 2] += padding\n    aligned_face = cv2.warpAffine(face_image,  \t\t\t\t\t\t  affine_matrix,  \t\t\t\t\t\t  (size, size))    return aligned_face\n```", "```\ncoverage = 200 range_ = numpy.linspace(128 - coverage//2, 128 + coverage//2, 5)mapx = numpy.broadcast_to(range_, (5, 5))\nmapy = mapx.T\nmapx = mapx + numpy.random.normal(size=(5, 5), scale=5)\nmapy = mapy + numpy.random.normal(size=(5, 5), scale=5)\ninterp_mapx = cv2.resize(mapx, (80, 80))\\ \t\t\t\t   [8:72, 8:72].astype('float32')\ninterp_mapy = cv2.resize(mapy, (80, 80))[8:72,\\ \t\t\t\t   8:72].astype('float32')\nwarped_image = cv2.remap(image, interp_mapx,  \t\t\t\t   interp_mapy, cv2.INTER_LINEAR)\n```", "```\ndef downsample(filters):\n    return Sequential([\n        Conv2D(filters, kernel_size=5, strides=2, \t\t\tpadding='same'),\n        LeakyReLU(0.1)])\n```", "```\ndef Encoder(z_dim=1024):\n    inputs = Input(shape=IMAGE_SHAPE)\n    x = inputs\n    x = downsample(128)(x)\n    x = downsample(256)(x)\n    x = downsample(512)(x)\n    x = downsample(1024)(x)\n    x = Flatten()(x)\n    x = Dense(z_dim)(x)\n    x = Dense(4 * 4 * 1024)(x)\n    x = Reshape((4, 4, 1024))(x)  \n    x = UpSampling2D((2,2))(x)\n    out = Conv2D(512, kernel_size=3, strides=1, \t\t\t padding='same')(x)        \n    return Model(inputs=inputs, outputs=out, name='encoder')\n```", "```\n    def upsample(filters, name=''):\n        return Sequential([\n            UpSampling2D((2,2)),\n            Conv2D(filters, kernel_size=3, strides=1, \t\t\tpadding='same'),        \n            LeakyReLU(0.1)\n        ], name=name)\n    ```", "```\n    def Decoder(input_shape=(8, 8 ,512)):\n        inputs = Input(shape=input_shape)\n        x = inputs    \n        x = upsample(256,”Upsample_1”)(x)\n        x = upsample(128,”Upsample_2”)(x)\n        x = upsample(64,”Upsample_3”)(x)\n        out = Conv2D(filters=3, kernel_size=5, \t\t\t padding='same', \t\t\t activation='sigmoid')(x)    \n        return Model(inputs=inputs, outputs=out, \t\t\t name='decoder')\n    ```", "```\nclass deepfake:\n    def __init__(self, z_dim=1024):\n        self.encoder = Encoder(z_dim)\n        self.decoder_a = Decoder()\n        self.decoder_b = Decoder()\n```", "```\n        x = Input(shape=IMAGE_SHAPE)\n        self.ae_a = Model(x, self.decoder_a(self.encoder(x)),\n                          name=”Autoencoder_A”)\n        self.ae_b = Model(x, self.decoder_b(self.encoder(x)),\n                          name=”Autoencoder_B”)\n        optimizer = Adam(5e-5, beta_1=0.5, beta_2=0.999)\n        self.ae_a.compile(optimizer=optimizer, loss='mae')\n        self.ae_b.compile(optimizer=optimizer, loss='mae')\n```", "```\ndef train_step(self, gen_a, gen_b):\n    warped_a, target_a = next(gen_a)\n    warped_b, target_b = next(gen_b)\n    loss_a = self.ae_a.train_on_batch(warped_a, target_a)\n    loss_b = self.ae_b.train_on_batch(warped_b, target_b)\n    return loss_a, loss_b\n```", "```\nh, w, _ = image.shape\nsize = 64\nnew_image = np.zeros_like(image, dtype=np.uint8)\nnew_image = cv2.warpAffine(np.array(new_face, \t\t\t\t\t\t   dtype=np.uint8)\n                          mat*size, (w, h), \n                          new_image,\n                          flags=cv2.WARP_INVERSE_MAP,\n                          borderMode=cv2.BORDER_TRANSPARENT)\n```", "```\ndef get_hull_mask(image, landmarks):\n    hull = cv2.convexHull(face_shape)\n    hull_mask = np.zeros_like(image, dtype=float)\n    hull_mask = cv2.fillConvexPoly(hull_mask,  \t\t\t\t\t\t   hull,(1,1,1))\n    return hull_mask\n```", "```\ndef apply_face(image, new_image, mask):\n    base_image = np.copy(image).astype(np.float32)\n    foreground = cv2.multiply(mask, new_image)\n    background = cv2.multiply(1 - mask, base_image)\n    output_image = cv2.add(foreground, background)\n    return output_image\n```", "```\nffmpeg -start_number 1 -i image_%04d.png -vcodec mpeg4 output.mp4\n```"]