- en: Advanced Convolutional Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding Convolutional
    Networks*, we discussed the building blocks of **convolutional neural networks**
    (**CNNs**) and some of their properties. In this chapter, we'll go a step further
    and talk about some of the most popular CNN architectures. These networks usually
    combine multiple primitive convolution and/or pooling operations in a novel building
    block that serves as a base for a complex architecture. This allows us to build
    very deep (and sometimes wide) networks with high representational power that
    perform well on complex tasks such as ImageNet classification, image segmentation,
    speech recognition, and so on. Many of these models were first released as participants
    in the ImageNet challenge, which they usually won. To simplify our task, we'll
    discuss all architecture within the context of image classification. We'll still
    discuss more complex tasks, but we'll do it in [Chapter 4](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml),
    *Object Detection and Image Segmentation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing AlexNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Visual Geometry Group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding residual networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Inception networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Xception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing MobileNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to DenseNets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The workings of neural architecture search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing capsule networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing AlexNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first model we'll discuss is the winner of the 2012 **ImageNet Large Scale
    Visual Recognition Challenge** (**ILSVRC**, or simply ImageNet). It's nicknamed
    AlexNet (*ImageNet Classification with Deep Convolutional Neural Networks*, [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)),
    after one of its authors, Alex Krizhevsky. Although this model is rarely used
    nowadays, it's an important milestone in contemporary deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec08175c-5282-4be2-b6e7-6f2d99272166.png)'
  prefs: []
  type: TYPE_IMG
- en: The AlexNet architecture. The original model was split in two, so it can fit
    on the memory of two GPUs
  prefs: []
  type: TYPE_NORMAL
- en: The model has five cross-correlated convolutional layers, three overlapping
    max pooling layers, three fully connected layers, and ReLU activations. The output
    is a 1,000-way softmax (one for each ImageNet class). The first and second convolutional
    layers use local response normalization—a type of normalization, somewhat similar
    to batch normalization. The fully connected layers have a dropout rate of 0.5\.
    To prevent overfitting, the network was trained using random 227×227 crops of
    the 256×256 input images. The network achieves top-1 and top-5 test set error
    rates of 37.5% and 17.0%.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll discuss an NN architecture that was introduced by Oxford's
    Visual Geometry Group in 2014, when it became a runner-up in the ImageNet challenge
    of that year.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Visual Geometry Group
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next architecture we're going to discuss is **Visual Geometry Group** (**VGG**)
    (from Oxford's Visual Geometry Group, *Very Deep Convolutional Networks for Large-Scale
    Ima*g*e* *Recognition*, [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)[)](https://arxiv.org/abs/1409.1556).
    The VGG family of networks remains popular today and is often used as a benchmark
    against newer architectures. Prior to VGG (for example, LeNet-5: [http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/) and
    AlexNet), the initial convolutional layers of a network used filters with large
    receptive fields, such as 11×11\. Additionally, the networks usually had alternating
    single convolutional and pooling layers. The authors of the paper observed that
    a convolutional layer with a large filter size can be replaced with a stack of
    two or more convolutional layers with smaller filters (factorized convolution).
    For example, we can replace one 5×5 layer with a stack of two 3×3 layers, or a
    7×7 layer with a stack of three 3×3 layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This structure has several advantages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The neurons of the last of the stacked layers have the equivalent receptive
    field size of a single layer with a large filter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of weights and operations of stacked layers is smaller, compared
    to a single layer with a large filter size. Let's assume we want to replace one
    5×5 layer with two 3×3 layers. Let's also assume that all layers have an equal
    number of input and output channels (slices), *M*. The total number of weights
    (excluding biases) of the 5×5 layer is *5*5*M*M = 25*M²*. On the other hand, the
    total weights of a single 3×3 layer is *3*3*M*M = 9*M²*, and simply *2*(3*3*M*M)
    = 18*M²* for two layers, which makes this arrangement 28% more efficient (18/25
    = 0.72). The efficiency will increase further with larger filters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking multiple layers makes the decision function more discriminative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The VGG networks consist of multiple blocks of two, three, or four stacked
    convolutional layers combined with a max pooling layer. We can see the two most
    popular variants, **VGG16** and **VGG19**, in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab51fa0e-ccbf-4a89-b3c7-f76561637925.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture of the VGG16 and VGG19 networks, named after the number of weighted
    layers in each network
  prefs: []
  type: TYPE_NORMAL
- en: As the depth of the VGG network increases, so does the width (the number of
    filters) in the convolutional layers. We have multiple pairs of cross-channel
    convolutions with a volume depth of 128/256/512 connected to other layers with
    the same depth. In addition, we also have two 4,096-unit fully connected layers,
    followed by a 1000-unit fully connected layer and a softmax (one for each ImageNet
    class). Because of this, the VGG networks have a large number of parameters (weights),
    which makes them memory-inefficient, as well as computationally expensive. Still,
    this is a popular and straightforward network architecture, which has been further
    improved by the addition of batch normalization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll use VGG as an example of how to load pretrained network
    models with TensorFlow and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: VGG with PyTorch and TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both PyTorch and TensorFlow have pretrained VGG models. Let's see how to use
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras is an official part of TensorFlow 2, therefore, we''ll use it to load
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: By setting the `weights='imagenet'` parameter, the network will be loaded with
    pretrained ImageNet weights (they will be downloaded automatically). You can set
    `include_top` to `False`, which will exclude the fully connected layers for a
    transfer learning scenario. In this case, you can also use an arbitrary input
    size by setting a tuple value to `input_shape`—the convolutional layers will automatically
    scale to match the desired input shape. This is possible because the convolution
    filter is shared along the whole feature map. Therefore, we can use the same filter on
    feature maps with different sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll continue with PyTorch, where you can choose whether you want to use
    a pretrained model (again, with automatic download):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can try other pretrained models, using the same procedures we described.
    To avoid repetition, we won't include the same code examples for the other architectures
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll discuss one of the most popular CNN architectures,
    which was released after VGG.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding residual networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Residual networks (**ResNets**, *Deep Residual Learning for Image Recognition*, [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))
    were released in 2015, when they won all five categories of the ImageNet challenge
    that year. In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts
    and Bolts of Neural* *Networks*, we mentioned that the layers of a neural network
    are not restricted to sequential order, but form a graph instead. This is the
    first architecture we'll learn, which takes advantage of this flexibility. This
    is also the first network architecture that has successfully trained a network
    with a depth of more than 100 layers.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to better weight initializations, new activation functions, as well as
    normalization layers, it's now possible to train deep networks. But, the authors
    of the paper conducted some experiments and observed that a network with 56 layers
    had higher training and testing errors compared to a network with 20 layers. They
    argue that this should not be the case. In theory, we can take a shallow network
    and stack identity layers (these are layers whose output just repeats the input)
    on top of it to produce a deeper network that behaves in exactly the same way
    as the shallow one. Yet, their experiments have been unable to match the performance
    of the shallow network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, they proposed a network constructed of residual blocks.
    A residual block consists of two or three sequential convolutional layers and
    a separate parallel identity (repeater) shortcut connection, which connects the
    input of the first layer and the output of the last one. We can see three types
    of residual blocks in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9629b25c-e912-469b-83c7-9d1f4058a249.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: original residual block; original bottleneck residual block;
    pre-activation residual block; pre-activation bottleneck residual block'
  prefs: []
  type: TYPE_NORMAL
- en: Each block has two parallel paths. The left-hand path is similar to the other
    networks we've seen, and consists of sequential convolutional layers + batch normalization.
    The right path contains the identity shortcut connection (also known as the skip
    connection). The two paths are merged via an element-wise sum. That is, the left
    and right tensors have the same shape and an element of the first tensor is added
    to the element in the same position in the second tensor. The output is a single
    tensor with the same shape as the input. In effect, we propagate forward the features
    learned by the block, but also the original unmodified signal. In this way, we
    can get closer to the original scenario, as described by the authors. The network
    can decide to skip some of the convolutional layers thanks to the skip connections,
    in effect reducing its own depth. The residual blocks use padding in such a way
    that the input and the output of the block have the same dimensions. Thanks to
    this, we can stack any number of blocks for a network with an arbitrary depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, let''s see how the blocks in the diagram differ:'
  prefs: []
  type: TYPE_NORMAL
- en: The first block contains two 3×3 convolutional layers. This is the original
    residual block, but if the layers are wide, stacking multiple blocks becomes computationally
    expensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second block is equivalent to the first, but it uses the so-called bottleneck
    layer. First, we use a 1×1 convolution to downsample the input volume depth (we
    discussed this in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),* Understanding
    Convolutional Networks*). Then, we apply a 3×3 (bottleneck) convolution to the
    reduced input. Finally, we expand the output back to the desired depth with another
    1×1 convolution. This layer is less computationally expensive than the first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third block is the latest revision of the idea, published in 2016 by the
    same authors (*Identity Mappings in Deep Residual Networks*, [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)).
    It uses pre-activations, and the batch normalization and the activation function
    come before the convolutional layer. This may seem strange at first, but thanks
    to this design, the skip connection path can run uninterrupted throughout the
    network. This is contrary to the other residual blocks, where at least one activation
    function is on the path of the skip connection. A combination of stacked residual
    blocks still has the layers in the right order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth block is the bottleneck version of the third layer. It follows the
    same principle as the bottleneck residual layer v1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following table, we can see the family of networks proposed by the authors
    of the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bf80ec8-ffb0-4f87-964a-3db43e6b09d0.png)'
  prefs: []
  type: TYPE_IMG
- en: The family of the most popular residual networks. The residual blocks are represented
    by rounded rectangles
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of their properties are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: They start with a 7×7 convolutional layer with stride 2, followed by 3×3 max-pooling.
    This layer also serves as a downsampling step—the rest of the network starts with
    a much smaller slice of 56×56, compared to 224×224 of the input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downsampling in the rest of the network is implemented with a modified residual
    block with stride 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average pooling downsamples the output after all residual blocks and before
    the 1,000-unit fully connected softmax layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ResNet family of networks is popular not only because of their accuracy,
    but also because of their relative simplicity and the versatility of the residual
    blocks. As we mentioned, the input and output shape of the residual block can
    be the same due to the padding. We can stack residual blocks in different configurations
    to solve various problems with wide-ranging training set sizes and input dimensions.
    Because of this universality, we'll implement an example of ResNet in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing residual blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll implement a pre-activation ResNet to classify the CIFAR-10
    images using PyTorch 1.3.1 and `torchvision` 0.4.2\. Let''s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we''ll start with the imports. Note that we''ll use the shorthand
    `F` for the PyTorch functional module ([https://pytorch.org/docs/stable/nn.html#torch-nn-functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s define the pre-activation regular (non-bottleneck) residual block.
    We''ll implement it as `nn.Module`—the base class for all neural network modules.
    Let''s start with the class definition and the `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will define only the learnable block components in the `__init__` method—these
    include the convolution and batch normalization operations. Also, note the way
    we implement the `shortcut` connection. If the input dimensions are the same as
    the output dimensions, we can directly use the input tensor as the shortcut. However,
    if the dimensions differ, we have to transform the input with the help of a 1×1
    convolution with the same stride and output channels as the one in the main path.
    The dimensions may differ either by height/width (`stride != 1`) or by depth (`in_slices
    != self.expansion * slices`). `self.expansion` is a hyper-parameter, which was
    included in the original ResNet implementation. It allows us to expand the output
    depth of the residual block.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual data propagation is implemented in the `forward` method (please
    mind the indentation, as it''s a member of `PreActivationBlock`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We use the functional `F.relu` for the activation function, as it doesn't have
    learnable parameters. Then, if the shortcut connection is a convolution and not
    an identity (that is, the input/output dimensions of the block differ), we'll
    reuse `F.relu(self.bn_1(x))` to add non-linearity and batch normalization to the
    shortcut. Otherwise, we'll just repeat the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, let''s implement the bottleneck version of the residual block. We''ll
    use the same blueprint as in the non-bottleneck implementation. We''ll start with
    the class definition and the `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `expansion` parameter is `4` after the original implementation. The `self.conv_1`
    convolution operation represents the 1×1 downsampling bottleneck connection, `self.conv_2`
    is the actual convolution, and `self.conv_3` is the upsampling 1×1 convolution.
    The shortcut mechanism follows the same logic as in `PreActivationBlock`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s implement the `PreActivationBottleneckBlock.forward` method. Once
    again, it follows the same logic as the one in `PreActivationBlock`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s implement the residual network itself. We''ll start with the class
    definition (it inherits `nn.Module`) and the `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The network contains four groups of residual blocks, just like the original
    implementation. The number of blocks of each group is specified by the `num_blocks`
    parameter. The initial convolution uses a 3×3 filter with stride 1, as opposed
    to a 7×7 with stride 2 of the original implementation. This is because the 32×32 CIFAR-10
    images are much smaller than the 224×224 ImageNet ones, and the downsampling is
    unnecessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we''ll implement the `PreActivationResNet._make_group` method, which
    creates one residual block group. All blocks in the group have stride 1, except
    for the first, where `stride` is supplied as a parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll implement the `PreActivationResNet.forward` method, which propagates
    the data through the network. We can see the downsampling average pooling before
    the fully connected final layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we''re done with the network, we can implement several ResNet configurations.
    The following is `ResNet34` with 34 convolution layers, grouped in `[3, 4, 6,
    3]` non-bottleneck residual blocks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train the network. We''ll start by defining the train and test
    datasets. We won''t go into much detail about the implementation, as we''ve already
    looked at a similar scenario, in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*. We''ll augment the training set by padding
    the samples with four pixels, and then we''ll take random 32×32 crops out of it.
    The following is the implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll instantiate the network model and the training parameters—cross-entropy
    loss and the Adam optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train the network for `EPOCHS` epochs. The `train_model`, `test_model`,
    and `plot_accuracy` functions are the same as the ones we defined in the *Implementing
    transfer learning with PyTorch* section of [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, and we won''t repeat their implementation
    here. The following is the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And, in the following graph, we can see the test accuracy in 15 iterations
    (the training might take a while):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ace8fd47-9786-401b-87fc-75cc65bba7e9.png)'
  prefs: []
  type: TYPE_IMG
- en: ResNet34 CIFAR accuracy in 15 epochs
  prefs: []
  type: TYPE_NORMAL
- en: The code in this section is partially based on the pre-activation ResNet implementation
    in [https://github.com/kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed the various types of ResNets, and then we implemented
    one with PyTorch. In the next section, we'll discuss Inception networks—yet another
    family of networks, which elevate the use of parallel connections to a new level.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Inception networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inception networks (*Going Deeper with Convolutions*, [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842))
    were introduced in 2014, when they won the ImageNet challenge of that year (there
    seems to be a pattern here). Since then, the authors have released multiple improvements
    (versions) of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fun fact: the name Inception comes in part from the **We need to go deeper**
    internet meme, related to the movie *Inception*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind Inception networks started from the basic premise that the
    objects in an image have different scales. A distant object might take up a small
    region of the image, but the same object, once nearer, might take up the majority
    of the image. This presents a difficulty for standard CNNs, where the neurons
    in the different layers have a fixed receptive field size as imposed on the input
    image. A regular network might be a good detector of objects at a certain scale,
    but could miss them otherwise. To solve this problem, the authors of the paper proposed
    a novel architecture: one composed of Inception blocks. An Inception block starts
    with a common input, and then splits it into different parallel paths (or towers).
    Each path contains either convolutional layers with a different-sized filter,
    or a pooling layer. In this way, we apply different receptive fields on the same
    input data. At the end of the Inception block, the outputs of the different paths
    are concatenated. In the next few sections, we''ll discuss the different variations
    of Inception networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Inception v1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram shows the first version of the Inception block, part
    of the GoogLeNet network architecture ([https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)).
    GoogLeNet contains nine such Inception blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/532d7c33-b7ba-4743-a7f2-c44ef6b9c4e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Inception v1 block, inspired by https://arxiv.org/abs/1409.4842
  prefs: []
  type: TYPE_NORMAL
- en: 'The v1 block has four paths:'
  prefs: []
  type: TYPE_NORMAL
- en: 1×1 convolution, which acts as a kind of repeater to the input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1×1 convolution, followed by a 3×3 convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1×1 convolution, followed by a 5×5 convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3×3 max pooling with stride 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The layers in the block use padding in such a way that the input and the output
    have the same shape (but different depths). The padding is also necessary, because
    each path would produce an output with a different shape, depending on the filter
    size. This is valid for all versions of Inception blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The other major innovation of this Inception block is the use of downsampling
    1×1 convolutions. They are needed because the output of all paths is concatenated
    to produce the final output of the block. The result of the concatenation is an
    output with a quadrupled depth. If another Inception block followed the current
    one, its output depth would quadruple again. To avoid such exponential growth,
    the block uses 1×1 convolutions to reduce the depth of each path, which in turn
    reduces the output depth of the block. This makes it possible to create deeper
    networks, without running out of resources.
  prefs: []
  type: TYPE_NORMAL
- en: GoogLeNet also utilizes auxiliary classifiers—that is, it has two additional
    classification outputs (with the same groundtruth labels) at various intermediate
    layers. During training, the total value of the loss is a weighted sum of the auxiliary
    losses and the real loss.
  prefs: []
  type: TYPE_NORMAL
- en: Inception v2 and v3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inception v2 and v3 were released together and propose several improvements
    over the original Inception block (*Rethinking the Inception Architecture for
    Computer Vision*, [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)).
    The first is the factorization of the 5×5 convolution in two stacked 3×3 convolutions.
    We discussed the advantages of this in the *Introduction to Visual Geometry Group* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the new Inception block in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26826db8-c95e-41ee-a113-970221a8668e.png)'
  prefs: []
  type: TYPE_IMG
- en: Inception block A, inspired by https://arxiv.org/abs/1512.00567
  prefs: []
  type: TYPE_NORMAL
- en: 'The next improvement is the factorization of an *n*×*n* convolution in two
    stacked asymmetrical 1×*n* and *n*×1 convolutions. For example, we can split a
    single 3×3 convolution into two 1×3 and 3×1 convolutions, where the 3×1 convolution
    is applied over the output of the 1×3 convolution. In the first case, the filter
    size would be 3*3 = 9, while in the second case, we would have a combined size
    of (3*1) + (1*3) = 3 + 3 = 6, resulting in 33% efficiency, as seen in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/023a33c1-b7ee-4535-ab68-18ab39d21038.png)'
  prefs: []
  type: TYPE_IMG
- en: Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions. Inspired by https://arxiv.org/abs/1512.00567
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors introduced two new blocks, which utilizes factorized convolutions.
    The first of these blocks (and the second in total) is equivalent of Inception
    block A:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43e66db9-3ccb-4855-b1df-0890467a8f60.png)'
  prefs: []
  type: TYPE_IMG
- en: Inception block B. When n=3, it is equivalent to block A. Inspired by https://arxiv.org/abs/1512.00567
  prefs: []
  type: TYPE_NORMAL
- en: 'The second (third in total) block is similar, but the asymmetrical convolutions
    are parallel, resulting in a higher output depth (more concatenated paths). The
    hypothesis here is that the more features (different filters) the network has,
    the faster it learns (we also discussed the need for more filters in [Chapter
    2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding Convolutional Networks*).
    On the other hand, the wider layers take more memory and computation time. As
    a compromise, this block is only used in the deeper part of the network, after
    the other blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e74a4669-e728-4122-ad8d-a5607c4ba63e.png)'
  prefs: []
  type: TYPE_IMG
- en: Inception block C, inspired by https://arxiv.org/abs/1512.00567
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these new blocks, the authors proposed two new Inception networks: v2
    and v3\. Another major improvement in this version is the use of batch normalization,
    which was introduced by the same authors.'
  prefs: []
  type: TYPE_NORMAL
- en: Inception v4 and Inception-ResNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the latest revision of Inception networks, the authors introduced three
    new streamlined Inception blocks that build upon the idea of the previous versions (*Inception-v4,
    Inception-ResNet and the Impact of Residual Connections on Learning*, [https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)).
    They introduced 7×7 asymmetric factorized convolutions, and average pooling instead
    of max pooling. More importantly, they created a residual/Inception hybrid network
    known as Inception-ResNet, where the Inception blocks also include residual connections.
    We can see the schematic of one such block in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c073e89b-8b95-4fb6-b486-aaa22c26193b.png)'
  prefs: []
  type: TYPE_IMG
- en: An Inception block with a residual skip connection
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed different types of Inception networks and the
    different principles used in the various Inception blocks. Next, we'll talk about
    a newer CNN architecture, which takes the Inception concept to a new depth (or
    width, as it should be).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Xception
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All Inception blocks so far start by splitting the input into several parallel
    paths. Each path continues with a dimensionality-reduction 1×1 cross-channel convolution,
    followed by regular cross-channel convolutions. On one hand, the 1×1 connection
    maps cross-channel correlations, but not spatial ones (because of the 1×1 filter
    size). On the other hand, the subsequent cross-channel convolutions map both types
    of correlations. Let''s recall that in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding
    Convolutional Networks*, we introduced **depthwise separable convolutions** (**DSC**),
    which combine the following two operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A depthwise convolution**: In a depthwise convolution, a single input slice
    produces a single output slice, therefore it only maps spatial (and not cross-channel)
    correlations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A 1×1 cross-channel convolution**: With 1×1 convolutions, we have the opposite,
    that is, they only map cross-channel correlations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The author of Xception (*Xception: Deep Learning with Depthwise Separable Convolutions*, [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))
    argues that, in fact, we can think of DSC as an extreme (hence the name) version
    of an Inception block, where each depthwise input/output slice pair represents
    one parallel path. We have as many parallel paths as the number of input slices. The
    following diagram shows a simplified Inception block and its transformation to
    an Xception block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41ef8798-793d-4b06-8b80-d4fd09bf4d0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: simplified Inception module. Right: Xception block. Inspired by https://arxiv.org/abs/1610.02357'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Xception block and the DSC have two differences:'
  prefs: []
  type: TYPE_NORMAL
- en: In Xception, the 1×1 convolution comes first, instead of last as in DSC. But,
    these operations are meant to be stacked anyway, and we can assume that the order
    is of no significance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Xception block uses ReLU activations after each convolution, while the DSC
    doesn't use non-linearity after the cross-channel convolution. According to the
    author's experiments, networks with absent non-linearity depthwise convolution
    converged faster and were more accurate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the architecture of the Xception network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c75638b5-9f2a-4871-874c-c3013d8f80ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: Entry flow; Middle flow, repeated eight times; Exit flow.
    Source: https://arxiv.org/abs/1610.02357'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is built of linearly stacked DSCs and some of its properties are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The network contains 36 convolutional layers, structured into 14 modules, all
    of which have linear residual connections around them, except for the first and
    last modules. The modules are grouped in three sequential virtual flows—entry,
    middle, and exit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downsampling with 3×3 max pooling in the entry and exit flows; no downsampling
    in the middle flow; global average pooling before the fully connected layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All convolutions and DSCs are followed by batch normalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All DSCs have a depth multiplier of 1 (no depth expansion).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section concludes the series of Inception-based models. In the next section,
    we'll focus on a special model, which prioritizes a small footprint and computational
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MobileNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll discuss a lightweight CNN model called MobileNet (*MobileNetV2:
    Inverted Residuals and Linear Bottlenecks*, [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)).
    We''ll focus on the second revision of this idea (MobileNetV1 was introduced in *MobileNets:
    Efficient Convolutional Neural Networks for Mobile Vision Applications*, [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)).'
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet is aimed at devices with limited memory and computing power, such
    as mobile phones (the name kind of gives it away). To reduce its footprint, the
    network uses DSC, linear bottlenecks, and inverted residuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are already familiar with DSC, so let''s discuss the other two:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear bottlenecks**: To understand this concept, we''ll quote the paper:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"Consider a deep neural network consisting of *n* layers *L[i]* each of which
    has an activation tensor of dimensions [![](img/684623c0-9307-4ec3-9630-699e64599285.png)].
    Throughout this section we will be discussing the basic properties of these activation
    tensors, which we will treat as containers of [![](img/d2c0077c-fecb-48f2-9e46-06527c760674.png)] "pixels"
    with *d[i]* dimensions. Informally, for an input set of real images, we say that
    the set of layer activations (for any layer *L[i]*) forms a "*manifold of interest"*.
    It has been long assumed that manifolds of interest in neural networks could be
    embedded in low-dimensional subspaces. In other words, when we look at all individual
    *d*-channel pixels of a deep convolutional layer, the information encoded in those
    values actually lie in some manifold, which can be embedded into a low-dimensional
    subspace."'
  prefs: []
  type: TYPE_NORMAL
- en: One way to do this is with 1×1 bottleneck convolutions. But, the authors of
    the paper argue that if this convolution is followed by non-linearity like ReLU,
    this might lead to a loss of manifold information. If the ReLU input is larger
    than 0, then the output of this unit is equivalent to the linear transformation
    of the input. But, if the input is smaller, then the ReLU collapses and the information
    of that unit is lost. Because of this, MobileNet uses 1×1 bottleneck convolution
    without non-linear activation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverted residuals**: In the *Residual networks* section, we introduced the
    bottleneck residual block, where the data flow in the non-shortcut path is **input
    -> 1×1 bottleneck conv -> 3×3 conv -> 1×1 unsampling conv**. In other words, it
    follows a **wide -> narrow -> wide** data representation. The authors argue that
    *the bottlenecks actually contain all the necessary information, while an expansion
    layer acts merely as an implementation detail that accompanies a non-linear transformation
    of the tensor*. Because of this, they propose having shortcut connections between
    the bottleneck connections instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these properties, the MobileNet model is composed of the following
    building blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f84ccba9-bb59-4d84-bcd9-3701fd3b9e62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Top: inverted residual block with stride 1\. Bottom: stride 2 block'
  prefs: []
  type: TYPE_NORMAL
- en: The model uses ReLU6 non-linearity: ReLU6 = min(max(input, 0),6). The maximum
    activation value is limited to 6—in this way, the non-linearity is more robust
    in low-precision floating-point computations. That's because 6 can take at most
    3 bits, leaving the rest for the floating-point portion of the number.
  prefs: []
  type: TYPE_NORMAL
- en: Besides stride, the blocks are described by an expansion factor, *t*, which
    determines the expansion ratio of the bottleneck convolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the relationship between the input and output dimensions
    of the blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/deac5fcf-f98e-40f8-b066-5e2f656d142a.png)'
  prefs: []
  type: TYPE_IMG
- en: The input and output dimensions relationship. Source: https://arxiv.org/abs/1801.04381
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding table, **h** and **w** are the input height and width, **s** is
    the stride, and **k** and **k'** are the input and output number of channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, here is the full model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4f0756c-e7fb-4b20-a32a-683e628747be.png)'
  prefs: []
  type: TYPE_IMG
- en: The MobileNetV2 architecture. Source: https://arxiv.org/abs/1801.04381
  prefs: []
  type: TYPE_NORMAL
- en: Each line describes a group of one or more identical blocks, repeated *n* times.
    All layers in the same group have the same number, **c**, of output channels.
    The first layer of each sequence has a stride, **s**, and all others use stride
    1\. All spatial convolutions use 3 × 3 kernels. The expansion factor, **t**, is
    always applied to the input size, as described in the preceding table.
  prefs: []
  type: TYPE_NORMAL
- en: The next model we'll discuss is a network model with a new type of building
    block, where all layers are interconnected.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to DenseNets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DenseNet (*Densely Connected Convolutional Networks*, [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)) try
    to alleviate the vanishing gradient problem and improve feature propagation, while
    reducing the number of network parameters. We''ve already seen how ResNets introduce
    residual blocks with skip connections to solve this. DenseNets take some inspiration
    from this idea and take it even further with the introduction of dense blocks.
    A dense block consists of sequential convolutional layers, where any layer has
    a direct connection to all subsequent layers. In other words, a network layer, *l*, will
    receive input, **x***[l]*, from all preceding network layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c100639-f0e1-4f6e-8fd0-f7df5a9cacad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/72b34058-eb37-45f3-b58e-6714361f0560.png) are the **concatenated **output
    feature maps of the preceding network layers. This is unlike ResNets, where we
    combine different layers with the element-wise sum. *H[l]* is a composite function,
    which defines three types of DenseNet blocks (only two are displayed):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e47bdbbc-aaad-41b4-9371-2498f5bbe3c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A dense block: the dimensionality-reduction layers (dashed lines) are part
    of the DenseNet-B architecture, while DenseNet-A doesn''t have them. DenseNet-C
    is not displayed'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DenseNet-A**: This is the base block, where *H[l]* consists of batch normalization,
    followed by activation, and a 3×3 convolution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2934a1ea-ee3a-404e-8913-e08533fd1c18.png)'
  prefs: []
  type: TYPE_IMG
- en: '**DenseNet-B**: The authors also introduced a second type of dense block, DenseNet-B,
    which applies a dimensionality-reduction 1×1 convolution after each concatenation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*![](img/7f2113f2-433e-409a-bca9-51c15c338392.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**DenseNet-C**: A further modification, which adds a downsampling 1×1 convolution
    after each dense block. The combination of B and C is referred to as DenseNet-BC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense block is specified by its number of convolutional layers and the output
    volume depth of each layer, which is called the **growth rate** in this context.
    Let's assume that the input of the dense block has a volume depth of *k[0]* and
    the output volume depth of each convolutional layer is *k*. Then, because of the
    concatenation, the input volume depth for the *l*-th layer will be *k[0]+k[x](l
    − 1)*. Although the later layers of a dense block have a large input volume depth
    (because of the many concatenations), DenseNets can work with growth rate values
    as low as 12, which reduces the total number of parameters. To understand why
    this works, let's think of the feature maps as the **collective knowledge** (or
    global state) of the network. Each layer adds its own *k* feature maps to this
    state, and the growth rate determines the amount of information the layer contributes
    to it. Because of the dense structure, the global state can be accessed from everywhere
    within the network (hence the term global). In other words, there is no need to
    replicate it from one layer to the next as in traditional network architectures,
    which allows us to start with a smaller number of feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: To make concatenation possible, dense blocks use padding in such a way that
    the height and width of all output slices are the same throughout the block. But
    because of this, downsampling is not possible within a dense block. Therefore, a
    dense network consists of multiple sequential dense blocks, separated by downsampling
    pooling operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the paper have proposed a family of DenseNets, whose overall
    architecture resembles ResNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8eed3144-e67e-4370-ac24-9c13d1bdbcd1.png)'
  prefs: []
  type: TYPE_IMG
- en: The family of DenseNet networks. Source: https://arxiv.org/abs/1608.06993
  prefs: []
  type: TYPE_NORMAL
- en: 'They have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a 7×7 stride 2 downsampling convolution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A further downsampling 3×3 max pooling with stride 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four groups of DenseNet-B blocks. The family of networks differs by the number
    of dense blocks within each group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downsampling is handled by a transition layer of a 2×2 pooling operation with
    stride 2 between the dense groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transition layer contains a further 1×1 bottleneck convolution to reduce
    the number of feature maps. The compression ratio of this convolution is specified
    by a hyper-parameter, *θ*, where *0* < *θ* ≤ *1*. If the number of input feature
    maps is *m*, then the number of output feature maps is *θm.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dense blocks end up with a 7×7 global average pooling, followed by a 1,000-unit
    fully connected softmax layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors of DenseNet have also released an improved DenseNet model called
    MSDNet (*Multi-Scale Dense Networks for Resource Efficient Image Classification*, [https://arxiv.org/abs/1703.09844](https://arxiv.org/abs/1703.09844)),
    which (as the name suggests) uses multi-scale dense blocks.
  prefs: []
  type: TYPE_NORMAL
- en: With DenseNet, we conclude our discussion about conventional CNN architectures.
    In the next section, we'll discuss whether it's possible to automate the process
    of finding the optimal NN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The workings of neural architecture search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NN models we've discussed so far were designed by their authors. But, what
    if we could make the computer itself design the NN? Enter **neural architecture
    search** (**NAS**)—a technique that automates the design of NNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue, let''s see what the network architecture consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: The graph of operations, which represents the network. As we discussed in [Chapter
    1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and bolts of Neural
    Networks*, the operations include (but are not limited to) convolutions, activation
    functions, fully connected layers, normalization, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters of each operation. For example, the convolution parameters are:
    type (cross-channel, depthwise, and so on), input dimensions, number of input
    and output slices, stride, and padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we'll discuss gradient-based NAS with reinforcement learning
    (*Neural Architecture Search with Reinforcement Learning*, [https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578)).
    At this point, we won't discuss reinforcement learning, and we'll focus on the
    algorithm instead. It starts with the premise that we can represent the network
    definition as a string (a sequence of tokens). Let's assume that we'll generate
    a sequential CNN, which consists only of convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, part of the string definition will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd4ee96f-aa67-4c50-b853-a98045890dbd.png)'
  prefs: []
  type: TYPE_IMG
- en: We don't have to specify the layer type, because we only use convolutions. We
    exclude padding for the sake of simplicity. The subscript text on the first line
    is included for clarity, but won't be included in the algorithmic version.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the algorithm overview in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c0eb578-01b6-4814-881f-79cb79301681.png)'
  prefs: []
  type: TYPE_IMG
- en: NAS overview. Source: https://arxiv.org/abs/1611.01578
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the controller. It is an RNN, whose task is to generate new
    network architectures. Although we haven't yet discussed RNNs (this honor won't
    come until [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml), *Understanding
    Recurrent Networks*), we'll try to explain how it works nevertheless. In [Chapter
    1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts of Neural
    Networks*, we mentioned that an RNN maintains an internal state—a summary of all
    its previous inputs. Based on that internal state and the latest input sample,
    the network generates a new output, updates its internal state, and waits for
    the next input.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the controller will generate the string sequence, which describes the
    network architecture. The controller output is a single token of the sequence.
    This could be filter height/width, stride width/height, or the number of output
    filters. The type of token depends on the length of the currently generated architecture.
    Once we have this token, we feed it back to the RNN controller as input. Then,
    the network generates the next token of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36bb6544-b95c-41a3-879b-78160ed1fd28.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating a network architecture with the RNN controller. The output token
    is fed back to the controller as input to generate the next token. Source: https://arxiv.org/abs/1611.01578
  prefs: []
  type: TYPE_NORMAL
- en: The white vertical squares in the diagram represent the RNN controller, which
    consists of a two-layer **Long short-term memory** (**LSTM**) cell (along the
    *y*-axis). Although the diagram shows multiple instances of the RNN (along the
    *x*-axis), it is in fact the same network; it's just **unfolded** in time, to
    represent the process of sequence generation. That is, each step along the *x*-axis
    represents a single token of the network definition. A token prediction at step
    *t* is carried out by a softmax classifier and then fed as controller input at
    step *t+1*. We continue this process until the length of the generated network
    reaches a certain value. Initially, this value is small (a short network), but
    it gradually increases (a longer network) as the training progresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand NAS, let''s see a step-by-step execution of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: The controller generates a new architecture, *A*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It builds and **trains** a new network with said architecture until it converges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It tests the new network on a withheld part of the training set and measures
    the error, *R*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It uses this error to update the controller parameters, *θ[c]*. As our controller
    is RNN, this means training the network and adjusting its weights. The model parameters
    are updated in such a way as to reduce the error, *R*, of the future architectures.
    This is made possible by a reinforcement learning algorithm called REINFORCE,
    which is beyond the scope of this section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It repeats these steps until the error, *R*, of the generated network falls
    below a certain threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The controller can generate network architectures with some restrictions. As
    we mentioned earlier in this section, the most severe is that the generated network
    only consists of convolutional layers. To simplify things, each convolutional
    layer automatically includes batch normalization and ReLU activation. But in theory,
    the controller could generate more complex architectures with other layers such
    as pooling or normalization. We could implement this by adding additional controller
    steps in the architecture sequence for the layer type.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the paper implemented a technique that allows us to add residual
    skip connections to the generated architecture. It works with a special type of
    controller step called an anchor point. The anchor point at layer *N* has content-based
    sigmoids. The output of a sigmoid *j (j = 1, 2, 3, ..., N-1)* represents the probability
    that the current layer has a residual connection to layer *j*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified controller is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6365b957-b098-48ef-b4fc-95a9c8460a5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'RNN controller with anchor points for the residual connections. Source: https://arxiv.org/abs/1611.01578'
  prefs: []
  type: TYPE_NORMAL
- en: 'If one layer has many input layers, all inputs are concatenated along the channel
    (depth) dimension. Skip connections could create some issues in the network design:'
  prefs: []
  type: TYPE_NORMAL
- en: The first hidden layer of the network (that is, the one not connected to any
    other input layer) uses the input image as an input layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the network, all layer outputs that have not been connected are
    concatenated in a final hidden state, which is sent to the classifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may happen that the outputs to be concatenated have different sizes. In that
    case, the smaller feature maps are padded with zeros to match the size of the
    bigger ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In their experiment, the authors used a controller with a 2-layer LSTM cell
    with 35 units in each layer. For every convolution, the controller has to select
    a filter height and width from the values {1, 3, 5, 7}, and a number of filters
    to be one of {24, 36, 48, 64}. Additionally, they performed 2 sets of experiments—one
    where the controller was allowed to select strides in {1, 2, 3} and another with
    a fixed stride of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Once the controller generates an architecture, the new network is trained for
    50 epochs on 45,000 images of the CIFAR-10 dataset. The remaining 5,000 images
    are used for validation. During training, the controller starts with an architecture
    depth of 6 layers and then increases the depth by 2 layers on every 1,600 iterations. The
    best performing model has a validation accuracy of 3.65%. It was discovered after
    12,800 architectures using 800 GPUs (wow!). The reason for these steep computational
    requirements is that each new network is trained from scratch just to produce
    one accuracy value, which can then be used to train the controller. More recently,
    the new ENAS algorithm (*Efficient Neural Architecture Search via Parameter Sharing*, [https://arxiv.org/abs/1802.03268](https://arxiv.org/abs/1802.03268))
    has made it possible to significantly reduce the computational resources of NAS
    by sharing the weights among the generated models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll discuss a novel type of NN, which tries to overcome
    some of the limitations of the CNNs we talked about so far.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing capsule networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capsule networks (*Dynamic Routing Between Capsules*, [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)) were
    introduced as a way to overcome some of the limitations of standard CNNs. To understand
    the idea behind capsule networks, we need to understand their limitations first,
    which we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with a quote from professor Hinton himself:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster."'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, CNNs are **translation-invariant**. Let's imagine a
    picture with a face, located in the right half of the picture. Translation invariance
    means that a CNN is very good at telling us that the picture contains a face,
    but it cannot tell us whether the face is in the left or right part of the image.
    The main culprit for this behavior is the pooling layers. Every pooling layer
    introduces a little translation invariance. For example, the max pooling routes
    forward the activation of only one of the input neurons, but the subsequent layers
    don't have any knowledge of which neuron is routed.
  prefs: []
  type: TYPE_NORMAL
- en: By stacking multiple pooling layers, we gradually increase the receptive field
    size. But, the detected object could be anywhere in the new receptive field, because
    none of the pooling layers relay such information. Therefore, we also increase
    the translation invariance. At first, this might seem to be a good thing, because
    the final labels have to be translation-invariant. But, it poses a problem, as
    CNNs cannot identify the position of one object relative to another. A CNN would
    identify both of the following images as a face, because they both contain the
    components of a face (a nose, mouth, and eyes) regardless of their relative positions
    to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also known as the **Picasso problem**, as demonstrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b264a39-059f-4898-8bbc-b3d5b423b1ff.png)'
  prefs: []
  type: TYPE_IMG
- en: A convolutional network would identify both of these images as a face
  prefs: []
  type: TYPE_NORMAL
- en: But, that's not all. A CNN would be confused even if the face had a different **orientation**,
    for example, if it was turned upside down. One way to overcome this is with data
    augmentation (rotation) during training. But, this only shows the limitations
    of the network. We have to explicitly show the object in different orientations
    and tell the CNN that this is, in fact, the same object.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've seen that a CNN discards the translation information (transitional
    invariance) and doesn't understand the orientation of an object. In computer vision,
    the combination of translation and orientation is known as the **pose**. The pose
    is enough to uniquely identify the object's properties in the coordinate system. Let's
    use computer graphics to illustrate this. A 3D object, say a cube, is entirely
    defined by its pose and the edge length. The process of transforming the representation
    of a 3D object into an image on the screen is called rendering. Knowing just its
    pose and the edge length of the cube, we can render it from any point of view
    we like.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if we can somehow train a network to understand these properties,
    we won't have to feed it with multiple augmented versions of the same object.
    A CNN cannot do that, because its internal data representation doesn't contain
    information about the object's pose (only about its type). In contrast, capsule
    networks **preserve information** for both the type and the pose of an object.
    Therefore, they can detect objects that can transform into each other, which is
    known as **equivariance**. We can also think of this as **reverse graphics**,
    that is, a reconstruction of the object's properties according to its rendered
    image.
  prefs: []
  type: TYPE_NORMAL
- en: To solve these problems, the authors of the paper propose a new type of network
    building block, called a **capsule**, instead of the neuron. Let's discuss it
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Capsules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The output of a capsule is a vector, compared to the output of a neuron, which
    is a scalar value. The capsule output vector carries the following meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: The elements of the vector represent the pose and other properties of the object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length of the vector is in the (0, 1) range and represents the probability
    of detecting the feature at that location. As a reminder, the length of a vector
    is [![](img/0164c9bd-513d-44ac-a282-cf9fc9a2293c.png)], where *v[i]* are the vector
    elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's consider a capsule that detects faces. If we start moving a face across
    an image, the values of the capsule vector will change to reflect the change in
    the position. However, its length will always stay the same, because the probability
    of the face doesn't change with the location.
  prefs: []
  type: TYPE_NORMAL
- en: Capsules are organized in interconnected layers, just like a regular network. The
    capsules in one layer serve as input to the capsules in the next. And, like a
    CNN, the shallower layers detect basic features, and the deeper layers combine
    them in more abstract and complex ones. But now, the capsules also relay positional
    information, instead of just detected objects. This allows the deeper capsules
    to analyze not only the presence of features, but also their relationship. For
    example, a capsule layer may detect a mouth, face, nose, and eyes. The subsequent
    capsule layer will be able to not only verify the presence of these features,
    but also whether they have the correct spatial relationship. Only if both conditions
    are true can the subsequent layer verify that a face is present. This is a high-level
    overview of capsule networks. Now, let's see how exactly capsules work.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the schematic of a capsule in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17593e60-cd8e-4ffd-8916-b875625f79c7.png)'
  prefs: []
  type: TYPE_IMG
- en: A capsule
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze it in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The capsule inputs are the output vectors, **u***[1],* **u***[2], ...* **u***[n]*,
    from the capsules of the previous layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We multiply each vector, **u***[i]*, by its corresponding weight matrix, *W[ij]*,
    to produce **pred****iction vectors**, [![](img/af47eced-a6b2-4d2c-9b72-529fc22462c1.png)].
    The weight matrices, **W**, encode spatial and other relationships between the
    lower-level features, coming from the capsules of the previous layer, and the
    high-level ones in the current layer. For example, imagine that the capsule in
    the current layer detects faces and the capsules from the previous layer detect
    the mouth (**u***[1]*), eyes (**u***[2]*), and nose (**u***[3]*). Then, [![](img/b4c26187-9be6-4b8e-8eb0-bc91e7ad1715.png)] is
    the predicted position of the face, given where the location of the mouth is.
    In the same way, [![](img/dcaec96e-5891-4d32-9a83-430790d1ff04.png)] predicts
    the location of the face based on the detected location of the eyes, and [![](img/309fb5d1-833a-4b43-b136-62fdd584760a.png)] predicts
    the location of the face based on the location of the nose. If all three lower-level
    capsule vectors agree on the same location, then the current capsule can be confident
    that a face is indeed present. We only used location for this example, but the
    vectors could encode other types of relationships between the features, such as
    scale and orientation. The weights, **W**, are learned with backpropagation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we multiply the [![](img/2970c647-8b48-4b8e-960d-c8616516740c.png)] vectors
    by the scalar coupling coefficients, *c[ij]*. These coefficients are a separate
    set of parameters, apart from the weight matrices. They exist between any two
    capsules, and indicate which high-level capsules will receive input from a lower-level
    capsule. But, unlike weight matrices, which are adjusted via backpropagation,
    coupling coefficients are computed on the fly during the forward pass via a process
    called **dynamic routing**. We'll describe it in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we perform the sum of the weighted input vectors. This step is similar to
    the weighted sum in neurons, but with vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ba2e77a3-d557-4965-a7cd-c66f0513762e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we''ll compute the output of the capsule, **v***[j]*, by squashing
    the vector, **s***[j]*. In this context, squashing means transforming the vector
    in such a way that its length comes in the (0, 1) range, without changing its
    direction. As mentioned, the length of the capsule vector represents the probability
    of the detected feature and squashing it in the (0, 1) range reflects that. To
    do this, the authors propose a novel formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/45aaa75e-ef3c-4427-8b4f-6ad373e78a0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we know the structure of the capsules, in the following section, we'll
    describe the algorithm to compute the coupling coefficients between capsules of
    different layers. That is, the mechanism by which they relay signals between one
    another.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s describe the dynamic routing process to compute the coupling coefficients, *c[ij]*,
    displayed in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3433e46b-78fc-42f1-a1f8-d187d0824843.png)'
  prefs: []
  type: TYPE_IMG
- en: Dynamic routing example. The grouped dots indicate lower-level capsules that
    agree with each other
  prefs: []
  type: TYPE_NORMAL
- en: We have a lower-level capsule, *I*, that has to decide whether to send its output
    to one of two higher-level capsules, *J* and *K*. The dark and light dots represent
    prediction vectors, ![](img/52b505fd-dae9-4348-8af6-c50be798ceab.png)and ![](img/d8f4d069-59c4-418d-8641-0644a4c1a5c5.png),
    which *J* and *K* have already received from other lower-level capsules. The arrows
    from the *I* capsule to the *J* and *K* capsules point to the ![](img/18952087-30c0-49a3-a1de-6641307e87f4.png) and
    ![](img/017be278-853c-4f1b-936c-0dfcbc6e49b3.png) prediction vectors from *I*
    to *J* and *K*. The clustered prediction vectors (lighter dots) indicate lower-level
    capsules that agree with each other with regard to the high-level feature. For
    example, if the *K* capsule describes a face, then the clustered predictions would
    indicate lower-level features, such as a mouth, nose, and eyes. Conversely, the
    dispersed (darker) dots indicate disagreement. If the *I* capsule predicts a vehicle
    tire, it would disagree with the clustered predictions in *K*.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the clustered predictions in *J* represent features such as headlights,
    windshield, or fenders, then the prediction of *I* would be in agreement with
    them. The lower-level capsules have a way of determining whether they fall into
    the clustered or dispersed group of each higher-level capsule. If they fall into
    the clustered group, they will increase the corresponding coupling coefficient
    with that capsule and will route their vector in that direction. Conversely, if
    they fall into the dispersed group, the coefficient will decrease.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s formalize this knowledge with a step-by-step algorithm, introduced by
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: For all *i* capsules in the *l* layer, and *j* capsules in the *(l + 1)* layer,
    we'll initialize [![](img/d0af0750-02cf-491b-9bcd-1965a7670f14.png)], where *b[ij]* is
    a temporary variable equivalent to *c[ij]*. The vector representation of all *b[ij]* is
    **b**[*i*]. At the start of the algorithm, the *i* capsule has an equal chance
    of routing its output to any of the capsules of the *(l + 1)* layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat for *r* iterations, where *r* is a parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For all *i* capsules in the *l* layer: [![](img/ac9bd249-5a93-4e77-9f99-f9583cef4ac8.png)]. The
    sum of all outgoing coupling coefficients, *c[i]*, of a capsule amounts to 1 (they
    have a probabilistic nature), hence the softmax.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For all *j* capsules in the *(l + 1)* layer: [![](img/5c2be571-0a64-4f10-be1c-3c7574a40c67.png)].
    That is, we''ll compute all non-squashed output vectors of the *(l + 1)* layer.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For all *j* capsules in the *(l + 1)* layer, we''ll compute the squashed vectors:
    [![](img/c045dd46-4691-40a5-b186-a2ad864ccd04.png)].'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For all *i* capsules in the *l* layer, and *j* capsules in the *(l + 1)* layer:
    [![](img/022f06a1-e7e3-446a-bbf5-8c55e5f72cfa.png)]. Here, [![](img/3768d012-866d-44cb-8efd-bb6d3210fa24.png)]is
    the dot product of the prediction vector of the low-level *i* capsule and the
    output vector of the high-level *j* capsule vectors. If the dot product is high,
    then the *i* capsule is in agreement with the other low-level capsules, which
    route their output to the *j* capsule, and the coupling coefficient increases.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The authors have recently released an updated dynamic routing algorithm using
    a clustering technique called expectation-maximization. You can read more about
    it in the original paper,*Matrix capsules with EM routing* ([https://ai.google/research/pubs/pub46653](https://ai.google/research/pubs/pub46653)).
  prefs: []
  type: TYPE_NORMAL
- en: The structure of the capsule network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll describe the structure of the capsule network, which
    the authors used to classify the MNIST dataset. The input of the network is the
    28×28 MNIST grayscale images, and the following are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with a single convolutional layer with 256 9×9 filters, stride 1,
    and ReLU activation. The shape of the output volume is (256, 20, 20).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have another convolutional layer with 256 9×9 filters and stride 2\. The
    shape of the output volume is (256, 6, 6).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the output of the layer as a foundation for the first capsule layer, called
    `PrimaryCaps`. Take the (256, 6, 6) output volume and split it into 32 separate
    (8, 6, 6) blocks. That is, each of the 32 blocks contains eight 6×6 slices. Take
    one activation value with the same coordinates from each slice and combine these
    values in a vector. For example, we can take activation (3, 7) of slice 1, (3,
    7) of slice 2, and so on and combine them in a vector with a length of 8\. We'll
    have 36 of these vectors. Then, we'll **transform** each vector into a capsule
    for a total of 36 capsules. The shape of the output volume of the `PrimaryCaps`
    layer is (32, 8, 6, 6).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second capsule layer is called `DigitCaps`. It contains 10 capsules (1 per
    digit), whose output is a vector with length 16. The shape of the output volume
    of the `DigitCaps` layer is (10, 16). During inference, we compute the length
    of each `DigitCaps` capsule vector. We then take the capsule with the longest
    vector as the prediction result of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During training, the network includes three additional, fully connected layers
    after `DigitCaps`, the last of which has 784 neurons (28×28). In the forward training
    pass, the longest capsule vector serves as input to these layers. They try to
    reconstruct the original image, starting from that vector. Then, the reconstructed
    image is compared to the original one and the difference serves as additional
    regularization loss for the backward pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Capsule networks are a new and promising approach to computer vision. However,
    they are not widely adopted yet and don't have an official implementation in any
    of the deep learning libraries discussed in this book, but you can find multiple
    third-party implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed some popular CNN architectures: we started with
    the classics, AlexNet and VGG. Then, we paid special attention to ResNets, as
    one of the most well-known network architectures. We also discussed the various
    incarnations of Inception networks and the Xception and MobileNetV2 models, which
    are related to them. We also talked about the exciting new ML area of neural architecture
    search. Finally, we discussed capsule networks—a new type of CV network, which
    tries to overcome some of the inherent CNN limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: We've already seen how to apply these models in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, where we employed ResNet and MobileNet
    in a transfer learning scenario for a classification task. In the next chapter,
    we'll see how to apply some of them to more complex tasks such as object detection
    and image segmentation.
  prefs: []
  type: TYPE_NORMAL
