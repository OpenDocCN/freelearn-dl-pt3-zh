- en: Advanced Convolutional Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级卷积网络
- en: In [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding Convolutional
    Networks*, we discussed the building blocks of **convolutional neural networks**
    (**CNNs**) and some of their properties. In this chapter, we'll go a step further
    and talk about some of the most popular CNN architectures. These networks usually
    combine multiple primitive convolution and/or pooling operations in a novel building
    block that serves as a base for a complex architecture. This allows us to build
    very deep (and sometimes wide) networks with high representational power that
    perform well on complex tasks such as ImageNet classification, image segmentation,
    speech recognition, and so on. Many of these models were first released as participants
    in the ImageNet challenge, which they usually won. To simplify our task, we'll
    discuss all architecture within the context of image classification. We'll still
    discuss more complex tasks, but we'll do it in [Chapter 4](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml),
    *Object Detection and Image Segmentation*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)，《理解卷积神经网络》中，我们讨论了**卷积神经网络**（**CNN**）的基本构件及其一些特性。在本章中，我们将更进一步，讨论一些最受欢迎的CNN架构。这些网络通常将多个基础卷积和/或池化操作结合在一个新的构件中，作为复杂架构的基础。这使得我们能够构建非常深（有时也很宽）的网络，具有较高的表示能力，能够在复杂任务中表现良好，如ImageNet分类、图像分割、语音识别等。许多这些模型最初是作为ImageNet挑战赛的参与者发布的，并且通常获得了胜利。为了简化任务，我们将在图像分类的背景下讨论所有架构。我们仍会讨论更复杂的任务，但会在[第四章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)，《目标检测与图像分割》中进行讨论。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing AlexNet
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍AlexNet
- en: An introduction to Visual Geometry Group
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Visual Geometry Group简介
- en: Understanding residual networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解残差网络
- en: Understanding Inception networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Inception网络
- en: Introducing Xception
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Xception
- en: Introducing MobileNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍MobileNet
- en: An introduction to DenseNets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet简介
- en: The workings of neural architecture search
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经架构搜索的工作原理
- en: Introducing capsule networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍胶囊网络
- en: Introducing AlexNet
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍AlexNet
- en: The first model we'll discuss is the winner of the 2012 **ImageNet Large Scale
    Visual Recognition Challenge** (**ILSVRC**, or simply ImageNet). It's nicknamed
    AlexNet (*ImageNet Classification with Deep Convolutional Neural Networks*, [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)),
    after one of its authors, Alex Krizhevsky. Although this model is rarely used
    nowadays, it's an important milestone in contemporary deep learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个模型是2012年**ImageNet大规模视觉识别挑战**（**ILSVRC**，简称ImageNet）的冠军。这个模型被称为AlexNet（*通过深度卷积神经网络进行ImageNet分类*，[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)），以其作者之一Alex
    Krizhevsky命名。虽然现在这个模型很少使用，但它是当代深度学习中的一个重要里程碑。
- en: 'The following diagram shows the network architecture:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了网络架构：
- en: '![](img/ec08175c-5282-4be2-b6e7-6f2d99272166.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec08175c-5282-4be2-b6e7-6f2d99272166.png)'
- en: The AlexNet architecture. The original model was split in two, so it can fit
    on the memory of two GPUs
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet架构。原始模型被拆分为两个部分，以便它可以适应两张GPU的内存
- en: The model has five cross-correlated convolutional layers, three overlapping
    max pooling layers, three fully connected layers, and ReLU activations. The output
    is a 1,000-way softmax (one for each ImageNet class). The first and second convolutional
    layers use local response normalization—a type of normalization, somewhat similar
    to batch normalization. The fully connected layers have a dropout rate of 0.5\.
    To prevent overfitting, the network was trained using random 227×227 crops of
    the 256×256 input images. The network achieves top-1 and top-5 test set error
    rates of 37.5% and 17.0%.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有五层交叉相关的卷积层，三层重叠的最大池化层，三层全连接层，以及ReLU激活。输出是一个1,000维的softmax（每个对应一个ImageNet类别）。第一层和第二层卷积使用局部响应归一化——这是一种归一化方法，类似于批量归一化。全连接层的dropout率为0.5。为了防止过拟合，网络使用随机227×227的裁剪图像（来自256×256的输入图像）进行训练。该网络在测试集上实现了37.5%的top-1误差率和17.0%的top-5误差率。
- en: In the next section, we'll discuss an NN architecture that was introduced by Oxford's
    Visual Geometry Group in 2014, when it became a runner-up in the ImageNet challenge
    of that year.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将讨论由牛津大学Visual Geometry Group于2014年提出的一个神经网络架构，该架构在当年的ImageNet挑战赛中获得了亚军。
- en: An introduction to Visual Geometry Group
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Visual Geometry Group（视觉几何组）简介
- en: The next architecture we're going to discuss is **Visual Geometry Group** (**VGG**)
    (from Oxford's Visual Geometry Group, *Very Deep Convolutional Networks for Large-Scale
    Ima*g*e* *Recognition*, [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)[)](https://arxiv.org/abs/1409.1556).
    The VGG family of networks remains popular today and is often used as a benchmark
    against newer architectures. Prior to VGG (for example, LeNet-5: [http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/) and
    AlexNet), the initial convolutional layers of a network used filters with large
    receptive fields, such as 11×11\. Additionally, the networks usually had alternating
    single convolutional and pooling layers. The authors of the paper observed that
    a convolutional layer with a large filter size can be replaced with a stack of
    two or more convolutional layers with smaller filters (factorized convolution).
    For example, we can replace one 5×5 layer with a stack of two 3×3 layers, or a
    7×7 layer with a stack of three 3×3 layers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要讨论的架构是**视觉几何组**（**VGG**）（来自牛津大学的视觉几何组，*非常深的卷积网络用于大规模图像识别*，[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)）。VGG系列网络至今仍然非常流行，且常常作为新的架构的基准。VGG之前的网络（例如，LeNet-5：[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)和AlexNet）中，网络的初始卷积层使用的是大感受野的滤波器，如11×11。此外，网络通常由交替的单一卷积层和池化层组成。论文的作者观察到，大滤波器的卷积层可以被堆叠的多个小滤波器的卷积层所替代（因式分解卷积）。例如，我们可以用两层3×3的卷积层代替一个5×5的层，或者用三层3×3的卷积层代替一个7×7的层。
- en: 'This structure has several advantages, as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构有几个优点，具体如下：
- en: The neurons of the last of the stacked layers have the equivalent receptive
    field size of a single layer with a large filter.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠层的最后一个神经元具有与单层大滤波器相同的感受野大小。
- en: The number of weights and operations of stacked layers is smaller, compared
    to a single layer with a large filter size. Let's assume we want to replace one
    5×5 layer with two 3×3 layers. Let's also assume that all layers have an equal
    number of input and output channels (slices), *M*. The total number of weights
    (excluding biases) of the 5×5 layer is *5*5*M*M = 25*M²*. On the other hand, the
    total weights of a single 3×3 layer is *3*3*M*M = 9*M²*, and simply *2*(3*3*M*M)
    = 18*M²* for two layers, which makes this arrangement 28% more efficient (18/25
    = 0.72). The efficiency will increase further with larger filters.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与单一大滤波器的层相比，堆叠层的权重和操作数较少。假设我们想用两个3×3层替代一个5×5层。假设所有层的输入和输出通道数（切片）相同，记为*M*。5×5层的总权重数（不包括偏置）为*5*5*M*M
    = 25*M²*。另一方面，单个3×3层的总权重为*3*3*M*M = 9*M²*，两个层的权重总数为*2*(3*3*M*M) = 18*M²*，使得这种安排比单层大滤波器更高效28%（18/25
    = 0.72）。当滤波器更大时，效率会进一步提高。
- en: Stacking multiple layers makes the decision function more discriminative.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠多个层使得决策函数更具判别力。
- en: 'The VGG networks consist of multiple blocks of two, three, or four stacked
    convolutional layers combined with a max pooling layer. We can see the two most
    popular variants, **VGG16** and **VGG19**, in the following table:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: VGG网络由多个堆叠的两层、三层或四层卷积层和最大池化层组成。我们可以在下表中看到两种最流行的变体，**VGG16**和**VGG19**：
- en: '![](img/ab51fa0e-ccbf-4a89-b3c7-f76561637925.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab51fa0e-ccbf-4a89-b3c7-f76561637925.png)'
- en: Architecture of the VGG16 and VGG19 networks, named after the number of weighted
    layers in each network
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16和VGG19网络的架构，命名来源于每个网络中的加权层数
- en: As the depth of the VGG network increases, so does the width (the number of
    filters) in the convolutional layers. We have multiple pairs of cross-channel
    convolutions with a volume depth of 128/256/512 connected to other layers with
    the same depth. In addition, we also have two 4,096-unit fully connected layers,
    followed by a 1000-unit fully connected layer and a softmax (one for each ImageNet
    class). Because of this, the VGG networks have a large number of parameters (weights),
    which makes them memory-inefficient, as well as computationally expensive. Still,
    this is a popular and straightforward network architecture, which has been further
    improved by the addition of batch normalization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随着VGG网络深度的增加，卷积层的宽度（滤波器的数量）也增加。我们有多个交叉通道卷积对，卷积深度为128/256/512，连接到其他相同深度的层。此外，我们还有两个4,096单元的完全连接层，后接一个1,000单元的完全连接层和一个softmax（每个ImageNet类一个）。因此，VGG网络有大量的参数（权重），这使得它们在内存使用上效率低下，且计算成本高昂。尽管如此，这仍然是一个流行且简单的网络架构，后来通过加入批归一化（batch
    normalization）得到了进一步的改进。
- en: In the next section, we'll use VGG as an example of how to load pretrained network
    models with TensorFlow and PyTorch.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用VGG作为示例，展示如何使用TensorFlow和PyTorch加载预训练的网络模型。
- en: VGG with PyTorch and TensorFlow
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch和TensorFlow的VGG
- en: Both PyTorch and TensorFlow have pretrained VGG models. Let's see how to use
    them.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch和TensorFlow都有预训练的VGG模型。我们来看一下如何使用它们。
- en: 'Keras is an official part of TensorFlow 2, therefore, we''ll use it to load
    the model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是TensorFlow 2的官方部分，因此我们将使用它来加载模型：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By setting the `weights='imagenet'` parameter, the network will be loaded with
    pretrained ImageNet weights (they will be downloaded automatically). You can set
    `include_top` to `False`, which will exclude the fully connected layers for a
    transfer learning scenario. In this case, you can also use an arbitrary input
    size by setting a tuple value to `input_shape`—the convolutional layers will automatically
    scale to match the desired input shape. This is possible because the convolution
    filter is shared along the whole feature map. Therefore, we can use the same filter on
    feature maps with different sizes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`weights='imagenet'`参数，网络将加载预训练的ImageNet权重（它们会自动下载）。你可以将`include_top`设置为`False`，以排除完全连接层，适用于迁移学习场景。在这种情况下，你还可以通过设置`input_shape`为一个元组值，来使用任意输入大小——卷积层将自动调整以匹配所需的输入形状。这之所以可行，是因为卷积滤波器在整个特征图中是共享的。因此，我们可以在具有不同大小的特征图上使用相同的滤波器。
- en: 'We''ll continue with PyTorch, where you can choose whether you want to use
    a pretrained model (again, with automatic download):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用PyTorch，你可以选择是否使用预训练模型（同样会自动下载）：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can try other pretrained models, using the same procedures we described.
    To avoid repetition, we won't include the same code examples for the other architectures
    in this section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试其他预训练模型，使用我们描述的相同流程。为了避免重复，我们不会在这一节中为其他架构提供相同的代码示例。
- en: In the next section, we'll discuss one of the most popular CNN architectures,
    which was released after VGG.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论VGG发布后最流行的卷积神经网络（CNN）架构之一。
- en: Understanding residual networks
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解残差网络
- en: Residual networks (**ResNets**, *Deep Residual Learning for Image Recognition*, [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))
    were released in 2015, when they won all five categories of the ImageNet challenge
    that year. In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts
    and Bolts of Neural* *Networks*, we mentioned that the layers of a neural network
    are not restricted to sequential order, but form a graph instead. This is the
    first architecture we'll learn, which takes advantage of this flexibility. This
    is also the first network architecture that has successfully trained a network
    with a depth of more than 100 layers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络（**ResNets**，*深度残差学习用于图像识别*， [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)）于2015年发布，并在当年赢得了ImageNet挑战赛的五个类别的冠军。在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本原理*中，我们提到神经网络的层并不受限于顺序排列，而是形成了一个图结构。这是我们将要学习的第一个架构，它利用了这种灵活性。这也是第一个成功训练了超过100层深度网络的网络架构。
- en: Thanks to better weight initializations, new activation functions, as well as
    normalization layers, it's now possible to train deep networks. But, the authors
    of the paper conducted some experiments and observed that a network with 56 layers
    had higher training and testing errors compared to a network with 20 layers. They
    argue that this should not be the case. In theory, we can take a shallow network
    and stack identity layers (these are layers whose output just repeats the input)
    on top of it to produce a deeper network that behaves in exactly the same way
    as the shallow one. Yet, their experiments have been unable to match the performance
    of the shallow network.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于更好的权重初始化、新的激活函数以及归一化层，现在可以训练深度网络了。但是，论文的作者进行了一些实验，观察到一个56层的网络在训练和测试误差上都高于一个20层的网络。他们认为这种情况不应该发生。从理论上讲，我们可以使用一个浅层网络，并在其上堆叠恒等层（这些层的输出仅重复输入），来构造一个深度网络，该网络的表现与浅层网络完全相同。然而，他们的实验未能使深度网络的表现达到浅层网络的水平。
- en: 'To solve this problem, they proposed a network constructed of residual blocks.
    A residual block consists of two or three sequential convolutional layers and
    a separate parallel identity (repeater) shortcut connection, which connects the
    input of the first layer and the output of the last one. We can see three types
    of residual blocks in the following screenshot:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，他们提出了由残差模块构成的网络。一个残差模块由两个或三个连续的卷积层和一个独立的并行恒等（重复器）快捷连接组成，它连接了第一个层的输入和最后一个层的输出。我们可以在以下截图中看到三种类型的残差模块：
- en: '![](img/9629b25c-e912-469b-83c7-9d1f4058a249.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9629b25c-e912-469b-83c7-9d1f4058a249.png)'
- en: 'From left to right: original residual block; original bottleneck residual block;
    pre-activation residual block; pre-activation bottleneck residual block'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右：原始残差模块；原始瓶颈残差模块；预激活残差模块；预激活瓶颈残差模块
- en: Each block has two parallel paths. The left-hand path is similar to the other
    networks we've seen, and consists of sequential convolutional layers + batch normalization.
    The right path contains the identity shortcut connection (also known as the skip
    connection). The two paths are merged via an element-wise sum. That is, the left
    and right tensors have the same shape and an element of the first tensor is added
    to the element in the same position in the second tensor. The output is a single
    tensor with the same shape as the input. In effect, we propagate forward the features
    learned by the block, but also the original unmodified signal. In this way, we
    can get closer to the original scenario, as described by the authors. The network
    can decide to skip some of the convolutional layers thanks to the skip connections,
    in effect reducing its own depth. The residual blocks use padding in such a way
    that the input and the output of the block have the same dimensions. Thanks to
    this, we can stack any number of blocks for a network with an arbitrary depth.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块有两个并行路径。左侧路径与我们见过的其他网络类似，由连续的卷积层 + 批归一化组成。右侧路径包含了恒等快捷连接（也称为跳跃连接）。这两个路径通过逐元素相加的方式进行合并。也就是说，左侧和右侧的张量具有相同的形状，第一个张量的一个元素会加到第二个张量中相同位置的元素上。输出是一个形状与输入相同的单一张量。实际上，我们向前传播的是模块学习到的特征，但也包括了原始的未修改信号。通过这种方式，我们可以更接近原始场景，正如作者所描述的那样。网络可以通过跳跃连接决定跳过一些卷积层，实际上减少了它自身的深度。残差模块使用了填充技术，使得输入和输出的尺寸相同。得益于此，我们可以堆叠任意数量的模块，从而实现任意深度的网络。
- en: 'And now, let''s see how the blocks in the diagram differ:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看图中不同模块的区别：
- en: The first block contains two 3×3 convolutional layers. This is the original
    residual block, but if the layers are wide, stacking multiple blocks becomes computationally
    expensive.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个模块包含两个3×3的卷积层。这是原始的残差模块，但如果层数较宽，堆叠多个模块会变得计算上很昂贵。
- en: The second block is equivalent to the first, but it uses the so-called bottleneck
    layer. First, we use a 1×1 convolution to downsample the input volume depth (we
    discussed this in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),* Understanding
    Convolutional Networks*). Then, we apply a 3×3 (bottleneck) convolution to the
    reduced input. Finally, we expand the output back to the desired depth with another
    1×1 convolution. This layer is less computationally expensive than the first.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个块与第一个块相同，但它使用了所谓的瓶颈层。首先，我们使用1×1卷积来下采样输入体积的深度（我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)，*理解卷积网络*中讨论了这一点）。然后，我们对缩小后的输入应用3×3（瓶颈）卷积。最后，我们使用另一个1×1卷积将输出恢复到所需的深度。该层的计算开销比第一个要小。
- en: The third block is the latest revision of the idea, published in 2016 by the
    same authors (*Identity Mappings in Deep Residual Networks*, [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)).
    It uses pre-activations, and the batch normalization and the activation function
    come before the convolutional layer. This may seem strange at first, but thanks
    to this design, the skip connection path can run uninterrupted throughout the
    network. This is contrary to the other residual blocks, where at least one activation
    function is on the path of the skip connection. A combination of stacked residual
    blocks still has the layers in the right order.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个块是该思想的最新修订版，由同一作者于2016年发布（*深度残差网络中的身份映射*，[https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)）。它使用了预激活，批量归一化和激活函数位于卷积层之前。这一设计起初可能显得有些奇怪，但正是由于这种设计，跳跃连接路径能够在整个网络中不间断地运行。这与其他残差块不同，后者至少有一个激活函数处于跳跃连接的路径上。堆叠的残差块组合仍然保持了正确的层级顺序。
- en: The fourth block is the bottleneck version of the third layer. It follows the
    same principle as the bottleneck residual layer v1.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四个块是第三层的瓶颈版本。它遵循与瓶颈残差层v1相同的原则。
- en: 'In the following table, we can see the family of networks proposed by the authors
    of the paper:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们可以看到论文作者提出的网络系列：
- en: '![](img/3bf80ec8-ffb0-4f87-964a-3db43e6b09d0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bf80ec8-ffb0-4f87-964a-3db43e6b09d0.png)'
- en: The family of the most popular residual networks. The residual blocks are represented
    by rounded rectangles
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的残差网络系列。残差块用圆角矩形表示
- en: 'Some of their properties are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的一些特性如下：
- en: They start with a 7×7 convolutional layer with stride 2, followed by 3×3 max-pooling.
    This layer also serves as a downsampling step—the rest of the network starts with
    a much smaller slice of 56×56, compared to 224×224 of the input.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们从一个7×7卷积层开始，步幅为2，接着是3×3的最大池化层。该层还充当了下采样步骤——网络的其余部分以一个更小的56×56切片开始，相较于输入的224×224。
- en: Downsampling in the rest of the network is implemented with a modified residual
    block with stride 2.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络其余部分的下采样是通过具有步幅2的修改残差块实现的。
- en: Average pooling downsamples the output after all residual blocks and before
    the 1,000-unit fully connected softmax layer.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均池化在所有残差块之后、1,000单元全连接softmax层之前进行下采样。
- en: The ResNet family of networks is popular not only because of their accuracy,
    but also because of their relative simplicity and the versatility of the residual
    blocks. As we mentioned, the input and output shape of the residual block can
    be the same due to the padding. We can stack residual blocks in different configurations
    to solve various problems with wide-ranging training set sizes and input dimensions.
    Because of this universality, we'll implement an example of ResNet in the next
    section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet系列网络不仅因其准确性而流行，还因为它们相对简单，并且残差块具有高度的通用性。正如我们之前提到的，残差块的输入和输出形状可以相同，这是由于填充操作。我们可以以不同的配置堆叠残差块，以解决各种问题，适应不同大小的训练集和输入维度。正因为这种通用性，我们将在下一节中实现一个ResNet示例。
- en: Implementing residual blocks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现残差块
- en: 'In this section, we''ll implement a pre-activation ResNet to classify the CIFAR-10
    images using PyTorch 1.3.1 and `torchvision` 0.4.2\. Let''s start:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个预激活ResNet，用于使用PyTorch 1.3.1和`torchvision` 0.4.2分类CIFAR-10图像。让我们开始吧：
- en: 'As usual, we''ll start with the imports. Note that we''ll use the shorthand
    `F` for the PyTorch functional module ([https://pytorch.org/docs/stable/nn.html#torch-nn-functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样，我们从导入库开始。请注意，我们将使用PyTorch功能模块的简写`F`（[https://pytorch.org/docs/stable/nn.html#torch-nn-functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)）：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, let''s define the pre-activation regular (non-bottleneck) residual block.
    We''ll implement it as `nn.Module`—the base class for all neural network modules.
    Let''s start with the class definition and the `__init__` method:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义预激活常规（非瓶颈）残差块。我们将其实现为`nn.Module`——所有神经网络模块的基类。我们从类定义和`__init__`方法开始：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will define only the learnable block components in the `__init__` method—these
    include the convolution and batch normalization operations. Also, note the way
    we implement the `shortcut` connection. If the input dimensions are the same as
    the output dimensions, we can directly use the input tensor as the shortcut. However,
    if the dimensions differ, we have to transform the input with the help of a 1×1
    convolution with the same stride and output channels as the one in the main path.
    The dimensions may differ either by height/width (`stride != 1`) or by depth (`in_slices
    != self.expansion * slices`). `self.expansion` is a hyper-parameter, which was
    included in the original ResNet implementation. It allows us to expand the output
    depth of the residual block.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`__init__`方法中仅定义可学习的块组件——这些组件包括卷积和批量归一化操作。另外，请注意我们如何实现`shortcut`连接。如果输入维度与输出维度相同，我们可以直接使用输入张量作为捷径连接。然而，如果维度不同，我们必须借助一个1×1卷积进行转换，卷积的步幅和输出通道与主路径中的卷积相同。维度的不同可能来源于高度/宽度（`stride
    != 1`）或者深度（`in_slices != self.expansion * slices`）。`self.expansion`是一个超参数，原始ResNet实现中包含了该参数，它允许我们扩展残差块的输出深度。
- en: 'The actual data propagation is implemented in the `forward` method (please
    mind the indentation, as it''s a member of `PreActivationBlock`):'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际的数据传播在`forward`方法中实现（请注意缩进，因为它是`PreActivationBlock`的成员）：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use the functional `F.relu` for the activation function, as it doesn't have
    learnable parameters. Then, if the shortcut connection is a convolution and not
    an identity (that is, the input/output dimensions of the block differ), we'll
    reuse `F.relu(self.bn_1(x))` to add non-linearity and batch normalization to the
    shortcut. Otherwise, we'll just repeat the input.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用函数式的`F.relu`作为激活函数，因为它没有可学习的参数。然后，如果捷径连接是卷积而不是恒等（也就是说，块的输入/输出维度不同），我们会重用`F.relu(self.bn_1(x))`来为捷径连接增加非线性和批量归一化。否则，我们只需重复输入。
- en: 'Then, let''s implement the bottleneck version of the residual block. We''ll
    use the same blueprint as in the non-bottleneck implementation. We''ll start with
    the class definition and the `__init__` method:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们实现残差块的瓶颈版本。我们将使用与非瓶颈实现相同的蓝图。我们从类定义和`__init__`方法开始：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `expansion` parameter is `4` after the original implementation. The `self.conv_1`
    convolution operation represents the 1×1 downsampling bottleneck connection, `self.conv_2`
    is the actual convolution, and `self.conv_3` is the upsampling 1×1 convolution.
    The shortcut mechanism follows the same logic as in `PreActivationBlock`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`expansion`参数在原始实现中为`4`。`self.conv_1`卷积操作表示1×1下采样瓶颈连接，`self.conv_2`是实际的卷积，`self.conv_3`是上采样的1×1卷积。捷径机制遵循与`PreActivationBlock`中相同的逻辑。'
- en: 'Next, let''s implement the `PreActivationBottleneckBlock.forward` method. Once
    again, it follows the same logic as the one in `PreActivationBlock`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现`PreActivationBottleneckBlock.forward`方法。它的逻辑与`PreActivationBlock`中的方法相同：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, let''s implement the residual network itself. We''ll start with the class
    definition (it inherits `nn.Module`) and the `__init__` method:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现残差网络本身。我们将从类定义开始（它继承自`nn.Module`）和`__init__`方法：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The network contains four groups of residual blocks, just like the original
    implementation. The number of blocks of each group is specified by the `num_blocks`
    parameter. The initial convolution uses a 3×3 filter with stride 1, as opposed
    to a 7×7 with stride 2 of the original implementation. This is because the 32×32 CIFAR-10
    images are much smaller than the 224×224 ImageNet ones, and the downsampling is
    unnecessary.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 网络包含四组残差块，和原始实现一样。每组中的块数由`num_blocks`参数指定。初始卷积使用3×3的滤波器，步幅为1，而原始实现中使用的是7×7，步幅为2。这是因为32×32的CIFAR-10图像比224×224的ImageNet图像要小得多，因此不需要下采样。
- en: 'Then, we''ll implement the `PreActivationResNet._make_group` method, which
    creates one residual block group. All blocks in the group have stride 1, except
    for the first, where `stride` is supplied as a parameter:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将实现`PreActivationResNet._make_group`方法，该方法创建一个残差块组。组中的所有块的步幅为1，只有第一个块的步幅由参数`stride`指定：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we''ll implement the `PreActivationResNet.forward` method, which propagates
    the data through the network. We can see the downsampling average pooling before
    the fully connected final layer:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现`PreActivationResNet.forward`方法，该方法通过网络传播数据。我们可以看到全连接层前的下采样平均池化：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once we''re done with the network, we can implement several ResNet configurations.
    The following is `ResNet34` with 34 convolution layers, grouped in `[3, 4, 6,
    3]` non-bottleneck residual blocks:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们完成了网络的构建，就可以实现多种ResNet配置。以下是`ResNet34`，它有34层卷积层，分组为`[3, 4, 6, 3]`非瓶颈残差块：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we can train the network. We''ll start by defining the train and test
    datasets. We won''t go into much detail about the implementation, as we''ve already
    looked at a similar scenario, in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*. We''ll augment the training set by padding
    the samples with four pixels, and then we''ll take random 32×32 crops out of it.
    The following is the implementation:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以训练网络。我们将首先定义训练和测试数据集。由于我们已经在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)
    *理解卷积网络*中看过类似的场景，这里不再详细讲解实现。我们将通过给样本填充四个像素来扩增训练集，然后从中随机裁剪出32×32的图像。以下是实现：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we''ll instantiate the network model and the training parameters—cross-entropy
    loss and the Adam optimizer:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化网络模型和训练参数——交叉熵损失和Adam优化器：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now train the network for `EPOCHS` epochs. The `train_model`, `test_model`,
    and `plot_accuracy` functions are the same as the ones we defined in the *Implementing
    transfer learning with PyTorch* section of [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, and we won''t repeat their implementation
    here. The following is the code:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以将网络训练`EPOCHS`轮。`train_model`、`test_model`和`plot_accuracy`函数与我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)
    *实现PyTorch迁移学习*部分中定义的相同，我们不会在此重复它们的实现。以下是代码：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And, in the following graph, we can see the test accuracy in 15 iterations
    (the training might take a while):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到15次迭代中的测试准确率（训练可能需要一些时间）：
- en: '![](img/ace8fd47-9786-401b-87fc-75cc65bba7e9.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ace8fd47-9786-401b-87fc-75cc65bba7e9.png)'
- en: ResNet34 CIFAR accuracy in 15 epochs
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet34 CIFAR在15轮训练中的准确率
- en: The code in this section is partially based on the pre-activation ResNet implementation
    in [https://github.com/kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码部分基于[https://github.com/kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar)中的预激活ResNet实现。
- en: In this section, we discussed the various types of ResNets, and then we implemented
    one with PyTorch. In the next section, we'll discuss Inception networks—yet another
    family of networks, which elevate the use of parallel connections to a new level.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们讨论了不同类型的ResNet，并用PyTorch实现了一个。在下一节中，我们将讨论Inception网络——另一类网络，它们将并行连接的使用提升到了一个新层次。
- en: Understanding Inception networks
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Inception网络
- en: Inception networks (*Going Deeper with Convolutions*, [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842))
    were introduced in 2014, when they won the ImageNet challenge of that year (there
    seems to be a pattern here). Since then, the authors have released multiple improvements
    (versions) of the architecture.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Inception网络（*通过卷积深入*，[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)）是在2014年提出的，当时它们赢得了当年的ImageNet挑战（这里似乎有个规律）。从那时起，作者们发布了该架构的多个改进版本。
- en: 'Fun fact: the name Inception comes in part from the **We need to go deeper**
    internet meme, related to the movie *Inception*.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实：Inception这个名字部分来源于**我们需要更深入**的网络迷因，这与电影*盗梦空间*有关。
- en: 'The idea behind Inception networks started from the basic premise that the
    objects in an image have different scales. A distant object might take up a small
    region of the image, but the same object, once nearer, might take up the majority
    of the image. This presents a difficulty for standard CNNs, where the neurons
    in the different layers have a fixed receptive field size as imposed on the input
    image. A regular network might be a good detector of objects at a certain scale,
    but could miss them otherwise. To solve this problem, the authors of the paper proposed
    a novel architecture: one composed of Inception blocks. An Inception block starts
    with a common input, and then splits it into different parallel paths (or towers).
    Each path contains either convolutional layers with a different-sized filter,
    or a pooling layer. In this way, we apply different receptive fields on the same
    input data. At the end of the Inception block, the outputs of the different paths
    are concatenated. In the next few sections, we''ll discuss the different variations
    of Inception networks.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网络背后的思想源于一个基本前提：图像中的物体具有不同的尺度。一个远处的物体可能占据图像的一个小区域，但当同一个物体靠近时，它可能占据图像的大部分。这对于标准的
    CNN 来说是一个难题，因为不同层中的神经元对输入图像的感受野大小是固定的。一个常规网络可能能很好地检测到某个尺度的物体，但在其他情况下可能会漏掉它们。为了解决这个问题，论文的作者提出了一种新型架构：由
    Inception 块组成的网络。Inception 块从一个共同的输入开始，然后将其分割成不同的并行路径（或塔）。每条路径包含不同大小滤波器的卷积层，或池化层。通过这种方式，我们对相同的输入数据应用不同的感受野。在
    Inception 块的末尾，不同路径的输出会被连接起来。在接下来的几个部分中，我们将讨论 Inception 网络的不同变种。
- en: Inception v1
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception v1
- en: 'The following diagram shows the first version of the Inception block, part
    of the GoogLeNet network architecture ([https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)).
    GoogLeNet contains nine such Inception blocks:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Inception 块的第一个版本，它是 GoogLeNet 网络架构的一部分（[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)）。GoogLeNet
    包含九个这样的 Inception 块：
- en: '![](img/532d7c33-b7ba-4743-a7f2-c44ef6b9c4e5.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/532d7c33-b7ba-4743-a7f2-c44ef6b9c4e5.png)'
- en: Inception v1 block, inspired by https://arxiv.org/abs/1409.4842
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v1 块，灵感来自于 [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)
- en: 'The v1 block has four paths:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: v1 块有四条路径：
- en: 1×1 convolution, which acts as a kind of repeater to the input
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，作为输入的某种中继器
- en: 1×1 convolution, followed by a 3×3 convolution
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，后接 3×3 卷积
- en: 1×1 convolution, followed by a 5×5 convolution
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，后接 5×5 卷积
- en: 3×3 max pooling with stride 1
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3×3 最大池化，步幅为 1
- en: The layers in the block use padding in such a way that the input and the output
    have the same shape (but different depths). The padding is also necessary, because
    each path would produce an output with a different shape, depending on the filter
    size. This is valid for all versions of Inception blocks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该块中的层使用填充方式，使输入和输出具有相同的形状（但深度不同）。填充也是必要的，因为每条路径会根据滤波器大小产生不同形状的输出。这对所有版本的 Inception
    块都适用。
- en: The other major innovation of this Inception block is the use of downsampling
    1×1 convolutions. They are needed because the output of all paths is concatenated
    to produce the final output of the block. The result of the concatenation is an
    output with a quadrupled depth. If another Inception block followed the current
    one, its output depth would quadruple again. To avoid such exponential growth,
    the block uses 1×1 convolutions to reduce the depth of each path, which in turn
    reduces the output depth of the block. This makes it possible to create deeper
    networks, without running out of resources.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Inception 块的另一个主要创新是使用下采样的 1×1 卷积。它们是必要的，因为所有路径的输出会被连接起来生成该块的最终输出。连接的结果是输出深度增加了四倍。如果接下来的
    Inception 块继续跟随当前块，它的输出深度将再次增加四倍。为了避免这种指数增长，该块使用 1×1 卷积来减少每条路径的深度，从而降低该块的输出深度。这使得可以创建更深的网络，而不会耗尽资源。
- en: GoogLeNet also utilizes auxiliary classifiers—that is, it has two additional
    classification outputs (with the same groundtruth labels) at various intermediate
    layers. During training, the total value of the loss is a weighted sum of the auxiliary
    losses and the real loss.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet 还利用了辅助分类器——也就是说，它在不同的中间层有两个额外的分类输出（具有相同的真实标签）。在训练过程中，总的损失值是辅助损失和真实损失的加权和。
- en: Inception v2 and v3
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception v2 和 v3
- en: Inception v2 and v3 were released together and propose several improvements
    over the original Inception block (*Rethinking the Inception Architecture for
    Computer Vision*, [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)).
    The first is the factorization of the 5×5 convolution in two stacked 3×3 convolutions.
    We discussed the advantages of this in the *Introduction to Visual Geometry Group* section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the new Inception block in the following diagram:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26826db8-c95e-41ee-a113-970221a8668e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Inception block A, inspired by https://arxiv.org/abs/1512.00567
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The next improvement is the factorization of an *n*×*n* convolution in two
    stacked asymmetrical 1×*n* and *n*×1 convolutions. For example, we can split a
    single 3×3 convolution into two 1×3 and 3×1 convolutions, where the 3×1 convolution
    is applied over the output of the 1×3 convolution. In the first case, the filter
    size would be 3*3 = 9, while in the second case, we would have a combined size
    of (3*1) + (1*3) = 3 + 3 = 6, resulting in 33% efficiency, as seen in the following
    diagram:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/023a33c1-b7ee-4535-ab68-18ab39d21038.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions. Inspired by https://arxiv.org/abs/1512.00567
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors introduced two new blocks, which utilizes factorized convolutions.
    The first of these blocks (and the second in total) is equivalent of Inception
    block A:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43e66db9-3ccb-4855-b1df-0890467a8f60.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Inception block B. When n=3, it is equivalent to block A. Inspired by https://arxiv.org/abs/1512.00567
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The second (third in total) block is similar, but the asymmetrical convolutions
    are parallel, resulting in a higher output depth (more concatenated paths). The
    hypothesis here is that the more features (different filters) the network has,
    the faster it learns (we also discussed the need for more filters in [Chapter
    2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding Convolutional Networks*).
    On the other hand, the wider layers take more memory and computation time. As
    a compromise, this block is only used in the deeper part of the network, after
    the other blocks:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e74a4669-e728-4122-ad8d-a5607c4ba63e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Inception block C, inspired by https://arxiv.org/abs/1512.00567
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these new blocks, the authors proposed two new Inception networks: v2
    and v3\. Another major improvement in this version is the use of batch normalization,
    which was introduced by the same authors.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Inception v4 and Inception-ResNet
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the latest revision of Inception networks, the authors introduced three
    new streamlined Inception blocks that build upon the idea of the previous versions (*Inception-v4,
    Inception-ResNet and the Impact of Residual Connections on Learning*, [https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)).
    They introduced 7×7 asymmetric factorized convolutions, and average pooling instead
    of max pooling. More importantly, they created a residual/Inception hybrid network
    known as Inception-ResNet, where the Inception blocks also include residual connections.
    We can see the schematic of one such block in the following diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c073e89b-8b95-4fb6-b486-aaa22c26193b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: An Inception block with a residual skip connection
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed different types of Inception networks and the
    different principles used in the various Inception blocks. Next, we'll talk about
    a newer CNN architecture, which takes the Inception concept to a new depth (or
    width, as it should be).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Xception
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All Inception blocks so far start by splitting the input into several parallel
    paths. Each path continues with a dimensionality-reduction 1×1 cross-channel convolution,
    followed by regular cross-channel convolutions. On one hand, the 1×1 connection
    maps cross-channel correlations, but not spatial ones (because of the 1×1 filter
    size). On the other hand, the subsequent cross-channel convolutions map both types
    of correlations. Let''s recall that in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding
    Convolutional Networks*, we introduced **depthwise separable convolutions** (**DSC**),
    which combine the following two operations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '**A depthwise convolution**: In a depthwise convolution, a single input slice
    produces a single output slice, therefore it only maps spatial (and not cross-channel)
    correlations.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A 1×1 cross-channel convolution**: With 1×1 convolutions, we have the opposite,
    that is, they only map cross-channel correlations.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The author of Xception (*Xception: Deep Learning with Depthwise Separable Convolutions*, [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))
    argues that, in fact, we can think of DSC as an extreme (hence the name) version
    of an Inception block, where each depthwise input/output slice pair represents
    one parallel path. We have as many parallel paths as the number of input slices. The
    following diagram shows a simplified Inception block and its transformation to
    an Xception block:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41ef8798-793d-4b06-8b80-d4fd09bf4d0c.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'Left: simplified Inception module. Right: Xception block. Inspired by https://arxiv.org/abs/1610.02357'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'The Xception block and the DSC have two differences:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: In Xception, the 1×1 convolution comes first, instead of last as in DSC. But,
    these operations are meant to be stacked anyway, and we can assume that the order
    is of no significance.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Xception block uses ReLU activations after each convolution, while the DSC
    doesn't use non-linearity after the cross-channel convolution. According to the
    author's experiments, networks with absent non-linearity depthwise convolution
    converged faster and were more accurate.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the architecture of the Xception network:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c75638b5-9f2a-4871-874c-c3013d8f80ee.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: Entry flow; Middle flow, repeated eight times; Exit flow.
    Source: https://arxiv.org/abs/1610.02357'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'It is built of linearly stacked DSCs and some of its properties are as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The network contains 36 convolutional layers, structured into 14 modules, all
    of which have linear residual connections around them, except for the first and
    last modules. The modules are grouped in three sequential virtual flows—entry,
    middle, and exit.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downsampling with 3×3 max pooling in the entry and exit flows; no downsampling
    in the middle flow; global average pooling before the fully connected layers.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All convolutions and DSCs are followed by batch normalization.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All DSCs have a depth multiplier of 1 (no depth expansion).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section concludes the series of Inception-based models. In the next section,
    we'll focus on a special model, which prioritizes a small footprint and computational
    efficiency.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MobileNet
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll discuss a lightweight CNN model called MobileNet (*MobileNetV2:
    Inverted Residuals and Linear Bottlenecks*, [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)).
    We''ll focus on the second revision of this idea (MobileNetV1 was introduced in *MobileNets:
    Efficient Convolutional Neural Networks for Mobile Vision Applications*, [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet is aimed at devices with limited memory and computing power, such
    as mobile phones (the name kind of gives it away). To reduce its footprint, the
    network uses DSC, linear bottlenecks, and inverted residuals.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'We are already familiar with DSC, so let''s discuss the other two:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear bottlenecks**: To understand this concept, we''ll quote the paper:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"Consider a deep neural network consisting of *n* layers *L[i]* each of which
    has an activation tensor of dimensions [![](img/684623c0-9307-4ec3-9630-699e64599285.png)].
    Throughout this section we will be discussing the basic properties of these activation
    tensors, which we will treat as containers of [![](img/d2c0077c-fecb-48f2-9e46-06527c760674.png)] "pixels"
    with *d[i]* dimensions. Informally, for an input set of real images, we say that
    the set of layer activations (for any layer *L[i]*) forms a "*manifold of interest"*.
    It has been long assumed that manifolds of interest in neural networks could be
    embedded in low-dimensional subspaces. In other words, when we look at all individual
    *d*-channel pixels of a deep convolutional layer, the information encoded in those
    values actually lie in some manifold, which can be embedded into a low-dimensional
    subspace."'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: One way to do this is with 1×1 bottleneck convolutions. But, the authors of
    the paper argue that if this convolution is followed by non-linearity like ReLU,
    this might lead to a loss of manifold information. If the ReLU input is larger
    than 0, then the output of this unit is equivalent to the linear transformation
    of the input. But, if the input is smaller, then the ReLU collapses and the information
    of that unit is lost. Because of this, MobileNet uses 1×1 bottleneck convolution
    without non-linear activation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverted residuals**: In the *Residual networks* section, we introduced the
    bottleneck residual block, where the data flow in the non-shortcut path is **input
    -> 1×1 bottleneck conv -> 3×3 conv -> 1×1 unsampling conv**. In other words, it
    follows a **wide -> narrow -> wide** data representation. The authors argue that
    *the bottlenecks actually contain all the necessary information, while an expansion
    layer acts merely as an implementation detail that accompanies a non-linear transformation
    of the tensor*. Because of this, they propose having shortcut connections between
    the bottleneck connections instead.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on these properties, the MobileNet model is composed of the following
    building blocks:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f84ccba9-bb59-4d84-bcd9-3701fd3b9e62.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'Top: inverted residual block with stride 1\. Bottom: stride 2 block'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The model uses ReLU6 non-linearity: ReLU6 = min(max(input, 0),6). The maximum
    activation value is limited to 6—in this way, the non-linearity is more robust
    in low-precision floating-point computations. That's because 6 can take at most
    3 bits, leaving the rest for the floating-point portion of the number.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Besides stride, the blocks are described by an expansion factor, *t*, which
    determines the expansion ratio of the bottleneck convolution.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the relationship between the input and output dimensions
    of the blocks:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/deac5fcf-f98e-40f8-b066-5e2f656d142a.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: The input and output dimensions relationship. Source: https://arxiv.org/abs/1801.04381
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding table, **h** and **w** are the input height and width, **s** is
    the stride, and **k** and **k'** are the input and output number of channels.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, here is the full model architecture:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4f0756c-e7fb-4b20-a32a-683e628747be.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: The MobileNetV2 architecture. Source: https://arxiv.org/abs/1801.04381
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Each line describes a group of one or more identical blocks, repeated *n* times.
    All layers in the same group have the same number, **c**, of output channels.
    The first layer of each sequence has a stride, **s**, and all others use stride
    1\. All spatial convolutions use 3 × 3 kernels. The expansion factor, **t**, is
    always applied to the input size, as described in the preceding table.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The next model we'll discuss is a network model with a new type of building
    block, where all layers are interconnected.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to DenseNets
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DenseNet (*Densely Connected Convolutional Networks*, [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)) try
    to alleviate the vanishing gradient problem and improve feature propagation, while
    reducing the number of network parameters. We''ve already seen how ResNets introduce
    residual blocks with skip connections to solve this. DenseNets take some inspiration
    from this idea and take it even further with the introduction of dense blocks.
    A dense block consists of sequential convolutional layers, where any layer has
    a direct connection to all subsequent layers. In other words, a network layer, *l*, will
    receive input, **x***[l]*, from all preceding network layers:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c100639-f0e1-4f6e-8fd0-f7df5a9cacad.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/72b34058-eb37-45f3-b58e-6714361f0560.png) are the **concatenated **output
    feature maps of the preceding network layers. This is unlike ResNets, where we
    combine different layers with the element-wise sum. *H[l]* is a composite function,
    which defines three types of DenseNet blocks (only two are displayed):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e47bdbbc-aaad-41b4-9371-2498f5bbe3c8.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'A dense block: the dimensionality-reduction layers (dashed lines) are part
    of the DenseNet-B architecture, while DenseNet-A doesn''t have them. DenseNet-C
    is not displayed'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define them:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '**DenseNet-A**: This is the base block, where *H[l]* consists of batch normalization,
    followed by activation, and a 3×3 convolution:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2934a1ea-ee3a-404e-8913-e08533fd1c18.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: '**DenseNet-B**: The authors also introduced a second type of dense block, DenseNet-B,
    which applies a dimensionality-reduction 1×1 convolution after each concatenation:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*![](img/7f2113f2-433e-409a-bca9-51c15c338392.png)*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '**DenseNet-C**: A further modification, which adds a downsampling 1×1 convolution
    after each dense block. The combination of B and C is referred to as DenseNet-BC.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense block is specified by its number of convolutional layers and the output
    volume depth of each layer, which is called the **growth rate** in this context.
    Let's assume that the input of the dense block has a volume depth of *k[0]* and
    the output volume depth of each convolutional layer is *k*. Then, because of the
    concatenation, the input volume depth for the *l*-th layer will be *k[0]+k[x](l
    − 1)*. Although the later layers of a dense block have a large input volume depth
    (because of the many concatenations), DenseNets can work with growth rate values
    as low as 12, which reduces the total number of parameters. To understand why
    this works, let's think of the feature maps as the **collective knowledge** (or
    global state) of the network. Each layer adds its own *k* feature maps to this
    state, and the growth rate determines the amount of information the layer contributes
    to it. Because of the dense structure, the global state can be accessed from everywhere
    within the network (hence the term global). In other words, there is no need to
    replicate it from one layer to the next as in traditional network architectures,
    which allows us to start with a smaller number of feature maps.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: To make concatenation possible, dense blocks use padding in such a way that
    the height and width of all output slices are the same throughout the block. But
    because of this, downsampling is not possible within a dense block. Therefore, a
    dense network consists of multiple sequential dense blocks, separated by downsampling
    pooling operations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the paper have proposed a family of DenseNets, whose overall
    architecture resembles ResNet:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8eed3144-e67e-4370-ac24-9c13d1bdbcd1.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: The family of DenseNet networks. Source: https://arxiv.org/abs/1608.06993
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'They have the following properties:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Start with a 7×7 stride 2 downsampling convolution.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A further downsampling 3×3 max pooling with stride 2.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four groups of DenseNet-B blocks. The family of networks differs by the number
    of dense blocks within each group.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downsampling is handled by a transition layer of a 2×2 pooling operation with
    stride 2 between the dense groups.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transition layer contains a further 1×1 bottleneck convolution to reduce
    the number of feature maps. The compression ratio of this convolution is specified
    by a hyper-parameter, *θ*, where *0* < *θ* ≤ *1*. If the number of input feature
    maps is *m*, then the number of output feature maps is *θm.*
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dense blocks end up with a 7×7 global average pooling, followed by a 1,000-unit
    fully connected softmax layer.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors of DenseNet have also released an improved DenseNet model called
    MSDNet (*Multi-Scale Dense Networks for Resource Efficient Image Classification*, [https://arxiv.org/abs/1703.09844](https://arxiv.org/abs/1703.09844)),
    which (as the name suggests) uses multi-scale dense blocks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: With DenseNet, we conclude our discussion about conventional CNN architectures.
    In the next section, we'll discuss whether it's possible to automate the process
    of finding the optimal NN architecture.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The workings of neural architecture search
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NN models we've discussed so far were designed by their authors. But, what
    if we could make the computer itself design the NN? Enter **neural architecture
    search** (**NAS**)—a technique that automates the design of NNs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue, let''s see what the network architecture consists of:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The graph of operations, which represents the network. As we discussed in [Chapter
    1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and bolts of Neural
    Networks*, the operations include (but are not limited to) convolutions, activation
    functions, fully connected layers, normalization, and so on.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The parameters of each operation. For example, the convolution parameters are:
    type (cross-channel, depthwise, and so on), input dimensions, number of input
    and output slices, stride, and padding.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we'll discuss gradient-based NAS with reinforcement learning
    (*Neural Architecture Search with Reinforcement Learning*, [https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578)).
    At this point, we won't discuss reinforcement learning, and we'll focus on the
    algorithm instead. It starts with the premise that we can represent the network
    definition as a string (a sequence of tokens). Let's assume that we'll generate
    a sequential CNN, which consists only of convolutions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, part of the string definition will look like this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd4ee96f-aa67-4c50-b853-a98045890dbd.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: We don't have to specify the layer type, because we only use convolutions. We
    exclude padding for the sake of simplicity. The subscript text on the first line
    is included for clarity, but won't be included in the algorithmic version.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the algorithm overview in the following diagram:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c0eb578-01b6-4814-881f-79cb79301681.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: NAS overview. Source: https://arxiv.org/abs/1611.01578
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the controller. It is an RNN, whose task is to generate new
    network architectures. Although we haven't yet discussed RNNs (this honor won't
    come until [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml), *Understanding
    Recurrent Networks*), we'll try to explain how it works nevertheless. In [Chapter
    1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts of Neural
    Networks*, we mentioned that an RNN maintains an internal state—a summary of all
    its previous inputs. Based on that internal state and the latest input sample,
    the network generates a new output, updates its internal state, and waits for
    the next input.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Here, the controller will generate the string sequence, which describes the
    network architecture. The controller output is a single token of the sequence.
    This could be filter height/width, stride width/height, or the number of output
    filters. The type of token depends on the length of the currently generated architecture.
    Once we have this token, we feed it back to the RNN controller as input. Then,
    the network generates the next token of the sequence.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is depicted in the following diagram:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36bb6544-b95c-41a3-879b-78160ed1fd28.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Generating a network architecture with the RNN controller. The output token
    is fed back to the controller as input to generate the next token. Source: https://arxiv.org/abs/1611.01578
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: The white vertical squares in the diagram represent the RNN controller, which
    consists of a two-layer **Long short-term memory** (**LSTM**) cell (along the
    *y*-axis). Although the diagram shows multiple instances of the RNN (along the
    *x*-axis), it is in fact the same network; it's just **unfolded** in time, to
    represent the process of sequence generation. That is, each step along the *x*-axis
    represents a single token of the network definition. A token prediction at step
    *t* is carried out by a softmax classifier and then fed as controller input at
    step *t+1*. We continue this process until the length of the generated network
    reaches a certain value. Initially, this value is small (a short network), but
    it gradually increases (a longer network) as the training progresses.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand NAS, let''s see a step-by-step execution of the algorithm:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The controller generates a new architecture, *A*.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It builds and **trains** a new network with said architecture until it converges.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It tests the new network on a withheld part of the training set and measures
    the error, *R*.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It uses this error to update the controller parameters, *θ[c]*. As our controller
    is RNN, this means training the network and adjusting its weights. The model parameters
    are updated in such a way as to reduce the error, *R*, of the future architectures.
    This is made possible by a reinforcement learning algorithm called REINFORCE,
    which is beyond the scope of this section.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It repeats these steps until the error, *R*, of the generated network falls
    below a certain threshold.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The controller can generate network architectures with some restrictions. As
    we mentioned earlier in this section, the most severe is that the generated network
    only consists of convolutional layers. To simplify things, each convolutional
    layer automatically includes batch normalization and ReLU activation. But in theory,
    the controller could generate more complex architectures with other layers such
    as pooling or normalization. We could implement this by adding additional controller
    steps in the architecture sequence for the layer type.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the paper implemented a technique that allows us to add residual
    skip connections to the generated architecture. It works with a special type of
    controller step called an anchor point. The anchor point at layer *N* has content-based
    sigmoids. The output of a sigmoid *j (j = 1, 2, 3, ..., N-1)* represents the probability
    that the current layer has a residual connection to layer *j*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified controller is depicted in the following diagram:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6365b957-b098-48ef-b4fc-95a9c8460a5c.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'RNN controller with anchor points for the residual connections. Source: https://arxiv.org/abs/1611.01578'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'If one layer has many input layers, all inputs are concatenated along the channel
    (depth) dimension. Skip connections could create some issues in the network design:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: The first hidden layer of the network (that is, the one not connected to any
    other input layer) uses the input image as an input layer.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the network, all layer outputs that have not been connected are
    concatenated in a final hidden state, which is sent to the classifier.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may happen that the outputs to be concatenated have different sizes. In that
    case, the smaller feature maps are padded with zeros to match the size of the
    bigger ones.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In their experiment, the authors used a controller with a 2-layer LSTM cell
    with 35 units in each layer. For every convolution, the controller has to select
    a filter height and width from the values {1, 3, 5, 7}, and a number of filters
    to be one of {24, 36, 48, 64}. Additionally, they performed 2 sets of experiments—one
    where the controller was allowed to select strides in {1, 2, 3} and another with
    a fixed stride of 1.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Once the controller generates an architecture, the new network is trained for
    50 epochs on 45,000 images of the CIFAR-10 dataset. The remaining 5,000 images
    are used for validation. During training, the controller starts with an architecture
    depth of 6 layers and then increases the depth by 2 layers on every 1,600 iterations. The
    best performing model has a validation accuracy of 3.65%. It was discovered after
    12,800 architectures using 800 GPUs (wow!). The reason for these steep computational
    requirements is that each new network is trained from scratch just to produce
    one accuracy value, which can then be used to train the controller. More recently,
    the new ENAS algorithm (*Efficient Neural Architecture Search via Parameter Sharing*, [https://arxiv.org/abs/1802.03268](https://arxiv.org/abs/1802.03268))
    has made it possible to significantly reduce the computational resources of NAS
    by sharing the weights among the generated models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll discuss a novel type of NN, which tries to overcome
    some of the limitations of the CNNs we talked about so far.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Introducing capsule networks
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capsule networks (*Dynamic Routing Between Capsules*, [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)) were
    introduced as a way to overcome some of the limitations of standard CNNs. To understand
    the idea behind capsule networks, we need to understand their limitations first,
    which we will see in the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of convolutional networks
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with a quote from professor Hinton himself:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '"The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster."'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, CNNs are **translation-invariant**. Let's imagine a
    picture with a face, located in the right half of the picture. Translation invariance
    means that a CNN is very good at telling us that the picture contains a face,
    but it cannot tell us whether the face is in the left or right part of the image.
    The main culprit for this behavior is the pooling layers. Every pooling layer
    introduces a little translation invariance. For example, the max pooling routes
    forward the activation of only one of the input neurons, but the subsequent layers
    don't have any knowledge of which neuron is routed.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: By stacking multiple pooling layers, we gradually increase the receptive field
    size. But, the detected object could be anywhere in the new receptive field, because
    none of the pooling layers relay such information. Therefore, we also increase
    the translation invariance. At first, this might seem to be a good thing, because
    the final labels have to be translation-invariant. But, it poses a problem, as
    CNNs cannot identify the position of one object relative to another. A CNN would
    identify both of the following images as a face, because they both contain the
    components of a face (a nose, mouth, and eyes) regardless of their relative positions
    to one another.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also known as the **Picasso problem**, as demonstrated in the following
    diagram:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b264a39-059f-4898-8bbc-b3d5b423b1ff.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: A convolutional network would identify both of these images as a face
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: But, that's not all. A CNN would be confused even if the face had a different **orientation**,
    for example, if it was turned upside down. One way to overcome this is with data
    augmentation (rotation) during training. But, this only shows the limitations
    of the network. We have to explicitly show the object in different orientations
    and tell the CNN that this is, in fact, the same object.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've seen that a CNN discards the translation information (transitional
    invariance) and doesn't understand the orientation of an object. In computer vision,
    the combination of translation and orientation is known as the **pose**. The pose
    is enough to uniquely identify the object's properties in the coordinate system. Let's
    use computer graphics to illustrate this. A 3D object, say a cube, is entirely
    defined by its pose and the edge length. The process of transforming the representation
    of a 3D object into an image on the screen is called rendering. Knowing just its
    pose and the edge length of the cube, we can render it from any point of view
    we like.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if we can somehow train a network to understand these properties,
    we won't have to feed it with multiple augmented versions of the same object.
    A CNN cannot do that, because its internal data representation doesn't contain
    information about the object's pose (only about its type). In contrast, capsule
    networks **preserve information** for both the type and the pose of an object.
    Therefore, they can detect objects that can transform into each other, which is
    known as **equivariance**. We can also think of this as **reverse graphics**,
    that is, a reconstruction of the object's properties according to its rendered
    image.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: To solve these problems, the authors of the paper propose a new type of network
    building block, called a **capsule**, instead of the neuron. Let's discuss it
    in the next section.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Capsules
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The output of a capsule is a vector, compared to the output of a neuron, which
    is a scalar value. The capsule output vector carries the following meaning:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The elements of the vector represent the pose and other properties of the object.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length of the vector is in the (0, 1) range and represents the probability
    of detecting the feature at that location. As a reminder, the length of a vector
    is [![](img/0164c9bd-513d-44ac-a282-cf9fc9a2293c.png)], where *v[i]* are the vector
    elements.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's consider a capsule that detects faces. If we start moving a face across
    an image, the values of the capsule vector will change to reflect the change in
    the position. However, its length will always stay the same, because the probability
    of the face doesn't change with the location.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Capsules are organized in interconnected layers, just like a regular network. The
    capsules in one layer serve as input to the capsules in the next. And, like a
    CNN, the shallower layers detect basic features, and the deeper layers combine
    them in more abstract and complex ones. But now, the capsules also relay positional
    information, instead of just detected objects. This allows the deeper capsules
    to analyze not only the presence of features, but also their relationship. For
    example, a capsule layer may detect a mouth, face, nose, and eyes. The subsequent
    capsule layer will be able to not only verify the presence of these features,
    but also whether they have the correct spatial relationship. Only if both conditions
    are true can the subsequent layer verify that a face is present. This is a high-level
    overview of capsule networks. Now, let's see how exactly capsules work.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the schematic of a capsule in the following screenshot:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17593e60-cd8e-4ffd-8916-b875625f79c7.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: A capsule
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze it in the following steps:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: The capsule inputs are the output vectors, **u***[1],* **u***[2], ...* **u***[n]*,
    from the capsules of the previous layer.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We multiply each vector, **u***[i]*, by its corresponding weight matrix, *W[ij]*,
    to produce **pred****iction vectors**, [![](img/af47eced-a6b2-4d2c-9b72-529fc22462c1.png)].
    The weight matrices, **W**, encode spatial and other relationships between the
    lower-level features, coming from the capsules of the previous layer, and the
    high-level ones in the current layer. For example, imagine that the capsule in
    the current layer detects faces and the capsules from the previous layer detect
    the mouth (**u***[1]*), eyes (**u***[2]*), and nose (**u***[3]*). Then, [![](img/b4c26187-9be6-4b8e-8eb0-bc91e7ad1715.png)] is
    the predicted position of the face, given where the location of the mouth is.
    In the same way, [![](img/dcaec96e-5891-4d32-9a83-430790d1ff04.png)] predicts
    the location of the face based on the detected location of the eyes, and [![](img/309fb5d1-833a-4b43-b136-62fdd584760a.png)] predicts
    the location of the face based on the location of the nose. If all three lower-level
    capsule vectors agree on the same location, then the current capsule can be confident
    that a face is indeed present. We only used location for this example, but the
    vectors could encode other types of relationships between the features, such as
    scale and orientation. The weights, **W**, are learned with backpropagation.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we multiply the [![](img/2970c647-8b48-4b8e-960d-c8616516740c.png)] vectors
    by the scalar coupling coefficients, *c[ij]*. These coefficients are a separate
    set of parameters, apart from the weight matrices. They exist between any two
    capsules, and indicate which high-level capsules will receive input from a lower-level
    capsule. But, unlike weight matrices, which are adjusted via backpropagation,
    coupling coefficients are computed on the fly during the forward pass via a process
    called **dynamic routing**. We'll describe it in the next section.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we perform the sum of the weighted input vectors. This step is similar to
    the weighted sum in neurons, but with vectors:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ba2e77a3-d557-4965-a7cd-c66f0513762e.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we''ll compute the output of the capsule, **v***[j]*, by squashing
    the vector, **s***[j]*. In this context, squashing means transforming the vector
    in such a way that its length comes in the (0, 1) range, without changing its
    direction. As mentioned, the length of the capsule vector represents the probability
    of the detected feature and squashing it in the (0, 1) range reflects that. To
    do this, the authors propose a novel formula:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/45aaa75e-ef3c-4427-8b4f-6ad373e78a0c.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Now that we know the structure of the capsules, in the following section, we'll
    describe the algorithm to compute the coupling coefficients between capsules of
    different layers. That is, the mechanism by which they relay signals between one
    another.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic routing
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s describe the dynamic routing process to compute the coupling coefficients, *c[ij]*,
    displayed in the following diagram:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3433e46b-78fc-42f1-a1f8-d187d0824843.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Dynamic routing example. The grouped dots indicate lower-level capsules that
    agree with each other
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: We have a lower-level capsule, *I*, that has to decide whether to send its output
    to one of two higher-level capsules, *J* and *K*. The dark and light dots represent
    prediction vectors, ![](img/52b505fd-dae9-4348-8af6-c50be798ceab.png)and ![](img/d8f4d069-59c4-418d-8641-0644a4c1a5c5.png),
    which *J* and *K* have already received from other lower-level capsules. The arrows
    from the *I* capsule to the *J* and *K* capsules point to the ![](img/18952087-30c0-49a3-a1de-6641307e87f4.png) and
    ![](img/017be278-853c-4f1b-936c-0dfcbc6e49b3.png) prediction vectors from *I*
    to *J* and *K*. The clustered prediction vectors (lighter dots) indicate lower-level
    capsules that agree with each other with regard to the high-level feature. For
    example, if the *K* capsule describes a face, then the clustered predictions would
    indicate lower-level features, such as a mouth, nose, and eyes. Conversely, the
    dispersed (darker) dots indicate disagreement. If the *I* capsule predicts a vehicle
    tire, it would disagree with the clustered predictions in *K*.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: However, if the clustered predictions in *J* represent features such as headlights,
    windshield, or fenders, then the prediction of *I* would be in agreement with
    them. The lower-level capsules have a way of determining whether they fall into
    the clustered or dispersed group of each higher-level capsule. If they fall into
    the clustered group, they will increase the corresponding coupling coefficient
    with that capsule and will route their vector in that direction. Conversely, if
    they fall into the dispersed group, the coefficient will decrease.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s formalize this knowledge with a step-by-step algorithm, introduced by
    the authors:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: For all *i* capsules in the *l* layer, and *j* capsules in the *(l + 1)* layer,
    we'll initialize [![](img/d0af0750-02cf-491b-9bcd-1965a7670f14.png)], where *b[ij]* is
    a temporary variable equivalent to *c[ij]*. The vector representation of all *b[ij]* is
    **b**[*i*]. At the start of the algorithm, the *i* capsule has an equal chance
    of routing its output to any of the capsules of the *(l + 1)* layer.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat for *r* iterations, where *r* is a parameter:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For all *i* capsules in the *l* layer: [![](img/ac9bd249-5a93-4e77-9f99-f9583cef4ac8.png)]. The
    sum of all outgoing coupling coefficients, *c[i]*, of a capsule amounts to 1 (they
    have a probabilistic nature), hence the softmax.'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For all *j* capsules in the *(l + 1)* layer: [![](img/5c2be571-0a64-4f10-be1c-3c7574a40c67.png)].
    That is, we''ll compute all non-squashed output vectors of the *(l + 1)* layer.'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For all *j* capsules in the *(l + 1)* layer, we''ll compute the squashed vectors:
    [![](img/c045dd46-4691-40a5-b186-a2ad864ccd04.png)].'
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For all *i* capsules in the *l* layer, and *j* capsules in the *(l + 1)* layer:
    [![](img/022f06a1-e7e3-446a-bbf5-8c55e5f72cfa.png)]. Here, [![](img/3768d012-866d-44cb-8efd-bb6d3210fa24.png)]is
    the dot product of the prediction vector of the low-level *i* capsule and the
    output vector of the high-level *j* capsule vectors. If the dot product is high,
    then the *i* capsule is in agreement with the other low-level capsules, which
    route their output to the *j* capsule, and the coupling coefficient increases.'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The authors have recently released an updated dynamic routing algorithm using
    a clustering technique called expectation-maximization. You can read more about
    it in the original paper,*Matrix capsules with EM routing* ([https://ai.google/research/pubs/pub46653](https://ai.google/research/pubs/pub46653)).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: The structure of the capsule network
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll describe the structure of the capsule network, which
    the authors used to classify the MNIST dataset. The input of the network is the
    28×28 MNIST grayscale images, and the following are the steps:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with a single convolutional layer with 256 9×9 filters, stride 1,
    and ReLU activation. The shape of the output volume is (256, 20, 20).
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have another convolutional layer with 256 9×9 filters and stride 2\. The
    shape of the output volume is (256, 6, 6).
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the output of the layer as a foundation for the first capsule layer, called
    `PrimaryCaps`. Take the (256, 6, 6) output volume and split it into 32 separate
    (8, 6, 6) blocks. That is, each of the 32 blocks contains eight 6×6 slices. Take
    one activation value with the same coordinates from each slice and combine these
    values in a vector. For example, we can take activation (3, 7) of slice 1, (3,
    7) of slice 2, and so on and combine them in a vector with a length of 8\. We'll
    have 36 of these vectors. Then, we'll **transform** each vector into a capsule
    for a total of 36 capsules. The shape of the output volume of the `PrimaryCaps`
    layer is (32, 8, 6, 6).
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second capsule layer is called `DigitCaps`. It contains 10 capsules (1 per
    digit), whose output is a vector with length 16. The shape of the output volume
    of the `DigitCaps` layer is (10, 16). During inference, we compute the length
    of each `DigitCaps` capsule vector. We then take the capsule with the longest
    vector as the prediction result of the network.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During training, the network includes three additional, fully connected layers
    after `DigitCaps`, the last of which has 784 neurons (28×28). In the forward training
    pass, the longest capsule vector serves as input to these layers. They try to
    reconstruct the original image, starting from that vector. Then, the reconstructed
    image is compared to the original one and the difference serves as additional
    regularization loss for the backward pass.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Capsule networks are a new and promising approach to computer vision. However,
    they are not widely adopted yet and don't have an official implementation in any
    of the deep learning libraries discussed in this book, but you can find multiple
    third-party implementations.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed some popular CNN architectures: we started with
    the classics, AlexNet and VGG. Then, we paid special attention to ResNets, as
    one of the most well-known network architectures. We also discussed the various
    incarnations of Inception networks and the Xception and MobileNetV2 models, which
    are related to them. We also talked about the exciting new ML area of neural architecture
    search. Finally, we discussed capsule networks—a new type of CV network, which
    tries to overcome some of the inherent CNN limitations.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: We've already seen how to apply these models in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, where we employed ResNet and MobileNet
    in a transfer learning scenario for a classification task. In the next chapter,
    we'll see how to apply some of them to more complex tasks such as object detection
    and image segmentation.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
