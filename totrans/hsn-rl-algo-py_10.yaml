- en: DDPG and TD3 Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we concluded a comprehensive overview of all the major
    policy gradient algorithms. Due to their capacity to deal with continuous action
    spaces, they are applied to very complex and sophisticated control systems. Policy
    gradient methods can also use a second-order derivative, as is done in TRPO, or
    use other strategies, in order to limit the policy update by preventing unexpected
    bad behaviors. However, the main concern when dealing with this type of algorithm
    is their poor efficiency, in terms of the quantity of experience needed to hopefully
    master a task. This drawback comes from the on-policy nature of these algorithms,
    which makes them require new experiences each time the policy is updated. In this
    chapter, we will introduce a new type of off-policy actor-critic algorithm that
    learns a target deterministic policy, while exploring the environment with a stochastic
    policy. We call these methods deterministic policy gradient methods, due to their
    characteristic of learning a deterministic policy. We''ll first show how these
    algorithms work, and we will also show their close relationship with Q-learning
    methods. Then, we''ll present two deterministic policy gradient algorithms: **deep
    deterministic policy gradient** (**DDPG**), and a successive version of it, known
    as **twin delayed deep deterministic policy gradient** (**TD3**). You''ll get
    a sense of their capabilities by implementing and applying them to a new environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Combining policy gradient optimization with Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep deterministic policy gradient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twin delayed deep deterministic policy gradient (TD3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining policy gradient optimization with Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, we approach two main types of model-free algorithms:
    the ones based on the gradient of the policy, and the ones based on the value
    function. From the first family, we saw REINFORCE, actor-critic, PPO, and TRPO.
    From the second, we saw Q-learning, SARSA, and DQN. As well as the way in which
    the two families learn a policy (that is, policy gradient algorithms use stochastic
    gradient ascent toward the steepest increment on the estimated return, and value-based
    algorithms learn an action value for each state-action to then build a policy),
    there are key differences that let us prefer one family over the other. These
    are the on-policy or off-policy nature of the algorithms, and their predisposition
    to manage large action spaces. We already discussed the differences between on-policy
    and off-policy in the previous chapters, but it is important to understand them
    well, in order to actually appreciate the algorithms that will be introduced in
    this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Off-policy learning is able to use previous experiences in order to refine the
    current policy, despite the fact that that experience comes from a different distribution.
    DQN benefits from this by storing all the memories that the agent had throughout
    its life in a replay buffer, and by sampling mini-batches from the buffer to update
    the target policy. At the opposite end of the spectrum, there is on-policy learning,
    which requires experience to be gained from the current policy. This means that
    old experiences cannot be used, and every time the policy is updated, the old
    data has to be discarded. As a result, because off-policy learning can reuse data
    multiple times, it requires fewer interactions with the environment in order to
    learn a task. In cases where the acquisition of new samples is expensive or very
    difficult to do, this difference matters a lot, and choosing off-policy algorithms
    could be vital.
  prefs: []
  type: TYPE_NORMAL
- en: The second factor is a matter of action spaces. As we saw in [Chapter 7](4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml),
    *TRPO and PPO Implementation*, policy gradient algorithms have the ability to
    deal with very large and continuous action spaces. Unfortunately, the same does
    not hold true for Q-learning algorithms. To choose an action, they have to perform
    maximization across all the action space, and whenever this is very large or continuous,
    it is intractable. Thus, Q-learning algorithms can be applied to arbitrarily complex
    problems (with a very large state space) but their action space has to be limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, none of the previous algorithms are always preferred over others,
    and the choice is mostly task dependent. Nevertheless, their advantages and disadvantages
    are quite complementary, and thus the question arises: Is it possible to combine
    the benefits of both families into a single algorithm?'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic policy gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing an algorithm that is both off-policy and able to learn stable policies
    in high-dimensional action spaces is challenging. DQN already solves the problem
    of learning a stable deep neural network policy in off-policy settings. An approach
    to making DQN also suitable for continuous actions is to discretize the action
    space. For example, if an action has values between 0 and 1, a solution could
    be to discretize it in 11 values (0, 0.1, 0.2,.., 0.9, 1.0), and predict their
    probabilities using DQN. However, this solution is not manageable with a lot of
    actions, because the number of possible discrete actions increases exponentially
    with the degree of freedom of the agent. Moreover, this technique isn't applicable
    in tasks that need more fine-grained control. Thus, we need to find an alternative.
  prefs: []
  type: TYPE_NORMAL
- en: 'A valuable idea is to learn a deterministic actor-critic. It has a close relationship
    with Q-learning. If you remember, in Q-learning, the best action is chosen in
    order to maximize the approximated Q-function among all of the possible actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37b1947e-366f-4dc0-aae8-cf6d48068ed8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The idea is to learn a deterministic ![](img/20d849ba-a9be-4cb2-968d-b246090126c4.png)
    policy that approximates ![](img/fc729a7a-f36c-4f89-bbc2-7503992c48d5.png). This
    overcomes the problem of computing a global maximization at every step, and opens
    up the possibility of extending it to very high-dimensional and continuous actions.
    **Deterministic policy gradient** (**DPG**) applies this concept successfully
    to some simple problems such as Mountain Car, Pendulum, and an octopus arm. After
    DPG, DDPG expands the ideas of DPG, using deep neural networks as policies and
    adopting some more careful design choices in order to make the algorithm more
    stable. A further algorithm, TD3, addresses the problems of high variance, and
    the overestimation bias that is common in DPG and DDPG. Both DDPG and TD3 will
    be explained and developed in the following sections. When we construct a map
    that categorizes RL algorithms, we place DPG, DDPG, and TD3 in the intersection
    between policy gradient and Q-learning algorithms, as in the following diagram.
    For now, let''s focus on the foundation of DPGs and how they work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89762d73-952a-4a43-8d77-b3fcb11cab5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Categorization of the model-free RL algorithms developed so far
  prefs: []
  type: TYPE_NORMAL
- en: 'The new DPG algorithms combine both Q-learning and policy gradient methods.
    A parametrized deterministic policy only outputs deterministic values. In continuous
    contexts, these can be the mean of the actions. The parameters of the policy can
    then be updated by solving the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6adb4b1-9a4f-4a43-97fb-a802b27c8b84.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b109059e-ac94-412e-a5c8-01f739c2ed2f.png) is the parametrized action-value
    function. Note that deterministic approaches differ from stochastic approaches
    in the absence of additional noise added to the actions. In PPO and TRPO, we were
    sampling from a normal distribution, with a mean and a standard deviation. Here,
    the policy has only a deterministic mean. Going back to the update (8.1), as always,
    maximization is done with stochastic gradient ascent, which will incrementally
    improve the policy with small updates. Then, the gradient of the objective function
    can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f24630d5-b820-4738-aa15-8dfe748502c4.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0655f41b-eb80-4c0a-abfd-6b618e8d6f4a.png) is the state distribution
    following the ![](img/4f702ec5-2725-43d2-a96f-100691224218.png) policy. This formulation
    comes from the deterministic policy gradient theorem. It says that the gradient
    of the objective function is obtained in expectation by following the chain rule
    that is applied to the Q-function, which is taken with respect to the ![](img/971e84fa-bf98-4484-bfe2-e8a2adae34da.png)
    policy parameters. Using automated differentiable software such as TensorFlow,
    it''s very easy to compute. In fact, the gradient is estimated just by computing
    the gradient, starting from the Q-values, all the way through the policy, but
    updating only the parameters of the latter, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23769543-a8d0-4e93-a3b2-4acefb925995.png)'
  prefs: []
  type: TYPE_IMG
- en: An illustration of the DPG theorem
  prefs: []
  type: TYPE_NORMAL
- en: The gradient is computed starting from the Q-values, but only the policy is
    updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a more theoretical result. As we know, deterministic policies don''t
    explore the environment, and thus, they won''t find a good solution. To make the
    DPG off-policy, we need to take a step further, and define the gradient of the
    objective function in such a way that the expectation follows the distribution
    of a stochastic exploratory policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5acbb97-a9a8-453b-8671-e2177990c528.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/52c37d4d-cd73-4b4c-b4cf-57e938d0f69e.png) is an exploratory policy,
    also called a behavior policy. This equation gives the *off-policy deterministic
    policy gradient* and gives the estimated gradient with respect to a deterministic
    policy (![](img/e4c50ea5-7487-4062-9df4-0ca08b932f7a.png)), while generating trajectories
    that follow a behavior policy (![](img/06214d17-b379-4516-8eda-479875d475b5.png)).
    Note that, in practice, the behavior policy is just the deterministic policy with
    additional noise.'
  prefs: []
  type: TYPE_NORMAL
- en: Though we have talked about deterministic actor-critic previously, until now,
    we have only shown how the policy learning takes place. Instead, we are learning
    both the actor that is represented by the deterministic policy (![](img/3d257251-7342-4eff-b411-bd9495d929b6.png)),
    and the critic that is represented by the Q-function (![](img/d63d86d6-029d-4d74-b710-2c01bcf514c5.png)).
    The differentiable action-value function (![](img/3e87bdfa-6420-478c-a7d8-417e4ad58fd4.png))
    can easily be learned with the Bellman updates that minimize the Bellman error
  prefs: []
  type: TYPE_NORMAL
- en: (![](img/175ffab3-52ba-428d-b399-51a73c9d5d9a.png)), as done in Q-learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Deep deterministic policy gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you implemented DPG with the deep neural networks that were presented in
    the previous section, the algorithm would be very unstable and it wouldn't be
    capable of learning anything. We encountered a similar problem when we extended
    Q-learning with deep neural networks. Indeed, to combine DNN and Q-learning in
    the DQN algorithm, we had to employ some other tricks to stabilize learning. The
    same holds true for DPG algorithms. These methods are off-policy, just like Q-learning,
    and as we'll soon see, some ingredients that make deterministic policies work
    with DNN are similar to the ones used in DQN.
  prefs: []
  type: TYPE_NORMAL
- en: 'DDPG (*Continuous Control with Deep Reinforcement Learning* by Lillicrap, and
    others: [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf))
    is the first deterministic actor-critic that employs deep neural networks, for
    learning both the actor and the critic. This model-free, off-policy, actor-critic
    algorithm extends both DQN and DPG, in that it uses some insight from DQN, such
    as the replay buffer and the target network, to make DPG work with deep neural
    networks.'
  prefs: []
  type: TYPE_NORMAL
- en: The DDPG algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DDPG uses two key ideas, both borrowed from DQN but adapted for the actor-critic
    case:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Replay buffer**: All the transitions acquired during the lifetime of the
    agent are stored in a replay buffer, also called experienced replay. Then, this
    is used for training the actor and the critic by sampling mini-batches from it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target network**: Q-learning is unstable, since the network that is updated
    is also the one that is used for computing the target values. If you remember,
    DQN mitigates this problem by employing a target network that is updated every
    *N* iterations (copying the parameters of the online network in the target network).
    In the DDQN paper, they show that a soft target update works better in this context.
    With a soft update, the parameters of the target network, ![](img/f8eea50d-f2cf-4b75-ba6c-7480ad8ae73c.png),
    are partially updated on each step with the parameters of the online network,
    ![](img/cc232cb3-2900-4f28-94cb-a03c1289e452.png): ![](img/739b738f-f8c7-4bd2-9e84-ee481c71d182.png) with
    ![](img/28e79dd5-fa6c-4377-bfb1-5fa98b8b7666.png). Yes, it may slow the learning,
    as the target network is changed only partially, but it outweighs the benefit
    that is derived from the increased instability. The trick of using a target network
    is used for both the actor and the critic, thereby the parameters of the target
    critic will also be updated following the soft update: ![](img/f1ef00e9-1350-46df-b8c5-ecca958a9efa.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that, from now on, we'll refer to ![](img/02dc85bc-53da-42d3-8fc7-cf38a36e651f.png) and ![](img/93e7cd98-72ad-41ef-a860-d24588e07569.png) as
    the parameters of the online actor and the online critic, and to ![](img/f242af40-2223-4dec-86e5-4fcf14df13b7.png) and ![](img/15440789-03c6-4ac7-861d-488d7336fe02.png) as
    the parameters of the target actor and the target critic.
  prefs: []
  type: TYPE_NORMAL
- en: A characteristic that DDPG inherits from DQN is the ability to update the actor
    and the critic for each step taken in the environment. This follows on from the
    fact that DDPG is off-policy, and learns from the mini-batches that were sampled
    from the replay buffer. DDPG doesn't have to wait until a sufficiently large batch
    is gathered from the environment, as would be the case in on-policy stochastic
    policy gradient methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we saw how DPG acts according to an exploratory behavior policy,
    despite that fact that it is still learning a deterministic policy. But, how is
    this exploratory policy built? In DDPG, the ![](img/01f396f5-40d7-45cc-b408-b4afb82ca2a6.png)
    policy is constructed by adding noise that is sampled from a noise process (![](img/daddfb35-8e51-4844-ad03-5e7b14112d30.png)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb09642e-9da9-443d-819b-a0996b16e0d0.png)'
  prefs: []
  type: TYPE_IMG
- en: The ![](img/b4e85b8c-e3a8-49ac-8471-a1e1a8a7c064.png) process will make sure
    that the environment is sufficiently explored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wrapping up, DDPG learns by cyclically repeating these three steps until convergence
    occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: The ![](img/01f396f5-40d7-45cc-b408-b4afb82ca2a6.png)behavior policy interacts
    with the environment, collecting observations and rewards from it by storing them
    in a buffer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each step, the actor and the critic are updated, based on the information
    held in the mini-batch that was sampled from the buffer. Specifically, the critic
    is updated by minimizing the mean squared error (MSE) loss between the values
    that were predicted by the online critic (![](img/cf5f868f-73db-4a02-8924-2544d452b30d.png)), and
    the target values that were computed using the target policy (![](img/1acd4514-f7a6-45e7-bb00-da4703729c8a.png))
    and the target critic (![](img/3c00dea5-2d6d-4277-bb06-2c7806fcb611.png)). Instead,
    the actor is updated following formula (8.3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target network parameters are updated following the soft update.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The whole algorithm is summarized in this pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With a more clear understanding of the algorithm, we can now start implementing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: DDPG implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pseudocode that was given in the preceding section already provides a comprehensive
    view of the algorithm, but from an implementation standpoint, there are a few
    things that are worth looking at in more depth. Here, we'll show the more interesting
    features that could also recur in other algorithms. The full code is available
    in the GitHub repository of the book: [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we''ll focus on a few main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: How to build a deterministic actor-critic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do soft updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to optimize a loss function, with respect to only some parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to calculate the target values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We defined a deterministic actor and a critic inside a function called `deterministic_actor_critic`.
    This function will be called twice, as we need to create both an online and a
    target actor-critic. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are three interesting things happening inside this function. The first
    is that we are distinguishing between two types of input for the same critic.
    One that takes a state as the input, and a `p_means` deterministic action is returned
    by the policy; and the other that takes a state and an arbitrary action as the
    input. This distinction is needed, because one critic will be used for optimizing
    the actor, while the other is used for optimizing the critic. Nevertheless, despite
    these two critics having two different inputs, they are the same neural network,
    meaning that they share the same parameters. This different use case is accomplished
    by defining the same variable scope for both instances of the critic, and setting
    `reuse=True` on the second one. This will make sure that the parameters are the
    same for both definitions, in practice creating only one critic.
  prefs: []
  type: TYPE_NORMAL
- en: The second observation is that we are defining the actor inside a variable scope
    called `p_mlp.` This is because, later on, we'll need to retrieve only these parameters,
    and not those of the critic.
  prefs: []
  type: TYPE_NORMAL
- en: The third observation is that, because the policy has a `tanh` function as its
    final activation layer (to constrain the values to be between -1 and 1) but our
    actor may need values out of this range, we have to multiply the output by a `max_act`
    factor (this assumes that the minimum and maximum values are opposite, that is,
    if the maximum allowed value is 3, the minimum is -3).
  prefs: []
  type: TYPE_NORMAL
- en: Nice! Let's now have a look through the remaining of the computational graph,
    where we define the placeholders; create the online and target actors, as well
    as the online and target critics; define the losses; implement the optimizers;
    and update the target networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start from the creation of the placeholders that we''ll need for the
    observations, the actions, and the target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `y_ph` is the placeholder for the target Q-values, `obs_ph`
    for the observations, and `act_ph` for the actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then call the previously defined `deterministic_actor_critic` function inside
    an `online` and `target` variable scope, so as to differentiate the four neural
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The loss of the critic is the MSE loss between the Q-values of the `qa_onl`
    online network, and the `y_ph` target action value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will be minimized with the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With regard to the actor''s loss function, it is the opposite sign of the online
    Q-network. In this case, the online Q-network has the actions chosen by the online
    deterministic actor as the input (as from formula (8.6), which was defined in
    the pseudocode of *The DDPG algorithm* section). Thus, the Q-values are represented
    by `qd_onl`, and the policy loss function is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We took the opposite sign of the objective function, because we have to convert
    it to a loss function, considering that the optimizers need to minimize a loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the most important thing to remember here is that, despite computing the
    gradient from the `p_loss` loss function that depends on both the critic and the
    actor, we only need to update the actor. Indeed, from DPG we know that ![](img/48dc9da4-7d84-47cf-8d29-860467c03f05.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is accomplished by passing `p_loss` to the `minimize` method of the optimizer,
    which specifies the variables that need updating. In this case, we need to update
    only the variables of the online actor that was defined in the `online/m_mlp`
    variable scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this way, the computation of the gradient will start from `p_loss`, go through
    the critic's network, and then the actor's network. By the end, only the parameters
    of the actor will be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to define the `variable_in_scope(scope)` function that returns
    the variables in the scope named `scope`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s now time to look at how the target networks are updated. We can use `variable_in_scope`
    to get the target and online variables of both the actors and the critics, and
    use the TensorFlow `assign` function on the target variables to update them, following
    the soft update formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c41a2d89-d577-4559-ad98-d464d69a4d12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is done in the following snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That's it! For the computational graph, that's everything. Pretty straightforward,
    right? Now we can take a quick look at the main cycle, where the parameters are
    updated, following the estimated gradient on a finite batch of samples. The interaction
    of the policy with the environment is standard, with the exception that now the
    actions that are returned by the policy are deterministic, and we have to add
    a certain amount of noise in order to adequately explore the environment. Here,
    we don't provide this part of the code, but you can find the full implementation
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a minimum amount of experience has been acquired, and the buffer has reached
    a certain threshold, the optimization of the policy and the critic starts. The
    steps that follow are those that are summarized in the DDPG pseudocode that was
    provided in *The DDPG algorithm* section. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample a mini-batch from the buffer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the target action values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimize the critic
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimize the actor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target networks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All these operations are executed in just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The first line of code samples a mini-batch of size `batch_size`, the second
    and third lines compute the target action values, as defined in equation (8.4),
    by running the critic and actor target networks on `mb_obs2`, which contains the
    next states. The fourth line optimizes the critic by feeding the dictionary with
    the target action values that were just computed, as well as the observations
    and actions. The fifth line optimizes the actor, and the last one updates the
    target networks by running `update_target_op`.
  prefs: []
  type: TYPE_NORMAL
- en: Appling DDPG to BipedalWalker-v2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now apply DDPG to a continuous task called BipedalWalker-v2, that is,
    one of the environments provided by Gym that uses Box2D, a 2D physical engine.
    A screenshot of this environment follows. The goal is to make the agent walk as
    fast as possible in rough terrains. A score of 300+ is given for moving until
    the end, but every application of the motors costs a small amount. The more optimally
    the agent moves, the less it costs. Furthermore, if the agent falls, it receives
    a reward of -100\. The state consists of 24 float numbers that represent the speeds
    and the positions of the joints and the hull, and LiDar rangefinder measurements.
    The agent is controlled by four continuous actions, with the range [-1,1]. The
    following is a screenshot of BipedalWalker 2D environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a6d8748-12b2-4a70-a6d7-a3f4ba780551.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of BipedalWalker2d environment
  prefs: []
  type: TYPE_NORMAL
- en: 'We run DDPG with the hyperparameters that are given in the following table.
    In the first row, the hyperparameters that are needed to run DDPG are listed,
    while the corresponding values that are used in this particular case are listed in
    the second row. Let''s refer to the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hyperparameter** | Actor Learning Rate | Critic Learning Rate | DNN architecture
    | Buffer Size | Batch Size | Tau |'
  prefs: []
  type: TYPE_TB
- en: '| **Value** | 3e-4 | 4e-4 | [64,relu,64,relu] | 200000 | 64 | 0.003 |'
  prefs: []
  type: TYPE_TB
- en: 'During training, we added extra noise in the actions that were predicted by
    the policy, however, to measure the performance of the algorithm, we run 10 games
    on a pure deterministic policy (without extra noise) every 10 episodes. The cumulative
    rewards that is averaged across the 10 games in the function of the timesteps is
    plotted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c73c2f1-19ad-4c2d-be39-942a66eb55cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance of the DDPG algorithm on BipedalWalker2d-v2
  prefs: []
  type: TYPE_NORMAL
- en: From the results, we can see that the performance is quite unstable, ranging
    from 250 to less than -100, after only a few thousand steps. It is known that
    DDPG is unstable and very sensitive to the hyperparameters, but with more careful
    fine-tuning, the results may be smoother. Nonetheless, we can see that the performance
    increases in the first 300k steps, reaching an average score of about 100, with
    peaks of up to 300.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, BipedalWalker-v2 is a notoriously difficult environment to solve.
    Indeed, it is considered solved when the agent obtains an average reward of at
    least 300 points, on 100 consecutive episodes. With DDPG, we aren't able to reach
    those performances, but still, we obtained a good policy that is able to make
    the agent run fairly fast.
  prefs: []
  type: TYPE_NORMAL
- en: In our implementation, we used a constant exploratory factor. By using a more
    sophisticated function, you could probably reach a higher performance in fewer
    iterations. For example, in the DDPG paper, they use an Ornstein-Uhlenbeck process.
    You can start from this process, if you wish to.
  prefs: []
  type: TYPE_NORMAL
- en: DDPG is a beautiful example of how deterministic policy can be used in contraposition
    to stochastic policies. However, because it's been the first of its kind to deal
    with complex problems, there are many further adjustments that can be applied
    to it. The next algorithm that is proposed in this chapter, takes DDPG one step
    further.
  prefs: []
  type: TYPE_NORMAL
- en: Twin delayed deep deterministic policy gradient (TD3)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DDPG is regarded as one of the most sample-efficient actor-critic algorithms,
    but it has been demonstrated to be brittle and sensitive to hyperparameters. Further
    studies have tried to alleviate these problems, by introducing novel ideas, or
    by using tricks from other algorithms on top of DDPG. Recently, one algorithm
    has taken over as a replacement of DDPG: twin delayed deep deterministic policy
    gradient, or for short, TD3 (the paper is *Addressing Function Approximation Error
    in Actor-Critic Methods*: [https://arxiv.org/pdf/1802.09477.pdf](https://arxiv.org/pdf/1802.09477.pdf)).
    We have used the word replacement here, because it''s actually a continuation
    of the DDPG algorithms, with some more ingredients that make it more stable, and
    more performant.'
  prefs: []
  type: TYPE_NORMAL
- en: TD3 focuses on some of the problems that are also common in other off-policy
    algorithms. These problems are the overestimation of the value estimate, and high-variance
    estimates of the gradient. For the former problem, they employ a solution similar
    to the one used in DQN, and for the latter, they employ two novel solutions. Let's
    first consider the overestimation bias problem.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing overestimation bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overestimation bias means that the action values that are predicted by the approximated
    Q-function are higher than what they should be. Having been widely studied in
    Q-learning algorithms with discrete actions, this often leads to bad predictions
    that affect the end performance. Despite being less affected, this problem is
    also present in DDPG.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember, the DQN variant that reduces the overestimation of the action
    values is called double DQN and it proposes two neural networks; one for choosing
    the action, and one for calculating the Q-value. In particular, the work of the
    second neural network is done by a frozen target network. This is a sound idea,
    but as explained in the TD3 paper, it isn''t effective on actor-critic methods,
    as in these methods, the policy changes too slowly. So, they propose a variation
    called clipped double Q-learning that takes the minimum between the estimates
    of two different critics (![](img/5b045c7e-76d0-452b-ad16-eb1363a7c52a.png)). Thus,
    the target value is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fb20200-fb00-4814-a4f4-746258e11d85.png)'
  prefs: []
  type: TYPE_IMG
- en: On the opposite side, this doesn't prevent an underestimation bias, but it is
    way less harmful than its overestimation. Clipped double Q-learning can be used
    in any actor-critic method, and it works following the assumption that the two
    critics will have different biases.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of TD3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To put this strategy into code, we have to create two critics with different
    initializations, compute the target action value as in (8.7), and optimize both
    critics.
  prefs: []
  type: TYPE_NORMAL
- en: TD3 is applied on the DDPG implementation that we discussed in the previous
    section. The following snippets are only a portion of the additional code that
    is needed to implement TD3\. The complete implementation is available in the GitHub
    repository of the book: [https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-Algorithms-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to the double critic, you have just to create them by calling  `deterministic_actor_double_critic` twice,
    once for the target and once for the online networks, as done in DDPG. The code
    will be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The clipped target value (![](img/f7156912-91fe-4f61-8ab6-883d68efd60d.png) (8.7))
    is implemented by first running the two target critics that we called `qa1_tar`
    and `qa2_tar`, and then calculating the minimum between the estimated values,
    and finally, using it to estimate the target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the critics can be optimized as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: An important observation to make is that the policy is optimized with respect
    to only one approximated Q-function, in our case, ![](img/6337f75c-b73a-4b3c-b2d5-43f5648fa242.png).
    In fact, if you look at the full code, you'll see that `p_loss` is defined as `p_loss
    = -tf.reduce_mean(qd1_onl)`.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing variance reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second, and last, contribution by TD3, is the reduction of the variance.
    Why is high variance a problem? Well, it provides a noisy gradient, which involves
    a wrong policy update impacting the performance of the algorithm. The complication
    of high variance arises in the TD error, which estimates the action values from
    subsequent states.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this problem, TD3 introduces a delayed policy update, and a target
    regularization technique. Let's see what they are, and why they work so well.
  prefs: []
  type: TYPE_NORMAL
- en: Delayed policy updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since high variance is attributed to an inaccurate critic, TD3 proposes to
    delay the update of the policy until the critic error is small enough. TD3 delays
    the update in an empirical way, by updating the policy only after a fixed number
    of iterations. In this manner, the critic has time to learn and stabilize itself,
    before the policy''s optimization takes place. In practice, the policy remains
    fixed only for a few iterations, typically between 1 and 6\. If set to 1, then
    it is the same as in DDPG. The delayed policy updates can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Target regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Critics that update from deterministic actions tend to overfit in narrow peaks.
    The consequence is an increase in variance. TD3 presents a smoothing regularization
    technique that adds a clipped noise to a small area near the target action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73c8b8d4-6997-43f3-94fc-42a755aee225.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The regularization can be implemented in a function that takes a vector and
    a scale as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, `add_normal_noise` is called after running the target policy, as shown
    in the following lines of code (the changes with respect to the DDPG implementation
    are written in bold):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We clipped the actions, after having added the extra noise, to make sure that
    they don't exceed the ranges that were set by the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting everything together, we obtain the algorithm that is shown in the following
    pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: That's everything for the TD3 algorithm. Now, you have a clear understanding
    of all the deterministic and non-deterministic policy gradient methods. Almost
    all of the model-free algorithms are based on the principles that we explained
    in these chapters, and if you master them, you will be able to understand and
    implement all of them.
  prefs: []
  type: TYPE_NORMAL
- en: Applying TD3 to BipedalWalker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a direct comparison of TD3 and DDPG, we tested TD3 in the same environment
    that we used for DDPG: BipedalWalker-v2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best hyperparameters for TD3 for this environment are listed in this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hyperparameter** | Actor l.r. | Critic l.r. | DNN Architecture | Buffer
    Size | Batch Size | Tau |'
  prefs: []
  type: TYPE_TB
- en: Policy Update Freq
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Sigma
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Value** | 4e-4 | 4e-4 | [64,relu,64,relu] | 200000 | 64 | 0.005 | 2 | 0.2
    |'
  prefs: []
  type: TYPE_TB
- en: 'The result is plotted in the following diagram. The curve has a smooth trend,
    and reaches good results after about 300K steps, with top peaks at 450K steps
    of training. It arrives very close to the goal of 300 points, but it does not
    actually gain them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95708f9b-ea27-40f2-a931-e9fa82d4b29b.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance of the TD3 algorithm on BipedalWalker-v2
  prefs: []
  type: TYPE_NORMAL
- en: 'The time spent finding a good set of hyperparameters for TD3 was less compared
    to DDPG. And, despite the fact that we are comparing the two algorithms on only
    one game, we think that it is a good first insight into their differences, in
    terms of stability and performance. The performance of both DDPG and TD3 on BipedalWalker-v2
    are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80e1f327-dc9d-48c4-87a8-42e15a32183c.png)'
  prefs: []
  type: TYPE_IMG
- en: DDPG versus TD3 performance comparison
  prefs: []
  type: TYPE_NORMAL
- en: If you want to train the algorithms in a harder environment, you can try BipedalWalkerHardcore-v2\.
    It is very similar to BipedalWalker-v2, with the exception that it has ladders,
    stumps, and pitfalls. Very few algorithms are able to finish and solve this environment.
    It's also funny to see how the agent fails to pass the obstacles!
  prefs: []
  type: TYPE_NORMAL
- en: The superiority of TD3 compared to DDPG is immediately clear, both in terms
    of the end performance, the rate of improvement, and the stability of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: For all the color references mentioned in the chapter, please refer to the color
    images bundle at [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we approached two different ways in which to solve an RL problem.
    The first is through the estimation of state-action values that are used to choose
    the best next action, so-called Q-learning algorithms. The second involves the
    maximization of the expected reward policy through its gradient. In fact, these
    methods are called policy gradient methods. In this chapter, we showed the advantages
    and disadvantages of such approaches, and demonstrated that many of these are
    complementary. For example, Q-learning algorithms are sample efficient but cannot
    deal with continuous action. Instead, policy gradient algorithms require more
    data, but are able to control agents with continuous actions. We then introduced
    DPG methods that combine Q-learning and policy gradient techniques. In particular,
    these methods overcome the global maximization of the Q-learning algorithms by
    predicting a deterministic policy. We also saw how the DPG theorem defines the
    deterministic policy update through the gradient of the Q-function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned and implemented two DPG algorithms: DDPG and TD3\. Both are off-policy,
    actor-critic algorithms that can be used in environments with continuous action
    spaces. TD3 is an upgrade of DDPG that encapsulates a few tricks for the reduction
    of variance, and to limit the overestimation bias that is common in Q-learning
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes the overview of the model-free reinforcement learning
    algorithms. We took a look at all the best, and most influential algorithms known
    so far, from SARSA to DQN, and from REINFORCE to PPO, and combined them in algorithms
    such as DDPG and TD3\. These algorithms alone are capable of amazing things, with
    the right fine-tuning and a large amount of data (see OpenAI Five and AlphaStar).
    However, this isn't all there is to know about RL. In the next chapter, we move
    away from model-free algorithms, showing a model-based algorithm whose intent
    is to reduce the amount of data that is required for learning a task, by learning
    a model of the environment. In subsequent chapters, we'll also show more advanced
    techniques, such as imitation learning, new useful RL algorithms such as ESBAS,
    and non-RL algorithms such as evolutional strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the primary limitation of Q-learning algorithms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are stochastic gradient algorithms sample inefficient?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does DPG overcome the maximization problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does DPG guarantee enough exploration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does DDPG stand for? And what is its main contribution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What problems does TD3 propose to minimize?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What new mechanisms does TD3 employ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can use the following links to learn more:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in the paper that introduced the **Deterministic Policy
    Gradient** (**DPG**) algorithm, read: [http://proceedings.mlr.press/v32/silver14.pdf](http://proceedings.mlr.press/v32/silver14.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are interested in the paper that introduced the **Deep Deterministic
    Policy Gradient **(**DDPG**) algorithm, read: [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper that presented **Twin Delayed Deep Deterministic** Policy Gradient
    (**TD3**) can be found here: [https://arxiv.org/pdf/1802.09477.pdf](https://arxiv.org/pdf/1802.09477.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a brief overview of all the main policy gradient algorithms, checkout this
    article by Lilian Weng: [https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
