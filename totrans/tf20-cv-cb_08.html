<html><head></head><body>
		<div id="_idContainer095">
			<h1 id="_idParaDest-229"><em class="italic"><a id="_idTextAnchor270"/>Chapter 8</em>: Fine-Grained Understanding of Images through Segmentation</h1>
			<p>Image segmentation is one of the biggest areas of study in computer vision. It consists of simplifying the visual contents of an image by grouping together pixels that share one or more defining characteristics, such as location, color, or texture. As is the case with many other subareas of computer vision, image segmentation has been greatly boosted by deep neural networks, mainly in industries such as medicine and autonomous driving.</p>
			<p>While it's great to classify the contents of an image, more often than not, it's not enough. What if we want to know exactly where an object is? What if we're interested in its shape? What if we need its contour? These fine-grained needs cannot be met with traditional classification techniques. However, as we'll discover in this chapter, we can frame an image segmentation problem in a very similar way to a regular classification project. How? Instead of labeling the image as a whole, we'll label each pixel! This is known as image segmentation and is what constitutes the recipes in this chapter.</p>
			<p>In this chapter, we will cover the following recipes:</p>
			<ul>
				<li>Creating a fully convolutional network for image segmentation</li>
				<li>Implementing a U-Net from scratch</li>
				<li><a id="_idTextAnchor271"/><a id="_idTextAnchor272"/>Implementing a U-Net with transfer learning</li>
				<li>Segmenting images using Mask-RCNN and TensorFlow Hub </li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor273"/>Technical requirements</h1>
			<p>In order to implement and experiment with the recipes in this chapter, it's recommended that you have access to a GPU. If you have recourse to a cloud-based provider, such as AWS or FloydHub, that's great, but keep the fees attached to them in mind as they might skyrocket if you're not careful! In the <em class="italic">Getting ready</em> section of each recipe, you'll find everything you'll need to prepare for what lies ahead. The code for this chapter is available here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch8">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch8</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/2Na77IF">https://bit.ly/2Na77IF</a>.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor274"/><a id="_idTextAnchor275"/>Creating a fully convolutional network for image segmentation</h1>
			<p>If you were to <a id="_idIndexMarker720"/>create your <a id="_idIndexMarker721"/>first network for image segmentation while knowing that, at its core, segmenting is just pixel-wise classification, what would you do? You would probably take a battle-tested architecture and swap the final layers (usually fully connected ones) with convolutions in order to produce an output volume, instead of an output vector.</p>
			<p>Well, that's exactly what we'll do in this recipe to build a <strong class="bold">Fully Convolutional Network</strong> (<strong class="bold">FCN</strong>) for image segmentation based on the famous <strong class="bold">VGG16</strong> network.</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor276"/>Getting ready</h2>
			<p>We need to install a couple of external libraries, starting with <strong class="source-inline">tensorflow_docs</strong>:</p>
			<p class="source-code">$&gt; pip install git+https://github.com/tensorflow/docs</p>
			<p>Next, we need to install TensorFlow Datasets, <strong class="source-inline">Pillow</strong>, and <strong class="source-inline">OpenCV</strong>:</p>
			<p class="source-code">$&gt; pip install tensorflow-datasets Pillow opencv-contrib-python</p>
			<p>Regarding the data, we will segment images from <strong class="source-inline">the Oxford-IIIT Pet</strong> dataset. The good news is that we'll access it using <strong class="source-inline">tensorflow-datasets</strong>, so we don't really need to do anything in that respect here. Each pixel in this dataset is classified as follows:</p>
			<ul>
				<li>1: The pixel belongs to a pet (cat or dog).</li>
				<li>2: The pixel belongs to the contour of a pet.</li>
				<li>3: The pixel belongs to the surroundings.</li>
			</ul>
			<p>Here are some sample images from the dataset:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B14768_08_001.jpg" alt="Figure 8.1 – Sample images from the Oxford-IIIT Pet dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Sample images from the Oxford-IIIT Pet dataset</p>
			<p>Let's start implementing!</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor277"/>How to do it…</h2>
			<p>Follow<a id="_idIndexMarker722"/> these steps to<a id="_idIndexMarker723"/> complete this recipe:</p>
			<ol>
				<li>Import all the required packages:<p class="source-code">import pathlib</p><p class="source-code">import cv2</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">import tensorflow_docs as tfdocs</p><p class="source-code">import tensorflow_docs.plots</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import \</p><p class="source-code">    SparseCategoricalCrossentropy</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p></li>
				<li>Define an alias for <strong class="source-inline">tf.data.experimental.AUTOTUNE</strong>:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE </p></li>
				<li>Define a function that will normalize the images in the dataset to the range [0, 1]. Just for<a id="_idIndexMarker724"/> consistency's <a id="_idIndexMarker725"/>sake, we'll subtract one from each pixel in the mask so that they go from 0 all the way to 2:<p class="source-code">def normalize(input_image, input_mask):</p><p class="source-code">   input_image = tf.cast(input_image, tf.float32) / 255.0</p><p class="source-code">    input_mask -= 1</p><p class="source-code">    return input_image, input_mask</p></li>
				<li>Define the <strong class="source-inline">load_image()</strong> function, which loads both the image and its mask, given a TensorFlow dataset element. We will seize the opportunity to resize the images to <em class="italic">256x256</em> here. Also, if the <strong class="source-inline">train</strong> flag is set to <strong class="source-inline">True</strong>, we can perform a bit of augmentation by randomly mirroring the image and its mask. Lastly, we must normalize the inputs:<p class="source-code">@tf.function</p><p class="source-code">def load_image(dataset_element, train=True):</p><p class="source-code">  input_image = tf.image.resize(dataset_element['image'],</p><p class="source-code">                                  (256, 256))</p><p class="source-code">    input_mask = tf.image.resize(</p><p class="source-code">        dataset_element['segmentation_mask'], (256, 256))</p><p class="source-code">    if train and np.random.uniform() &gt; 0.5:</p><p class="source-code">        input_image = </p><p class="source-code">              tf.image.flip_left_right(input_image)</p><p class="source-code">        input_mask = tf.image.flip_left_right(input_mask)</p><p class="source-code">    input_image, input_mask = normalize(input_image,</p><p class="source-code">                                        input_mask)</p><p class="source-code">    return input_image, input_mask</p></li>
				<li>Implement<a id="_idIndexMarker726"/> the <strong class="source-inline">FCN()</strong> class, which<a id="_idIndexMarker727"/> encapsulates all the logic required to build, train, and evaluate our <strong class="bold">FCN</strong> image segmentation model. First, define the constructor:<p class="source-code">class FCN(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 input_shape=(256, 256, 3),</p><p class="source-code">                 output_channels=3):</p><p class="source-code">        self.input_shape = input_shape</p><p class="source-code">        self.output_channels = output_channels</p><p class="source-code">        self.vgg_weights_path = str(pathlib.Path.home() /</p><p class="source-code">                                    '.keras' / </p><p class="source-code">                                       'models' /</p><p class="source-code">                             'vgg16_weights_tf_dim_'</p><p class="source-code">                             'ordering_tf_kernels.h5')</p><p class="source-code">        self.model = self._create_model()</p><p class="source-code">        loss = SparseCategoricalCrossentropy(from_</p><p class="source-code">                                          logits=True)</p><p class="source-code">        self.model.compile(optimizer=RMSprop(),</p><p class="source-code">                           loss=loss,</p><p class="source-code">                           metrics=['accuracy'])</p><p>In this step, we <a id="_idIndexMarker728"/>are<a id="_idIndexMarker729"/> creating the model, which we'll train using <strong class="source-inline">RMSProp</strong> as the optimizer and <strong class="source-inline">SparseCategoricalCrossentropy</strong> as the loss. Notice that <strong class="source-inline">output_channels</strong> is, by default, 3, because each pixel can be categorized into one of three classes. Also, notice that we are defining the path to the weights of the <strong class="bold">VGG16</strong> this model is based on. We'll use these weights to give our network a head start when training. </p></li>
				<li>Now, it's time to define the architecture itself:<p class="source-code">def _create_model(self):</p><p class="source-code">        input = Input(shape=self.input_shape)</p><p class="source-code">        x = Conv2D(filters=64,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block1_conv1')(input)</p><p class="source-code">        x = Conv2D(filters=64,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block1_conv2')(x)</p><p class="source-code">        x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                         strides=2,</p><p class="source-code">                         name='block1_pool')(x)</p></li>
				<li>We started by<a id="_idIndexMarker730"/> defining<a id="_idIndexMarker731"/> the input and the first block of convolutions and max pooling layers. Now, define the second block of convolutions, this time with 128 filters each:<p class="source-code">        x = Conv2D(filters=128,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block2_conv1')(x)</p><p class="source-code">        x = Conv2D(filters=128,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block2_conv2')(x)</p><p class="source-code">        x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                         strides=2,</p><p class="source-code">              </p><p class="source-code">           name='block2_pool')(x)</p></li>
				<li>The third <a id="_idIndexMarker732"/>block contains<a id="_idIndexMarker733"/> convolutions with 256 filters:<p class="source-code">        x = Conv2D(filters=256,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block3_conv1')(x)</p><p class="source-code">        x = Conv2D(filters=256,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block3_conv2')(x)</p><p class="source-code">        x = Conv2D(filters=256,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block3_conv3')(x)</p><p class="source-code">        x = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                         strides=2,</p><p class="source-code">                         name='block3_pool')(x)</p><p class="source-code">        block3_pool = x</p></li>
				<li>The fourth<a id="_idIndexMarker734"/> block <a id="_idIndexMarker735"/>uses convolutions with 512 filters:<p class="source-code">        x = Conv2D(filters=512,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block4_conv1')(x)</p><p class="source-code">        x = Conv2D(filters=512,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block4_conv2')(x)</p><p class="source-code">        x = Conv2D(filters=512,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block4_conv3')(x)</p><p class="source-code">        block4_pool = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                                   strides=2,</p><p class="source-code">                             name='block4_pool')(x)</p></li>
				<li>The fifth block is a repetition of block four, again with 512 filter-deep convolutions:<p class="source-code">        x = Conv2D(filters=512,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block5_conv1')(block4_pool)</p><p class="source-code">        x = Conv2D(filters=512,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block5_conv2')(x)</p><p class="source-code">        x = Conv2D(filters=512,</p><p class="source-code">                   kernel_size=(3, 3),</p><p class="source-code">                   activation='relu',</p><p class="source-code">                   padding='same',</p><p class="source-code">                   name='block5_conv3')(x)</p><p class="source-code">        block5_pool = MaxPooling2D(pool_size=(2, 2),</p><p class="source-code">                                   strides=2,</p><p class="source-code">                               name='block5_pool')(x)</p></li>
				<li>The reason we've<a id="_idIndexMarker736"/> been<a id="_idIndexMarker737"/> naming the layers so far is so that we can match them with the pre-trained weights we'll import next (notice <strong class="source-inline">by_name=True</strong>):<p class="source-code">        model = Model(input, block5_pool)</p><p class="source-code">        model.load_weights(self.vgg_weights_path,</p><p class="source-code">                           by_name=True)</p></li>
				<li><strong class="source-inline">output</strong>, in a traditional <strong class="bold">VGG16</strong> architecture, is comprised of fully connected layers. However, we'll be replacing them with transposed convolutions. Notice we are <a id="_idIndexMarker738"/>connecting <a id="_idIndexMarker739"/>these layers to the output of the fifth block:<p class="source-code">        output = Conv2D(filters=self.output_channels,</p><p class="source-code">                        kernel_size=(7, 7),</p><p class="source-code">                        activation='relu',</p><p class="source-code">                        padding='same',</p><p class="source-code">                        name='conv6')(block5_pool)</p><p class="source-code">        conv6_4 = Conv2DTranspose(</p><p class="source-code">            filters=self.output_channels,</p><p class="source-code">            kernel_size=(4, 4),</p><p class="source-code">            strides=4,</p><p class="source-code">            use_bias=False)(output)</p></li>
				<li>Create a 1x1 convolution, followed by a transposed convolution, and connect it to the output of the fourth block (this is, indeed, a skip connection):<p class="source-code">        pool4_n = Conv2D(filters=self.output_channels,</p><p class="source-code">                         kernel_size=(1, 1),</p><p class="source-code">                         activation='relu',</p><p class="source-code">                         padding='same',</p><p class="source-code">                         name='pool4_n')(block4_pool)</p><p class="source-code">        pool4_n_2 = Conv2DTranspose(</p><p class="source-code">            filters=self.output_channels,</p><p class="source-code">            kernel_size=(2, 2),</p><p class="source-code">            strides=2,</p><p class="source-code">            use_bias=False)(pool4_n)</p></li>
				<li>Pass the output of the third block through a 1x1 convolution. Then, merge these three paths into one and pass them through a final transposed convolution. This will be<a id="_idIndexMarker740"/> activated<a id="_idIndexMarker741"/> with <strong class="source-inline">Softmax</strong>. This output constitutes the segmentation mask predicted by the model:<p class="source-code">        pool3_n = Conv2D(filters=self.output_channels,</p><p class="source-code">                         kernel_size=(1, 1),</p><p class="source-code">                         activation='relu',</p><p class="source-code">                         padding='same',</p><p class="source-code">                         name='pool3_n')(block3_pool)</p><p class="source-code">        output = Add(name='add')([pool4_n_2,</p><p class="source-code">                                  pool3_n,</p><p class="source-code">                                  conv6_4])</p><p class="source-code">        output = Conv2DTranspose</p><p class="source-code">                       (filters=self.output_channels,</p><p class="source-code">                                 kernel_size=(8, 8),</p><p class="source-code">                                 strides=8,</p><p class="source-code">                             use_bias=False)(output)</p><p class="source-code">        output = Softmax()(output)</p><p class="source-code">        return Model(input, output)</p></li>
				<li>Now, let's create a private helper method to plot the relevant training curves:<p class="source-code">    @staticmethod</p><p class="source-code">    def _plot_model_history(model_history, metric, </p><p class="source-code">                                        ylim=None):</p><p class="source-code">        plt.style.use('seaborn-darkgrid')</p><p class="source-code">        plotter = tfdocs.plots.HistoryPlotter()</p><p class="source-code">        plotter.plot({'Model': model_history}, </p><p class="source-code">                            metric=metric)</p><p class="source-code">        plt.title(f'{metric.upper()}')</p><p class="source-code">        if ylim is None:</p><p class="source-code">            plt.ylim([0, 1])</p><p class="source-code">        else:</p><p class="source-code">            plt.ylim(ylim)</p><p class="source-code">        plt.savefig(f'{metric}.png')</p><p class="source-code">        plt.close()</p></li>
				<li>The <strong class="source-inline">train()</strong> method <a id="_idIndexMarker742"/>takes<a id="_idIndexMarker743"/> the training and validation datasets, as well as the number of epochs and training and validation steps to perform, in order to fit the model. It also saves the loss and accuracy plots to disk for later analysis:<p class="source-code">    def train(self, train_dataset, epochs, </p><p class="source-code">                     steps_per_epoch,</p><p class="source-code">              validation_dataset, validation_steps):</p><p class="source-code">        hist = \</p><p class="source-code">            self.model.fit(train_dataset,</p><p class="source-code">                           epochs=epochs,</p><p class="source-code">                   steps_per_epoch=steps_per_epoch,</p><p class="source-code">                  validation_steps=validation_steps,</p><p class="source-code">                  validation_data=validation_dataset)</p><p class="source-code">        self._plot_model_history(hist, 'loss', [0., 2.0])</p><p class="source-code">        self._plot_model_history(hist, 'accuracy')</p></li>
				<li>Implement <strong class="source-inline">_process_mask()</strong>, which is used to make the segmentation masks<a id="_idIndexMarker744"/> compatible <a id="_idIndexMarker745"/>with OpenCV. What this function does is create a three-channeled version of a grayscale mask and upscale the class values to the [0, 255] range:<p class="source-code">    @staticmethod</p><p class="source-code">    def _process_mask(mask):</p><p class="source-code">        mask = (mask.numpy() * 127.5).astype('uint8')</p><p class="source-code">        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)</p><p class="source-code">        return mask</p></li>
				<li>The <strong class="source-inline">_save_image_and_masks()</strong> helper method creates a mosaic of the original image, the ground truth mask, and the predicted segmentation mask, and then saves it to disk for later revision:<p class="source-code">    def _save_image_and_masks(self, image,</p><p class="source-code">                              ground_truth_mask,</p><p class="source-code">                              prediction_mask,</p><p class="source-code">                              image_id):</p><p class="source-code">        image = (image.numpy() * 255.0).astype('uint8')</p><p class="source-code">        gt_mask = self._process_mask(ground_truth_mask)</p><p class="source-code">        pred_mask = self._process_mask(prediction_mask)</p><p class="source-code">        mosaic = np.hstack([image, gt_mask, pred_mask])</p><p class="source-code">        mosaic = cv2.cvtColor(mosaic, cv2.COLOR_RGB2BGR)</p><p class="source-code">        cv2.imwrite(f'mosaic_{image_id}.jpg', mosaic)</p></li>
				<li>In order to <a id="_idIndexMarker746"/>pass <a id="_idIndexMarker747"/>the output volume produced by the network to a valid segmentation mask, we must take the index with the highest value at each pixel location. This corresponds to the most likely category for that pixel. The <strong class="source-inline">_create_mask()</strong> method does this:<p class="source-code">    @staticmethod</p><p class="source-code">    def _create_mask(prediction_mask):</p><p class="source-code">        prediction_mask = tf.argmax(prediction_mask, </p><p class="source-code">                                     axis=-1)</p><p class="source-code">        prediction_mask = prediction_mask[..., </p><p class="source-code">                                          tf.newaxis]</p><p class="source-code">        return prediction_mask[0]</p></li>
				<li>The <strong class="source-inline">_save_predictions()</strong> method uses the <strong class="bold">FCN</strong> to predict the mask of a sample of images in the input dataset. It then saves the result to disk using the <strong class="source-inline">_save_image_and_mask()</strong> helper method, which we defined in <em class="italic">Step 18</em>:<p class="source-code">    def _save_predictions(self, dataset, </p><p class="source-code">                           sample_size=1):</p><p class="source-code">        for id, (image, mask) in \</p><p class="source-code">                enumerate(dataset.take(sample_size), </p><p class="source-code">                                        start=1):</p><p class="source-code">            pred_mask = self.model.predict(image)</p><p class="source-code">            pred_mask = self._create_mask(pred_mask)</p><p class="source-code">            image = image[0]</p><p class="source-code">            ground_truth_mask = mask[0]</p><p class="source-code">            self._save_image_and_masks(image,</p><p class="source-code">                                  ground_truth_mask,</p><p class="source-code">                                       pred_mask,</p><p class="source-code">                                       image_id=id)</p></li>
				<li>The <strong class="source-inline">evaluate()</strong> method<a id="_idIndexMarker748"/> computes<a id="_idIndexMarker749"/> the accuracy of the <strong class="bold">FCN</strong> on the test set and generates predictions for a sample of images, which are then stored on disk:<p class="source-code">    def evaluate(self, test_dataset, sample_size=5):</p><p class="source-code">        result = self.model.evaluate(test_dataset)</p><p class="source-code">        print(f'Accuracy: {result[1] * 100:.2f}%')</p><p class="source-code">        self._save_predictions(test_dataset, </p><p class="source-code">                                sample_size)</p></li>
				<li>Download (or<a id="_idIndexMarker750"/> load, if cached) <strong class="source-inline">Oxford IIIT Pet Dataset</strong>, along with its metadata, using <strong class="bold">TensorFlow Datasets</strong>:<p class="source-code">dataset, info = tfdata.load('oxford_iiit_pet', </p><p class="source-code">                             with_info=True)</p></li>
				<li>Use the <a id="_idIndexMarker751"/>metadata to <a id="_idIndexMarker752"/>define the corresponding number of steps the network will take over the training and validation datasets. Also, define the batch and buffer sizes:<p class="source-code">TRAIN_SIZE = info.splits['train'].num_examples</p><p class="source-code">VALIDATION_SIZE = info.splits['test'].num_examples</p><p class="source-code">BATCH_SIZE = 32</p><p class="source-code">STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE</p><p class="source-code">VALIDATION_SUBSPLITS = 5</p><p class="source-code">VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE</p><p class="source-code">VALIDATION_STEPS //= VALIDATION_SUBSPLITS</p><p class="source-code">BUFFER_SIZE = 1000</p></li>
				<li>Define the training and testing datasets' pipelines:<p class="source-code">train_dataset = (dataset['train']</p><p class="source-code">                 .map(load_image, num_parallel_</p><p class="source-code">                 calls=AUTOTUNE)</p><p class="source-code">                 .cache()</p><p class="source-code">                 .shuffle(BUFFER_SIZE)</p><p class="source-code">                 .batch(BATCH_SIZE)</p><p class="source-code">                 .repeat()</p><p class="source-code">                 .prefetch(buffer_size=AUTOTUNE))</p><p class="source-code">test_dataset = (dataset['test']</p><p class="source-code">                .map(lambda d: load_image(d,train=False),</p><p class="source-code">                     num_parallel_calls=AUTOTUNE)</p><p class="source-code">                .batch(BATCH_SIZE))</p></li>
				<li>Instantiate<a id="_idIndexMarker753"/> the <strong class="bold">FCN</strong> and<a id="_idIndexMarker754"/> train it for 120 epochs:<p class="source-code">fcn = FCN(output_channels=3)</p><p class="source-code">fcn.train(train_dataset,</p><p class="source-code">          epochs=120,</p><p class="source-code">          steps_per_epoch=STEPS_PER_EPOCH,</p><p class="source-code">          validation_steps=VALIDATION_STEPS,</p><p class="source-code">          validation_dataset=test_dataset)</p></li>
				<li>Lastly, evaluate the network on the test dataset:<p class="source-code">unet.evaluate(test_dataset)</p><p>As shown in the following graph, the accuracy on the test set should be around 84% (specifically, I got 84.47%): </p></li>
			</ol>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B14768_08_002.jpg" alt="Figure 8.2 – Training and validation accuracy curves&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Training and validation accuracy curves</p>
			<p>The training<a id="_idIndexMarker755"/> curves <a id="_idIndexMarker756"/>display a healthy behavior, meaning that the network did, indeed, learn. However, the true test is to visually assess the results:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B14768_08_003.jpg" alt="Figure 8.3 – The original image (left), the ground truth mask (center), and the predicted mask (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – The original image (left), the ground truth mask (center), and the predicted mask (right)</p>
			<p>In the preceding image, we can see that the mask that was produced by the network follows the shape of the ground truth segmentation. However, there's an unsatisfying pixelated effect across the segments, as well as noise in the upper-right corner. Let's take a look at another example:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/Image86700.jpg" alt="Figure 8.4 – The original image (left), the ground truth mask (center), and the predicted mask (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – The original image (left), the ground truth mask (center), and the predicted mask (right)</p>
			<p>In the preceding image, we can see a very deficient, spotty, and overall low-quality mask<a id="_idIndexMarker757"/> that <a id="_idIndexMarker758"/>proves that the network still needs a lot of improvement. This could be achieved by doing more fine-tuning and experimentation. However, in the next recipe, we'll discover a network that's best suited to performing image segmentation and capable of producing a really good mask with way less effort.</p>
			<p>We'll discuss what we've just done in the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor278"/>How it works…</h2>
			<p>In this recipe, we implemented an <strong class="bold">FCN</strong> for image segmentation. Even though we adapted a well-known architecture, <strong class="bold">VGG16</strong>, to our purposes, in reality, there are many different <a id="_idIndexMarker759"/>adaptations<a id="_idIndexMarker760"/> of <strong class="bold">FCNs</strong> that extend or modify other seminal architectures, such as <strong class="bold">ResNet50</strong>, <strong class="bold">DenseNet</strong>, and other variants of <strong class="bold">VGG</strong>.</p>
			<p>What we need to remember is that <strong class="bold">FCN</strong> is more of a template than a concrete implementation. Such a template consists of swapping the fully connected layers at the end of these networks, which are often used for traditional image classification, with 1x1 convolutions and upsampling layers (either <strong class="source-inline">UpSampling2D()</strong> with bilinear interpolation or <strong class="source-inline">ConvTranspose2D()</strong>). The achieved result is that instead of classifying the whole image with an output vector of probabilities, we produce an output volume that has the same dimensions as the input image, where each pixel contains a<a id="_idIndexMarker761"/> probability<a id="_idIndexMarker762"/> distribution of the classes it can belong to. Such an output volume of pixel-wise likelihood is known as a predicted segmentation mask.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor279"/>See also</h2>
			<p>You can read <a id="_idIndexMarker763"/>more about <strong class="bold">FCNs</strong> here: <a href="https://arxiv.org/abs/1411.4038">https://arxiv.org/abs/1411.4038</a>. If you want to find out more about <strong class="source-inline">Oxford IIIT Pet Dataset</strong>, visit the official site here: <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">https://www.robots.ox.ac.uk/~vgg/data/pets/</a>. </p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor280"/>Implementing a U-Net from scratch</h1>
			<p>It's difficult <a id="_idIndexMarker764"/>to talk about image segmentation without mentioning <strong class="bold">U-Net</strong>, one of the seminal architectures when it comes to pixel-wise classification.</p>
			<p>A <strong class="bold">U-Net</strong> is a composite network comprised of an encoder and a decoder, whose layers, as the name suggests, are arranged in a U shape. It's intended for fast and precise segmentation, and in this recipe, we'll implement one from scratch. </p>
			<p>Let's get started, shall we?</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor281"/>Getting ready</h2>
			<p>In this example, we'll rely on several external libraries, such as TensorFlow Datasets, TensorFlow Docs, <strong class="source-inline">Pillow</strong>, and <strong class="source-inline">OpenCV</strong>. The good news is that we can easily install them all with <strong class="source-inline">pip</strong>. First, install <strong class="source-inline">tensorflow_docs</strong>, as follows:</p>
			<p class="source-code">$&gt; pip install git+https://github.com/tensorflow/docs</p>
			<p>Next, install the remaining libraries:</p>
			<p class="source-code">$&gt; pip install tensorflow-datasets Pillow opencv-contrib-python</p>
			<p>We will be using <strong class="source-inline">Oxford-IIIT Pet Dataset</strong> in this recipe. However, we don't need to do anything at this stage since we'll download it and manipulate it using <strong class="source-inline">tensorflow-datasets</strong>. In this dataset, the segmentation mask (an image where each location contains the class of the corresponding pixel in the original image) contains pixels categorized into three classes:</p>
			<ul>
				<li>1: The pixel belongs to a pet (cat or dog).</li>
				<li>2: The pixel belongs to the contour of a pet.</li>
				<li><a id="_idTextAnchor282"/>3: The pixel belongs to the surroundings.</li>
			</ul>
			<p>Here are some <a id="_idIndexMarker765"/>sample images from the dataset:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B14768_08_005.jpg" alt="Figure 8.5 – Sample images from the Oxford-IIIT Pet dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Sample images from the Oxford-IIIT Pet dataset</p>
			<p>Great! Let's start implementing! </p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor283"/>How to do it…</h2>
			<p>Follow these steps to implement your own <strong class="bold">U-Net</strong> so that you can segment images of your own pets:</p>
			<ol>
				<li value="1">Let's import all the necessary dependencies:<p class="source-code">import cv2</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfdata</p><p class="source-code">import tensorflow_docs as tfdocs</p><p class="source-code">import tensorflow_docs.plots</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import \</p><p class="source-code">    SparseCategoricalCrossentropy</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p></li>
				<li>Define an alias for <strong class="source-inline">tf.data.experimental.AUTOTUNE</strong>:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE </p></li>
				<li>Define a <a id="_idIndexMarker766"/>function that will normalize the images in the dataset. We must also normalize the masks so that the classes are numbered from 0 through 2, instead of from 1 through 3:<p class="source-code">def normalize(input_image, input_mask):</p><p class="source-code">    input_image = tf.cast(input_image, tf.float32) / 255.0</p><p class="source-code">    input_mask -= 1</p><p class="source-code">    return input_image, input_mask</p></li>
				<li>Define a function that will load an image, given an element from a TensorFlow dataset data structure. Note that we resize both the image and the mask to <em class="italic">256x256</em>. Also, if the <strong class="source-inline">train</strong> flag is set to <strong class="source-inline">True</strong>, we perform augmentation by randomly mirroring the image and its mask. Finally, we normalize the inputs:<p class="source-code">@tf.function</p><p class="source-code">def load_image(dataset_element, train=True):</p><p class="source-code">  input_image = tf.image.resize(dataset element['image'],</p><p class="source-code">                                  (256, 256))</p><p class="source-code">    input_mask = tf.image.resize(</p><p class="source-code">        dataset_element['segmentation_mask'],(256, 256))</p><p class="source-code">    if train and np.random.uniform() &gt; 0.5:</p><p class="source-code">      input_image = tf.image.flip_left_right(input_image)</p><p class="source-code">        input_mask = tf.image.flip_left_right(input_mask)</p><p class="source-code">    input_image, input_mask = normalize(input_image,</p><p class="source-code">                                        input_mask)</p><p class="source-code">    return input_image, input_mask</p></li>
				<li>Now, let's define<a id="_idIndexMarker767"/> a class, <strong class="source-inline">UNet()</strong>, that will contain all the logic necessary to build, train, and evaluate our <strong class="bold">U-Net</strong>. First, let's define the constructor:<p class="source-code">class UNet(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 input_size=(256, 256, 3),</p><p class="source-code">                 output_channels=3):</p><p class="source-code">        self.input_size = input_size</p><p class="source-code">        self.output_channels = output_channels</p><p class="source-code">        self.model = self._create_model()</p><p class="source-code">        loss = SparseCategoricalCrossentropy(from_logits=True)</p><p class="source-code">        self.model.compile(optimizer=RMSprop(),</p><p class="source-code">                           loss=loss,</p><p class="source-code">                           metrics=['accuracy'])</p><p>In this step, we are creating the model, which we'll train using <strong class="source-inline">RMSProp</strong> as the optimizer and <strong class="source-inline">SparseCategoricalCrossentropy</strong> as the loss. Note that <strong class="source-inline">output_channels</strong> is, by default, <strong class="source-inline">3</strong>, because each pixel can be categorized into one of three classes.</p></li>
				<li>Now, let's <a id="_idIndexMarker768"/>define the <strong class="source-inline">_downsample()</strong> helper method, which builds a downsampling block. This is a convolution that can be (optionally) batch normalized and that's activated with <strong class="source-inline">LeakyReLU</strong>:<p class="source-code">    @staticmethod</p><p class="source-code">    def _downsample(filters, size, batch_norm=True):</p><p class="source-code">    initializer = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        layers = Sequential()</p><p class="source-code">        layers.add(Conv2D(filters=filters,</p><p class="source-code">                          kernel_size=size,</p><p class="source-code">                          strides=2,</p><p class="source-code">                          padding='same',</p><p class="source-code">                          kernel_initializer=initializer,</p><p class="source-code">                          use_bias=False))</p><p class="source-code">        if batch_norm:</p><p class="source-code">            layers.add(BatchNormalization())</p><p class="source-code">        layers.add(LeakyReLU())</p><p class="source-code">        return layers</p></li>
				<li>Conversely, the <strong class="source-inline">_upsample()</strong> helper method expands its input through a transposed convolution, which is also batch normalized and <strong class="source-inline">ReLU</strong> activated (optionally, we<a id="_idIndexMarker769"/> can add a dropout layer to prevent overfitting):<p class="source-code">    def _upsample(filters, size, dropout=False):</p><p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        layers = Sequential()</p><p class="source-code">        layers.add(Conv2DTranspose(filters=filters,</p><p class="source-code">                                   kernel_size=size,</p><p class="source-code">                                   strides=2,</p><p class="source-code">                                   padding='same',</p><p class="source-code">                             kernel_initializer=init,</p><p class="source-code">                                   use_bias=False))</p><p class="source-code">        layers.add(BatchNormalization())</p><p class="source-code">        if dropout:</p><p class="source-code">            layers.add(Dropout(rate=0.5))</p><p class="source-code">        layers.add(ReLU())</p><p class="source-code">        return layers</p></li>
				<li>Armed with <strong class="source-inline">_downsample()</strong> and <strong class="source-inline">_upsample()</strong>, we can iteratively build the full <strong class="bold">U-Net</strong> architecture. The encoding part of the network is just a stack of downsampling blocks, while the decoding portion is, as expected, comprised of a series of <a id="_idIndexMarker770"/>upsampling blocks:<p class="source-code">    def _create_model(self):</p><p class="source-code">        down_stack = [self._downsample(64, 4,</p><p class="source-code">                                  batch_norm=False)]</p><p class="source-code">        for filters in (128, 256, 512, 512, 512, 512, </p><p class="source-code">                          512):</p><p class="source-code">            down_block = self._downsample(filters, 4)</p><p class="source-code">            down_stack.append(down_block)</p><p class="source-code">        up_stack = []</p><p class="source-code">        for _ in range(3):</p><p class="source-code">            up_block = self._upsample(512, 4, </p><p class="source-code">                                      dropout=True)</p><p class="source-code">            up_stack.append(up_block)</p><p class="source-code">        for filters in (512, 256, 128, 64):</p><p class="source-code">            up_block = self._upsample(filters, 4)</p><p class="source-code">            up_stack.append(up_block)</p></li>
				<li>In order to shield the network against the vanishing gradient problem (a phenomenon where very deep networks forget what they've learned), we must add skip connections at every level:<p class="source-code">        inputs = Input(shape=self.input_size)</p><p class="source-code">        x = inputs</p><p class="source-code">        skip_layers = []</p><p class="source-code">        for down in down_stack:</p><p class="source-code">            x = down(x)</p><p class="source-code">            skip_layers.append(x)</p><p class="source-code">        skip_layers = reversed(skip_layers[:-1])</p><p class="source-code">        for up, skip_connection in zip(up_stack, </p><p class="source-code">                                       skip_layers):</p><p class="source-code">            x = up(x)</p><p class="source-code">            x = Concatenate()([x, skip_connection])</p><p>The output<a id="_idIndexMarker771"/> layer of the <strong class="bold">U-Net</strong> is a transposed convolution whose dimensions are the same as the input image's, but it has as many channels as there are classes in the segmentation mask:</p><p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        output = Conv2DTranspose(</p><p class="source-code">            filters=self.output_channels,</p><p class="source-code">            kernel_size=3,</p><p class="source-code">            strides=2,</p><p class="source-code">            padding='same',</p><p class="source-code">            kernel_initializer=init)(x)</p><p class="source-code">        return Model(inputs, outputs=output)</p></li>
				<li>Let's<a id="_idIndexMarker772"/> define a <strong class="source-inline">helper</strong> method in order to plot the relevant training curves:<p class="source-code">    @staticmethod</p><p class="source-code">    def _plot_model_history(model_history, metric, </p><p class="source-code">                            ylim=None):</p><p class="source-code">        plt.style.use('seaborn-darkgrid')</p><p class="source-code">        plotter = tfdocs.plots.HistoryPlotter()</p><p class="source-code">        plotter.plot({'Model': model_history}, </p><p class="source-code">                           metric=metric)</p><p class="source-code">        plt.title(f'{metric.upper()}')</p><p class="source-code">        if ylim is None:</p><p class="source-code">            plt.ylim([0, 1])</p><p class="source-code">        else:</p><p class="source-code">            plt.ylim(ylim)</p><p class="source-code">        plt.savefig(f'{metric}.png')</p><p class="source-code">        plt.close()</p></li>
				<li>The <strong class="source-inline">train()</strong> method takes the training and validation datasets, as well as the number of epochs and training and validation steps to perform, in order to fit the model. It also saves the loss and accuracy plots to disk for later analysis:<p class="source-code">    def train(self, train_dataset, epochs, </p><p class="source-code">                     steps_per_epoch,</p><p class="source-code">              validation_dataset, validation_steps):</p><p class="source-code">        hist = \</p><p class="source-code">            self.model.fit(train_dataset,</p><p class="source-code">                           epochs=epochs,</p><p class="source-code">                    steps_per_epoch=steps_per_epoch,</p><p class="source-code">                   validation_steps=validation_steps,</p><p class="source-code">                 validation_data=validation_dataset)</p><p class="source-code">        self._plot_model_history(hist, 'loss', [0., 2.0])</p><p class="source-code">        self._plot_model_history(hist, 'accuracy')</p></li>
				<li>Define a <a id="_idIndexMarker773"/>helper method named <strong class="source-inline">_process_mask()</strong>, which will be used to make the segmentation masks compatible with OpenCV. What this function does is create a three-channeled version of a grayscale mask and upscale the class values to the [0, 255] range:<p class="source-code">    @staticmethod</p><p class="source-code">    def _process_mask(mask):</p><p class="source-code">        mask = (mask.numpy() * 127.5).astype('uint8')</p><p class="source-code">        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)</p><p class="source-code">        return mask</p></li>
				<li>The <strong class="source-inline">_save_image_and_masks()</strong> helper method creates a mosaic of the original image, the ground truth mask, and the predicted segmentation mask, and saves it to disk for later revision:<p class="source-code">    def _save_image_and_masks(self, image,</p><p class="source-code">                              ground_truth_mask,</p><p class="source-code">                              prediction_mask,</p><p class="source-code">                              image_id):</p><p class="source-code">        image = (image.numpy() * </p><p class="source-code">                 255.0).astype('uint8')</p><p class="source-code">        gt_mask = self._process_mask(ground_truth_mask)</p><p class="source-code">        pred_mask = self._process_mask(prediction_mask)</p><p class="source-code">        mosaic = np.hstack([image, gt_mask, pred_mask])</p><p class="source-code">        mosaic = cv2.cvtColor(mosaic, cv2.COLOR_RGB2BGR)</p><p class="source-code">        cv2.imwrite(f'mosaic_{image_id}.jpg', mosaic)</p></li>
				<li>In order to pass <a id="_idIndexMarker774"/>the output volume produced by the network to a valid segmentation mask, we must take the index of the highest value at each pixel location, which corresponds to the most likely category for that pixel. The <strong class="source-inline">_create_mask()</strong> method does this:<p class="source-code">    @staticmethod</p><p class="source-code">    def _create_mask(prediction_mask):</p><p class="source-code">        prediction_mask = tf.argmax(prediction_mask, </p><p class="source-code">                                       axis=-1)</p><p class="source-code">        prediction_mask = prediction_mask[...,tf.newaxis]</p><p class="source-code">        return prediction_mask[0]</p><p>The <strong class="source-inline">_save_predictions()</strong> method uses the <strong class="bold">U-Net</strong> to predict the mask of a sample of images in the input dataset and saves the result to disk. It does this using the <strong class="source-inline">_save_image_and_mask()</strong> helper method we defined in <em class="italic">Step 13</em>:</p><p class="source-code">    def _save_predictions(self, dataset, </p><p class="source-code">                            sample_size=1):</p><p class="source-code">        for id, (image, mask) in \</p><p class="source-code">                enumerate(dataset.take(sample_size), </p><p class="source-code">                                       start=1):</p><p class="source-code">            pred_mask = self.model.predict(image)</p><p class="source-code">            pred_mask = self._create_mask(pred_mask)</p><p class="source-code">            image = image[0]</p><p class="source-code">            ground_truth_mask = mask[0]</p><p class="source-code">            self._save_image_and_masks(image,</p><p class="source-code">                                  ground_truth_mask,</p><p class="source-code">                                       pred_mask,</p><p class="source-code">                                       image_id=id)</p></li>
				<li>The <strong class="source-inline">evaluate()</strong> method <a id="_idIndexMarker775"/>computes the accuracy of the <strong class="bold">U-Net</strong> on the test set, and also generates predictions for a sample of images, which are then stored on disk:<p class="source-code">    def evaluate(self, test_dataset, sample_size=5):</p><p class="source-code">        result = self.model.evaluate(test_dataset)</p><p class="source-code">        print(f'Accuracy: {result[1] * 100:.2f}%')</p><p class="source-code">        self._save_predictions(test_dataset, </p><p class="source-code">                               sample_size)</p></li>
				<li>Download (or load, if cached) <strong class="source-inline">Oxford IIIT Pet Dataset</strong>, along with its metadata, using TensorFlow Datasets:<p class="source-code">dataset, info = tfdata.load('oxford_iiit_pet',</p><p class="source-code">                            with_info=True)</p></li>
				<li>Use the metadata to define the corresponding number of steps the network will go over for the<a id="_idIndexMarker776"/> training and validation datasets. Also, define the batch and buffer sizes:<p class="source-code">TRAIN_SIZE = info.splits['train'].num_examples</p><p class="source-code">VALIDATION_SIZE = info.splits['test'].num_examples</p><p class="source-code">BATCH_SIZE = 64</p><p class="source-code">STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE</p><p class="source-code">VALIDATION_SUBSPLITS = 5</p><p class="source-code">VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE</p><p class="source-code">VALIDATION_STEPS //= VALIDATION_SUBSPLITS</p><p class="source-code">BUFFER_SIZE = 1000</p></li>
				<li>Define the training and testing datasets' pipelines:<p class="source-code">train_dataset = (dataset['train']</p><p class="source-code">                 .map(load_image, num_parallel_</p><p class="source-code">                  calls=AUTOTUNE)</p><p class="source-code">                 .cache()</p><p class="source-code">                 .shuffle(BUFFER_SIZE)</p><p class="source-code">                 .batch(BATCH_SIZE)</p><p class="source-code">                 .repeat()</p><p class="source-code">                 .prefetch(buffer_size=AUTOTUNE))</p><p class="source-code">test_dataset = (dataset['test']</p><p class="source-code">                .map(lambda d: load_image(d, </p><p class="source-code">                  train=False),</p><p class="source-code">                     num_parallel_calls=AUTOTUNE)</p><p class="source-code">                .batch(BATCH_SIZE))</p></li>
				<li>Instantiate<a id="_idIndexMarker777"/> the <strong class="bold">U-Net</strong> and train it for 50 epochs:<p class="source-code">unet = UNet()</p><p class="source-code">unet.train(train_dataset,</p><p class="source-code">           epochs=50,</p><p class="source-code">           steps_per_epoch=STEPS_PER_EPOCH,</p><p class="source-code">           validation_steps=VALIDATION_STEPS,</p><p class="source-code">           validation_dataset=test_dataset)</p></li>
				<li>Lastly, evaluate the network on the test dataset:<p class="source-code">unet.evaluate(test_dataset)</p><p>The accuracy on the test set should be around 83% (in my case, I got 83.49%): </p></li>
			</ol>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B14768_08_006.jpg" alt="Figure 8.6 – Training and validation accuracy curves&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Training and validation accuracy curves</p>
			<p>Here, we can see that after about epoch 12, the gap between the training and validation accuracy curves slowly widens. This isn't a sign of overfitting, but an indication that we could do better. How does this accuracy translate to actual images?</p>
			<p>Take a look at the following image, which shows the original image, the ground truth mask, and <a id="_idIndexMarker778"/>the produced mask:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B14768_08_007.jpg" alt="Figure 8.7 – The original image (left), the ground truth mask (center), and the predicted mask (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – The original image (left), the ground truth mask (center), and the predicted mask (right)</p>
			<p>Here, we can see that there's a good resemblance between the ground truth mask (center) and the predicted one (right), although there is some noise, such as the small white region and the pronounced bump on the lower half of the dog's silhouette, that could be cleaned up with more training:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B14768_08_008.jpg" alt="Figure 8.8 – The original image (left), the ground truth mask (center), and the predicted mask (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – The original image (left), the ground truth mask (center), and the predicted mask (right)</p>
			<p>The preceding image clearly shows that the network could use more training or fine-tuning. This is because even though it gets the overall shape and location of the dog right, there's really too much noise for this mask to be usable in a real-world application.</p>
			<p>Let's head over <a id="_idIndexMarker779"/>to the <em class="italic">How it works…</em> section to connect the dots.</p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor284"/>How it works…</h2>
			<p>In this recipe, we<a id="_idIndexMarker780"/> implemented and trained a <strong class="bold">U-Net</strong> from scratch to segment the body and contour of household pets. As we saw, the network did learn, but still offers room for improvement. </p>
			<p>The ability to semantically segment the contents of an image is of paramount importance in several domains, such as in medicine, where what's more important than knowing if a condition, such as a malignant tumor, is present, is to determine the actual location, shape, and area of said ailment. The field of biomedicine is where <strong class="bold">U-Net</strong> made its debut. In 2015, it outperformed established methods for segmentation, such as sliding-windows convolutional networks, using far less data. </p>
			<p>How does <strong class="bold">U-Net</strong> achieve such good results? As we learned in this recipe, the key is in its end-to-end nature, where both the encoder and decoder are comprised of convolutions that form a contracting path, whose job is to capture context and a symmetric expanding path, thereby enabling precise localization.</p>
			<p>Both of the aforementioned paths can be as deep as needed, depending on the nature of the dataset. This depth customization is viable due to the presence of skip connections, which allow the gradients to flow farther down the network, thus preventing the vanishing gradient problem (this is similar to what <strong class="bold">ResNet</strong> does, as we learned in <a href="B14768_02_Final_JM_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 2</em></a>, <em class="italic">Performing Image Classification</em>).</p>
			<p>In the next recipe, we'll combine a very powerful concept with this implementation of <strong class="bold">U-Net</strong> to increase the performance of <strong class="source-inline">Oxford IIIT Pet Dataset</strong>: transfer learning.</p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor285"/>See also</h2>
			<p>A great way to familiarize yourself with <strong class="bold">U-Net</strong> is to read the original paper: <a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a>. Also, if you want to find out more about <strong class="source-inline">Oxford IIIT Pet Dataset</strong>, visit the official site here: <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">https://www.robots.ox.ac.uk/~vgg/data/pets/</a>. </p>
			<p>In this recipe, we mentioned the vanishing gradient problem a few times, so it's a good idea to understand the concept by reading this article: <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">https://en.wikipedia.org/wiki/Vanishing_gradient_problem</a>.</p>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor286"/>Implementing a U-Net with transfer learning</h1>
			<p>Training <a id="_idIndexMarker781"/>a <strong class="bold">U-Net</strong> from scratch is a very good first <a id="_idIndexMarker782"/>step toward creating a performant image segmentation system. However, one of the biggest superpowers in deep learning that's applied to computer vision is being able to build solutions on top of the knowledge of other networks, which usually leads to faster and better results.</p>
			<p>Image segmentation is no exception to this rule, and in this recipe, we'll implement a better segmentation network using transfer learning. </p>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor287"/>Getting ready</h2>
			<p>This recipe is very similar to the previous one (<em class="italic">Implementing a U-Net from scratch</em>), so we'll only go into depth on the parts that are different. For a deeper explanation, I recommend that you complete the <em class="italic">Implementing a U-Net from scratch</em> recipe before attempting this one. As expected, the libraries we'll need are the same as they were for that recipe, all of which can be installed using <strong class="source-inline">pip</strong>. Let's start with <strong class="source-inline">tensorflow_docs</strong>, as follows:</p>
			<p class="source-code">$&gt; pip install git+https://github.com/tensorflow/docs</p>
			<p>Now, let's set up the remaining dependencies:</p>
			<p class="source-code">$&gt; pip install tensorflow-datasets Pillow opencv-contrib-python</p>
			<p>Once again, we'll work with <strong class="source-inline">Oxford-IIIT Pet Dataset</strong>, which can be accessed through <strong class="source-inline">tensorflow-datasets</strong>. Each pixel in this dataset falls within one of these classes:</p>
			<ul>
				<li>1: The pixel belongs to a pet (cat or dog).</li>
				<li>2: The pixel belongs to the contour of a pet.</li>
				<li>3: The pixel belongs to the surroundings.</li>
			</ul>
			<p>The following<a id="_idIndexMarker783"/> image shows two sample images<a id="_idIndexMarker784"/> from the dataset:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B14768_08_009.jpg" alt="Figure 8.9 – Sample images from the Oxford-IIIT Pet dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Sample images from the Oxford-IIIT Pet dataset</p>
			<p>With that, we are good to go! </p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor288"/>How to do it…</h2>
			<p>Complete these steps to implement a transfer learning-powered <strong class="bold">U-Net</strong>:</p>
			<ol>
				<li value="1">Import all the needed packages:<p class="source-code">import cv2</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfdata</p><p class="source-code">import tensorflow_docs as tfdocs</p><p class="source-code">import tensorflow_docs.plots</p><p class="source-code">from tensorflow.keras.applications import MobileNetV2</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import \</p><p class="source-code">    SparseCategoricalCrossentropy</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p></li>
				<li>Define an alias for <strong class="source-inline">tf.data.experimental.AUTOTUNE</strong>:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE </p></li>
				<li>Define a <a id="_idIndexMarker785"/>function that will normalize the<a id="_idIndexMarker786"/> images and masks in the dataset:<p class="source-code">def normalize(input_image, input_mask):</p><p class="source-code">    input_image = tf.cast(input_image, tf.float32) / </p><p class="source-code">                                   255.0</p><p class="source-code">    input_mask -= 1</p><p class="source-code">    return input_image, input_mask</p></li>
				<li>Define a function that will load an image and its corresponding mask, given an element from a TensorFlow Datasets data structure. Optionally, it should perform image mirroring on training images:<p class="source-code">@tf.function</p><p class="source-code">def load_image(dataset_element, train=True):</p><p class="source-code">    input_image = tf.image.resize(dataset</p><p class="source-code">                           element['image'],(256, 256))</p><p class="source-code">    input_mask = tf.image.resize(</p><p class="source-code">        dataset_element['segmentation_mask'], (256,256))</p><p class="source-code">    if train and np.random.uniform() &gt; 0.5:</p><p class="source-code">        input_image = tf.image.flip_left_right(input_</p><p class="source-code">                                              image)</p><p class="source-code">        input_mask = </p><p class="source-code">           tf.image.flip_left_right(input_mask)</p><p class="source-code">    input_image, input_mask = normalize(input_image,</p><p class="source-code">                                        input_mask)</p><p class="source-code">    return input_image, input_mask</p></li>
				<li>Define <strong class="source-inline">UNet()</strong>, a<a id="_idIndexMarker787"/> container class for the<a id="_idIndexMarker788"/> logic necessary to build, train, and evaluate our transfer learning-aided <strong class="bold">U-Net</strong>. Start by defining the constructor:<p class="source-code">class UNet(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 input_size=(256, 256, 3),</p><p class="source-code">                 output_channels=3):</p><p class="source-code">        self.pretrained_model = MobileNetV2(</p><p class="source-code">            input_shape=input_size,</p><p class="source-code">            include_top=False,</p><p class="source-code">            weights='imagenet')</p><p class="source-code">        self.target_layers = [</p><p class="source-code">            'block_1_expand_relu',</p><p class="source-code">            'block_3_expand_relu',</p><p class="source-code">            'block_6_expand_relu',</p><p class="source-code">            'block_13_expand_relu',</p><p class="source-code">            'block_16_project'</p><p class="source-code">        ]</p><p class="source-code">        self.input_size = input_size</p><p class="source-code">        self.output_channels = output_channels</p><p class="source-code">        self.model = self._create_model()</p><p class="source-code">        loss = SparseCategoricalCrossentropy(from_</p><p class="source-code">                                            logits=True)</p><p class="source-code">        self.model.compile(optimizer=RMSprop(),</p><p class="source-code">                           loss=loss,</p><p class="source-code">                           metrics=['accuracy'])</p><p>In this step, we<a id="_idIndexMarker789"/> are creating the model, which <a id="_idIndexMarker790"/>we'll train using <strong class="source-inline">RMSProp</strong> as the optimizer and <strong class="source-inline">SparseCategoricalCrossentropy</strong> as the loss. Notice that <strong class="source-inline">output_channels</strong> is, by default, <strong class="source-inline">3</strong>, because each pixel can be categorized into one of three classes. The encoder will be a pre-trained <strong class="source-inline">MobileNetV2</strong>. However, we'll only use a select group of layers, defined in <strong class="source-inline">self.target_layers</strong>.</p></li>
				<li>Now, let's define the <strong class="source-inline">_upsample()</strong> helper method, which builds an upsampling block: <p class="source-code">    @staticmethod</p><p class="source-code">    def _upsample(filters, size, dropout=False):</p><p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        layers = Sequential()</p><p class="source-code">        layers.add(Conv2DTranspose(filters=filters,</p><p class="source-code">                                   kernel_size=size,</p><p class="source-code">                                   strides=2,</p><p class="source-code">                                   padding='same',</p><p class="source-code">                           kernel_initializer=init,</p><p class="source-code">                                   use_bias=False))</p><p class="source-code">        layers.add(BatchNormalization())</p><p class="source-code">        if dropout:</p><p class="source-code">            layers.add(Dropout(rate=0.5))</p><p class="source-code">        layers.add(ReLU())</p><p class="source-code">        return layers</p></li>
				<li>Armed with <a id="_idIndexMarker791"/>our <a id="_idIndexMarker792"/>pre-trained <strong class="source-inline">MobileNetV2</strong> and <strong class="source-inline">_upsample()</strong>, we can iteratively build the full <strong class="bold">U-Net</strong> architecture. The encoding part of the network is just a model of <strong class="source-inline">self.target_layers</strong>, which are frozen (<strong class="source-inline">down_stack.trainable = False</strong>), meaning we only train the decoder or upsampling blocks of the architecture:<p class="source-code">    def _create_model(self):</p><p class="source-code">      layers = [self.pretrained_model.get_layer(l).output</p><p class="source-code">                  for l in self.target_layers]</p><p class="source-code">    down_stack = Model(inputs=self.pretrained_model.</p><p class="source-code">                              input, outputs=layers)</p><p class="source-code">        down_stack.trainable = False</p><p class="source-code">        up_stack = []</p><p class="source-code">        for filters in (512, 256, 128, 64):</p><p class="source-code">            up_block = self._upsample(filters, 4)</p><p class="source-code">            up_stack.append(up_block)</p></li>
				<li>Now, we can<a id="_idIndexMarker793"/> add the skip connections to<a id="_idIndexMarker794"/> facilitate the flow of the gradient throughout the network:<p class="source-code">        inputs = Input(shape=self.input_size)</p><p class="source-code">        x = inputs</p><p class="source-code">        skip_layers = down_stack(x)</p><p class="source-code">        x = skip_layers[-1]</p><p class="source-code">        skip_layers = reversed(skip_layers[:-1])</p><p class="source-code">        for up, skip_connection in zip(up_stack, </p><p class="source-code">                                       skip_layers):</p><p class="source-code">            x = up(x)</p><p class="source-code">            x = Concatenate()([x, skip_connection])</p></li>
				<li>The output layer of the <strong class="bold">U-Net</strong> is a transposed convolution that has the same dimensions as the input image, but has as many channels as there are classes in the segmentation mask:<p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        output = Conv2DTranspose(</p><p class="source-code">            filters=self.output_channels,</p><p class="source-code">            kernel_size=3,</p><p class="source-code">            strides=2,</p><p class="source-code">            padding='same',</p><p class="source-code">            kernel_initializer=init)(x)</p><p class="source-code">        return Model(inputs, outputs=output)</p></li>
				<li>Define <strong class="source-inline">_plot_model_history()</strong>, a helper<a id="_idIndexMarker795"/> method<a id="_idIndexMarker796"/> that plots the relevant training curves:<p class="source-code">    @staticmethod</p><p class="source-code">    def _plot_model_history(model_history, metric, </p><p class="source-code">                             ylim=None):</p><p class="source-code">        plt.style.use('seaborn-darkgrid')</p><p class="source-code">        plotter = tfdocs.plots.HistoryPlotter()</p><p class="source-code">        plotter.plot({'Model': model_history}, </p><p class="source-code">                        metric=metric)</p><p class="source-code">        plt.title(f'{metric.upper()}')</p><p class="source-code">        if ylim is None:</p><p class="source-code">            plt.ylim([0, 1])</p><p class="source-code">        else:</p><p class="source-code">            plt.ylim(ylim)</p><p class="source-code">        plt.savefig(f'{metric}.png')</p><p class="source-code">        plt.close()</p></li>
				<li>Define<a id="_idIndexMarker797"/> the <strong class="source-inline">train()</strong> method, which <a id="_idIndexMarker798"/>is in charge of fitting the model:<p class="source-code">    def train(self, train_dataset, epochs, </p><p class="source-code">                  steps_per_epoch,</p><p class="source-code">              validation_dataset, validation_steps):</p><p class="source-code">        hist = \</p><p class="source-code">            self.model.fit(train_dataset,</p><p class="source-code">                         epochs=epochs,</p><p class="source-code">                        steps_per_epoch=steps_per_epoch,</p><p class="source-code">                       validation_steps=validation_steps,</p><p class="source-code">                      validation_data=validation_dataset)</p><p class="source-code">        self._plot_model_history(hist, 'loss', [0., 2.0])</p><p class="source-code">        self._plot_model_history(hist, 'accuracy')</p></li>
				<li>Define <strong class="source-inline">_process_mask()</strong>, a helper method that makes the segmentation masks compatible with OpenCV:<p class="source-code">    @staticmethod</p><p class="source-code">    def _process_mask(mask):</p><p class="source-code">        mask = (mask.numpy() * 127.5).astype('uint8')</p><p class="source-code">        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)</p><p class="source-code">        return mask</p></li>
				<li>Define <a id="_idIndexMarker799"/>the <strong class="source-inline">_save_image_and_masks()</strong> helper method to create a visualization of the original image, along <a id="_idIndexMarker800"/>with the real and predicted masks:<p class="source-code">    def _save_image_and_masks(self, image,</p><p class="source-code">                              ground_truth_mask,</p><p class="source-code">                              prediction_mask,</p><p class="source-code">                              image_id):</p><p class="source-code">        image = (image.numpy() * 255.0).astype('uint8')</p><p class="source-code">        gt_mask = self._process_mask(ground_truth_mask)</p><p class="source-code">        pred_mask = self._process_mask(prediction_mask)</p><p class="source-code">        mosaic = np.hstack([image, gt_mask, pred_mask])</p><p class="source-code">        mosaic = cv2.cvtColor(mosaic, cv2.COLOR_RGB2BGR)</p><p class="source-code">        cv2.imwrite(f'mosaic_{image_id}.jpg', mosaic)</p></li>
				<li>Define <strong class="source-inline">_create_mask()</strong>, which produces a valid segmentation mask from the network's predictions:<p class="source-code">    @staticmethod</p><p class="source-code">    def _create_mask(prediction_mask):</p><p class="source-code">        prediction_mask = tf.argmax(prediction_mask, </p><p class="source-code">                                   axis=-1)</p><p class="source-code">        prediction_mask = prediction_mask[..., </p><p class="source-code">                                      tf.newaxis]</p><p class="source-code">        return prediction_mask[0]</p></li>
				<li>The <strong class="source-inline">_save_predictions()</strong> method uses the <strong class="bold">U-Net</strong> to predict the mask of a sample of<a id="_idIndexMarker801"/> images in the input dataset <a id="_idIndexMarker802"/>and saves the result to disk. It does this using the <strong class="source-inline">_save_image_and_mask()</strong> helper method, which we defined in <em class="italic">Step 13</em>:<p class="source-code">    def _save_predictions(self, dataset, </p><p class="source-code">                          sample_size=1):</p><p class="source-code">        for id, (image, mask) in \</p><p class="source-code">                enumerate(dataset.take(sample_size), </p><p class="source-code">                            start=1):</p><p class="source-code">            pred_mask = self.model.predict(image)</p><p class="source-code">            pred_mask = self._create_mask(pred_mask)</p><p class="source-code">            image = image[0]</p><p class="source-code">            ground_truth_mask = mask[0]</p><p class="source-code">            self._save_image_and_masks(image,</p><p class="source-code">                                  ground_truth_mask,</p><p class="source-code">                                       pred_mask,</p><p class="source-code">                                       image_id=id)</p></li>
				<li>The <strong class="source-inline">evaluate()</strong> method computes the accuracy of the <strong class="bold">U-Net</strong> on the test set, while also generating predictions for a sample of images. These are then stored on disk:<p class="source-code">    def evaluate(self, test_dataset, sample_size=5):</p><p class="source-code">        result = self.model.evaluate(test_dataset)</p><p class="source-code">        print(f'Accuracy: {result[1] * 100:.2f}%')</p><p class="source-code">        self._save_predictions(test_dataset, sample_size)</p></li>
				<li>Download (or <a id="_idIndexMarker803"/>load, if cached) <strong class="source-inline">Oxford IIIT Pet Dataset</strong>, along <a id="_idIndexMarker804"/>with its metadata, using TensorFlow Datasets:<p class="source-code">dataset, info = tfdata.load('oxford_iiit_pet',</p><p class="source-code">                            with_info=True)</p></li>
				<li>Use the metadata to define the corresponding number of steps the network will take over the training and validation datasets. Also, define the batch and buffer sizes:<p class="source-code">TRAIN_SIZE = info.splits['train'].num_examples</p><p class="source-code">VALIDATION_SIZE = info.splits['test'].num_examples</p><p class="source-code">BATCH_SIZE = 64</p><p class="source-code">STEPS_PER_EPOCH = TRAIN_SIZE // BATCH_SIZE</p><p class="source-code">VALIDATION_SUBSPLITS = 5</p><p class="source-code">VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE</p><p class="source-code">VALIDATION_STEPS //= VALIDATION_SUBSPLITS</p><p class="source-code">BUFFER_SIZE = 1000</p></li>
				<li>Define<a id="_idIndexMarker805"/> the training and testing datasets' <a id="_idIndexMarker806"/>pipelines:<p class="source-code">train_dataset = (dataset['train']</p><p class="source-code">                 .map(load_image, num_parallel_</p><p class="source-code">                  calls=AUTOTUNE)</p><p class="source-code">                 .cache()</p><p class="source-code">                 .shuffle(BUFFER_SIZE)</p><p class="source-code">                 .batch(BATCH_SIZE)</p><p class="source-code">                 .repeat()</p><p class="source-code">                 .prefetch(buffer_size=AUTOTUNE))</p><p class="source-code">test_dataset = (dataset['test']</p><p class="source-code">                .map(lambda d: load_image(d, </p><p class="source-code">                     train=False),</p><p class="source-code">                     num_parallel_calls=AUTOTUNE)</p><p class="source-code">                .batch(BATCH_SIZE))</p></li>
				<li>Instantiate the <strong class="bold">U-Net</strong> and train it for 30 epochs:<p class="source-code">unet = UNet()</p><p class="source-code">unet.train(train_dataset,</p><p class="source-code">           epochs=50,</p><p class="source-code">           steps_per_epoch=STEPS_PER_EPOCH,</p><p class="source-code">           validation_steps=VALIDATION_STEPS,</p><p class="source-code">           validation_dataset=test_dataset)</p></li>
				<li>Evaluate the network on the test dataset:<p class="source-code">unet.evaluate(test_dataset)</p><p>The accuracy<a id="_idIndexMarker807"/> on the test set should b<a id="_idIndexMarker808"/>e close to 90% (in my case, I obtained 90.78% accuracy): </p></li>
			</ol>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B14768_08_010.jpg" alt="Figure 8.10 – Training and validation accuracy curves&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – Training and validation accuracy curves</p>
			<p>The accuracy curves show that the network is not overfitting because both the training and validation plots follow the same trajectory, with a very thin gap. This also confirms that the knowledge the model is acquiring is transferrable and usable on unseen data.</p>
			<p>Let's take a look at some of the outputs from the network, starting with the following image:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B14768_08_011.jpg" alt="Figure 8.11 – The original image (left), the ground truth mask (center), and the predicted mask (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – The original image (left), the ground truth mask (center), and the predicted mask (right)</p>
			<p>Compared to <em class="italic">Figure 8.7</em> in the <em class="italic">Implementing a U-Net from scratch</em> recipe, in the preceding image, we can see that the <strong class="bold">U-Net</strong> produces a much cleaner result, with the background (gray pixels), contour (white pixels), and pet (black pixels) clearly <a id="_idIndexMarker809"/>separated and almost identical <a id="_idIndexMarker810"/>to the ground truth mask (center):</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B14768_08_012.jpg" alt="Figure 8.12 – The original image (left), the ground truth mask (center), and the predicted mask (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – The original image (left), the ground truth mask (center), and the predicted mask (right)</p>
			<p>The preceding image is a great improvement in comparison to <em class="italic">Figure 8.8</em> in the <em class="italic">Implementing a U-Net from scratch</em> recipe. This time, the predicted mask (right), although not perfect, presents less noise and is much closer to the actual segmentation mask (center).</p>
			<p>We'll dig deeper in the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor289"/>How it works…</h2>
			<p>In this recipe, we made a small yet substantial change to the <strong class="bold">U-Net</strong> architecture we implemented in <em class="italic">Implementing a U-Net from scratch</em>. Instead of training both the encoder and the decoder from scratch, we focused only on the upsampling layers (the decoder), leaving the encoding portion of the problem to a subset of target layers handpicked from a <strong class="source-inline">MobileNetV2</strong> trained on the massive <strong class="source-inline">ImageNet</strong> dataset.</p>
			<p>The reason transfer learning worked so well in this context is that there are hundreds of classes in <strong class="source-inline">ImageNet</strong> focused on different breeds of cats and dogs, meaning the overlap with <strong class="source-inline">Oxford IIIT Pet</strong> is very substantial. However, if this wasn't the case, this doesn't mean we should drop transfer learning entirely! What we should do in that situation is fine-tune <a id="_idIndexMarker811"/>the encoder by making <a id="_idIndexMarker812"/>some (or all) of its layers trainable.</p>
			<p>By leveraging the knowledge encoded in <strong class="source-inline">MobileNetV2</strong>, we were able to bump the accuracy on the test set from 83% up to 90%, an impressive gain that translated into better, cleaner prediction masks, even on challenging examples.</p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor290"/>See also</h2>
			<p>You can read the <a id="_idIndexMarker813"/>original <strong class="bold">U-Net</strong> paper here: <a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a>. If you're interested in the details of <strong class="source-inline">Oxford IIIT Pet Dataset</strong>, please go to <a href="https://www.robots.ox.ac.uk/~vgg/data/pets/">https://www.robots.ox.ac.uk/~vgg/data/pets/</a>. To learn how to combat the vanishing gradient problem, read this article: <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">https://en.wikipedia.org/wiki<span id="_idTextAnchor291"/><span id="_idTextAnchor292"/>/Vanishing_gradient_problem</a>.</p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor293"/>Segmenting images using Mask-RCNN and TensorFlow Hub</h1>
			<p><strong class="bold">Mask-RCNN</strong> is a <a id="_idIndexMarker814"/>state-of-the-art architecture<a id="_idIndexMarker815"/> for<a id="_idIndexMarker816"/> object detection. However, as<a id="_idIndexMarker817"/> its <a id="_idIndexMarker818"/>name suggests, it's also excellent at performing image segmentation. In this recipe, we'll leverage an implementation of <strong class="bold">Mask-RCNN</strong> hosted in <strong class="bold">TensorFlow Hub</strong> (<strong class="bold">TFHub</strong>) that<a id="_idIndexMarker819"/> has been trained on the gargantuan <strong class="source-inline">COCO</strong> dataset. This will help us perform out-of-the-box object detection and image segmentation.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor294"/>Getting ready</h2>
			<p>First, we must install <strong class="source-inline">Pillow</strong> and <strong class="bold">TFHub</strong>, as follows:</p>
			<p class="source-code">$&gt; pip install Pillow tensorflow-hub</p>
			<p>We also need to install the <strong class="bold">TensorFlow Object Detection API</strong> since it contains a series of convenient visualization tools that'll come in handy for looking at the bounding boxes and segmentation masks. First, <strong class="source-inline">cd</strong> to a location of your preference and clone the <strong class="source-inline">tensorflow/models</strong> repository:</p>
			<p class="source-code">$&gt; git clone –-depth 1 https://github.com/tensorflow/models</p>
			<p>Next, install <a id="_idIndexMarker820"/>the <strong class="bold">TensorFlow Object Detection API</strong>, like <a id="_idIndexMarker821"/>this:</p>
			<p class="source-code">$&gt; sudo apt install -y protobuf-compiler</p>
			<p class="source-code">$&gt; cd models/research</p>
			<p class="source-code">$&gt; protoc object_detection/protos/*.proto --python_out=.</p>
			<p class="source-code">$&gt; cp object_detection/packages/tf2/setup.py .</p>
			<p class="source-code">$&gt; python -m pip install -q . </p>
			<p>That's it! Let's <a id="_idIndexMarker822"/>get <a id="_idIndexMarker823"/>started.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor295"/>How to do it…</h2>
			<p>Follow these steps to learn how to segment your images using <strong class="bold">Mask-RCNN</strong>:</p>
			<ol>
				<li value="1">Import the necessary packages:<p class="source-code">import glob</p><p class="source-code">from io import BytesIO</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">from PIL import Image</p><p class="source-code">from object_detection.utils import ops</p><p class="source-code">from object_detection.utils import visualization_utils as viz</p><p class="source-code">from object_detection.utils.label_map_util import \</p><p class="source-code">    create_category_index_from_labelmap</p></li>
				<li>Define a<a id="_idIndexMarker824"/> function <a id="_idIndexMarker825"/>that will load an image<a id="_idIndexMarker826"/> into a<a id="_idIndexMarker827"/> NumPy array:<p class="source-code">def load_image(path):</p><p class="source-code">    image_data = tf.io.gfile.GFile(path, 'rb').read()</p><p class="source-code">    image = Image.open(BytesIO(image_data))</p><p class="source-code">    width, height = image.size</p><p class="source-code">    shape = (1, height, width, 3)</p><p class="source-code">    image = np.array(image.getdata())</p><p class="source-code">    image = image.reshape(shape).astype('uint8')</p><p class="source-code">    return image</p></li>
				<li>Define a function that will make predictions with <strong class="bold">Mask-RCNN</strong> and save the results to disk. Start by loading the image and passing it through the model:<p class="source-code">def get_and_save_predictions(model, image_path):</p><p class="source-code">    image = load_image(image_path)</p><p class="source-code">    results = model(image)</p></li>
				<li>Convert the results into <strong class="source-inline">NumPy</strong> arrays:<p class="source-code">model_output = {k: v.numpy() for k, v in </p><p class="source-code">                results.items()}</p></li>
				<li>Extract<a id="_idIndexMarker828"/> both the detection masks and<a id="_idIndexMarker829"/> boxes from the model output and convert them into tensors:<p class="source-code">  detection_masks = model_output['detection_masks'][0]</p><p class="source-code"> detection_masks = tf.convert_to_tensor(detection_masks)</p><p class="source-code"> detection_boxes = model_output['detection_boxes'][0]</p><p class="source-code"> detection_boxes = tf.convert_to_tensor(detection_boxes)</p></li>
				<li>Reframe<a id="_idIndexMarker830"/> the<a id="_idIndexMarker831"/> box masks to image masks:<p class="source-code">    detection_masks_reframed = \</p><p class="source-code">     ops.reframe_box_masks_to_image_masks(detection_</p><p class="source-code">                                masks,detection_boxes,</p><p class="source-code">                                 image.shape[1],</p><p class="source-code">                               image.shape[2])</p><p class="source-code">    detection_masks_reframed = \</p><p class="source-code">        tf.cast(detection_masks_reframed &gt; 0.5, </p><p class="source-code">                 tf.uint8)</p><p class="source-code">    model_output['detection_masks_reframed'] = \</p><p class="source-code">        detection_masks_reframed.numpy()</p></li>
				<li>Create a <a id="_idIndexMarker832"/>visualization<a id="_idIndexMarker833"/> of<a id="_idIndexMarker834"/> the detections <a id="_idIndexMarker835"/>and their boxes, scores, classes, and masks:<p class="source-code">    boxes = model_output['detection_boxes'][0]</p><p class="source-code">    classes = \</p><p class="source-code">       model_output['detection_classes'][0].astype('int')</p><p class="source-code">    scores = model_output['detection_scores'][0]</p><p class="source-code">    masks = model_output['detection_masks_reframed']</p><p class="source-code">    image_with_mask = image.copy()</p><p class="source-code">    viz.visualize_boxes_and_labels_on_image_array(</p><p class="source-code">        image=image_with_mask[0],</p><p class="source-code">        boxes=boxes,</p><p class="source-code">        classes=classes,</p><p class="source-code">        scores=scores,</p><p class="source-code">        category_index=CATEGORY_IDX,</p><p class="source-code">        use_normalized_coordinates=True,</p><p class="source-code">        max_boxes_to_draw=200,</p><p class="source-code">        min_score_thresh=0.30,</p><p class="source-code">        agnostic_mode=False,</p><p class="source-code">        instance_masks=masks,</p><p class="source-code">        line_thickness=5</p><p class="source-code">    )</p></li>
				<li>Save the <a id="_idIndexMarker836"/>result to disk:<p class="source-code">    plt.figure(figsize=(24, 32))</p><p class="source-code">    plt.imshow(image_with_mask[0])</p><p class="source-code">    plt.savefig(f'output/{image_path.split("/")[-1]}')</p></li>
				<li>Load<a id="_idIndexMarker837"/> the <strong class="source-inline">COCO</strong> dataset's <a id="_idIndexMarker838"/>category <a id="_idIndexMarker839"/>index:<p class="source-code">labels_path = 'resources/mscoco_label_map.pbtxt'</p><p class="source-code">CATEGORY_IDX =create_category_index_from_labelmap(labels_path)</p></li>
				<li>Load <strong class="bold">Mask-RCNN</strong> from <strong class="bold">TFHub</strong>:<p class="source-code">MODEL_PATH = ('https://tfhub.dev/tensorflow/mask_rcnn/'</p><p class="source-code">              'inception_resnet_v2_1024x1024/1')</p><p class="source-code">mask_rcnn = hub.load(MODEL_PATH)</p></li>
				<li>Run <strong class="bold">Mask-RCNN</strong> over all the test images: <p class="source-code">test_images_paths = glob.glob('test_images/*')</p><p class="source-code">for image_path in test_images_paths:</p><p class="source-code">    get_and_save_predictions(mask_rcnn, image_path)</p><p>After a while, the labeled images should be in the <strong class="source-inline">output</strong> folder. Let's review an easy one:</p></li>
			</ol>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B14768_08_013.jpg" alt="Figure 8.13– Single instance of segmentation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13– Single instance of segmentation</p>
			<p>Here, we can see <a id="_idIndexMarker840"/>that the network correctly<a id="_idIndexMarker841"/> detected and <a id="_idIndexMarker842"/>segmented the dog with 100% accuracy! Let's<a id="_idIndexMarker843"/> try a more challenging one:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B14768_08_014.jpg" alt="Figure 8.14 – Multiple instances of segmentation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Multiple instances of segmentation</p>
			<p>This image is much more crowded than the previous one, and even then, the network correctly identified most of the objects in the scene (cars, people, trucks, and so on) – even occluded ones! However, the model fails in some circumstances, as shown in the following image:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B14768_08_015.jpg" alt="Figure 8.15 – Segmentation with errors and redundancies&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – Segmentation with errors and redundancies</p>
			<p>This time, the network correctly identified me and my dogs, as well as the coffee cup and the couch, but it threw duplicate and nonsensical detections, such as my leg being a <a id="_idIndexMarker844"/>person. This happened <a id="_idIndexMarker845"/>because I'm holding my dog, and parts of my body <a id="_idIndexMarker846"/>are disconnected in the photo, leading<a id="_idIndexMarker847"/> to incorrect or low confidence segmentations.</p>
			<p>Let's head over to the next section.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor296"/>How it works…</h2>
			<p>In this recipe, we learned how to detect objects and perform image segmentation using one of the most powerful neural networks in existence: <strong class="bold">Mask-RCNN</strong>. Training such a model is not an easy task, let alone implementing it from scratch! Fortunately, thanks to <strong class="bold">TensorFlow Hub</strong>, we were able to use all its predicting power with just a few lines of code.</p>
			<p>We must take into consideration that this pre-trained model will work best on images containing objects the network has been trained on. More precisely, the more the images that we pass to <strong class="bold">Mask-RCNN</strong> resemble those in <strong class="source-inline">COCO</strong>, the better the results will be. Nevertheless, a degree of tweaking and experimentation is always needed in order to achieve the best<a id="_idIndexMarker848"/> detections possible because, as we<a id="_idIndexMarker849"/> saw in the previous example, the network, although<a id="_idIndexMarker850"/> great, isn't <a id="_idIndexMarker851"/>perfect.</p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor297"/>See also</h2>
			<p>You can learn <a id="_idIndexMarker852"/>more about the model we used here: <a href="https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1">https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1</a>. Also, reading the <strong class="bold">Mask-RCNN</strong> paper is a sound decision: <a href="https://arxiv.org/abs/1703.06870">https://arxiv.org/abs/1703.06870</a>.</p>
		</div>
	</body></html>