- en: Computer Vision and Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉与神经网络
- en: In recent years, computer vision has grown into a key domain for innovation,
    with more and more applications reshaping businesses and lifestyles. We will start
    this book with a brief presentation of this field and its history so that we can
    get some background information. We will then introduce artificial neural networks
    and explain how they have revolutionized computer vision. Since we believe in
    learning through practice, by the end of this first chapter, we will even have
    implemented our own network from scratch!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机视觉已经成长为创新的一个关键领域，越来越多的应用正在重新塑造企业和生活方式。我们将从简要介绍该领域及其历史开始，以便为读者提供一些背景信息。接着，我们将介绍人工神经网络，并解释它们如何革新计算机视觉。因为我们相信通过实践学习，因此在本章结束时，我们甚至会从零开始实现我们自己的神经网络！
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Computer vision and why it is a fascinating contemporary domain
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉及其作为一个迷人的当代领域的原因
- en: How we got there—from local hand-crafted descriptors to deep neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是如何走到这一步的——从本地手工制作的描述符到深度神经网络
- en: Neural networks, what they actually are, and how to implement our own for a
    basic recognition task
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络，它们究竟是什么，以及如何为一个基础的识别任务实现我们自己的神经网络
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Throughout this book, we will be using Python 3.5 (or higher). As a general-purpose
    programming language, Python has become the main tool for data scientists thanks
    to its useful built-in features and renowned libraries.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用Python 3.5（或更高版本）。作为一种通用编程语言，Python凭借其有用的内建特性和著名的库，已成为数据科学家们的主要工具。
- en: For this introductory chapter, we will only use two cornerstone libraries—NumPy
    and Matplotlib. They can be found at and installed from [www.numpy.org](http://www.numpy.org/)
    and [matplotlib.org](https://matplotlib.org/). However, we recommend using Anaconda
    ([www.anaconda.com](https://www.anaconda.com)), a free Python distribution that
    makes package management and deployment easy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本介绍性章节，我们将只使用两个基础库——NumPy和Matplotlib。它们可以在[www.numpy.org](http://www.numpy.org/)和[matplotlib.org](https://matplotlib.org/)上找到并进行安装。然而，我们建议使用Anaconda（[www.anaconda.com](https://www.anaconda.com)），这是一个免费的Python发行版，使得包管理和部署变得简单。
- en: Complete installation instructions—as well as all the code presented alongside
    this chapter—can be found in the GitHub repository at [github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow2/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow2/tree/master/Chapter01).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的安装说明以及本章展示的所有代码，可以在GitHub上的[github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow2/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow2/tree/master/Chapter01)找到。
- en: We assume that our readers already have some knowledge of Python and a basic
    understanding of image representation (pixels, channels, and so on) and matrix
    manipulation (shapes, products, and so on).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设读者已经具备一些Python的知识，并且对图像表示（像素、通道等）和矩阵操作（形状、乘积等）有基本的理解。
- en: Computer vision in the wild
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉的实际应用
- en: Computer vision is everywhere nowadays, to the point that its definition can
    drastically vary from one expert to another. In this introductory section, we
    will paint a global picture of computer vision, highlighting its domains of application
    and the challenges it faces.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉如今无处不在，以至于它的定义在不同专家之间可能有很大的差异。在本节介绍中，我们将勾画计算机视觉的整体图景，重点介绍它的应用领域和面临的挑战。
- en: Introducing computer vision
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍计算机视觉
- en: Computer vision can be hard to define because it sits at the junction of several
    research and development fields, such as *computer science* (algorithms, data
    processing, and graphics), *physics* (optics and sensors), *mathematics* (calculus
    and information theory), and *biology* (visual stimuli and neural processing).
    At its core, computer vision can be summarized as the *automated extraction of
    information from* *digital images*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉很难定义，因为它位于多个研究和开发领域的交汇点，例如*计算机科学*（算法、数据处理和图形学）、*物理学*（光学和传感器）、*数学*（微积分和信息论）和*生物学*（视觉刺激和神经处理）。从本质上讲，计算机视觉可以总结为*从*
    *数字图像*中自动提取信息。
- en: Our brain works wonders when it comes to vision. Our ability to decipher the
    visual stimuli our eyes constantly capture, to instantly tell one object from
    another, and to recognize the face of someone we have met only once, is just incredible.
    For computers, images are just blobs of pixels, matrices of red-green-blue values
    with no further meaning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑在视觉方面的表现令人惊叹。我们能够解读眼睛不断捕捉到的视觉刺激，瞬间区分不同物体，并识别出我们只见过一次的人脸，这一切都不可思议。对于计算机来说，图像只是像素块，是由红绿蓝值组成的矩阵，毫无其他意义。
- en: The goal of computer vision is to teach computers *how to make sense of these
    pixels* the way humans (and other creatures) do, or even better. Indeed, computer
    vision has come a long way and, since the rise of deep learning, it has started
    achieving *super human* performance in some tasks, such as face verification and
    handwritten text recognition.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的目标是教会计算机*如何像人类（及其他生物）一样理解这些像素*，甚至做得更好。事实上，计算机视觉已经取得了长足进展，尤其是在深度学习的兴起后，它在一些任务中已经达到了*超越人类*的表现，例如人脸验证和手写文字识别。
- en: 'With a hyper active research community fueled by the biggest IT companies,
    and the ever-increasing availability of data and visual sensors, more and more
    ambitious problems are being tackled: vision-based navigation for autonomous driving,
    content-based image and video retrieval, and automated annotation and enhancement,
    among others. It is truly an exciting time for experts and newcomers alike.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着由全球最大IT公司推动的研究社区的高度活跃，以及数据和视觉传感器日益普及，越来越多的雄心勃勃的问题正在被解决：基于视觉的自动驾驶导航、基于内容的图像和视频检索，以及自动标注和增强等。这无疑是专家和新手们都充满期待的时代。
- en: Main tasks and their applications
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要任务及其应用
- en: New computer vision-based products are appearing every day (for instance, control
    systems for industries, interactive smartphone apps, and surveillance systems)
    that cover a wide range of tasks. In this section, we will go through the main
    ones, detailing their applications in relation to real-life problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于计算机视觉的新产品每天都在出现（例如，工业控制系统、互动智能手机应用和监控系统），它们涵盖了广泛的任务。在本节中，我们将介绍其中的主要应用，并详细说明它们如何解决实际问题。
- en: Content recognition
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容识别
- en: A central goal in computer vision is to *make sense* of images, that is, to
    extract meaningful, semantic information from pixels (such as the objects present
    in images, their location, and their number). This generic problem can be divided
    into several sub-domains. Here is a non-exhaustive list.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的核心目标是*理解*图像，即从像素中提取有意义的、语义层面的信息（例如图像中出现的物体、它们的位置和数量）。这个通用问题可以分为多个子领域。以下是一个非详尽的列表。
- en: Object classification
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体分类
- en: '**Object classification** (or **image classification**) is the task of assigning
    proper labels (or classes) to images among a predefined set and is illustrated
    in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**物体分类**（或称**图像分类**）是将图像分配到预定义类别中的任务，示意图如下：'
- en: '![](img/fd9258d3-5f2a-4bf7-8b63-a752edc36d03.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd9258d3-5f2a-4bf7-8b63-a752edc36d03.png)'
- en: Figure 1.1: Example of a classifier for the labels of people and cars applied
    to an image set
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：对人和车标签的分类器应用示例，应用于图像集
- en: Object classification became famous for being the first success story of deep convolutional
    neural networks being applied to computer vision back in 2012 (this will be presented
    later in this chapter). Progress in this domain has been so fast since then that
    super human performance is now achieved in various use cases (a well-known example
    is the classification of dog breeds; deep learning methods have become extremely
    efficient at spotting the discriminative features of man's best friend).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 物体分类因2012年深度卷积神经网络首次成功应用于计算机视觉而成名（这一点将在本章后面介绍）。自那时以来，这一领域的进展非常迅速，以至于现在在各种应用场景中都达到了超越人类的表现（一个著名的例子是狗品种的分类；深度学习方法已经变得极其高效，能够识别出人类最好的朋友的区分特征）。
- en: Common applications are text digitization (using character recognition) and
    the automatic annotation of image databases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 常见应用包括文本数字化（使用字符识别）和图像数据库的自动标注。
- en: In [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml), *Influential Classification
    Tools*, we will present advanced classification methods and their impact on computer
    vision in general.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)《*有影响力的分类工具*》中，我们将介绍先进的分类方法及其对计算机视觉的整体影响。
- en: Object identification
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体识别
- en: While *object classification* methods assign labels from a predefined set, *object
    identification* (or *instance classification*) methods learn to *recognize specific
    instances of a class*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 而*物体分类*方法从预定义的集合中分配标签，*物体识别*（或*实例分类*）方法则学习*识别类别的特定实例*。
- en: 'For example, an *object classification* tool could be configured to return
    images containing faces, while an *identification* method would focus on the face''s
    features to identify the person and recognize them in other images (*identifying*
    each face in all of the images, as shown in the following diagram):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个*物体分类*工具可以配置为返回包含面部的图像，而*识别*方法则会专注于面部特征，以识别该人并在其他图像中识别他们（*识别*每一张面孔，如下图所示）：
- en: '![](img/a2479d1e-b3e6-4302-a9bb-63f142231b17.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2479d1e-b3e6-4302-a9bb-63f142231b17.png)'
- en: Figure 1.2: Example of an identifier applied to portraits
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：应用于肖像的标识符示例
- en: Therefore, object identification can be seen as a procedure to *cluster* a dataset,
    often applying some dataset analysis concepts (which will be presented in [Chapter
    6](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml), *Enhancing and Segmenting Images)*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，物体识别可以看作是一个*聚类*数据集的过程，通常应用一些数据集分析概念（将在[第六章](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml)，*增强与图像分割*中介绍）。
- en: Object detection and localization
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测与定位
- en: Another task is the *detection of specific elements in an image*. It is commonly
    applied to face detection for surveillance applications or even advanced camera
    apps, the detection of cancerous cells in medicine, the detection of damaged components
    in industrial plants, and so on.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个任务是*图像中特定元素的检测*。它通常应用于监控应用中的人脸检测，甚至高级相机应用，医学中癌细胞的检测，工业工厂中损坏组件的检测等。
- en: 'Detection is often a preliminary step before further computations, providing
    smaller patches of the image to be analyzed separately (for instance, cropping
    someone''s face for facial recognition, or providing a bounding box around an
    object to evaluate its pose for augmented reality applications), as shown in the
    following diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 检测通常是进一步计算的前置步骤，它提供了图像的较小区域，以便分别进行分析（例如，裁剪某人的面部用于人脸识别，或提供围绕物体的边界框以评估其在增强现实应用中的姿态），如下图所示：
- en: '![](img/e667b9c5-8c1c-4fae-8e3c-dc8811da1825.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e667b9c5-8c1c-4fae-8e3c-dc8811da1825.png)'
- en: 'Figure 1.3: Example of a car detector, returning bounding boxes for the candidates'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：汽车检测器示例，返回候选物体的边界框
- en: State-of-the-art solutions will be detailed in [Chapter 5](593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml),
    *Object Detection Models*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的解决方案将在[第五章](593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml)，*物体检测模型*中详细介绍。
- en: Object and instance segmentation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体与实例分割
- en: 'Segmentation can be seen as a more advanced type of detection. Instead of simply
    providing bounding boxes for the recognized elements, segmentation methods *return
    masks labeling all the pixels* belonging to a specific class or to a specific
    instance of a class (refer to the following *Figure 1.4*). This makes the task
    much more complex, and actually one of the few in computer vision where deep neural
    networks are still far from human performance (our brain is indeed remarkably
    efficient at drawing the precise boundaries/contours of visual elements). Object
    segmentation and instance segmentation are illustrated in the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 分割可以看作是更高级的检测类型。与仅提供识别元素的边界框不同，分割方法*返回标记所有像素的掩码*，这些像素属于特定类别或特定实例（请参见下图*图1.4*）。这使得任务变得更加复杂，实际上是计算机视觉中少数几个仍远不及人类表现的任务之一（我们的脑袋确实在绘制视觉元素的精确边界/轮廓方面异常高效）。物体分割和实例分割在下图中进行了说明：
- en: '![](img/0918369a-e623-454c-befa-803d8424ee2a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0918369a-e623-454c-befa-803d8424ee2a.png)'
- en: 'Figure 1.4: Comparing the results of object segmentation methods and instance
    segmentation methods for cars'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：比较汽车的物体分割方法和实例分割方法的结果
- en: In *Figure 1.4*, while the object segmentation algorithm returns a single mask
    for all pixels belonging to the *car* class, the instance segmentation one returns
    a different mask for each *car *instance that it recognized. This is a key task
    for robots and smart cars in order to understand their surroundings (for example,
    to identify all the elements in front of a vehicle), but it is also used in medical
    imagery. Precisely segmenting the different tissues in medical scans can enable
    faster diagnosis and easier visualization (such as coloring each organ differently
    or removing clutter from the view). This will be demonstrated in [Chapter 6](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml),
    *Enhancing and Segmenting Images*, with concrete experiments for autonomous driving
    applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 1.4*中，虽然物体分割算法为所有属于*汽车*类的像素返回一个单一的掩膜，但实例分割算法则为每个被识别的*汽车*实例返回不同的掩膜。这对于机器人和智能汽车来说是一个关键任务，目的是让它们理解周围环境（例如，识别车辆前方的所有元素），但在医学影像中也有应用。精准分割医学扫描中的不同组织可以加速诊断并简化可视化（例如，为每个器官上色，或去除视图中的杂物）。这将在[第6章](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml)《图像增强与分割》中进行展示，并针对自动驾驶应用提供具体实验。
- en: Pose estimation
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 姿态估计
- en: Pose estimation can have different meanings depending on the targeted tasks.
    For rigid objects, it usually means *the estimation of the objects' positions
    and orientations* relative to the camera in the 3D space. This is especially useful
    for robots so that they can interact with their environment (object picking, collision
    avoidance, and so on). It is also often used in augmented reality to overlay 3D
    information on top of objects.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态估计的含义可能根据目标任务不同而有所不同。对于刚性物体，它通常意味着*估计物体在三维空间中相对于相机的位置和方向*。这对于机器人尤其有用，使它们能够与环境互动（物体拾取、碰撞避免等）。它也常常应用于增强现实，通过叠加三维信息到物体上。
- en: 'For non-rigid elements, pose estimation can also mean *the estimation of the
    positions of their sub-parts relative to each other*. More concretely, when considering
    humans as non-rigid targets, typical applications are the recognition of human
    poses (standing, sitting, running, and so on) or understanding sign language.
    These different cases are illustrated in the following diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非刚性元素，姿态估计也可以意味着*估计其子部件之间的位置关系*。更具体地说，当将人类视为非刚性目标时，典型应用包括人类姿态的识别（站立、坐着、跑步等）或手语的理解。这些不同的情况在下图中有所示例：
- en: '![](img/1aab463e-4e8b-4894-827c-b0cc110515b5.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1aab463e-4e8b-4894-827c-b0cc110515b5.png)'
- en: 'Figure 1.5: Examples of rigid and non-rigid pose estimation'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5：刚性与非刚性姿态估计示例
- en: In both cases—that is, for whole or partial elements—the algorithms are tasked
    with evaluating their actual position and orientation relative to the camera in
    the 3D world, based on their 2D representation in an image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下——无论是针对完整还是部分元素——算法的任务都是基于其在图像中的二维表示，评估它们在三维世界中相对于相机的实际位置和方向。
- en: Video analysis
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频分析
- en: Computer vision not only applies to single images, but also to videos. If video
    streams are sometimes analyzed frame by frame, some tasks require that you consider
    an image sequence as a whole in order to take temporal consistency into account
    (this will be one of the topics of [Chapter 8](97884989-bb57-4611-8c66-ebe8ab387965.xhtml),
    *Video and Recurrent Neural Networks*).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉不仅适用于单张图像，还适用于视频。如果视频流有时逐帧分析，则有些任务要求你将图像序列整体考虑，以便考虑时间一致性（这将是[第8章](97884989-bb57-4611-8c66-ebe8ab387965.xhtml)《视频与递归神经网络》中的一个主题）。
- en: Instance tracking
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实例跟踪
- en: Some tasks relating video streams could naively be accomplished by studying
    each frame separately (memory less), but more efficient methods either take into
    account differences from image to image to guide the process to new frames or
    take complete image sequences as input for their predictions. *Tracking*, that
    is, *localizing specific elements in a video stream*, is a good example of such
    a task.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一些与视频流相关的任务可以通过简单地逐帧分析来完成（无记忆），但更高效的方法要么考虑图像间的差异，以指导处理到新帧，要么将完整的图像序列作为输入进行预测。*跟踪*，即*在视频流中定位特定元素*，就是这样一个任务的典型示例。
- en: Tracking could be done frame by frame by applying detection and identification
    methods to each frame. However, it is much more efficient to use previous results
    to model the motion of the instances in order to partially predict their locations
    in future frames. **Motion continuity** is, therefore, a key predicate here, though
    it does not always hold (such as for fast-moving objects).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪可以通过对每一帧应用检测和识别方法逐帧完成。然而，使用之前的结果来建模实例的运动，以便部分预测它们在未来帧中的位置要高效得多。因此，**运动连续性**在这里是一个关键前提，尽管它并不总是成立（例如，对于快速移动的物体）。
- en: Action recognition
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作识别
- en: On the other hand, **action recognition** belongs to the list of tasks that
    can only be run with a sequence of images. Similar to how we cannot understand
    a sentence when we are given the words separately and unordered, we cannot recognize
    an action without studying a continuous sequence of images (refer to *Figure 1.6*).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**动作识别**属于只能通过图像序列来执行的任务之一。就像我们不能仅凭单独且无序的词语理解一个句子一样，我们无法在不研究连续图像序列的情况下识别一个动作（参见*图1.6*）。
- en: 'Recognizing an action means recognizing a particular motion among a predefined
    set (for instance, for human actions—dancing, swimming, drawing a square, or drawing
    a circle). Applications range from surveillance (such as the detection of abnormal
    or suspicious behavior) to human-machine interactions (such as for gesture-controlled
    devices):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 识别一个动作意味着从预定义的一组中识别特定的运动（例如，针对人类动作——跳舞、游泳、画正方形，或画圆）。应用范围从监控（例如，检测异常或可疑行为）到人机交互（例如，手势控制设备）：
- en: '![](img/cbb6ea03-2af4-4aba-abbf-b000dddf7d5a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbb6ea03-2af4-4aba-abbf-b000dddf7d5a.png)'
- en: 'Figure 1.6: Is Barack Obama in the middle of waving, pointing at someone, swatting
    a mosquito, or something else?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：巴拉克·奥巴马是在挥手、指向某人、拍打蚊子，还是做其他事情？
- en: Only the complete sequence of frames could help to label this action
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 只有完整的帧序列才能帮助标注这一动作。
- en: Since object recognition can be split into object classification, detection,
    segmentation, and so on, so can action recognition (action classification, detection,
    and so on).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于物体识别可以分为物体分类、检测、分割等，动作识别也可以如此（动作分类、检测等）。
- en: Motion estimation
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运动估计
- en: Instead of trying to recognize moving elements, some methods focus on *estimating
    the actual velocity/trajectory* that is captured in videos. It is also common
    to evaluate the motion of the camera itself relative to the represented scene
    (*egomotion*). This is particularly useful in the entertainment industry, for
    example, to capture motion in order to apply visual effects or to overlay 3D information
    in TV streams such as sports broadcasting.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法侧重于*估计实际速度/轨迹*，而不是试图识别运动中的元素，这些方法在视频中被捕捉到。评估相对于表示场景的相机自身的运动（*自运动*）也是常见的做法。这在娱乐行业中尤为重要，例如，捕捉运动以便应用视觉效果，或在体育转播等电视直播中叠加3D信息。
- en: Content-aware image edition
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容感知图像编辑
- en: 'Besides the analysis of their content, computer vision methods can also be
    applied to *improve the images themselves*. More and more, basic image processing
    tools (such as low-pass filters for image denoising) are being replaced by *smarter*
    methods that are able to use prior knowledge of the image content to improve its
    visual quality. For instance, if a method learns what a bird typically looks like,
    it can apply this knowledge in order to replace noisy pixels with coherent ones
    in bird pictures. This concept applies to any type of image restoration, whether
    it be denoising, deblurring, or resolution enhancing (*super-resolution*, as illustrated
    in the following diagram):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分析其内容，计算机视觉方法还可以应用于*改善图像本身*。越来越多的基础图像处理工具（例如，图像去噪的低通滤波器）正被*更智能*的方法所取代，这些方法能够利用图像内容的先验知识来改善其视觉质量。例如，如果一个方法学习到鸟类的典型样子，它可以运用这些知识，将鸟类图片中的噪点像素替换为连贯的像素。这个概念适用于任何类型的图像修复，无论是去噪、去模糊，还是分辨率增强（*超分辨率*，如下面的图示所示）：
- en: '![](img/21c96ec9-8078-4c70-a912-935040d9034a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21c96ec9-8078-4c70-a912-935040d9034a.png)'
- en: 'Figure 1.7: Comparison of traditional and deep learning methods for image super-resolution.
    Notice how the details are sharper in the second image'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7：传统方法与深度学习方法在图像超分辨率上的比较。注意第二张图像中的细节更加清晰。
- en: Content-aware algorithms are also used in some photography or art applications,
    such as the *smart portrait* or *beauty* modes for smartphones, which aim to enhance some
    of the models' features, or the *smart removing/editing* tools, which get rid
    of unwanted elements and replace them with a coherent background.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 内容感知算法也被应用于一些摄影或艺术应用中，比如智能手机上的*智能人像*或*美颜*模式，旨在增强某些模型的特征，或者*智能去除/编辑*工具，能够去除不需要的元素并用连贯的背景替代。
- en: In [Chapter 6](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml), *Enhancing and Segmenting
    Images*, and in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml), *Training
    on Complex and Scarce Datasets*, we will demonstrate how such *generative* methods
    can be built and served.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml)《图像增强与分割》和[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)《复杂稀缺数据集的训练》中，我们将展示如何构建并实现这些*生成性*方法。
- en: Scene reconstruction
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景重建
- en: Finally, though we won't tackle it in this book, *scene reconstruction* is the
    task of *recovering the 3D geometry of a scene,* given one or more images. A simple
    example, based on human vision, is stereo matching. This is the process of finding
    correspondences between two images of a scene from different viewpoints in order
    to derive the distance of each visualized element. More advanced methods take
    several images and match their content together in order to obtain a 3D model
    of the target scene. This can be applied to the 3D scanning of objects, people,
    buildings, and so on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管我们在本书中不会深入讨论，*场景重建*是指在给定一张或多张图像的情况下，*恢复场景的3D几何结构*。一个简单的基于人类视觉的例子是立体匹配。这个过程是从不同视角对同一场景的两张图像进行匹配，以推算出每个可视化元素的距离。更先进的方法会使用多张图像，并将它们的内容匹配在一起，从而获取目标场景的3D模型。这种技术可以应用于物体、人类、建筑等的3D扫描。
- en: A brief history of computer vision
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉简史
- en: '"Study the past if you would define the future."'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: “如果你想定义未来，就要研究过去。”
- en: – Confucius
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ——孔子
- en: In order to better understand the current stand of the heart and current challenges
    of computer vision, we suggest that we quickly have a look at where it came from
    and how it has evolved in the past decades.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解计算机视觉的现状及其当前面临的挑战，我们建议快速回顾一下它的起源以及过去几十年的演变过程。
- en: First steps to initial successes
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从初步成功到第一步
- en: Scientists have long dreamed of developing artificial intelligence, including *visual
    intelligence*. The first advances in computer vision were driven by this idea.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 科学家们长期以来一直梦想着开发人工智能，包括*视觉智能*。计算机视觉的最初进展正是源自这一想法。
- en: Underestimating the perception task
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低估感知任务
- en: Computer vision as a domain started as early as the 60s, among the **Artificial
    Intelligence** (**AI**) research community. Still heavily influenced by the *symbolist*
    philosophy, which considered playing chess and other purely intellectual activities
    the epitome of human intelligence, these researchers underestimated the complexity
    of *lower animal functions* such as **perception**. How these researchers believed
    they could reproduce human perception through a single summer project in 1966
    is a famous anecdote in the computer vision community.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉作为一个领域，最早起步于60年代，源于**人工智能**（**AI**）研究社区。尽管当时深受*符号主义*哲学的影响，该哲学认为下棋和其他纯粹的智力活动是人类智慧的巅峰，这些研究人员低估了*低级动物功能*（如**感知**）的复杂性。1966年，这些研究人员曾相信他们能够通过一个夏季项目重现人类感知，这成为计算机视觉领域的一个著名轶事。
- en: Marvin Minsky was one of the first to outline an approach toward building AI
    systems based on perception (in *Steps toward artificial intelligence*, Proceedings
    of the IRE, 1961). He argued that with the use of lower functions such as pattern
    recognition, learning, planning, and induction, it could be possible to build
    machines capable of solving a broad variety of problems. However, this theory
    was only properly explored from the 80s onward. In *Locomotion, Vision, and Intelligence* in
    1984, Hans Moravec noted that our nervous system, through the process of evolution,
    has developed to tackle perceptual tasks (more than 30% of our brain is dedicated
    to vision!).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 马文·明斯基是最早提出基于感知构建AI系统方法的人之一（在《人工智能的步伐》一文中，发表在1961年的IRE会议录上）。他认为，利用模式识别、学习、规划和归纳等低级功能，有可能构建能够解决各种问题的机器。然而，这一理论直到80年代才得到深入研究。在1984年的《运动、视觉与智能》一书中，汉斯·莫拉维克指出，我们的神经系统通过进化过程已经发展出应对感知任务的能力（我们的大脑中超过30%的区域专门负责视觉！）。
- en: As he noted, even if computers are pretty good at arithmetic, they cannot compete
    with our perceptual abilities. In this sense, programming a computer to solve
    purely intellectual tasks (for example, playing chess) does not necessarily contribute
    to the development of systems that are intelligent in a general sense or relative
    to human intelligence.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如他所指出的，尽管计算机在算术运算方面表现得相当不错，但它们无法与我们感知能力相抗衡。从这个意义上说，编程让计算机解决纯粹的智力任务（例如，下棋）并不一定有助于开发在一般意义上或相对人类智能的智能系统。
- en: Hand-crafting local features
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手工制作局部特征
- en: Inspired by human perception, the basic mechanisms of computer vision are straightforward
    and have not evolved much since the early years—the idea is to *first extract
    meaningful features from the raw pixels*, and *then match these features to known,
    labeled ones* in order to achieve recognition.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 受人类感知的启发，计算机视觉的基本机制是简单直接的，并且自早期以来没有太大演变——其思想是*首先从原始像素中提取有意义的特征*，*然后将这些特征与已知的标记特征进行匹配*，以实现识别。
- en: In computer vision, a **feature** is a piece of information (often mathematically
    represented as a one or two-dimensional vector) that is extracted from data that
    is relevant to the task at hand. Features include some key points in the images,
    specific edges, discriminative patches, and so on. They should be easy to obtain
    from new images and contain the necessary information for further recognition.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，**特征**是从与当前任务相关的数据中提取的信息片段（通常数学上表示为一维或二维向量）。特征包括图像中的一些关键点、特定的边缘、区分性补丁等。它们应该容易从新图像中获取，并包含进一步识别所需的必要信息。
- en: Researchers used to come up with more and more complex features. The extraction
    of edges and lines was first considered for the basic geometrical understanding
    of scenes or for character recognition; then, texture and lighting information
    was also taken into account, leading to early object classifiers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员曾提出越来越复杂的特征。最初考虑提取边缘和线条用于场景的基本几何理解或字符识别；接着，纹理和光照信息也被纳入考虑，促使了早期的物体分类器的发展。
- en: 'In the 90s, features based on statistical analysis, such as **principal component
    analysis** (**PCA**), were successfully applied for the first time to complex
    recognition problems such as face classification. A classic example is the *Eigenface*
    method introduced by Matthew Turk and Alex Pentland (*Eigenfaces for Recognition*,
    MIT Press, 1991). Given a database of face images, the mean image and the *eigenvectors/images*
    (also known as **characteristic vectors/images**) were computed through PCA. This
    small set of *eigenimages* can theoretically be linearly combined to reconstruct
    any face in the original dataset, or beyond. In other words, each face picture
    can be approximated through a weighted sum of the *eigenimages* (refer to *Figure
    1.8*). This means that a particular face can simply be defined by the list of
    reconstruction weights for each *eigenimage*. As a result, classifying a new face
    is just a matter of decomposing it into *eigenimages* to obtain its weight vector,
    and then comparing it with the vectors of known faces:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在90年代，基于统计分析的特征，如**主成分分析**（**PCA**），首次成功应用于面部分类等复杂识别问题。一个经典的例子是由Matthew Turk和Alex
    Pentland提出的*特征脸*方法（《Eigenfaces for Recognition》，MIT出版社，1991年）。给定一个面部图像数据库，通过PCA计算出平均图像和*特征向量/图像*（也称为**特征向量/图像**）。这组小型的*特征图像*理论上可以线性组合，重建原始数据集中的任何面孔，甚至超出该数据集。换句话说，每张面部图像可以通过*特征图像*的加权和来逼近（参见*图1.8*）。这意味着，一个特定的面孔仅通过每个*特征图像*的重建权重列表即可定义。因此，分类一个新面孔只需要将其分解为*特征图像*，获得其权重向量，然后与已知面孔的向量进行比较：
- en: '![](img/ba6a5070-1243-47b6-a044-0e9a2dbc7b7a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba6a5070-1243-47b6-a044-0e9a2dbc7b7a.png)'
- en: 'Figure 1.8: Decomposition of a portrait image into the mean image and weighted
    sum of eigenimages. These mean and eigenimages were computed over a larger face
    dataset'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8：将肖像图像分解为平均图像和特征图像的加权和。这些平均图像和特征图像是在更大的面部数据集上计算得出的
- en: 'Another method that appeared in the late 90s and revolutionized the domain
    is called **Scale Invariant Feature Transform** (**SIFT**). As its name suggests,
    this method, introduced by David Lowe (in *Distinctive Image Features from Scale-Invariant
    Keypoints*, Elsevier), represents visual objects by a set of features that are
    robust to changes in scale and orientation. In the simplest terms, this method
    looks for some **key points** in images (searching for discontinuities in their
    *gradient*), extracts a patch around each key point, and computes a feature vector
    for each (for example, a histogram of the values in the patch or in its gradient).
    The **local features** of an image, along with their corresponding key points,
    can then be used to match similar visual elements across other images. In the
    following image, the SIFT method was applied to a picture using OpenCV ([https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html](https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html)).
    For each localized key point, the radius of the circle represents the size of
    the patch considered for the feature computation, and the line shows the feature
    orientation (that is, the main orientation of the neighborhood''s gradient):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在90年代末出现并彻底改变了该领域的方法叫做**尺度不变特征变换**（**SIFT**）。顾名思义，这种方法由David Lowe提出（在《来自尺度不变关键点的独特图像特征》一文中，Elsevier），它通过一组对尺度和方向变化具有鲁棒性的特征来表示视觉对象。简而言之，这种方法在图像中寻找一些**关键点**（通过寻找其*梯度*中的不连续性），提取每个关键点周围的图像块，并为每个关键点计算一个特征向量（例如，图像块中或其梯度中的值的直方图）。然后，图像的**局部特征**及其相应的关键点可以用来在其他图像中匹配相似的视觉元素。在下图中，SIFT方法应用于一张图片，使用了OpenCV（[https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html](https://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html)）。对于每个局部化的关键点，圆圈的半径代表考虑用于特征计算的图像块的大小，线条表示特征方向（即邻域梯度的主要方向）：
- en: '![](img/69a21917-259b-4c6b-a9cb-345589cdbc4e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69a21917-259b-4c6b-a9cb-345589cdbc4e.png)'
- en: 'Figure 1.9: Representation of the SIFT key points extracted from an image (using
    OpenCV)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9：表示从图像中提取的SIFT关键点（使用OpenCV）
- en: More advanced methods were developed over the years—with more robust ways of
    extracting key points, or computing and combining discriminative features—but
    they followed the same overall procedure (extracting features from one image,
    and comparing them to the features of others).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，出现了更先进的方法——这些方法提供了更鲁棒的方式来提取关键点、计算和结合区分性特征——但它们遵循了相同的总体流程（从一张图像中提取特征，并将其与其他图像的特征进行比较）。
- en: Adding some machine learning on top
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在此基础上加入一些机器学习
- en: It soon appeared clear, however, that extracting robust, discriminative features
    was only half the job for recognition tasks. For instance, different elements
    from the same class can look quite different (such as different-looking dogs)
    and, as a result, share only a small set of common features. Therefore, unlike
    image-matching tasks, higher-level problems such as semantic classification cannot
    be solved by simply comparing pixel features from query images with those from
    labeled pictures (such a procedure can also become sub-optimal in terms of processing
    time if the comparison has to be done with every image from a large labeled dataset).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，很快就清楚了，提取鲁棒的、区分度高的特征只是完成识别任务的一半工作。例如，同一类中的不同元素可能看起来完全不同（比如不同品种的狗），因此它们共享的共同特征可能仅限于少数几个。因此，与图像匹配任务不同，更高层次的问题，如语义分类，不能仅仅通过比较查询图像中的像素特征与标签图片中的像素特征来解决（如果必须与大型标签数据集中的每张图片进行比较，这种过程在处理时间上也可能变得不理想）。
- en: This is where *machine learning* come into play. With an increasing number of
    researchers trying to tackle image classification in the 90s, more statistical
    ways to discriminate images based on their features started to appear. **Support
    vector machines** (**SVMs**), which were standardized by Vladimir Vapnik and Corinna
    Cortes (*Support-vector networks*, Springer, 1995), were, for a long time, the
    default solution for learning a mapping from complex structures (such as images)
    to simpler labels (such as classes).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，*机器学习*发挥了作用。随着越来越多的研究人员在90年代尝试解决图像分类问题，基于特征来区分图像的统计方法逐渐出现。**支持向量机**（**SVMs**），由Vladimir
    Vapnik和Corinna Cortes标准化（《支持向量网络》，Springer，1995），在很长一段时间内，是从复杂结构（如图像）到简单标签（如类别）进行映射学习的默认解决方案。
- en: 'Given a set of image features and their binary labels (for example, *cat* or
    *not cat,* as illustrated in *Figure 1.10*), an SVM can be optimized to learn
    the function to separate one class from another, based on extracted features.
    Once this function is obtained, it is just a matter of applying it to the feature
    vector of an unknown image so that we can map it to one of the two classes (SVMs
    that could extend to a larger number of classes were later developed). In the
    following diagram, an SVM was taught to regress a linear function separating two
    classes based on features extracted from their images (features as vectors of
    only two values in this example):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fb96b6d-1f26-491c-90c2-6b00a1362d04.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: An illustration of a linear function regressed by an SVM. Note
    that using a concept known as the kernel trick, SVMs can also find non-linear
    solutions to separate classes'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Other machine learning algorithms were adapted over the years by the computer
    vision community, such as *random forests*, *bags of words*, *Bayesian* *models*,
    and obviously *neural networks*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Rise of deep learning
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, how did neural networks take over computer vision and become what we nowadays
    know as **deep learning**? This section offers some answers, detailing the technical
    development of this powerful tool.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Early attempts and failures
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It may be surprising to learn that artificial neural networks appeared even
    before modern computer vision. Their development is the typical story of an invention
    too early for its time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Rise and fall of the perceptron
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the 50s, Frank Rosenblatt came up with the **perceptron**, a machine learning
    algorithm inspired by neurons and the underlying block of the first neural networks
    (*The Perceptron: A Probabilistic Model for Information Storage and Organization
    in the Brain*, American Psychological Association, 1958). With the proper learning
    procedure, this method was already able to recognize characters. However, the
    hype was short-lived. Marvin Minsky (one of the fathers of AI) and Seymor Papert
    quickly demonstrated that the perceptron could not learn a function as simple
    as `XOR` (exclusive *OR*, the function that, given two binary input values, returns
    `1` if one, and only one, input is `1`, and returns `0` otherwise). This makes
    sense to us nowadays—as the perceptron back then was modeled with a linear function
    while `XOR` is a non-linear one—but, at that time, it simply discouraged any further
    research for years.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Too heavy to scale
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It was only in the late 70s to early 80s that neural networks got some attention
    put back on them. Several research papers introduced how neural networks, with
    multiple *layers* of perceptrons put one after the other, could be trained using
    a rather straightforward scheme—backpropagation. As we will detail in the next
    section, this training procedure works by computing the network's error and backpropagating
    it through the layers of perceptrons to update their parameters using *derivatives*.
    Soon after, the first **convolutional neural network** (**CNN**), the ancestor
    of current recognition methods, was developed and applied to the recognition of
    handwritten characters with some success.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Alas, these methods were computationally heavy, and just could not scale to
    larger problems. Instead, researchers adopted lighter machine learning methods
    such as SVMs, and the use of neural networks stalled for another decade. So, what
    brought them back and led to the deep learning era we know of today?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Reasons for the comeback
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reasons for this comeback are twofold and rooted in the explosive evolution
    of the internet and hardware efficiency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The internet – the new El Dorado of data science
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The internet was not only a revolution in communication; it also deeply transformed
    data science. It became much easier for scientists to share images and content
    by uploading them online, leading to the creation of public datasets for experimentation
    and benchmarking. Moreover, not only researchers but soon everyone, all over the
    world, started adding new content online, sharing images, videos, and more at
    an exponential rate. This started *big data* and the *golden age of data science*,
    with the internet as the new El Dorado.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: By simply indexing the content that is constantly published online, image and
    video datasets reached sizes that were never imagined before, from *Caltech-101*
    (10,000 images, published in 2003 by Li Fei-Fei et al., Elsevier) to *ImageNet*
    (14+ million images, published in 2009 by Jia Deng et al., IEEE) or *Youtube-8M*
    (8+ million videos, published in 2016 by Sami Abu-El-Haija et al., including Google).
    Even companies and governments soon understood the numerous advantages of gathering
    and releasing datasets to boost innovation in their specific domains (for example,
    the i-LIDS datasets for video surveillance released by the British government
    and the COCO dataset for image captioning sponsored by Facebook and Microsoft,
    among others).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: With so much data available covering so many use cases, new doors were opened
    (*data-hungry* algorithms, that is, methods requiring a lot of training samples
    to converge could finally be applied with success), and new challenges were raised
    (such as how to efficiently process all this information).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: More power than ever
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Luckily, since the internet was booming, so was computing power. Hardware kept
    becoming cheaper as well as faster, seemingly following Moore's famous law (which
    states that processor speeds should double every two years—this has been true
    for almost four decades, though a deceleration is now being observed). As computers
    got faster, they also became better designed for computer vision. And for this,
    we have to thank video games.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The **graphical processing unit** (**GPU**) is a computer component, that is,
    a chip specifically designed to handle the kind of operations needed to run 3D
    games. Therefore, a GPU is optimized to generate or manipulate images, parallelizing
    these heavy matrix operations. Though the first GPUs were conceived in the 80s,
    they became affordable and popular only with the advent of the new millennium.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In 2007, NVIDIA, one of the main companies designing GPUs, released the first
    version of **CUDA**, a programming language that allows developers to directly
    program for compatible GPUs. **OpenCL**, a similar language, appeared soon after.
    With these new tools, people started to harness the power of GPUs for new tasks,
    such as machine learning and computer vision.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning or the rebranding of artificial neural networks
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The conditions were finally there for data-hungry, computationally-intensive
    algorithms to shine. Along with *big data* and *cloud computing*, *deep learning* was
    suddenly everywhere.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: What makes learning deep?
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Actually, the term **deep learning** had already been coined back in the 80s,
    when neural networks first began stacking two or three layers of neurons. As opposed
    to the early, simpler solutions, *deep learning* regroups *deeper* neural networks,
    that is, networks with multiple *hidden layers—*additional layers set between
    their input and output layers. Each layer processes its inputs and passes the
    results to the next layer, all trained to extract increasingly abstract information.
    For instance, the first layer of a neural network would learn to react to basic
    features in the images, such as edges, lines, or color gradients; the next layer
    would learn to use these cues to extract more advanced features; and so on until
    the last layer, which infers the desired output (such as predicted class or detection
    results).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: However, *deep learning* only really started being used from 2006, when Geoff
    Hinton and his colleagues proposed an effective solution to train these deeper
    models, one layer at a time, until reaching the desired depth (*A Fast Learning
    Algorithm for Deep Belief Nets*, MIT Press, 2006).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning era
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With research into neural networks once again back on track, deep learning started
    growing, until a major breakthrough in 2012, which finally gave it its contemporary
    prominence. Since the publication of ImageNet, a competition (**ImageNet Large
    Scale Visual Recognition Challenge** (**ILSVRC**)—[image-net.org/challenges/LSVRC](http://image-net.org/challenges/LSVRC/))
    has been organized every year for researchers to submit their latest classification
    algorithms and compare their performance on ImageNet with others. The winning
    solutions in 2010 and 2011 had classification errors of 28% and 26% respectively,
    and applied traditional concepts such as SIFT features and SVMs. Then came the
    2012 edition, and a new team of researchers reduced the recognition error to a
    staggering 16%, leaving all the other contestants far behind.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: In their paper describing this achievement (*Imagenet Classification with Deep
    Convolutional Neural Networks*, NIPS, 2012), Alex Krizhevsky, Ilya Sutskever,
    and Geoff Hinton presented what would become the basis for modern recognition
    methods. They conceived an 8-layer neural network, later named **AlexNet**, with
    several *convolutional layers* and other modern components such as **dropout**
    and **rectified linear activation units** (**ReLUs**), which will all be presented
    in detail in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern
    Neural Networks*, as they have became central to computer vision. More importantly,
    they used CUDA to implement their method so that it can be run on GPUs, finally
    making it possible to train deep neural networks in a reasonable time, iterating
    over datasets as big as ImageNet.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: That same year, Google demonstrated how advances in **cloud computing** could
    also be applied to computer vision. Using a dataset of 10 million random images
    extracted from YouTube videos, they taught a neural network to identify images
    containing cats and parallelized the training process over 16,000 machines to
    finally double the accuracy compared to previous methods.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: And so started the deep learning era we are currently in. Everyone jumped on
    board, coming up with deeper and deeper models, more advanced training schemes,
    and lighter solutions for portable devices. It is an exciting period, as the more
    efficient deep learning solutions become, the more people try to apply them to
    new applications and domains. With this book, we hope to convey some of this current
    enthusiasm and provide you with an overview of the modern methods and how to develop
    solutions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with neural networks
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, we know that neural networks form the core of deep learning and are
    powerful tools for modern computer vision. But what are they exactly? How do they
    work? In the following section, not only will we tackle the theoretical explanations
    behind their efficiency, but we will also directly apply this knowledge to the
    implementation and application of a simple network to a recognition task.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial neural networks** (**ANNs**), or simply **neural networks** (**NNs**),
    are powerful machine learning tools that are excellent at processing information,
    recognizing usual patterns or detecting new ones, and approximating complex processes.
    They have to thank their structure for this, which we will now explore.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Imitating neurons
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is well-known that neurons are the elemental supports of our thoughts and
    reactions. What might be less evident is how they actually work and how they can
    be simulated.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Biological inspiration
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs are loosely inspired by how animals' brains work. Our brain is a complex
    network of neurons, each passing information to each other and processing sensory
    inputs (as electrical and chemical signals) into thoughts and actions. Each neuron
    receives its electrical inputs from its *dendrites*, which are cell fibers that
    propagate the electrical signal from the *synapses* (the junctions with preceding
    neurons) to the *soma* (the neuron's main body). If the accumulated electrical
    stimulation exceeds a specific threshold, the cell is *activated* and the electrical
    impulse is *propagated further* to the next neurons through the cell's *axon*
    (the neuron's *output cable*, ending with several synapses linking to other neurons).
    Each neuron can, therefore, be seen as a really *simple signal processing unit*,
    which—once stacked together—can achieve the thoughts we are having right now,
    for instance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical model
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inspired by its biological counterpart (represented in *Figure 1.11*), the
    artificial neuron takes several *inputs* (each a number), sums them together,
    and finally applies an *activation function* to obtain the *output* signal, which
    can be passed to the following neurons in the network (this can be seen as a directed
    graph):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff29311c-b8a5-44cb-9823-92c9c8475829.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11: On the left, we can see a simplified biological neuron. On the
    right, we can see its artificial counterpart'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The summation of the inputs is usually done in a weighted way. Each **input**
    is scaled up or down, depending on a weight specific to this particular **input**.
    These *weights* are the parameters that are adjusted during the training phase
    of the network in order for the neuron to react to the correct features. Often,
    another parameter is also trained and used for this summation process—the neuron's
    *bias*. Its value is simply added to the weighted sum as an *offset*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly formalize this process mathematically. Suppose we have a neuron
    that takes two input values, *x*[0] and *x*[1]. Each of these values would be
    weighted by a factor, *w*[0] and *w*[1], respectively, before being summed together,
    with an optional bias, *b*. For simplification, we can express the input values
    as a horizontal vector, *x*, and the weights as a vertical vector, *w*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/194aefc5-8738-44ff-a5e0-8c475607c401.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'With this formulation, the whole operation can simply be expressed as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5932c702-38e9-4f57-a380-52fc732f36ee.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: 'This step is straightforward, isn''t it? The *dot product* between the two
    vectors takes care of the weighted summation:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad4c7615-419e-47dd-857c-b5a2c17fe3d7.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Now that the inputs have been scaled and summed together into the result, *z*,
    we have to apply the *activation function* to it in order to get the neuron''s
    output. If we go back to the analogy with the biological neuron, its activation
    function would be a binary function such as *if* *y* *is above a threshold* *t**,
    return an electrical impulse that is 1, or else return 0* (with *t* = 0 usually).
    If we formalize this, the activation function, *y* = *f*(*z*), can be expressed
    as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49c11b52-fdff-4288-aa93-f9a6d7ff217e.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'The **step function** is a key component of the original perceptron, but more
    advanced activation functions have been introduced since then with more advantageous
    properties, such as *non-linearity* (to model more complex behaviors) and *continuous
    differentiability* (important for the training process, which we will explain
    later). The most common activation functions are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The **sigmoid** function,  ![](img/16c9208a-00ed-4907-90cf-58e1032b1ec5.png) (with 𝑒
    the exponential function)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **hyperbolic tangent**, ![](img/d24e0919-a1ab-4c88-b522-585dfcc8e905.png)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **REctified Linear Unit **(**ReLU**),  ![](img/9a4211de-d66d-4c68-b300-67c9e81acca8.png)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plots of the aforementioned common activation functions are shown in the following
    diagram:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c243ef46-fa4e-453b-8d4c-fc1c89efff73.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.12: Plotting common activation functions'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: In any case, that's it! We have modeled a simple artificial neuron. It is able
    to receive a signal, process it, and output a value that can be *forwarded* (a
    term that is commonly used in machine learning) to other neurons, building a network.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Chaining neurons with no non-linear activation functions would be equivalent
    to having a single neuron. For instance, if we had a linear neuron with parameters
    *w*[*A*] and *b*[*A*] followed by a linear neuron with parameters *w*[*B*] and
    *b*[*B*], then ![](img/b218a0de-15c0-4dc1-8be8-36f0cf1e1015.png), where *w* = *w*[*A**^(![](img/7d79fcb3-e427-486e-9eef-3060c702b2c1.png))*]*w*[*B*]
    and *b* =*b*[*A*] + *b*[*B*]. Therefore, non-linear activation functions are a
    necessity if we want to create complex models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Such a model can be implemented really easily in Python (using NumPy for vector
    and matrix manipulations):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As we can see, this is a direct adaptation of the mathematical model we defined
    previously. Using this artificial neuron is just as straightforward. Let''s instantiate
    a perceptron (a neuron with the step function for the activation method) and forward
    a random input through it:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We suggest that you take some time and experiment with different inputs and
    neuron parameters before we scale up their dimensions in the next section.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Layering neurons together
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually, neural networks are organized into *layers*, that is, sets of neurons
    that typically receive the same input and apply the same operation (for example,
    by applying the same activation function, though each neuron first sums the inputs
    with its own specific weights).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical model
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In networks, the information flows from the input layer to the output layer,
    with one or more *hidden* layers in-between. In *Figure 1.13*, the three neurons
    **A**, **B**, and **C** belong to the input layer, the neuron **H** belongs to
    the output or activation layer, and the neurons **D**, **E**, **F**, and **G** belong
    to the hidden layer. The first layer has an input, *x*, of size 2, the second
    (hidden) layer takes the three activation values of the previous layer as input,
    and so on. Such layers, with each neuron connected to all the values from the
    previous layer, are classed as being **fully connected** or **dense**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5d3267d-951a-46f0-b216-9823d147580a.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13: A 3-layer neural network, with two input values and one final
    output'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, we can compact the calculations by representing these elements
    with vectors and matrices. The following operations are done by the first layers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f948048e-840f-4561-bd3c-4691966f767c.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: 'This can be expressed as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b712dec9-383e-46af-a7dc-bcb79e7928d7.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'In order to obtain the previous equation, we must define the variables as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29647de3-3ec2-48f8-b302-dbe8963eaa8b.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: The activation of the first layer can, therefore, be written as a vector, ![](img/20f4ff4b-4c6c-4e71-8e39-692e6e3fe7d7.png),
    which can be directly passed as an input vector to the next layer, and so on until
    the last layer.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like the single neuron, this model can be implemented in Python. Actually,
    we do not even have to make too many edits compared to our `Neuron` class:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We just have to change the *dimensionality* of some of the variables in order
    to reflect the multiplicity of neurons inside a layer. With this implementation,
    our layer can even process several inputs at once! Passing a single column vector *x*
    (of shape 1 × *s* with *s* number of values in *x*) or a stack of column vectors
    (of shape *n* × *s* with *n* number of samples) does not change anything with
    regard to our matrix calculations, and our layer will correctly output the stacked
    results (assuming *b* is added to each row):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A stack of input data is commonly called a **batch**.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: With this implementation, it is now just a matter of chaining fully connected
    layers together to build simple neural networks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Applying our network to classification
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know how to define layers, but have yet to initialize and connect them into
    networks for computer vision. To demonstrate how to do this, we will tackle a
    famous recognition task.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the task
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Classifying images of handwritten digits (that is, recognizing whether an image
    contains a `0` or a `1` and so on) is a historical problem in computer vision.
    The **Modified National Institute of Standards and Technology** (**MNIST**) dataset
    ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)), which
    contains 70,000 grayscale images (*28 × 28* pixels) of such digits, has been used
    as a reference over the years so that people can test their methods for this recognition
    task (Yann LeCun and Corinna Cortes hold all copyrights for this dataset, which
    is shown in the following diagram):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c1f9cf5-56a6-4297-b1e9-42b757bced01.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14: Ten samples of each digit from the MNIST dataset'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: For digit classification, what we want is a network that takes one of these
    images as input and returns an output vector expressing *how strongly the network
    believes the image corresponds to each class*. The input vector has *28 × 28 =
    784* values, while the output has 10 values (for the 10 different digits, from
    `0` to `9`). In-between all of this, it is up to us to define the number of hidden
    layers and their sizes. To predict the class of an image, it is then just a matter
    of *forwarding the image vector through the network, collecting the output*, and
    *returning the class with the highest belief score*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: These *belief* scores are commonly transformed into probabilities to simplify
    further computations or the interpretation. For instance, let's suppose that a
    classification network gives a score of 9 to the class *dog*, and a score of 1
    to the other class, *cat*. This is equivalent to saying that *according to this
    network, there is a 9/10 probability that the image shows a dog and a 1/10 probability
    it shows a cat*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we implement a solution, let''s prepare the data by loading the MNIST
    data for training and testing methods. For simplicity, we will use the `mnist` Python
    module ([https://github.com/datapythonista/mnist](https://github.com/datapythonista/mnist)),
    which was developed by Marc Garcia (under the BSD 3-Clause *New* or *Revised* license,
    and is already installed in this chapter''s source directory):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: More detailed operations for the preprocessing and visualization of the dataset
    can be found in this chapter's source code.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the network
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the neural network itself, we have to wrap the layers together and add
    some methods to forward through the complete network and to predict the class
    according to the output vector. After the layer''s implementation, the following
    code should be self-explanatory:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We just implemented a feed-forward neural network that can be used for classification!
    It is now time to apply it to our problem:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We only got an accuracy of ~12.06%. This may look disappointing since it is
    an accuracy that's barely better than random guessing. But it makes sense—right
    now, our network is defined by random parameters. We need to train it according
    to our use case, which is a task that we will tackle in the next section.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Neural networks** are a particular kind of algorithm because they need to
    be *trained*, that is, their parameters need to be optimized for a specific task
    by making them learn from available data. Once the networks are optimized to perform
    well on this *training dataset*, they can be used on new, similar data to provide
    satisfying results (if the training was done properly).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Before solving the problem of our MNIST task, we will provide some theoretical
    background, cover different learning strategies, and present how training is actually
    done. Then, we will directly apply some of these notions to our example so that
    our simple network finally learns how to solve the recognition task!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Learning strategies
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to teaching neural networks, there are three main paradigms, depending
    on the task and the availability of training data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Supervised learning* may be the most common paradigm, and it is certainly
    the easiest to grasp. It applies when we want to *teach neural networks a mapping
    between two modalities* (for example, mapping images to their class labels or
    to their semantic masks). It requires access to a training dataset containing
    both the *images* and their *ground truth labels* (such as the class information
    per image or the semantic masks).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, the training is then straightforward:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Give the images to the network and collect its results (that is, predicted labels).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the network's *loss*, that is, how wrong its predictions are when comparing
    it to the ground truth labels.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjust the network parameters accordingly to reduce this loss.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat until the network *converges,* that is, until it cannot improve further
    on this training data.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, this strategy deserves the adjective *supervised—*an entity (us)
    supervises the training of the network by providing it with feedback for each
    prediction (the loss computed from the ground truths) so that the method can learn
    by repetition (*it was correct/false*; *try again*).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, how do we train a network when we do not have any ground truth information
    available? *Unsupervised learning* is one answer to this. The idea here is to
    craft a function that *computes the network's loss only based on its input and
    its corresponding output*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: This strategy applies very well to applications such as clustering (grouping
    images with similar properties together) or compression (reducing the content
    size while preserving some properties). For clustering, the loss function could
    measure how similar images from one cluster are compared to images from other
    clusters. For compression, the loss function could measure how well preserved
    the important properties are in the compressed data compared to the original ones.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning thus requires some *expertise* regarding the use cases
    so that we can come up with meaningful loss functions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Reinforcement learning* is an **interactive strategy**. An *agent* navigates
    through an *environment* (for example, a robot moving around a room or a video
    game character going through a level). The agent has a predefined list of actions
    it can make (*walk*, *turn*, *jump*, and so on) and, after each action, it ends
    up in a new *state*. Some states can bring *rewards*, which are immediate or delayed,
    and positive or negative (for instance, a positive reward when the video game
    character touches a bonus item, and a negative reward when it is hit by an enemy).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: At each instant, the neural network is provided only with *observations* from
    the environment (for example, the robot's visual feed, or the video game screen)
    and reward feedback (the *carrot and stick*). From this, it has to learn what
    brings higher rewards and *estimate the best short-term or long-term policy for
    the agent* accordingly. In other words, it has to estimate the series of actions
    that would maximize its end reward.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is a powerful paradigm, but it is less commonly applied
    to computer vision use cases. It won't be presented further here, though we encourage
    machine learning enthusiasts to learn more.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Teaching time
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whatever the learning strategy, the overall training steps are the same. Given
    some training data, the network makes its predictions and receives some feedback
    (such as the results of a loss function), which is then used to update the network's
    parameters. These steps are then repeated until the network cannot be optimized
    further. In this section, we will detail and implement this process, from loss
    computation to weights optimization.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the loss
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the *loss function* is to evaluate how well the network, with its
    current weights, is performing. More formally, this function expresses the *quality
    of the predictions as a function of the network's parameters* (such as its weights
    and biases). The smaller the loss, the better the parameters are for the chosen
    task.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Since loss functions represent the goal of networks (*return the correct labels*,
    *compress the image while preserving the content*, and so on), there are as many
    different functions as there are tasks. Still, some loss functions are more commonly
    used than others. This is the case for the *sum-of-squares* function, also called
    **L2 loss** (based on the L2 norm), which is omnipresent in supervised learning.
    This function simply computes the squared difference between each element of the
    output vector *y* (the per-class probabilities estimated by our network) and each
    element of the ground truth vector *y^(true)* (the target vector with null values
    for every class but the correct one):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e0dfbfc-c782-426c-a6fe-76401aa59515.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: 'There are plenty of other losses with different properties, such as **L1 loss**,
    which computes the *absolute difference* between the vectors, or **binary cross-entropy**
    (**BCE**) loss, which converts the predicted probabilities into a logarithmic
    scale before comparing them to the expected values:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5968b1f-69bc-4df1-bae3-7981bc4fd5f7.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: The logarithmic operation converts the probabilities from [0, 1] into [-![](img/e7bf8923-5c1e-4e3d-bc50-0f943f7d31e7.png),
    0]. So, by multiplying the results by -1, the loss value moves from +![](img/edb35237-2b09-485c-84e3-05e05cf4f2ba.png) 
    to 0 as the neural network learns to predict properly. Note that the cross-entropy
    function can also be applied to multi-class problems (not just binary).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: It is also common for people to divide the losses by the number of elements
    in the vectors, that is, computing the mean instead of the sum. The **mean square
    error** (**MSE**) is the averaged version of the L2 loss, and the **mean absolute
    error** (**MAE**) is the average version of the L1 loss.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: For now, we will stick with the L2 loss as an example. We will use it for the
    rest of the theoretical explanations, as well as to train our MNIST classifier.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagating the loss
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How can we update the network parameters so that they minimize the loss? For
    each parameter, what we need to know is how slightly changing its value would
    affect the loss. If we know which changes would slightly decrease the loss, then
    it is just a matter of applying these changes and repeating the process until
    reaching a minimum. This is exactly what the *gradient* of the loss function expresses,
    and what the *gradient descent* process is.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'At each training iteration, the derivatives of the loss with respect to each
    parameter of the network are computed. These derivatives indicate which small
    changes to the parameters need to be applied (with a -1 coefficient since the
    gradient indicates the direction of increase of the function, while we want to
    minimize it). It can be seen as walking step by step down the *slope* of the loss
    function with respect to each parameter, hence the name **gradient descent** for
    this iterative process (refer to the following diagram):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cec2eab-f584-4603-a985-15aa12a040ba.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.15: Illustrating the gradient descent to optimize a parameter *P*
    of the neural network'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'The question now is, how can we compute all of these derivatives (the *slope* values
    as a function of each parameter)? This is where the **chain rule** comes to our
    aid. Without going too deep into calculus, the chain rule tells us that the derivatives
    with respect to the parameters of a layer, *k*, can be *simply* computed with
    the input and output values of that layer (*x*[*k*], *y*[*k*]), and the derivatives
    of the following layer, *k* + 1\. More formally, for the layer''s weights, *W*[*k*],
    we have the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa5594e4-861d-4262-bf99-d7a22d877e22.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'Here, *l*''[*k*+1] is the derivative that is computed for layer *k* + 1 with
    respect to its input, *x*[*k*+1] = *y*[*k*], with *f*''[*k*] being the derivative
    of the layer''s activation function, and ![](img/8df72250-938a-4586-a84c-345f20534504.png)
    being the *transpose* of *x*. Note that *z*[*k*] represents the result of the
    weighted sum performed by the layer *k* (that is, before the input of the layer''s
    activation function), as defined in the *Layering neurons together *section. Finally,
    the ![](img/48ca7008-9ba0-43bc-b7b1-f6e517d07566.png) symbol represents the *element-wise
    multiplication* between two vectors/matrices. It is also known as the *Hadamard
    product*. As shown in the following equation, it basically consists of multiplying
    the elements pair-wise:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9c4ac95-ae33-4c81-be83-e8d48a69ace1.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: 'Back to the chain rule, the derivatives with respect to the bias can be computed
    in a similar fashion, as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8970bce6-2446-4d8f-9913-4df643ef4a6e.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: 'Finally, to be exhaustive, we have the following equation:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24941b4e-17b3-4ddf-af12-40a787c5ce4a.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: These calculations may look complex, but we only need to understand what they
    represent—we can compute how each parameter affects the loss recursively, layer
    by layer, going backward (using the derivatives for a layer to compute the derivatives
    for the previous layer). This concept can also be illustrated by representing
    neural networks as *computational graphs*, that is, as graphs of mathematical
    operations chained together (the weighted summation of the first layer is performed
    and its result is passed to the first activation function, then its own output
    is passed to the operations of the second layer, and so on). Therefore, computing
    the result of a whole neural network with respect to some inputs consists of *forwarding*
    the data through this computational graph, while obtaining the derivatives with
    respect to each of its parameters consists of propagating the resulting loss through
    the graph backward, hence the term **backpropagation**.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'To start this process by the output layer, the derivatives of the loss itself
    with respect to the output values are needed (refer to the previous equation).
    Therefore, it is primordial that the loss function can be easily derived. For
    instance, the derivative of the L2 loss is simply the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed9c6804-44af-4058-aab8-6014e2e23030.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: 'As we mentioned earlier, once we know the loss derivatives with respect to
    each parameter, it is just a matter of updating them accordingly:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71e98db2-8295-4c1e-b096-7f5f25d6e86a.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the derivatives are often multiplied by a factor ![](img/5b725f39-5b64-4faf-80c2-6ca9ce5b412d.png)
    (*epsilon*) before being used to update the parameters. This factor is called
    the **learning rate**. It helps to control how strongly each parameter should
    be updated at each iteration. A large learning rate may allow the network to learn
    faster, but with the risk of making steps so big that the network may *miss* the
    loss minimum. Therefore, its value should be set with care. Let''s now summarize
    the complete training process:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Select the *n* next training images and feed them to the network.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute and backpropagate the loss, using the chain rule to get the derivatives
    with respect to the parameters of the layers.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameters with the values of the corresponding derivatives (scaled
    with the learning rate).
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 to 3 to iterate over the whole training set.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 to 4 until convergence or until a fixed number of iterations.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One iteration over the whole training set (*steps 1* to* 4*) is called an **epoch**.
    If *n* = 1 and the training sample is randomly selected among the remaining images,
    this process is called **stochastic gradient descent** (**SGD**), which is easy
    to implement and visualize, but slower (more updates are done) and *noisier*.
    People tend to prefer *mini-batch stochastic gradient descent*. It implies using
    larger *n* values (limited by the capabilities of the computer) so that the gradient
    is averaged over each *mini-batch* (or, more simply, named *batch*) of *n* random
    training samples (and is thus less noisy).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, the term SGD is commonly used, regardless of the value of *n*.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have covered how neural networks are trained. It is now
    time to put this into practice!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Teaching our network to classify
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have only implemented the feed-forward functionality for our network
    and its layers. First, let''s update our `FullyConnectedLayer` class so that we
    can add methods for backpropagation and optimization:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The code presented in this section has been simplified and stripped of comments
    to keep its length reasonable. The complete sources are available in this book's
    GitHub repository, along with a Jupyter notebook that connects everything together.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to update the `SimpleNetwork` class by adding methods to backpropagate
    and optimize layer by layer, and a final method to cover the complete training:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Everything is now ready! We can train our model and see how it performs:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Congratulations! If your machine is powerful enough to complete this training
    (this simple implementation does not take advantage of the GPU), we just obtained
    our very own neural network that is able to classify handwritten digits with an
    accuracy of ~94.8%!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Training considerations – underfitting and overfitting
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We invite you to play around with the framework we just implemented, trying
    different *hyperparameters* (layer sizes, learning rate, batch size, and so on).
    Choosing the proper topography (as well as other *hyperparameters*) can require
    lots of tweaking and testing. While the sizes of the input and output layers are
    conditioned by the use case (for example, for classification, the input size would
    be the number of pixel values in the images, and the output size would be the
    number of classes to predict from), the hidden layers should be carefully engineered.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if the network has too few layers, or the layers are too small,
    the accuracy may stagnate. This means the network is **underfitting**, that is,
    it does not have enough parameters for the complexity of the task. In this case,
    the only solution is to adopt a new architecture that is more suited to the application.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if the network is too complex and/or the training dataset
    is too small, the network may start **overfitting** the training data. This means
    that the network will learn to fit very well to the training distribution (that
    is, its particular noise, details, and so on), but won''t generalize to new samples
    (since these new images may have a slightly different noise, for instance). The
    following diagram highlights the differences between these two problems. The regression
    method on the extreme left does not have enough parameters to model the data variations,
    while the method on the extreme right has too many, which means it will struggle
    to generalize:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df4d89d2-9ef3-4ba9-bb46-d20f537ea3ec.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.16: A common illustration of underfitting and overfitting'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: While gathering a larger, more diverse training dataset seems the logical solution
    to overfitting, it is not always possible in practice (for example, due to limited
    access to the target objects). Another solution is to adapt the network or its
    training in order to constrain how much detail the network learns. Such methods
    will be detailed in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern
    Neural Networks*, among other advanced neural network solutions.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered a lot of ground in this first chapter. We introduced computer vision,
    the challenges associated with it, and some historical methods, such as SIFT and
    SVMs. We got familiar with neural networks and saw how they are built, trained,
    and applied. After implementing our own classifier network from scratch, we can
    now better understand and appreciate how machine learning frameworks work.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: With this knowledge, we are now more than ready to start with TensorFlow in
    the next chapter.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which of the following tasks does not belong to computer vision?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A web search for images similar to a query
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A 3D scene reconstruction from image sequences
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Animation of a video character
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Which activation function were the original perceptrons using?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose we want to train a method to detect whether a handwritten digit is a
    4 or not. How should we adapt the network that we implemented in this chapter
    for this task?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Hands-On Image Processing with Python* ([https://www.packtpub.com/big-data-and-business-intelligence/hands-image-processing-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-image-processing-python)),
    by Sandipan Dey: A great book to learn more about image processing itself, and
    how Python can be used to manipulate visual data'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenCV 3.x with Python By Example – Second Edition* ([https://www.packtpub.com/application-development/opencv-3x-python-example-second-edition](https://www.packtpub.com/application-development/opencv-3x-python-example-second-edition)),
    by Gabriel Garrido and Prateek Joshi: Another recent book introducing the famous
    computer vision library *OpenCV*, which has been around for years (it implements
    some of the traditional methods we introduced in this chapter, such as edge detectors,
    SIFT, and SVM)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
