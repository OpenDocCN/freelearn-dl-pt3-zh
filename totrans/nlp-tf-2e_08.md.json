["```\nurl = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\ndir_name = 'data'\ndef download_data(url, filename, download_dir):\n    \"\"\"Download a file if not present, and make sure it's the right \n    size.\"\"\"\n\n    # Create directories if doesn't exist\n    os.makedirs(download_dir, exist_ok=True)\n\n    # If file doesn't exist download\n    if not os.path.exists(os.path.join(download_dir,filename)):\n        filepath, _ = urlretrieve(url + filename, \n        os.path.join(download_dir,filename))\n    else:\n        filepath = os.path.join(download_dir, filename)\n\n    return filepath\n# Number of files and their names to download\nnum_files = 209\nfilenames = [format(i, '03d')+'.txt' for i in range(1,num_files+1)]\n# Download each file\nfor fn in filenames:\n    download_data(url, fn, dir_name)\n# Check if all files are downloaded\nfor i in range(len(filenames)):\n    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n    assert file_exists\nprint('{} files found.'.format(len(filenames))) \n```", "```\nfrom sklearn.model_selection import train_test_split\n# Fix the random seed so we get the same output everytime\nrandom_state = 54321\nfilenames = [os.path.join(dir_name, f) for f in os.listdir(dir_name)]\n# First separate train and valid+test data\ntrain_filenames, test_and_valid_filenames = train_test_split(filenames, test_size=0.2, random_state=random_state)\n# Separate valid+test data to validation and test data\nvalid_filenames, test_filenames = train_test_split(test_and_valid_filenames, test_size=0.5, random_state=random_state) \n# Print out the sizes and some sample filenames\nfor subset_id, subset in zip(('train', 'valid', 'test'), (train_filenames, valid_filenames, test_filenames)):\n    print(\"Got {} files in the {} dataset (e.g. \n    {})\".format(len(subset), subset_id, subset[:3])) \n```", "```\nGot 167 files in the train dataset (e.g. ['data\\\\117.txt', 'data\\\\133.txt', 'data\\\\069.txt'])\nGot 21 files in the valid dataset (e.g. ['data\\\\023.txt', 'data\\\\078.txt', 'data\\\\176.txt'])\nGot 21 files in the test dataset (e.g. ['data\\\\129.txt', 'data\\\\207.txt', 'data\\\\170.txt']) \n```", "```\nbigram_set = set()\n# Go through each file in the training set\nfor fname in train_filenames:\n    document = [] # This will hold all the text\n    with open(fname, 'r') as f:\n        for row in f:\n            # Convert text to lower case to reduce input dimensionality\n            document.append(row.lower())\n        # From the list of text we have, generate one long string \n        # (containing all training stories)\n        document = \" \".join(document)\n        # Update the set with all bigrams found\n        bigram_set.update([document[i:i+2] for i in range(0, \n        len(document), 2)])\n# Assign to a variable and print \nn_vocab = len(bigram_set)\nprint(\"Found {} unique bigrams\".format(n_vocab)) \n```", "```\nFound 705 unique bigrams \n```", "```\nBatch 1: [\"th\", \"e \", \"ki\", \" ng\", \" w\"] -> [\"e \", \"ki\", \"ng\", \" w\", \"as\"]\nBatch 2: [\"as\", \" h\", \"un\", \"ti\", \"ng\"] -> [\" h\", \"un\", \"ti\", \"ng\", \" i\"]\n… \n```", "```\ndef generate_tf_dataset(filenames, ngram_width, window_size, batch_size, shuffle=False):\n    \"\"\" Generate batched data from a list of files speficied \"\"\"\n    # Read the data found in the documents\n    documents = []\n    for f in filenames:\n        doc = tf.io.read_file(f)\n        doc = tf.strings.ngrams(    # Generate ngrams from the string\n            tf.strings.bytes_split(\n            # Create a list of chars from a string\n                tf.strings.regex_replace(\n                # Replace new lines with space\n                    tf.strings.lower(    # Convert string to lower case\n                        doc\n                    ), \"\\n\", \" \"\n                )\n            ),\n            ngram_width, separator=''\n        )\n        documents.append(doc.numpy().tolist())\n\n    # documents is a list of list of strings, where each string is a story\n    # From that we generate a ragged tensor\n    documents = tf.ragged.constant(documents)\n    # Create a dataset where each row in the ragged tensor would be a \n    # sample\n    doc_dataset = tf.data.Dataset.from_tensor_slices(documents)\n    # We need to perform a quick transformation - tf.strings.ngrams \n    # would generate all the ngrams (e.g. abcd -> ab, bc, cd) with\n    # overlap, however for our data we do not need the overlap, so we need\n    # to skip the overlapping ngrams\n    # The following line does that\n    doc_dataset = doc_dataset.map(lambda x: x[::ngram_width])\n    # Here we are using a window function to generate windows from text\n    # For a text sequence with window_size 3 and shift 1 you get\n    # e.g. ab, cd, ef, gh, ij, ... -> [ab, cd, ef], [cd, ef, gh], [ef, \n    # gh, ij], ...\n    # each of these windows is a single training sequence for our model\n    doc_dataset = doc_dataset.flat_map(\n        lambda x: tf.data.Dataset.from_tensor_slices(\n            x\n        ).window(\n            size=window_size+1, shift=int(window_size * 0.75)\n        ).flat_map(\n            lambda window: window.batch(window_size+1, \n            drop_remainder=True)\n        )\n    )\n\n    # From each windowed sequence we generate input and target tuple\n    # e.g. [ab, cd, ef] -> ([ab, cd], [cd, ef])\n    doc_dataset = doc_dataset.map(lambda x: (x[:-1], x[1:]))\n    # Batch the data\n    doc_dataset = doc_dataset.batch(batch_size=batch_size)\n    # Shuffle the data if required\n    doc_dataset = doc_dataset.shuffle(buffer_size=batch_size*10) if \n    shuffle else doc_dataset\n\n    # Return the data\n    return doc_dataset \n```", "```\ndoc = tf.io.read_file(f) \n```", "```\ndocuments = tf.ragged.constant(documents) \n```", "```\na = tf.ragged.constant([[1, 2, 3], [1,2], [1]]) \n```", "```\nb = tf.RaggedTensor.from_row_splits([1,2,3,4,5,6,7],\nrow_splits=[0, 3, 3, 6, 7]) \n```", "```\n<tf.RaggedTensor [[1, 2, 3], [], [4, 5, 6], [7]]> \n```", "```\n[4, None] \n```", "```\ndoc_dataset = tf.data.Dataset.from_tensor_slices(documents) \n```", "```\ndoc_dataset = doc_dataset.map(lambda x: x[::ngram_width]) \n```", "```\ndoc_dataset = doc_dataset.flat_map(\n    lambda x: tf.data.Dataset.from_tensor_slices(\n        x\n    ).window(\n        size=window_size+1, shift=int(window_size * 0.75)\n    ).flat_map(\n        lambda window: window.batch(window_size+1, \n        drop_remainder=True)\n    )\n) \n```", "```\ndoc_dataset = doc_dataset.map(lambda x: (x[:-1], x[1:])) \n```", "```\ndoc_dataset = doc_dataset.shuffle(buffer_size=batch_size*10) if shuffle else doc_dataset\ndoc_dataset = doc_dataset.batch(batch_size=batch_size) \n```", "```\nngram_length = 2\nbatch_size = 256\nwindow_size = 128\ntrain_ds = generate_tf_dataset(train_filenames, ngram_length, window_size, batch_size, shuffle=True)\nvalid_ds = generate_tf_dataset(valid_filenames, ngram_length, window_size, batch_size)\ntest_ds = generate_tf_dataset(test_filenames, ngram_length, window_size, batch_size) \n```", "```\nds = generate_tf_dataset(train_filenames, 2, window_size=10, batch_size=1).take(5)\nfor record in ds:\n        print(record[0].numpy(), '->', record[1].numpy()) \n```", "```\n[[b'th' b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ']] -> [[b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ' b'a ']]\n[[b' u' b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph']] -> [[b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph' b'er']]\n[[b' s' b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ']] -> [[b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ' b'fa']]\n… \n```", "```\nimport tensorflow.keras.layers as layers\nimport tensorflow.keras.models as models\n# The vectorization layer that will convert string bigrams to IDs\ntext_vectorizer = tf.keras.layers.TextVectorization(\n    max_tokens=n_vocab, standardize=None,\n    split=None, input_shape=(window_size,)\n) \n```", "```\ntext_vectorizer.adapt(train_ds) \n```", "```\ntext_vectorizer.get_vocabulary()[:10] \n```", "```\n['', '[UNK]', 'e ', 'he', ' t', 'th', 'd ', ' a', ', ', ' h'] \n```", "```\ntrain_ds = train_ds.map(lambda x, y: (x, text_vectorizer(y)))\nvalid_ds = valid_ds.map(lambda x, y: (x, text_vectorizer(y))) \n```", "```\nimport tensorflow.keras.backend as K\nK.clear_session()\nlm_model = models.Sequential([\n    text_vectorizer,\n    layers.Embedding(n_vocab+2, 96),\n    layers.LSTM(512, return_state=False, return_sequences=True),\n    layers.LSTM(256, return_state=False, return_sequences=True),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(n_vocab, activation='softmax')\n]) \n```", "```\nlm_model.summary() \n```", "```\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text_vectorization (TextVec  multiple                 0         \n torization)                                                     \n\n embedding (Embedding)       (None, 128, 96)           67872     \n\n lstm (LSTM)                 (None, 128, 512)          1247232   \n\n lstm_1 (LSTM)               (None, 128, 256)          787456    \n\n dense (Dense)               (None, 128, 1024)         263168    \n\n dropout (Dropout)           (None, 128, 1024)         0         \n\n dense_1 (Dense)             (None, 128, 705)          722625    \n\n=================================================================\nTotal params: 3,088,353\nTrainable params: 3,088,353\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\nclass PerplexityMetric(tf.keras.metrics.Mean):\n    def __init__(self, name='perplexity', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.cross_entropy = \n        tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=False, reduction='none')\n    def _calculate_perplexity(self, real, pred):\n\n        # The next 4 lines zero-out the padding from loss \n        # calculations, this follows the logic from: \n        # https://www.tensorflow.org/beta/tutorials/text/transformer#loss_\n        # and_metrics \n        loss_ = self.cross_entropy(real, pred)\n        # Calculating the perplexity steps: \n        step1 = K.mean(loss_, axis=-1)\n        perplexity = K.exp(step1)\n        return perplexity \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        perplexity = self._calculate_perplexity(y_true, y_pred)\n        super().update_state(perplexity) \n```", "```\nlm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]) \n```", "```\nlm_model.fit(train_ds, validation_data=valid_ds, epochs=60) \n```", "```\nlm_model.evaluate(test_ds) \n```", "```\n5/5 [==============================] - 0s 45ms/step - loss: 2.4742 - accuracy: 0.3968 - perplexity: 12.3155 \n```", "```\n# Define inputs to the model\ninp = tf.keras.layers.Input(dtype=tf.string, shape=(1,))\ninp_state_c_lstm = tf.keras.layers.Input(shape=(512,))\ninp_state_h_lstm = tf.keras.layers.Input(shape=(512,))\ninp_state_c_lstm_1 = tf.keras.layers.Input(shape=(256,))\ninp_state_h_lstm_1 = tf.keras.layers.Input(shape=(256,))\ntext_vectorized_out = lm_model.get_layer('text_vectorization')(inp)\n# Define embedding layer and output\nemb_layer = lm_model.get_layer('embedding')\nemb_out = emb_layer(text_vectorized_out)\n# Defining a LSTM layers and output\nlstm_layer = tf.keras.layers.LSTM(512, return_state=True, return_sequences=True)\nlstm_out, lstm_state_c, lstm_state_h = lstm_layer(emb_out, initial_state=[inp_state_c_lstm, inp_state_h_lstm])\nlstm_1_layer = tf.keras.layers.LSTM(256, return_state=True, return_sequences=True)\nlstm_1_out, lstm_1_state_c, lstm_1_state_h = lstm_1_layer(lstm_out, initial_state=[inp_state_c_lstm_1, inp_state_h_lstm_1])\n# Defining a Dense layer and output\ndense_out = lm_model.get_layer('dense')(lstm_1_out)\n# Defining the final Dense layer and output\nfinal_out = lm_model.get_layer('dense_1')(dense_out)\n# Copy the weights from the original model\nlstm_layer.set_weights(lm_model.get_layer('lstm').get_weights())\nlstm_1_layer.set_weights(lm_model.get_layer('lstm_1').get_weights())\n# Define final model\ninfer_model = tf.keras.models.Model(\n    inputs=[inp, inp_state_c_lstm, inp_state_h_lstm, \n    inp_state_c_lstm_1, inp_state_h_lstm_1], \n    outputs=[final_out, lstm_state_c, lstm_state_h, lstm_1_state_c, \n    lstm_1_state_h]) \n```", "```\ninp = tf.keras.layers.Input(dtype=tf.string, shape=(1,))\ninp_state_c_lstm = tf.keras.layers.Input(shape=(512,))\ninp_state_h_lstm = tf.keras.layers.Input(shape=(512,))\ninp_state_c_lstm_1 = tf.keras.layers.Input(shape=(256,))\ninp_state_h_lstm_1 = tf.keras.layers.Input(shape=(256,)) \n```", "```\ntext_vectorized_out = lm_model.get_layer('text_vectorization')(inp) \n```", "```\nemb_layer = lm_model.get_layer('embedding')\nemb_out = emb_layer(text_vectorized_out) \n```", "```\nlstm_layer = tf.keras.layers.LSTM(512, return_state=True, return_sequences=True)\nlstm_out, lstm_state_c, lstm_state_h = lstm_layer(emb_out, initial_state=[inp_state_c_lstm, inp_state_h_lstm]) \n```", "```\n# Defining a Dense layer and output\ndense_out = lm_model.get_layer('dense')(lstm_1_out)\n# Defining the final Dense layer and output\nfinal_out = lm_model.get_layer('dense_1')(dense_out) \n```", "```\nlstm_layer.set_weights(lm_model.get_layer('lstm').get_weights())\nlstm_1_layer.set_weights(lm_model.get_layer('lstm_1').get_weights()) \n```", "```\ninfer_model = tf.keras.models.Model(\n    inputs=[inp, inp_state_c_lstm, inp_state_h_lstm, \n    inp_state_c_lstm_1, inp_state_h_lstm_1], \n    outputs=[final_out, lstm_state_c, lstm_state_h, lstm_1_state_c, \n    lstm_1_state_h]) \n```", "```\ntext = [\"When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren ground\"]\nseq = [text[0][i:i+2] for i in range(0, len(text[0]), 2)]\n# build up model state using the given string\nprint(\"Making predictions from a {} element long input\".format(len(seq)))\nvocabulary = infer_model.get_layer(\"text_vectorization\").get_vocabulary()\nindex_word = dict(zip(range(len(vocabulary)), vocabulary))\n# Reset the state of the model initially\ninfer_model.reset_states()\n# Defining the initial state as all zeros\nstate_c = np.zeros(shape=(1,512))\nstate_h = np.zeros(shape=(1,512))\nstate_c_1 = np.zeros(shape=(1,256))\nstate_h_1 = np.zeros(shape=(1,256))\n# Recursively update the model by assigning new state to state\nfor c in seq:    \n    #print(c)\n    out, state_c, state_h, state_c_1, state_h_1 = infer_model.predict(\n        [np.array([[c]]), state_c, state_h, state_c_1, state_h_1]\n)\n# Get final prediction after feeding the input string\nwid = int(np.argmax(out[0],axis=-1).ravel())\nword = index_word[wid]\ntext.append(word)\n# Define first input to generate text recursively from\nx = np.array([[word]])\n# Code listing 10.7\nfor _ in range(500):\n\n    # Get the next output and state\n    out, state_c, state_h, state_c_1, state_h_1  = \n    infer_model.predict([x, state_c, state_h, state_c_1, state_h_1 ])\n\n    # Get the word id and the word from out\n    out_argsort = np.argsort(out[0], axis=-1).ravel()\n    wid = int(out_argsort[-1])\n    word = index_word[wid]\n\n    # If the word ends with space, we introduce a bit of randomness\n    # Essentially pick one of the top 3 outputs for that timestep \n    # depending on their likelihood\n    if word.endswith(' '):\n        if np.random.normal()>0.5:\n            width = 5\n            i = np.random.choice(list(range(-width,0)), \n            p=out_argsort[-width:]/out_argsort[-width:].sum())\n            wid = int(out_argsort[i])    \n            word = index_word[wid]\n\n    # Append the prediction\n    text.append(word)\n\n    # Recursively make the current prediction the next input\n    x = np.array([[word]])\n\n# Print the final output    \nprint('\\n')\nprint('='*60)\nprint(\"Final text: \")\nprint(''.join(text)) \n```", "```\n out, state_c, state_h, state_c_1, state_h_1  = \n    infer_model.predict([x, state_c, state_h, state_c_1, state_h_1 ]) \n```", "```\nif word.endswith(' '):\n        if np.random.normal()>0.5:\n            width = 5\n            i = np.random.choice(list(range(-width,0)), \n            p=out_argsort[-width:]/out_argsort[-width:].sum())\n            wid = int(out_argsort[i])    \n            word = index_word[wid] \n```", "```\nWhen adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren groundy the king's daughter and said, i will so the king's daughter angry this they were and said, \"i will so the king's daughter.  the king's daughter.' they were to the forest of the stork.  then the king's daughters, and they were to the forest of the stork, and, and then they were to the forest.  ... \n```", "```\ntext_vectorizer = tf.keras.layers.TextVectorization(\n    max_tokens=n_vocab, standardize=None,\n    split=None, input_shape=(window_size,)\n)\n# Train the model on existing data\ntext_vectorizer.adapt(train_ds)\nlm_gru_model = models.Sequential([\n    text_vectorizer,\n    layers.Embedding(n_vocab+2, 96),\n    layers.GRU(512, return_sequences=True),\n    layers.GRU(256, return_sequences=True),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(n_vocab, activation='softmax')\n]) \n```", "```\ntext_vectorizer = tf.keras.layers.TextVectorization(\n    max_tokens=n_vocab, standardize=None,\n    split=None, input_shape=(window_size,)\n)\n# Train the model on existing data\ntext_vectorizer.adapt(train_ds)\nlm_peephole_model = models.Sequential([\n    text_vectorizer,\n    layers.Embedding(n_vocab+2, 96),\n    layers.RNN(\n        tfa.rnn.PeepholeLSTMCell(512),\n        return_sequences=True\n    ),\n    layers.RNN(\n        tfa.rnn.PeepholeLSTMCell(256),\n        return_sequences=True\n    ),\n    layers.Dense(1024, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(n_vocab, activation='softmax')\n]) \n```", "```\ndef beam_one_step(model, input_, states): \n    \"\"\" Perform the model update and output for one step\"\"\"\n    out = model.predict([input_, *states])\n    output, new_states = out[0], out[1:]\n    return output, new_states \n```", "```\ndef beam_search(model, input_, states, beam_depth=5, beam_width=3):\n    \"\"\" Defines an outer wrapper for the computational function of \n    beam search \"\"\"\n    vocabulary = \n    infer_model.get_layer(\"text_vectorization\").get_vocabulary()\n    index_word = dict(zip(range(len(vocabulary)), vocabulary))\n    def recursive_fn(input_, states, sequence, log_prob, i):\n        \"\"\" This function performs actual recursive computation of the \n        long string\"\"\"\n\n        if i == beam_depth:\n            \"\"\" Base case: Terminate the beam search \"\"\"\n            results.append((list(sequence), states, np.exp(log_prob)))\n            return sequence, log_prob, states\n        else:\n            \"\"\" Recursive case: Keep computing the output using the \n            previous outputs\"\"\"\n            output, new_states = beam_one_step(model, input_, states)\n\n            # Get the top beam_width candidates for the given depth\n            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)\n            top_probs, top_ids = top_probs.numpy().ravel(), \n            top_ids.numpy().ravel()\n            # For each candidate compute the next prediction\n            for p, wid in zip(top_probs, top_ids):\n                new_log_prob = log_prob + np.log(p)\n\n                # we are going to penalize joint probability whenever \n                # the same symbol is repeating\n                if len(sequence)>0 and wid == sequence[-1]:\n                    new_log_prob = new_log_prob + np.log(1e-1)\n\n                sequence.append(wid)\n                _ = recursive_fn(np.array([[index_word[wid]]]), \n                new_states, sequence, new_log_prob, i+1)\n                sequence.pop()\n    results = []\n    sequence = []\n    log_prob = 0.0\n    recursive_fn(input_, states, sequence, log_prob, 0)\n    results = sorted(results, key=lambda x: x[2], reverse=True)\n    return results \n```", "```\nfor i in range(50):\n    print('.', end='')\n    # Get the results from beam search\n    result = beam_search(infer_model, x, states, 5, 5)\n\n    # Get one of the top 10 results based on their likelihood\n    n_probs = np.array([p for _,_,p in result[:10]])\n    p_j = np.random.choice(list(range(n_probs.size)), \n    p=n_probs/n_probs.sum())                    \n    best_beam_ids, states, _ = result[p_j]\n    x = np.array([[index_word[best_beam_ids[-1]]]])\n    text.extend([index_word[w] for w in best_beam_ids]) \n```", "```\nWhen adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren groundr, said the king's daughter went out of the king's son to the king's daughter, and then the king's daughter went into the world, and asked the hedgehog's daughter that the king was about to the forest, and there was on the window, and said, \"if you will give her that you have been and said, i will give him the king's daughter, but when she went to the king's sister, and when she was still before the window, and said to himself, and when he said to her father, and that he had nothing and said to hi \n```", "```\nWhen adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren groundr, and then this they were all the third began to be able to the forests, and they were.  the king's daughter was no one was about to the king's daughter to the forest of them to the stone.  then the king's daughter was, and then the king's daughter was nothing-eyes, and the king's daughter was still, and then that had there was about through the third, and the king's daughters was seems to the king's daughter to the forest of them to the stone for them to the forests, and that it was not been to be ables, and the king's daughter wanted to be and said, ... \n```"]