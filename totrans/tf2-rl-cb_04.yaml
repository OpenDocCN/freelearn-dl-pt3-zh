- en: '*Chapter 4*: Reinforcement Learning in the Real World – Building Cryptocurrency
    Trading Agents'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep reinforcement learning** (**deep RL**) agents have a lot of potential
    when it comes to solving challenging problems in the real world and a lot of opportunities
    exist. However, only a few successful stories of using deep RL agents in the real
    world beyond games exist due to the various challenges associated with real-world
    deployments of RL agents. This chapter contains recipes that will help you successfully
    develop RL agents for an interesting and rewarding real-world problem: **cryptocurrency
    trading**. The recipes in this chapter contain information on how to implement
    custom OpenAI Gym-compatible learning environments for cryptocurrency trading
    with both discrete and continuous-value action spaces. In addition, you will learn
    how to build and train RL agents for trading cryptocurrency. Trading learning
    environments will also be provided.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Building a Bitcoin trading RL platform using real market data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an Ethereum trading RL platform using price charts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an advanced cryptocurrency trading platform for RL agents
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a cryptocurrency trading bot using RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in the book has been extensively tested on Ubuntu 18.04 and Ubuntu
    20.04 and should work with later versions of Ubuntu if Python 3.6+ is available.
    With Python 3.6+ installed, along with the necessary Python packages listed at
    the start of each of recipe, the code should run fine on Windows and macOS X too.
    You should create and use a Python virtual environment named `tf2rl-cookbook`
    to install the packages and run the code in this book. Installing Miniconda or
    Anaconda for Python virtual environment management is recommended.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code for each recipe in each chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Building a Bitcoin trading RL platform using real market data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will help you build a cryptocurrency trading RL environment for
    your agents. This environment simulates a Bitcoin trading exchange based on real-world
    data from the Gemini cryptocurrency exchange. In this environment, your RL agent
    can place buy/sell/hold trades and get rewards based on the profit/loss it makes,
    starting with an initial cash balance in the agent's trading account.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, make sure you have the latest version. You will need
    to activate the `tf2rl-cookbook` Python/conda virtual environment. Make sure to
    update the environment so that it matches the latest conda environment specification
    file (`tfrl-cookbook.yml`) in this cookbook''s code repository. If the following
    `import` statements run without issues, you are ready to get started:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, let's begin!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to learn how to implement `CryptoTradingEnv`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by importing the necessary Python modules.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll also be using the `TradeVisualizer` class implemented in `trading_utils.py`.
    We''ll discuss this in more deail when we actually use it:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To make it easy to configure the cryptocurrency trading environment, we will
    set up an environment config dictionary. Notice that our cryptocurrency trading
    environment has been configured so that we can trade Bitcoin based on real data
    from the Gemini cryptocurrency exchange:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s begin our `CryptoTradingEnv` class definition:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We''ll be using a file object as our cryptocurrency exchange data source. We
    must make sure that the data source exists before loading/streaming the data into
    memory:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The opening balance in the Agent''s account is configured using `env_config`.
    Let''s initialize the opening account balance based on the configured value:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, let''s define the action and observation space for this cryptocurrency
    trading environment using the standard space type definitions provided by the
    OpenAI Gym library:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s define the trade order size that will be executed when the agent places
    a trade:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With that, we have successfully initialized the environment! Now, let''s move
    on and define the `step(…)` method. You will notice that we have simplified the
    implementation of the `step (…)` method for ease of understanding using two helper
    member methods: `self.execute_trade_action` and `self.get_observation`. We''ll
    define these helper member methods later, once we have finished implementing the
    basic RL Gym environment methods (`step`, `reset`, and `render`) . Now, let''s
    look at the implementation of the `step` method:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s define the `reset()` method, which will be executed at the start
    of every episode:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As the next step, we'll define the `render()` method, which will provide us
    with a view into the cryptocurrency trading environment so that we understand
    what's going on! This is where we will be using the `TradeVisualizer` class from
    the `trading_utils.py` file. `TradeVisualizer` helps us visualize the live account
    balance of the Agent as the Agent learns in the environment. The visualizer also
    provides a visual indication of the buy and sell trades that the Agent performs
    by performing actions in the environment. A sample screenshot of the output from
    the `render()` method has been provided here for your reference:![Figure 4.1 –
    A sample rendering of the CryptoTradingEnv environment ](img/B15074_04_01.jpg)
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we''ll implement a method that will close all the visualization windows
    once the training is complete:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we can implement the `execute_trade_action` method, which we used in the
    `step (…)` method earlier in Step 9\. We''ll split the implementation into three
    steps, one for each order type: Hold, Buy, and Sell. Let''s start with the Hold
    order type as that''s the simplest. You will see why in a bit!'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We actually need to implement one more intermediate step before we can move
    on and implement the Buy and Sell order execution logic. Here, we must determine
    the order type (buy versus sell) and then the price of the Bitcoin at the current
    simulated time:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we are ready to implement the logic for executing a Buy trade order, as
    follows:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s update the `trades` list with the latest buy trade:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The next step is to implement the logic for executing Sell trade orders:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To finish up our trade execution function, we need to add a couple of lines
    of code that will update the account value once the trade order has been executed:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With that, we have finished implementing a Bitcoin trading RL environment powered
    by real BTCUSD data from the Gemini cryptocurrency exchange! Let''s look at how
    we can easily create the environment and run a sample, rather than using a random
    agent in this environment with just six lines of code:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You should see the sample random agent acting in the `CryptoTradingEnv` environment.
    The `env.render()` function should produce a rendering that looks similar to the
    following:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – A rendering of the CryptoTradingEnv environment showing the
    agent''s current account balance and the buy/sell trade being executed ](img/B15074_04_02.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – A rendering of the CryptoTradingEnv environment showing the agent's
    current account balance and the buy/sell trade being executed
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how this all works.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented the `CryptoTradingEnv` function, which offers
    tabular observations of shape (6, horizon + 1), where the horizon can be configured
    through the `env_config` dictionary. The horizon parameter specifies the horizon
    of the duration of the time window (for example, 3 days) that the Agent is allowed
    to observe the cryptocurrency market data at every step before making a trade.
    Once the Agent takes one of the allowed discrete actions – 0(hold), 1(buy), or
    2(sell) – the appropriate trade is executed at the current exchange price of the
    cryptocurrency (Bitcoin) and the trading account balance is updated accordingly.
    The Agent will also receive a reward based on the profit (or loss) that's made
    through the trades from the start of the episode.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Building an Ethereum trading RL platform using price charts
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will teach you to implement an Ethereum cryptocurrency trading environment
    for RL Agents with visual observations. The Agent will observe a price chart with
    Open, High, Low, Close, and Volume information over a specified time period to
    take an action (Hold, Buy, or Sell). The objective of the Agent is to maximize
    its reward, which is the profit you would make if you deployed the Agent to trade
    in your account!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, make sure you have the latest version. You will need
    to activate the `tf2rl-cookbook` Python/conda virtual environment. Make sure that
    will update the environment so that it matches the latest conda environment specification
    file (`tfrl-cookbook.yml`), which can be found in this cookbook''s code repository.
    If the following `import` statements run without any issues, you are ready to
    get started:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How to do it…
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's follow the OpenAI Gym framework in order to implement our learning environment
    interface. We will add some logic that will simulate cryptocurrency trade execution
    and reward the agent appropriately since this will aid your learning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to complete your implementation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by configuring the environment using a dictionary:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s define the `CryptoTradingVisualEnv` class and load the settings from
    `env_config`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As the next step, based on the frequency configuration for the market data
    feed, let''s load the cryptocurrency exchange data from the input stream:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s initialize other environment class variables and define the state and
    action space:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s define the `reset` method in order to (re)initialize the environment
    class variables:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The key feature of this environment is that the Agent''s observations are images
    of the price chart, similar to the one you can see on a human trader''s computer
    screen. This chart contains flashy plots with red and green bars and candles!
    Let''s define the `get_observation` method in order to return an image of the
    charting screen:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we''ll implement the trade execution logic of the trading environment.
    The current price of the Ethereum cryptocurrency (in USD) must be extracted from
    the market data stream (a file, in this case):'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If the Agent decides to execute a buy order, we must calculate the number of
    Ethereum tokens/coins the Agent can buy in a single step and execute the "Buy"
    order at the simulated exchange:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Instead, if the Agent decides to sell, the following logic will execute the
    sell order:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s update the account balance to reflect the effect of the Buy/Sell trade:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We are now ready to implement the `step` method:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s implement a method that will render the current state as an image to
    the screen. This will help us understand what''s going on in the environment while
    the Agent is learning to trade:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'That completes our implementation! Let''s quickly check out the environment
    by using a random agent:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You should see the sample random agent acting in `CryptoTradinVisualEnv`, wherein
    the agent receives visual/image observations similar to the one shown here:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Sample observation sent to the learning Agent ](img/B15074_04_03.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Sample observation sent to the learning Agent
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: That's it for this recipe!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a visual Ethereum cryptocurrency trading environment
    that provides images as input to the agents. The images contain charting information,
    such as Open, High, Low, Close, and Volume data. This chart looks like what a
    human trader's screen will look like and informs the agent about the current market
    signals.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们实现了一个可视化的以太坊加密货币交易环境，提供图像作为代理的输入。图像包含了图表信息，如开盘、最高、最低、收盘和成交量数据。这个图表看起来就像一个人类交易员的屏幕，向代理提供当前市场的信号。
- en: Building an advanced cryptocurrency trading platform for RL agents
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个高级的加密货币交易平台为RL代理
- en: Instead of allowing the Agent to only take discrete actions, such as buying/selling/holding
    a pre-set amount of Bitcoin or Ethereum tokens, what if we allowed the Agent to
    decide how many crypto coins/tokens it would like to buy or sell? That is exactly
    what this recipe will allow you to create in the form of a `CryptoTradingVisualContinuousEnv`
    RL environment.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不让代理只采取离散的动作，比如购买/卖出/持有预设数量的比特币或以太坊代币，而是让代理决定它想买或卖多少加密货币/代币呢？这正是这个食谱所要让你实现的功能，创建一个`CryptoTradingVisualContinuousEnv`的RL环境。
- en: Getting ready
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To complete this recipe, you need to ensure you have the latest version. You
    will need to activate the `tf2rl-cookbook` Python/conda virtual environment. Make
    sure that you update the environment so that it matches the latest conda environment
    specification file (`tfrl-cookbook.yml`), which can be found in this cookbook''s
    code repository. If the following `import` statements run without any issues,
    you are ready to get started:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个方案，你需要确保你拥有最新版本的内容。你需要激活`tf2rl-cookbook` Python/conda 虚拟环境。确保你更新环境，以便它符合最新的
    conda 环境规范文件（`tfrl-cookbook.yml`），该文件可以在这个食谱的代码库中找到。如果以下的`import`语句没有任何问题地运行，那么你就可以开始了：
- en: '[PRE33]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How to do it…
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 怎么做…
- en: This is going to be a complex environment as it uses high-dimensional images
    as observations and allows for continuous, real-value actions to be performed.
    However, you are likely familiar with the components of this recipe due to having
    experience implementing the previous recipes in this chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个复杂的环境，因为它使用高维图像作为观察输入，并允许执行连续的真实值动作。不过，由于你在本章中已经实现了前面几个食谱的经验，你很可能已经熟悉这个食谱的各个组成部分。
- en: 'Let''s get started:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'First, we must define the configuration parameters that are allowed for this
    environment:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要定义该环境允许的配置参数：
- en: '[PRE34]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s jump right into the definition of the learning environment class:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们直接进入学习环境类的定义：
- en: '[PRE35]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This step is straightforward as we simply load the market data into memory
    from the input source:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一步很直接，因为我们只需要将市场数据从输入源加载到内存中：
- en: '[PRE36]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, let''s define the continuous action space and the observation space of
    the environment:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义环境的连续动作空间和观察空间：
- en: '[PRE37]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let''s define the outline of the `step` method for the environment. We''ll
    complete the helper method implementations in the following steps:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义环境中`step`方法的大致框架。接下来的步骤中我们将完成帮助方法的实现：
- en: '[PRE38]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The first helper method is the `execute_trade_action` method. The implementation
    in the next few steps should be straightforward, given that the previous recipes
    also implemented the logic behind buying and selling cryptocurrency at an exchange
    rate:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个帮助方法是`execute_trade_action`方法。接下来的几步实现应该很简单，因为前面几个食谱已经实现了在交易所按汇率买卖加密货币的逻辑：
- en: '[PRE39]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A Buy order at the exchange can be simulated as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过如下方式模拟交易所中的买入订单：
- en: '[PRE40]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Similarly, a Sell order can be simulated in the following manner:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样地，卖出订单可以通过以下方式模拟：
- en: '[PRE41]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Once the Buy/Sell order has been executed, the account balance needs to be
    updated:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦买入/卖出订单执行完毕，账户余额需要更新：
- en: '[PRE42]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To test `CryptoTradingVisualcontinuousEnv`, you can use the following lines
    of code for the `__main__` function:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试`CryptoTradingVisualcontinuousEnv`，你可以使用以下代码行来进行`__main__`函数的测试：
- en: '[PRE43]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works…
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: '`CryptoTradingVisualcontinuousEnv` provides an RL environment with a trader
    screen-like image as the observation and provides a continuous, real-valued action
    space for the Agents to act in. The actions in this environment are one-dimensional,
    continuous, and real-valued and the magnitude indicates the fraction amount of
    the crypto coins/tokens. If the action has a positive sign (0 to 1), it''s interpreted
    as a Buy order, while if the action has a negative sign (-1 to 0), it''s interpreted
    as a Sell order. The fraction amount is converted into a number of allowable coins
    that can be bought or sold based on the balance in the trading account.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Training a cryptocurrency trading bot using RL
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The soft actor-critic Agent is one of the most popular and state-of-the-art
    RL Agents available and is based on an off-policy, maximum entropy-based deep
    RL algorithm. This recipe provides all the ingredients you will need to build
    a soft actor-critic Agent from scratch using TensorFlow 2.x and train it for cryptocurrency
    (Bitcoin, Ethereum, and so on) trading using real data from the Gemini cryptocurrency
    exchange.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, make sure you have the latest version. You will need
    to activate the `tf2rl-cookbook` Python/conda virtual environment. Make sure that
    you update the environment so that it matches the latest conda environment specification
    file (`tfrl-cookbook.yml`), which can be found in this cookbook''s code repository.
    If the following `import` statements run without any issues, you are ready to
    get started:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: How to do it…
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will guide you through the step-by-step process of implementing
    the SAC Agent. It will also help you train the agent in the cryptocurrency trading
    environments so that you can automate your profit-making machine!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s gear up and begin the implementation:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'SAC is an actor-critic Agent, so it has both the actor and the critic components.
    Let''s begin by defining our actor neural network using TensorFlow 2.x:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, let''s define the critic neural network:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Given the current model weights and the target model weights, let''s implement
    a quick function that will slowly update the target weights using `tau` as the
    averaging factor. This is like the Polyak averaging step:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We are now ready to initialize our SAC Agent class:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As the next step, we''ll initialize the actor network and print a summary of
    the actor neural network:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we''ll define the two critic networks and print the summary of the critic
    neural network as well:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let''s initialize the `alpha` temperature parameter and the target entropy:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We''ll also initialize the other hyperparameters of SAC:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'That completes the `__init__` method of the SAC agent. Next, we''ll implement
    a method that will (pre)process the action that''s taken:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We are now ready to implement the `act` method in order to generate the SAC
    agent''s action, given a state:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In order to save experiences to the Replay memory, let''s implement the `remember`
    function:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, let''s begin implementing the experience replay process. We''ll start
    by initializing the replay method. We''ll complete the implementation of the replay
    method in the upcoming steps:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s start a persistent `GradientTape` function and begin accumulating gradients.
    We''ll do this by processing the actions and obtaining the next set of actions
    and log probabilities:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'With that, we can now compute the losses of the two critic networks:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The current state-action and log probabilities, as prescribed by the actor,
    can be computed as follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We can now compute the actor loss and apply gradients to the critic:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Similarly, we can compute and apply the actor''s gradients:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now, let''s log the summaries to TensorBoard:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'That completes our experience replay method. Now, we can move on to the `train`
    method''s implementation. Let''s begin by initializing the `train` method. We
    will complete the implementation of this method in the following steps:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, we are ready to start the main training loop. First, let''s handle the
    end of episode case:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'For every step into the environment, the following steps will need to be executed
    for the SAC agent to learn:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'With the agent updates taken care of, we can now log some more useful information
    to TensorBoard:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'As the last step in our train method implementation, we can save the actor
    and critic models to facilitate resuming our training or reloading from a checkpoint:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now, we''ll actually implement the `save_model` method we referenced previously:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s quickly implement a method that will load the actor and critic states
    from the saved model so that we can restore/resume from a previously saved checkpoint
    when needed:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'To run the SAC agent in "test" mode, we can implement a helper method:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'That completes our SAC agent implementation. We are now ready to train the
    SAC agent in `CryptoTradingContinuousEnv`:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: How it works…
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SAC is a powerful RL algorithm and has proven to be effective across a variety
    of RL simulation environments. SAC maximizes the entropy of the agent''s policy,
    in addition to optimizing for the maximum episodic rewards. You can watch the
    progress of the agent as it learns to trade using the TensorBoard since this recipe
    includes code for logging the agent''s progress along the way. You can launch
    TensorBoard using the following command:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The preceding command will launch TensorBoard. You can access it with your
    browser at the default address of `http://localhost:6006`. A sample TensorBoard
    screenshot has been provided here for reference:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – A screenshot of TensorBoard showing the SAC agent''s training
    progress in CryptoTradingContinuousEnv ](img/B15074_04_04.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – A screenshot of TensorBoard showing the SAC agent's training progress
    in CryptoTradingContinuousEnv
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this recipe and this chapter. Happy training!
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
