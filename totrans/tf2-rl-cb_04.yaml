- en: '*Chapter 4*: Reinforcement Learning in the Real World – Building Cryptocurrency
    Trading Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep reinforcement learning** (**deep RL**) agents have a lot of potential
    when it comes to solving challenging problems in the real world and a lot of opportunities
    exist. However, only a few successful stories of using deep RL agents in the real
    world beyond games exist due to the various challenges associated with real-world
    deployments of RL agents. This chapter contains recipes that will help you successfully
    develop RL agents for an interesting and rewarding real-world problem: **cryptocurrency
    trading**. The recipes in this chapter contain information on how to implement
    custom OpenAI Gym-compatible learning environments for cryptocurrency trading
    with both discrete and continuous-value action spaces. In addition, you will learn
    how to build and train RL agents for trading cryptocurrency. Trading learning
    environments will also be provided.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a Bitcoin trading RL platform using real market data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an Ethereum trading RL platform using price charts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an advanced cryptocurrency trading platform for RL agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a cryptocurrency trading bot using RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in the book has been extensively tested on Ubuntu 18.04 and Ubuntu
    20.04 and should work with later versions of Ubuntu if Python 3.6+ is available.
    With Python 3.6+ installed, along with the necessary Python packages listed at
    the start of each of recipe, the code should run fine on Windows and macOS X too.
    You should create and use a Python virtual environment named `tf2rl-cookbook`
    to install the packages and run the code in this book. Installing Miniconda or
    Anaconda for Python virtual environment management is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code for each recipe in each chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Building a Bitcoin trading RL platform using real market data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will help you build a cryptocurrency trading RL environment for
    your agents. This environment simulates a Bitcoin trading exchange based on real-world
    data from the Gemini cryptocurrency exchange. In this environment, your RL agent
    can place buy/sell/hold trades and get rewards based on the profit/loss it makes,
    starting with an initial cash balance in the agent's trading account.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, make sure you have the latest version. You will need
    to activate the `tf2rl-cookbook` Python/conda virtual environment. Make sure to
    update the environment so that it matches the latest conda environment specification
    file (`tfrl-cookbook.yml`) in this cookbook''s code repository. If the following
    `import` statements run without issues, you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to learn how to implement `CryptoTradingEnv`:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by importing the necessary Python modules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll also be using the `TradeVisualizer` class implemented in `trading_utils.py`.
    We''ll discuss this in more deail when we actually use it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To make it easy to configure the cryptocurrency trading environment, we will
    set up an environment config dictionary. Notice that our cryptocurrency trading
    environment has been configured so that we can trade Bitcoin based on real data
    from the Gemini cryptocurrency exchange:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s begin our `CryptoTradingEnv` class definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll be using a file object as our cryptocurrency exchange data source. We
    must make sure that the data source exists before loading/streaming the data into
    memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The opening balance in the Agent''s account is configured using `env_config`.
    Let''s initialize the opening account balance based on the configured value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s define the action and observation space for this cryptocurrency
    trading environment using the standard space type definitions provided by the
    OpenAI Gym library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the trade order size that will be executed when the agent places
    a trade:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we have successfully initialized the environment! Now, let''s move
    on and define the `step(…)` method. You will notice that we have simplified the
    implementation of the `step (…)` method for ease of understanding using two helper
    member methods: `self.execute_trade_action` and `self.get_observation`. We''ll
    define these helper member methods later, once we have finished implementing the
    basic RL Gym environment methods (`step`, `reset`, and `render`) . Now, let''s
    look at the implementation of the `step` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define the `reset()` method, which will be executed at the start
    of every episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As the next step, we'll define the `render()` method, which will provide us
    with a view into the cryptocurrency trading environment so that we understand
    what's going on! This is where we will be using the `TradeVisualizer` class from
    the `trading_utils.py` file. `TradeVisualizer` helps us visualize the live account
    balance of the Agent as the Agent learns in the environment. The visualizer also
    provides a visual indication of the buy and sell trades that the Agent performs
    by performing actions in the environment. A sample screenshot of the output from
    the `render()` method has been provided here for your reference:![Figure 4.1 –
    A sample rendering of the CryptoTradingEnv environment ](img/B15074_04_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll implement a method that will close all the visualization windows
    once the training is complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can implement the `execute_trade_action` method, which we used in the
    `step (…)` method earlier in Step 9\. We''ll split the implementation into three
    steps, one for each order type: Hold, Buy, and Sell. Let''s start with the Hold
    order type as that''s the simplest. You will see why in a bit!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We actually need to implement one more intermediate step before we can move
    on and implement the Buy and Sell order execution logic. Here, we must determine
    the order type (buy versus sell) and then the price of the Bitcoin at the current
    simulated time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to implement the logic for executing a Buy trade order, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s update the `trades` list with the latest buy trade:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to implement the logic for executing Sell trade orders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To finish up our trade execution function, we need to add a couple of lines
    of code that will update the account value once the trade order has been executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we have finished implementing a Bitcoin trading RL environment powered
    by real BTCUSD data from the Gemini cryptocurrency exchange! Let''s look at how
    we can easily create the environment and run a sample, rather than using a random
    agent in this environment with just six lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the sample random agent acting in the `CryptoTradingEnv` environment.
    The `env.render()` function should produce a rendering that looks similar to the
    following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – A rendering of the CryptoTradingEnv environment showing the
    agent''s current account balance and the buy/sell trade being executed ](img/B15074_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – A rendering of the CryptoTradingEnv environment showing the agent's
    current account balance and the buy/sell trade being executed
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how this all works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented the `CryptoTradingEnv` function, which offers
    tabular observations of shape (6, horizon + 1), where the horizon can be configured
    through the `env_config` dictionary. The horizon parameter specifies the horizon
    of the duration of the time window (for example, 3 days) that the Agent is allowed
    to observe the cryptocurrency market data at every step before making a trade.
    Once the Agent takes one of the allowed discrete actions – 0(hold), 1(buy), or
    2(sell) – the appropriate trade is executed at the current exchange price of the
    cryptocurrency (Bitcoin) and the trading account balance is updated accordingly.
    The Agent will also receive a reward based on the profit (or loss) that's made
    through the trades from the start of the episode.
  prefs: []
  type: TYPE_NORMAL
- en: Building an Ethereum trading RL platform using price charts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will teach you to implement an Ethereum cryptocurrency trading environment
    for RL Agents with visual observations. The Agent will observe a price chart with
    Open, High, Low, Close, and Volume information over a specified time period to
    take an action (Hold, Buy, or Sell). The objective of the Agent is to maximize
    its reward, which is the profit you would make if you deployed the Agent to trade
    in your account!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, make sure you have the latest version. You will need
    to activate the `tf2rl-cookbook` Python/conda virtual environment. Make sure that
    will update the environment so that it matches the latest conda environment specification
    file (`tfrl-cookbook.yml`), which can be found in this cookbook''s code repository.
    If the following `import` statements run without any issues, you are ready to
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's follow the OpenAI Gym framework in order to implement our learning environment
    interface. We will add some logic that will simulate cryptocurrency trade execution
    and reward the agent appropriately since this will aid your learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to complete your implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by configuring the environment using a dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the `CryptoTradingVisualEnv` class and load the settings from
    `env_config`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the next step, based on the frequency configuration for the market data
    feed, let''s load the cryptocurrency exchange data from the input stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s initialize other environment class variables and define the state and
    action space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the `reset` method in order to (re)initialize the environment
    class variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The key feature of this environment is that the Agent''s observations are images
    of the price chart, similar to the one you can see on a human trader''s computer
    screen. This chart contains flashy plots with red and green bars and candles!
    Let''s define the `get_observation` method in order to return an image of the
    charting screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll implement the trade execution logic of the trading environment.
    The current price of the Ethereum cryptocurrency (in USD) must be extracted from
    the market data stream (a file, in this case):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the Agent decides to execute a buy order, we must calculate the number of
    Ethereum tokens/coins the Agent can buy in a single step and execute the "Buy"
    order at the simulated exchange:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instead, if the Agent decides to sell, the following logic will execute the
    sell order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s update the account balance to reflect the effect of the Buy/Sell trade:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to implement the `step` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement a method that will render the current state as an image to
    the screen. This will help us understand what''s going on in the environment while
    the Agent is learning to trade:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our implementation! Let''s quickly check out the environment
    by using a random agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the sample random agent acting in `CryptoTradinVisualEnv`, wherein
    the agent receives visual/image observations similar to the one shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Sample observation sent to the learning Agent ](img/B15074_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Sample observation sent to the learning Agent
  prefs: []
  type: TYPE_NORMAL
- en: That's it for this recipe!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a visual Ethereum cryptocurrency trading environment
    that provides images as input to the agents. The images contain charting information,
    such as Open, High, Low, Close, and Volume data. This chart looks like what a
    human trader's screen will look like and informs the agent about the current market
    signals.
  prefs: []
  type: TYPE_NORMAL
- en: Building an advanced cryptocurrency trading platform for RL agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of allowing the Agent to only take discrete actions, such as buying/selling/holding
    a pre-set amount of Bitcoin or Ethereum tokens, what if we allowed the Agent to
    decide how many crypto coins/tokens it would like to buy or sell? That is exactly
    what this recipe will allow you to create in the form of a `CryptoTradingVisualContinuousEnv`
    RL environment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you need to ensure you have the latest version. You
    will need to activate the `tf2rl-cookbook` Python/conda virtual environment. Make
    sure that you update the environment so that it matches the latest conda environment
    specification file (`tfrl-cookbook.yml`), which can be found in this cookbook''s
    code repository. If the following `import` statements run without any issues,
    you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is going to be a complex environment as it uses high-dimensional images
    as observations and allows for continuous, real-value actions to be performed.
    However, you are likely familiar with the components of this recipe due to having
    experience implementing the previous recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must define the configuration parameters that are allowed for this
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s jump right into the definition of the learning environment class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This step is straightforward as we simply load the market data into memory
    from the input source:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define the continuous action space and the observation space of
    the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the outline of the `step` method for the environment. We''ll
    complete the helper method implementations in the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first helper method is the `execute_trade_action` method. The implementation
    in the next few steps should be straightforward, given that the previous recipes
    also implemented the logic behind buying and selling cryptocurrency at an exchange
    rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A Buy order at the exchange can be simulated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, a Sell order can be simulated in the following manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the Buy/Sell order has been executed, the account balance needs to be
    updated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To test `CryptoTradingVisualcontinuousEnv`, you can use the following lines
    of code for the `__main__` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`CryptoTradingVisualcontinuousEnv` provides an RL environment with a trader
    screen-like image as the observation and provides a continuous, real-valued action
    space for the Agents to act in. The actions in this environment are one-dimensional,
    continuous, and real-valued and the magnitude indicates the fraction amount of
    the crypto coins/tokens. If the action has a positive sign (0 to 1), it''s interpreted
    as a Buy order, while if the action has a negative sign (-1 to 0), it''s interpreted
    as a Sell order. The fraction amount is converted into a number of allowable coins
    that can be bought or sold based on the balance in the trading account.'
  prefs: []
  type: TYPE_NORMAL
- en: Training a cryptocurrency trading bot using RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The soft actor-critic Agent is one of the most popular and state-of-the-art
    RL Agents available and is based on an off-policy, maximum entropy-based deep
    RL algorithm. This recipe provides all the ingredients you will need to build
    a soft actor-critic Agent from scratch using TensorFlow 2.x and train it for cryptocurrency
    (Bitcoin, Ethereum, and so on) trading using real data from the Gemini cryptocurrency
    exchange.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, make sure you have the latest version. You will need
    to activate the `tf2rl-cookbook` Python/conda virtual environment. Make sure that
    you update the environment so that it matches the latest conda environment specification
    file (`tfrl-cookbook.yml`), which can be found in this cookbook''s code repository.
    If the following `import` statements run without any issues, you are ready to
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will guide you through the step-by-step process of implementing
    the SAC Agent. It will also help you train the agent in the cryptocurrency trading
    environments so that you can automate your profit-making machine!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s gear up and begin the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SAC is an actor-critic Agent, so it has both the actor and the critic components.
    Let''s begin by defining our actor neural network using TensorFlow 2.x:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s define the critic neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Given the current model weights and the target model weights, let''s implement
    a quick function that will slowly update the target weights using `tau` as the
    averaging factor. This is like the Polyak averaging step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to initialize our SAC Agent class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the next step, we''ll initialize the actor network and print a summary of
    the actor neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll define the two critic networks and print the summary of the critic
    neural network as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s initialize the `alpha` temperature parameter and the target entropy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll also initialize the other hyperparameters of SAC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes the `__init__` method of the SAC agent. Next, we''ll implement
    a method that will (pre)process the action that''s taken:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to implement the `act` method in order to generate the SAC
    agent''s action, given a state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to save experiences to the Replay memory, let''s implement the `remember`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s begin implementing the experience replay process. We''ll start
    by initializing the replay method. We''ll complete the implementation of the replay
    method in the upcoming steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s start a persistent `GradientTape` function and begin accumulating gradients.
    We''ll do this by processing the actions and obtaining the next set of actions
    and log probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we can now compute the losses of the two critic networks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The current state-action and log probabilities, as prescribed by the actor,
    can be computed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now compute the actor loss and apply gradients to the critic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, we can compute and apply the actor''s gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s log the summaries to TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our experience replay method. Now, we can move on to the `train`
    method''s implementation. Let''s begin by initializing the `train` method. We
    will complete the implementation of this method in the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to start the main training loop. First, let''s handle the
    end of episode case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For every step into the environment, the following steps will need to be executed
    for the SAC agent to learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the agent updates taken care of, we can now log some more useful information
    to TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the last step in our train method implementation, we can save the actor
    and critic models to facilitate resuming our training or reloading from a checkpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll actually implement the `save_model` method we referenced previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s quickly implement a method that will load the actor and critic states
    from the saved model so that we can restore/resume from a previously saved checkpoint
    when needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To run the SAC agent in "test" mode, we can implement a helper method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our SAC agent implementation. We are now ready to train the
    SAC agent in `CryptoTradingContinuousEnv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SAC is a powerful RL algorithm and has proven to be effective across a variety
    of RL simulation environments. SAC maximizes the entropy of the agent''s policy,
    in addition to optimizing for the maximum episodic rewards. You can watch the
    progress of the agent as it learns to trade using the TensorBoard since this recipe
    includes code for logging the agent''s progress along the way. You can launch
    TensorBoard using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will launch TensorBoard. You can access it with your
    browser at the default address of `http://localhost:6006`. A sample TensorBoard
    screenshot has been provided here for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – A screenshot of TensorBoard showing the SAC agent''s training
    progress in CryptoTradingContinuousEnv ](img/B15074_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – A screenshot of TensorBoard showing the SAC agent's training progress
    in CryptoTradingContinuousEnv
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this recipe and this chapter. Happy training!
  prefs: []
  type: TYPE_NORMAL
