<html><head></head><body>
  <div id="_idContainer417">
    <h1 class="chapterNumber">3</h1>
    <h1 id="_idParaDest-91" class="chapterTitle">The Bellman Equation and Dynamic Programming</h1>
    <p class="normal">In the previous chapter, we learned that in reinforcement learning our goal is to find the optimal policy. The optimal policy is the policy that selects the correct action in each state so that the agent can get the maximum return and achieve its goal. In this chapter, we'll learn about two interesting classic reinforcement learning algorithms called the value and policy iteration methods, which we can use to find the optimal policy.</p>
    <p class="normal">Before diving into the value and policy iteration methods directly, first, we will learn about the Bellman equation. The Bellman equation is ubiquitous in reinforcement learning and it is used for finding the optimal value and Q functions. We will understand what the Bellman equation is and how it finds the optimal value and Q functions.</p>
    <p class="normal">After understanding the Bellman equation, we will learn about two interesting dynamic programming methods called value and policy iterations, which use the Bellman equation to find the optimal policy. At the end of the chapter, we will learn how to solve the Frozen Lake problem by finding an optimal policy using the value and policy iteration methods.</p>
    <p class="normal">In this chapter, we will learn about the following topics:</p>
    <ul>
      <li class="bullet">The Bellman equation</li>
      <li class="bullet">The Bellman optimality equation</li>
      <li class="bullet">The relationship between the value and Q functions</li>
      <li class="bullet">Dynamic programming – value and policy iteration methods</li>
      <li class="bullet">Solving the Frozen Lake problem using value and policy iteration</li>
    </ul>
    <h1 id="_idParaDest-92" class="title">The Bellman equation</h1>
    <p class="normal">The Bellman equation, named after Richard Bellman, helps us solve the <strong class="keyword">Markov decision process</strong> (<strong class="keyword">MDP</strong>). When we say solve the MDP, we mean finding the optimal policy. </p>
    <p class="normal">As stated in the introduction of the chapter, the <a id="_idIndexMarker241"/>Bellman equation is ubiquitous in reinforcement learning and is widely used for finding the optimal value and Q functions recursively. Computing the optimal value and Q functions is very important because once we have the optimal value or optimal Q function, then we can use them to derive the optimal policy.</p>
    <p class="normal">In this section, we'll learn what exactly the Bellman equation is and how we can use it to find the optimal value and Q functions.</p>
    <h2 id="_idParaDest-93" class="title">The Bellman equation of the value function</h2>
    <p class="normal">The Bellman equation states that the value <a id="_idIndexMarker242"/>of a state can be obtained as a sum of the<a id="_idIndexMarker243"/> immediate reward and the discounted value of the next state. Say we perform an action <em class="italic">a</em> in state <em class="italic">s</em> and move to the next state <img src="../Images/B15558_03_001.png" alt="" style="height: 1.2em;"/> and obtain a reward <em class="italic">r</em>, then the Bellman equation of the value function can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_002.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">In the above equation, the following applies:</p>
    <ul>
      <li class="bullet"><img src="../Images/B15558_03_003.png" alt="" style="height: 1.2em;"/> implies the immediate reward obtained while performing an action <em class="italic">a</em> in state <em class="italic">s</em> and moving to the next state <img src="../Images/B15558_03_004.png" alt="" style="height: 1.2em;"/></li>
      <li class="bullet"><img src="../Images/B15558_03_005.png" alt="" style="height: 0.93em;"/> is the discount factor</li>
      <li class="bullet"><img src="../Images/B15558_03_006.png" alt="" style="height: 1.2em;"/> implies the value of the next state</li>
    </ul>
    <p class="normal">Let's understand the Bellman equation with an example. Say we generate a trajectory <img src="../Images/B15558_03_007.png" alt="" style="height: 0.84em;"/> using some policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.1: Trajectory</p>
    <p class="normal">Let's suppose we need to<a id="_idIndexMarker244"/> compute the value of state <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub>. According to the<a id="_idIndexMarker245"/> Bellman equation, the value of state <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_009.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">In the preceding equation, <img src="../Images/B15558_03_010.png" alt="" style="height: 1.11em;"/> implies the immediate reward we obtain while performing an action <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub> in state <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub> and moving to state <em class="italic">s</em><sub class="Subscript--PACKT-">3</sub>. From the trajectory, we can tell that the immediate reward <img src="../Images/B15558_03_011.png" alt="" style="height: 1.11em;"/> is <em class="italic">r</em><sub class="Subscript--PACKT-">2</sub>. And the term <img src="../Images/B15558_03_012.png" alt="" style="height: 1.11em;"/> is the discounted value of the next state.</p>
    <p class="normal">Thus, according to the Bellman equation, the value of state <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_013.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Thus, the Bellman equation of the value function can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_014.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where the superscript <img src="../Images/B15558_01_047.png" alt="" style="height: 0.84em;"/> implies that we are using policy <img src="../Images/B15558_03_016.png" alt="" style="height: 0.84em;"/>. The right-hand side term <img src="../Images/B15558_03_017.png" alt="" style="height: 1.2em;"/> is often <a id="_idIndexMarker246"/>called the<strong class="keyword"> Bellman backup</strong>.</p>
    <p class="normal">The preceding Bellman equation <a id="_idIndexMarker247"/>works only when we have a deterministic environment. Let's suppose our environment is stochastic, then in that case, when we perform an action <em class="italic">a</em> in state <em class="italic">s</em>, it is not guaranteed that our next state will<a id="_idIndexMarker248"/> always be <img src="../Images/B15558_03_018.png" alt="" style="height: 1.2em;"/>; it could be some other states too. For instance, look at the trajectory in <em class="italic">Figure 3.2</em>. </p>
    <p class="normal">As we can see, when we perform an action <em class="italic">a</em><sub class="Subscript--PACKT-">1</sub> in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, with a probability 0.7, we reach state s<sub class="Subscript--PACKT-">2</sub>, and with a probability 0.3, we reach state s<sub class="Subscript--PACKT-">3</sub>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.2: Transition probability of performing action <em class="italic">a</em><sub class="Subscript--PACKT-">1</sub> in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub></p>
    <p class="normal">Thus, when we perform action <em class="italic">a</em><sub class="Subscript--PACKT-">1</sub> in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, there is a 70% chance the next state will be <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub> and a 30% chance the next state will be <em class="italic">s</em><sub class="Subscript--PACKT-">3</sub>. We learned that the Bellman equation is a sum of immediate reward and the discounted value of the next state. But when our next state is not guaranteed due to the stochasticity present in the environment, how can we define our Bellman equation?</p>
    <p class="normal">In this case, we can slightly modify our Bellman equation with the expectations (the weighted average), that is, a sum of the Bellman backup multiplied by the corresponding transition probability of the next state:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_019.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">In the preceding equation, the following applies:</p>
    <ul>
      <li class="bullet"><img src="../Images/B15558_03_020.png" alt="" style="height: 1.2em;"/> denotes the<a id="_idIndexMarker249"/> transition probability of reaching <img src="../Images/B15558_03_021.png" alt="" style="height: 1.2em;"/> by performing an action <em class="italic">a</em> in state <em class="italic">s</em></li>
      <li class="bullet"><img src="../Images/B15558_03_022.png" alt="" style="height: 1.2em;"/> denotes the<a id="_idIndexMarker250"/> Bellman backup</li>
    </ul>
    <p class="normal">Let's understand this equation better by considering the same trajectory we just used. As we notice, when we perform an action <em class="italic">a</em><sub class="Subscript--PACKT-">1</sub> in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, we go to <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub> with a probability of 0.70 and <em class="italic">s</em><sub class="Subscript--PACKT-">3</sub> with a probability of 0.30. Thus, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_023.png" alt="" style="height: 1.2em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_03_024.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Thus, the Bellman equation of the value function including the stochasticity present in the environment using the expectation (weighted average) is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_019.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Okay, but what if our policy is a stochastic policy? We learned that with a stochastic policy, we select actions based on a probability distribution; that is, instead of performing the same action in a state, we <a id="_idIndexMarker251"/>select an action based on the probability distribution over the action space. Let's understand this with a different trajectory, shown in <em class="italic">Figure 3.3</em>. As we see, in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, with a probability of 0.8, we select action <em class="italic">a</em><sub class="Subscript--PACKT-">1</sub> and reach state <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub>, and with a <a id="_idIndexMarker252"/>probability of 0.2, we select action <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub> and reach state <em class="italic">s</em><sub class="Subscript--PACKT-">3</sub>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.3: Trajectory using a stochastic policy</p>
    <p class="normal">Thus, when we use a stochastic<a id="_idIndexMarker253"/> policy, our next state will not always be the same; it will be different states with some probability. Now, how can we define the Bellman equation including the stochastic policy?</p>
    <ul>
      <li class="bullet">We learned that to include the stochasticity present in the environment in the Bellman equation, we took the expectation (the weighted average), that is, a sum of the Bellman backup multiplied by the corresponding transition probability of the next state.</li>
      <li class="bullet">Similarly, to include the stochastic nature of the policy in the Bellman equation, we can use the expectation (the weighted average), that is, a sum of the Bellman backup multiplied by the corresponding probability of action.</li>
    </ul>
    <p class="normal">Thus, our final Bellman equation of the value function can be written as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_026.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">The preceding equation is also known as the <strong class="keyword">Bellman expectation equation</strong> of the value function. We can also express the<a id="_idIndexMarker254"/> above equation in expectation form. Let's recollect the definition of expectation:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_027.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">In equation (1), <img src="../Images/B15558_03_028.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_03_029.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_03_030.png" alt="" style="height: 1.11em;"/> which denote the probability of the stochastic environment <a id="_idIndexMarker255"/>and stochastic policy, respectively.</p>
    <p class="normal">Thus, we can write the Bellman equation <a id="_idIndexMarker256"/>of the value function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_031.png" alt="" style="height: 2.78em;"/></figure>
    <h2 id="_idParaDest-94" class="title">The Bellman equation of the Q function</h2>
    <p class="normal">Now, let's learn how to <a id="_idIndexMarker257"/>compute the Bellman equation of the state-action value function, that is, the Q function. The Bellman equation of the Q function is very similar to the Bellman equation of the value function except for a small difference. Similar to the Bellman equation of the value function, the Bellman equation of the Q function states that the Q value of a state-action pair can be obtained as a sum of the immediate reward and the discounted Q value of the next state-action pair:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_032.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">In the preceding equation, the following applies:</p>
    <ul>
      <li class="bullet"><img src="../Images/B15558_03_033.png" alt="" style="height: 1.2em;"/> implies the immediate reward obtained while performing an action <em class="italic">a</em> in state <em class="italic">s</em> and moving to the next state <img src="../Images/B15558_03_034.png" alt="" style="height: 1.2em;"/></li>
      <li class="bullet"><img src="../Images/B15558_03_035.png" alt="" style="height: 0.93em;"/> is the discount factor</li>
      <li class="bullet"><img src="../Images/B15558_03_036.png" alt="" style="height: 1.2em;"/> is the Q value of the next state-action pair</li>
    </ul>
    <p class="normal">Let's understand this with an example. Say we generate a trajectory <img src="../Images/B15558_03_037.png" alt="" style="height: 0.84em;"/> using some policy <img src="../Images/B15558_03_038.png" alt="" style="height: 0.84em;"/> as shown in <em class="italic">Figure 3.4</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.4: Trajectory</p>
    <p class="normal">Let's suppose we need to compute the<a id="_idIndexMarker258"/> Q value of a state-action pair (<em class="italic">s</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub>). Then, according to the Bellman equation, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_039.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">In the above equation, <em class="italic">R</em>(<em class="italic">s</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">a</em><sub class="Subscript--PACKT-">3</sub>) represents the immediate reward we obtain while performing an action <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub> in state <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub> and moving to state <em class="italic">s</em><sub class="Subscript--PACKT-">3</sub>. From the preceding trajectory, we can tell that the immediate reward <em class="italic">R</em>(<em class="italic">s</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">a</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">s</em><sub class="Subscript--PACKT-">3</sub>) is <em class="italic">r</em><sub class="Subscript--PACKT-">2</sub>. And the term <img src="../Images/B15558_03_040.png" alt="" style="height: 1.11em;"/> represents the discounted Q value of the next state-action pair. Thus:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_041.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Thus, the Bellman equation for the Q function can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_042.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where the superscript <img src="../Images/B15558_03_038.png" alt="" style="height: 0.84em;"/> implies that we are using the policy <img src="../Images/B15558_03_038.png" alt="" style="height: 0.84em;"/> and the right-hand side term <img src="../Images/B15558_03_045.png" alt="" style="height: 1.2em;"/> is the<strong class="keyword"> Bellman backup</strong>.</p>
    <p class="normal">Similar to what we<a id="_idIndexMarker259"/> learned in the Bellman<a id="_idIndexMarker260"/> equation of the value function, the preceding Bellman equation works only when we have a deterministic environment because<a id="_idIndexMarker261"/> in the stochastic environment our next state will not always be the same and it will be based on a probability distribution. Suppose we have a stochastic environment, then when we perform an action <em class="italic">a</em> in state <em class="italic">s</em>. It is not guaranteed that our next state will always be <img src="../Images/B15558_03_046.png" alt="" style="height: 1.2em;"/>; it could be some other states too with some probability.</p>
    <p class="normal">So, just like we did in the previous section, we can use the expectation (the weighted average), that is, a sum of the Bellman backup multiplied by their corresponding transition probability of the next state, and rewrite our Bellman equation of the Q function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_047.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Similarly, when we use a stochastic policy, our next state will not always be the same; it will be different states with some probability. So, to include the stochastic nature of the policy, we can rewrite our Bellman equation with the expectation (the weighted average), that is, a sum of Bellman backup multiplied by the corresponding probability of action, just like we did in the Bellman equation of the value function. Thus, the Bellman equation of the Q function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_048.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">But wait! There is a small change<a id="_idIndexMarker262"/> in the above equation. Why do we need to add the term <img src="../Images/B15558_03_049.png" alt="" style="height: 2.69em;"/> in the case of a Q function? Because in the value function <em class="italic">V</em>(<em class="italic">s</em>), we are given only a state <em class="italic">s</em> and we choose an action <em class="italic">a</em> based on the policy <img src="../Images/B15558_03_050.png" alt="" style="height: 0.84em;"/>. So, we added the term <img src="../Images/B15558_03_049.png" alt="" style="height: 2.69em;"/> to include the stochastic nature of the policy. But in the case<a id="_idIndexMarker263"/> of the Q function <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>), we will be given both state <em class="italic">s</em> and action <em class="italic">a</em>, so we don't need to add the term <img src="../Images/B15558_03_049.png" alt="" style="height: 2.69em;"/> in our equation since we are not selecting any action <em class="italic">a</em> based on the policy <img src="../Images/B15558_03_050.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">However, if you look at the above equation, we need to select action <img src="../Images/B15558_03_054.png" alt="" style="height: 1.2em;"/> based on the policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/> while computing the Q value of the next state-action pair <img src="../Images/B15558_03_056.png" alt="" style="height: 1.2em;"/> since <img src="../Images/B15558_03_057.png" alt="" style="height: 1.2em;"/> will not be given. So, we can just place the term <img src="../Images/B15558_03_058.png" alt="" style="height: 2.69em;"/> before the Q value of the next state-action pair. Thus, our final Bellman equation of the Q function can be written as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_059.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">Equation (3) is also known as the Bellman expectation equation of the Q function. We can also express the equation (3) in expectation form as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_060.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Now that we have understood<a id="_idIndexMarker264"/> what the Bellman expectation equation is, in the next <a id="_idIndexMarker265"/>section, we will learn about the Bellman optimality equation and explore how it is useful for finding the optimal Bellman value and Q functions.</p>
    <h2 id="_idParaDest-95" class="title">The Bellman optimality equation</h2>
    <p class="normal">The Bellman optimality equation gives the optimal <a id="_idIndexMarker266"/>Bellman value and Q functions. First, let's look at the optimal Bellman value function. We learned that the Bellman equation of the value function is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_061.png" alt="" style="height: 2.78em;"/></figure>
    <p class="normal">In the first chapter, we learned that the value function depends on the policy, that is, the value of the state varies based on the policy we choose. There can be many different value functions according to different policies. The optimal value function, <img src="../Images/B15558_03_062.png" alt="" style="height: 1.11em;"/> is the one that yields the maximum value compared to all the other value functions. Similarly, there can be many different Bellman value functions according to different policies. The optimal Bellman value function is the one that has the maximum value.</p>
    <p class="normal">Okay, how can we compute the optimal Bellman value function that has the maximum value?</p>
    <p class="normal">We can compute the optimal Bellman value function by selecting the action that gives the maximum value. But we don't know which action gives the maximum value, so, we compute the value of state using all possible actions, and then we select the maximum value as the value of the state. </p>
    <p class="normal">That is, instead of using some policy <img src="../Images/B15558_01_047.png" alt="" style="height: 0.84em;"/> to select the action, we compute the value of the state using all possible actions, and then we select the maximum value as the value of the state. Since we are not using any policy, we can remove the expectation over the policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/> and add the max over the action and express our optimal Bellman value function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_065.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">It's just the same as the<a id="_idIndexMarker267"/> Bellman equation, except here we are taking a maximum over all the possible actions instead of the expectation (weighted average) over the policy since we are only interested in the maximum value. Let's understand this with an example. Say we are in a state <em class="italic">s</em> and we have two possible actions in the state. Let the actions be 0 and 1. Then <img src="../Images/B15558_03_066.png" alt="" style="height: 1.11em;"/> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_067.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">As we can observe from the above equation, we compute the state value using all possible actions (0 and 1) and then select the maximum value as the value of the state.</p>
    <p class="normal">Now, let's look at the optimal Bellman Q function. We learned that the Bellman equation of the Q function is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_068.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Just like we learned with the optimal Bellman value function, instead of using the policy to select action <img src="../Images/B15558_03_069.png" alt="" style="height: 1.2em;"/> in the next state <img src="../Images/B15558_03_070.png" alt="" style="height: 1.2em;"/>, we choose all possible actions in that state <img src="../Images/B15558_03_004.png" alt="" style="height: 1.2em;"/> and compute the maximum Q value. It can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_072.png" alt="" style="height: 1.84em;"/></figure>
    <p class="normal">Let's understand this with an example. Say we are in a state <em class="italic">s</em> with an action <em class="italic">a</em>. We perform action <em class="italic">a</em> in state <em class="italic">s</em> and reach the next state <img src="../Images/B15558_03_073.png" alt="" style="height: 1.2em;"/>. We need to compute the Q value for the next state <img src="../Images/B15558_03_018.png" alt="" style="height: 1.2em;"/>. There can be many actions in state <img src="../Images/B15558_03_018.png" alt="" style="height: 1.2em;"/>. Let's say we have two actions 0 and 1 in state <img src="../Images/B15558_03_034.png" alt="" style="height: 1.2em;"/>. Then<a id="_idIndexMarker268"/> we can write the optimal Bellman Q function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_077.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">Thus, to summarize, the Bellman optimality equations of the value function and Q function are:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_078.png" alt="" style="height: 1.58em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_03_079.png" alt="" style="height: 1.84em;"/></figure>
    <p class="normal">We can also expand the expectation and rewrite the preceding Bellman optimality equations as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_080.png" alt="" style="height: 2.69em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_03_081.png" alt="" style="height: 2.69em;"/></figure>
    <h2 id="_idParaDest-96" class="title">The relationship between the value and Q functions</h2>
    <p class="normal">Let's take a little detour and recap the value and <a id="_idIndexMarker269"/>Q functions we covered in <em class="chapterRef">Chapter 1</em>, <em class="italic">Fundamentals of Reinforcement Learning</em>. We learned that the value of a state (value function) denotes the expected return starting from that state following a policy <img src="../Images/B15558_03_082.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_083.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">Similarly, the Q value of a state-action pair (Q function) represents the expected return starting from that state-action pair following a policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_085.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">We learned that the optimal value function gives the maximum state-value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_086.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">And the optimal Q function gives the maximum state-action value (Q value):</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_087.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Can we derive some relation between the optimal value function and optimal Q function? We know that the optimal value function has the maximum expected return when we start from a state <em class="italic">s</em> and the optimal Q function has the maximum expected return when we start from state <em class="italic">s</em> performing some action <em class="italic">a</em>. So, we can say that the optimal value function is the<a id="_idIndexMarker270"/> maximum of optimal Q value over all possible actions, and it can be expressed as follows (that is, we can derive V from Q):</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Alright, now let's get back to our Bellman equations. Before going ahead, let's just recap the Bellman equations:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Bellman expectation equation of the value function and Q function</strong>:<ul>
          <li class="bullet-l2"><img src="../Images/B15558_03_089.png" alt="" style="height: 2.69em;"/></li>
          <li class="bullet-l2"><img src="../Images/B15558_03_090.png" alt="" style="height: 2.69em;"/></li>
        </ul>
      </li>
      <li class="bullet"><strong class="keyword">Bellman </strong><strong class="keyword">optimality</strong><strong class="keyword"> equation of the value function and Q function</strong>:<ul>
          <li class="bullet-l2"><img src="../Images/B15558_03_091.png" alt="" style="height: 2.69em;"/></li>
          <li class="bullet-l2"><img src="../Images/B15558_03_092.png" alt="" style="height: 2.69em;"/></li>
        </ul>
      </li>
    </ul>
    <p class="normal">We learned that the optimal Bellman Q function is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_092.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">If we have an optimal value function <img src="../Images/B15558_03_094.png" alt="" style="height: 1.11em;"/>, then we can use it to derive the preceding optimal Bellman Q function, (that is, we can derive <em class="italic">Q</em> from <em class="italic">V</em>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_095.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">The preceding equation is one of the most useful identities in reinforcement learning, and we will see how it will help <a id="_idIndexMarker271"/>us in finding the optimal policy in the upcoming section.</p>
    <p class="normal">Thus, to summarize, we learned that we can derive <em class="italic">V</em> from <em class="italic">Q</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_096.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">And derive <em class="italic">Q</em> from <em class="italic">V</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_097.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Substituting equation (8) in equation (7), we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_080.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">As we can observe, we just obtained the optimal Bellman value function. Now that we understand the Bellman equation and the relationship between the value and the Q function, we can move on to the next section on how to make use of these equations to find the optimal policy.</p>
    <h1 id="_idParaDest-97" class="title">Dynamic programming</h1>
    <p class="normal"><strong class="keyword">Dynamic programming</strong> (<strong class="keyword">DP</strong>) is a technique for solving complex problems. In DP, instead of solving a complex problem as a whole, we break the problem into simple sub-problems, then for each<a id="_idIndexMarker272"/> sub-problem, we compute and store the solution. If the same subproblem occurs, we don't recompute; instead, we use the already computed solution. Thus, DP helps in drastically minimizing the computation time. It has its applications in a wide variety of fields including computer science, mathematics, bioinformatics, and so on.</p>
    <p class="normal">Now, we will learn about two important methods that use DP to find the optimal policy. The two methods are:</p>
    <ul>
      <li class="bullet">Value iteration</li>
      <li class="bullet">Policy iteration</li>
    </ul>
    <p class="normal">Note that dynamic programming is a model-based method meaning that it will help us to find the optimal policy only when the model dynamics (transition probability) of the environment are known. If we don't have the model dynamics, we cannot apply DP methods.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">The upcoming sections are explained with manual calculations, for a better understanding, follow along with a pen and paper.</p>
    </div>
    <h2 id="_idParaDest-98" class="title">Value iteration</h2>
    <p class="normal">In the value iteration method, we try<a id="_idIndexMarker273"/> to find the optimal policy. We learned that the optimal policy is the one that tells the agent to perform the correct action in each state. In order to find the optimal policy, first, we compute the optimal value function and once we have the optimal value function, we can use it to derive the optimal policy. Okay, how can we compute the optimal value function? We can use our optimal Bellman equation of the value function. We learned that, according to the Bellman optimality equation, the optimal value function can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_099.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">In the <em class="italic">The relationship between the value and Q functions</em> section, we learned that given the value function, we can derive the Q function:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_100.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Substituting (10) in (9), we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Thus, we can compute the optimal value function by just taking the maximum over the optimal Q function. So, in order to compute the value of a state, we compute the Q value for all state-action<a id="_idIndexMarker274"/> pairs. Then, we select the maximum Q value as the value of the state.</p>
    <p class="normal">Let's understand this with an example. Say we have two states, <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, and we have two possible actions in these states; let the actions be 0 and 1. First, we compute the Q value for all possible state-action pairs. <em class="italic">Table 3.1</em> shows the Q values for all possible state-action pairs:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_05.png" alt=""/></figure>
    <p class="packt_figref">Table 3.1: Q values of all possible state-action pairs</p>
    <p class="normal">Then, in each state, we select the maximum Q value as the optimal value of a state. Thus, the value of state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> is 3 and the value of state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> is 4. The optimal value of the state (value function) is shown <em class="italic">Table 3.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_06.png" alt=""/></figure>
    <p class="packt_figref">Table 3.2: Optimal state values</p>
    <p class="normal">Once we obtain the optimal value function, we can use it to extract the optimal policy.</p>
    <p class="normal">Now that we have a basic understanding of how the value iteration method finds the optimal value function, in the next section, we will go into detail and learn how exactly the value iteration method works and<a id="_idIndexMarker275"/> how it finds the optimal policy from the optimal value function.</p>
    <h3 id="_idParaDest-99" class="title">The value iteration algorithm</h3>
    <p class="normal">The algorithm of value iteration is<a id="_idIndexMarker276"/> given as follows:</p>
    <ol>
      <li class="numbered">Compute the<a id="_idIndexMarker277"/> optimal value function by taking the maximum over the Q function, that is, <img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></li>
      <li class="numbered">Extract the optimal policy from the computed optimal value function</li>
    </ol>
    <p class="normal">Let's go into detail and learn exactly how the above two steps work. For better understanding, let's perform the value iteration manually. Consider the small grid world environment shown in <em class="italic">Figure 3.5</em>. Let's say we are in state <strong class="keyword">A</strong> and our goal is to reach state <strong class="keyword">C</strong> without visiting the shaded state <strong class="keyword">B</strong>, and say we have two actions, 0—left/right, and 1—up/down:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.5: Grid world environment </p>
    <p class="normal">Can you think of what the optimal policy is here? The optimal policy here is the one that tells us to perform action 1 in state <strong class="keyword">A</strong> so that we can reach <strong class="keyword">C</strong> without visiting <strong class="keyword">B</strong>. Now we will see how to find this optimal policy using value iteration.</p>
    <p class="normal"><em class="italic">Table 3.3</em> shows the model dynamics of state <strong class="keyword">A</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_08.png" alt=""/></figure>
    <p class="packt_figref">Table 3.3: Model dynamics of state A</p>
    <h4 class="title">Step 1 – Compute the optimal value function</h4>
    <p class="normal">We can compute<a id="_idIndexMarker278"/> the optimal value function by computing the maximum over the Q function.</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">That is, we compute the Q value for all state-action pairs and then we select the maximum Q value as the value of a state.</p>
    <p class="normal">The Q value for a state <em class="italic">s</em> and action <em class="italic">a</em> can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_104.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">For notation simplicity, we can denote <img src="../Images/B15558_03_105.png" alt="" style="height: 1.2em;"/> by <img src="../Images/B15558_03_106.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_03_107.png" alt="" style="height: 1.2em;"/> by <img src="../Images/B15558_03_108.png" alt="" style="height: 1.2em;"/><strong class="keyword"> </strong>and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_109.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Thus, using the preceding <a id="_idIndexMarker279"/>equation, we can compute the Q function. If you look at the equation, to compute the Q function, we need the transition probability <img src="../Images/B15558_03_106.png" alt="" style="height: 1.2em;"/>, the reward function <img src="../Images/B15558_03_108.png" alt="" style="height: 1.2em;"/><strong class="keyword">,</strong><strong class="keyword"> </strong>and the value of the next state <img src="../Images/B15558_03_112.png" alt="" style="height: 1.2em;"/>. The model dynamics provide us with the transition probability <img src="../Images/B15558_03_106.png" alt="" style="height: 1.2em;"/> and the reward function <img src="../Images/B15558_03_108.png" alt="" style="height: 1.2em;"/>. But what about the value of the next state <img src="../Images/B15558_03_115.png" alt="" style="height: 1.2em;"/>? We don't know the value of any states yet. So, we will initialize the value function (state values) with random values or zeros as shown in <em class="italic">Table 3.4</em> and compute the Q function.</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_09.png" alt=""/></figure>
    <p class="packt_figref">Table 3.4: Initial value table</p>
    <p class="normal"><strong class="keyword">Iteration 1</strong>:</p>
    <p class="normal">Let's compute the Q value of state <strong class="keyword">A</strong>. We have two actions in state <strong class="keyword">A,</strong> which are 0 and 1. So, first let's compute the Q value for state <strong class="keyword">A</strong> and action 0 (note that we use the discount factor <img src="../Images/B15558_03_116.png" alt="" style="height: 1.11em;"/> throughout this section):</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_117.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Now, let's compute the Q value for state <strong class="keyword">A</strong> and action 1:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_118.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">After computing the Q values for both the<a id="_idIndexMarker280"/> actions in state <strong class="keyword">A</strong>, we can update the Q table as shown in <em class="italic">Table 3.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_10.png" alt=""/></figure>
    <p class="packt_figref">Table 3.5: Q table</p>
    <p class="normal">We learned that the optimal value of a state is just the max of the Q function. That is, <img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/>. By looking at <em class="italic">Table 3.5</em>, we can say that the value of state <strong class="keyword">A</strong>, <em class="italic">V</em>(<em class="italic">A</em>), is <em class="italic">Q</em>(<em class="italic">A</em>, 1) since <em class="italic">Q</em>(<em class="italic">A</em>, 1) has a higher value than <em class="italic">Q</em>(<em class="italic">A</em>, 0). Thus, <em class="italic">V</em>(<em class="italic">A</em>) = 0.9.</p>
    <p class="normal">We can update the value of state <strong class="keyword">A</strong> in our value table as shown in <em class="italic">Table 3.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_11.png" alt=""/></figure>
    <p class="packt_figref">Table 3.6: Updated value table</p>
    <p class="normal">Similarly, in order to compute the value of state <strong class="keyword">B</strong>, <em class="italic">V</em>(<em class="italic">B</em>), we compute the Q value of <em class="italic">Q</em>(<em class="italic">B</em>, 0) and <em class="italic">Q</em>(<em class="italic">B</em>, 1) and select the highest Q value as the value of state <strong class="keyword">B</strong>. In the same way, to compute the values of other states, we compute the Q value for all state-action pairs and select the maximum Q value<a id="_idIndexMarker281"/> as the value of a state. </p>
    <p class="normal">After computing the value of all the states, our updated value table may resemble <em class="italic">Table 3.7</em>. This is the result of the first iteration:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_12.png" alt=""/></figure>
    <p class="packt_figref">Table 3.7: Value table from iteration 1</p>
    <p class="normal">However, the value function (value table) shown in <em class="italic">Table 3.7</em> obtained as a result of the first iteration is not an optimal one. But why? We learned that the optimal value function is the maximum of the optimal Q function. That is, <img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/>. Thus to find the optimal value function, we need the optimal Q function. But the Q function may not be an optimal one in the first iteration as we computed the Q function based on the randomly initialized state values.</p>
    <p class="normal">As the following shows, when we started off computing the Q function, we used the randomly initialized state values.</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_31.png" alt=""/></figure>
    <p class="normal">So, what we can do is, in the<a id="_idIndexMarker282"/> next iteration, while computing the Q function, we can use the updated state values obtained as a result of the first iteration.</p>
    <p class="normal">That is, in the second iteration, to compute the value function, we compute the Q value of all state-action pairs and select the maximum Q value as the value of a state. In order to compute the Q value, we need to know the state values, in the first iteration, we used the randomly initialized state values. But in the second iteration, we use the updated state values (value table) obtained from the first iteration as the following shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_32.png" alt=""/></figure>
    <p class="normal"><strong class="keyword">Iteration 2</strong>:</p>
    <p class="normal">Let's compute the Q value of state <strong class="keyword">A</strong>. Remember that while computing the Q value, we use the updated state values from the previous iteration.</p>
    <p class="normal">First, let's compute the Q value of state <strong class="keyword">A</strong> and action 0:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_121.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Now, let's compute the Q value for state <strong class="keyword">A</strong> and action 1:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_122.png" alt="" style="height: 3.42em;"/></figure>
    <p class="normal">As we may observe, since the<a id="_idIndexMarker283"/> Q value of action 1 in state A is higher than action 0, the value of state A becomes 1.44. Similarly, we compute the value for all the states and update the value table. <em class="italic">Table 3.8</em> shows the updated value table: </p>
    <figure class="mediaobject"><img src="../Images/B15558_03_13.png" alt=""/></figure>
    <p class="packt_figref">Table 3.8: Value table from iteration 2</p>
    <p class="normal"><strong class="keyword">Iteration 3</strong>:</p>
    <p class="normal">We repeat the same steps we saw in the previous iteration and compute the value of all the states by selecting the maximum Q value. Remember that while computing the Q value, we use the updated state values (value table) obtained from the previous iteration. So, we use the updated state values from iteration 2 to compute the Q value.</p>
    <p class="normal"><em class="italic">Table 3.9</em> shows the updated state values obtained as a result of the third iteration:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_14.png" alt=""/></figure>
    <p class="packt_figref">Table 3.9: Value table from iteration 3</p>
    <p class="normal">So, we repeat these steps for many iterations until we find the optimal value function. But how can we understand whether we have found the optimal value function or not? When the value function (value table) does not change over iterations or when it changes by a very small fraction, then we can say that we have attained convergence, that is, we have found an optimal value function.</p>
    <p class="normal">Okay, how can we find <a id="_idIndexMarker284"/>out whether the value table is changing or not changing from the previous iteration? We can calculate the difference between the value table obtained from the previous iteration and the value table obtained from the current iteration. If the difference is very small—say, the difference is less than a very small threshold number—then we can say that we have attained convergence as there is not much change in the value function. </p>
    <p class="normal">For example, let's suppose <em class="italic">Table 3.10</em> shows the value table obtained as a result of <strong class="keyword">iteration 4</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_14.png"/></figure>
    <p class="packt_figref">Table 3.10: Value table from iteration 4</p>
    <p class="normal">As we can notice, the difference between the value table obtained as a result of iteration 4 and iteration 3 is very small. So, we can say that we have attained convergence and we take the value table obtained as a result of iteration 4 as our optimal value function. Please note that the above example is just for better understanding; in practice, we cannot attain convergence in just four iterations—it usually takes many iterations.</p>
    <p class="normal">Now that we have found the optimal value function, in the next step, we will use this optimal value function to extract an optimal policy. </p>
    <h4 class="title">Step 2 – Extract the optimal policy from the optimal value function obtained from step 1</h4>
    <p class="normal">As a result of <em class="italic">Step 1</em>, we obtained the optimal value function:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_15.png" alt=""/></figure>
    <p class="packt_figref">Table 3.11: Optimal value table (value function)</p>
    <p class="normal">Now, how can we extract the <a id="_idIndexMarker285"/>optimal policy from the obtained optimal value function?</p>
    <p class="normal">We generally use the Q function to compute the policy. We know that the Q function gives the Q value for every state-action pair. Once we have the Q values for all state-action pairs, we extract the policy by selecting the action that has the maximum Q value in each state. For example, consider the Q table in <em class="italic">Table 3.12</em>. It shows the Q values for all state-action pairs. Now we can extract the policy from the Q function (Q table) by selecting action 1 in the state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and action 0 in the state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> as they have the maximum Q value.</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_16.png" alt=""/></figure>
    <p class="packt_figref">Table 3.12: Q table</p>
    <p class="normal">Okay, now we compute the Q function using the optimal value function obtained from <em class="italic">Step 1</em>. Once we have the Q function, then we extract the policy by selecting the action that has the maximum Q value in each state. Since we are computing the Q function using the optimal value function, the policy extracted from the Q function will be the optimal policy. </p>
    <p class="normal">We learned that the Q function can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_123.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Now, while computing Q values, we use the optimal value function we obtained from <em class="italic">step 1</em>. After computing the Q function, we can extract the optimal policy by selecting the action that has the maximum Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_124.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">For instance, let's compute the<a id="_idIndexMarker286"/> Q value for all actions in state <strong class="keyword">A</strong> using the optimal value function. The Q value for action 0 in state <strong class="keyword">A</strong> is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_125.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">The Q value for action 1 in state <strong class="keyword">A</strong> is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_126.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Since <em class="italic">Q</em>(<em class="italic">A</em>, 1) is higher than <em class="italic">Q</em>(<em class="italic">A</em>, 0), our optimal policy will select action 1 as the optimal action in state <strong class="keyword">A</strong>. <em class="italic">Table 3.13</em> shows the Q table after computing the Q values for all state-action pairs using the optimal value function:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_17.png" alt=""/></figure>
    <p class="packt_figref">Table 3.13: Q table</p>
    <p class="normal">From this Q table, we pick the action in each state that has the maximum value as an optimal policy. Thus, our optimal policy would select action 1 in state <strong class="keyword">A</strong>, action 1 in state <strong class="keyword">B</strong>, and action 1 in state <strong class="keyword">C</strong>. </p>
    <p class="normal">Thus, according to our <a id="_idIndexMarker287"/>optimal policy, if we perform action 1 in state <strong class="keyword">A</strong>, we can reach state <strong class="keyword">C</strong> without visiting state <strong class="keyword">B</strong>.</p>
    <p class="normal">In this section, we learned how to compute the optimal policy using the value iteration method. In the next section, we will learn how to implement the value iteration method to compute the optimal policy in the Frozen Lake environment using the Gym toolkit.</p>
    <h3 id="_idParaDest-100" class="title">Solving the Frozen Lake problem with value iteration</h3>
    <p class="normal">In the previous <a id="_idIndexMarker288"/>chapter, we learned <a id="_idIndexMarker289"/>about the Frozen Lake<a id="_idIndexMarker290"/> environment. The Frozen Lake environment is shown in <em class="italic">Figure 3.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.6: Frozen Lake environment </p>
    <p class="normal">Let's recap the Frozen Lake environment a bit. In the Frozen Lake environment shown in <em class="italic">Figure 3.6</em>, the following applies:</p>
    <ul>
      <li class="bullet"><strong class="keyword">S</strong> implies the starting state</li>
      <li class="bullet"><strong class="keyword">F</strong> implies the frozen states</li>
      <li class="bullet"><strong class="keyword">H</strong> implies the hole states</li>
      <li class="bullet"><strong class="keyword">G</strong> implies the goal state</li>
    </ul>
    <p class="normal">We learned that in the Frozen Lake<a id="_idIndexMarker291"/> environment, our<a id="_idIndexMarker292"/> goal is to reach the goal state <strong class="keyword">G</strong> from the starting state <strong class="keyword">S</strong> without visiting the hole states <strong class="keyword">H</strong>. That is, while<a id="_idIndexMarker293"/> trying to reach the goal state <strong class="keyword">G</strong> from the starting state <strong class="keyword">S</strong>, if the agent visits the hole states <strong class="keyword">H</strong>, then it will fall into the hole and die as <em class="italic">Figure 3.7</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.7: Agent falling into the hole</p>
    <p class="normal">So, we want the agent to avoid the<a id="_idIndexMarker294"/> hole states <strong class="keyword">H</strong> to reach the<a id="_idIndexMarker295"/> goal state <strong class="keyword">G</strong> as shown in the following:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_20.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.8: Agent reaches the goal state</p>
    <p class="normal">How can we achieve this goal? That is, how can we reach state <strong class="keyword">G</strong> from <strong class="keyword">S</strong> without visiting <strong class="keyword">H</strong>? We learned that the optimal <a id="_idIndexMarker296"/>policy tells the agent to perform the correct action in each state. So, if we find the optimal policy, then we can reach state <strong class="keyword">G</strong> from <strong class="keyword">S</strong> without visiting state <strong class="keyword">H</strong>. Okay, how can we find the optimal policy? We can use the value iteration method we just learned to find the optimal policy.</p>
    <p class="normal">Remember that all our states (<strong class="keyword">S</strong> to <strong class="keyword">G</strong>) will be<a id="_idIndexMarker297"/> encoded from 0 to 16 and all four actions—<em class="italic">left</em>, <em class="italic">down</em>, <em class="italic">up</em>, <em class="italic">right</em>—will be encoded from 0 to 3 in the Gym toolkit.</p>
    <p class="normal">In this section, we will learn how to find the optimal policy using the value iteration method so that the agent can reach state <strong class="keyword">G</strong> from <strong class="keyword">S</strong> without visiting <strong class="keyword">H</strong>.</p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">Now, let's create the Frozen Lake environment using Gym:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'FrozenLake-v0'</span>)
</code></pre>
    <p class="normal">Let's look at the Frozen Lake environment using the <code class="Code-In-Text--PACKT-">render</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">env.render()
</code></pre>
    <p class="normal">The preceding code will display:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_21.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.9: Gym Frozen Lake environment</p>
    <p class="normal">As we can notice, our agent is in state <strong class="keyword">S</strong> and it has to reach state <strong class="keyword">G</strong> without visiting the <strong class="keyword">H</strong> states. So, let's learn how to compute the optimal policy using the value iteration method.</p>
    <p class="normal">In the value iteration <a id="_idIndexMarker298"/>method, we perform two steps:</p>
    <ol>
      <li class="numbered" value="1">Compute the optimal value function<a id="_idIndexMarker299"/><a id="_idIndexMarker300"/> by taking the maximum over the Q function, that is, <img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></li>
      <li class="numbered">Extract the optimal policy from the computed optimal value function</li>
    </ol>
    <p class="normal">First, let's learn how to <a id="_idIndexMarker301"/>compute the optimal value function, and then we will see how to extract the optimal policy from the computed optimal value function.</p>
    <h4 class="title">Computing the optimal value function</h4>
    <p class="normal">We will define a<a id="_idIndexMarker302"/> function called <code class="Code-In-Text--PACKT-">value_iteration</code> where we compute the optimal value function iteratively by taking the maximum over the Q function, that is, <img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/>. For better understanding, let's closely look at every line of the function, and then we'll look at the complete function at the end, which will provide more clarity. </p>
    <p class="normal">Define the <code class="Code-In-Text--PACKT-">value_iteration</code> function, which takes the environment as a parameter: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">value_iteration</span><span class="hljs-function">(</span><span class="hljs-params">env</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Set the number of iterations:</p>
    <pre class="programlisting code"><code class="hljs-code">    num_iterations = <span class="hljs-number">1000</span>
</code></pre>
    <p class="normal">Set the threshold number for checking the convergence of the value function:</p>
    <pre class="programlisting code"><code class="hljs-code">    threshold = <span class="hljs-number">1e-20</span>
</code></pre>
    <p class="normal">We also set the discount factor <img src="../Images/B15558_03_035.png" alt="" style="height: 0.93em;"/> to 1:</p>
    <pre class="programlisting code"><code class="hljs-code">    gamma = <span class="hljs-number">1.0</span>
</code></pre>
    <p class="normal">Now, we will initialize the value table by setting the value of all states to zero:</p>
    <pre class="programlisting code"><code class="hljs-code">    value_table = np.zeros(env.observation_space.n)
</code></pre>
    <p class="normal">For every iteration:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
</code></pre>
    <p class="normal">Update the value table, that is, we learned that on every iteration, we use the updated value table (state values) from the previous iteration:</p>
    <pre class="programlisting code"><code class="hljs-code">        updated_value_table = np.copy(value_table) 
</code></pre>
    <p class="normal">Now, we compute the<a id="_idIndexMarker303"/> value function (state value) by taking the maximum of the Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></figure>
    <p class="center">Where <img src="../Images/B15558_03_123.png" alt="" style="height: 2.69em;"/>.</p>
    <p class="normal">Thus, for each state, we compute the Q values of all the actions in the state and then we update the value of the state as the one that has the maximum Q value:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(env.observation_space.n):
</code></pre>
    <p class="normal">Compute the Q value of all the actions, <img src="../Images/B15558_03_123.png" alt="" style="height: 2.69em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">            Q_values = [sum([prob*(r + gamma * updated_value_table[s_])
                             <span class="hljs-keyword">for</span> prob, s_, r, _ <span class="hljs-keyword">in</span> env.P[s][a]]) 
                                   <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> range(env.action_space.n)] 
</code></pre>
    <p class="normal">Update the value of the state as a maximum Q value, <img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">            value_table[s] = max(Q_values) 
</code></pre>
    <p class="normal">After computing the value table, that is, the value of all the states, we check whether the difference between the value table obtained in the current iteration and the previous iteration is less than or equal to a threshold value. If the difference is less than the threshold, then we break the<a id="_idIndexMarker304"/> loop and return the value table as our optimal value function as the following code shows:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> (np.sum(np.fabs(updated_value_table - value_table)) &lt;= threshold):
             <span class="hljs-keyword">break</span>
    
    <span class="hljs-keyword">return</span> value_table
</code></pre>
    <p class="normal">The following complete snippet of the <code class="Code-In-Text--PACKT-">value_iteration</code> function is shown to provide more clarity:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">value_iteration</span><span class="hljs-function">(</span><span class="hljs-params">env</span><span class="hljs-function">):</span>
    
    num_iterations = <span class="hljs-number">1000</span>
    threshold = <span class="hljs-number">1e-20</span>
    gamma = <span class="hljs-number">1.0</span>    
   
    value_table = np.zeros(env.observation_space.n)
    
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
        updated_value_table = np.copy(value_table) 
             
        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(env.observation_space.n):
            
            Q_values = [sum([prob*(r + gamma * updated_value_table[s_])
                             <span class="hljs-keyword">for</span> prob, s_, r, _ <span class="hljs-keyword">in</span> env.P[s][a]])
                                   <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> range(env.action_space.n)]
                                        
            value_table[s] = max(Q_values)
                    
        
        <span class="hljs-keyword">if</span> (np.sum(np.fabs(updated_value_table - value_table)) &lt;= threshold):
             <span class="hljs-keyword">break</span>
    
    <span class="hljs-keyword">return</span> value_table
</code></pre>
    <p class="normal">Now that we have computed the optimal value function by taking the maximum of the Q values, let's see how to extract the <a id="_idIndexMarker305"/>optimal policy from the optimal value function. </p>
    <h4 class="title">Extracting the optimal policy from the optimal value function</h4>
    <p class="normal">In the previous step, we <a id="_idIndexMarker306"/>computed the optimal value function. Now, let's see how to extract the optimal policy from the computed optimal value function.</p>
    <p class="normal">First, we define a function called <code class="Code-In-Text--PACKT-">extract_policy</code>, which takes <code class="Code-In-Text--PACKT-">value_table</code> as a parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">extract_policy</span><span class="hljs-function">(</span><span class="hljs-params">value_table</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Set the discount factor <img src="../Images/B15558_03_005.png" alt="" style="height: 0.93em;"/> to 1:</p>
    <pre class="programlisting code"><code class="hljs-code">    gamma = <span class="hljs-number">1.0</span>
</code></pre>
    <p class="normal">First, we initialize the policy with zeros, that is, we set the actions for all the states to be zero:</p>
    <pre class="programlisting code"><code class="hljs-code">    policy = np.zeros(env.observation_space.n) 
</code></pre>
    <p class="normal">Now, we compute the Q function using the optimal value function obtained from the previous step. We learned that the Q function can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_123.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">After computing the Q function, we can extract the policy by selecting the action that has the maximum Q value. Since we are computing the Q function using the optimal value function, the policy extracted from the Q function will be the optimal policy.</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_124.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">As the following code shows, for each state, we compute the Q values for all the actions in the state and then we extract the policy by selecting the action that has the maximum Q value.</p>
    <p class="normal">For each state:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(env.observation_space.n):
</code></pre>
    <p class="normal">Compute the Q value of all the<a id="_idIndexMarker307"/> actions in the state, <img src="../Images/B15558_03_123.png" alt="" style="height: 2.69em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        Q_values = [sum([prob*(r + gamma * value_table[s_])
                             <span class="hljs-keyword">for</span> prob, s_, r, _ <span class="hljs-keyword">in</span> env.P[s][a]])
                                   <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> range(env.action_space.n)]
</code></pre>
    <p class="normal">Extract the policy by selecting the action that has the maximum Q value, <img src="../Images/B15558_03_124.png" alt="" style="height: 1.49em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        policy[s] = np.argmax(np.array(Q_values))
    
    <span class="hljs-keyword">return</span> policy
</code></pre>
    <p class="normal">The complete snippet of the <code class="Code-In-Text--PACKT-">extract_policy</code> function is shown here to give us more clarity: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">extract_policy</span><span class="hljs-function">(</span><span class="hljs-params">value_table</span><span class="hljs-function">):</span>
    gamma = <span class="hljs-number">1.0</span>
 
    policy = np.zeros(env.observation_space.n) 
   
    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(env.observation_space.n):
        
        Q_values = [sum([prob*(r + gamma * value_table[s_])
                             <span class="hljs-keyword">for</span> prob, s_, r, _ <span class="hljs-keyword">in</span> env.P[s][a]]) 
                                   <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> range(env.action_space.n)]
                
        policy[s] = np.argmax(np.array(Q_values)) 
    
    <span class="hljs-keyword">return</span> policy
</code></pre>
    <p class="normal">That's it! Now, we will see how to<a id="_idIndexMarker308"/> extract the optimal policy in our Frozen Lake environment.</p>
    <h4 class="title">Putting it all together </h4>
    <p class="normal">We learned that in the Frozen Lake environment, our goal is to find the optimal policy that selects the correct action in each state so that we can reach state <strong class="keyword">G</strong> from state <strong class="keyword">A</strong> without visiting the hole states.</p>
    <p class="normal">First, we compute the optimal value function using our <code class="Code-In-Text--PACKT-">value_iteration</code> function by passing our Frozen Lake environment as the parameter: </p>
    <pre class="programlisting code"><code class="hljs-code">optimal_value_function = value_iteration(env)
</code></pre>
    <p class="normal">Next, we extract the optimal policy from the optimal value function using our <code class="Code-In-Text--PACKT-">extract_policy</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">optimal_policy = extract_policy(optimal_value_function)
</code></pre>
    <p class="normal">We can print the obtained optimal policy:</p>
    <pre class="programlisting code"><code class="hljs-code">print(optimal_policy)
</code></pre>
    <p class="normal">The preceding code will print the following. As we can observe, our optimal policy tells us to perform the correct action in each state:</p>
    <pre class="programlisting code"><code class="hljs-code">[<span class="hljs-number">0.</span> <span class="hljs-number">3.</span> <span class="hljs-number">3.</span> <span class="hljs-number">3.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">3.</span> <span class="hljs-number">1.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">2.</span> <span class="hljs-number">1.</span> <span class="hljs-number">0.</span>]
</code></pre>
    <p class="normal">Now that we have learned what value iteration is and how to perform the value iteration method to compute the optimal policy in our Frozen Lake environment, in the next section, we will learn about another interesting method, called policy iteration.</p>
    <h2 id="_idParaDest-101" class="title">Policy iteration</h2>
    <p class="normal">In the value iteration method, first, we <a id="_idIndexMarker309"/>computed the optimal value function by taking the maximum over the Q function (Q values) iteratively. Once we found the optimal value function, we used it to extract the optimal policy. Whereas in policy iteration we try to compute the optimal value function using the policy iteratively, once we found the optimal value function, we can use it to extract the optimal policy.</p>
    <p class="normal">First, let's learn how to compute<a id="_idIndexMarker310"/> the value function using a policy. Say we have a policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/>, how can we compute the value function using the policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/>? Here, we can use our Bellman equation. We learned that according to the Bellman equation, we can compute the value function using the policy <img src="../Images/B15558_03_082.png" alt="" style="height: 0.84em;"/> as the following shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_142.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Let's suppose our policy is a deterministic policy, so we can remove the term <img src="../Images/B15558_03_143.png" alt="" style="height: 2.69em;"/> from the preceding equation since there is no stochasticity in the policy and rewrite our Bellman equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_144.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">For notation simplicity, we can denote <img src="../Images/B15558_03_145.png" alt="" style="height: 1.2em;"/> by <img src="../Images/B15558_03_106.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_03_147.png" alt="" style="height: 1.2em;"/> with <img src="../Images/B15558_03_108.png" alt="" style="height: 1.2em;"/><strong class="keyword"> </strong>and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_149.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Thus using the above equation we can compute the value function using a policy. Our goal is to find the optimal value function because once we have found the optimal value function, we can use it to extract the optimal policy.</p>
    <p class="normal">We will not be given any policy<a id="_idIndexMarker311"/> as an input. So, we will initialize the random policy and compute the value function using the random policy. Then we check if the computed value function is optimal or not. It will not be optimal since it is computed based on the random policy.</p>
    <p class="normal">So, we will extract a new policy from the computed value function, then we will use the extracted new policy to compute the new value function, and then we will check if the new value function is optimal. If it's optimal we will stop, else we repeat these steps for a series of iterations. For a better understanding, look at the following steps:</p>
    <p class="normal"><strong class="keyword">Iteration 1</strong>: Let <img src="../Images/B15558_03_150.png" alt="" style="height: 0.84em;"/> be the random policy. We use this random policy to compute the value function <img src="../Images/B15558_03_151.png" alt="" style="height: 1.11em;"/>. Our value function will not be optimal as it is computed based on the random policy. So, from <img src="../Images/B15558_03_152.png" alt="" style="height: 1.11em;"/>, we extract a new policy <img src="../Images/B15558_03_153.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal"><strong class="keyword">Iteration 2</strong>: Now, we use the new policy <img src="../Images/B15558_03_153.png" alt="" style="height: 0.84em;"/> derived from the previous iteration to compute the new value function <img src="../Images/B15558_03_155.png" alt="" style="height: 1.11em;"/>, then we check if <img src="../Images/B15558_03_156.png" alt="" style="height: 1.11em;"/> is optimal. If it is optimal, we stop, else from this value function <img src="../Images/B15558_03_157.png" alt="" style="height: 1.11em;"/>, we extract a new policy <img src="../Images/B15558_03_158.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal"><strong class="keyword">Iteration 3</strong>: Now, we use the new policy <img src="../Images/B15558_03_159.png" alt="" style="height: 0.84em;"/> derived from the previous iteration to compute the new value function <img src="../Images/B15558_03_160.png" alt="" style="height: 1.11em;"/>, then we check if <img src="../Images/B15558_03_161.png" alt="" style="height: 1.11em;"/> is optimal. If it is optimal, we stop, else from this value function <img src="../Images/B15558_03_162.png" alt="" style="height: 1.11em;"/>, we extract a new policy <img src="../Images/B15558_03_163.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">We repeat this process for many<a id="_idIndexMarker312"/> iterations until we find the optimal value function <img src="../Images/B15558_03_164.png" alt="" style="height: 1.29em;"/> as the following shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_165.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">The preceding step is called policy evaluation and improvement. Policy evaluation implies that at each step we evaluate the policy by checking if the value function computed using that policy is optimal. Policy improvement means that at each step we find the new improved policy to compute the optimal value function.</p>
    <p class="normal">Once we have found the optimal value function <img src="../Images/B15558_03_166.png" alt="" style="height: 1.29em;"/>, then it implies that we have also found the optimal policy. That is, if <img src="../Images/B15558_03_164.png" alt="" style="height: 1.29em;"/> is optimal, then the policy that is used to compute <img src="../Images/B15558_03_168.png" alt="" style="height: 1.29em;"/> will be an optimal policy.</p>
    <p class="normal">To get a better understanding of how policy iteration works, let's look into the below steps with pseudocode. In the first iteration, we will initialize a random policy and use it to compute the value function:</p>
    <pre class="programlisting code"><code class="hljs-code">policy = random_policy
value_function = compute_value_function(policy)
</code></pre>
    <p class="normal">Since we computed the value function using the random policy, the computed value function will not be optimal. So, we need to find a new policy with which we can compute the optimal value function.</p>
    <p class="normal">So, we extract a new policy from the value function computed using a random policy:</p>
    <pre class="programlisting code"><code class="hljs-code">new_policy = extract_policy(value_function)
</code></pre>
    <p class="normal">Now, we will use this new policy to compute the new value function:</p>
    <pre class="programlisting code"><code class="hljs-code">policy = new_policy 
value_function = compute_value_function(policy)
</code></pre>
    <p class="normal">If the new value function is optimal, we<a id="_idIndexMarker313"/> stop, else we repeat the preceding steps for a number of iterations until we find the optimal value function. The following pseudocode gives us a better understanding:</p>
    <pre class="programlisting code"><code class="hljs-code">policy = random_policy
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations): 
    value_function = compute_value_function(policy)
    new_policy = extract_policy(value_function)
    <span class="hljs-keyword">if</span> value_function = optimal:
        <span class="hljs-keyword">break</span>
    <span class="hljs-keyword">else</span>:
        policy = new_policy 
</code></pre>
    <p class="normal">Wait! How do we say our value function is optimal? If the value function is not changing over iterations, then we can say that our value function is optimal. Okay, how can we check if the value function is not changing over iterations? </p>
    <p class="normal">We learned that we compute the value function using a policy. If the policy is not changing over iterations, then our value function also doesn't change over the iterations. Thus, when the policy doesn't change over iterations, then we can say that we have found the optimal value function.</p>
    <p class="normal">Thus, over a series of iterations when the policy and new policy become the same, then we can say that we obtained the optimal value function. The following final pseudocode is given for clarity:</p>
    <pre class="programlisting code"><code class="hljs-code">policy = random_policy
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations): 
    value_function = compute_value_function(policy)
    new_policy = extract_policy(value_function)
<span class="code-highlight"><strong class="hljs-slc">    </strong><strong class="hljs-keyword-slc">if</strong><strong class="hljs-slc"> policy == new_policy:</strong></span>
        <span class="hljs-keyword">break</span>
    <span class="hljs-keyword">else</span>:
        policy = new_policy 
</code></pre>
    <p class="normal">Thus, when the policy is not changing, that is, when the policy and new policy become the same, then we can say that we obtained the optimal value function and the policy that is used to compute the optimal value function will be the optimal policy. </p>
    <p class="normal">Remember that in the value iteration<a id="_idIndexMarker314"/> method, we compute the optimal value function using the maximum over Q function (Q value) iteratively and once we have found the optimal value function, we extract the optimal policy from it. But in the policy iteration method, we compute the optimal value function using the policy iteratively and once we have found the optimal value function, then the policy that is used to compute the optimal value function will be the optimal policy. </p>
    <p class="normal">Now that we have a basic understanding of how the policy iteration method works, in the next section, we will get into the details and learn how to compute policy iteration manually.</p>
    <h3 id="_idParaDest-102" class="title">Algorithm – policy iteration</h3>
    <p class="normal">The steps of the policy <a id="_idIndexMarker315"/>iteration algorithm is given as follows:</p>
    <ol>
      <li class="numbered" value="1">Initialize a random policy</li>
      <li class="numbered">Compute the value function using the given policy</li>
      <li class="numbered">Extract a new policy using the value function obtained from <em class="italic">step</em> 2</li>
      <li class="numbered">If the extracted policy is the same as the policy used in <em class="italic">step 2</em>, then stop, else send the extracted new policy to <em class="italic">step 2</em> and repeat <em class="italic">steps 2</em> to <em class="italic">4</em></li>
    </ol>
    <p class="normal">Now, let's get into the details and learn how exactly the preceding steps work. For a clear understanding, let's perform policy iteration manually. Let's take the same grid world environment we used in the value iteration method. Let's say we are in state <strong class="keyword">A</strong> and our goal is to reach state <strong class="keyword">C</strong> without visiting the shaded state <strong class="keyword">B</strong>, and say we have two actions, 0 – <em class="italic">left</em>/<em class="italic">right</em> and 1 – <em class="italic">up</em>/<em class="italic">down</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_22.png" alt=""/></figure>
    <p class="packt_figref">Figure 3.10: Grid world environment</p>
    <p class="normal">We know that in the above environment, the optimal policy is the one that tells us to perform action 1 in state <strong class="keyword">A</strong> so that we can reach <strong class="keyword">C</strong> without visiting <strong class="keyword">B</strong>. Now, we will see how to find this optimal policy <a id="_idIndexMarker316"/>using policy iteration. </p>
    <p class="normal"><em class="italic">Table 3.14</em> shows the model dynamics of state <strong class="keyword">A</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_23.png" alt=""/></figure>
    <p class="packt_figref">Table 3.14: Model dynamics of state A</p>
    <h4 class="title">Step 1 – Initialize a random policy</h4>
    <p class="normal">First, we will initialize a<a id="_idIndexMarker317"/> random policy. As the following shows, our random policy tells us to perform action 1 in state <strong class="keyword">A</strong>, 0 in state <strong class="keyword">B</strong>, and action 1 in state <strong class="keyword">C</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_169.png" alt="" style="height: 3.16em;"/></figure>
    <h4 class="title">Step 2 – Compute the value function using the given policy </h4>
    <p class="normal">This step is exactly the same as how<a id="_idIndexMarker318"/> we computed the value function in value iteration but with a small difference. In value iteration, we computed the value function by taking the maximum over the Q function. But here in policy iteration, we will compute the value function using the policy.</p>
    <p class="normal">To understand this step better, let's quickly recollect how we compute the value function in value iteration. In value iteration, we compute the optimal value function as the maximum over the optimal Q function as the following shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></figure>
    <p class="center">where <img src="../Images/B15558_03_171.png" alt="" style="height: 1.2em;"/></p>
    <p class="normal">In policy iteration, we compute the<a id="_idIndexMarker319"/> value function using a policy <img src="../Images/B15558_03_172.png" alt="" style="height: 0.84em;"/>, unlike value iteration, where we computed the value function using the maximum over the Q function. The value function using a policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_174.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">If you look at the preceding equation, to compute the value function, we need the transition probability <img src="../Images/B15558_03_175.png" alt="" style="height: 1.2em;"/>, the reward function <img src="../Images/B15558_03_176.png" alt="" style="height: 1.2em;"/><strong class="keyword"> </strong>and the value of the next state <img src="../Images/B15558_03_177.png" alt="" style="height: 1.2em;"/>. The values of the transition probability <img src="../Images/B15558_03_175.png" alt="" style="height: 1.2em;"/> and the reward function <img src="../Images/B15558_03_176.png" alt="" style="height: 1.2em;"/> can be obtained from the model dynamics. But what about the value of the next state <img src="../Images/B15558_03_180.png" alt="" style="height: 1.2em;"/>? We don't know the value of any states yet. So, we will initialize the value function (state values) with random values or zeros as <em class="italic">Figure 3.15</em> shows and compute the value function:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_24.png" alt=""/></figure>
    <p class="packt_figref">Table 3.15: Initial value table</p>
    <p class="normal"><strong class="keyword">Iteration 1</strong>:</p>
    <p class="normal">Let's compute the value of state <strong class="keyword">A</strong> (note that here, we only compute the value for the action given by the policy, unlike in value iteration, where we computed the Q value for all the actions in the state and selected the maximum value).</p>
    <p class="normal">So, the action given by the policy for<a id="_idIndexMarker320"/> state <strong class="keyword">A</strong> is 1 and we can compute the value of state <strong class="keyword">A</strong> as the following shows (note that we have used a discount factor <img src="../Images/B15558_03_181.png" alt="" style="height: 1.11em;"/> throughout this section):</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_182.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Similarly, we compute the value for all the states using the action given by the policy. <em class="italic">Table 3.16</em> shows the updated state values obtained as a result of the first iteration:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_25.png" alt=""/></figure>
    <p class="packt_figref">Table 3.16: Value table from iteration 1</p>
    <p class="normal">However, the value function (value table) shown in <em class="italic">Table 3.16</em> obtained as a result of the first iteration will not be accurate. That is, the state values (value function) will not be accurate according to the given policy.</p>
    <p class="normal">Note that unlike the value iteration method, here we are not checking whether our value function is optimal or not; we just check whether our value function is accurately computed according to the given policy.</p>
    <p class="normal">The value function will not be<a id="_idIndexMarker321"/> accurate because when we started off computing the value function using the given policy, we used the randomly initialized state values:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_33.png" alt=""/></figure>
    <p class="normal">So, in the next iteration, while computing the value function, we will use the updated state values obtained as a result of the first iteration:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_34.png" alt=""/></figure>
    <p class="normal"><strong class="keyword">Iteration 2</strong>:</p>
    <p class="normal">Now, in iteration 2, we compute the value function using the policy <img src="../Images/B15558_03_082.png" alt="" style="height: 0.84em;"/>. Remember that while computing the value function, we will use the updated state values (value table) obtained from iteration 1.</p>
    <p class="normal">For instance, let's compute the value of state A:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_184.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Similarly, we compute the value for<a id="_idIndexMarker322"/> all the states using the action given by the policy. <em class="italic">Table 3.17</em> shows the updated state values obtained as a result of the second iteration:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_26.png" alt=""/></figure>
    <p class="packt_figref">Table 3.17: Value table from iteration 2</p>
    <p class="normal"><strong class="keyword">Iteration 3</strong>:</p>
    <p class="normal">Similarly, in iteration 3, we compute the value function using the policy <img src="../Images/B15558_03_185.png" alt="" style="height: 0.84em;"/> and while computing the value function, we will use the updated state values (value table) obtained from iteration 2.</p>
    <p class="normal"><em class="italic">Table 3.18</em> shows the updated state values obtained from the third iteration:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_27.png" alt=""/></figure>
    <p class="packt_figref">Table 3.18: Value table from iteration 3</p>
    <p class="normal">We repeat this for many iterations until the value table does not change or changes very little over iterations. For example, let's suppose <em class="italic">Table 3.19</em> shows the value table obtained as a<a id="_idIndexMarker323"/> result of <strong class="keyword">iteration 4</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_28.png" alt=""/></figure>
    <p class="packt_figref">Table 3.19: Value table from iteration 4</p>
    <p class="normal">As we can see, the difference between the value tables obtained from iteration 4 and iteration 3 is very small. So, we can say that the value table is not changing much over iterations and we stop at this iteration and take this as our final value function.</p>
    <h4 class="title">Step 3 – Extract a new policy using the value function obtained from the previous step </h4>
    <p class="normal">As a result of <em class="italic">step 2</em>, we obtained the <a id="_idIndexMarker324"/>value function, which is computed using the given random policy. However, this value function will not be optimal as it is computed using the random policy. So will extract a new policy from the value function obtained in the previous step. The value function (value table) obtained from the previous step is shown in <em class="italic">Table 3.20</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_29.png" alt=""/></figure>
    <p class="packt_figref">Table 3.20: Value table from the previous step </p>
    <p class="normal">Okay, how can we extract a new policy from the value function? (Hint: This step is exactly the same as how we extracted a policy given the value function in <em class="italic">step 2</em> of the value iteration method.) </p>
    <p class="normal">In order to extract a new policy, we compute the Q function using the value function (value table) obtained from the<a id="_idIndexMarker325"/> previous step. Once we compute the Q function, we pick up actions in each state that have the maximum value as a new policy. We know that the Q function can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_186.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Now, while computing Q values, we use the value function we obtained from the previous step. </p>
    <p class="normal">For instance, let's compute the Q value for all actions in state <strong class="keyword">A</strong> using the value function obtained from the previous step. The Q value for action 0 in state <strong class="keyword">A</strong> is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_187.png" alt="" style="height: 3.42em;"/></figure>
    <p class="normal">The Q value for action 1 in state <strong class="keyword">A</strong> is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_188.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal"><em class="italic">Table 3.21</em> shows the Q table after computing the Q values for all state-action pairs:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_30.png" alt=""/></figure>
    <p class="packt_figref">Table 3.21: Q table</p>
    <p class="normal">From this <em class="italic">Q</em> table, we pick up <a id="_idIndexMarker326"/>actions in each state that have the maximum value as a new policy.</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_189.png" alt="" style="height: 3.16em;"/></figure>
    <h4 class="title">Step 4 – Check the new policy</h4>
    <p class="normal">Now we will check if the extracted <a id="_idIndexMarker327"/>new policy from <em class="italic">step 3</em> is the same as the policy we used in <em class="italic">step 2</em>. If it is the same, then we stop, else we send the extracted new policy to <em class="italic">step 2</em> and repeat <em class="italic">steps 2</em> to <em class="italic">4</em>.</p>
    <p class="normal">Thus, in this section, we learned how to compute the optimal policy using the policy iteration method. In the next section, we will learn how to implement the policy iteration method to compute the optimal policy in the Frozen Lake environment using the Gym toolkit. </p>
    <h3 id="_idParaDest-103" class="title">Solving the Frozen Lake problem with policy iteration</h3>
    <p class="normal">We <a id="_idIndexMarker328"/>learned that in the Frozen Lake environment, our<a id="_idIndexMarker329"/> goal is to reach the goal state <strong class="keyword">G</strong> from the starting state <strong class="keyword">S</strong> without visiting the hole states <strong class="keyword">H</strong>. Now, let's learn how to compute the optimal policy using the policy iteration method in the Frozen Lake environment. </p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">Now, let's create the Frozen Lake environment using Gym:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'FrozenLake-v0'</span>)
</code></pre>
    <p class="normal">We learned that in the policy<a id="_idIndexMarker330"/> iteration, we compute the value function using the policy iteratively. Once we have found the optimal value function, then the policy that is used to compute the optimal value function will be the optimal policy.</p>
    <p class="normal">So, first, let's learn how to compute the value function using the policy.</p>
    <h4 class="title">Computing the value function using the policy </h4>
    <p class="normal">This step is exactly the same as how<a id="_idIndexMarker331"/> we computed the value function in the value iteration method but with a small difference. Here, we compute the value function using the policy but in the value iteration method, we compute the value function by taking the maximum over Q values. Now, let's learn how to define a function that computes the value function using the given policy.</p>
    <p class="normal">Let's define a function called <code class="Code-In-Text--PACKT-">compute_value_function</code>, which takes the policy as a parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">compute_value_function</span><span class="hljs-function">(</span><span class="hljs-params">policy</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Now, let's define the number of iterations:</p>
    <pre class="programlisting code"><code class="hljs-code">    num_iterations = <span class="hljs-number">1000</span>
</code></pre>
    <p class="normal">Define the threshold value:</p>
    <pre class="programlisting code"><code class="hljs-code">    threshold = <span class="hljs-number">1e-20</span>
</code></pre>
    <p class="normal">Set the discount factor <img src="../Images/B15558_03_190.png" alt="" style="height: 0.93em;"/> value to 1.0:</p>
    <pre class="programlisting code"><code class="hljs-code">    gamma = <span class="hljs-number">1.0</span>
</code></pre>
    <p class="normal">Now, we will initialize the value table by setting all the state values to zero:</p>
    <pre class="programlisting code"><code class="hljs-code">    value_table = np.zeros(env.observation_space.n)
</code></pre>
    <p class="normal">For every iteration:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
</code></pre>
    <p class="normal">Update the value table; that is, we<a id="_idIndexMarker332"/> learned that on every iteration, we use the updated value table (state values) from the previous iteration:</p>
    <pre class="programlisting code"><code class="hljs-code">        updated_value_table = np.copy(value_table) 
</code></pre>
    <p class="normal">Now, we compute the value function using the given policy. We learned that a value function can be computed according to some policy <img src="../Images/B15558_01_047.png" alt="" style="height: 0.84em;"/> as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_192.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Thus, for each state, we select the action according to the policy and then we update the value of the state using the selected action as follows.</p>
    <p class="normal">For each state:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(env.observation_space.n):
</code></pre>
    <p class="normal">Select the action in the state according to the policy:</p>
    <pre class="programlisting code"><code class="hljs-code">            a = policy[s]
</code></pre>
    <p class="normal">Compute the value of the state using the selected action,<img src="../Images/B15558_03_192.png" alt="" style="height: 2.69em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">            value_table[s] = sum(
                [prob * (r + gamma * updated_value_table[s_])
                    <span class="hljs-keyword">for</span> prob, s_, r, _ <span class="hljs-keyword">in</span> env.P[s][a]])
</code></pre>
    <p class="normal">After computing the value table, that is, the value of all the states, we check whether the difference between the value table obtained in the current iteration and the previous iteration is less than or equal to a threshold value. If it is less, then we break the loop and return the value table as<a id="_idIndexMarker333"/> an accurate value function of the given policy:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> (np.sum(np.fabs(updated_value_table - value_table)) &lt;= threshold):
             <span class="hljs-keyword">break</span>
    
    <span class="hljs-keyword">return</span> value_table
</code></pre>
    <p class="normal">Now that we have computed the value function of the policy, let's see how to extract the policy from the value function. </p>
    <h4 class="title">Extracting the policy from the value function </h4>
    <p class="normal">This step is exactly the same as <a id="_idIndexMarker334"/>how we extracted the policy from the value function in the value iteration method. Thus, similar to what we learned in the value iteration method, we define a function called <code class="Code-In-Text--PACKT-">extract_policy</code> to extract a policy given the value function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">extract_policy</span><span class="hljs-function">(</span><span class="hljs-params">value_table</span><span class="hljs-function">):</span>
    
    gamma = <span class="hljs-number">1.0</span>
    policy = np.zeros(env.observation_space.n) 
    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(env.observation_space.n):
        
        Q_values = [sum([prob*(r + gamma * value_table[s_])
                             <span class="hljs-keyword">for</span> prob, s_, r, _ <span class="hljs-keyword">in</span> env.P[s][a]]) 
                                   <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> range(env.action_space.n)] 
                
        policy[s] = np.argmax(np.array(Q_values)) 
    
    <span class="hljs-keyword">return</span> policy
</code></pre>
    <h4 class="title">Putting it all together</h4>
    <p class="normal">First, let's define a function called <code class="Code-In-Text--PACKT-">policy_iteration</code>, which takes the environment as a parameter:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">policy_iteration</span><span class="hljs-function">(</span><span class="hljs-params">env</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Set the number of iterations:</p>
    <pre class="programlisting code"><code class="hljs-code">    num_iterations = <span class="hljs-number">1000</span>
</code></pre>
    <p class="normal">We learned that in the policy iteration method, we begin by initializing a random policy. So, we will initialize the<a id="_idIndexMarker335"/> random policy, which selects the action 0 in all the states: </p>
    <pre class="programlisting code"><code class="hljs-code">    policy = np.zeros(env.observation_space.n)
</code></pre>
    <p class="normal">For every iteration:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
</code></pre>
    <p class="normal">Compute the value function using the policy:</p>
    <pre class="programlisting code"><code class="hljs-code">        value_function = compute_value_function(policy)
</code></pre>
    <p class="normal">Extract the new policy from the computed value function: </p>
    <pre class="programlisting code"><code class="hljs-code">        new_policy = extract_policy(value_function)
</code></pre>
    <p class="normal">If <code class="Code-In-Text--PACKT-">policy</code> and <code class="Code-In-Text--PACKT-">new_policy</code> are the same, then break the loop:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> (np.all(policy == new_policy)):
            <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">Else update the current <code class="Code-In-Text--PACKT-">policy</code> to <code class="Code-In-Text--PACKT-">new_policy</code></p>
    <pre class="programlisting code"><code class="hljs-code">        policy = new_policy
        
    <span class="hljs-keyword">return</span> policy
</code></pre>
    <p class="normal">Now, let's learn how to perform policy iteration and find the optimal policy in the Frozen Lake environment. So, we just feed the Frozen Lake environment to our <code class="Code-In-Text--PACKT-">policy_iteration</code> function as shown here and get the optimal policy:</p>
    <pre class="programlisting code"><code class="hljs-code">optimal_policy = policy_iteration(env)
</code></pre>
    <p class="normal">We can print the optimal policy: </p>
    <pre class="programlisting code"><code class="hljs-code">print(optimal_policy)
</code></pre>
    <p class="normal">The preceding code will print the following:</p>
    <pre class="programlisting code"><code class="hljs-code">array([<span class="hljs-number">0.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>])
</code></pre>
    <p class="normal">As we can observe, our<a id="_idIndexMarker336"/> optimal policy tells us to perform the correct action in each state. Thus, we learned how to perform the policy iteration method to compute the optimal policy. </p>
    <h1 id="_idParaDest-104" class="title">Is DP applicable to all environments? </h1>
    <p class="normal">In dynamic<a id="_idIndexMarker337"/> programming, that is, in the value and policy iteration methods, we try to find the optimal policy.</p>
    <p class="normal"><strong class="keyword">Value iteration</strong>: In the value iteration method, we compute the optimal value function by taking the maximum over the Q function (Q values) iteratively:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_03_123.png" alt="" style="height: 2.69em;"/>. After finding the optimal value function, we extract the optimal policy from it.</p>
    <p class="normal"><strong class="keyword">Policy iteration</strong>: In the policy iteration method, we compute the optimal value function using the policy iteratively:</p>
    <figure class="mediaobject"><img src="../Images/B15558_03_192.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">We will start off with the random policy and compute the value function. Once we have found the optimal value function, then the policy that is used to create the optimal value function will be the optimal policy.</p>
    <p class="normal">If you look at the preceding two equations, in order to find the optimal policy, we compute the value function and Q function. But to compute the value and the Q function, we need to know the transition probability <img src="../Images/B15558_03_175.png" alt="" style="height: 1.2em;"/> of the environment, and when we don't know the transition probability of the environment, we cannot compute the value and the Q function in order to find the optimal policy.</p>
    <p class="normal">That is, dynamic programming is a model-based method and to apply this method, we need to know the model dynamics (transition probability) of the environment, and when we don't know the model dynamics, we cannot apply the dynamic programming method.</p>
    <p class="normal">Okay, how can we find the<a id="_idIndexMarker338"/> optimal policy when we don't know the model dynamics of the environment? In such a case, we can use model-free methods. In the next chapter, we will learn about one of the interesting model-free methods, called Monte Carlo, and how it is used to find the optimal policy without requiring the model dynamics. </p>
    <h1 id="_idParaDest-105" class="title">Summary</h1>
    <p class="normal">We started off the chapter by understanding the Bellman equation of the value and Q functions. We learned that, according to the Bellman equation, the value of a state is the sum of the immediate reward, and the discounted value of the next state and the value of a state-action pair is the sum of the immediate reward and the discounted value of the next state-action pair. Then we learned about the optimal Bellman value function and the Q function, which gives the maximum value.</p>
    <p class="normal">Moving forward, we learned about the relation between the value and Q functions. We learned that the value function can be extracted from the Q function as <img src="../Images/B15558_03_088.png" alt="" style="height: 1.58em;"/> and then we learned that the Q function can be extracted from the value function as <img src="../Images/B15558_03_199.png" alt="" style="height: 2.69em;"/>.</p>
    <p class="normal">Later we learned about two interesting methods called value iteration and policy iteration, which use dynamic programming to find the optimal policy. </p>
    <p class="normal">In the value iteration method, first, we compute the optimal value function by taking the maximum over the Q function iteratively. Once we have found the optimal value function, then we use it to extract the optimal policy. In the policy iteration method, we try to compute the optimal value function using the policy iteratively. Once we have found the optimal value function, then the policy that is used to create the optimal value function will be extracted as the optimal policy.</p>
    <h1 id="_idParaDest-106" class="title">Questions</h1>
    <p class="normal">Let's try answering the following questions to assess our knowledge of what we learned in this chapter:</p>
    <ol>
      <li class="numbered" value="1">Define the Bellman equation.</li>
      <li class="numbered">What is the difference between the Bellman expectation and Bellman optimality equations?</li>
      <li class="numbered">How do we derive the value function from the Q function?</li>
      <li class="numbered">How do we derive the Q function from the value function?</li>
      <li class="numbered">What are the steps involved in value iteration?</li>
      <li class="numbered">What are the steps involved in policy iteration?</li>
      <li class="numbered">How does policy iteration differ from value iteration?</li>
    </ol>
  </div>
</body></html>