- en: Digit Classification Using TensorFlow Lite
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There has been a lot of progress in the field of **machine learning** (**ML**)
    in the last five years. These days, a variety of ML applications are being used
    in our daily lives and we don't even realize it. Since ML has taken the spotlight,
    it would be helpful if we could use it to run deep models on mobile devices, which
    is one of the most used devices in our daily life.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Innovation in mobile hardware, coupled with new software frameworks for deploying
    ML models on mobile devices, is proving to be one of the major accelerators for
    developing ML based applications on mobile or other edge devices like tablet..
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about Google''s new library, TensorFlow Lite, which
    can be used to deploy ML models on mobile devices. We will train a deep learning
    model on the MNIST digits dataset and look at how we can convert this model into
    a mobile-friendly format by understanding the following concepts:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to TensorFlow Lite and its architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Classification model evaluation metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a deep learning model on the MNIST dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting the trained model into a mobile-friendly format using TensorFlow
    Lite
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this chapter will not discuss building an Android application to deploy
    these models since this has been extensively documented in Google's TensorFlow
    tutorials ([https://www.tensorflow.org/lite/](https://www.tensorflow.org/lite/)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow Lite?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we take a deep dive into TensorFlow Lite, let's try to understand what
    are the advantages of doing ML on edge devices like mobile/tablet and others.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy**: If inference on a ML model can be performed on a device, user
    data doesn''t need to leave the device, which helps in preserving the privacy
    of the user.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offline predictions**: The device doesn''t need to be connected to a network
    to make predictions on a ML model. This unlocks a lot of use cases in developing
    nations such as India where network connectivity is not so great.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smart devices**: This can also enable the development of smart home devices
    such as microwaves and thermostats with on-device intelligence.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power efficient**: An on-device ML can be more power-efficient as there is
    no need to transfer data back and forth to the server.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensor data utilization**: ML models can make use of rich sensor data since
    it is easily available on mobile.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, mobile devices are not same as our desktops and laptops. There are
    different considerations when deploying the model on mobile or embedded devices
    such as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Model size**:As we know, mobiles have limited memory and we can''t store
    a memory-heavy model on a device. There are two ways of handling this:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can round or quantize the weights of the model so that they require fewer
    floating-point representations. This is in line with our understanding that integers
    always require less memory to store than the floating point numbers.
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we only use devices for inferences or predictions, we can strip out all
    the training operations in our Tensorflow graph which are not useful for making
    predictions.
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们仅在设备上进行推断或预测，因此可以去除Tensorflow图中所有对进行预测无用的训练操作。
- en: '**Speed**:One of the important things for deploying models on mobile devices
    is the speed with which we can run an inference so that we gain a better user
    experience. Models have to be optimized in such a manner that they don''t exceed
    the latency budget on the phone but are still fast.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：在移动设备上部署模型的重要因素之一是我们能够运行推断的速度，以获得更好的用户体验。必须优化模型，以确保它们不会超出手机的延迟预算，同时又保持快速。'
- en: '**Ease of deployment**: We need efficient frameworks/libraries so that deployment
    on mobile devices is very straightforward.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署简便性**：我们需要高效的框架/库，以便在移动设备上部署非常简单。'
- en: With these considerations in mind, Google has developed TensorFlow Lite, which
    is a lightweight version of original Tensorflow for deploying deep learning models
    on mobile and embedded devices.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，Google开发了TensorFlow Lite，这是原始TensorFlow的轻量级版本，用于在移动和嵌入式设备上部署深度学习模型。
- en: 'To understand TensorFlow Lite, take a look at the following diagram, which
    shows its high-level architecture:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解TensorFlow Lite，请查看以下显示其高级架构的图表：
- en: '![](img/7ee6c212-fbed-4caf-b6fc-bd44dc9b4ee3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ee6c212-fbed-4caf-b6fc-bd44dc9b4ee3.png)'
- en: This architecture makes it evident that we need to convert a trained TF model
    into `.tflite` format. This format is different from usual TF models as it is
    optimized for inference on devices. We will learn about the conversion process
    in detail later in the chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构清楚地表明，我们需要将训练好的TF模型转换为`.tflite`格式。这种格式与通常的TF模型不同，因为它经过优化，可用于设备上的推断。我们将在本章后面详细学习转换过程。
- en: 'For now, let''s try to understand the major features of using TF Lite format:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们试着了解使用TF Lite格式的主要特点：
- en: The model is serialized and converted to a Flatbuffer format([https://google.github.io/flatbuffers/](https://google.github.io/flatbuffers/)).
    Flatbuffers have the advantage that data can be directly accessed without parsing/unpacking
    of large files that contain weights.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型被序列化并转换为Flatbuffer格式([https://google.github.io/flatbuffers/](https://google.github.io/flatbuffers/))。Flatbuffers的优点在于数据可以直接访问，无需解析/解包包含权重的大文件。
- en: Weights and biases of the model are pre-fused into TF lite format.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的权重和偏差已预先融合到TF Lite格式中。
- en: TF lite is cross-platform and can be deployed on Android, iOS, Linux, and hardware
    devices such as Raspberry Pi.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: TF Lite跨平台，可部署在Android、iOS、Linux和硬件设备（如Raspberry Pi）上。
- en: It includes an on-device interpreter that has been optimized for faster execution
    on mobile. The core interpreter with all of the supported operations is around
    400 KB, and 75 KB without the supported operations. This means that the model
    takes up little space on the device. Overall, the idea is to keep the parts of
    the model that are essential for inference and strip out all the other parts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括一个在设备上优化为更快执行的解释器。所有支持的操作的核心解释器大小约为400 KB，不支持操作时为75 KB。这意味着模型在设备上占用的空间很小。总体而言，理念是保留仅对推断至关重要的模型部分，并剥离所有其他部分。
- en: With innovation in hardware, many companies are also developing GPUs and Digital
    Signal Processors (DSPs) that are optimized for neural network inference. TF Lite
    provides the Android Neural Networks API, which can perform hardware acceleration
    on these devices.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 利用硬件创新，许多公司还在开发专为神经网络推断优化的GPU和数字信号处理器（DSP）。TF Lite提供了Android神经网络API，可以在这些设备上进行硬件加速。
- en: Classification Model Evaluation Metrics
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类模型评估指标
- en: Just building a model does not suffice; we need to make sure that our model
    functions well and gives us a good and accurate output. To do this, we need to
    understand some classification metrics that will be used to evaluate the model
    throughout this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅构建模型是不够的；我们需要确保我们的模型功能良好，并为我们提供良好和准确的输出。为此，我们需要了解一些分类指标，这些指标将用于全书中评估模型。
- en: 'Let''s begin by defining some building blocks of the metrics that will be used
    to evaluate the classification models. To do this, take a simple example of spam
    detection that is done by any online mailbox for reference. A spam email shall
    be considered to be of a positive class and the normal email to be of a negative
    class. We can summarize this spam detection model into four categories, which
    are illustrated in the following matrix:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '| **True positives** (**TP**) | **False positives** (**FP**) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| Reality: Email is spam | Reality: Email is NOT spam |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| Model Prediction: Email is spam | Model Prediction: Email is spam |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| **False negatives** (**FN**) | **True negatives** (**TN**) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| Reality: Email is spam | Reality: Email is NOT spam |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Model Prediction: Email is NOT spam | Model Prediction: Email is NOT spam
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: This matrix is also commonly known as the **confusion matrix.**
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'The three major metrics that we will use to define classifier quality, primarily
    in an unbalanced dataset, are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy: **Accuracy is the most basic metric used for classification problems.
    It is defined as follows:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8f86fe08-4b08-4072-ade1-c38090aa1b26.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: '**Precision: **Precision tries to measure the true positives out of all predicted
    positives from the model. If your Gmail doesn''t misclassify a lot of emails from
    your friends (or normal emails) and put them into spam, then it has very high
    precision. It is mathematically represented as follows:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f1baadc5-fcc9-4bcc-b39e-5fa42314b6d0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: '**Recall: **Recall tries to measure the number of values classified as positive
    out of all real positives in the dataset. In simple terms, if your Gmail doesn''t
    misclassify a lot of your spam emails as normal emails and sends them to your
    inbox, then it has very high recall:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/aa450a95-27b8-4bc7-940c-50fd19ba4e4b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Ideally, we want a model with high precision and high recall. However, there
    is always a trade-off between high precision and high recall in machine learning.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Classifying digits using TensorFlow Lite
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete this project, we will use the MNIST digit dataset, which is available
    in the TensorFlow datasets library ([https://www.tensorflow.org/guide/datasets](https://www.tensorflow.org/guide/datasets)).
    It consists of images of handwritten digits from 0 to 9\. The training dataset
    has 60,000 images and the testing set has 10,000 images. Some of the images in
    the dataset are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc9f7e6e-15de-4103-9800-449c941e283c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: If we take a look at TensorFlow Lite tutorials, we will see that the focus is
    on using pre-trained models such as Mobilenet or retraining the existing ones.
    However, none of these tutorials talk about building new models, which is something
    we will be doing here.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Note that we specifically choose a simple model because at the time of writing
    this book, TensorFlow Lite doesn't have adequate support for all types of complex
    models
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: We will use categorical cross entropy as the loss function for this classification
    problem. Categorical cross entropy was explained in detail in [Chapter 3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml), *Sentiment
    Analysis in Your Browser Using TensorFlow.js*, of this book. In this chapter,
    we have 10 different digits in the dataset, so we will use the categorical cross
    entropy over 10 classes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing data and defining the model
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to pre-process our data for making it ready to feed into our model,
    define our model, and create an evaluation metric:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Pre-process the data by ensuring the images are of shape 28x28x1 and converting
    the pixels into a float type variable for training. Also, here we define NUM_CLASSES
    = 10 as there are 10 different digits in the images.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Define the model as having two convolutional layers with the same filter sizes, two
    fully connected layers, two dropout layers with dropout probabilities of 0.25
    and 0.5 respectively, a Rectified Linear (ReLU) after every fully connected or
    convolutional layer except the last one, and one max pool layer. Also we add a
    Softmax activation to convert the output of the model to probabilities for each
    of the 10 digits. Note that we use this model as it produces good results. You
    can try improving the model by adding more layers or trying different shapes of
    the existing layers.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we have named our output tensor `softmax_tensor`, which will come
    in pretty handy when we try to convert this model into a TensorFlow Lite format.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Further define the following parameters for the model:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss = Categorical Cross Entropy
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizer = AdaDelta. Adam optimizer, defined in  [Chapter 3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml), *Sentiment
    Analysis in Your Browser Using TensorFlow.js*, is an extension of AdaDelta. We
    use AdaDelta as it gives good result for this model. You can find more details
    about AdaDelta in the original paper ([https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)).
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation Metric = Classification Accuracy
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code for defining these is as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Enable Tensorboard logging to visualize the model graph and training progress.
    Code is defined as follows:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Train the model using the following parameters:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Epochs = 12
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch Size = 128:'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We achieved **99.24% **accuracy on the test dataset with just 12 epochs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Note that we use the `callbacks` parameter to log the training progress on TensorBoard.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Converting TensorFlow model to TensorFlow Lite
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have trained the model in the usual way, let's look at how we can
    convert this model into a TensorFlow Lite format.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'The general procedure for conversion is illustrated in the following diagram:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c787e53-0ca7-404e-82ad-051bff8c99c5.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'The procedure is simple: we take a trained model, freeze the graph, optimize
    it for inference/prediction, and convert it into `.tflite` format. Before going
    further, let''s understand what we mean by Freeze Graph and Optimize For Inference:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**Freeze Graph : **Freeze graph operation effectively freezes the weights of
    the model by converting all the of the TF Variables as Constants. As you can imagine,
    having all of the weights as constants can save space compared to keeping them
    as variables. As we only perform inference on mobile (and not training), we don''t
    want to change the model weights anyway'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize For Inference**: Once the graph is frozen, we remove all the operations
    in the graph which are not useful for inference. For example, Dropout operation
    is used to train the model such that it doesn''t overfit. However, there is absolutely
    no use of this operation during prediction on mobile.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the rest of this section, we will heavily use TensorBoard visualization
    ([https://www.TensorFlow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard))
    for graph visualization. :'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have trained the model, you must have a file with the prefix `events.out.tfevents.` in
    your model folder. Go to the `logs` folder and type the following into terminal:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TensorBoard will start in port `6006` by default. Launch it by going to your
    browser and typing `localhost:6006` into the address bar*. *Once the Tensorboard
    opens, if you navigate to the Graphs tab at the top, you will be able to see the
    Tensorflow Graph of your model. In the following diagram, we illustrate the main
    graph, with annotations for the input tensor, output tensor, and training part
    of the graph. As we can see, we shouldn't keep anything from the training graph
    for inference/making predictions on mobile.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d23c048-2039-40ad-87e5-922447fddf5b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: Next implement a function `freeze_sesssion` which takes TF session as input,
    converts all variables into constants, and returns the frozen graph. After executing
    this function, you will obtain a frozen graph file named `MNIST_model.pb` in the `<model_folder>/logs/freeze` folder.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, here is where it gets really strange: you can''t visualize the `MNIST_model.pb`
    file directly through TensorBoard. You need to write the graph in a format that
    TensorBoard can pick up. Execute the function `pb_to_tensorboard` mentioned below
    and you will see another file in `<model_folder>/logs/freeze` folder with prefix
    `events.out.tfevents`.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, start TensorBoard again with `logdir` as `<model_folder>/logs/freeze` and
    visualize the frozen graph. You will observe the you have stripped out most of
    the variables from the graph. The following diagram illustrates the frozen graph
    you will obtain:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8c2b389b-9f29-42ec-b89a-3baa069e4159.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'Next step is further trim to optimize it for inference. As explained before,
    we will remove Dropout variables from the graph as they are not useful for inference
    on mobile. However, there is no perfect way to remove those from the graph based
    on the existing TensorFlow functions/programs. The new improvements to TensorFlow
    Lite don''t work for this example, which suggests that they are still under development.
    Instead, you will have to manually specify the operations you want to remove and
    connect the input of the Dropout operations to the operations after them in the
    graph. For example, in the frozen graph, let''s say we want to remove the `dropout_3`
    operation. The following diagram shows the zoomed-in version of the frozen graph:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/001f621c-afb7-42cb-ba3c-42b2af5ae316.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: In such a case, you will have to connect the `max_pooling2` operation directly
    to the `flatten_2` operation, thereby skipping the `dropout_3` op in the graph.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Execute the function `optimize_graph` mentioned below to remove all of the dropout
    ops in the graph. It manually flushes out all the Dropout operations from the
    graph. This will result in a new file named `MNIST_optimized.pb` under the `<model_folder>/logs/optimized` folder.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Again, to visualize the graph in TensorBoard, you need to convert it using the
    function `pb_to_tensorboar` defined in step 3 so that it's TensorBoard-friendly
    and obtain a new file with the prefix `events.out.tfevents` in the same folder.
    The following figure illustrates the graph you will obtain after removing the
    Dropout operations.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/30c933b9-110b-4297-8945-76860bc3e017.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Note that getting rid of Dropout from the graph will not affect testing set
    accuracy as Dropout is not used for inference.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step in obtaining the model in a mobile-friendly format is to convert
    it into a `.tflite` file. For this step, you will use `toco` command, which stands
    for TensorFlow Lite Optimizing Converter ([https://www.tensorflow.org/lite/convert/](https://www.tensorflow.org/lite/convert/)).
    The code is provided as follows:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This will produce a file named `mnist.tflite` in `<model_folder>`. Essentially,
    this step is trying to convert the optimized graph into a Flatbuffer for efficient
    on-device inference.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: We will not cover deploying our project to the mobile device as its development
    is outside the scope of this book. However, feel free to take a look at TensorFlow
    Lite tutorials on how to deploy the TF Lite models Android ([https://www.tensorflow.org/lite/demo_android](https://www.tensorflow.org/lite/demo_android))
    or iOS ([https://www.tensorflow.org/lite/demo_ios](https://www.tensorflow.org/lite/demo_ios)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is at the edge of the next wave, where we try to make ML ubiquitous
    in our everyday life. It has several advantages such as offline access, data privacy,
    and so on.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we looked at a new library from Google known as TensorFlow
    Lite, which has been optimized for deploying ML models on mobile and embedded
    devices. We understood the architecture of TensorFlow Lite, which converts the
    trained TensorFlow model into `.tflite` format. This is designed for inference
    at fast speed and low memory on devices. TensorFlow Lite also supports multiple
    platforms, such as Android, iOS, Linux, and Raspberry Pi.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了来自谷歌的一个新库——TensorFlow Lite，它已针对在移动设备和嵌入式设备上部署机器学习模型进行了优化。我们了解了 TensorFlow
    Lite 的架构，它将训练好的 TensorFlow 模型转换为 `.tflite` 格式。这是为在设备上进行快速推理和低内存占用而设计的。TensorFlow
    Lite 还支持多个平台，如 Android、iOS、Linux 和 Raspberry Pi。
- en: 'Next, we used the MNIST handwritten digit dataset to train a deep learning
    model. Subsequently, we followed the necessary steps to convert the trained model
    into `.tflite` format. The steps are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用了 MNIST 手写数字数据集来训练一个深度学习模型。随后，我们按照必要的步骤将训练好的模型转换为 `.tflite` 格式。步骤如下：
- en: Froze the graph with variables converted to constants
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图冻结，将变量转换为常量
- en: Optimized the graph for inference by removing the unused ops like Dropout
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过移除未使用的操作（如 Dropout），优化了推理图
- en: Used **TensorFlow Optimization Converter Tool** (**toco**) to convert the optimized
    model to `.tflitte` format
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **TensorFlow 优化转换工具** (**toco**) 将优化后的模型转换为 `.tflite` 格式
- en: At every step, we used TensorBoard to visualize the state of the graph.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步，我们都使用 TensorBoard 来可视化图的状态。
- en: This is a very exciting field that is continuing to evolve, both in terms of
    hardware and software. Once this technology reaches maturity, it will open up
    new use cases and business models across the world.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常激动人心的领域，正在不断发展，无论是在硬件还是软件方面。一旦这项技术成熟，它将在全球范围内开辟新的使用场景和商业模式。
- en: In the next chapter, we'll create a project that will help us convert text to
    speech.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将创建一个项目，帮助我们将文本转换为语音。
- en: Questions
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The following are the questions:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些问题：
- en: How is TensorFlow Lite different from usual TensorFlow?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow Lite 与常规 TensorFlow 有什么不同？
- en: Can you try and if you can build the model on a movie review dataset in  [Chapter
    3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml), *Sentiment Analysis in Your Browser
    Using TensorFlow.js*? Do you face some issues with Tensorflow Lite in that case?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能尝试在[第 3 章](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml)中使用电影评论数据集构建模型吗？*在浏览器中使用
    TensorFlow.js 进行情感分析*？如果是这样，你在使用 TensorFlow Lite 时遇到什么问题了吗？
- en: Can you try Adam optimizer and see if it improves the performance of the model?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能尝试使用 Adam 优化器，看看它是否能提升模型的性能吗？
- en: Can you think you operations other than Dropout which are also not important
    for inference on mobile?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到除了 Dropout 之外，也不重要于移动端推理的操作吗？
