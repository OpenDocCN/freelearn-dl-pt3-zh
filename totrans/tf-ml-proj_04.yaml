- en: Digit Classification Using TensorFlow Lite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There has been a lot of progress in the field of **machine learning** (**ML**)
    in the last five years. These days, a variety of ML applications are being used
    in our daily lives and we don't even realize it. Since ML has taken the spotlight,
    it would be helpful if we could use it to run deep models on mobile devices, which
    is one of the most used devices in our daily life.
  prefs: []
  type: TYPE_NORMAL
- en: Innovation in mobile hardware, coupled with new software frameworks for deploying
    ML models on mobile devices, is proving to be one of the major accelerators for
    developing ML based applications on mobile or other edge devices like tablet..
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about Google''s new library, TensorFlow Lite, which
    can be used to deploy ML models on mobile devices. We will train a deep learning
    model on the MNIST digits dataset and look at how we can convert this model into
    a mobile-friendly format by understanding the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to TensorFlow Lite and its architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Classification model evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a deep learning model on the MNIST dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting the trained model into a mobile-friendly format using TensorFlow
    Lite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this chapter will not discuss building an Android application to deploy
    these models since this has been extensively documented in Google's TensorFlow
    tutorials ([https://www.tensorflow.org/lite/](https://www.tensorflow.org/lite/)).
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow Lite?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we take a deep dive into TensorFlow Lite, let's try to understand what
    are the advantages of doing ML on edge devices like mobile/tablet and others.
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy**: If inference on a ML model can be performed on a device, user
    data doesn''t need to leave the device, which helps in preserving the privacy
    of the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offline predictions**: The device doesn''t need to be connected to a network
    to make predictions on a ML model. This unlocks a lot of use cases in developing
    nations such as India where network connectivity is not so great.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smart devices**: This can also enable the development of smart home devices
    such as microwaves and thermostats with on-device intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power efficient**: An on-device ML can be more power-efficient as there is
    no need to transfer data back and forth to the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensor data utilization**: ML models can make use of rich sensor data since
    it is easily available on mobile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, mobile devices are not same as our desktops and laptops. There are
    different considerations when deploying the model on mobile or embedded devices
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model size**:As we know, mobiles have limited memory and we can''t store
    a memory-heavy model on a device. There are two ways of handling this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can round or quantize the weights of the model so that they require fewer
    floating-point representations. This is in line with our understanding that integers
    always require less memory to store than the floating point numbers.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we only use devices for inferences or predictions, we can strip out all
    the training operations in our Tensorflow graph which are not useful for making
    predictions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed**:One of the important things for deploying models on mobile devices
    is the speed with which we can run an inference so that we gain a better user
    experience. Models have to be optimized in such a manner that they don''t exceed
    the latency budget on the phone but are still fast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of deployment**: We need efficient frameworks/libraries so that deployment
    on mobile devices is very straightforward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these considerations in mind, Google has developed TensorFlow Lite, which
    is a lightweight version of original Tensorflow for deploying deep learning models
    on mobile and embedded devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand TensorFlow Lite, take a look at the following diagram, which
    shows its high-level architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ee6c212-fbed-4caf-b6fc-bd44dc9b4ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: This architecture makes it evident that we need to convert a trained TF model
    into `.tflite` format. This format is different from usual TF models as it is
    optimized for inference on devices. We will learn about the conversion process
    in detail later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let''s try to understand the major features of using TF Lite format:'
  prefs: []
  type: TYPE_NORMAL
- en: The model is serialized and converted to a Flatbuffer format([https://google.github.io/flatbuffers/](https://google.github.io/flatbuffers/)).
    Flatbuffers have the advantage that data can be directly accessed without parsing/unpacking
    of large files that contain weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights and biases of the model are pre-fused into TF lite format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF lite is cross-platform and can be deployed on Android, iOS, Linux, and hardware
    devices such as Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: It includes an on-device interpreter that has been optimized for faster execution
    on mobile. The core interpreter with all of the supported operations is around
    400 KB, and 75 KB without the supported operations. This means that the model
    takes up little space on the device. Overall, the idea is to keep the parts of
    the model that are essential for inference and strip out all the other parts.
  prefs: []
  type: TYPE_NORMAL
- en: With innovation in hardware, many companies are also developing GPUs and Digital
    Signal Processors (DSPs) that are optimized for neural network inference. TF Lite
    provides the Android Neural Networks API, which can perform hardware acceleration
    on these devices.
  prefs: []
  type: TYPE_NORMAL
- en: Classification Model Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just building a model does not suffice; we need to make sure that our model
    functions well and gives us a good and accurate output. To do this, we need to
    understand some classification metrics that will be used to evaluate the model
    throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by defining some building blocks of the metrics that will be used
    to evaluate the classification models. To do this, take a simple example of spam
    detection that is done by any online mailbox for reference. A spam email shall
    be considered to be of a positive class and the normal email to be of a negative
    class. We can summarize this spam detection model into four categories, which
    are illustrated in the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **True positives** (**TP**) | **False positives** (**FP**) |'
  prefs: []
  type: TYPE_TB
- en: '| Reality: Email is spam | Reality: Email is NOT spam |'
  prefs: []
  type: TYPE_TB
- en: '| Model Prediction: Email is spam | Model Prediction: Email is spam |'
  prefs: []
  type: TYPE_TB
- en: '| **False negatives** (**FN**) | **True negatives** (**TN**) |'
  prefs: []
  type: TYPE_TB
- en: '| Reality: Email is spam | Reality: Email is NOT spam |'
  prefs: []
  type: TYPE_TB
- en: '| Model Prediction: Email is NOT spam | Model Prediction: Email is NOT spam
    |'
  prefs: []
  type: TYPE_TB
- en: This matrix is also commonly known as the **confusion matrix.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The three major metrics that we will use to define classifier quality, primarily
    in an unbalanced dataset, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy: **Accuracy is the most basic metric used for classification problems.
    It is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8f86fe08-4b08-4072-ade1-c38090aa1b26.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Precision: **Precision tries to measure the true positives out of all predicted
    positives from the model. If your Gmail doesn''t misclassify a lot of emails from
    your friends (or normal emails) and put them into spam, then it has very high
    precision. It is mathematically represented as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f1baadc5-fcc9-4bcc-b39e-5fa42314b6d0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Recall: **Recall tries to measure the number of values classified as positive
    out of all real positives in the dataset. In simple terms, if your Gmail doesn''t
    misclassify a lot of your spam emails as normal emails and sends them to your
    inbox, then it has very high recall:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/aa450a95-27b8-4bc7-940c-50fd19ba4e4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Ideally, we want a model with high precision and high recall. However, there
    is always a trade-off between high precision and high recall in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying digits using TensorFlow Lite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete this project, we will use the MNIST digit dataset, which is available
    in the TensorFlow datasets library ([https://www.tensorflow.org/guide/datasets](https://www.tensorflow.org/guide/datasets)).
    It consists of images of handwritten digits from 0 to 9\. The training dataset
    has 60,000 images and the testing set has 10,000 images. Some of the images in
    the dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc9f7e6e-15de-4103-9800-449c941e283c.png)'
  prefs: []
  type: TYPE_IMG
- en: If we take a look at TensorFlow Lite tutorials, we will see that the focus is
    on using pre-trained models such as Mobilenet or retraining the existing ones.
    However, none of these tutorials talk about building new models, which is something
    we will be doing here.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we specifically choose a simple model because at the time of writing
    this book, TensorFlow Lite doesn't have adequate support for all types of complex
    models
  prefs: []
  type: TYPE_NORMAL
- en: We will use categorical cross entropy as the loss function for this classification
    problem. Categorical cross entropy was explained in detail in [Chapter 3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml), *Sentiment
    Analysis in Your Browser Using TensorFlow.js*, of this book. In this chapter,
    we have 10 different digits in the dataset, so we will use the categorical cross
    entropy over 10 classes.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing data and defining the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to pre-process our data for making it ready to feed into our model,
    define our model, and create an evaluation metric:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-process the data by ensuring the images are of shape 28x28x1 and converting
    the pixels into a float type variable for training. Also, here we define NUM_CLASSES
    = 10 as there are 10 different digits in the images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Define the model as having two convolutional layers with the same filter sizes, two
    fully connected layers, two dropout layers with dropout probabilities of 0.25
    and 0.5 respectively, a Rectified Linear (ReLU) after every fully connected or
    convolutional layer except the last one, and one max pool layer. Also we add a
    Softmax activation to convert the output of the model to probabilities for each
    of the 10 digits. Note that we use this model as it produces good results. You
    can try improving the model by adding more layers or trying different shapes of
    the existing layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have named our output tensor `softmax_tensor`, which will come
    in pretty handy when we try to convert this model into a TensorFlow Lite format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further define the following parameters for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss = Categorical Cross Entropy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizer = AdaDelta. Adam optimizer, defined in  [Chapter 3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml), *Sentiment
    Analysis in Your Browser Using TensorFlow.js*, is an extension of AdaDelta. We
    use AdaDelta as it gives good result for this model. You can find more details
    about AdaDelta in the original paper ([https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation Metric = Classification Accuracy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code for defining these is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable Tensorboard logging to visualize the model graph and training progress.
    Code is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model using the following parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Epochs = 12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch Size = 128:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We achieved **99.24% **accuracy on the test dataset with just 12 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we use the `callbacks` parameter to log the training progress on TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Converting TensorFlow model to TensorFlow Lite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have trained the model in the usual way, let's look at how we can
    convert this model into a TensorFlow Lite format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general procedure for conversion is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c787e53-0ca7-404e-82ad-051bff8c99c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The procedure is simple: we take a trained model, freeze the graph, optimize
    it for inference/prediction, and convert it into `.tflite` format. Before going
    further, let''s understand what we mean by Freeze Graph and Optimize For Inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Freeze Graph : **Freeze graph operation effectively freezes the weights of
    the model by converting all the of the TF Variables as Constants. As you can imagine,
    having all of the weights as constants can save space compared to keeping them
    as variables. As we only perform inference on mobile (and not training), we don''t
    want to change the model weights anyway'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimize For Inference**: Once the graph is frozen, we remove all the operations
    in the graph which are not useful for inference. For example, Dropout operation
    is used to train the model such that it doesn''t overfit. However, there is absolutely
    no use of this operation during prediction on mobile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the rest of this section, we will heavily use TensorBoard visualization
    ([https://www.TensorFlow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard))
    for graph visualization. :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have trained the model, you must have a file with the prefix `events.out.tfevents.` in
    your model folder. Go to the `logs` folder and type the following into terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: TensorBoard will start in port `6006` by default. Launch it by going to your
    browser and typing `localhost:6006` into the address bar*. *Once the Tensorboard
    opens, if you navigate to the Graphs tab at the top, you will be able to see the
    Tensorflow Graph of your model. In the following diagram, we illustrate the main
    graph, with annotations for the input tensor, output tensor, and training part
    of the graph. As we can see, we shouldn't keep anything from the training graph
    for inference/making predictions on mobile.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d23c048-2039-40ad-87e5-922447fddf5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Next implement a function `freeze_sesssion` which takes TF session as input,
    converts all variables into constants, and returns the frozen graph. After executing
    this function, you will obtain a frozen graph file named `MNIST_model.pb` in the `<model_folder>/logs/freeze` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, here is where it gets really strange: you can''t visualize the `MNIST_model.pb`
    file directly through TensorBoard. You need to write the graph in a format that
    TensorBoard can pick up. Execute the function `pb_to_tensorboard` mentioned below
    and you will see another file in `<model_folder>/logs/freeze` folder with prefix
    `events.out.tfevents`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, start TensorBoard again with `logdir` as `<model_folder>/logs/freeze` and
    visualize the frozen graph. You will observe the you have stripped out most of
    the variables from the graph. The following diagram illustrates the frozen graph
    you will obtain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8c2b389b-9f29-42ec-b89a-3baa069e4159.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next step is further trim to optimize it for inference. As explained before,
    we will remove Dropout variables from the graph as they are not useful for inference
    on mobile. However, there is no perfect way to remove those from the graph based
    on the existing TensorFlow functions/programs. The new improvements to TensorFlow
    Lite don''t work for this example, which suggests that they are still under development.
    Instead, you will have to manually specify the operations you want to remove and
    connect the input of the Dropout operations to the operations after them in the
    graph. For example, in the frozen graph, let''s say we want to remove the `dropout_3`
    operation. The following diagram shows the zoomed-in version of the frozen graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/001f621c-afb7-42cb-ba3c-42b2af5ae316.png)'
  prefs: []
  type: TYPE_IMG
- en: In such a case, you will have to connect the `max_pooling2` operation directly
    to the `flatten_2` operation, thereby skipping the `dropout_3` op in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Execute the function `optimize_graph` mentioned below to remove all of the dropout
    ops in the graph. It manually flushes out all the Dropout operations from the
    graph. This will result in a new file named `MNIST_optimized.pb` under the `<model_folder>/logs/optimized` folder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Again, to visualize the graph in TensorBoard, you need to convert it using the
    function `pb_to_tensorboar` defined in step 3 so that it's TensorBoard-friendly
    and obtain a new file with the prefix `events.out.tfevents` in the same folder.
    The following figure illustrates the graph you will obtain after removing the
    Dropout operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/30c933b9-110b-4297-8945-76860bc3e017.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that getting rid of Dropout from the graph will not affect testing set
    accuracy as Dropout is not used for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step in obtaining the model in a mobile-friendly format is to convert
    it into a `.tflite` file. For this step, you will use `toco` command, which stands
    for TensorFlow Lite Optimizing Converter ([https://www.tensorflow.org/lite/convert/](https://www.tensorflow.org/lite/convert/)).
    The code is provided as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This will produce a file named `mnist.tflite` in `<model_folder>`. Essentially,
    this step is trying to convert the optimized graph into a Flatbuffer for efficient
    on-device inference.
  prefs: []
  type: TYPE_NORMAL
- en: We will not cover deploying our project to the mobile device as its development
    is outside the scope of this book. However, feel free to take a look at TensorFlow
    Lite tutorials on how to deploy the TF Lite models Android ([https://www.tensorflow.org/lite/demo_android](https://www.tensorflow.org/lite/demo_android))
    or iOS ([https://www.tensorflow.org/lite/demo_ios](https://www.tensorflow.org/lite/demo_ios)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is at the edge of the next wave, where we try to make ML ubiquitous
    in our everyday life. It has several advantages such as offline access, data privacy,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we looked at a new library from Google known as TensorFlow
    Lite, which has been optimized for deploying ML models on mobile and embedded
    devices. We understood the architecture of TensorFlow Lite, which converts the
    trained TensorFlow model into `.tflite` format. This is designed for inference
    at fast speed and low memory on devices. TensorFlow Lite also supports multiple
    platforms, such as Android, iOS, Linux, and Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we used the MNIST handwritten digit dataset to train a deep learning
    model. Subsequently, we followed the necessary steps to convert the trained model
    into `.tflite` format. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Froze the graph with variables converted to constants
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimized the graph for inference by removing the unused ops like Dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Used **TensorFlow Optimization Converter Tool** (**toco**) to convert the optimized
    model to `.tflitte` format
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At every step, we used TensorBoard to visualize the state of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very exciting field that is continuing to evolve, both in terms of
    hardware and software. Once this technology reaches maturity, it will open up
    new use cases and business models across the world.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll create a project that will help us convert text to
    speech.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How is TensorFlow Lite different from usual TensorFlow?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you try and if you can build the model on a movie review dataset in  [Chapter
    3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml), *Sentiment Analysis in Your Browser
    Using TensorFlow.js*? Do you face some issues with Tensorflow Lite in that case?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you try Adam optimizer and see if it improves the performance of the model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you think you operations other than Dropout which are also not important
    for inference on mobile?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
