- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Graph Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图神经网络
- en: In this chapter, we will look at a relatively new class of neural networks,
    the **Graph Neural Network** (**GNN**), which is ideally suited for processing
    graph data. Many real-life problems in areas such as social media, biochemistry,
    academic literature, and many others are inherently “graph-shaped,” meaning that
    their inputs are composed of data that can best be represented as graphs. We will
    cover what graphs are from a mathematical point of view, then explain the intuition
    behind “graph convolutions,” the main idea behind GNNs. We will then describe
    a few popular GNN layers that are based on variations of the basic graph convolution
    technique. We will describe three major applications of GNNs, covering node classification,
    graph classification, and edge prediction, with examples using TensorFlow and
    the **Deep Graph Library** (**DGL**). DGL provides the GNN layers we have just
    mentioned plus many more. In addition, it also provides some standard graph datasets,
    which we will use in the examples. Following on, we will show how you could build
    a DGL-compatible dataset from your own data, as well as your own layer using DGL’s
    low-level message-passing API. Finally, we will look at some extensions of graphs,
    such as heterogeneous graphs and temporal graphs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一种相对较新的神经网络类别——**图神经网络**（**GNN**），它非常适合处理图数据。许多现实生活中的问题，如社交媒体、生物化学、学术文献等，天生就是“图形化”的，意味着它们的输入由可以最适合用图表示的数据组成。我们将从数学角度讲解什么是图，然后解释“图卷积”这一概念，这是GNN的核心思想。接着，我们将介绍一些基于基本图卷积技术变体的流行GNN层。我们将描述GNN的三个主要应用，涵盖节点分类、图分类和边预测，并通过使用TensorFlow和**深度图书馆**（**DGL**）的示例来说明。DGL提供了我们刚刚提到的GNN层以及更多的层。此外，它还提供了一些标准的图数据集，我们将在示例中使用这些数据集。随后，我们将展示如何从自己的数据构建一个与DGL兼容的数据集，以及如何使用DGL的低级消息传递API构建自己的层。最后，我们将探讨图的扩展，例如异构图和时间图。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Graph basics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的基础
- en: Graph machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图机器学习
- en: Graph convolutions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图卷积
- en: Common graph layers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的图层
- en: Common graph applications
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的图应用
- en: Graph customizations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的定制
- en: Future directions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来方向
- en: All the code files for this chapter can be found at https://packt.link/dltfchp17
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在 https://packt.link/dltfchp17 找到
- en: Let’s begin with the basics.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础开始。
- en: Graph basics
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的基础
- en: 'Mathematically speaking, a graph *G* is a data structure consisting of a set
    of vertices (also called nodes) *V*, connected to each other by a set of edges
    *E*, i.e:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，一个图*G*是一个数据结构，包含一组顶点（也叫节点）*V*，这些顶点通过一组边*E*相互连接，即：
- en: '![](img/B18331_17_001.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_001.png)'
- en: A graph can be equivalently represented as an adjacency matrix *A* of size (*n*,
    *n*) where *n* is the number of vertices in the set *V*. The element *A[I, j]*
    of this adjacency matrix represents the edge between vertex *i* and vertex *j*.
    Thus the element *A[I, j] = 1* if there is an edge between vertex *i* and vertex
    *j*, and 0 otherwise. In the case of weighted graphs, the edges might have their
    own weights, and the adjacency matrix will reflect that by setting the edge weight
    to the element *A[i, j]*. Edges may be directed or undirected. For example, an
    edge representing the friendship between a pair of nodes *x* and *y* is undirected,
    since *x* is friends with *y* implies that *y* is friends with *x*. Conversely,
    a directed edge can be one in a follower network (social media), where *x* following
    *y* does not imply that *y* follows *x*. For undirected graphs, *A[I, j] = A[j,
    i]*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图可以等价地表示为一个邻接矩阵*A*，其大小为（*n*, *n*），其中*n*是集合*V*中顶点的数量。该邻接矩阵的元素*A[I, j]*表示顶点*i*和顶点*j*之间的边。因此，若顶点*i*和顶点*j*之间有一条边，则元素*A[I,
    j] = 1*，否则为0。在加权图的情况下，边可能有自己的权重，邻接矩阵会通过将边的权重设置为元素*A[i, j]*来反映这一点。边可以是有向的或无向的。例如，表示节点*x*和节点*y*之间友谊的边是无向的，因为*x*是*y*的朋友意味着*y*也是*x*的朋友。相反，有向边可以是社交媒体中的关注网络，在这种情况下，*x*关注*y*并不意味着*y*关注*x*。对于无向图，*A[I,
    j] = A[j, i]*。
- en: Another interesting property of the adjacency matrix *A* is that *A*^n, i.e.,
    the product of *A* taken *n* times, exposes *n*-hop connections between nodes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接矩阵*A*的另一个有趣特性是，*A*^n，即* A*的* n*次乘积，揭示了节点之间的*n*跳连接。
- en: The graph-to-matrix equivalence is bi-directional, meaning the adjacency matrix
    can be converted back to the graph representation without any loss of information.
    Since **Machine Learning** (**ML**) methods, including **Deep Learning** (**DL**)
    methods, consume input data in the form of tensors, this equivalence means that
    graphs can be efficiently represented as inputs to all kinds of machine learning
    algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图到矩阵的等价性是双向的，这意味着邻接矩阵可以无损地转换回图的表示。由于**机器学习**（**ML**）方法，包括**深度学习**（**DL**）方法，消耗的输入数据是张量形式，因此这种等价性意味着图形可以有效地作为各种机器学习算法的输入表示。
- en: Each node can also be associated with its own feature vector, much like records
    in tabular input. Assuming a feature vector of size *f*, the set of nodes *X*
    can be represented as *(n, f)*. It is also possible for edges to have their own
    feature vectors. Because of the equivalence between graphs and matrices, graphs
    are usually represented by libraries as efficient tensor-based structures. We
    will examine this in more detail later in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点还可以与其自己的特征向量关联，就像表格输入中的记录一样。假设特征向量的大小为*f*，那么节点集*X*可以表示为*(n, f)*。边也可以有自己的特征向量。由于图和矩阵之间的等价性，图通常由库表示为高效的基于张量的结构。我们将在本章后面详细讨论这一点。
- en: Graph machine learning
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图形机器学习
- en: 'The goal of any ML exercise is to learn a mapping *F* from an input space *X*
    to an output space *y*. Early machine learning methods required feature engineering
    to define the appropriate features, whereas DL methods can infer the features
    from the training data itself. DL works by hypothesizing a model *M* with random
    weights ![](img/B18331_17_002.png), formulating the task as an optimization problem
    over the parameters ![](img/B18331_17_003.png):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习任务的目标都是学习从输入空间*X*到输出空间*y*的映射*F*。早期的机器学习方法需要特征工程来定义合适的特征，而深度学习方法则可以从训练数据本身推断特征。深度学习通过假设一个具有随机权重的模型*M*来工作！[](img/B18331_17_002.png)，并将任务表述为一个关于参数![](img/B18331_17_003.png)的优化问题：
- en: '![](img/B18331_17_004.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_004.png)'
- en: 'and using gradient descent to update the model weights over multiple iterations
    until the parameters converge:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用梯度下降法在多次迭代中更新模型权重，直到参数收敛：
- en: '![](img/B18331_17_005.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_005.png)'
- en: Not surprisingly, GNNs follow this basic model as well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，图神经网络（**GNNs**）也遵循这一基本模型。
- en: However, as you have seen in previous chapters, ML and DL are often optimized
    for specific structures. For example, you might instinctively choose a simple
    **FeedForward Network** (**FFN**) or “dense” network when working with tabular
    data, a **Convolutional Neural Network** (**CNN**) when dealing with image data,
    and a **Recurrent Neural Network** (**RNN**) when dealing with sequence data like
    text or time series. Some inputs may reduce to simpler structures such as pixel
    lattices or token sequences, but not necessarily so. In their natural form, graphs
    are topologically complex structures of indeterminate size and are not permutation
    invariant (i.e., instances are not independent of each other).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你在前几章中所看到的，机器学习（**ML**）和深度学习（**DL**）通常是针对特定结构进行优化的。例如，在处理表格数据时，你可能会直观地选择一个简单的**前馈网络**（**FFN**）或“密集”网络，在处理图像数据时选择**卷积神经网络**（**CNN**），而在处理像文本或时间序列这样的序列数据时选择**递归神经网络**（**RNN**）。有些输入可能简化为像素格子或令牌序列这样的结构，但也不一定如此。在其自然形式下，图形是拓扑复杂、大小不确定的结构，并且不是置换不变的（即实例之间不是相互独立的）。
- en: For these reasons, we need special tooling to deal with graph data. We will
    introduce in this chapter the DGL, a cross-platform graph library that supports
    users of MX-Net, PyTorch, and TensorFlow through the use of a configurable backend
    and is widely considered one of the most powerful and easy-to-use graph libraries
    available.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，我们需要特殊的工具来处理图数据。本章将介绍DGL，它是一个跨平台的图形库，支持MX-Net、PyTorch和TensorFlow用户，通过使用可配置的后端，广泛被认为是最强大且易于使用的图形库之一。
- en: Graph convolutions – the intuition behind GNNs
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图卷积——图神经网络的直觉
- en: The convolution operator, which effectively allows values of neighboring pixels
    on a 2D plane to be aggregated in a specific way, has been successful in deep
    neural networks for computer vision. The 1-dimensional variant has seen similar
    success in natural language processing and audio processing as well. As you will
    recall from *Chapter 3*, *Convolutional Neural Networks*, a network applies convolution
    and pooling operations across successive layers and manages to learn enough global
    features across a sufficiently large number of input pixels to succeed at the
    task it is trained for.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积算子有效地允许在二维平面上将相邻像素的值以特定方式聚合，这在计算机视觉中的深度神经网络中取得了成功。其一维变体在自然语言处理和音频处理领域也取得了类似的成功。正如你在*第3章*《卷积神经网络》中回忆的那样，网络在连续的层之间应用卷积和池化操作，并能够学习到足够多的全局特征，从而在它所训练的任务中获得成功。
- en: Examining the analogy from the other end, an image (or each channel of an image)
    can be thought of as a lattice-shaped graph where neighboring pixels link to each
    other in a specific way. Similarly, a sequence of words or audio signals can be
    thought of as another linear graph where neighboring tokens are linked to each
    other. In both cases, the deep learning architecture progressively applies convolutions
    and pooling operations across neighboring vertices of the input graph until it
    learns to perform the task, which is generally classification. Each convolution
    step encompasses an additional level of neighbors. For example, the first convolution
    merges signals from distance 1 (immediate) neighbors of a node, the second merges
    signals from distance 2 neighbors, and so on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，图像（或图像的每个通道）可以被视为一种网格形状的图，其中相邻的像素以特定的方式彼此连接。类似地，一串单词或音频信号也可以被看作是另一个线性图，其中相邻的词元彼此相连。在这两种情况下，深度学习架构会在输入图的相邻节点之间逐步应用卷积和池化操作，直到它学会执行任务，通常是分类任务。每一步卷积都涉及额外层次的邻居。例如，第一个卷积合并来自距离1（直接）邻居的信号，第二个合并来自距离2的邻居的信号，以此类推。
- en: '*Figure 17.1* shows the equivalence between a 3 x 3 convolution in a CNN and
    the corresponding “graph convolution” operation. The convolution operator applies
    the filter, essentially a set of nine learnable model parameters, to the input
    and combines them via a weighted sum. You can achieve the same effect by treating
    the pixel neighborhood as a graph of nine nodes centered around the middle pixel.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.1* 显示了CNN中的3 x 3卷积与对应的“图卷积”操作之间的等价性。卷积算子将滤波器（本质上是一组九个可学习的模型参数）应用于输入，并通过加权和将它们合并。通过将像素邻域视为一个以中心像素为核心的九个节点的图，你可以达到相同的效果。'
- en: 'A graph convolution on such a structure would just be a weighted sum of the
    node features, the same as the convolution operator in the CNN:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种结构上的图卷积将只是节点特征的加权和，这与CNN中的卷积算子相同：
- en: '![Diagram  Description automatically generated](img/B18331_17_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图示 自动生成的说明](img/B18331_17_01.png)'
- en: 'Figure 17.1: Parallels between convolutions in images and convolutions in graphs.
    Image source: CS-224W machine learning with Graphs, Stanford Univ.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：图像卷积和图卷积之间的相似之处。图像来源：CS-224W机器学习与图，斯坦福大学。
- en: 'The corresponding equations for the convolution operation on the CNN and the
    graph convolution are shown below. As you can see, on CNN, the convolution can
    be considered as a weighted linear combination of the input pixel and each of
    its neighbors. Each pixel brings its own weight in the form of the filter being
    applied. On the other hand, the graph convolution is also a weighted linear combination
    of the input pixel and an aggregate of all its neighbors. The aggregate effect
    of all neighbors is averaged into the convolution output:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中的卷积操作和图卷积的相应方程如下所示。正如你所看到的，在CNN中，卷积可以视为输入像素和它的每个邻居的加权线性组合。每个像素都以所应用的滤波器的形式带来了自己的权重。另一方面，图卷积也是输入像素和所有邻居的聚合加权线性组合。所有邻居的聚合效应会平均到卷积输出中：
- en: '![](img/B18331_17_006.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_006.png)'
- en: '![](img/B18331_17_007.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_007.png)'
- en: Graph convolutions are thus a variation of convolutions that we are already
    familiar with. In the following section, we will see how these convolutions can
    be composed to build different kinds of GCN layers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，图卷积是我们已经熟悉的卷积的一种变体。在接下来的部分中，我们将看到如何将这些卷积组合起来构建不同类型的GCN层。
- en: Common graph layers
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的图层
- en: All the graph layers that we discuss in this section use some variation of the
    graph convolution operation described above. Contributors to graph libraries such
    as DGL provide prebuilt versions of many of these layers within a short time of
    it being proposed in an academic paper, so you will realistically never have to
    implement one of these. The information here is mainly for understanding how things
    work under the hood.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论的所有图层都使用了上述描述的图卷积操作的某种变体。像DGL这样的图库贡献者，在学术论文提出这些层之后不久，就会提供许多这些层的预构建版本，因此实际上你永远不需要自己实现其中的一个。这部分信息主要是为了帮助理解其底层工作原理。
- en: Graph convolution network
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图卷积网络
- en: The **Graph Convolution Network** (**GCN**) is the graph convolution layer proposed
    by Kipf and Welling [1]. It was originally presented as a scalable approach for
    semi-supervised learning on graph-structured data. They describe the GCN as an
    operation over the node feature vectors *X* and the adjacency matrix *A* of the
    underlying graph and point out that this can be exceptionally powerful when the
    information in *A* is not present in the data *X*, such as citation links between
    documents in a citation network, or relations in a knowledge graph.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**图卷积网络**（**GCN**）是由Kipf和Welling提出的图卷积层[1]。最初，它被提出作为一种可扩展的半监督学习方法，用于图结构数据上。他们将GCN描述为对节点特征向量*X*和底层图的邻接矩阵*A*的操作，并指出当*A*中的信息不包含在数据*X*中时，这种方法特别强大，例如在引文网络中，文档之间的引文链接，或在知识图谱中的关系。'
- en: 'GCNs combine the value of each node’s feature vector with those of its neighbors
    using some weights (initialized to random values). Thus, for every node, the sum
    of the neighboring node’s features is added. This operation can be represented
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GCN结合了每个节点特征向量与其邻居的特征向量，通过一些权重（初始化为随机值）进行加权。因此，对于每个节点，邻居节点的特征之和会被加到一起。这个操作可以表示为如下：
- en: '![](img/B18331_17_008.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_008.png)'
- en: Here the *update* and *aggregate* are different kinds of summation functions.
    This sort of projection on node features is called a message-passing mechanism.
    A single iteration of this message passing is equivalent to a graph convolution
    over each node’s immediate neighbors. If we wish to incorporate information from
    more distant nodes, we can repeat this operation several times.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*update*和*aggregate*是不同类型的求和函数。这种对节点特征的投影被称为消息传递机制。这个消息传递的单次迭代等同于对每个节点的直接邻居进行图卷积。如果我们希望结合来自更远节点的信息，可以多次重复这个操作。
- en: 'The following equation describes the output of the GCN at layer *(l+1)* at
    node *i*. Here, *N(i)* is the set of neighbors of node *I* (including itself),
    *c*[ij] is the product of the square root of node degrees, and sigma is an activation
    function. The *b(l)* term is an optional bias term:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程描述了GCN在第*(l+1)*层对节点*i*的输出。这里，*N(i)*是节点*i*的邻居集合（包括它本身），*c*[ij]是节点度数平方根的乘积，sigma是激活函数。*b(l)*项是一个可选的偏置项：
- en: '![](img/B18331_17_009.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_17_009.png)'
- en: Next up, we will look at the graph attention network, a variant of the GCN where
    the coefficients are learned via an attentional mechanism instead of being explicitly
    defined.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论图注意力网络（Graph Attention Network，简称GAT），它是GCN的一个变体，其中系数是通过注意力机制学习的，而不是显式定义的。
- en: Graph attention network
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图注意力网络
- en: The **Graph Attention Network** (**GAT**) layer was proposed by Velickovic,
    et al. [2]. Like the GCN, the GAT performs local averaging of its neighbors’ features.
    The difference is instead of explicitly specifying the normalization term *c*[ij],
    the GAT allows it to be learned using self-attention over the node features to
    do so. The corresponding normalization term is written as ![](img/B18331_11_021.png)
    for the GAT, which is computed based on the hidden features of the neighboring
    nodes and the learned attention vector. Essentially, the idea behind the GAT is
    to prioritize feature signals from similar neighbor nodes compared to dissimilar
    ones.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**图注意力网络**（**GAT**）层是Velickovic等人提出的[2]。与GCN类似，GAT也对其邻居的特征进行局部平均。不同之处在于，GAT不是显式指定归一化项*c*[ij]，而是通过自注意力机制在节点特征上学习它。对应的归一化项写作![](img/B18331_11_021.png)，它是基于邻居节点的隐藏特征和学习到的注意力向量计算出来的。本质上，GAT的理念是优先考虑来自相似邻居节点的特征信号，而非来自不相似邻居节点的信号。'
- en: 'Every neighbor ![](img/B18331_17_011.png) neighborhood *N*(*i*) of node *i*
    sends its own vector of attentional coefficients ![](img/B18331_17_012.png). The
    following set of equations describes the output of the GAT at layer (*i+1*) for
    node *i*. The attention ![](img/B18331_11_021.png) is computed using Bahdanau’s
    attention model using a feedforward network:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_17_014.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_17_015.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_17_016.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: GCN and GAT architectures are suitable for small to medium-sized networks. The
    GraphSAGE architecture, described in the next section, is more suitable for larger
    networks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: GraphSAGE (sample and aggregate)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, the convolutions we have considered require that all nodes in the graph
    be present during the training, and are therefore transductive and do not naturally
    generalize to unseen nodes. Hamilton, Ying, and Leskovec [3] proposed GraphSAGE,
    a general, inductive framework that can generate embeddings for previously unseen
    nodes. It does so by sampling and aggregating from a node’s local neighborhood.
    GraphSAGE has proved successful at node classification on temporally evolving
    networks such as citation graphs and Reddit post data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: GraphSAGE samples a subset of neighbors instead of using them all. It can define
    a node neighborhood using random walks and sum up importance scores to determine
    the optimum sample. An aggregate function can be one of MEAN, GCN, POOL, and LSTM.
    Mean aggregation simply takes the element-wise mean of the neighbor vectors. The
    LSTM aggregation is more expressive but is inherently sequential and not symmetric;
    it is applied on an unordered set derived from a random permutation of the node’s
    neighbors. The POOL aggregation is both symmetric and trainable; here, each neighbor
    vector is independently fed through a fully connected neural network and max pooling
    is applied across the aggregate information across the neighbor set.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'This set of equations shows how the output for node *i* at layer *(l+1)* is
    generated from node *i* and its neighbors *N(i)* at layer *l*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_17_017.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_17_018.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_17_019.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Now that we have seen strategies for handling large networks using GNNs, we
    will look at strategies for maximizing the representational (and therefore the
    discriminative) power of GNNs, using the graph isomorphism network.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Graph isomorphism network
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Xu, et al. [4] proposed the **Graph Isomorphism Network** (**GIN**) as a graph
    layer with more expressive power compared to the ones available. Graph layers
    with high expressive power should be able to distinguish between a pair of graphs
    that are topologically similar but not identical. They showed that GCNs and GraphSAGE
    are unable to distinguish certain graph structures. They also showed that SUM
    aggregation is better than MEAN and MAX aggregation in terms of distinguishing
    graph structures. The GIN layer thus provides a better way to represent neighbor’s
    aggregation compared to GCNs and GraphSAGE.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equation shows the output at node *i* and layer *(l+1)*. Here,
    the function *f*[θ] is a callable activation function, *aggregate* is an aggregation
    function such as SUM, MAX, or MEAN, and ![](img/B18331_10_003.png) is a learnable
    parameter that will be learned over the course of the training:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_17_021.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Having been introduced to several popular GNN architectures, let us now direct
    our attention to the kind of tasks we can do with GNNs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Common graph applications
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now look at some common applications of GNNs. Typically, applications
    fall into one of the three major classes listed below. In this section, we will
    see code examples on how to build and train GNNs for each of these tasks, using
    TensorFlow and DGL:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Node classification
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph classification
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge classification (or link prediction)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other applications of GNNs as well, such as graph clustering or generative
    graph models, but they are less common and we will not consider them here.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Node classification
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Node classification is a popular task on graph data. Here, a model is trained
    to predict the node category. Non-graph classification methods can use the node
    feature vectors alone to do so, and some pre-GNN methods such as DeepWalk and
    node2vec can use the adjacency matrix alone, but GNNs are the first class of techniques
    that can use both the node feature vectors and the connectivity information together
    to do node classification.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, the idea is to apply one or more graph convolutions (as described
    in the previous section) to all nodes of a graph, to project the feature vector
    of the node to a corresponding output category vector that can be used to predict
    the node category. Our node classification example will use the CORA dataset,
    a collection of 2,708 scientific papers classified into one of seven categories.
    The papers are organized into a citation network, which contains 5,429 links.
    Each paper is described by a word vector of size 1,433.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'We first set up our imports. If you have not already done so, you will need
    to install the DGL library into your environment with `pip install dgl`. You will
    also need to set the environment variable `DGLBACKEND` to TensorFlow. On the command
    line, this is achieved by the command `export DGLBACKEND=tensorflow`, and in a
    notebook environment, you can try using the magic `%env DGLBACKEND=tensorflow`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The CORA dataset is pre-packaged as a DGL dataset, so we load the dataset into
    memory using the following call:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The first time this is called, it will log that it is downloading and extracting
    to a local file. Once done, it will print out some useful statistics about the
    CORA dataset. As you can see, there are 2,708 nodes and 10,566 edges in the graph.
    Each node has a feature vector of size 1,433 and a node is categorized as being
    in one of seven classes. In addition, we see that it has 140 training samples,
    500 validation samples, and 1,000 test samples:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since this is a graph dataset, it is expected to contain data pertaining to
    a set of graphs. However, CORA is a single citation graph. You can verify this
    by `len(dataset)`, which will give you `1`. This also means that downstream code
    will work on the graph given by `dataset[0]` rather than on the complete dataset.
    The node features will be contained in the dictionary `dataset[0].ndata` as key-value
    pairs, and the edge features in `dataset[0].edata`. The `ndata` contains the keys
    `train_mask`, `val_mask`, and `test_mask`, which are Boolean masks signifying
    which nodes are part of the train, validation, and test splits, respectively,
    and a `feat` key, which contains the feature vector for each node in the graph.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个图数据集，它预计包含与一组图相关的数据。然而，CORA是一个单一的引用图。你可以通过`len(dataset)`验证这一点，它将返回`1`。这也意味着下游代码将作用于由`dataset[0]`提供的图，而不是整个数据集。节点特征将包含在字典`dataset[0].ndata`中作为键值对，边特征则包含在`dataset[0].edata`中。`ndata`包含`train_mask`、`val_mask`和`test_mask`键，这些布尔掩码表示哪些节点属于训练集、验证集和测试集，另有一个`feat`键，其中包含图中每个节点的特征向量。
- en: We will build a `NodeClassifier` network with two `GraphConv` layers. Each layer
    will compute a new node representation by aggregating neighbor information. `GraphConv`
    layers are just simple `tf.keras.layers.Layer` objects and can therefore be stacked.
    The first `GraphConv` layer projects the incoming feature size (1,433) to a hidden
    feature vector of size 16, and the second `GraphConv` layer projects the hidden
    feature vector to an output category vector of size 2, from which the category
    is read.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个包含两个`GraphConv`层的`NodeClassifier`网络。每一层将通过聚合邻居信息来计算新的节点表示。`GraphConv`层只是简单的`tf.keras.layers.Layer`对象，因此可以堆叠。第一层`GraphConv`将输入特征的大小（1,433）映射到一个大小为16的隐藏特征向量，第二层`GraphConv`将隐藏特征向量映射到一个大小为2的输出类别向量，从中可以读取类别。
- en: 'Note that `GraphConv` is just one of many graph layers that we can drop into
    the `NodeClassifier` model. DGL makes available a variety of graph convolution
    layers that can be used to replace `GraphConv` if needed:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`GraphConv`只是我们可以放入`NodeClassifier`模型中的众多图层之一。DGL提供了多种图卷积层，如果需要，可以用它们替换`GraphConv`：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will train this model with the CORA dataset using the code shown below. We
    will use the `AdamW` optimizer (a variation of the more popular `Adam` optimizer
    that results in models with better generalization capabilities), with a learning
    rate of *1e-2* and weight decay of *5e-4*. We will train for 200 epochs. Let us
    also detect if we have a GPU available, and if so, assign the graph to the GPU.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码在CORA数据集上训练该模型。我们将使用`AdamW`优化器（这是更流行的`Adam`优化器的变种，能够使模型具有更好的泛化能力），学习率设置为*1e-2*，权重衰减为*5e-4*。我们将训练200个周期。我们还将检测是否有可用的GPU，如果有，我们会将图分配给GPU。
- en: 'TensorFlow will automatically move the model to the GPU if the GPU is detected:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检测到GPU，TensorFlow会自动将模型移动到GPU上：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We also define a `do_eval()` method that computes the accuracy given the features
    and the Boolean mask for the split being evaluated:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个`do_eval()`方法，它通过给定特征和用于评估拆分的布尔掩码来计算准确度：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we are ready to set up and run our training loop as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好按照如下方式设置并运行我们的训练循环：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the training run shows the training loss decreasing from `1.9`
    to `0.02` and the validation accuracy increasing from `0.13` to `0.78`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 训练运行的输出显示训练损失从`1.9`下降到`0.02`，验证准确率从`0.13`上升到`0.78`：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now evaluate our trained node classifier against the hold-out test split:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以评估我们训练的节点分类器在保留测试集上的表现：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This prints out the overall accuracy of the model against the hold-out test
    split:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出模型在保留测试集上的总体准确度：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Graph classification
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图分类
- en: Graph classification is done by predicting some attribute of the entire graph
    by aggregating all node features and applying one or more graph convolutions to
    it. This could be useful, for example, when trying to classify molecules during
    drug discovery as having a particular therapeutic property. In this section, we
    will showcase graph classification using an example.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图分类是通过聚合所有节点特征并对其应用一个或多个图卷积来预测整个图的某个属性完成的。例如，在药物发现过程中，当试图将分子分类为具有某种特定治疗特性时，这可能会很有用。在本节中，我们将通过一个示例展示图分类。
- en: 'In order to run the example, please make sure DGL is installed and set to use
    the TensorFlow backend; refer to the previous section on node classification for
    information on how to do this. To begin the example, let us import the necessary
    libraries:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行该示例，请确保已经安装DGL并设置为使用TensorFlow后端；有关如何操作的信息，请参阅上一节中的节点分类部分。要开始示例，请导入必要的库：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will use the protein dataset from DGL. The dataset is a set of graphs, each
    with node features and a single label. Each graph represents a protein molecule
    and each node in the graph represents an atom in the molecule. Node features list
    the chemical properties of the atom. The label indicates if the protein molecule
    is an enzyme:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用DGL提供的蛋白质数据集。该数据集是一组图，每个图都有节点特征和一个标签。每个图表示一个蛋白质分子，图中的每个节点表示分子中的一个原子。节点特征列出了原子的化学性质。标签表示该蛋白质分子是否是酶：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The call above downloads the protein dataset locally and prints out some information
    about the dataset. As you can see, each node has a feature vector of size `3`,
    the number of graph categories is `2` (enzyme or not), and the number of graphs
    in the dataset is `1113`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的调用会将蛋白质数据集下载到本地，并打印出一些数据集的信息。如您所见，每个节点的特征向量大小为`3`，图的类别数量为`2`（酶或非酶），数据集中的图数量为`1113`：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will first split the dataset into training, validation, and test. We will
    use the training dataset to train our GNN, validate using the validation dataset,
    and publish the results of our final model against the test dataset:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先把数据集分为训练集、验证集和测试集。我们将使用训练集来训练我们的GNN，使用验证集进行验证，并在测试集上发布最终模型的结果：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This splits the dataset into a training, validation, and test split of 801,
    89, and 223 graphs, respectively. Since our datasets are large, we need to train
    our network using mini-batches so as not to overwhelm GPU memory. So, this example
    will also demonstrate mini-batch processing using our data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将数据集分成训练集、验证集和测试集，分别包含801、89和223个图。由于我们的数据集很大，我们需要使用小批量来训练网络，以免占满GPU内存。因此，本示例还将展示如何使用我们的数据进行小批量处理。
- en: 'Next, we define our GNN for graph classification. This consists of two `GraphConv`
    layers stacked together that will encode the nodes into their hidden representations.
    Since the objective is to predict a single category for each graph, we need to
    aggregate all the node representations into a graph-level representation, which
    we do by averaging the node representations using `dgl.mean_nodes()`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义用于图分类的GNN。它由两个`GraphConv`层堆叠而成，这些层将节点编码为它们的隐藏表示。由于目标是为每个图预测一个单一类别，我们需要将所有节点表示聚合为图级表示，我们通过使用`dgl.mean_nodes()`平均节点表示来实现：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For the training, we set the training parameters and the `do_eval()` function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们设置了训练参数和`do_eval()`函数：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we define and run our training loop to train our `GraphClassifier`
    model. We use the `Adam` optimizer with a learning rate of `1e-2` and the `SparseCategoricalCrossentropy`
    as the loss function, training, or `20` epochs:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义并运行我们的训练循环来训练`GraphClassifier`模型。我们使用`Adam`优化器，学习率为`1e-2`，损失函数为`SparseCategoricalCrossentropy`，进行`20`轮训练：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output shows that the loss decreases and validation accuracy increases
    as the `GraphClassifier` model is trained over 20 epochs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，随着`GraphClassifier`模型训练了20轮，损失逐渐下降，验证准确度逐渐提高：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we evaluate the trained model against our hold-out test dataset:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在保留的测试数据集上评估训练好的模型：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This prints out the accuracy of the trained `GraphClassifier` model against
    the held-out test split:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印出训练好的`GraphClassifier`模型在保留的测试集上的准确度：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The accuracy shows that the model can successfully identify a molecule as an
    enzyme or non-enzyme slightly less than 70% of the time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度显示该模型可以成功地识别一个分子是酶还是非酶的概率略低于70%。
- en: Link prediction
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接预测
- en: Link prediction is a type of edge classification problem, where the task is
    to predict if an edge exists between two given nodes in the graph.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 链接预测是一种边分类问题，任务是预测图中两个给定节点之间是否存在边。
- en: Many applications, such as social recommendation, knowledge graph completion,
    etc., can be formulated as link prediction, which predicts whether an edge exists
    between a pair of nodes. In this example, we will predict if a citation relationship,
    either citing or cited, exists between two papers in a citation network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The general approach would be to treat all edges in the graph as positive examples
    and sample a number of non-existent edges as negative examples and train the link
    prediction classifier for binary classification (edge exists or not) on these
    positive and negative examples.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Before running the example, please make sure DGL is installed and set to use
    the TensorFlow backend; refer to the *Node classification* section for information
    on how to do this. Let us start by importing the necessary libraries:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For our data, we will reuse the CORA citation graph from the DGL datasets that
    we had used for our node classification example earlier. We already know what
    the dataset looks like, so we won’t dissect it again here. If you would like to
    refresh your memory, please refer to the node classification example for the relevant
    details:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let us prepare our data. For training our link prediction model, we need
    a set of positive edges and a set of negative edges. Positive edges are one of
    the 10,556 edges that already exist in the CORA citation graph, and negative edges
    are going to be 10,556 node pairs without connecting edges sampled from the rest
    of the graph. In addition, we need to split both the positive and negative edges
    into training, validation, and test splits:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We now construct a GNN that will compute the node representation using two
    `GraphSAGE` layers, each layer computing the node representation by averaging
    its neighbor information:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'However, link prediction requires us to compute representations of pairs of
    nodes, DGL recommends that you treat the pairs of nodes as another graph since
    you can define a pair of nodes as an edge. For link prediction, we will have a
    positive graph containing all the positive examples as edges, and a negative graph
    containing all the negative examples as edges. Both positive and negative graphs
    contain the same set of nodes as the original graph:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we will define a predictor class that will take the set of node representations
    from the `LinkPredictor` class and use the `DGLGraph.apply_edges` method to compute
    edge feature scores, which are the dot product of the source node features and
    the destination node features (both output together from the `LinkPredictor` in
    this case):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can also build a custom predictor such as a multi-layer perceptron with
    two dense layers, as the following code shows. Note that the `apply_edges` method
    describes how the edge score is calculated:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We instantiate the `LinkPredictor` model we defined earlier, select the `Adam`
    optimizer, and declare our loss function to be `BinaryCrossEntropy` (since our
    task is binary classification). The predictor head that we will use in our example
    is the `DotProductPredictor`. However, the `MLPPredictor` can be used as a drop-in
    replacement instead; just replace the `pred` variable below to point to the `MLPPredictor`
    instead of the `DotProductPredictor`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We also define a couple of convenience functions for our training loop. The
    first one computes the loss between the scores returned from the positive graph
    and the negative graphs, and the second computes the **Area Under the Curve**
    (**AUC**) from the two scores. AUC is a popular metric to evaluate binary classification
    models:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We now train our `LinkPredictor` GNN for 100 epochs of training, using the
    following training loop:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This returns the following training logs:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can now evaluate the trained model against the hold-out test set:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This returns the following test AUC for our `LinkPredictor` GNN:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is quite impressive as it implies that the link predictor can correctly
    predict 82% of the links presented as ground truths in the test set.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Graph customizations
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how to build and train GNNs for common graph ML tasks. However,
    for convenience, we have chosen to use prebuilt DGL graph convolution layers in
    our models. While unlikely, it is possible that you might need a layer that is
    not provided with the DGL package. DGL provides a message passing API to allow
    you to build custom graph layers easily. In the first part of this section, we
    will look at an example where we use the message-passing API to build a custom
    graph convolution layer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: We have also loaded datasets from the DGL data package for our examples. It
    is far more likely that we will need to use our own data instead. So, in the second
    part of this section, we will see how to convert our own data into a DGL dataset.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Custom layers and message passing
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although DGL provides many graph layers out of the box, there may be cases where
    the ones provided don’t meet our needs exactly and we need to build your own.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, all these graph layers are based on a common underlying concept
    of message passing between nodes in the graph. So, in order to build a custom
    GNN layer, you need to understand how the message-passing paradigm works. This
    paradigm is also known as the **Message Passing Neural Network** (**MPNN**) framework
    [5]:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_17_022.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_17_023.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_17_024.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Each node *u* in the graph has a hidden state (initially its feature vector)
    represented by *h*[u]. For each node *u* and *v*, where nodes *u* and *v* are
    neighbors, i.e., connected by an edge *e*[u->v], we apply some function *M* called
    the *message function*. The message function *M* is applied to every node on the
    graph. We then aggregate the output of *M* for all nodes with the output of all
    their neighboring nodes to produce the message *m*. Here ![](img/B18331_07_002.png)
    is called the *reduce function*. Note that even though we represent the reduce
    function by the summation symbol ![](img/B18331_07_002.png), it can be any aggregation
    function. Finally, we update the hidden state of node *v* using the obtained message
    and the previous state of the node. The function *U* applied at this step is called
    the *update function*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The message-passing algorithm is repeated a specific number of times. After
    that, we reach the *readout phase* where we extract the feature vector from each
    node that represents the entire graph. For example, the final feature vector for
    a node might represent the node category in the case of node classification.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will use the MPNN framework to implement a GraphSAGE layer.
    Even though DGL provides the `dgl.nn.SAGEConv`, which implements this already,
    this is an example to illustrate the creation of custom graph layers using MPNN.
    The message-passing steps of a GraphSAGE layer are given by:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_17_027.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_17_028.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'The code to implement our custom GraphSAGE layer using MPNN is shown below.
    The DGL function `update_all` call allows you to specify a `message_fn` and a
    `reduce_fn`, which are also DGL built-in functions, and the `tf.concat` and `Dense`
    layers represent the final update function:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, we see that the `update_all` function specifies a `message_func`, which
    just copies the node’s current feature vector to a message vector *m*, and then
    averages all the message vectors in the neighborhood of each node. As you can
    see, this faithfully follows the first GraphSAGE equation above. DGL provides
    many such built-in functions ([https://docs.dgl.ai/api/python/dgl.function.xhtml](https://docs.dgl.ai/api/python/dgl.function.xhtml)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Once the neighborhood vector *h_N* is computed in the first step, it is concatenated
    with the input feature vector *h*, and then passed through a `Dense` layer with
    a ReLU activation, as described by the second equation for GraphSAGE above. We
    have thus implemented the GraphSAGE layer with our `CustomGraphSAGE` object.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to put it into a GNN to see how it works. The following code
    shows a `CustomGNN` model that uses two layers of our custom `SAGEConv` implementation:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We will run it to do node classification against the CORA dataset, details of
    which should be familiar from previous examples.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The above code assumes an unweighted graph, i.e., edges between nodes have the
    same weight. This condition is true for the CORA dataset, where each edge represents
    a citation from one paper to another.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: However, we can imagine scenarios where edges may be weighted based on how many
    times some edge has been invoked, for example, an edge that connects a product
    and a user for user recommendations.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'The only change we need to make to handle weighted edges is to allow the weight
    to play a part in our message function. That is, if an edge between our node `u`
    and a neighbor node `v` occurs `k` times, we should consider that edge `k` times.
    The code below shows our custom GraphSAGE layer with the ability to handle weighted
    edges:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This code expects an additional edge property *w*, which contains the edge
    weights, which you can simulate on the CORA dataset by:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `message_func` in `CustomWeightedGraphSAGE` has changed from simply copying
    the feature vector *h* to the message vector *m*, to multiplying *h* and *w* to
    produce the message vector *m*. Everything else is the same as in `CustomGraphSAGE`.
    The new `CustomWeightedGraphSAGE` layer can now be simply dropped into the calling
    class `CustomGNN` where `CustomGraphSAGE` was originally being called.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Custom graph dataset
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A more common use case that you are likely to face is to use your own data to
    train a GNN model. Obviously, in such cases, you cannot use a DGL-provided dataset
    (as we have been using in all our examples so far) and you must wrap your data
    into a custom graph dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Your custom graph dataset should inherit from the `dgl.data.DGLDataset` object
    provided by DGL and implement the following methods:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '`__getitem__(self, i)` – retrieve the `i`-th example from the dataset. The
    retrieved example contains a single DGL graph and its label if applicable.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__len__(self)` – the number of examples in the dataset.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process(self)` – defines how to load and process raw data from the disk.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have seen before, node classification and link prediction operate on a
    single graph, and graph classification operates on a set of graphs. While the
    approach is largely identical for both cases, there are some concerns specific
    to either case, so we will provide an example to do each of these below.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Single graphs in datasets
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For our example, we will choose Zachary’s Karate Club graph, which represents
    the members of a Karate Club observed over three years. Over time, there was a
    disagreement between an administrator (Officer) and the instructor (Mr. Hi), and
    the club members split and reformed under the Officer and Mr. Hi (shown below
    as blue and red nodes, respectively). The Zachary Karate Club network is available
    for download from the NetworkX library:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing plant, red  Description automatically generated](img/B18331_17_02.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.2: Graph representation of the Karate Club Network'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph contains 34 nodes labeled with one of “Officer” or “Mr. Hi” depending
    on which group they ended up in after the split. It contains 78 edges, which are
    undirected and unweighted. An edge between a pair of members indicates that they
    interact with each other outside the club. To make this dataset more realistic
    for GNN usage, we will attach a 10-dimensional random feature vector to each node,
    and an edge weight as an edge feature. Here is the code to convert the Karate
    Club graph into a DGL dataset that you can then use for downstream node or edge
    classification tasks:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Most of the logic is in the `process` method. We call the NetworkX method to
    get the Karate Club as a NetworkX graph, then convert it to a DGL graph object
    with node features and labels. Even though the Karate Club graph does not have
    node and edge features defined, we manufacture some random numbers and set them
    to these properties. Note that this is only for purposes of this example, to show
    where these features would need to be updated if your graph had node and edge
    features. Note that the dataset contains a single graph.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we also want to split the graph into training, validation, and
    test splits for node classification purposes. For that, we assign masks indicating
    whether a node belongs to one of these splits. We do this rather simply by splitting
    the nodes in the graph 60/20/20 and assigning Boolean masks for each split.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to instantiate this dataset from our code, we can say:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will give us the following output (reformatted a little for readability).
    The two main structures are the `ndata_schemas` and `edata_schemas`, accessible
    as `g.ndata` and `g.edata`, respectively. Within `ndata_schemas`, we have keys
    that point to the node features (`feats`), node labels (`label`), and the masks
    to indicate the training, validation, and test splits (`train_mask`, `val_mask`,
    and `test_mask`), respectively. Under `edata_schemas`, there is the `weight` attribute
    that indicates the edge weights:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Please refer to the examples on node classification and link prediction for
    information on how to use this kind of custom dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Set of multiple graphs in datasets
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets that support the graph classification task will contain multiple graphs
    and their associated labels, one per graph. For our example, we will consider
    a hypothetical dataset of molecules represented as graphs, and the task would
    be to predict if the molecule is toxic or not (a binary prediction).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: We will use the NetworkX method `random_regular_graph()` to generate synthetic
    graphs with a random number of nodes and node degree. To each node of each graph,
    we will attach a random 10-dimensional feature vector. Each node will have a label
    (0 or 1) indicating if the graph is toxic. Note that this is just a simulation
    of what real data might look like. With real data, the structure of each graph
    and the values of the node vectors, which are random in our case, will have a
    real impact on the target variable, i.e., the toxicity of the molecule.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The figure below shows some examples of what the synthetic “molecules” might
    look like:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, radar chart  Description automatically generated](img/B18331_17_03.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.3: Some examples of random regular graphs generated using NetworkX'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to convert a set of random NetworkX graphs into a DGL graph
    dataset for graph classification. We will generate 100 such graphs and store them
    in a list in the form of a DGL dataset:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once created, we can then call it from our code as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This produces the following output for the first graph in the DGL dataset (reformatted
    slightly for readability). As you can see, the first graph in the dataset has
    `6` nodes and `15` edges and contains a feature vector (accessible using the `feats`
    key) of size `10`. The label is a `0`-dimensional tensor (i.e., a scalar) of type
    long (`int64`):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As before, in order to see how you would use this custom dataset for some task
    such as graph classification, please refer to the example on graph classification
    earlier in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Future directions
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Graph neural networks are a rapidly evolving discipline. We have covered working
    with static homogeneous graphs on various popular graph tasks so far, which covers
    many real-world use cases. However, it is likely that some graphs are neither
    homogeneous nor static, and neither can they be easily reduced to this form. In
    this section, we will look at our options for dealing with heterogenous and temporal
    graphs.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Heterogeneous graphs
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Heterogeneous graphs [7], also called heterographs, differ from the graphs we
    have seen so far in that they may contain different kinds of nodes and edges.
    These different types of nodes and edges might also contain different types of
    attributes, including possible representations with different dimensions. Popular
    examples of heterogeneous graphs are citation graphs that contain authors and
    papers, recommendation graphs that contain users and products, and knowledge graphs
    that can contain many different types of entities.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: You can use the MPNN framework on heterogeneous graphs by manually implementing
    message and update functions individually for each edge type. Each edge type is
    defined by the triple (source node type, edge type, and destination node type).
    However, DGL provides support for heterogeneous graphs using the `dgl.heterograph()`
    API, where a graph is specified as a series of graphs, one per edge type.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Typical learning tasks associated with heterogeneous graphs are similar to their
    homogeneous counterparts, namely node classification and regression, graph classification,
    and edge classification/link prediction. A popular graph layer for working with
    heterogeneous graphs is the **Relational GCN** or **R-GCN**, available as a built-in
    layer in DGL.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Temporal Graphs
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Temporal Graphs [8] is a framework developed at Twitter to handle dynamic graphs
    that change over time. While GNN models have primarily focused on static graphs
    that do not change over time, adding the time dimension allows us to model many
    interesting phenomena in social networks, financial transactions, and recommender
    systems, all of which are inherently dynamic. In such systems, it is the dynamic
    behavior that conveys the important insights.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: A dynamic graph can be represented as a stream of timed events, such as additions
    and deletions of nodes and edges. This stream of events is fed into an encoder
    network that learns a time-dependent encoding for each node in the graph. A decoder
    is trained on this encoding to support some specific task such as link prediction
    at a future point in time. There is currently no support in the DGL library for
    Temporal Graphs, mainly because it is a very rapidly evolving research area.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, a **Temporal Graph Network** (**TGN**) encoder works by creating
    a compressed representation of the nodes based on their interaction and updates
    over time. The current state of each node is stored in TGN memory and acts as
    the hidden state *s*[t] of an RNN; however, we have a separate state vector *s*[t]*(t)*
    for each node *i* and time point *t*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: A message function similar to what we have seen in the MPNN framework computes
    two messages *m*[i] and *m*[j] for a pair of nodes *i* and *j* using the state
    vectors and their interaction as input. The message and state vectors are then
    combined using a memory updater, which is usually implemented as an RNN. TGNs
    have been found to outperform their static counterparts on the tasks of future
    edge prediction and dynamic node classification both in terms of accuracy and
    speed.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered graph neural networks, an exciting set of techniques
    to learn not only from node features but also from the interaction between nodes.
    We have covered the intuition behind why graph convolutions work and the parallels
    between them and convolutions in computer vision. We have described some common
    graph convolutions, which are provided as layers by DGL. We have demonstrated
    how to use the DGL for popular graph tasks of node classification, graph classification,
    and link prediction. In addition, in the unlikely event that our needs are not
    met by standard DGL graph layers, we have learned how to implement our own graph
    convolution layer using DGL’s message-passing framework. We have also seen how
    to build DGL datasets for our own graph data. Finally, we look at some emerging
    directions of graph neural networks, namely heterogeneous graphs and temporal
    graphs. This should equip you with skills to use GNNs to solve interesting problems
    in this area.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will turn our attention to learning about some best
    ML practices associated with deep learning projects.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kipf, T. and Welling, M. (2017). *Semi-supervised Classification with Graph
    Convolutional Networks*. Arxiv Preprint, arXiv: 1609.02907 [cs.LG]. Retrieved
    from [https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Velickovic, P., et al. (2018). *Graph Attention Networks*. Arxiv Preprint, arXiv
    1710.10903 [stat.ML]. Retrieved from [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hamilton, W. L., Ying, R., and Leskovec, J. (2017). *Inductive Representation
    Learning on Large Graphs*. Arxiv Preprint, arXiv: 1706.02216 [cs.SI]. Retrieved
    from [https://arxiv.org/abs/1706.02216](https://arxiv.org/abs/1706.02216)'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Xu, K., et al. (2018). *How Powerful are Graph Neural Networks?*. Arxiv Preprint,
    arXiv: 1810.00826 [cs.LG]. Retrieved from [https://arxiv.org/abs/1810.00826](https://arxiv.org/abs/1810.00826)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gilmer, J., et al. (2017). *Neural Message Passing for Quantum Chemistry*.
    Arxiv Preprint, arXiv: 1704.01212 [cs.LG]. Retrieved from [https://arxiv.org/abs/1704.01212](https://arxiv.org/abs/1704.01212)'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zachary, W. W. (1977). *An Information Flow Model for Conflict and Fission in
    Small Groups*. Journal of Anthropological Research. Retrieved from [https://www.journals.uchicago.edu/doi/abs/10.1086/jar.33.4.3629752](https://www.journals.uchicago.edu/doi/abs/10.1086/jar.33.4.3629752)
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pengfei, W. (2020). *Working with Heterogeneous Graphs in DGL*. Blog post. Retrieved
    from [https://www.jianshu.com/p/767950b560c4](https://www.jianshu.com/p/767950b560c4)
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bronstein, M. (2020). *Temporal Graph Networks*. Blog post. Retrieved from [https://towardsdatascience.com/temporal-graph-networks-ab8f327f2efe](https://towardsdatascience.com/temporal-graph-networks-ab8f327f2efe)
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
