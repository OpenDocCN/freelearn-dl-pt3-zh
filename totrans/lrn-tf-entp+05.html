<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer080">&#13;
			<p id="_idParaDest-41" class="chapter-number"><a id="_idTextAnchor081"/>Chapter 3:</p>&#13;
			<h1 id="_idParaDest-42"><a id="_idTextAnchor082"/>Data Preparation and Manipulation Techniques</h1>&#13;
			<p>In this chapter, you will learn how to convert the two common data types into structures suitable for ingestion pipelines—structured CSVs or pandas DataFrames into a dataset, and unstructured data such as images into <strong class="bold">TFRecords</strong>.</p>&#13;
			<p>Along the way, there will be some tips and utility functions that are reusable in many situations. You will also understand the rationale of the conversion process.</p>&#13;
			<p>As demonstrated in the previous chapter, TensorFlow Enterprise takes advantage of the flexibility offered by the Google Cloud AI platform to access training data. Once access to the training data is resolved, our next task is to develop a workflow to let the model consume the data efficiently. In this chapter, we will learn how to examine and manipulate commonly used data structures. </p>&#13;
			<p>While TensorFlow can consume Pythonic data structures such as pandas or numpy directly, for resource throughput and ingestion efficiency, TensorFlow built the dataset API to convert data from its native Pythonic structure into TensorFlow's specific structure. The dataset API can handle and parse many commonly used types of data. For instance, structured or tabular data with defined schemas are typically presented as a pandas DataFrame. The dataset API converts this data structure into a Tensorflow dataset. Image data is typically presented as a numpy array. In TensorFlow, it is preferred to convert it into <strong class="source-inline">TFRecord</strong>. </p>&#13;
			<p>In working with these data structures, it is important to be certain that the conversion process is performed correctly and that the data can be verified. This chapter will demonstrate some techniques that help to ensure that data structure conversions are done correctly; for example, decoding a byte stream into an image. It is always helpful to decode these data structures into a readable format just for the purpose of a quick check of the data quality. </p>&#13;
			<p>We will start with the TensorFlow dataset as applied to structured data. In particular, we'll cover the following main topics:</p>&#13;
			<ul>&#13;
				<li>Converting tabular data to a TensorFlow dataset</li>&#13;
				<li>Converting distributed CSV files to a TensorFlow dataset</li>&#13;
				<li>Handling image data for input pipelines</li>&#13;
				<li>Decoding <strong class="source-inline">TFRecord</strong> and reconstructing the image</li>&#13;
				<li>Handling image data at scale</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-43"><a id="_idTextAnchor083"/>Converting tabular data to a TensorFlow dataset</h1>&#13;
			<p><strong class="bold">Tabular</strong> or <a id="_idTextAnchor084"/><strong class="bold">comma separated values (CSV)</strong> data with fixed schemas and data types are <a id="_idIndexMarker105"/>commonly encountered. We <a id="_idIndexMarker106"/>typically work it into <a id="_idIndexMarker107"/>a pandas DataFrame. We have seen in the previous chapter how <a id="_idIndexMarker108"/>this can be easily done when the data is hosted in a <strong class="bold">BigQuery table</strong> (the BigQuery magic command that returns a query result to a pandas DataFrame by default).</p>&#13;
			<p>Let's take a look at how to handle data that can fit into the memory. In this example, we are going to read a public dataset using the BigQuery magic<a id="_idTextAnchor085"/> command, so we can easily obtain the data in a pandas DataFrame. Then we are going to convert it to a TensorFlow dataset. A TensorFlow dataset is the data structure for streaming training data in batches without using up the compute node's runtime memory. </p>&#13;
			<h2 id="_idParaDest-44"><a id="_idTextAnchor086"/>Converting a BigQuery table to a TensorFlow dataset</h2>&#13;
			<p>Each of the <a id="_idIndexMarker109"/>following steps is executed in a <a id="_idIndexMarker110"/>cell. Again, use any of the AI platforms you prefer (AI Notebook, Deep Learning VM, Deep Learning Container). An AI notebook is the simplest and cheapest choice:</p>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">The table in this example is selected for demonstration purposes only. We are going to treat <strong class="source-inline">daily_deaths</strong> as if it is the target for machine learning model training. While we are going to treat it as if it is our training data (in other words, containing features and target columns), in actual training data engineering practices, there are other steps involved, such as feature engineering, aggregation, and normalization. </p>&#13;
			<ol>&#13;
				<li>Let's look at <a id="_idIndexMarker111"/>the data from BigQuery, so we can be sure of its data structure and the data <a id="_idIndexMarker112"/>type of each column, and then take a preview of the table:<div id="_idContainer065" class="IMG---Figure"><img src="Images/image001.jpg" alt="Figure 3.1 – Using BigQuery to examine the data structure&#13;&#10;"/></div><p class="figure-caption">Figure 3.1 – Using BigQuery to examine the data structure</p><p>Once the preceding query is run, you will see output as shown in the following screenshot:</p><div id="_idContainer066" class="IMG---Figure"><img src="Images/image002.jpg" alt="Figure 3.2 – Table preview&#13;&#10;"/></div><p class="figure-caption">Figure 3.2 – Table preview</p></li>&#13;
				<li>Load the library, define the variables, and define the project ID only if you are running in a different project:<p class="source-code"><strong class="source-inline">PROJECT_ID = '&lt;PROJECT_ID&gt;' </strong></p><p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p></li>&#13;
				<li>Use the BigQuery magic command to <a id="_idIndexMarker113"/>read a table into a pandas <a id="_idIndexMarker114"/>DataFrame (<strong class="source-inline">train_raw_df</strong>):<p class="source-code">%%bigquery train_raw_df</p><p class="source-code">SELECT countries_and_territories, geo_id, country_territory_code, </p><p class="source-code">year, month, day, confirmed_cases, daily_deaths, pop_data_2019</p><p class="source-code">FROM bigquery-public-data.covid19_ecdc.covid_19_geographic_distribution_worldwide</p></li>&#13;
				<li>Take a look at a few sample rows:<p class="source-code">train_raw_df.sample(n=5)</p><p>Here's the output:</p><div id="_idContainer067" class="IMG---Figure"><img src="Images/image003.jpg" alt="Figure 3.3 – Output of a few rows from the table&#13;&#10;"/></div><p class="figure-caption">Figure 3.3 – Output of a few rows from the table</p></li>&#13;
				<li>Some columns are categorical. We need to encode them as integers. First, we designate the column as a pandas categorical feature:<p class="source-code">train_raw_df['countries_and_territories'] = pd.Categorical(train_raw_df['countries_and_territories'])</p></li>&#13;
				<li>Then we replace the column content with category code:<p class="source-code">train_raw_df['countries_and_territories'] = train_raw_df.countries_and_territories.cat.codes</p></li>&#13;
				<li>Then we repeat <a id="_idIndexMarker115"/>this procedure for <a id="_idIndexMarker116"/>the other categorical columns:<p class="source-code">train_raw_df['geo_id'] = pd.Categorical(train_raw_df['geo_id'])</p><p class="source-code">train_raw_df['geo_id'] = train_raw_df.geo_id.cat.codes</p><p class="source-code">train_raw_df['country_territory_code'] = pd.Categorical(train_raw_df['country_territory_code'])</p><p class="source-code">train_raw_df['country_territory_code'] = train_raw_df.country_territory_code.cat.codes</p></li>&#13;
				<li>Make lists to hold column names according to data type. The reason is to ensure that the dataset can cast the columns in our DataFrame to the correct TensorFlow data type:<p class="source-code">int32_features = ['confirmed_cases']</p><p class="source-code">float32_features = ['pop_data_2019']</p><p class="source-code">int16_features = ['year', 'month', 'day']</p><p class="source-code">categorical_features = ['countries_and_territories', 'geo_id', 'country_territory_code']</p><p class="source-code">int32_target = ['daily_deaths']</p></li>&#13;
				<li>Creating a dataset from a pandas DataFrame requires us to specify the correct column <a id="_idIndexMarker117"/>names and the data type. Column names <a id="_idIndexMarker118"/>are held in the respective list based on their data type:<p class="source-code">training_dataset = tf.data.Dataset.from_tensor_slices(</p><p class="source-code">        (</p><p class="source-code">            tf.cast(train_raw_df[int32_features].values, </p><p class="source-code">            tf.int32),</p><p class="source-code">            tf.cast(train_raw_df[float32_features].</p><p class="source-code">            values, tf.float32),</p><p class="source-code">            tf.cast(train_raw_df[int16_features].values, </p><p class="source-code">            tf.int16),</p><p class="source-code">            tf.cast(train_raw_df[categorical_features].</p><p class="source-code">            values, tf.int32),</p><p class="source-code">            tf.cast(train_raw_df[int32_target].values, </p><p class="source-code">            tf.int32)</p><p class="source-code">        )</p><p class="source-code">    )</p></li>&#13;
				<li>Look at the structure of the dataset to make sure its metadata is as specified during the creation process in the previous step:<p class="source-code">training_dataset</p><p>The output is as follows:</p><p class="source-code">&lt;TensorSliceDataset shapes: ((1,), (1,), (3,), (3,), (1,)), types: (tf.int32, tf.float32, tf.int16, tf.int32, tf.int32)&gt;</p><p>The tensor shapes and data types are in the exact order as indicated in the previous step.</p></li>&#13;
			</ol>&#13;
			<p>Now you have created a dataset from a pandas DataFrame. The dataset is now a part of the input pipeline. If this <a id="_idIndexMarker119"/>dataset's features and targets are properly normalized and selected (for example, having performed a normalization operation such as min-max scaling, or a <a id="_idIndexMarker120"/>standardization operation such as Z-score conversion if the distribution of the column data can be assumed to be Gaussian), then it is ready to be fed into a model for training as-is.</p>&#13;
			<p>So far, from this exercise, you've learned the following points:</p>&#13;
			<ul>&#13;
				<li>Use BigQuery as much as possible to examine data schemas and data types first. </li>&#13;
				<li>For data that can fit into the memory, leverage the BigQuery magic command to output a pandas DataFrame. </li>&#13;
				<li>Bin the column names by their data types for clarity and organization.</li>&#13;
				<li>Encode categorical features to integers so they can be cast into a TensorFlow data type that is compatible with a TensorFlow dataset.</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-45"><a id="_idTextAnchor087"/>Converting distributed CSV files to a TensorFlow dataset</h1>&#13;
			<p>If <a id="_idIndexMarker121"/>you are not sure about the data size, or are unsure as to <a id="_idIndexMarker122"/>whether it can all fit in the Python runtime's memory, then reading the data into a pandas DataFrame is not a viable option. In this case, we may use a <strong class="bold">TF dataset</strong> to directly access the data without opening it. </p>&#13;
			<p>Typically, when data is stored in a storage bucket as parts, the naming convention follows a general pattern. This pattern is similar to that of a <strong class="bold">Hadoop Distributed File System (HDFS)</strong>, where the <a id="_idIndexMarker123"/>data is stored in parts and the complete data can be inferred via a wildcard symbol, <strong class="source-inline">*</strong>. </p>&#13;
			<p>When storing distributed files in a Google Cloud Storage bucket, a common pattern for filenames is as follows:</p>&#13;
			<p class="source-code">&lt;FILE_NAME&gt;-&lt;pattern&gt;-001.csv</p>&#13;
			<p class="source-code">…</p>&#13;
			<p class="source-code">&lt;FILE_NAME&gt;-&lt;pattern&gt;-00n.csv</p>&#13;
			<p>Alternatively, there is the following pattern: </p>&#13;
			<p class="source-code">&lt;FILE_NAME&gt;-&lt;pattern&gt;-aa.csv</p>&#13;
			<p class="source-code">…</p>&#13;
			<p class="source-code">&lt;FILE_NAME&gt;-&lt;pattern&gt;-zz.csv</p>&#13;
			<p>There is always a pattern in the filenames. The TensorFlow module <strong class="source-inline">tf.io.gfile.glob</strong> is a convenient API that encodes such filename patterns in a distributed filesystem. This is critical for inferring distributed files that are stored in a storage bucket. In this section, we will use this API to infer our structured data (multiple CSV files), which is distributed in a storage bucket. Once inferred, we will then convert it to a dataset (using <strong class="source-inline">tf.data.experimental.make_csv_dataset</strong>). </p>&#13;
			<h2 id="_idParaDest-46"><a id="_idTextAnchor088"/>Preparing an example CSV</h2>&#13;
			<p>Since we need <a id="_idIndexMarker124"/>multiple CSV files of the same schema for this demonstration, we may use open source CSV data such as the <strong class="bold">Pima Indians Diabetes</strong> dataset (CSV) as our data source. This CSV is hosted in <a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv">https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv</a>.</p>&#13;
			<p>You may simply run the following command on your local system (where you downloaded the aforementioned file):</p>&#13;
			<p class="source-code">wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv</p>&#13;
			<p>Again, for demonstration purposes only, we need to split this data into multiple smaller CSVs, and then upload these CSVs to a Google Cloud Storage bucket. </p>&#13;
			<p>The column names for this file are as follows:</p>&#13;
			<p class="source-code">['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigree', 'Age', 'Outcome']</p>&#13;
			<p>Column names are not included in the CSV thus we may split the file into multiple parts without extracting the header row. Let’s start with steps below: </p>&#13;
			<ol>&#13;
				<li value="1">Split the file into multiple parts.<p>Once you have downloaded the CSV, you may split it into multiple parts with the following <strong class="source-inline">awk</strong> <a id="_idIndexMarker125"/>command. This will split the file into multiple CSV parts at every 200 rows:</p><p class="source-code">awk '{filename = 'pima_indian_diabetes_data_part0' int((NR-1)/200) '.csv'; print &gt;&gt; filename}' pima-indians-diabetes.data.csv</p><p>The following CSV files are generated:</p><p class="source-code">-rw-r--r--  1 mbp16  staff      6043 Jul 21 16:25 pima_indian_diabetes_data_part00.csv</p><p class="source-code">-rw-r--r--  1 mbp16  staff      6085 Jul 21 16:25 pima_indian_diabetes_data_part01.csv</p><p class="source-code">-rw-r--r--  1 mbp16  staff      6039 Jul 21 16:25 pima_indian_diabetes_data_part02.csv</p><p class="source-code">-rw-r--r--  1 mbp16  staff      5112 Jul 21 16:25 pima_indian_diabetes_data_part03.csv</p></li>&#13;
				<li>Upload the files to storage. After you have created multiple CSV files from the downloaded file, you may upload these files to a Google Cloud Storage bucket: </li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer068" class="IMG---Figure">&#13;
					<img src="Images/image005.jpg" alt="Figure 3.4 – Uploading CSV files to a Cloud Storage bucket&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 3.4 – Uploading CSV files to a Cloud Storage bucket</p>&#13;
			<p>All the files are here:</p>&#13;
			<div>&#13;
				<div id="_idContainer069" class="IMG---Figure">&#13;
					<img src="Images/image007.jpg" alt="Figure 3.5 – Multi-part CSV file in the Cloud Storage bucket&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 3.5 – Multi-part CSV file in the Cloud Storage bucket</p>&#13;
			<h2 id="_idParaDest-47"><a id="_idTextAnchor089"/>Building filename patterns with TensorFlow I/O</h2>&#13;
			<p>Once the files <a id="_idIndexMarker126"/>are uploaded, let's now go to our AI Platform notebook environment and execute the following lines of code:</p>&#13;
			<p class="source-code">import tensorflow as tf</p>&#13;
			<p class="source-code">distributed_files_pattern = 'gs://myworkdataset/pima_indian_diabetes_data_part*'</p>&#13;
			<p class="source-code">filenames = tf.io.gfile.glob(distributed_files_pattern)</p>&#13;
			<p><strong class="source-inline">Tf.io.gfile.glob</strong> takes a file pattern string as the input and creates a <strong class="source-inline">filenames</strong> list: </p>&#13;
			<p class="source-code">['gs://myworkdataset/pima_indian_diabetes_data_part00.csv',</p>&#13;
			<p class="source-code"> 'gs://myworkdataset/pima_indian_diabetes_data_part01.csv',</p>&#13;
			<p class="source-code"> 'gs://myworkdataset/pima_indian_diabetes_data_part02.csv',</p>&#13;
			<p class="source-code"> 'gs://myworkdataset/pima_indian_diabetes_data_part03.csv']</p>&#13;
			<p>Now that we have a list of filenames that match the pattern, we are ready to convert these files to a dataset. </p>&#13;
			<h2 id="_idParaDest-48"><a id="_idTextAnchor090"/>Creating a dataset from CSV files</h2>&#13;
			<p>Typically, multiple CSV files are <a id="_idIndexMarker127"/>stored with either no header or all with a header. In this case, there is no header. We need to prepare the column names for the CSV before we convert it to dataset:</p>&#13;
			<p class="source-code">COLUMN_NAMES = ['Pregnancies', 'Glucose', 'BloodPressure', </p>&#13;
			<p class="source-code">                'SkinThickness', 'Insulin', 'BMI', </p>&#13;
			<p class="source-code">                'DiabetesPedigree', 'Age', 'Outcome']</p>&#13;
			<p>Here's the source for column names: <a href="https://data.world/data-society/pima-indians-diabetes-database">https://data.world/data-society/pima-indians-diabetes-database</a>.</p>&#13;
			<p>Then we need to specify that the first lines in these files are not headers as we convert the CSVs to a dataset:</p>&#13;
			<p class="source-code">ds = tf.data.experimental.make_csv_dataset(</p>&#13;
			<p class="source-code">      filenames,</p>&#13;
			<p class="source-code">      header = False,</p>&#13;
			<p class="source-code">      column_names = COLUMN_NAMES,</p>&#13;
			<p class="source-code">      batch_size=5, # Intentionally make it small for </p>&#13;
			<p class="source-code">      # convenience.</p>&#13;
			<p class="source-code">      label_name='Outcome',</p>&#13;
			<p class="source-code">      num_epochs=1,</p>&#13;
			<p class="source-code">      ignore_errors=True)</p>&#13;
			<p>In <strong class="source-inline">make_csv_dataset</strong>, we use a list of filenames as the input and specify there is no header, and we then assign <strong class="source-inline">COLUMN_NAMES</strong>, make small batches for showing the result, select a <a id="_idIndexMarker128"/>column as the target column (<strong class="source-inline">'Outcome'</strong>), and set the number of epochs to <strong class="source-inline">1</strong> since we are not going to train a model with it at this point. </p>&#13;
			<h2 id="_idParaDest-49"><a id="_idTextAnchor091"/>Inspecting the dataset</h2>&#13;
			<p>Now we may verify the content of the dataset. Recall that since we specified a column as the label, that <a id="_idIndexMarker129"/>means the rest of the columns are features. The output will be a tuple that contains features and targets.</p>&#13;
			<p>Let's take the first batch of the dataset, which contains five observations, and print the data in features and target columns. In a dataset, the data is stored as arrays, and each column is now a key-value pair. Within <strong class="source-inline">features</strong> is another level of key-value pairs for each feature:</p>&#13;
			<p class="source-code">for features, target in ds.take(1):</p>&#13;
			<p class="source-code">    print(''Outcome': {}'.format(target))</p>&#13;
			<p class="source-code">    print(''Features:'')</p>&#13;
			<p class="source-code">    for k, v in features.items():</p>&#13;
			<p class="source-code">        print('  {!r:20s}: {}'.format(k, v))</p>&#13;
			<p>The output is as follows:</p>&#13;
			<p class="source-code">'Outcome': [1 0 0 0 0]</p>&#13;
			<p class="source-code">'Features:'</p>&#13;
			<p class="source-code">  'Pregnancies'       : [ 7 12  1  0  2]</p>&#13;
			<p class="source-code">  'Glucose'           : [129  88 128  93  96]</p>&#13;
			<p class="source-code">  'BloodPressure'     : [ 68  74  82 100  68]</p>&#13;
			<p class="source-code">  'SkinThickness'     : [49 40 17 39 13]</p>&#13;
			<p class="source-code">  'Insulin'           : [125  54 183  72  49]</p>&#13;
			<p class="source-code">  'BMI'               : [38.5 35.3 27.5 43.4 21.1]</p>&#13;
			<p class="source-code">  'DiabetesPedigree'  : [0.439 0.378 0.115 1.021 0.647]</p>&#13;
			<p class="source-code">  'Age'               : [43 48 22 35 26]</p>&#13;
			<p>During training, the data will be passed to the training process in batches, and not as a single file to be opened and possibly consume a large amount of runtime memory. In the preceding example, we see that as a good practice, distributed files stored in Cloud Storage follow a <a id="_idIndexMarker130"/>certain naming pattern. The <strong class="source-inline">tf.io.gfile.glob</strong> API can easily infer multiple files that are distributed in a Cloud Storage bucket. We may easily use <strong class="source-inline">tf.data.experimental.make_csv_dataset</strong> to create a dataset instance from the <strong class="source-inline">gfile</strong> instance. Overall, the <strong class="source-inline">tf.io</strong> and <strong class="source-inline">tf.data</strong> APIs together make it possible to build a data input pipeline without explicitly reading data into memory.</p>&#13;
			<h1 id="_idParaDest-50"><a id="_idTextAnchor092"/>Handling image data for input pipelines</h1>&#13;
			<p>While there are many types of unstructured data, images are probably the most frequently <a id="_idIndexMarker131"/>encountered type. TensorFlow provided <strong class="source-inline">TFRecord</strong> as a type of dataset for image data. In this section, we are going to learn how to convert image data in Cloud Storage into a <strong class="source-inline">TFRecord</strong> object for input pipelines. </p>&#13;
			<p>When working with image data in a TensorFlow pipeline, the raw image is typically converted to a <strong class="source-inline">TFRecord</strong> object for the same reason as for CSV or DataFrames. Compared to a raw numpy array, a <strong class="source-inline">TFRecord</strong> object is a more efficient and scalable representation of the image collections. Converting raw images to a <strong class="source-inline">TFRecord</strong> object is not a straightforward process. In <strong class="source-inline">TFRecord</strong>, the data is stored as a binary string. In this section, we are going to show how to do this step by step. </p>&#13;
			<p>Let's start with the conversion process of converting a raw image to a <strong class="source-inline">TFRecord</strong> object. Feel free to upload your own images to the JupyterLab instance:</p>&#13;
			<ol>&#13;
				<li value="1">Upload images of your choice to the JupyterLab runtime. Create a folder for your images that we are going to upload. Give the folder a name, and this is the folder where the images will be uploaded: <div id="_idContainer070" class="IMG---Figure"><img src="Images/image009.jpg" alt="Figure 3.6 – Creating a folder in the notebook runtime&#13;&#10;"/></div><p class="figure-caption">Figure 3.6 – Creating a folder in the notebook runtime</p><p>Now that the folder has a name, you may proceed to the next step.</p></li>&#13;
				<li>Double-click <a id="_idIndexMarker132"/>on the folder you just named. Now you are inside this folder. In this example, I named this folder <strong class="source-inline">image-ai-platform-examle</strong>. Then, within this folder, I created another folder named <strong class="source-inline">maldives</strong>. Once inside, you may click the upload button to upload a few of your own images to this folder:<div id="_idContainer071" class="IMG---Figure"><img src="Images/image011.jpg" alt="Figure 3.7 – Uploading an item to JupyterLab runtime&#13;&#10;"/></div><p class="figure-caption">Figure 3.7 – Uploading an item to JupyterLab runtime</p><p>Here, I uploaded an image named <strong class="source-inline">maldives-1.jpg</strong>.</p></li>&#13;
				<li>You may acquire the path to this image by right-clicking on the image file:<div id="_idContainer072" class="IMG---Figure"><img src="Images/image013.jpg" alt="Figure 3.8 – Finding the path to images uploaded to the notebook&#13;&#10;"/></div><p class="figure-caption">Figure 3.8 – Finding the path to images uploaded to the notebook</p><p>You may paste the <a id="_idIndexMarker133"/>file path to a notepad or editor for quick reference for the next step.</p></li>&#13;
				<li>Select <strong class="bold">Copy Path</strong> to an editor. In this case, it is <strong class="source-inline">images-ai-platform-example/maldives/maldives-1.jpg</strong>:</li>&#13;
				<li>Display the image for verification:<p class="source-code">import IPython.display as display</p><p class="source-code">my_image = 'images-ai-platform-example/maldives/maldives-1.jpg'</p><p class="source-code">display.display(display.Image(filename=my_image))</p><p> Here's the output:</p><div id="_idContainer073" class="IMG---Figure"><img src="Images/image015.jpg" alt="Figure 3.9 – Displaying the image&#13;&#10;"/></div><p class="figure-caption">Figure 3.9 – Displaying the image</p></li>&#13;
				<li>Create a dictionary to map the filename with a label. We can use the <strong class="source-inline">my_image</strong> alias as <a id="_idIndexMarker134"/>the key and we may verify this dictionary:<p class="source-code">image_labels = {</p><p class="source-code">    my_image : 0</p><p class="source-code">}</p><p class="source-code">image_labels.items()</p><p>The output should be as follows: </p><p class="source-code">dict_items([('images-ai-platform-example/maldives/maldives-1.jpg', 0)])</p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-51"><a id="_idTextAnchor093"/>Constructing a protobuf message</h2>&#13;
			<p>Now we have <strong class="source-inline">image_labels</strong> that <a id="_idIndexMarker135"/>map image files to their labels. The next thing we need to do is to convert this image to a <strong class="source-inline">tf.Example</strong> <strong class="bold">protobuf</strong> message. Protobuf is Google's language-neutral mechanism or structure for efficient serialization of data. When using this standard in formatting the image data, you effectively convert the raw image into a collection of key-value pairs, with most of the keys being the metadata of the image, including the filename, width, height, channels, label, and one key being the actual pixel values as a byte array. Similar to <strong class="source-inline">image_label</strong>, the <strong class="source-inline">tf.Example</strong> message consists of key-value pairs. The key-value pairs are the metadata of the image, including the three dimensions and their respective values, the label and its value, and finally the image itself in byte array format. The values are represented as <strong class="source-inline">tf.Tensor</strong>. Let's now construct this protobuf message. </p>&#13;
			<p>At this time, the <strong class="source-inline">tf.Example</strong> protobuf message can only accept three types of <strong class="source-inline">tf.Tensor</strong>. These are as follows:</p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">tf.train.ByteList</strong> can handle <strong class="source-inline">string</strong> <strong class="source-inline">and</strong> <strong class="source-inline">byte</strong>.</li>&#13;
				<li><strong class="source-inline">tf.train.FloatList</strong> can handle <strong class="source-inline">float (float32) </strong><strong class="source-inline">and</strong> <strong class="source-inline">double (float64)</strong>.</li>&#13;
				<li><strong class="source-inline">tf.train.Int64List</strong> can handle <strong class="source-inline">bool</strong>, <strong class="source-inline">enum</strong>, <strong class="source-inline">int32</strong>, <strong class="source-inline">uint32</strong>, <strong class="source-inline">int64</strong> <strong class="source-inline">and</strong> <strong class="source-inline">uint64</strong>.</li>&#13;
			</ul>&#13;
			<p>Most other generic data types can be coerced into one of these three types as per TensorFlow's documentation, which is available at: <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord#tftrainexample">https://www.tensorflow.org/tutorials/load_data/tfrecord#tftrainexample</a>:</p>&#13;
			<ol>&#13;
				<li value="1">First, we may use these functions that are provided as per TensorFlow's documentation. These functions can convert values to types that are compatible with <strong class="source-inline">tf.Example</strong>:<p class="source-code">def _bytes_feature(value):</p><p class="source-code">  '''Returns a bytes_list from a string / byte.'''</p><p class="source-code">  if isinstance(value, type(tf.constant(0))):</p><p class="source-code">    value = value.numpy() # BytesList won't unpack a </p><p class="source-code">    # string from an EagerTensor.</p><p class="source-code">  return tf.train.Feature(bytes_list=</p><p class="source-code">  tf.train.BytesList(value=[value]))</p><p class="source-code">def _float_feature(value):</p><p class="source-code">  '''Returns a float_list from a float / double.'''</p><p class="source-code">  return tf.train.Feature(float_list=</p><p class="source-code">  tf.train.FloatList(value=[value]))</p><p class="source-code">def _int64_feature(value):</p><p class="source-code">  '''Returns an int64_list from a bool / enum / int / </p><p class="source-code">  uint.'''</p><p class="source-code">  return tf.train.Feature(int64_list=</p><p class="source-code">  tf.train.Int64List(value=[value]))</p><p>Generally speaking, from <a id="_idIndexMarker136"/>the pattern in the preceding function, we can see that the raw value from the data is first coerced into one of the three acceptable types and then it is converted to a <strong class="source-inline">feature</strong>.   </p></li>&#13;
				<li>Then, we can open the image as a byte string and extract its dimensions:<p class="source-code">image_string = open(my_image, 'rb').read()</p><p class="source-code">image_shape = tf.image.decode_jpeg(image_string).shape</p><p class="source-code">image_shape</p></li>&#13;
				<li>Let's now construct a dictionary that puts these key-value pairs together:<p class="source-code">label = image_labels[my_image]</p><p class="source-code">feature_dictionary = {</p><p class="source-code">      'height': _int64_feature(image_shape[0]),</p><p class="source-code">      'width': _int64_feature(image_shape[1]),</p><p class="source-code">      'depth': _int64_feature(image_shape[2]),</p><p class="source-code">      'label': _int64_feature(label),</p><p class="source-code">      'image_raw': _bytes_feature(image_string),</p><p class="source-code">  }</p><p>Notice that the feature dictionary consists of key-value pairs of the metadata, where the values are one of the three coerced data types for <strong class="source-inline">tf.Example</strong>.</p></li>&#13;
				<li>We will then convert this dictionary to <strong class="source-inline">tf.Train.Features</strong>:<p class="source-code">features_msg = tf.train.Features(feature=feature_dictionary)</p></li>&#13;
				<li>Convert a <strong class="source-inline">tf.Features</strong> protobuf <a id="_idIndexMarker137"/>message to a <strong class="source-inline">tf.Example</strong> protobuf message:<p class="source-code">example_msg = tf.train.Example(features=features_msg)</p></li>&#13;
				<li>Now, create a directory for storing <strong class="source-inline">tfrecords</strong>:<p class="source-code">!mkdir tfrecords-collection</p></li>&#13;
				<li>Specify a target name, and then execute the write operation:<p class="source-code">record_file = 'tfrecords-collection/maldives-1.tfrecord'</p><p class="source-code">with tf.io.TFRecordWriter(record_file) as writer:</p><p class="source-code">    writer.write(example_msg.SerializeToString())</p><p>The image is now written into a protobuf message, which is a collection of key-value pairs that store its dimensions, labels, and raw images (the image value is stored as a byte string). </p></li>&#13;
			</ol>&#13;
			<h1 id="_idParaDest-52"><a id="_idTextAnchor094"/>Decoding TFRecord and reconstructing the image</h1>&#13;
			<p>In the previous section, we learned how to write a <strong class="source-inline">.jpg</strong> image into a <strong class="source-inline">TFRecord</strong> dataset. Now <a id="_idIndexMarker138"/>we are going to see how to read it back and display it. An important requirement is <a id="_idIndexMarker139"/>that you must know the feature structure of the <strong class="source-inline">TFRecord</strong> protobuf as indicated by its keys. The feature structure is the same as the feature description used to build the <strong class="source-inline">TFRecord</strong> in the previous section. In other words, in the same way as a raw image was structured into a <strong class="source-inline">tf.Example</strong> protobuf with a defined feature description, we can use that feature description to parse or reconstruct the image using the same knowledge stored in the feature description:</p>&#13;
			<ol>&#13;
				<li value="1">Read <strong class="source-inline">TFRecord</strong> back from the path where it is stored:<p class="source-code">read_back_tfrecord = tf.data.TFRecordDataset('tfrecords-collection/maldives-1.tfrecord')</p></li>&#13;
				<li>Create a dictionary to specify the keys and values in <strong class="source-inline">TFRecord</strong>, and use it to parse all elements in the <strong class="source-inline">TFRecord</strong> dataset:<p class="source-code"># Create a dictionary describing the features.</p><p class="source-code">image_feature_description = {</p><p class="source-code">    'height': tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'width': tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'depth': tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'label': tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image_raw': tf.io.FixedLenFeature([], tf.string),</p><p class="source-code">}</p><p class="source-code">def _parse_image_function(example_proto):</p><p class="source-code">  # Parse the input tf.Example proto using the dictionary </p><p class="source-code">  # above.</p><p class="source-code">  return tf.io.parse_single_example(example_proto, </p><p class="source-code">  image_feature_description)</p><p class="source-code">parsed_image_dataset = read_back_tfrecord.map(_parse_image_function)</p><p>In the preceding code, <strong class="source-inline">_parse_image_function</strong> uses <strong class="source-inline">image_feature_description</strong> to parse the <strong class="source-inline">tfrecord</strong> protobuf. We use the <strong class="source-inline">map</strong> function to apply <strong class="source-inline">_parse_iamge_function</strong> to each image in <strong class="source-inline">read_back_tfrecord</strong>. </p></li>&#13;
				<li>Next, we <a id="_idIndexMarker140"/>will show <a id="_idIndexMarker141"/>the image using the following code:<p class="source-code">for image_features in parsed_image_dataset:</p><p class="source-code">  image_raw = image_features['image_raw'].numpy()</p><p class="source-code">  display.display(display.Image(data=image_raw))</p><p>Here's the output:</p></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer074" class="IMG---Figure">&#13;
					<img src="Images/image019.jpg" alt="Figure 3.10 – Displaying a dataset as an image&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 3.10 – Displaying a dataset as an image</p>&#13;
			<p>In this section, you learned how to convert raw data (an image) to <strong class="source-inline">TFRecord</strong> format, and verify that the conversion was done correctly by reading the <strong class="source-inline">TFRecord</strong> back and displaying it as an image. From this example, we <a id="_idIndexMarker142"/>can also see that in order to decode and <a id="_idIndexMarker143"/>inspect <strong class="source-inline">TFRecord</strong> data, we need the feature dictionary as was used during the encoding process. It is important to bear this in mind when working with <strong class="source-inline">TFRecord</strong>.</p>&#13;
			<h1 id="_idParaDest-53"><a id="_idTextAnchor095"/>Handling image data at scale</h1>&#13;
			<p>Handling data and their respective labels is simple if the everything can be loaded into Python engine's runtime memory. However, in the case of constructing a data pipeline for ingestion into a <a id="_idIndexMarker144"/>model training workflow, we want to ingest or stream data in batches so that we don't rely on the runtime memory to hold all the training data. In this case, maintaining the one-to-one relationship between the data (image) and label has to be preserved. We are going to see how to do this with <strong class="source-inline">TFRecord</strong>. We have already seen how to convert one image to a <strong class="source-inline">TFRecord</strong>. With multiple images, the conversion process is exactly the same for each image. </p>&#13;
			<p>Let's take a look at how we can reuse and refactor the code from the previous section to apply to a batch of images. Since you have seen how it was done for a single image, you will have little to no problem understanding the code and rationale here. </p>&#13;
			<p>Typically, when working with images for classification, we would organize images in the following directory structure, starting with a base directory (in other words, project name). The next level of directories are <strong class="source-inline">train</strong>, <strong class="source-inline">validation</strong>, and <strong class="source-inline">test</strong>. Within each of the three directories, there are image class directories. In other words, labels are the lowest directory name. For example, the directories may be organized into the following:</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;</p>&#13;
			<p>Then, below this level, we would have the following:</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;train</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;train/&lt;class_1_dir&gt;</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;train/&lt;class_2_dir&gt;</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;train/&lt;class_n_dir&gt;</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;validation</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;/validation/&lt;class_1_dir&gt;</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;/validation/&lt;class_2_dir&gt;</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;/validation/&lt;class_n_dir&gt;</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt;test</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt; /test /&lt;class_1_dir&gt;</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt; test/&lt;class_2_dir&gt;</p>&#13;
			<p class="source-code">/home/&lt;user_name&gt;/Documents/&lt;project_name&gt; /test/&lt;class_n_dir&gt;</p>&#13;
			<p>Another way of <a id="_idIndexMarker145"/>demonstrating the organization of images by classes is as follows:</p>&#13;
			<p class="source-code">-base_dir</p>&#13;
			<p class="source-code">       -train_dir</p>&#13;
			<p class="source-code">            -class_1_dir </p>&#13;
			<p class="source-code">            -class_2_dir</p>&#13;
			<p class="source-code">            -class_n_dir</p>&#13;
			<p class="source-code">       -validation_dir</p>&#13;
			<p class="source-code">           -class_1_dir</p>&#13;
			<p class="source-code">           -class_2_dir</p>&#13;
			<p class="source-code">           -class_n_dir</p>&#13;
			<p class="source-code">       -test</p>&#13;
			<p class="source-code">          -class_1_dir</p>&#13;
			<p class="source-code">          -class_2_dir</p>&#13;
			<p class="source-code">          -class_n_dir</p>&#13;
			<p>Images are placed in a directory based on their class. In this section, the example is simplified to the following structure in Cloud Storage:</p>&#13;
			<p class="source-code"><strong class="bold">-bucket</strong></p>&#13;
			<p class="source-code"><strong class="bold">	-badlands (Badlands national park)</strong></p>&#13;
			<p class="source-code"><strong class="bold">	-kistefos (Kistefos Museum)</strong></p>&#13;
			<p class="source-code"><strong class="bold">	-maldives (Maldives beaches)</strong></p>&#13;
			<p>You may find example jpg images in: <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_03/from_gs">https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_03/from_gs</a></p>&#13;
			<p>Each folder name corresponds to the label of images. The general procedure is as follows:</p>&#13;
			<ol>&#13;
				<li value="1">Copy images stored in the Cloud Storage bucket to the JupyterLab runtime.</li>&#13;
				<li>Map image <a id="_idIndexMarker146"/>filenames to their respective labels.</li>&#13;
				<li>Write each image's dimension, label, and byte array to the <strong class="source-inline">tf.Example</strong> protobuf.</li>&#13;
				<li>Store multiple protobufs together in a single <strong class="source-inline">TFRecord</strong>.</li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-54"><a id="_idTextAnchor096"/>Executing the steps</h2>&#13;
			<p>Here are the detailed <a id="_idIndexMarker147"/>steps that need to be run in each cell for this example: </p>&#13;
			<ol>&#13;
				<li value="1">Copy images from a storage bucket to the notebook runtime:<p class="source-code">!mkdir from_gs</p><p class="source-code">!gsutil cp -r gs://image-collection from_gs</p><p>In this step, a folder, <strong class="source-inline">from_gs</strong>, is created, and the <strong class="source-inline">image-collection</strong> bucket is copied into it.</p><p>Consider the base directory to be <strong class="source-inline">/from_gs/image-collection</strong>:</p><div id="_idContainer075" class="IMG---Figure"><img src="Images/image016.jpg" alt="Figure 3.11 – Base directory&#13;&#10;"/></div><p class="figure-caption">Figure 3.11 – Base directory</p></li>&#13;
				<li>Since this <a id="_idIndexMarker148"/>example is for demonstrating how to create <strong class="source-inline">TFRecordDataset</strong>, and not about partitioning data into training, validation, and testing, we can go right to the image class directory level, as shown in the following screenshot:<div id="_idContainer076" class="IMG---Figure"><img src="Images/image017.jpg" alt="Figure 3.12 – Image class directory level&#13;&#10;"/></div><p class="figure-caption">Figure 3.12 – Image class directory level</p><p>Upon inspecting one of <a id="_idIndexMarker149"/>the image class directories, we see the image files:</p><div id="_idContainer077" class="IMG---Figure"><img src="Images/image018.jpg" alt="Figure 3.13 – Image files&#13;&#10;"/></div><p class="figure-caption">Figure 3.13 – Image files</p></li>&#13;
				<li>Import libraries and designate the label names as <strong class="source-inline">CLASS_NAMES</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">import numpy as np</p><p class="source-code">import IPython.display as display</p><p class="source-code">import pathlib</p><p class="source-code">data_dir = pathlib.Path('from_gs/image-collection')</p><p class="source-code">data_dir = pathlib.Path(data_dir)</p><p class="source-code">CLASS_NAMES = np.array([item.name for item in data_dir.glob('*')])</p><p class="source-code">CLASS_NAMES</p><p>And CLASS_NAMES is captured properly as shown: </p><p class="source-code">array(['kistefos', 'badlands', 'maldives'], dtype='&lt;U8')</p></li>&#13;
				<li>Now we need to construct a dictionary that maps filenames to their respective label from <strong class="source-inline">CLASS_NAMES</strong>. We can use <strong class="source-inline">glob</strong> to encode directory and filename patterns. A <a id="_idIndexMarker150"/>couple of empty lists are created so that we may iterate recursively through directories, and append the path-to-filename into the filename list, and the label (denoted by the directory name) into the class list:<p class="source-code">import glob</p><p class="source-code">file_name_list = []</p><p class="source-code">class_list = []</p><p class="source-code">for name in glob.glob('from_gs/image-collection/*/*.jpg', recursive=True): </p><p class="source-code">  file_name_list.append(name)</p><p class="source-code">  # label is next to the last substring before the file </p><p class="source-code">  # name.</p><p class="source-code">  class_str = name.split('/')[-2]</p><p class="source-code">  idx_tuple = np.where(CLASS_NAMES == class_str)</p><p class="source-code">  idx = int(idx_tuple[0]) # first element of the idx </p><p class="source-code">  # tuple is the index</p><p class="source-code">  class_list.append(idx)</p></li>&#13;
				<li>Once both lists are populated in exact order, we may zip these lists together and encode the <a id="_idIndexMarker151"/>result as key-value pairs (dictionary):<p class="source-code">image_label_dict = dict(zip(file_name_list, class_list))</p><p class="source-code">image_label_dict should look similar to:</p><p class="source-code">{'from_gs/image-collection/kistefos/kistefos-1.jpg': 0,</p><p class="source-code"> 'from_gs/image-collection/kistefos/kistefos-3.jpg': 0,</p><p class="source-code"> 'from_gs/image-collection/kistefos/kistefos-2.jpg': 0,</p><p class="source-code"> 'from_gs/image-collection/badlands/badlands-1.jpg': 1,</p><p class="source-code"> 'from_gs/image-collection/badlands/badlands-2.jpg': 1,</p><p class="source-code"> 'from_gs/image-collection/maldives/maldives-2.jpg': 2,</p><p class="source-code"> 'from_gs/image-collection/maldives/maldives-1.jpg': 2}</p><p>As indicated, this is a dictionary, with the keys being the file path, and the values encoding respective labels (image classes).</p></li>&#13;
				<li>We want to convert our data into a <strong class="source-inline">tf.Example</strong> protobuf message, which is the predecessor format for <strong class="source-inline">TFRecord</strong>. <strong class="source-inline">tf.Example</strong> requires us to specify features in the image (metadata such as the image width pixel count, height pixel count, or data such as decimal values expressed as a <strong class="source-inline">numpy</strong> array). The three data types designated by <strong class="source-inline">tf.Example</strong> are <strong class="source-inline">tf.train.BytesList</strong>, <strong class="source-inline">tf.train.FloatList</strong>, and <strong class="source-inline">tf.train.Int64List</strong>. Therefore, commonly observed Pythonic data types need to be coerced into one of these three types. These are what each <strong class="source-inline">tf.Example</strong> data type can accept and coerce: <ul><li><strong class="source-inline">tf.train.BytesList</strong>: <strong class="source-inline">string</strong>, <strong class="source-inline">byte</strong>.</li><li><strong class="source-inline">tf.train.FloatList</strong>: <strong class="source-inline">float</strong> (float32, float64)</li><li><strong class="source-inline">tf.train.Int64List</strong>: <strong class="source-inline">bool</strong>, <strong class="source-inline">enum</strong>, <strong class="source-inline">int32</strong>, <strong class="source-inline">uint32</strong>, <strong class="source-inline">int64</strong>, <strong class="source-inline">uint64</strong> <p>In <a id="_idIndexMarker152"/>order to coerce common data types into the respective compatible <strong class="source-inline">tf.Example</strong> data type, the TensorFlow team provides the following helper functions: </p><p>If we want to convert a string of text (byte string) into a feature of the type <strong class="source-inline">tf.train.ByteList</strong>, the following function first converts the text (which is an eager tensor) into a <strong class="source-inline">numpy</strong> array, because <strong class="source-inline">tf.train.BytesList</strong> currently can only unpack <strong class="source-inline">numpy</strong> format into a byte list. After a protobuf message's value is casted to the <strong class="source-inline">ByteList</strong> type, then it is converted into a <strong class="source-inline">f</strong>eature object with the <strong class="source-inline">ByteList</strong> data type:</p><p class="source-code">def _bytes_feature(value):</p><p class="source-code">  if not tf.is_tensor(value):</p><p class="source-code">    value = tf.convert_to_tensor(value)</p><p class="source-code">  value = value.numpy()</p><p class="source-code">  bytes_list_msg = tf.train.BytesList(value = [value])</p><p class="source-code">  coerced_list = tf.train.Feature(bytes_list = </p><p class="source-code">  bytes_list_msg)</p><p class="source-code">  return coerced_list</p><p>If we need to convert numbers with floating points into a feature of the <strong class="source-inline">tf.train.FloatList</strong> type, then the following function does the job: </p><p class="source-code">def _float_feature(value):</p><p class="source-code">  float_list_msg = tf.train.FloatList(value=[value])</p><p class="source-code">  coerced_list = tf.train.Feature(float_list = </p><p class="source-code">  float_list_msg)</p><p class="source-code">  return coerced_list</p></li></ul></li>&#13;
				<li>And finally, for <a id="_idIndexMarker153"/>generating a feature of the <strong class="source-inline">tf.train.Int64List</strong> type, this can be done accordingly:<p class="source-code">def _int64_feature(value):</p><p class="source-code">  int64_list_msg = tf.train.Int64List(value=[value])</p><p class="source-code">  coerced_list = tf.train.Feature(int64_list = </p><p class="source-code">  int64_list_msg)</p><p class="source-code">  return coerced_list</p><p class="callout-heading">A note of caution</p><p class="callout"><strong class="source-inline">tf.train.Feature</strong> accepts one feature at a time. Each of these functions deals with converting and coercing one data feature at a time. This function is different from <strong class="source-inline">tf.train.Features</strong>, which accepts a dictionary of multiple features. In the next step, we are going to use <strong class="source-inline">tf.train.Features</strong>. </p></li>&#13;
				<li>Consolidate the workflow of creating the <strong class="source-inline">tf.Example</strong> protobuf message into a wrapper function. This function takes two inputs: a byte string that represents the image, and the corresponding label of that image.<p>Inside this function, first, the image shape is specified through the output of <strong class="source-inline">decode_jpeg</strong>, which converts a byte array into a <strong class="source-inline">jpeg</strong>. Dimension values are held in <strong class="source-inline">image_shape</strong> as a <strong class="source-inline">numpy</strong> array, and we may pass these values into the feature dictionary. Inside the <strong class="source-inline">feature</strong> dictionary, keys are specified, and corresponding values are derived and type casted from the helper functions in the <a id="_idIndexMarker154"/>previous step. The feature dictionary is then used to specify schemas of the feature into a <strong class="source-inline">features</strong> protobuf. The <strong class="source-inline">feature</strong> protobuf is then converted to an example protobuf, which is the final format to be serialized into a <strong class="source-inline">TFRecord</strong>:</p><p class="source-code">def image_example(image_str, label):</p><p class="source-code">  image_shape = tf.image.decode_jpeg(image_string).shape</p><p class="source-code">  feature = {</p><p class="source-code">      'height': _int64_feature(image_shape[0]),</p><p class="source-code">      'width': _int64_feature(image_shape[1]),</p><p class="source-code">      'depth': _int64_feature(image_shape[2]),</p><p class="source-code">      'label': _int64_feature(label),</p><p class="source-code">      'image_raw': _bytes_feature(image_string),</p><p class="source-code">  }</p><p class="source-code">  features_msg = tf.train.Features(feature=feature)</p><p class="source-code">  example_msg = tf.train.Example(features=features_msg)</p><p class="source-code">  return example_msg</p></li>&#13;
				<li>Write multiple image files into <strong class="source-inline">TFRecords</strong> by looping through <strong class="source-inline">image_label_dict</strong>:<p class="source-code">record_file = 'image-collection.tfrecords'</p><p class="source-code">with tf.io.TFRecordWriter(record_file) as writer:</p><p class="source-code">  for filename, label in image_image_label_dict.items():</p><p class="source-code">    image_string = open(filename, 'rb').read()</p><p class="source-code">    tf_example = image_example(image_string, label)</p><p class="source-code">    writer.write(tf_example.SerializeToString())</p></li>&#13;
			</ol>&#13;
			<p>In the preceding steps, we wrote all seven images in the three classes into a single <strong class="source-inline">TFRecord</strong>. </p>&#13;
			<h2 id="_idParaDest-55"><a id="_idTextAnchor097"/>Reading TFRecord and displaying it as images</h2>&#13;
			<p>To be assured <a id="_idIndexMarker155"/>about the image data as presented by the TFRecord format, it would be helpful if we can read it back and display it, just <a id="_idIndexMarker156"/>to be sure everything was formatted correctly. Now, let's read <strong class="source-inline">TFRecord</strong> back and display it as images:</p>&#13;
			<ol>&#13;
				<li value="1">Use the same API as in the previous section to read <strong class="source-inline">tfrecords</strong>:<p class="source-code">image_collection_dataset = tf.data.TFRecordDataset('image-collection.tfrecords')</p></li>&#13;
				<li>Define the specs for the dataset:<p class="source-code">feature_specs = {</p><p class="source-code">    'height': tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'width': tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'depth': tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'label': tf.io.FixedLenFeature([], tf.int64),</p><p class="source-code">    'image_raw': tf.io.FixedLenFeature([], tf.string),</p><p class="source-code">}</p></li>&#13;
				<li>Parse the protobuf. This is also exactly the same as shown in the previous section:<p class="source-code">def parse_image(example):</p><p class="source-code">  return tf.io.parse_single_example(example, </p><p class="source-code">  feature_specs)</p><p class="source-code">parsed_image_dataset = image_collection_dataset.map(parse_image)</p></li>&#13;
				<li>Display the images with the help of the following code:<p class="source-code">import IPython.display as display</p><p class="source-code">for image_features in parsed_image_dataset:</p><p class="source-code">  image_raw = image_features['image_raw'].numpy()</p><p class="source-code">  display.display(display.Image(data=image_raw))</p></li>&#13;
			</ol>&#13;
			<p>And you should see all the images contained in this protobuf message. For brevity, we will show only two <a id="_idIndexMarker157"/>images, and notice that <a id="_idIndexMarker158"/>Figures 3.14 and 3.15 have different dimensions, which are preserved and retrieved correctly by t<a id="_idTextAnchor098"/>he protobuf.</p>&#13;
			<p>Here's the first image:</p>&#13;
			<div>&#13;
				<div id="_idContainer078" class="IMG---Figure">&#13;
					<img src="Images/image0191.jpg" alt="Figure 3.14 – Image of Maldives class (1)&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 3.14 – Image of Maldives cla<a id="_idTextAnchor099"/>ss (1)</p>&#13;
			<p>And here's the second image:</p>&#13;
			<div>&#13;
				<div id="_idContainer079" class="IMG---Figure">&#13;
					<img src="Images/image021.jpg" alt="Figure 3.15 – Image of Maldives class (2)&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 3.15 – Image of Maldives class (2)</p>&#13;
			<p class="callout-heading">A few words about having multiple images in a single TFRecord</p>&#13;
			<p class="callout">You have seen that whether it's one image or multiple images, everything can be written in a single <strong class="source-inline">TFRecord</strong>. There is no right or wrong way as to which one is preferred, as factors such as memory and I/O bandwidth all come into play. A rule of thumb is to distribute your training images to at least 32 - 128 shards (each shard is a <strong class="source-inline">TFRecord</strong>) to maintain a file-level parallelism in the I/O process whenever you have sufficient images to do so.</p>&#13;
			<h1 id="_idParaDest-56"><a id="_idTextAnchor100"/>Summary</h1>&#13;
			<p>This chapter provided explanations and examples for dealing with commonly seen structured and unstructured data. We first looked at how to read and format a pandas DataFrame or CSV type of data structure and converted it to a dataset for efficient data ingestion pipelines. Then, as regards unstructured data, we used image files as examples. While dealing with image data, we have to organize these image files in a hierarchical pattern, such that labels can be easily mapped to each image file. <strong class="source-inline">TFRecord</strong> is the preferred format for handling image data, as it wraps the image dimension, label, and image raw bytes together in a format known as <strong class="source-inline">tf.Example</strong>. </p>&#13;
			<p>In the next chapter, we are going to take a look at reusable models and patterns that can consume these data structures we have learned here. </p>&#13;
		</div>&#13;
	</div></body></html>