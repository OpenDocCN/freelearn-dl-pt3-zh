<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Model-Based RL</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning algorithms are divided into two classes—model-free methods and model-based methods. These two classes differ by the assumption made about the model of the environment. Model-free algorithms learn a policy from mere interactions with the environment without knowing anything about it, whereas model-based algorithms <span>already </span>have a deep understanding of the environment and use this knowledge to take the next actions according to the dynamics of the model.</p>
<p>In this chapter, we'll give you a comprehensive overview of model-based approaches, highlighting their advantages and disadvantages vis-à-vis model-free approaches, and the differences that arise when the model is known or has to be learned. This latter division is important because it influences how problems are approached and the tools used to solve them. After this introduction, we'll talk about more advanced cases where model-based algorithms have to deal with high-dimensional observation spaces such as images. </p>
<p>Furthermore, we'll look at a class of algorithms that combine both model-based and model-free methods to learn both a model and a policy in high dimensional spaces. We'll learn their inner workings and give the reasons for using such methods. Then, to deepen our understanding of model-based algorithms, and especially of algorithms that combine both model-based and model-free approaches, we'll develop a state-of-the-art algorithm called <strong>model-ensemble trust region policy optimization</strong> (<strong>ME-TRPO</strong>) and apply it to a continuous inverted pendulum.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Model-based methods</li>
<li>Combining model-based with model-free learning</li>
<li>ME-TRPO applied to an inverted pendulum</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-based methods</h1>
                </header>
            
            <article>
                
<p>Model-free algorithms are a formidable kind of algorithm that have the ability to learn very complex policies and accomplish objectives in complicated and composite environments. As demonstrated in the latest works by OpenAI (<a href="https://openai.com/five/">https://openai.com/five/</a>) and <span>DeepMind (</span><a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii</a>), these algorithms can actually show long-term planning, teamwork, and adaptation to unexpected situations in challenge games such as StarCraft and Dota 2.</p>
<p>Trained agents have been able to beat top professional players. However, the biggest downside is in the huge number of games that need to be played in order to train agents to master these games. In fact, to achieve these <span>results, the algorithms have been scaled massively to let the agents play hundreds of years' worth of games against themselves. But, what's the problem with this approach?</span></p>
<p><span>Well, until you are training an agent for a simulator, you can gather as much experience as you want. The problem arises when you are running the agents in an environment as slow and complex as the world you live in. In this case, you cannot wait hundreds of years before seeing some interesting capabilities. So, can we develop an algorithm that uses fewer interactions with the real environment? Yes. And, as you probably remember, we already tackled this question in model-free algorithms.</span></p>
<p><span>The solution was to use off-policy algorithms. However, the gains were relatively marginal and not substantial enough for many real-world problems.</span></p>
<p>As you might expect, the answer (or at least one possible answer) is in model-based reinforcement learning algorithms. You have already developed a model-based algorithm. Do you remember which one? In <a href="f2414b11-976a-4410-92d8-89ee54745d99.xhtml">Chapter 3</a>, <em>Solving Problems with Dynamic Programming</em>, we used a model of the environment in conjunction with dynamic programming to train an agent to navigate a map with pitfalls. And because DP uses a model of the environment, it is considered a model-based algorithm.</p>
<p>Unfortunately, DP isn't usable in moderate or complex problems. So, we need to explore other types of model-based algorithms that can scale up and be useful in more challenging environments. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A broad perspective on model-based learning</h1>
                </header>
            
            <article>
                
<p>Let's first remember what a model is. A model consists of the transition dynamics and rewards <span>of an environment. </span><span>Transition dynamics are a mapping from a state,</span> <em>s,</em><span> and an action<em>,</em></span> <em>a,</em><span> to the next state<em>,</em> <em>s'</em></span><span>.</span></p>
<p><span>Having this information, the environment is fully represented by the model that can be used in its place. And if an agent has access to it, then the agent has the ability to predict its own future.</span></p>
<p><span>In the following sections, we'll see that a model can be either known or unknown. In the former case, the model is used as it is to exploit the dynamics of the environment; that is, the model provides a representation that is used in place of the environment. In the latter case, where the model of the environment is unknown, it can be learned by direct interaction with the environment. But since, in most cases, only an approximation of the environment is learned<em>,</em> additional factors have to be taken into account when using it.</span></p>
<p><span>Now that we have explained what a model is, we can see how can we use one and how it can help us to reduce the number of interactions with the environment. </span><span>The way in which a model is used depends on two very important factors—the model itself and the way in which actions are chosen.</span></p>
<p><span>Indeed, as we just noted, the model can be known or unknown, and actions can be planned or chosen by a learned policy. The algorithms vary a lot depending on each case, so let's first elaborate on the approaches used when the model is known (meaning that we already have the transition dynamics and rewards of the environment). </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A known model</h1>
                </header>
            
            <article>
                
<p>When a model is known, it can be used to simulate complete trajectories and compute the return for each of them. Then, the actions that yield the highest reward are chosen. This process is called <strong>planning</strong>, and the model of the environment is indispensable as it provides the information required to produce the next state (given a state and an action) and reward.</p>
<p>Planning algorithms are used everywhere, but the ones we are interested in differ from the type of action space on which they operate. Some of them work with discrete actions, others with continuous actions. </p>
<p>Planning algorithms for discrete actions are usually search algorithms that build a decision tree, such as the one illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1987 image-border" src="assets/6dd2283d-42c8-433b-b2dd-00a51f1b619b.png" style="width:16.33em;height:20.17em;"/></p>
<p>The current state is the root, the possible actions are represented by the arrows, and the other nodes are the states that are reached following a sequence of actions.</p>
<p>You can see that by trying every possible sequence of actions, you'll eventually find the optimal one. Unfortunately, in most problems, this procedure is intractable as the number of possible actions increases exponentially. Planning algorithms used for complex problems adopt strategies that allow planning by relying on a limited number of trajectories.</p>
<p>An algorithm of these, adopted also in AlphaGo, is called Monte Carlo Tree Search (MCTS). MCTS iteratively <span>builds </span>a decision tree by generating a finite series of simulated games, while sufficiently exploring parts of the tree that haven't been visited yet. Once a simulated game or trajectory reaches a leaf (that is, it ends the game), it backpropagates the results on the states visited and updates the information of win/loss or reward held by the nodes. Then, the action that yields to the next state with the higher win/loss ratio or reward is taken.</p>
<p>On the opposite side, planning algorithms that operate with continuous actions involve trajectory optimization techniques. These are much more difficult to solve than their counterpart with discrete actions, as they deal with an infinite-dimensional optimization problem.</p>
<p>Furthermore, many of them require the gradient of the model. An example is Model Predictive Control (MPC), which optimizes for a finite time horizon, but instead of executing the trajectory found, it only executes the first action. Doing so, MPC has a faster response compared to other methods with infinite time horizon planning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unknown model</h1>
                </header>
            
            <article>
                
<p>What should you<span> do when the model of the environment is unknown? Learn it! Almost everything we have seen so far involves learning. So, is it the best approach? Well, if you actually want to use a model-based approach, the answer is yes, and soon we'll see how to do it. However, this isn't always the best way to proceed.</span></p>
<p>In reinforcement learning, the end goal is to learn an optimal policy for a given task. Previously in this chapter, we said that the model-based approach is primarily used to reduce the number of interactions with the environment, but is this always true? Imagine your goal is to prepare an omelet. Knowing the exact breaking point of the egg isn't useful at all; you just need to know approximately how to break it. Thus, in this situation, a model-free algorithm that doesn't deal with the exact structure of the egg is more appropriate.</p>
<p>However, this shouldn't lead you to think that model-based algorithms are not worth it. For example, model-based approaches outweigh model-free approaches in situations where the model is much easier to learn than the policy.</p>
<p>The only way to learn a model is (unfortunately) through interactions with the environment. This is an obligatory step, as it allows us to acquire and create a dataset about the environment. Usually, the learning process takes place in a supervised fashion, where a function approximator (such as a deep neural network) is trained to minimize a loss function, such as the mean squared error loss between the transitions obtained from the environment and the prediction. An example of this is shown in the following diagram, where a deep neural network is trained to model the environment by predicting the next state, <em>s'</em>, and the reward<span>,</span> <em>r</em>, from a state<span>,</span> <em>s</em> and an action<span>,</span> <em>a</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1988 image-border" src="assets/a8af758b-2245-4228-8c7a-0558afd1e0c6.png" style="width:29.25em;height:12.17em;"/></p>
<p>There are other options besides neural networks, such as Gaussian processes, and Gaussian mixture models. In particular, Gaussian processes have the particularity of taking into account the <span>uncertainty of the </span>model and are regarded as being very data efficient. In fact, until the advent of deep neural networks, they were the most popular choice.</p>
<p>However, the main drawback of Gaussian processes is that they are slow with large datasets. Indeed, to learn more complex environments (thereby requiring bigger datasets), deep neural networks are preferred. Furthermore, deep neural networks can learn models of environments that have images as observations. </p>
<p>There are two main ways to learn a model of the environment; one in which the model is learned once and then kept fixed, and one in which the model is learned at the beginning but retrained once the plan or policy has changed. The two options are illustrated in the following diagram: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1989 image-border" src="assets/b5b6d82e-746e-4ab5-b6c3-281f1a5f0c17.png" style="width:35.83em;height:14.83em;"/></p>
<p><span>In the top half of the diagram, a sequential model-based algorithm is shown, where the agent interacts with the environment only before learning the model. In the bottom half, a cyclic approach to model-based learning is shown, where the model is refined with additional data from a different policy.</span></p>
<p>To understand how an algorithm can benefit from the second option, we have to define a key concept. In order to collect the dataset for learning the dynamics of the environment, you need a policy that lets you navigate it. But in the beginning, the policy may be deterministic or completely random. Thus, with a limited number of interactions, the space explored <span>will be very restricted.</span></p>
<p class="mce-root"/>
<p>This precludes the model from learning those parts of the environment that are needed to plan or learn optimal trajectories. But if the model is retrained with new interactions coming from a newer and better policy, it will iteratively adapt to the new policy and capture all the parts of the environment (from a policy perspective) that haven't been visited yet. This is called data aggregation. </p>
<p>In practice, in most cases, the model is unknown and is learned using data aggregation methods to adapt to the new policy produced. However, learning a model can be challenging, and the potential problems are the following:</p>
<ul>
<li><strong>Overfitting the model</strong>: The learned model overfits on a local region of the environment, missing its global structure.</li>
<li><strong>Inaccurate model</strong>: Planning or learning a policy on top of an imperfect model may induce a cascade of errors with potentially catastrophic conclusions. </li>
</ul>
<p>Good model-based algorithms that learn a model have to deal with those problems. A potential solution may be to use algorithms that estimate the uncertainty, such as Bayesian neural networks, or by using an ensemble of models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advantages and disadvantages</h1>
                </header>
            
            <article>
                
<p class="mce-root">When developing a reinforcement learning algorithm (all kinds of RL algorithms), there are three basic aspects to consider:</p>
<ul>
<li class="mce-root"><strong>Asymptotical performance</strong>: This is the maximum performance that an algorithm can achieve if it has infinite resources <span>available</span> in terms of both time and hardware. </li>
<li><strong>Wall clock time</strong>: This is the learning time required for an algorithm to reach a given performance with a given computational power.</li>
<li><strong>Sample efficiency</strong>: This is the number of interactions with the environment to reach a given performance. </li>
</ul>
<p>We already explored sample efficiency in both model-free and model-based RL, and we saw how the latter is much more sample efficient. But what about wall clock time and performance? Well, model-based algorithms usually have lower asymptotic performance and are slower to train than model-free algorithms. Generally, higher data efficiency occurs to the detriment of performance and speed.</p>
<p class="mce-root"/>
<p>One of the reasons behind the lower performance of model-based learning can be attributed to model inaccuracies (if it's learned) that introduce additional errors into the policies. The higher learning wall clock time is due to the slowness of the planning algorithm or to the higher number of interactions needed to learn the policy in an inaccurate learned environment. Furthermore, planning model-based algorithms experience slower inference time due to the high computational cost of planning, which still has to be done on each step.</p>
<p>In conclusion, you have to take into account the extra time required to train a model-based algorithm and recognize the lower <span>asymptotical </span>performance of these approaches. However, model-based learning is extremely useful when the model is easier to learn than the policy itself and when interactions with the environment are costly or slow.</p>
<p>From the two sides, we have model-free learning and model-based learning, both with compelling characteristics but distinct disadvantages. Can we take the best from both worlds?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining model-based with model-free learning</h1>
                </header>
            
            <article>
                
<p>We just saw how planning can be <span>computationally </span>expensive both during training and runtime, and how, in more complex environments, <span><span>planning algorithms </span></span>aren't able to achieve good performances. The other strategy <span>that we briefly hinted at</span> is to learn a policy. A policy is certainly much faster in inference as it doesn't have to plan at each step.</p>
<p>A simple, yet effective, way to learn a policy is to combine model-based with <span>model-free </span>learning. With the latest innovations in model-free algorithms, this combination has gained in popularity and is the most common approach <span>to date</span>. The algorithm we'll develop in the next section, ME-TRPO, is one such method. Let's dive further into these algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A useful combination</h1>
                </header>
            
            <article>
                
<p>As you know, model-free learning has good asymptotic performance but poor sample complexity. On the other side, model-based learning is efficient from a data standpoint, but struggles when it comes to more complex tasks. By combining model-based and model-free approaches, it is possible to reach a smooth spot where sample complexity decreases consistently, while achieving the high performance of model-free algorithms. </p>
<p>There are many ways to integrate both worlds, and the algorithms that propose to do it are very different from one another. For example, when the model is given (as they are in the games of Go and Chess), search tree and value-based algorithms can help each other to produce a better action value estimate.</p>
<p>Another example is to combine the learning of the environment and the policy directly in a deep neural network architecture so that the learned dynamics can contribute to the planning of a policy. Another strategy used by a fair number of algorithms is to use a learned model of the environment to generate additional samples to optimize the policy.</p>
<p>To put it in another way, the policy is trained by playing simulated games inside the learned model. This can be done in multiple ways, but the main recipe is shown in the pseudocode that follows: </p>
<pre><strong>while</strong> not done:<br/>    &gt; collect transitions <img class="fm-editor-equation" src="assets/d9893c82-54d0-4801-9363-081645300609.png" style="width:6.08em;height:1.33em;"/> from the real environment using a policy <img class="fm-editor-equation" src="assets/220e6354-af35-42a3-b55d-785b68b3be75.png" style="width:0.67em;height:0.67em;"/><br/>    &gt; add the transitions to the buffer <img class="fm-editor-equation" src="assets/a99438d0-7106-461d-b613-8ee6c2bb112e.png" style="width:0.83em;height:0.92em;"/><br/>    &gt; learn a model <img class="fm-editor-equation" src="assets/91b1f544-4676-4d9a-8acd-3c9ad16ae9d5.png" style="width:2.75em;height:1.17em;"/> that minimizes <img class="fm-editor-equation" src="assets/1cfea5ac-cc15-4cb3-a955-2dd42d30d6c1.png" style="width:8.33em;height:1.75em;"/> in a supervised way using data in <img class="fm-editor-equation" src="assets/8b91e827-d099-4d9f-a5bd-b3c40c9c5329.png" style="width:0.83em;height:0.92em;"/><br/>    &gt; (optionally learn <img class="fm-editor-equation" src="assets/48b17734-a830-4afc-81aa-de137a9bcbc7.png" style="width:2.67em;height:1.17em;"/>)<br/><br/><br/>    <strong>repeat</strong> K times: <br/>        &gt; sample an initial state <img class="fm-editor-equation" src="assets/0b0af635-c7df-4cfb-951a-5e27dd3bb30a.png" style="width:1.17em;height:1.00em;"/><br/>        &gt; simulate transitions <img class="fm-editor-equation" src="assets/14a2ab49-c9b4-4890-a9de-33d5dbc0b61a.png" style="width:7.33em;height:1.33em;"/> from the model <img class="fm-editor-equation" src="assets/67531361-e161-4b97-b692-0fac9b5e5ce0.png" style="width:5.75em;height:1.25em;"/>using a policy <img class="fm-editor-equation" src="assets/83e7f511-e5ee-4020-a843-9e416d633e65.png" style="width:0.75em;height:0.75em;"/><br/>        &gt; update the policy <img class="fm-editor-equation" src="assets/fe849621-5dc7-4803-b59a-85a096be2a6c.png" style="width:0.75em;height:0.75em;"/> using a model-free RL</pre>
<p>This blueprint involves two cycles. The outermost cycle collects data from the real environment to train the model, while, in the innermost cycle, the model generates simulated samples that are used to optimize the policy using model-free algorithms. Usually, the dynamics model is trained to minimize the MSE loss in a supervised fashion. The more precise the predictions made by the model, the more accurate the policy can be.</p>
<p class="mce-root"/>
<p>In the innermost cycle, either full or fixed-length trajectories <span>can be simulated</span>. In practice, the latter option can be adopted to mitigate the imperfections of the model. Furthermore, the trajectories can start from a random state sampled from the buffer that contains real transitions or from an initial state. The former option is preferred in situations where the model is inaccurate, because that prevents the trajectory from diverging too much from the real one. To illustrate this situation, take the following diagram. The trajectories that have been collected in the real environment are colored black, while those simulated are colored blue:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1990 image-border" src="assets/64607389-0e90-4fd1-a6f7-6114bf729bee.png" style="width:41.42em;height:28.58em;"/></p>
<p>You can see that the trajectories that start from an initial state are longer, and thus will diverge more rapidly as the errors of the inaccurate model propagate in all the subsequent predictions.</p>
<div class="packt_infobox">Note that you could do only a single iteration of the main cycle and gather all the data required to learn a decent approximated model of the environment. However, for the reasons outlined previously, it's better to use iterative data aggregation methods to cyclically retrain the model with transitions that come from the newer policy.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a model from images</h1>
                </header>
            
            <article>
                
<p>The methods <span>seen so far </span>that combine model-based and model-free learning have been designed especially to work with low-dimensional state spaces. So, how do we deal with high-dimensional observation spaces as images?</p>
<p>One choice is to learn in latent space. Latent space is a low-dimensional representation, also called embedding, <em>g(s),</em> of a high-dimensional input, <em>s</em>, such as an image. It can be produced by neural networks such as autoencoders. An example of an autoencoder is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1992 image-border" src="assets/16988e27-a6a1-409a-acbc-515b9fc29e61.png" style="width:33.75em;height:11.92em;"/></p>
<p><span>It comprises an encoder that maps the image to a small latent space, <em>g(s),</em></span><span> and the decoder that maps the latent space to the reconstructed image. As a result of the autoencoder, the latent space should represent the main features of an image in a constrained space so that two similar images are also similar in latent space.</span></p>
<p>In RL, the autoencoder may be trained to reconstruct the input<span>,</span> <em>S</em>, or trained to <span>predict </span>the next frame observation<span>,</span> <em>S'<span>,</span></em> (along with the reward<span>,</span> if needed). Then, we can use the latent space to learn both the dynamic model and the policy. The main benefit arising from this approach is the big gain in speed due to the smaller representation of the image. However, the policy learned in the latent space <span>may suffer from severe deficits when the autoencoder isn't able to recover the right representation.</span><span> </span></p>
<p>Model-based learning on high-dimensional spaces is still a very active area of research.</p>
<div class="packt_tip">If you are interested in model-based algorithms that learn from image observation, you may find the paper entitled <em>Model-Based Reinforcement Learning for Atari</em>, by Kaiser, quite interesting (<a href="https://arxiv.org/pdf/1903.00374.pdf">https://arxiv.org/pdf/1903.00374.pdf</a>).</div>
<p>So far, we have covered model-based learning and its combination with model-free learning in a more figurative and theoretical way. Although it's indispensable in terms of understanding these paradigms, we want to put them into practice. So, without further ado, let's focus on the details and implementation of our first model-based algorithm. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ME-TRPO applied to an inverted pendulum</h1>
                </header>
            
            <article>
                
<p>Many variants exist of the vanilla model-based and model-free algorithms introduced in the pseudocode in the <em>A useful combination</em> section. Pretty much all of them propose different ways to deal with the imperfections of the model of the environment.</p>
<p>This is a key problem to address in order to reach the same performance as model-free methods. Models learned from complex environments will always have some inaccuracies. So, the main challenge is to estimate or control the <span>uncertainty of the </span>model to stabilize and accelerate the learning process. </p>
<p>ME-TRPO proposes the use of an ensemble of models to maintain the model uncertainty and regularize the learning process. The models are deep neural networks with different weight initialization and training data. Together, they provide a more robust general model of the environment that is less prone to exploit regions where insufficient data is available.</p>
<p>Then, the policy is learned from trajectories simulated with the ensemble. In particular, the algorithm chosen to learn the policy is <strong>trust region policy optimization</strong> (<strong>TRPO</strong>), which was explained in <a href="4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml">Chapter 7</a>, <em>TRPO and PPO Implementation</em>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding ME-TRPO</h1>
                </header>
            
            <article>
                
<p>In the first part of ME-TRPO, the dynamics of the environment (that is, the ensemble of models)<span> </span><span>are learned</span>. The algorithm starts by interacting with the environment with a random policy, <img class="fm-editor-equation" src="assets/df3fb161-2261-4a34-a110-c8d0c8a18390.png" style="width:0.83em;height:0.83em;"/><span>,</span> to collect a dataset of transitions<span>,</span> <img class="fm-editor-equation" src="assets/69af50a2-9788-4b93-a702-0f8ba8f22be9.png" style="width:5.67em;height:1.50em;"/>. This dataset is then used to train all the dynamic models, <img class="fm-editor-equation" src="assets/2b5b58c7-fe8c-47f9-8939-26fc497f4355.png" style="width:1.42em;height:1.33em;"/>, in a supervised fashion. The models, <img class="fm-editor-equation" src="assets/2b5b58c7-fe8c-47f9-8939-26fc497f4355.png" style="width:1.42em;height:1.33em;"/>, are initialized with different random weights and are trained with different mini-batches. To avoid overfitting issues, a validation set is created from the dataset. Also, a mechanism of <em>early stopping</em> (a regularization technique widely used in machine learning) interrupts the training process whenever the loss on the validation set stops improving.</p>
<p>In the second part of the algorithm, the policy is learned with TRPO. Specifically, the policy is trained on the data gathered from the learned models, which we'll also call the <em>simulated environment,</em> instead of the real environment. To avoid the policy exploiting inaccurate regions of a single learned model, the policy, <img class="fm-editor-equation" src="assets/5fcbff6e-968d-4c05-b106-3241552c4c7c.png" style="width:0.75em;height:0.75em;"/>, is trained using the predicted transitions from the whole ensemble of models, <img class="fm-editor-equation" src="assets/2b5b58c7-fe8c-47f9-8939-26fc497f4355.png" style="width:1.42em;height:1.33em;"/>. In particular, the policy is trained on the simulated dataset composed of transitions acquired from the models, <img class="fm-editor-equation" src="assets/2b5b58c7-fe8c-47f9-8939-26fc497f4355.png" style="width:1.42em;height:1.33em;"/>, randomly chosen among the ensemble. During training, the policy is monitored constantly, and the process stops as soon as the performance stops improving. </p>
<p>Finally, the cycle constituted by the <span>two parts is repeated until convergence. However, at each new iteration, the data from the real environment is collected by running the newly learned policy, <img class="fm-editor-equation" src="assets/5fcbff6e-968d-4c05-b106-3241552c4c7c.png" style="width:0.75em;height:0.75em;"/>, and the data collected is aggregated with the dataset of the previous iterations</span>. The ME-TRPO algorithm is briefly summarized in the following pseudocode:</p>
<pre>Initialize randomly policy <img class="fm-editor-equation" src="assets/2bdfad59-3231-4dbf-adff-e26fa30262f0.png" style="width:0.75em;height:0.75em;"/> and models <img class="fm-editor-equation" src="assets/7714865c-36ca-44fd-bb6d-7bdcd4da6ced.png" style="width:4.17em;height:1.33em;"/><br/>Initialize empty buffer <img class="fm-editor-equation" src="assets/98b4aab5-7b53-4b90-b120-5cfd7d64eaac.png" style="width:0.83em;height:0.92em;"/><br/><br/><strong>while</strong> not done:<br/>    &gt; populate buffer <img class="fm-editor-equation" src="assets/87fb4413-65b4-407a-b3fe-07ec1729c017.png" style="width:0.83em;height:0.92em;"/> with transitions <img class="fm-editor-equation" src="assets/c7da6067-2cad-4d3d-bcbe-bb7e20bc146c.png" style="width:5.33em;height:1.42em;"/> from the real environment using policy <img class="fm-editor-equation" src="assets/864fb96b-74bf-4868-b8d6-b09b4bddb292.png" style="width:0.75em;height:0.75em;"/> (or random)<br/>    &gt; learn models <img class="fm-editor-equation" src="assets/7e86543b-089b-42bc-80fb-7568726aff40.png" style="width:9.58em;height:1.42em;"/> that minimize <img class="fm-editor-equation" src="assets/c2651954-ea44-41ba-afd8-02c89297e3aa.png" style="width:9.50em;height:1.83em;"/> in a supervised way using data in <img class="fm-editor-equation" src="assets/02620f2a-db96-48b1-bf39-a0ca3baa3432.png" style="width:0.83em;height:0.92em;"/><br/><br/>    <strong>until </strong>convergence: <br/>        &gt; sample an initial state <img class="fm-editor-equation" src="assets/fc9cb5b4-1196-4de8-91e0-580f9de00f21.png" style="width:1.17em;height:1.00em;"/><br/>        &gt; simulate transitions <img class="fm-editor-equation" src="assets/a41a25aa-421c-422a-aa5b-2822403b8d95.png" style="width:7.00em;height:1.50em;"/> using models <img class="fm-editor-equation" src="assets/9474a037-8a50-475e-97ad-9dba83c22e9e.png" style="width:4.08em;height:1.75em;"/> and the policy <img class="fm-editor-equation" src="assets/44703a8c-a897-4a68-b7e9-216a6a8b8e18.png" style="width:0.75em;height:0.75em;"/><br/>        &gt; take a TRPO update to optimize policy <img class="fm-editor-equation" src="assets/8283820e-4834-4b7c-b0a1-01edbb766a35.png" style="width:0.75em;height:0.75em;"/></pre>
<p>An important note to make here is that, unlike most model-based algorithms, the reward is not embedded in the model of the environment. Therefore, ME-TRPO assumes that the reward function is known.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing ME-TRPO</h1>
                </header>
            
            <article>
                
<p><span>The code of ME-TRPO is quite long and, i</span>n this section, we won't give you the full code. Also, many parts are not interesting, and all the code concerning TRPO has already been discussed in <a href="4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml">Chapter 7</a>, <em>TRPO and PPO Implementation</em>. However, if you are interested in the complete implementation, or if you want to play with the algorithm, the full code is available in the GitHub repository of this chapter. </p>
<p class="mce-root"/>
<p>Here, we'll provide an explanation and the implementation of the following:</p>
<ul>
<li>The inner cycle, where the games are simulated and the policy is optimized</li>
<li>The function that trains the models</li>
</ul>
<p>The remaining code is very similar to that of TRPO.</p>
<p><span>The following steps will guide us through the process of </span>building and implementing the core of ME-TRPO:</p>
<ol>
<li><strong>Changing the policy</strong>: The only change in the interaction procedure with the real environment is the policy. In particular, the policy will act randomly on the first episode but, on the others, it will sample the actions from a Gaussian distribution with a random standard deviation fixed at the start of the algorithm. This change is done by replacing the line, <kbd><span>act, val</span> <span>=</span> <span>sess.</span><span>run</span><span>([a_sampl, s_values],</span> <span>feed_dict</span><span>=</span></kbd><span><kbd>{obs_ph:[env.n_obs]})</kbd></span>, <span>in the TRPO implementation </span>with <span>the following lines of code:</span></li>
</ol>
<pre style="padding-left: 60px">...<br/>if ep == 0:<br/>    act = env.action_space.sample()<br/>else:<br/>    act = sess.run(a_sampl, feed_dict={obs_ph:[env.n_obs], log_std:init_log_std})<br/>...</pre>
<ol start="2">
<li><strong>Fitting the deep neural networks,</strong> <img class="fm-editor-equation" src="assets/70ab6f5a-ae81-47f0-8607-637f56ff429a.png" style="width:1.58em;height:1.50em;"/>: The neural networks learn the model of the environment with the dataset acquired in the preceding step. The dataset is divided into a training and a validation set, wherein the validation set is used by the early stopping technique to determine whether it is worth continuing with the training:</li>
</ol>
<pre style="padding-left: 60px"><span>...<br/>model_buffer.generate_random_dataset()<br/>train_obs, train_act, _, train_nxt_obs, _ </span><span>=</span><span> model_buffer.</span><span>get_training_batch</span><span>()<br/></span><span>valid_obs, valid_act, _, valid_nxt_obs, _ </span><span>=</span><span> model_buffer.</span><span>get_valid_batch</span><span>()<br/></span><span>print</span><span>(</span><span>'Log Std policy:'</span><span>, sess.</span><span>run</span><span>(log_std))<br/>        <br/></span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(num_ensemble_models):<br/></span><span>train_model</span><span>(train_obs, train_act, train_nxt_obs, valid_obs, valid_act, valid_nxt_obs, step_count, i)</span></pre>
<p class="mce-root"/>
<p style="padding-left: 60px"><kbd>model_buffer</kbd> is an instance of the <kbd>FullBuffer</kbd> <span>class </span>that contains the samples generated by the environment, and <kbd>generate_random_dataset</kbd> creates two partitions for training and validation, which are then returned by calling <kbd>get_training_batch</kbd> and <kbd>get_valid_batch</kbd>.</p>
<p style="padding-left: 60px">In the next lines, each model is trained with the <kbd>train_model</kbd> function by passing the datasets, the current number of steps, and the index of the model that has to be trained. <kbd>num_ensemble_models</kbd> is the total number of models that populate the ensemble. In the ME-TRPO paper, it is shown that 5 to 10 models are sufficient. The argument, <kbd>i</kbd>, establishes which model of the ensemble has to be optimized.</p>
<ol start="3">
<li><strong>Generating fictitious trajectories in the simulated environments and fitting the policy</strong>:</li>
</ol>
<pre><span>        best_sim_test </span><span>=</span><span> np.</span><span>zeros</span><span>(num_ensemble_models)<br/>        </span><span>for</span><span> it </span><span>in</span><span> </span><span>range</span><span>(</span><span>80</span><span>):<br/></span><span>            </span><span>obs_batch, act_batch, adv_batch, rtg_batch </span><span>=</span><span> </span><span>simulate_environment</span><span>(sim_env, action_op_noise, simulated_steps)<br/><br/>            </span><span>policy_update</span><span>(obs_batch, act_batch, adv_batch, rtg_batch)</span></pre>
<p style="padding-left: 60px"><span>This is repeated 80 times or at least until the policy continues improving.</span> <kbd>simulate_environment</kbd> collects a dataset (constituted by observations, actions, advantages, values, and return values) by rolling the policy in the simulated environment (represented by the learned models). In our case, the policy is represented by the function, <kbd>action_op_noise</kbd>, which, when given a state, returns an action following the learned policy. Instead, the environment, <kbd>sim_env</kbd>, is a model of the environment, <img class="fm-editor-equation" src="assets/70ab6f5a-ae81-47f0-8607-637f56ff429a.png" style="width:1.58em;height:1.50em;"/>, chosen randomly at each step among those in the ensemble. <span>The</span> <span>last argument</span><span> passed to the</span> <span><kbd>simulated_environment</kbd> function </span><span>is</span> <kbd>simulated_steps</kbd>, <span>which establishes the number of steps to take in the fictitious environments.</span></p>
<p style="padding-left: 60px"><span>Ultimately, t</span><span>he</span> <kbd>policy_update</kbd><span> function does a TRPO</span><span> step</span> <span>to update the policy with the data collected in the fictitious environments. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Implementing the early step mechanism and evaluating the policy: <span>The early stopping mechanism prevents the policy from overfitting on the models of the environment. It works by monitoring the performance of the policy on each separate model. I</span><span>f the percentage of models on which the policy improved exceeds a certain threshold, then the cycle is terminated. This should be a good indication of whether the policy has started to overfit. Note that, unlike the training, <span>during testing, </span>the policy is tested on one model at a time. During training, each trajectory is produced by all the learned models of the environment:</span></li>
</ol>
<pre>            if (it+1) % 5 == 0:<br/>                sim_rewards = []<br/><br/>                for i in range(num_ensemble_models):<br/>                    sim_m_env = NetworkEnv(gym.make(env_name), model_op, pendulum_reward, pendulum_done, i+1)<br/>                    mn_sim_rew, _ = test_agent(sim_m_env, action_op, num_games=5)<br/>                    sim_rewards.append(mn_sim_rew)<br/><br/>                sim_rewards = np.array(sim_rewards)<br/>                if (np.sum(best_sim_test &gt;= sim_rewards) &gt; int(num_ensemble_models*0.7)) \<br/>                    or (len(sim_rewards[sim_rewards &gt;= 990]) &gt; int(num_ensemble_models*0.7)):<br/>                    break<br/>                else:<br/>                  best_sim_test = sim_rewards</pre>
<p style="padding-left: 60px">The evaluation of the policy is done every five training iterations. For each model of the ensemble, a new object of the <kbd>NetworkEnv</kbd> class is instantiated. It provides the same functionalities of a real environment but, under the hood, it returns transitions from a learned model of the environment. <kbd>NetworkEnv</kbd> does this by inheriting <kbd>Gym.wrapper</kbd> and overriding the <kbd>reset</kbd> and <kbd>step</kbd> functions. The first parameter of the constructor is a real environment that is used merely to get a real initial state, while <kbd>model_os</kbd> is a function that, when given a state and action, produces the next state. Lastly, <kbd>pendulum_reward</kbd> and <kbd>pendulum_done</kbd> are functions that return the reward and the done flag. These two functions are built around the particular functionalities of the environment.</p>
<ol start="5">
<li><span><strong>Training the dynamic model</strong>: </span>The <kbd>train_model</kbd><span> function optimizes a model to predict the future state. It is very simple to understand. We used this function in step 2, when we were training the ensemble of models. <kbd>train_model</kbd> is an inner function and takes the arguments that we saw earlier. On each ME-TRPO iteration of the outer loop, we retrain all the models, that is, </span><span>we train the models starting from their random initial weights; we don't resume from the preceding optimization. Hence, every time <kbd>train_model</kbd> is called and before the training takes place, we restore the initial random weights of the model</span>. The following code snippet restores the weights and computes the loss before and after this operation:</li>
</ol>
<pre>    def train_model(tr_obs, tr_act, tr_nxt_obs, v_obs, v_act, v_nxt_obs, step_count, model_idx):<br/>        mb_valid_loss1 = run_model_loss(model_idx, v_obs, v_act, v_nxt_obs)<br/><br/>        model_assign(model_idx, initial_variables_models[model_idx])<br/><br/>        mb_valid_loss = run_model_loss(model_idx, v_obs, v_act, v_nxt_obs)</pre>
<p style="padding-left: 60px"><kbd>run_model_loss</kbd> returns the loss of the current model, and <kbd>model_assign</kbd> restores the parameters that are in <kbd>initial_variables_models[model_idx].</kbd></p>
<p style="padding-left: 60px">We then train the model, as long as the loss on the validation set improved in the last <kbd>model_iter</kbd> iterations. But because the best model may not be the last one, we keep track of the best one and restore its parameters at the end of the training. We also randomly shuffle the dataset and divide it into mini-batches. The code is as follows:</p>
<pre>        acc_m_losses = []<br/>        last_m_losses = []<br/>        md_params = sess.run(models_variables[model_idx])<br/>        best_mb = {'iter':0, 'loss':mb_valid_loss, 'params':md_params}<br/>        it = 0<br/><br/>        lb = len(tr_obs)<br/>        shuffled_batch = np.arange(lb)<br/>        np.random.shuffle(shuffled_batch)<br/><br/>        while best_mb['iter'] &gt; it - model_iter:<br/>            <br/>            # update the model on each mini-batch<br/>            last_m_losses = []<br/>            for idx in range(0, lb, model_batch_size):<br/>                minib = shuffled_batch[idx:min(idx+minibatch_size,lb)]<br/>                <br/>                if len(minib) != minibatch_size:<br/>                  _, ml = run_model_opt_loss(model_idx, tr_obs[minib], tr_act[minib], tr_nxt_obs[minib])<br/>                  acc_m_losses.append(ml)<br/>                  last_m_losses.append(ml)<br/><br/>            # Check if the loss on the validation set has improved<br/>            mb_valid_loss = run_model_loss(model_idx, v_obs, v_act, v_nxt_obs)<br/>            if mb_valid_loss &lt; best_mb['loss']:<br/>                best_mb['loss'] = mb_valid_loss<br/>                best_mb['iter'] = it<br/>                best_mb['params'] = sess.run(models_variables[model_idx])<br/><br/>            it += 1<br/><br/>        # Restore the model with the lower validation loss<br/>        model_assign(model_idx, best_mb['params'])<br/><br/>        print('Model:{}, iter:{} -- Old Val loss:{:.6f} New Val loss:{:.6f} -- New Train loss:{:.6f}'.format(model_idx, it, mb_valid_loss1, best_mb['loss'], np.mean(last_m_losses)))</pre>
<p><kbd>run_model_opt_loss</kbd> is a function that executes the optimizer of the model with the <kbd>model_idx</kbd> <span>index.</span></p>
<p>This concludes the implementation of ME-TRPO. In the next section, we'll see how it performs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimenting with RoboSchool</h1>
                </header>
            
            <article>
                
<p>Let's test ME-TRPO on <strong>RoboSchoolInvertedPendulum</strong>, a continuous inverted pendulum environment similar to the well-known discrete control counterpart, CartPole. A screenshot of <strong><span>RoboSchoolIn</span><span>vertedPe</span><span>ndulum-</span><span>v</span></strong><span><strong>1</strong> is shown here:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1993 image-border" src="assets/71ea6457-3ae9-4e07-931d-c24748beabda.png" style="width:28.50em;height:14.50em;"/></p>
<p>The goal is to keep the pole upright by moving the cart. A reward of +1 is obtained for every step that the pole points upward.</p>
<p>Considering that ME-TRPO needs the reward function and, consequently, a <kbd>done</kbd> function, we have to define both for this task. To this end, we defined <kbd>pendulum_reward</kbd>, which returns 1 no matter what the observation and actions are:</p>
<div>
<pre><span>def</span><span> </span><span>pendulum_reward</span><span>(</span><span>ob</span><span>, </span><span>ac</span><span>):<br/>    </span><span>return</span><span> </span><span>1</span></pre></div>
<p> <kbd>pendulum_done</kbd> returns <kbd>True</kbd> if the absolute value of the angle of the pole is higher than a fixed threshold. We can retrieve the angle directly from the state. In fact, the third and fourth elements of the state are the cosine and sine of the angle, <span>respectively</span>. We can then arbitrarily choose one of the two to compute the angle. Hence, <kbd>pendulum_done</kbd> is as follows:</p>
<div>
<pre><span>def</span><span> </span><span>pendulum_done</span><span>(</span><span>ob</span><span>):<br/>    </span><span>return</span><span> np.</span><span>abs</span><span>(np.</span><span>arcsin</span><span>(np.</span><span>squeeze</span><span>(ob[</span><span>3</span><span>]))) </span><span>&gt;</span><span> </span><span>.2</span></pre></div>
<p><span>Besides the usual hyperparameters of TRPO that remain almost unchanged compared to the ones used in <a href="4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml">Chapter 7</a>, <em>TRPO and PPO Implementation</em>, ME-TRPO asks for the following:</span></p>
<ul>
<li><span>The learning rate of the dynamic models' optimizer, <kbd>mb_lr</kbd></span></li>
<li><span><span>The mini-batch size, <kbd>model_batch_size</kbd>, which is used to train the dynamic models</span></span></li>
<li><span>The number of simulated steps to execute on each iteration, <kbd>simulated_steps</kbd> (this is also the batch size used to train the policy)</span></li>
<li><span>The number of models that constitute the ensemble, <kbd>num_ensemble_models</kbd></span></li>
<li><span>The number of iterations to wait before interrupting the <kbd>model_iter</kbd> training of the model if the validation hasn't decreased</span></li>
</ul>
<p>The values of these hyperparameters used in this environment are as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 60.4955%" class="CDPAlignCenter CDPAlign"><strong><span>Hyperparameters</span></strong></td>
<td style="width: 34.5045%" class="CDPAlignCenter CDPAlign"><strong>Values</strong></td>
</tr>
<tr>
<td style="width: 60.4955%" class="CDPAlignCenter CDPAlign"><span>Learning rate (<kbd>mb_lr</kbd>)</span></td>
<td style="width: 34.5045%" class="CDPAlignCenter CDPAlign">1e-5</td>
</tr>
<tr>
<td style="width: 60.4955%" class="CDPAlignCenter CDPAlign"><span>Model batch size (<kbd>model_batch_size</kbd>)</span></td>
<td style="width: 34.5045%" class="CDPAlignCenter CDPAlign">50</td>
</tr>
<tr>
<td style="width: 60.4955%" class="CDPAlignCenter CDPAlign"><span>Number of simulated steps (<kbd>simulated_steps</kbd>)</span></td>
<td style="width: 34.5045%" class="CDPAlignCenter CDPAlign">50000</td>
</tr>
<tr>
<td style="width: 60.4955%" class="CDPAlignCenter CDPAlign"><span>Number of models (<kbd>num_ensemble_models</kbd>)</span></td>
<td style="width: 34.5045%" class="CDPAlignCenter CDPAlign">10</td>
</tr>
<tr>
<td style="width: 60.4955%" class="CDPAlignCenter CDPAlign"><span>Early stopping iterations (<kbd>model_iter</kbd>)</span></td>
<td style="width: 34.5045%" class="CDPAlignCenter CDPAlign">15</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results on RoboSchoolInvertedPendulum</h1>
                </header>
            
            <article>
                
<p>The performance graph is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1994 image-border" src="assets/edef2f65-2ce0-4389-b722-16b62f80a031.png" style="width:119.58em;height:72.42em;"/></p>
<p>The reward is plotted as a function of the number of interactions with the real environment. After 900 steps and about 15 games, the agent achieves the top performance of 1,000. The policy updated itself 15 times and learned from 750,000 simulated steps. From a computational point of view, the algorithm trained for about 2 hours on a mid-range computer.</p>
<p>We noted that the results have very high variability and, if trained with different random seeds, you can obtain very different performance curves. This is also true for model-free algorithms, but here, the differences are more acute. One reason for this may be the different data collected in the real environment. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we took a break from model-free algorithms and started discussing and exploring algorithms that learn from a model of the environment. We looked at the key reasons behind the change of paradigm that inspired us to develop this kind of algorithm. We then distinguished two main cases that can be found when dealing with a model, the first in which the model is already known, and the second in which the model has to be learned.</p>
<p>Moreover, we learned how the model can either be used to plan the next actions or to learn a policy. There's no fixed rule to choose one over the other, but generally, it is related to the complexity of the action and observation space and the inference speed. We then investigated the advantages and disadvantages of model-free algorithms and deepened our understanding of how to learn a policy with model-free algorithms by combining them with model-based learning. This revealed a new way to use models in very high-dimensional observation spaces such as images. </p>
<p>Finally, to better grasp all the material related to model-based algorithms, we developed ME-TRPO. This proposed dealing with the uncertainty of the model by using an ensemble of models and trust region policy optimization to learn the policy. All the models are used to predict the next states and thus create simulated trajectories on which the policy is learned. As a consequence, the policy is trained entirely on the learned model of the environment.</p>
<p>This chapter concludes the arguments about model-based learning and, in the next one, we'll introduce new genera of learning. We'll talk about algorithms that learn by imitation. Moreover, we'll develop and train an agent that, by following the behavior of an expert, will be able to play FlappyBird.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Would you use a model-based or a model-free algorithm if you had only 10 games in which to train your agent to play checkers?</li>
<li>What are the disadvantages of model-based algorithms?</li>
<li>If a model of the environment is unknown, how can it be learned?</li>
<li>Why are data aggregation methods<span> used</span>?</li>
<li>How does ME-TRPO stabilize training?</li>
<li>How does using an ensemble of models improve policy learning?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>To expand your knowledge of model-based algorithms that learn policies from image observations, read the paper <em>Model-Based Reinforcement Learning for Atari</em>: <a href="https://arxiv.org/pdf/1903.00374.pdf">https://arxiv.org/pdf/1903.00374.pdf</a>.</li>
<li><span>To read the original paper relating to ME-TRPO, follow this link: <a href="https://arxiv.org/pdf/1802.10592.pdf">https://arxiv.org/pdf/1802.10592.pdf</a>.</span></li>
</ul>


            </article>

            
        </section>
    </body></html>